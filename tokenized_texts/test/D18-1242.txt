Knowledge Base Question Answering via Encoding of Complex Query Graphs
Kangqi Luo1
Fengli Lin1
Xusheng Luo2
Kenny Q. Zhu1
1Shanghai Jiao Tong University , Shanghai , China 2Alibaba Group , Hangzhou , China { luokangqi,fenglilin}@sjtu.edu.cn , lxs140564@alibaba-inc.com , kzhu@cs.sjtu.edu.cn
Abstract
Answering complex questions that involve multiple entities and multiple relations using a standard knowledge base is an open and challenging task .
Most existing KBQA approaches focus on simpler questions and do not work very well on complex questions because they were not able to simultaneously represent the question and the corresponding complex query structure .
In this work , we encode such complex query structure into a uniform vector representation , and thus successfully capture the interactions between individual semantic components within a complex question .
This approach consistently outperforms existing methods on complex questions while staying competitive on simple questions .
1
Introduction
as
using
question
Freebase
structured
knowledge - based
knowledge ( Bollacker et al , 2007 )
answering The ( KBQA ) is a task which takes a natural language question as input and returns a factual bases answer 2008 ) , such YAGO ( Suchanek et al , and DBpedia ( Auer et al , 2007 ) .
One simple example is a question like this : “ What ’s the capital of the United States ? ”
A common answer to such question is to identify the focus entity and the main relation predicate ( or a sequence ) in the question , and map the question to a triple fact query ( U S , capital , ? ) over KB .
The object answers are returned by executing the query .
The mapping above is typically learned from question - answer pairs through distant supervision .
While the above question can be answered by querying a single predicate or predicate sequence in the KB , many other more complex questions can not , e.g. the question in Figure 1 .
To answer the question “ What is the second longest river in United States ” , we need to infer several semantic
2
MaxAtN
river
isA
A
length
contained_by
US
What is the second longest river in the United States ?
Figure 1 : Running example of complex question .
clues : 1 ) the answer is contained by United States ; 2 ) the answer is a river ; 3 ) the answer ranks second by its length in descending order .
Thus , multiple predicates are required to constrain the answer set , and we call such questions “ complex questions ” throughout this paper .
For answering complex questions , it ’s more important to understand the compositional semantic meanings of the question .
As a classic branch of KBQA solutions , semantic parsing ( SP ) technique ( Berant et al , 2013 ; Yih et al , 2015 ; Reddy et al , 2016 ; Hu et al , 2018 ) aims at learning semantic parse trees or equivalent query graphs 1 for representing semantic structures of the questions .
For example in Figure 1 , the query graph forms a tree shape .
The answer node A , serving as the root of the tree , is the variable vertex that represents the real answer entities .
The focus nodes ( US , river , 2nd ) are extracted from the mentions of the question , and they constrain the answer node via predicate sequences in the knowledge base .
Recently , neural network ( NN ) models have shown great promise in improving the performance of KBQA systems , and SP+NN techniques become the state(Qu et
al , of - the - art on several KBQA datasets 2018 ; Bao et al , 2016 ) .
According to the discussion above , our work extends the current research in the SP+NN direction .
The common step of SP - based approaches
1The term “ query graph ” is interchangeable with “ query structure ” and “ semantic parsing tree ” throughout this paper .
Proceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing , pages2185–2194Brussels , Belgium , October31 - November4,2018.c(cid:13)2018AssociationforComputationalLinguistics2185  is to ﬁrst collect candidate query graphs using bottom up parsing ( Berant et al , 2013 ; Cai and Yates , 2013 ) or staged query generation methods ( Yih et al , 2015 ; Bao et al , 2016 ) , then predict the best graph mainly based on the semantic similarity with the given question .
Existing NN - based methods follow an encode - andcompare framework for answering simple questions , where both the question and the predicate sequence are encoded as semantic vectors in a common embedding space , and the semantic similarity is calculated by the cosine score between In order to deﬁne the similarity funcvectors .
tion between one question and a complex query graph , an intuitive solution is to split the query graph into multiple semantic components , as the predicate sequences separated by dashed boxes in Figure 1 .
Then previous methods can be applied for modeling the similarity between the question and each part of the graph .
However , such approach faces two limitations .
First , each semantic component is not directly comparable with the whole question , since it conveys only partial information of the question .
Second , and more importantly , the model encodes different components separately , without learning the representation of the whole graph , hence it ’s not able to capture the compositional semantics in a global perspective .
In order to attack the above limitations , we propose a neural network based approach to improve the performance of semantic similarity measurement in complex question answering .
Given candidate query graphs generated from one question , our model embeds the question surface and predicate sequences into a uniform vector space .
The main difference between our approach and previous methods is that we integrate hidden vectors of various semantic components and encode their interaction as the hidden semantics of the entire query graph .
In addition , to cope with different semantic components of a query graph , we leverage dependency parsing information as a complementary of sentential information for question encoding , which makes the model better align each component to the question .
The contribution of this paper is summarized below .
• We propose a light - weighted and effective neural network model to solve complex KBQA task .
To the best of our knowledge , this is the ﬁrst attempt to explicitly encode the complete semantics of a complex query
graph ( Section 2.2 ) ;
• We leverage dependency parsing to enrich question representation in the NN model , and conduct thorough investigations to verify its effectiveness ( Section 2.2.2 ) ;
• We propose an ensemble method to enrich entity linking from a state - of - the - art linking tool , which further improves the performance of the overall task ( Section 2.3 ) ;
• We perform comprehensive experiments on multiple QA datasets , and our proposed method consistently outperforms previous approaches on complex questions , and produces competitive results on datasets made up of simple questions ( Section 3 ) .
2 Approach
In this section , we present our approach for solving complex KBQA .
First , we generate candidate query graphs by staged generation method ( Section 2.1 ) .
Second , we measure the semantic similarities between the question and each query graph using deep neural networks ( Section 2.2 ) .
Then we introduce an ensemble approach for entity linking enrichment ( Section 2.3 ) , Finally , we discuss the prediction and parameter learning step of this task ( Section 2.4 ) .
2.1 Query Graph Generation
We illustrate our staged candidate generation method in this section .
Compared to previous methods , such as Bao et al ( 2016 ) , we employ a more effective candidate generation strategy , which takes advantage of implicit type information in query graphs and time interval information in the KB .
In our work , we take 4 kinds of semantic constraints into account : entity , type , time and ordinal constraints .
Figure 2 shows a concrete example of our candidate generation .
For simplicity of discussion , we assume Freebase as the KB in this section .
Step 1 : Focus linking .
We extract possible ( mention , focus node ) pairs from the question .
Focus nodes are the starting points of various semantic constraints , refer to Figure 2(a ) .
For entity linking , we generate ( mention , entity ) pairs using the state - of - the - art entity linking tool SMART ( Yang and Chang , 2015 ) .
For type linking , we brutally combine each type with all uni- , bi2186  Who is the youngest US president after 2002 ?
1
United States
us_president
US Senate
US football team
us_vice_president
president
2002
government_position
jurisdiction
2002
1
A
US football team
president
v1
us_president
us_vice_president
United States
US Senate
( a ) Focus linking
( b ) Main path generation
US football team
1
president
basic_title
v1
US Senate
MaxAtN
1
date_of_birth
v2
A
government_position
jurisdiction
government_position
jurisdiction
2002
A
us_president
us_vice_president
United States
isA
us_vice_president
United States
us_president
2002
US Senate
president
basic_title
v1
v3
from >
US football team
( c ) Applying entity constraints
( d ) Applying all constraints
Figure 2 : Running example of candidate generation .
and tri - gram mentions in the question , and pick top-10 ( mention , type ) pairs with the highest word embedding similarities of each pair .
For time linking , we extract time mentions by simply matching year regex .
For ordinal linking , we leverage a predeﬁned superlative word list2 and recognize mentions by matching superlative words , or the “ ordinal number + superlative ” pattern .
The ordinal node is an integer representing the ordinal number in the mention .
Step 2 : Main path generation .
We build different main paths by connecting the answer node to different focus entities using 1 - hop or 2 - hopwith - mediator3 predicate sequence .
Figure 2(b ) shows one of the main paths .
Further constraints are attached by connecting an anchor node x to an unused focus node through predicate sequences , where the anchor node x is a non - focus node in the main path ( A or v1 in the example ) .
Step 3 : Attaching entity constraints .
We apply a depth-ﬁrst search to search for combinations of multiple entity constraints to the main path through 1 - hop predicate .
Figure 2(c ) shows a valid ( v1 , basic title , president ) .
entity The advantage of depth-ﬁrst search is that we can involve unlimited number of entities in a query graph , which has a better coverage than template - based methods .
constraint ,
Step 4 : Type constraint generation .
Type constraints can only be applied at the answer node using IsA predicate .
Our improvement in this step is to ﬁlter type constraints using implicit types
2 ~20 superlative words , such as largest , highest , latest .
3 Mediator is a kind of auxiliary nodes in Freebase maintaining N - ary facts .
of the answer , derived from the outgoing predicates of the answer node .
For example in Figure 2(c ) , the domain type of the predicate government position is politician , which becomes the implicit type of the answer .
Thus we can ﬁlter type constraints which are irrelevant to the implicit types , preventing semantic drift and speeding up the generation process .
To judge whether two types in Freebase are relevant or not , we adopt the method in Luo et al ( 2015 ) to build a rich type hierarchy of Freebase .
Focus types are discarded , if they are not the super- or sub- types of any implicit types of the answer .
Step 5 : Time and ordinal constraint generation .
As shown in Figure 2(d ) , the time constraint is represented as a 2 - hop predicate sequence , where the second is a virtual predicate determined by the preposition before the focus time , indicating the time comparing operation , like “ before ” , “ after ” and “ in ” .
Similarly , the ordinal constraint also forms a 2 - hop predicate sequence , where the second predicate represents descending ( MaxAtN ) or ascending order ( MinAtN ) .
For the detail of time constraint , while existing approaches ( Yih et al , 2015 ; Bao et al , 2016 ) link the focus time with only single time predicate , our improvement is to leverage paired time predicates for representing a more accurate time constraint .
In Freebase , paired time predicates are used to represent facts within certain time intervals , like f rom and to4 in Figure 2(d ) .
For time comparing operation “ in ” , we link the time focus to the starting time predicate , but use both predi4 Short for governmental position held.f rom and
governmental position held.to respectively .
2187  cates in SPARQL query , restricting that the focus time lies in the time interval of the paired predicates .
After ﬁnishing all these querying stages , we translate candidate graphs into SPARQL query , and produce their ﬁnal output answers .
Finally , we discard query graphs with zero outputs , or using overlapped mentions .
2.2 NN - based Semantic Matching Model
The architecture of the proposed model is shown in Figure 3 .
We ﬁrst replace all entity ( or time ) mentions used in the query graph by dummy tokens hEi ( or hT mi ) .
To encode the complex query structure , we split it into predicate sequences starting from answer to focus nodes , which we call semantic components .
The predicate sequence does n’t include the information of focus nodes , except for type constraints , where we append the focus type to the IsA predicate , resulting in the predicate sequence like { IsA , river } .
We introduce in detail the encoding methods for questions and predicate sequences , and how to calculate the semantic similarity score .
2.2.1 Semantic Component Representation
To encode a semantic component p , we take the sequence of both predicate ids and predicate names into consideration .
As the example shown in Figure 3 , the i d sequence of the ﬁrst semantic component is { contained by } , and the predicate word sequence is the concatenation of canonical names for each predicate , that is { “ contained ” , “ by ” } .
.
1
1
, .
. .
, p(w )
Given the word sequence { p(w )
n } , we ﬁrst use a word embedding matrix Ew ∈ R|Vw|×d to convert the original sequence into word embeddings { p(w ) n } , where |Vw| denotes the vocabulary size of natural language words , and d denotes the embedding dimension .
Then we represent the word sequence using word averaging : p(w ) = 1
, .
. .
, p(w )
n Pi p(w )
i For the i d sequence { p(id )
m } , we simply take it as a whole unit , and directly translate it into vector representation using the embedding matrix Ep ∈ R|Vp×d| at path level , where |Vp| is the vocabulary size of predicate sequences .
There are two reasons for using such path embedding : 1 ) the length of i d sequence is not larger than two , based on our generation method ; 2 ) the number of distinct predicate sequences is roughly the same as the number of distinct predicates .
We get the ﬁ , . . . , p(id )
1
nal vector of the semantic component by elementwise addition : p = p(w ) + p(id ) .
2.2.2 Question Representation
We encode the question in both global and local level , which captures the semantic information with respect to each component p.
1
The global
−→ h ( w ) n ] .
, .
. .
, q(w )
information takes the token sequence as the input .
We use the same word embedding matrix Ew to convert the token sequence into vectors { q(w ) n } .
Then we encode the token sequence by applying bidirectional GRU network ( Cho et al , 2014 ) .
The representation of the token sequence is the concatenation of the last forward and backward hidden states through the Bi←− h ( w ) GRU layer , q(tok ) = [ 1
; To encode the question at local level , we leverage dependency parsing to represent long - range dependencies between the answer and the focus node in p. Since the answer is denoted by the whword in the question , we extract the dependency path from the answer node to the focus mention in the question .
Similar with Xu et al ( 2016 ) , we treat the path as the concatenation of words and dependency labels with directions .
For example , the dependency path between “ what ” and “ United −−→ pobj , hEi } .
States ” is { what , We apply another bidirectional GRU layer to produce the vector representation at dependency level q(dep ) , capturing both syntactic features and local p semantic features .
Finally we combine global and local representation by element - wise addition , returning the representation of the question with respect to the semantic component , qp = q(tok ) + q(dep ) p
−−−→ nsubj , is , −−→prep , in ,
.
2.2.3 Semantic Similarity Calculation
Given the query graph with multiple semantic components , G = { p(1 ) , . . .
, p(N ) } , now all its semantic components have been projected into a common vector space , representing hidden features in different aspects .
We apply max pooling over the hidden vectors of semantic components , and get the compositional semantic representation of the entire query graph .
Similarly , we perform max pooling for the question vectors with respect to each semantic component .
Finally , we compute the semantic similarity score between the graph and question :
Ssem(q , G )
= cos(max
p(i ) , max
q(i ) p ) .
i
i
( 1 )
2188  pooling
cos
pooling
+
+
avg
river
isA
A
2
A
MaxAtN
length
BiGRU
BiGRU
what is the second longest river in the ! " #
what nsubj river prep in pobj < E >
contained by
contained_by
contained_by
A
US
Figure 3 : Overview of proposed semantic matching model .
Based on this framework , our proposed method ensures the vector spaces of the question and the entire query graph are comparable , and captures complementary semantic features from different parts of the query graph .
It ’s worth mentioning that the semantic matching model is agnostic to the candidate generation method of the query graphs , hence it can be applied to the other existing semantic parsing frameworks .
2.3 Entity Linking Enrichment
The S - MART linker is a black box for our system , which is not extendable and tend to produce high precision but low recall linking results .
To seek a better balance at entity linking , we propose an ensemble approach to enrich linking results .
We ﬁrst build a large lexicon by collecting all ( mention , entity ) pairs from article titles , anchor texts , redirects and disambiguation pages of Wikipedia .
Each pair is associated with statistical features , such as linking probability , letter - tri - gram jaccard similarity and popularity of the entity in Wikipedia .
For the pairs found in S - MART results , we take the above features as the input to a 2 - layer linear regression model ﬁtting their linking scores .
Thus we learn a pseudo linking score for every pair in the lexicon , and for each question , we pick top - K highest pairs to enrich S - MART linking results , where K is a hyperparameter .
2.4 Training and Prediction
To predict the best query graph from candidates , we calculate the overall association score S(q , G ) between the question q and each candidate G , which is the weighted sum of features over entity
linking , semantic matching and structural level .
Table 1 lists the detail features .
During training step , we adopt hinge loss to maximize the margin between positive graphs G+ and negative graphs G− :
loss = max{0 , λ − S(q , G+ ) + S(q , G− ) } .
( 2 )
For each question , we pick a candidate graph as positive data , if the F1 score of its answer is larger than a threshold ( set to 0.1 in our work ) .
We randomly sample 20 negative graphs G− from the candidate set whose F1 is lower than the corresponding G+ .
Category Entity
Semantics Structural
Description Sum of S - MART scores of all entities ; Number of entities from S - MART ; Number of entities from enriched lexicon ; Semantic similarity score Ssem(q , G ) ; Number of each kind of constraints in G ; Whether a kind of constraints is used in G ; Whether the main path is one - hop ; Number of output answers .
Table 1 : Full set of features .
3 Experiments
In this section , we introduce the QA datasets and state - of - the - art systems that we compare .
We show the end - to - end results of the KBQA task , and perform detail analysis to investigate the importance of different modules used in our approach .
3.1 Experimental Setup
QA datasets : We conduct our experiments on ComplexQuestions ( Bao et al , 2016 ) , We2189  bQuestions ( Berant et al , 2013 ) and SimpleQuestions ( Bordes et al , 2015 ) .
We use CompQ , WebQ and SimpQ as abbreviations of the above datasets , respectively .
CompQ contains 2,100 complex questions collected from Bing search query log , and the dataset is split into 1,300 training and 800 testing questions .
WebQ contains 5,810 questions collected from Google Suggest API , and is split into 3,778 training and 2,032 testing QA pairs .
Each question is manually labeled with at least one answer entity in both datasets .
SimpQ consists of more than 100 K questions , and the gold answer of each question is a gold focus entity paired with a single predicate .
This dataset is designed mainly for answering simple questions , and we use it for complementary evaluation .
Knowledge bases : For experiments on both CompQ and WebQ , we follow the settings of Berant et al ( 2013 ) and
Xu et al ( 2016 ) to use the full Freebase dump 5 as the knowledge base , which contains 46 M entities and 5,323 predicates .
We host the knowledge base with Virtuoso engine 6 .
For the experiments on SimpQ , the knowledge base we use is FB2 M , which is a subset of Freebase provided with the dataset , consisting 2 M entities and 10 M triple facts .
Implementation detail : For all experiments in this section , we initialize word embeddings using GloVe ( Pennington et al , 2014 ) word vectors with dimensions set to 300 , and the size of BiGRU hidden layer is also set to 300 .
We tune the margin λ in { 0.1 , 0.2 , 0.5 } , the ensemble threshold K in { 1 , 2 , 3 , 5 , 10 , + INF } , and the batch size B in { 16 , 32 , 64 } .
All the source codes , QA datasets , and detail results can be downloaded from http://202.120.38.146/CompQA/.
3.2 End - to - End Results
Now we perform KBQA experiments on WebQ and CompQ. We use the average F1 score over all questions as our evaluation metric .
The ofﬁcial evaluation script 7 measures the correctness of output entities at string level .
While in CompQ , the annotated names of gold answer entities do n’t match the case of their names in Freebase , thus we follow Bao et al ( 2016 ) to lowercase both annotated names and the output answer names before
5detail information of the Freebase dump is available at
https://github.com/syxu828/QuestionAnsweringOverFB/.
6http://virtuoso.openlinksw.com/ 7The evaluation script
nlp.stanford.edu/software/sempre/.
is available at http://wwwcalculating the F1 score .
We set λ = 0.5 , B = 32 , K = 3 for WebQ and K = 5 for CompQ , as reaching the highest average F1 on the validation set of each dataset .
We report the experimental results in Table 2 .
The result of Yih et al ( 2015 ) on CompQ is reported by Bao et al ( 2016 ) as their implemented result .
Our approach outperforms existing approaches on CompQ dataset , and ranks 2nd on WebQ among a long list of state - of - the - art works .
Jain ( 2016 ) achieves highest F1 score on WebQ using memory networks , which is not semantic parsing based , and thus less interpretable .
We point out that Xu et al ( 2016 ) uses Wikipedia texts as the external community knowledge for verifying candidate answers , and achieves a slightly higher F1 score ( 53.3 ) than our model , but the performance decreases to 47.0 if this step is removed .
Besides , Yih et al ( 2015 ) and Bao et al ( 2016 ) used ClueWeb dataset for learning more accurate semantics , while based on the ablation test of Yih , the F1 score of WebQ drops by 0.9 if ClueWeb information is removed .
Method Dong et al ( 2015 ) Yao ( 2015 ) Bast and Haussmann ( 2015 ) Berant and Liang ( 2015 )
Yih et al ( 2015 )
Reddy et al ( 2016 )
Xu et al ( 2016 ) ( w/o text )
Bao et al ( 2016 ) Jain ( 2016 )
Abujabal et al ( 2017 ) Cui et al ( 2017 )
Hu et al ( 2018 ) Talmor and Berant ( 2018 ) Ours ( w/o linking enrich )
Ours ( w/ linking enrich )
CompQ WebQ 40.8 44.3 49.4 49.7 52.5 50.3 47.0 52.4 55.6 51.0 34.0 49.6 52.0 52.7
36.9 40.9 39.7 42.0 42.8
Table 2 : Average F1 scores on CompQ and WebQ datasets .
Our results show that entity enrichment method improves the results on both datasets by a large margin ( 0.8 ) , which is a good help to our approach .
We argue that the enriched results are directly comparable with other approaches , as SMART itself is learned from semi - structured information in Wikipedia , such as anchor texts , redirect links and disambiguation pages , the enrichment step does not bring extra knowledge into our system .
In addition , the improvements of the candidate generation step also show a positive effect .
If we remove our implicit type ﬁltering in Step 4 and time interval constraints in Step 5 , the F1 of CompQ slightly drops from 42.84 to 42.37 .
Al2190  though these improvements mainly concern timerelated questions ( around 25 % in CompQ ) , we believe these strategies can be useful tricks in the further researches .
As a complementary evaluation , we perform semantic matching experiments on SimpQ. Given the gold entity of each question , we recognize the entity mention in the question , replace it with hEi , then predict the correct predicate .
Table 3 shows the experimental results .
The best result is from Qu et al ( 2018 ) , which learns the semantic similarity through both attentive RNN and similarity matrix based CNN .
Yu et al ( 2017 ) proposed another approach using multi - layer BiLSTM with residual connections .
Our semantic matching model performs slightly below these two systems , since answering simple questions is not the main goal of this paper .
Comparing with these approaches , our semantic matching model is lightweighted , with a simpler structure and fewer parameters , thus is easier to tune and remains effective .
Method BiLSTM w/ words BiLSTM
w/ rel name
Yih et al ( 2015 )
Yin et al ( 2016 )
Yu et al ( 2017 )
Qu et al ( 2018 ) Ours
Relation Inputs words rel name char-3 - gram words words+rel name words+rel separated words+path
Accuracy 91.2 88.9 90.0 91.3 93.3 93.7 93.1
Table 3 : Accuracy on the SimpleQuestions dataset .
3.3 Ablation Study
In this section , we explore the contributions of various components in our system .
Semantic component representation : We ﬁrst evaluate the results on CompQ and WebQ under different path encoding methods .
Recap that the encoding result of a semantic component is the summation of its word and i d path representations ( Section 2.2.1 ) , thus we compare encoding methods by multiple combinations .
For encoding predicate word sequence , we use BiGRU ( the same setting as encoding question word sequence ) as the alternative of average word embedding .
For encoding predicate i d sequence , we use average predicate embedding as the alternative of the current path - level embedding ( P athEmb ) .
The experimental results are shown in Table 4 .
The encoding method N one means that we do n’t encode the i d or word sequence , and simply take
the result of the other sequence as the representation of the whole component .
we observe that the top three combination settings , ignoring either word or i d sequence , perform worse than the bottom three settings .
The comparison demonstrates that predicate word and i d representation can be complementary to each other .
The performance gain is not that large , mainly because predicate i d features are largely covered by their word name features .
For the encoding of i d sequences , P athEmb works better than average embedding , consistently boosting F1 by 0.65 on both datasets .
The former method treats the whole sequence as a single unit , which is more ﬂexible and can potentially learn diverse representations of i d sequences that share the same predicates .
For the encoding of word sequences , the average word embedding method outperforms BiGRU on CompQ , and the gap becomes smaller when running on WebQ. This is mainly because the training set of WebQ is about 3 times larger than that of CompQ , making it easier for training a more complex model .
Word repr .
None Average BiGRU Average BiGRU Average
I d repr .
PathEmb
None
None Average PathEmb PathEmb
CompQ F1 WebQ F1
41.11 42.18 41.80 42.16 41.52 42.84
51.86 51.74 51.87 52.00 52.33 52.66
Table 4 : Ablation results on path representation .
Semantic composition and question representation : To demonstrate the effectiveness of semantic composition , we construct a straightforward baseline , where we remove the max pool(2 ) , and instead calculate ing operation in Eq .
the semantic similarity score as the summation of individual cosine similarities : Ssem(q , G ) = Pi cos(p(i ) , q(i ) p ) .
For methods of question encoding , we setup ablations by turning off either sentential encoding or dependency encoding .
Table 5 shows the ablation results on CompQ and WebQ. When dependency path information is augmented with sentential information , the performance boosts by 0.42 on average .
Dependency paths focus on hidden features at syntactic and functional perspective , which is a good complementary to sentential encoding results .
However , performances drop by 2.17 if only dependency information is used , we ﬁnd that under certain dependency structures , crucial words ( bolded ) are
2191  not in the path between the answer and the focus mention ( underlined ) , for example , “ who did draco malloy end up marrying ” and “ who did the philippines gain independence from ” .
While we observe about 5 % of such questions in WebQ , it ’s hard to predict the correct query graph without crucial words .
In terms of semantic composition , Our max pooling based method consistently outperforms the baseline method .
The improvement on WebQ is smaller than on CompQ , largely due to the fact that 85 % questions in WebQ are simple questions ( Bao et al , 2016 ) .
As a result of combination , our approach signiﬁcantly outperforms the vanilla SP+NN approach on CompQ by 1.28 , demonstrating the effectiveness of our approach .
Theoretically , the pooling outcome may lead to worse end - to - end result when there are too many semantic components in one graph , because the pooling layer takes too many vectors as input , different semantic features between similar query graphs become indistinguishable .
In our task , only 0.5 % of candidate graphs have more than 3 semantic components , so pooling is a reasonable way to aggregate semantic components in this scenario .
Composition Baseline Baseline Ours
Ours Ours
Q repr sentential both dependency sentential both
CompQ F1 WebQ F1
41.56 42.35 41.48 42.59 42.84
52.14 52.39 49.69 52.28 52.66
a simple “ children ” predicate .
That ’s why our method manages to answer it correctly .
3.4 Error Analysis
from We randomly analyzed 100 questions CompQ where no correct answers are returned .
We list the major causes of errors as follows :
Main path error ( 10 % ): This type of error occurred when the model failed to understand the main semantics when facing some difﬁcult questions ( e.g. “ What native american sports heroes earning two gold medals in the 1912 Olympics ” ) ; Constraint missing ( 42 % ): These types of questions involve implicit constraints , for example , the question “ Who was US president when Traicho Kostov was teenager ” is difﬁcult to answer because it implies an implicit time constraint “ when Traicho Kostov was teenager ” ;
Entity linking error ( 16 % ): This error occurs due to the highly ambiguity of mentions .
For example , the question “ What character did Robert Pattinson play in Harry Potter ” expects the ﬁlm “ Harry Potter and the Goblet of Fire ” as the focus , while there are 7 movies in Harry Potter series ;
Miscellaneous ( 32 % ): This error class contains questions with semantic ambiguity or not reasonable .
For example , the question “ Where is Byron Nelson 2012 ” is hard to understand , because “ Byron Nelson ” died in 2006 and maybe this question wants to ask where did he die .
Table 5 : Ablation results on question representation and compositional strategy .
4 Related Work
To further explain the advantage of semantic composition , we take the following question as an example : “ who is gimli ’s father in the hobbit ” .
Two query graphs are likely to be the ﬁnal answer : 1 ) ( ? , children , gimli person ) ; 2 ) ( ? , f ictional children , gimli character ) ∧ ( ? , appear in , hobbit ) .
If observing semantic comthe predicate children is ponents individually , most likely to be the correct one since “ ’s father ” is highly related and with plenty of positive training data .
Both f ictional children and appear in get a much lower similarity compared with children , hence the baseline method prefer the ﬁrst query graph .
In the meantime , our proposed method learns the hidden semantics of the second candidate by absorbing salient features from both predicates , and such compositional representation is closer to the semantics of the entire question than
Knowledge Base Question Answering(KBQA ) has been a hot research top in recent years .
Generally speaking , the most popular methods for KBQA can be mainly divided into two classes : information retrieval and semantic parsing .
Information retrieval based system tries to obtain target answer directly from question information and KB knowledge without explicit There considering interior query structure .
are various methods ( Yao and Van Durme , 2014 ; Bordes et al , 2015 ; Dong et al , 2015 ;
Xu et
al , 2016 ) to select candidate answers and to rank results .
Semantic parsing based approach focuses on constructing a semantic parsing tree or equivalent query structure that represents the semantic meaning of the question .
In terms of logical representation of natural language questions , many methods have been tried , such as query graph ( Yih et al ,
2192  2014 , 2015 ) or RDF query language ( Unger et al , 2012 ; Cui et al , 2017 ; Hu et al , 2018 ) .
Recently , as the development of deep learning , NN - based approaches have been combined into the KBQA task ( Bordes et al , 2014 ) , showing promising result .
These approaches tries to use neural network models to encode both questions and answers ( or query structures ) into the vector space .
Subsequently , similarity functions are used to select the most appropriate query structure to generate the ﬁnal answer .
For example , Bordes et al ( 2014 ) focuses on embedding the subgraph of the candidate answer ;
Yin et al ( 2016 ) uses character - level CNN and word - level information ; Yu et al CNN to match different introduces the method of hierarchical ( 2017 ) residual RNN to compare questions and relation names ; Qu et al ( 2018 ) proposes the AR - SMCNN model , which uses RNN to capture semantic - level correlation and employs CNN to extract literallevel words interaction .
Belonging to NN - based semantic parsing category , our approach employs a novel encoding structure method to solve complex questions .
Previous works such as Yih et al ( 2015 ) and Bao et al ( 2016 ) require a recognition of a main relation and regard other constraints as variables added to this main relation .
Unlike their approaches , our method encodes multiple relations ( paths ) into a uniform query structure representation ( semantic composition ) , which allows more ﬂexible query structures .
There are also some works ca n’t be simply classiﬁed in to IR based methods or SP based methods .
Jain ( 2016 ) introduces Factual Memory Network , which tries to encode KB and questions in same word vector space , extract a subset of initial candidate facts , then try to employ multi - hop reasoning and reﬁnement to ﬁnd a path to answer entity .
Reddy et al ( 2016 ) , Abujabal et al ( 2017 ) , and Cui et al ( 2017 ) try to interpret question intention by templates , which learned from KB or QA corpora .
Talmor and Berant ( 2018 ) attempts to answering complex questions by decomposing them into a sequence of simple questions .
5 Conclusion
ied different methods to further improve the performance , mainly leveraging dependency parse and the ensemble method for linking enrichment .
Our model becomes the state - of - the - art on ComplexQuestions dataset , and produces competitive results on other simple question based datasets .
Possible future work includes supporting more complex semantics like implicit time constraints .
Acknowledgment
Kenny Q. Zhu is the contact author and was supported by NSFC grants 91646205 and 61373031 .
Thanks to the anonymous reviewers for their valuable feedback .
