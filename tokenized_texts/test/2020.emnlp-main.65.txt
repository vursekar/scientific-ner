Will I Sound Like Me ?
Improving Persona Consistency in Dialogues through Pragmatic Self - Consciousness
Hyunwoo Kim
Byeongchang Kim
Gunhee Kim
Department of Computer Science and Engineering Seoul National University , Seoul , Korea { hyunw.kim , byeongchang.kim}@vl.snu.ac.kr gunhee@snu.ac.kr https://vl.snu.ac.kr/projects/consistency
Abstract
We explore the task of improving persona consistency of dialogue agents .
Recent models tackling consistency often train with additional Natural Language Inference ( NLI ) labels or attach trained extra modules to the generative agent for maintaining consistency .
However , such additional labels and training can be demanding .
Also , we Ô¨Ånd even the bestperforming persona - based agents are insensitive to contradictory words .
Inspired by social cognition and pragmatics , we endow existing dialogue agents with public self - consciousness on the Ô¨Çy through an imaginary listener .
Our approach , based on the Rational Speech Acts framework ( Frank and Goodman , 2012 ) , can enforce dialogue agents to refrain from uttering contradiction .
We further extend the framework by learning the distractor selection , which has been usually done manually or randomly .
Results on Dialogue NLI ( Welleck et al , 2019 ) and PersonaChat ( Zhang et al , 2018 ) dataset show that our approach reduces contradiction and improves consistency of existing dialogue models .
Moreover , we show that it can be generalized to improve contextconsistency beyond persona in dialogues .
1
Introduction
In the study of dialogue agents , consistency has been a long - standing issue .
To resolve this , much research has been conducted to endow dialogue agents with personas .
Li et al ( 2016 ) propose to encode persona in embeddings and Zhang et al ( 2018 ) introduce a persona - conditioned dialogue dataset .
On top of these works , many efforts have been made to improve consistency .
In spite of such recent signiÔ¨Åcant progress , there is much room for improving persona - based dialogue agents .
We observe that even the best performing persona - based generative models ( See et al , 2019 ; Wolf et al , 2019b ; Roller et al , 2020 )
Figure 1 : Illustration of the consistency issue in dialogue .
While a literal dialogue agent ( S0 ) fails to deliver a consistent persona , our self - conscious agent ( S1 ) does so , by modeling an imaginary listener .
Icons are designed by Nhor Phai and Vincent Le Moign .
are highly insensitive to contradictory words , and thus fail to deliver consistent persona to the interlocutor ( Figure 1 ) .
Also , extra modules other than the generative model is often required for improving consistency .
Recent works on consistency in persona - based dialogue actively adopt the NLIbased approach ( Welleck et al , 2019 ; Song et al , 2019 ; Li et al , 2020 ; Song et al , 2020 ) , which have the following prerequisites .
First , they require labeled pairs of persona sentences and dialogue utterances with three categories : entailment , neutral , and contradiction .
Next , methods with NLI models for rating the agent ‚Äôs consistency also need to train them separately with those labels .
In this work , we step back from this NLI - based supervised approach and ponder : how do humans maintain consistency ?
We humans never learn how to be consistent .
Instead , we have an innate drive for consistency to hold our beliefs and behavior in harmony ( Festinger , 1962 ) .
If so , how do we
Proceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing , pages904‚Äì916,November16‚Äì20,2020.c(cid:13)2020AssociationforComputationalLinguistics904I like to stay at home .
InterlocutorLiteral Agent : ùëÜ![Inconsistent]I like going outside .
InterlocutorSelf - Conscious Agent : ùëÜ"[Consistent]I like going outside .
I love Disneyland!I go there every week .
‚Äò Will I sound like me ? ‚Äô  know we are consistent or not ?
We do not ask others .
We ask ourselves by predicting how we are perceived by others .
Public self - consciousness is this awareness of the self as a social object that can be observed and evaluated by others ( Fenigstein et al , 1975 ) .
We particularly emphasize that public self - consciousness is not equivalent to the philosophical self - consciousness ( or self - awareness)1 .
Simply put , public self - consciousness is the concern about how oneself will be perceived by others , as opposed to the philosophical state of being conscious of self - existence .
According to Doherty and Schlenker ( 1991 ) , people with high public self - consciousness tend to act more consistent with known information about themselves .
They care deeply about how others will evaluate them and have a strong tendency to avoid negative evaluations ( Fenigstein et al , 1975 ) .
Since inconsistency is condemned by others , one who has high public self - consciousness will try more to maintain consistency .
In order to predict how we are perceived , we rely on abstract models of others ( Gopnik and Wellman , 1992 ) and simulate others ‚Äô reactions based on imagination ( Hassabis et al , 2013 ) .
Inspired by this , our intuition is that self - consciousness through an imaginary listener will let dialogue agents better maintain consistency .
Modeling a listener has been one of the main topics in computational pragmatics .
Our work extends this long line of work in cognitive science by making use of the Bayesian Rational Speech Acts framework ( Frank and Goodman , 2012 ) , which has been originally applied to improving informativeness of referring expressions .
Since personas ought to express who we are , we adopt this framework for dialogue agents by regarding personas as targets that should be conveyed to the interlocutor .
As the agent tries to generate tokens that help the imaginary listener identify the agent ‚Äôs persona , it can lastly generate more consistent utterances .
In summary , we take inspiration from social cognition and pragmatics to endow generative agents with self - consciousness , which makes them imagine the listener ‚Äôs reaction and incorporate it to the generation process for improving consistency .
Our major contributions can be outlined as follows :
( 1 ) We propose an orthogonally applicable approach for any persona - based generative agents to improve consistency without the use of additional
1https://plato.stanford.edu/entries/
self - consciousness/
consistency labels and training .
Moreover , it is even generalizable to improve context - consistency beyond persona in dialogue .
( 2 ) We extend the Rational Speech Acts framework ( Frank and Goodman , 2012 ) with two new technical features : ( i ) a learning method for distractor selection ( e.g. other samples different from the given target ( Andreas and Klein , 2016 ) ) , which has been usually done manually or randomly , and ( ii ) a different update for the listener ‚Äôs world prior that better preserves information of previous states .
( 3 ) Our approach improves consistency of three recent generative agents ( See et al , 2019 ; Wolf et al , 2019b ; Roller et al , 2020 ) over Dialogue NLI ( Welleck et al , 2019 ) and PersonaChat ( Zhang et al , 2018 ) .
Along with large reduction in contradiction , the utterance accuracy signiÔ¨Åcantly increases too .
2 Related Work
Persona & Consistency in Dialogue .
Li et al ( 2016 ) learn personas in embeddings .
Zhang et al ( 2018 ) release the PersonaChat dataset , a chitchat dialogue set involving two interlocutors each playing their given persona .
Madotto et al ( 2019 ) use meta - learning to adapt to new personas with few dialogue samples .
Liu et al ( 2020 ) use reinforcement learning to enhance mutual persona perception .
Recent works use extra modules or NLI labels to improve consistency .
Shum et al ( 2019 ) Ô¨Åll generated templates , and rank with a language model .
Zhang et al ( 2019 ) use self - supervised feature extractors for generation .
Welleck et al ( 2019 ) annotate NLI labels to the PersonaChat dataset .
They train an NLI model and run pairwise comparison between candidates and persona to compute contradiction scores .
The NLI approach is applied for coherence evaluation ( Dziri et al , 2019 ) , rewards to reinforcement learning agents ( Song et al , 2019 ) , Ô¨Ånding inconsistent words ( Song et al , 2020 ) , and unlikelihood training ( Li et al , 2020 ) .
They require NLI labels on the target dialogue dataset ; otherwise , sharp decrease in performance is observed , due to mismatch of data distribution ( Welleck et al , 2019 ) .
Such dataset - speciÔ¨Åc NLI annotations and training NLI models can be costly and time - consuming .
Compared to previous methods , the novelty of our approach is to improve consistency without NLI labels and extra modules .
Pragmatics .
Our approach belongs to the general family of Bayesian Rational Speech Acts
905  Figure 2 : Proportion of Hits@1 , Entail@1 , Neutral@1 and Contradict@1 in the top-1 candidates returned by the models on the Dialogue NLI dataset .
ROUGE-1 ROUGE - L
SPICE
GT Utterance Top Entail - Utt Contradict@1 - Utt
15.7 15.3 16.3
14.6 14.5 15.9
10.6 7.1 6.6
Table 1 : Comparison between ground - truth utterances , top - ranked entailing candidates and Contradict@1 utterances in ROUGE and SPICE scores .
( RSA ) frameworks ( Frank and Goodman , 2012 ) in pragmatics .
It has improved informativeness in a number of NLP tasks , including reference games ( Andreas and Klein , 2016 ) , image captioning ( Mao et al , 2016 ; Vedantam et al , 2017 ; Cohn - Gordon et al , 2018 ) , instruction following ( Fried et al , 2017 ) , navigating ( Fried et al , 2018 ) , translation ( Cohn - Gordon and Goodman , 2019 ) , summarization ( Shen et al , 2019 ) and referring expression generation ( Zarrie√ü and Schlangen , 2019 ) .
However , its application to the dialogue domain remains understudied .
In this work , we explore how the RSA framework can be adopted in dialogue agents to alleviate the inconsistency problem .
Also , we further extend the framework by making the distractor selection as a learnable process .
3
Insensitivity to Contradictory Words in Existing Persona - based Agents
Although conditional language generation has shown promising progress , maintaining consistency within the generation yet remains unsolved .
From quantitative evaluation , we reveal existing generative models for dialogues are highly insensitive to contradictory words .
Dialogue NLI Evaluation .
Welleck et al ( 2019 ) introduce the Dialogue NLI dataset based on the PersonaChat dataset ( Zhang et al , 2018 ) .
They collect entailing and contradictory utterances to the given persona , and release an evaluation set comprised of dialogues each with 31 utterance candidates : 10 entailing , 10 neutral , and 10 contradictory utterances with 1 ground - truth ( GT ) utterance .
On this evaluation set , we run three recent models ( See et al , 2019 ; Wolf et al , 2019b ; Roller
Persona
I love wearing skinny jeans and shirts .
I am a blonde girl with short hair .
GT Utterance
( I , 1.87 ) ( have , 51.42 ) ( really , 201.45 ) ( short , 1.78 ) ( hair , 1.30 ) ( and , 2.81 ) ( it , 45.25 ) ( is , 2.19 ) ( blonde , 461.60 ) .
Contradict@1 - Utt
( What , 60.89 ) ( color , 103.11 ) ( is , 1.99 ) ( your , 1.06 ) ( hair , 1.05 ) ( ? , 1.11 ) ( Mine , 3.57 ) ( is , 1.03 ) ( brown , 17.25 ) .
Table 2 : Example of a contradictory utterance returned by the model and its GT utterance with perplexity per token .
The words of entailment and contradiction to the persona are shown in blue and red , respectively .
et al , 2020 ) that achieve the best performance on PersonaChat .
We report four ranking metrics following Welleck et al ( 2019 ): Hits@1 , Entail@1 , Neutral@1 and Contradict@1 .
Each metric is the proportion of GT , entailing , neutral and contradictory utterances in the top-1 candidates returned by the model , respectively .
The models rank the candidates by perplexity scores .
Figure 2 shows that all three models select contradictory candidates much more often than the GT utterances ( see further results in Table 3 ) .
Though models are conditioned on a given persona , they are highly insensitive to contradictions .
3.1 Analysis of Contradict@1 Utterances
To investigate why insensitivity to contradiction prevails in the state - of - the - art models , we further analyze the contradictory utterances returned by the models ( Contradict@1 - Utt ) , comparing with the GT utterances and the top - ranked entailing candidates ( Top Entail - Utt ) .
Table 1 reports language metrics between the selected candidates and the given persona sentences using SPICE ( Anderson et al , 2016 ) and ROUGE ( Lin , 2004 ) .
SPICE metric measures semantic similarity and ROUGE metric measures n - gram overlaps between two sentences .
Contradict@1 - Utt shows lower SPICE scores and higher ROUGE scores than other utterances , implying that it may be different in semantics but similar in syntax to the given persona .
To take a closer look , we extract the contradicting words from Contradict@1 - Utt and their counterparts from GT utterances to compare their average perplexity scores .
In the Dialogue NLI dataset , every utterance is labeled with a triple ( entity1 , relation , entity2 ) , such as ‚Äú I just like to listen to rock music ‚Äù with ( i , like music , rock ) .
9060%20%40%60%80%100%BlenderTransferTransfoControlSeq2SeqHits@1Entail@1Neutral@1Contradict@1  By construction , Contradict@1 - Utt must contain words that are contradictory to the GT utterance and the given persona .
The perplexity scores of contradictory words ( 106.7 ) were considerably lower than those of the counterparts in GT utterances ( 280.1 ) .
Table 2 shows an example of such dialogue instance with perplexity per word .
If properly conditioned with the given persona , models should show lower perplexity for the words in the persona .
However , their perplexity scores are signiÔ¨Åcantly higher than those of contradictory words .
It reveals that models behave more as a plain language model rather than as a persona - conditioned model .
Thus , guarantee of consistency for each word generation step is required for persona - based dialogue agents to resolve such issue .
4 Approach
We introduce how to endow dialogue agents with public self - consciousness , which helps them keep consistency in mind at each generation step by reÔ¨Çecting an imaginary listener ‚Äôs distribution over personas .
Since the imaginary listener arises from the plain dialogue - agent , separate training is not needed .
Figure 3 illustrates its overall structure .
We present how to model public
selfthe Rational Speech consciousness Acts ( RSA ) framework ( Frank and Goodman , 2012 ) in Section 4.1 .
We then discuss learning of distractor selection as our major novelty for the RSA in Section 4.2 .
using
4.1 Modeling the Public Self - Consciousness
We seek to build a dialogue agent who is selfconscious about its consistency without the need for training on NLI labels or rating consistency with NLI models .
Given that modeling the interactions between listener and speaker is a main topic in pragmatics , we take advantage of the RSA framework ( Frank and Goodman , 2012 ) .
It treats language use as a recursive process where probabilistic speaker and listener reason about each other ‚Äôs intentions in a Bayesian fashion .
To apply the framework to sequence generation for dialogues , we extend the incremental approach proposed for image captioning ( Cohn - Gordon et al , 2018 ) .
To generate an utterance , the agent computes the distribution of every next token ut at timestep t in Bayesian fashion as follows .
Base Speaker S0 .
We Ô¨Årst assume persona i is given to the base speaker , along with the dialogue
Figure 3 : The proposed self - conscious agent S1 consists of base speaker S0 and imaginary listener L0 .
It recursively generates the next token ut at every time t.
history h and partial utterance u < t , as shown in Figure 3 .
The base speaker St 0 returns a distribution over the next token at timestep t : St 0(ut|i , h , u < t ) .
Any conditional dialogue agent can be used as a base speaker .
See the details in Section 5.2 .
Imaginary Listener L0 .
While the base speaker generates each token one at a time , the imaginary listener reasons about the speaker ‚Äôs persona .
The imaginary listener Lt 0 is the posterior distribution of the speaker ‚Äôs persona in terms of the base speaker and the world prior pt(i ) over personas as follows ,
Lt
0(i|h , u‚â§t , pt ) St 0(ut|i , h , u < t)Œ≤ √ó pt(i ) i(cid:48)‚ààI St
0(ut|i(cid:48 ) , h , u < t)Œ≤ √ó pt(i(cid:48 ) )
( cid:80 )
‚àù
.
( 1 )
where Œ≤ on St 0 is the listener rationality coefÔ¨Åcient that controls the amount of information from the current timestep compared to the cumulative prior pt(i ) .
L0 returns a probability distribution over the personas in world I , which is a Ô¨Ånite set ( |I| = 3 ) comprising the given persona i and distractor personas .
The distractors are different personas from other dialogue instances in the dataset .
We decide world I per dialogue instance through learning , which will be elaborated in Section 4.2 .
Self - Conscious Speaker S1 .
With St 1 is deÔ¨Åned as
the self - conscious speaker St
0 and Lt 0 ,
St
1(ut|i , h , u < t )
‚àù Lt
0(i|h , u‚â§t , pt)Œ± √ó St
0(ut|i , h , u < t ) ,
( 2 )
where Œ± is the speaker rationality coefÔ¨Åcient that determines how much the likelihood is considered .
By taking the listener ‚Äôs distribution into account , the speaker is now self - conscious about what persona it sounds like .
Especially , the agent seeks
907Self - ConsciousSpeaker : ùëÜ!"Persona : ùëñDialogue History :
‚ÑéLearned Distractor Personas : ùëñ‚Ä≤Speaker ‚Äôs Next Token : ùë¢"‚àùùêø#"ùëñ‚Ñé,ùë¢$",ùëù"%√óùëÜ#"ùë¢"ùëñ,‚Ñé,ùë¢&")ùëù"‚Äô!(ùëñ)Imaginary Listener : ùêø#"(ùëñ|ùë¢$",‚Ñé,ùëù")Base Speaker:ùëÜ#"ùë¢"ùëñ,‚Ñé,ùë¢ & " )  to be perceived as the given persona i rather than some other persona i(cid:48 ) .
The likelihood of each token being identiÔ¨Åed as the persona i acts as a bonus added to the base speaker ‚Äôs token scores .
Hence , tokens that are consistent to the given persona are preferred to others .
The token with the highest probability is added to the partial utterance , becoming the next input u < t+1 for the speaker .
Updating the world prior with L0 .
Starting from a uniform distribution as the initial prior p0(i ) , we update the world prior pt+1(i ) according to S1 ‚Äôs output ut at every time step :
pt+1(i )
=
Lt
0(i|h , u‚â§t , pt ) .
( 3 )
Hence , pt(i ) represents the cumulative state of the partial utterance up to t. Cohn - Gordon et al ( 2018 ) report the prior update with L1 ‚àù St 0(ut|i , h , u < t ) √ó Lt 0(i|h , u‚â§t , pt ) makes little practical effect compared to a uniform prior .
We Ô¨Ånd that updating the prior with Eq .
( 3 ) instead is effective .
See the results in Section 5.6 .
4.2 Learning to Select Distractors
Distractors ( Andreas and Klein , 2016 ) are samples ( e.g. other personas in the dataset ) which are different from the given target .
In previous works of RSA , the distractors to be included in world I are selected manually or randomly from the dataset .
However , we Ô¨Ånd that performance variance is large according to the selected distractors .
We thus propose to learn distractor selection , especially based on the life - long memory network ( Kaiser et al , 2017 ) .
The life - long memory network is capable of implicitly clustering similar dialogue contexts into a few slots with associated persona .
Therefore , it can efÔ¨Åciently memorize and retrieve distractor personas for each context .
In Appendix , we experiment that our approach outperforms other models including BERT - based algorithms .
To better select useful distractor personas , supervised learning is desirable .
However , there is no explicit label indicating which distractors are helpful for each dialogue .
We select the persona that have the best Hits@1 as the distractor label per training dialogue .
The Hits@1 is the score for favoring the ground - truth next utterance ( consistent and context - relevant ) over other candidate utterances which are just being consistent ( i.e. entailing ) or contradictory to the given persona .
In other words , the score represents consistency and also appropriateness at the same time .
Thus , such
distractors can help the self - conscious agent to generate responses which are context - relevant and allow the imaginary listener to identify the speaker ‚Äôs persona .
Each training datapoint comprises a given persona , a distractor persona and dialogue context .
Memory Structure .
The memory consists of three types of information : M = ( K , v , a ) .
K ‚àà Rm√ód is a key matrix , where m is the number of memory slots and d is the dimension of the key vectors , which are the embedding of datapoints .
The value vector v ‚àà Rm stores the index of a persona .
a ‚àà Rm is an age vector , which is used for memory update .
We set m = 16 , 000 and d = 768 .
Memory Addressing .
We construct the query vector q for each datapoint with the BERTUncased - Base ( Devlin et al , 2019 ) model .
We use the output embedding of BERT ‚Äôs
[ CLS ] token , and normalize it to a unit length to build q ‚àà Rd .
Using the cosine similarity between q and each memory key , we can Ô¨Ånd the k nearest neighbors :
( n1 , n2 , ... , nk ) = N Nk(q , K ) .
( 4 )
Memory Loss .
Suppose that the query datapoint has a distractor label l.
Among ( n1 , ... , nk ) , we denote the positive neighbor np as the one with v[np ] = l and the negative neighbor nb with v[nb ] ( cid:54)= l.
If there are multiple positive neighbors , we pick the one with the smallest memory index .
If no positive neighbor is found , we select a random key whose value is l. For the negative neighbor , we select one randomly from ( n1 , ... , nk ) .
We set k = 2048 .
Then , the loss is computed as
L = max(q ¬∑ K[nb ] ‚àí q ¬∑ K[np ] + Œ± , 0 ) ,
( 5 )
where Œ± is a positive margin , which we set as 0.2 .
This loss maximizes the cosine similarity between the query q and the positive key K[np ] , while minimizing the similarity to the negative key K[nb ] .
We Ô¨Ånetune the query network BERT with this loss .
Memory Update .
After computing the loss , memory M is updated differently for two cases .
( 1 ) If the top-1 neighbor ‚Äôs value ( i.e. persona ) is correct ( v[n1 ] = l ) , the key vector is updated as :
K[n1 ] ‚Üê
q + K[n1 ] ( cid:107)q + K[n1](cid:107 )
.
( 6 )
( 2 ) Otherwise ( v[n1 ] ( cid:54)= l ) , we make a slot for the query ; we Ô¨Ånd the oldest memory slot n(cid:48 ) according to the age vector a and write
K[n(cid:48 ) ]
‚Üê q , v[n(cid:48 ) ] ‚Üê l , a[n(cid:48 ) ] ‚Üê 0 .
( 7 )
908  Training & Inference .
In our Distractor Memory network , training corresponds to updating the memory and the parameters of the query network .
At inference , given a test example , we obtain the query by encoding the dialogue context and the persona using BERT .
We Ô¨Ånd n nearest keys from the memory , and use their values ( i.e. persona indices ) as the distractor personas .
We set n = 2 .
5 Experiments We show that our self - conscious framework can signiÔ¨Åcantly improve consistency and accuracy of state - of - the - art persona - based agents on two benchmark datasets .
We prove its effectiveness using both automatic and human evaluations .
We also show our framework can be generalized to improve consistency of dialogue context beyond persona .
5.1 Datasets
Dialogue NLI Evaluation Set ( Welleck et al , 2019 ) .
This dataset is based on PersonaChat with additional NLI annotations .
Its main task is to rank next - utterance candidates given previous context .
For each dialogue , they collect 31 next - utterance candidates in respect to the given persona : 10 entailing , 10 neutral and 10 contradicting candidates with 1 ground - truth utterance .
In total , the evaluation set includes 542 instances .
PersonaChat dialogue ( Zhang et al , 2018 ) .
This dataset involves two interlocutors who are each given a persona and asked to get to know each other while playing their roles .
This task was the subject of the ConvAI2 competition ( Dinan et al , 2019 ) at NeurIPS 2018 .
The competition version contains 17,878 chitchat conversations conditioned on 1,155 personas for training and 1,000 conversations conditioned on 100 personas for validation .
5.2 Experimental Setting
Base Speakers .
We experiment on three pretrained models including ControlSeq2Seq ( See et al , 2019 ) , TransferTransfo ( Wolf et al , 2019b ) , and Blender ( Roller et al , 2020 ) as base speakers ( S0 ) for our self - conscious agents ( S1 ) .
The ControlSeq2Seq is a Seq2Seq model with attention trained on Twitter dataset ( Miller et al , 2017 ) and Ô¨Ånetuned on PersonaChat .
TranferTransfo based on GPT ( Radford et al , 2018 ) is the winner of the ConvAI2 competition in automatic evaluation .
Blender , a recently released generative dialogue model , is the state - of - the - art open - domain chatbot .
Our approach improves these base speakers by
Model
Hits@1 ‚Üë
Entail@1 ‚Üë Contradict@1 ‚Üì
ControlSeq2Seq ( See et al , 2019 ) S0 27.9 7.9 S1 36.4 10.5 S1+DM 40.8 13.1
TransferTransfo ( Wolf et al , 2019b ) S0 11.1 S1 17.5 S1+DM 18.8
26.4 40.4 45.8
Blender ( Roller et al , 2020 ) S0 S1 S1+DM
18.8 21.8 22.5
27.3 38.0 44.1
46.3 34.0 24.5
46.5 29.7 19.7
42.4 30.6 19.6
Table 3 : Comparison of our approach ( S1 ) with base speakers ( S0 ) on the Dialogue NLI evaluation set ( Welleck et al , 2019 ) .
+ DM is the Distractor Memory .
High scores in Hits@1 , Entail@1 and low scores in Contradict@1 imply better consistency .
granting them the sense of self - consciousness .
We defer implementation details to Appendix .
Evaluation Metrics .
For Dialogue NLI , we report three ranking metrics introduced in the original paper : Hits@1 , Entail@1 , and Contradict@1 .
Each metric is the proportion of GT , entailing , and contradictory utterances in the top-1 candidates returned by the model , respectively .
High scores in Entail@1 and low scores in Contradict@1 indicate better consistency with the persona .
For PersonaChat , we report Hits@1 , standard F1 score , perplexity and C score , following the ConvAI2 protocol .
Hits@1 is the accuracy of choosing the ground - truth next - utterance among 20 candidates as the models rank the candidates by perplexity .
The C score is a metric for dialogue consistency , introduced in Madotto et al ( 2019 ) .
It computes pairwise comparison between utterance u and persona sentence pj with a pretrained NLI model .
The NLI model returns 1 , 0 , -1 for entailment , neutrality , and contradiction , respectively .
We sum the NLI scores across persona sentences per dialogue instance : C(u ) = ( cid:80 )
j NLI(u , pj ) .
5.3 Quantitative Results
Results on Dialogue NLI .
Table 3 compares the performance of dialogue agents on the Dialogue NLI evaluation set .
Our self - conscious agent S1 signiÔ¨Åcantly reduces Contradict@1 scores and increases the Entail@1 along with the Hits@1 accuracy of the literal agents S0 .
We remind that each entailing candidate shares the same annotated triple as the GT utterance .
In other words , they have similar semantics to the GT utterance and follow the
909  Model
Hits@1 ‚Üë
F1 ‚Üë
Perplexity ‚Üì
C ‚Üë
Model
Hits@1 ‚Üë Entail@1 ‚Üë
Contradict@1 ‚Üì
ControlSeq2Seq ( See et al , 2019 ) S0 17.0
16.1
S1 S1+DM
16.4 16.7
16.9 17.1
TransferTransfo ( Wolf et al , 2019b ) S0
16.2
19.2
Blender ( Roller et al , 2020 ) S0
27.6
S1 S1+DM
S1 S1+DM
17.5 18.2
28.8 29.1
19.4 19.5
19.5
19.7 19.8
22.9
23.9 23.9
17.6
19.1 19.1
12.0
13.2 13.2
0.45
0.54 0.55
0.86
0.96 0.97
0.85
0.93 0.95
Table 4 : Comparison of our approach ( S1 ) with base speakers ( S0 ) on PersonaChat ( Zhang et al , 2018 ) .
C is the consistency score evaluated by a pretrained NLI model ( Madotto et al , 2019 ) .
For TransferTransfo , we use the generative version to calculate Hits@1 .
given persona .
Thus , Entail@1 is a lenient version of Hits@1 ( Welleck et al , 2019 ) .
The Distractor Memory ( DM ) is better than random distractor selection for S1 across all metrics .
It concludes that learned distractors are more effective than random distractors for pragmatic agents .
Results on PersonaChat .
Table 4 compares the performance of different dialogue agents on the PersonaChat dataset .
Our model S1 outperforms all other generative dialogue agents in terms of consistency related metrics , i.e. Hits@1 and C score .
Since the posterior update of our self - conscious agent revises the distribution learned by the base speaker , the increase in perplexity is natural due to the effect of regularization .
Nevertheless , our approach improves the F1 score for TransferTransfo and Blender .
Thus , being consistent to the given persona can also help improve the generation performance of dialogue agents .
Comparison with agents that use NLI model .
We also test agents with pretrained NLI models attached ( Welleck et al , 2019 ) , denoted by + NLI in Table 5 .
The NLI model computes contradiction scores of each candidate utterances , and penalize its rank accordingly .
Compared to base agents with no self - consciousness , our agents improve consistency in all three metrics even further when using additional NLI models .
Another notable result is that our agents without NLI ( S1+DM in Table 3 ) for ControlSeq2Seq and TransferTransfo even outperform the base agents with NLI ( S0+NLI ) on Hits@1 .
That is , our self - conscious agents achieve
ControlSeq2Seq ( See et al , 2019 ) S0+NLI
[ S1+DM]+NLI
12.7 14.4
48.2 51.7
TransferTransfo ( Wolf et al , 2019b ) S0+NLI 44.4
[ S1+DM]+NLI 54.6
17.2 21.4
Blender ( Roller et al , 2020 ) S0+NLI
[ S1+DM]+NLI
24.9 26.6
44.7 52.0
8.1 7.0
9.8 5.4
6.0 5.7
Table 5 : Comparison of our approach ( S1 ) with base speakers ( S0 ) on the Dialogue NLI evaluation set ( Welleck et al , 2019 ) with pretrained NLI model attached .
Raw
Calibrated
Model
Consistent Engaging Consistent Engaging
TransferTransfo ( Wolf et al , 2019b )
S0
0.53 ( 0.02 ) 2.48 ( 0.03 ) 0.44 ( 0.01 ) 2.48 ( 0.01 )
S1+DM 0.61 ( 0.02 ) 2.55 ( 0.03 ) 0.52 ( 0.01 ) 2.52 ( 0.01 )
Table 6 : Human evaluation results comparing the consistency and engagingness of the base speaker ( S0 ) and our self - conscious agent ( S1 ) .
Numbers in parentheses are the standard errors .
better GT accuracy even without the help of an NLI model trained on consistency labels .
5.4 Human Evaluation
We perform human evaluation via Amazon Mechanical Turk .
We random sample 250 test examples , each is rated by three unique human judges in terms of ( i ) Consistency and ( ii ) Engagingness .
Turkers are shown a given persona , a dialogue context , and the model ‚Äôs generated utterance .
For consistency , we follow Madotto et al ( 2019 ) and ask judges to assign 1 , 0 , ‚àí1 to the utterance for consistency , neutrality , and contradiction , respectively .
Following See et al ( 2019 ) , we evaluate the engagingness of the utterance in a 4 - point scale , where higher scores are better .
To alleviate annotator bias and inter - annotator variability , we apply Bayesian calibration ( Kulikov et al , 2019 ) to the scores .
Table 6 summarizes the human evaluation results .
The agent with our self - consciousness method S1 is rated as more consistent than the base agent S0 while maintaining a similar level of engagingness .
While it can be trivial to increase consistency at the cost of engagingness ( e.g. perfect consistency can by generating boring utterances with very little variance ) , it is not the case for our agent .
Since
910  Model
Hits@1 ‚Üë Entail@1 ‚Üë
Contradict@1 ‚Üì
Dialogue NLI ( Welleck et al , 2019 ) S0 27.3 S1 ( on context ) 27.7
18.8 32.7
42.4 26.4
Model
Hits@1 ‚Üë
F1 ‚Üë
Perplexity ‚Üì C ‚Üë
PersonaChat ( Zhang et al , 2018 ) S0 S1 ( on context )
27.6 30.5
19.5 19.9
12.0 13.5
EmpatheticDialogue ( Rashkin et al , 2019 ) S0 S1 ( on context )
32.6 34.2
20.5 20.6
14.7 15.4
0.57 0.58
0.47 0.50
Table 7 : Comparison of our approach ( S1 ) with base speaker Blender ( S0 ) when conditioned on dialogue context in three datasets .
We compute the consistency score C respect to the dialogue context .
our agent seeks to be heard as the given persona to the listener , self - distinctive words tend to meld into generated responses ( see Figure 6 ) .
Thus , the responses from self - conscious agents have their own color , which can help improving engagingness .
Figure 4 displays selected examples of utterance generation .
Each example is comprised of dialogue history , human response , and utterances generated by our method and baselines .
5.5 Consistency for Dialogue Context
We demonstrate that our self - conscious agent can be generalized to generate context - consistent utterances beyond persona .
We condition the agent with its previous responses in the dialogue history ; that is , i in Eq .
( 2 ) is the agent ‚Äôs past responses instead of persona sentences .
Hence , tokens that are inconsistent to the agent ‚Äôs past response would be less favored by the model .
Table 7 reports the results of context conditioned self - conscious agents .
The EmpatheticDialogue ( Rashkin et al , 2019 ) is an open - domain dialogue dataset where a speaker describes a past emotional experience and the listener responds accordingly .
Since the speaker ‚Äôs descriptions should be consistent to the experience and previous utterances , it is a suitable benchmark for consistency .
We model the speaker ‚Äôs utterances and measure its consistency .
Our S1 agent outperforms other literal agents on all three datasets in terms of consistency .
Thus , our approach can also be applied to help agents stay more consistent to its context .
Figure 4 : Examples of generated responses by our selfconscious agent with Distractor Memory ( S1+DM ) on the PersonaChat dataset ( Zhang et al , 2018 ) .
We compare it with the base speaker ( S0 ) of TransferTransfo ( Wolf et al , 2019b ) and the human response ( Human ) .
5.6 Controlling the Self - Conscious Agent
To further analyze our self - conscious agent , we conduct experiments by controlling three features
911P1 ‚Äôs PersonaI own a house in Florida .
I work in it and have been at the same company for 15 years .
I enjoy American sportsI‚Äôve a children and a dogs .
Dialogue History[P2 ]
Hello , how are you today?[P1 ]
Hey !
Just watching a game of football with my children .
You?[P2 ]
That ‚Äôs cool !
I am an alcoholic who is recovering.(S1+DM)iwork in a company and i ‚Äôm a workaholic.(S0)i‚Äômsorry to hear that .
I ‚Äôm a retired professional athlete.(Human)ah man congrats for trying to get back on the road !
P1 ‚Äôs PersonaMy family does not support my career choices .
My dream car is a Rolls Royce ghost .
I often shop for shoes and watches .
I like shopping .
I currently work for a watch dealer .
Dialogue History[P1 ] I really enjoy shopping and my dream is to one day own a Rolls Royce ghost.[P2 ]
Wow .
I enjoy running over driving.[P1 ] Running is also quite lovely .
Breathing in the lovely outside air.[P2 ]
Yes it is .
It clears my head when I need to as well.(S1+DM)shopping is a great way to clear myhead.(S0)ilove to shop and watch movies.(Human)yes , and it also helps with depression ihave found.‚Ä¢ ‚Ä¢ ‚Ä¢‚Ä¢ ‚Ä¢ ‚Ä¢  of our agent : world prior updates pt(i ) , listener rationality Œ≤ and speaker rationality Œ± .
0 instead of Lt
World Prior Update .
0 twice ( i.e. in Lt
0(ut|i , h , u < t ) √ó Lt
In the self - conscious agent , the world prior acts as a cumulative state over personas .
We remind that we propose to update the world prior with Lt 1 in Eq .
( 3 ) .
As reported in Cohn - Gordon et al ( 2018 ) , our experiments on the Dialogue NLI dataset conÔ¨Årm the prior update with Lt 1 makes little difference in performance compared with using a uniform distribution .
However , our approach with Lt 0 makes signiÔ¨Åcant difference , as shown in Figure 5 .
The reason is that the pragmatic listener Lt 1 ‚àù St 0(i|h , u‚â§t , pt ) reÔ¨Çects the current St 0 and in itself ) per time step .
Hence , the update with Lt 1 becomes more of an instantaneous prior rather than a cumulative one .
On the other hand , Lt 0 moderately combines the information from both St 0 and pt(i ) , preserving better cumulative information .
Listener Rationality Œ≤ .
We add Œ≤ in Lt
0 to control the amount of information incorporated to the world prior pt(i ) .
Figure 5 depicts that when Œ≤ is large , the Hits@1 scores ( i.e. the GT accuracy ) drop .
With a big Œ≤ , the information St 0 at current time step overrides the cumulative prior pt(i ) .
That is , the utterance state evolves shortsightedly , ignoring the context information from the previous steps .
Therefore , setting of Œ≤ ‚â§ 1 is advantageous for the self - conscious agent to incrementally decode .
Speaker Rationality Œ± .
Figure 6 shows an example of how generated responses vary according to the intensity of speaker rationality Œ± .
As Œ± increases , the self - conscious agent reÔ¨Çects the listener ‚Äôs distribution ( i.e. the likelihood ) more into the posterior .
When Œ± is too large , the posterior distribution is overwhelmed by the likelihood of the persona .
Then , the language model degenerates to favor uttering fragments of the given persona while even ignoring the syntax .
Hence , Œ± can control the degree of copying the given condition text .
An appropriate Œ± value allows the given persona condition to blend smoothly in the utterance .
6 Conclusion This work investigated how modeling public selfconsciousness can help dialogue agents improve persona - consistency .
We showed existing dialogue agents are highly insensitive to contradiction , and introduced an orthogonally applicable method using the RSA framework ( Frank and Goodman , 2012 ) to alleviate the issue .
We also designed a
Figure 5 : Performance variation of the self - conscious agent for TransferTransfo ( left ) and Blender ( right ) according to Œ≤ .
We compare different methods of updating the world prior pt(i ) with L0 ( Ours ) , L1 and a uniform prior .
The dashed line is the base speaker S0 .
Figure 6 : An example of utterance changes by controlling the speaker rationality Œ± on the PersonaChat .
learning method for distractor selection , named Distractor Memory and proposed a better update for the listener ‚Äôs world prior .
Furthermore , we demonstrated how our approach can be generalized to improve dialogue context - consistency .
Our self - conscious agents improved the base agents on the Dialogue NLI ( Welleck et al , 2019 ) and PersonaChat ( Zhang et al , 2018 ) dataset , without consistency labels and NLI models .
An important future direction will be generating the distractors and learning the rationality coefÔ¨Åcients .
