BERT - based distractor generation for Swedish reading comprehension questions using a small - scale dataset
Dmytro Kalpakchi Division of Speech , Music and Hearing KTH Royal Institute of Technology Stockholm , Sweden dmytroka@kth.se
Johan Boye Division of Speech , Music and Hearing KTH Royal Institute of Technology Stockholm , Sweden jboye@kth.se
Abstract
An important part when constructing multiplechoice questions ( MCQs ) for reading comprehension assessment are the distractors , the incorrect but preferably plausible answer options .
In this paper , we present a new BERTbased method for automatically generating distractors using only a small - scale dataset .
We also release a new such dataset of Swedish MCQs ( used for training the model ) , and propose a methodology for assessing the generated distractors .
Evaluation shows that from a student ’s perspective , our method generated one or more plausible distractors for more than 50 % of the MCQs in our test set .
From a teacher ’s perspective , about 50 % of the generated distractors were deemed appropriate .
We also do a thorough analysis of the results .
1
Introduction
Multiple - choice questions ( MCQs ) are widely used for student assessments , from high - stakes graduation tests to lower - stakes reading comprehension tests .
An MCQ consists of a question ( stem ) , the correct answer ( key ) and a number of wrong , but plausible options ( distractors ) .
The problem of automatically generating stems with a key has received a great deal of attention , e.g. , see the survey by Amidei et al ( 2018 ) .
By comparison , automatically generating distractors is substantially less researched , although Welbl et al ( 2017 ) report that manually ﬁnding reasonable distractors was the most time - consuming part in writing science MCQs .
Indeed , reasonable distractors should be grammatically consistent and similar in length compared to the key and within themselves .
Given the challenges above , we attempt using machine learning ( ML ) to aid teachers in creating distractors for reading comprehension MCQs .
The problem is not new , however most of the prior work has been done for English .
In this paper we propose
the ﬁrst such solution for Swedish ( although the proposed method is novel even for English , to the best of our knowledge ) .
The key contributions of this work are : proposing a BERT - based method for generating distractors using only a small - scale dataset , releasing SweQUAD - MC1 , a dataset of Swedish MCQs , and proposing a methodology for conducting human evaluation aimed at assessing the plausibility of distractors .
2 Background
2.1 BERT for NLG
Devlin et al ( 2019 ) introduced BERT as the ﬁrst application of the Transformer architecture ( Vaswani et al , 2017 ) to language modelling .
BERT uses only Transformer ’s encoder stacks ( with multihead self - attention , MHSA ) , while the NLG community relies more on Transformer ’s decoder stacks ( with masked MHSA ) for text generation , e.g. , GPT ( Radford et al , 2018 ) .
However , Wang and Cho ( 2019 ) showed that BERT is a Markov random ﬁeld , meaning that BERT learns a joint probability distribution over all sentences of a ﬁxed length , and one could use Gibbs sampling to generate a new sentence .
The authors compared samples generated autoregressively left - to - right by BERT and GPT , and found the perplexity of BERT samples to be higher than GPT ’s ( BERT samples are of worse quality ) , but the n - gram overlap between the generated texts and texts from the dataset to be lower ( BERT samples are more diverse ) .
Liao et al ( 2020 ) show a way to improve BERT ’s generation capabilities via changing the masking scheme to a probabilistic one at training time .
Probabilistically masked language models ( PMLMs ) assume that the masking ratio r for each sentence is drawn from a prior distribution p(r ) .
The au1The dataset and implementation of our models are available in this GitHub repository
Proceedingsofthe14thInternationalConferenceonNaturalLanguageGeneration(INLG),pages387–403,Aberdeen , Scotland , UK,20 - 24September2021. © 2021AssociationforComputationalLinguistics387  Property # of texts # of MCQs # of D Len(Text ) Len(A ) Len(D ) |Len(A ) - Len(D)|
Training 434 962 2.1 ± 0.5 384.9 ± 330.1 4.2 ± 3.4 4.5 ± 3.9 1.9 ± 2.4
Development 64 126 2.1 ± 0.4 355.1 ± 233.1 4.4 ± 3.5 4.3 ± 4.0 1.9 ± 2.3
Test 45 102 2.0 ± 0.2 357.9 ± 254.3 4.6 ± 4.5 4.0 ± 3.7 1.9 ± 2.9
Table 1 : Descriptive statistics of SweQUAD - MC dataset splits .
A denotes the key , D denotes a distractor , Len(X ) denotes a length of X in words .
x ± y shows mean x and a standard deviation y
thors proposed to train a PMLM with a uniform prior ( referred to as u - PMLM ) .
The absence of the left - to - right restriction allows the model to generate sequences in an word arbitrary order .
In fact , Liao et al ( 2020 ) propose to generate sentences by randomly selecting the masked position , predicting a token for it , replacing the masked token with the predicted one and repeating the process until no masked tokens are left .
The authors showed that the perplexity of the texts generated by u - PMLM is comparable to the ones by GPT .
2.2 Convolution partial tree kernels
As mentioned previously , plausible distractors should be grammatically consistent with the key .
Hence , a metric measuring grammatical consistency would be useful both for quantitative evaluation and as a basis for a baseline method .
We propose to use convolution partial tree kernels ( CPTK ) for these purposes .
CPTK were proposed by Moschitti ( 2006 ) for dependency trees and essentially calculate the number of common tree structures ( not only full subtrees ) between two given trees .
However , CPTKs can not handle labeled edges and were applied to dependency trees containing only lexicals .
Another solution , proposed by Croce et al ( 2011 ) and used in this article , is to include edge labels , i.e. , grammatical relations ( GR ) , as separate nodes .
A resulting computational structure is Grammatical Relation Centered Tree ( GRCT ) , which transforms the original dependency tree by making each PoS - tag a child of a GR node and a father of a lexical node .
CPTKs can take any non - negative values and are thus hard to interpret .
Hence , we use normalized CPTK ( NCPTK ) shown in Equation ( 1 ) , where K(T1 , T2 ) is the CPTK applied to the dependency trees T1 and T2 .
Evidently , when T1 and T2 are the same , ( cid:101)K(T1 , T2 ) equals to 1 , which is the highest value it can take .
3 Data
We have collected a Swedish dataset , henceforth referred to as SweQUAD - MC , consisting of texts and MCQs for the given texts .
The dataset was created by three paid linguistics students instructed to pose unambiguous and independent questions .
They were also asked to identify the key with at least two distractors , all of which are contiguous phrases in a given text .
Additionally , as the distractors were required to be in the same grammatical form as the key ( e.g. , both in plural ) , the students were allowed to change the grammatical form of phrases if they constituted plausible distractors after this change .
The exact instructions given to the students along with more details on the used texts are provided in Appendix A.
Each datapoint in SweQUAD - MC consists of a base text and an MCQ , i.e. a stem , the key and at least two distractors .
The same text can be reused for different MCQs , but the sets of texts in training ( ∼ 80 % ) , development ( ∼ 10 % ) and test ( ∼ 10 % ) datasets are disjoint .
However , some overlap in sentences is possible , since the texts might come from the same source .
Descriptive statistics of all SweQUAD - MC splits is provided in Table 1 .
4 Method
Given the small scale of SweQUAD - MC we have decided to ﬁne - tune a pretrained BERT2 for Swedish ( Malmsten et al , 2020 ) on the task of distractor generation ( DG ) .
For achieving this , we have added on top of BERT two linear layers with layer normalization ( Ba et al , 2016 ) in the middle to be trained from scratch ( see architecture in Figure 1 ) .
The last linear layer is followed by a
( cid:101)K(T1 , T2 )
=
K(T1 , T2 ) ( cid:112)K(T1 , T1)(cid:112)K(T2 , T2 )
,
( 1 )
2bert - base - cased
388  softmax activation giving probabilities over the tokens in the vocabulary for each position in the text .
We trained the model using cross - entropy loss only for tokens in masked positions .
Recall that each MCQ consists of a base text T , the stem Q based on T , the key A and ( on average ) two distractors D1 and D2 .
The DG problem is then to generate distractors conditioned on the context , consisting of T , Q and A.
We provide all context components as input to the BERT model , separated from each other by the special separator token
[ SEP ] .
Given that BERT ’s maximum input length is 512 tokens , we trim T to the ﬁrst 384 tokens ( later referred to as T 384 ) , since that is the average text length of the training set .
We have explored two different solution variants of DG .
The ﬁrst variant aims at generating distractors autoregressively , left to right .
At generation time , the input to BERT consists of a context CTX ( T 384 , Q and A separated by [ SEP ] token ) , a [ SEP ] token , and a [ MASK ] token at the end .
After a forward pass through BERT , the [ MASK ] token gets replaced by the word with the highest softmax score , which becomes the ﬁrst word of the ﬁrst distractor ( dubbed D11 ) .
The generation of the ﬁrst distractor continues by appending a [ MASK ] token after each forward pass until the network generates a separator token
[ SEP ] , which concludes the generation of the ﬁrst distractor D1 .
The next distractor D2 is generated in the same way , except that the CTX is extended by D1 .
At training time , we use the same procedure , but with teacher forcing , allowing us to use the correct distractor tokens as targets for the cross - entropy loss ( see example training datapoints for one MCQ in Table 2 ) .
The second variant is inspired by u - PMLM , and aims at generating distractors autoregressively , but in an arbitrary word order .
At generation time , the input to BERT consists of a context CTX , a [ SEP ] token , and a predeﬁned number of [ MASK ] tokens ( see Section 6.1 ) .
The generation proceeds by unmasking the token at the position where the model is most conﬁdent .
This differs from unmasking a random position , proposed by Liao et al ( 2020 ) .
The training procedure largely follows a masking scheme employed by u - PMLM by drawing the masking ratio from the uniform distribution ( see example training datapoints for one MCQ in Table 2 ) .
Note that we do not include the [ SEP ] token when training , since we found that the trained model would constantly generate [ SEP ] tokens .
Figure 1 : The DG model architecture .
B is the batch size and V is the vocabulary size .
The light green blocks represent the activation functions for the respective linear layers .
The purple block represents parts of the network initialized with the pretrained weights .
Each sampled masking ratio r for the u - PMLM variant means that each token in the distractors from the dataset has a probability r to be masked .
Hence , different r will potentially result in different number of masked tokens and at different positions .
The number of times we draw r per distractor DX is proposed to be min(Len(DX ) , MAX MASKINGS ) .
4.1 Baseline
As mentioned in Section 2.2 , NCPTK measures grammatical consistency between the key and a distractor .
Our baseline uses NCPTK on Universal Dependencies ( UD ) trees ( Nivre et al , 2020 ) in the following way .
For each given MCQ , we exclude the sentence containing the key from the base text and then parse each remaining sentence si of the text , and the key using the UD parser for Swedish .
Let Tsi and Tk denote a dependency tree corresponding to si and the key respectively .
For each Tsi , we ﬁnd all subtrees with the root having the same universal PoS - tag and the same universal features ( representing morphological properties of the token ) as the root of Tk .
If no subtrees are found , no distractors can be suggested for this MCQ .
Otherwise , we calculate NCPTK between each found subtree and Tk ( both as GRCT , but without lexicals ) .
Then we take the textual representation of the K subtrees with the highest NCPTK as the distractor suggestions .
389LinearGELULinearSOFTMAXLayerNormBERT(B , 512 , 768)(B , 512 , 768)(B , 512 , 768)(B , 512 , V)[CLS ] T
[ SEP ] Q
[ SEP ]
A [ SEP]D11 D12 [ MASK ]
[ CLS ] T [ SEP ] Q
[ SEP ]
A [ SEP][MASK ] D12
[ MASK ] left to rightu - PMLMD13D11D13left to rightu - PMLMCross - entropy lossInput dataLabels  Input for left - to - right variant
[ CLS ] CTX [ SEP ]
[ MASK ]
[ CLS ] CTX [ SEP ]
D11 [ MASK ]
[ CLS ] CTX [ SEP ] D11 D12
[ MASK ]
[ CLS ] CTX [ SEP ] D11 D12
[ SEP ]
[ MASK ]
[ CLS ] CTX [ SEP ] D11 D12
[ SEP ] D21 [ MASK ]
[ CLS ] CTX [ SEP ] D11 D12
[ SEP ] D21 D22 [ MASK ]
[ CLS ] CTX [ SEP ] D11 D12
[ SEP ] D21 D22 D23 [ MASK ]
[ SEP ]
Target D11 D12
[ SEP ] D21 D22 D23
Input for u - PMLM variant
[ CLS ] CTX [ SEP ]
D11 [ MASK ]
[ CLS ] CTX [ SEP ]
[ MASK ] D12
[ CLS ] CTX [ SEP ] D11 D12
[ SEP ] D21 [ MASK ]
[ MASK ]
[ CLS ] CTX [ SEP ] D11 D12
[ SEP ] D21 [ MASK ]
D23 [ CLS ] CTX [ SEP ] D11 D12
[ SEP ]
[ MASK ] D22 [ MASK ]
Target(s )
D12 D11 D22 , D23 D22 D21 , D23
Table 2 : Example datapoints extracted from one MCQ if training the autoregressive left - to - right variant ( top table ) or u - PMLM variant ( bottom table ) .
D1 and D2 are distractors , assumed to have 2 and 3 words , respectively .
CTX represents the context , i.e. , the sequence T 384
[ SEP ] Q
[ SEP ] A , where T 384 is the ﬁrst 384 tokens of the text , Q is a stem and A is the key .
5 Experimental setup
6.1 Quantitative evaluation
We have used Huggingface ’s Transformers library ( Wolf et al , 2020 ) for implementing the DG model .
The training hardware setup included 16 Intel Xeon CPU E5 - 2620 v4 ( 2.10GHz ) , 64 GB of RAM and 1 NVIDIA GeForce RTX 2080
Ti ( 11 GB VRAM ) .
For this setup , we have ﬁxed the random seed to 42 , the number of training epochs to 6 , the batch size to 4 ( for both training and dev sets ) and MAX MASKINGS to 20 ( for u - PMLM variant only ) .
With these settings , training took about 3.67h for the left - to - right and 3h for the u - PMLM variant .
UD trees for the baseline were obtained using Stanza package ( Qi et al , 2020 ) and convolution partial tree kernels on the UD trees were calculated using UDon2 library ( Kalpakchi and Boye , 2020 ) .
Baseline requires no training and running our implementation of the baseline takes about a minute on the development or test set .
6 Evaluation
Following the analysis of Rodriguez ( 2005 ) , we generate three distractors per MCQ for each model .
Due to prohibitively high costs of human evaluation , we have divided the evaluation process into two stages .
The ﬁrst stage is quantitative evaluation , which gives limited information about the model ’s quality , but is sufﬁcient for model selection .
The second stage is human evaluation of the best model , selected during the ﬁrst stage .
Automatic evaluation metrics , such as BLEU ( Papineni et al , 2002 ) , ROUGE ( Lin , 2004 ) , METEOR ( Denkowski and Lavie , 2014 ) , CIDEr ( Vedantam et al , 2015 ) , became popular in NLG in recent years .
Essentially , these metrics rely on comparing word overlap between a generated distractor and a reference one .
Such metrics can yield a low score even if the generated distractor is valid but just happens to be different from the reference one , or a high score even though the distractor is ungrammatical but happens to have a high word overlap with the reference one ( see the article by Callison - Burch et al ( 2006 ) for a further discussion ) .
Furthermore , they do not take into account how well a generated distractor is aligned with the key grammatically or how challenging the whole group of generated distractors would be .
To account for the properties mentioned above , we have experimented with a number of quantitative metrics and propose the following set to be used ( the whole list is available in Appendix B ) .
In the following list MCQ% means “ Percentage of MCQ ” and DIS means “ generated distractor(s ) ” .
1 . DisRecall .
Distractor recall .
2 . AnyDisRefMatch .
MCQ% with at least 1 DIS
matching a reference one .
3 . AnyDisInText .
MCQ% with at least 1 DIS
appearing in the base text .
390  4 . KeyInDis .
MCQ% with key being among DIS .
5 . AnySameDis .
MCQ% with ≥ 2 identical DIS .
6 . AllSameDis .
MCQ% with all identical DIS .
7 . AnyDisRep .
MCQ% with ≥ 1 DIS containing
repetitive words contiguously .
8 .
AnyDisEmpty .
MCQ% with ≥ 1 DIS being
an empty string3 .
9 . AnyDisFromTrainDis .
MCQ% with at least 1 DIS matching with a distractor from training data , but not appearing in the base text .
10 . MeanNCPTK , MedianNCPTK , ModeNCPTK .
Mean , median , and mode NCPTK for pairs of UD trees for DIS and keys ( all trees as GRCT , but ignoring nodes corresponding to lexicals ) .
The ﬁrst group consists of metrics 1 - 3 .
The ﬁrst two metrics count exact matches between generated and reference distractors .
The rationale behind metric 3 is our assumption that distractors coming from the same text are more challenging .
The higher the values of all these metrics are , the better .
The second group contains metrics 4 - 8 , which give an idea of how challenging the whole group of distractors would be .
For instance , duplicate distractors or ones with word repetitions could be excluded by students using common sense .
The lower the metrics in this group are , the better .
The third group consists only of metric 9 , serving as an overﬁtting indicator .
The metric accounts for the distractors appearing as distractors in training data and high percentage indicates an overﬁtting possibility .
The lower the values , the better .
The ﬁnal group ( item 10 ) measures how syntactically aligned generated distractors and the respective keys are .
We employ NCPTK to measure the similarity of syntactic structures between each distractor and the respective key .
Then we take mean , median and mode of the sequence of NCPTKs obtained in the previous step .
The higher the values of these metrics are , the better .
Based on these metrics , we performed a model selection on the development set and chose the models performing best on the most of these metrics .
Left - to - right model generated distractors token by token until either a [ SEP ] token was generated or the length of the distractor was 20 tokens .
3After excluding the special tokens , e.g. , [ SEP ]
Metric DisRecall ↑ AnyDisRefMatch ↑
AnyDisInText ↑ KeyInDis ↓
AnySameDis ↓ AllSameDis ↓
AnyDisRep ↓ AnyDisEmpty ↓ AnyDisFromTrainDis ↓ MeanNCPTK ↑ MedianNCPTK ↑
ModeNCPTK ↑
Baseline 1.44 % 2.94 % 100.0 % 0.00 % 4.9 % 0.00 % 0.00 % 11.76 % NA 0.43 0.28 1.0 ( 20.56 % )
u - PMLM 15.31 % 26.47 % 72.55 % 4.9 % 13.73 % 1.96 % 2.94 % 0.00 % 0.98 % 0.43 0.28 1.0 ( 20.69 % )
Table 3 : Evaluation of DG models on the test set .
When using u - PMLM , shortest distractors were generated ﬁrst .
↑ ( ↓ ) means “ the higher ( lower ) , the better ” .
In contrast , u - PMLM needs the lengths of the distractors to be decided in beforehand , which we set to be the lengths of the two reference distractors and the length of the key4 .
Surprisingly , the order of distractors in terms of their length also matters for generation with u - PMLM , so we have tested three options : shortest ﬁrst , longest ﬁrst and random order .
According to the results of model selection on the development set ( presented in detail in Appendix C ) , u - PMLM models outperformed left - to - right models by a substantial margin .
The best u - PMLM model ( generating shortest distractors ﬁrst ) and the baseline have been evaluated on the test set ( see Table 3 ) .
Interestingly , the similarity of syntactic structures between the key and distractors ( assessed by NCPTK ) is the same for both baseline ( that actually relies on NCPTK ) and u - PMLM .
At the same time , u - PMLM generates more distractors matching the reference ones compared to the baseline ( as seen from DisRecall and AnyDisRefMatch ) .
The baseline generates at least one empty string as a distractor 11.76 % of the time ( compared to no such cases for u - PMLM ) limiting possibilities of using the baseline in the real - life applications .
6.2 Human evaluation
We have used distractors generated on the test set by the best u - PMLM model ( selected after quantitative evaluation in Section 6.1 ) to conduct human
4If reference distractors are not available , we propose to generate distractors with the length differing by at most two words compared to the length of the key .
391  evaluation in 2 stages : from a perspective of a student and a teacher .
6.2.1 Student ’s perspective
A desirable property of reading comprehension MCQs is that the students should be unable to answer them correctly without reading the actual text .
To put more formally , the average number of correctly answered MCQs without reading the actual text ( denoted N s ) should not differ signiﬁcantly from the average number of correctly answered MCQs when choosing the answer uniformly at random ( denoted N r ) .
To test for this property , we have formulated the following two hypotheses.5
H0 : N s = N r. H1 : N s ( cid:54)= N r.
For N MCQs with 4 options , N r = 0.25N , which for our test set would be equal to N r = 0.25 · 102 = 25.5 .
The appropriate statistical test in this case is one - sample two - tailed t - test with the aim of not being able to reject H0 .
Given that the purpose is to show that the data supports H0 , we have set both the probability α of type I errors and the probability β of type II errors to be 0.05 .
Then we have used G*Power ( Faul et al , 2009 ) to calculate the required sample size for ﬁnding a medium effect size ( 0.5 ) and the given α and β , which turned out to be 54 subjects .
Following the calculations above , we have recruited 54 subjects on the Proliﬁc platform6 , and instructed them to choose the most plausible answer to a number of reading comprehension MCQs without providing the original texts .
The collected data did not violate any assumptions for a one - sample t - test ( see Appendix D.1 for more details ) .
On average , the subjects correctly answered a signiﬁcantly larger number of questions than N r ( N s = 62.26 , SE = 1.09 , t(53 ) = 33.51 , p < 0.05 , r = 0.98 ) .
To summarize , the chances of this sample to be collected are very low if H0 were true .
However , evidently some of the generated distractors were actually plausible , given that N s ( cid:54)= N .
To investigate the matter we have plotted the histogram of the frequency of choice of distractors by the subjects in Figure 2 .
As suggested by Haladyna and Downing ( 1993 ) , distractors that are chosen by less than 5 % of students should not be used , which in our case amounts to 39 % of the dis5Preregistration is available here 6https://www.proliﬁc.co/
Figure 2 : A histogram showing the frequency of choice of distractors in subjects ’ answers
Figure 3 : A histogram showing the entropy distribution per question
tractors ( the leftmost bar in Figure 2 ) .
If we eliminate these low - frequency distractors ( LF - DIS ) , 68 MCQs ( 66.67 % ) will lose at least one distractor , 10 MCQs ( 9.8 % ) will lose all distractors and thus 34 MCQs ( 33.33 % ) will keep all 3 distractors .
A more relaxed question is how many MCQs had at least one plausible distractor , which can be estimated by calculating the entropy for each question as shown in Equation ( 2 ) , where A is the key , D is a distractor , Q is the stem , PQ(A ) ( PQ(D ) ) is the probability that the key ( any distractor ) is chosen for Q by a subject .
H(Q )
= −
pQ(O ) log(pQ(O ) )
( 2 )
( cid:88 )
O∈{A , D }
The distribution of entropies per question is shown in Figure 3 .
Assuming the natural logarithm , the highest theoretically possible value for H(Q ) is 0.69 , if pQ(A ) = pQ(D )
= 0.5 .
32 % of MCQs had an entropy larger than 0.65 , whereas 51 % had an entropy larger than 0.6 , which means that half of MCQs had at least one plausible distractor .
3920.00.10.20.30.40.50.60.70.80.91.0Proportion of students0.000.050.100.150.200.250.300.350.40Proportion of distractors0.390.20.10.060.060.030.040.020.040.020.00.010.00.00.010.00.00.00.00.10.20.30.40.50.60.7Entropy0.000.050.100.150.200.250.30Proportion of questions0.050.020.030.040.030.10.020.010.050.080.070.190.32  6.2.2 Teacher ’s perspective
Bearing in mind the ﬁndings of Section 6.2.1 , it is interesting to see which of the proposed distractors ( especially , among LF - DIS ) teachers would mark as acceptable .
Given the complexity of such evaluation , using the whole test set was infeasible .
To get a representative sample , we used entropy per question ( shown in Figure 3 ) .
All MCQs were divided into 5 equally sized buckets by entropy and 9 MCQs were sampled uniformly at random from each bucket , resulting in 45 MCQs in total .
We asked 5 teachers to evaluate each MCQ ( presented in a random order for each of them ) .
Each MCQ contained the base text , the stem , the key and the generated distractors .
The teachers were instructed to select those of generated distractors ( if any ) deemed suitable for testing reading comprehension .
Additionally , we asked to provide their reasons for each rejected distractor in a free - text input .
The inter - annotator agreement ( IAA ) was estimated using Goodman - Kruskal ’s γ ( Goodman and Kruskal , 1979 ) , speciﬁcally its multirater version γN proposed by Kalpakchi and Boye ( 2021 ) .
On the scale proposed by Rosenthal ( 1996 ) , we have found a very large agreement ( γN = 0.85 , see Appendix D.2.2 for more details on IAA calculations ) .
On average , 1.47 distractors per MCQ were accepted by a teacher .
Their reasons for rejections are distributed as shown in Figure 4 .
All teachers accepted at least one generated distractor for 39 MCQs ( 86.7 % ) , whereas the majority of teachers did so for 27 MCQs ( 60 % ) .
Interestingly , there are no MCQs in which all 5 teachers have either accepted or rejected all generated distractors .
However , the majority of teachers has accepted or rejected all distractors for 4 MCQs ( 8.9 % ) and 6 MCQs ( 13.3 % ) respectively .
Out of 45 MCQs , 31 ( 68.9 % ) had at least one LF - DIS , as deﬁned in Section 6.2.1 .
For these 31 MCQs we report a distribution of accepted / rejected LF - DIS by the majority of teachers in Figure 5 .
Let us call the 15 MCQs with all LF - DIS accepted by the majority of teachers as mismatch MCQs ( lowest row in Figure 5 ) .
Interestingly , 12 of the 15 mismatch MCQs had at least one more distractor in addition to LF - DIS being accepted by the majority of teachers .
Furthermore , all mismatch MCQs had entropy higher than 0.3 .
This entails that almost a half of LF - DIS should not necessarily be thrown away , since they were accepted by teachers , but
Figure 4 : A histogram showing the distribution of teachers ’ reasons behind rejecting distractors .
Figure 5 : A bi - variate histogram showing the distribution of the 31 MCQs ( the numbers on the bars sum to 31 ) with at least 1 LF - DIS , with respect to their LF - DIS being accepted / rejected by the majority of teachers .
the MCQs either happened to have more plausible distractors or subjects might have had relevant background knowledge to answer the questions .
7 Related work
We employed a systematic process to get a comprehensive overview of DG methods ( see Appendix E for more details ) .
Out of the resulting 28 articles ( see an overview in Table 4 ) , only 2 worked with a language other than English ( Chinese and Basque ) .
In this paper we work on reading comprehension MCQs , which makes only 12 papers , dealing with factual questions , relevant .
Two of these used rule - based approaches .
Majumder and Saha ( 2015 ) generated MCQs for cricket domain and used a number of hand - crafted rules based on gazeteers and Wikipedia entries to generate distractors .
Mitkov and Ha ( 2003 ) proposed to generate distractors for MCQs on electronic instructional documents using WordNet .
Six of these relied on extractive approaches .
393Not wrongUnreasonableGrammatically wrongUnreas .
wrt questionIdenticalObviously wrongDissimilar with keyToo longToo easyUnreas .
wrt textVagueToo complexWrong formNot in the textToo abstractNegationBoth right and wrong051015202530Percentage of judgements0123Number of accepted distractors0123Number of rejected distractors105451222  Liang et al ( 2018 ) , Welbl et al ( 2017 ) , and Ha and Yaneva ( 2018 ) formulated choosing a distractor as a ranking problem from the given candidate set .
In the ﬁrst two articles the candidate set constituted all distractors from the available MCQ dataset .
The authors then trained ML - based ranker(s ) for choosing the best distractors .
In the last one , the candidate set was created using content engineers .
Distractors with a high similarity of their concept embeddings ( summed for multiple words ) and appearing in the same document as the key are ranked higher .
Stasaski and Hearst ( 2017 ) and Araki et al ( 2016 ) worked in the domain of biology .
The former used an ontology and the latter employed event graphs containing information about coreferences to generate distractors .
Karamanis et al ( 2006 ) used thesaurus and tf - idf to identify key concepts in the given text and then select as distractors those having the same semantic type as the key .
The remaining four employed neural methods and are most relevant among the surveyed .
Qiu et al ( 2020 ) trained a sequence - to - sequence ( seq2seq ) model with a number of attention layers .
Zhou
et
al ( 2020 ) also employed a seq2seq model , but with a hierarchical attention to capture the interaction between a text and a question , as well as semantic similarity loss .
Both articles used a beam search combined with ﬁltering based on Jaccard coefﬁcient at generation time .
Offerijns et al ( 2020 ) trained a GPT-2 model to generate 3 distractors for a given MCQ , and used BERT - based question answering model for quantitative evaluation ( along with human evaluation ) .
Finally , Chung et al ( 2020 ) proposed a BERTbased method for English with answer - negative regularization , penalizing distractors for containing
Problem / method property ( cid:4 ) Extractive ( cid:4 ) Generative , rule - based ( cid:4 ) Generative , neural
Only automatic evaluation Only human evaluation ( cid:32 ) Automatic and human evaluattion ( cid:32 ) ( cid:78 ) Cloze - style , single - word answers ( cid:32 ) ( cid:78 ) Cloze - style , continue the sentence ( cid:78 )
Factual questions
# 14 7 7 5 19 4 14 2 12
Table 4 : 28 related works broken down by method ( ( cid:4 ) ) , type of evaluation ( ) and types of questions for which distractors have been generated ( ( cid:78 ) )
( cid:32 )
the same words as the key , and training a sequential and a parallel MLM model simultaneously .
At generation time , they generate one distractor , and then create a distractor set of the predeﬁned size based on sampling from the probability distribution returned by BERT for each token of the distractor .
Then they rank every triple of distractors based on the entropy of a separately trained QA model .
Our method also relies on BERT , but has a number of differences beyond being applied to Swedish .
Firstly , we did not include answer - negative regularization , since it is not always a good strategy .
For instance , given the stem “ When should you pay a fee if you apply for a visa ? ” and a key “ before you have submitted the application ” , the best distractor would be “ after you have submitted the application ” , which shares most of the words with the key .
Secondly , we generate distractors in arbitrary word order compared to left - to - right generation in ( Chung et al , 2020 ) .
Thirdly , at generation time , we use previously generated distractors as input for generating next ones , and always take tokens with a maximum probability .
This lowers the risk of generating ungrammatical distractors .
Finally , our training set is 100 times smaller compared to the training set used by Chung et al ( 2020 ) .
8 Conclusion
We have collected SweQUAD - MC , the ﬁrst dataset of Swedish MCQs , and showed the possibility of training usable BERT - based DG models , despite the small scale of the dataset .
We have showed that a u - PMLM variant of the BERT - based DG model performs best on the dataset , and proposed a novel methodology of evaluating the plausibility of generated distractors .
Around half of the generated distractors were found acceptable by the majority of teachers , and more than 50 % of MCQs had at least one plausible generated distractor , judging by the entropy of students ’ responses .
Bearing in mind that the aim of the proposed method is to support ( not replace ) teachers , we deem that our method works well for MCQs in Swedish ( and potentially in other languages with a pretrained BERT and a dataset of a similar scale ) .
Furthermore , we have presented a baseline applicable to any language with a UD treebank ( currently about 100 languages ) .
Although its performance is nowhere near the u - PMLM variant , we believe that it can serve as a good point of comparison to emerging neural methods for other languages .
394  Acknowledgments
This work was supported by Vinnova ( Sweden ’s Innovation Agency ) within project 2019 - 02997 .
We would like to thank the anonymous reviewers for their comments , as well as Gabriel Skantze and Bram Willemsen for their helpful feedback prior to the submission of the paper .
