Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing , pages 3252–3262 August 1–6 , 2021 .
© 2021 Association for Computational Linguistics3252Joint Models for Answer Veriﬁcation in Question Answering Systems
Zeyu
ZhangThuy
Vu Alessandro Moschitti School of Information , The University of Arizona , Tucson , AZ , USA Amazon Alexa AI , Manhattan Beach , CA , USA zeyuzhang@email.arizona.edu , fthuyvu , amosch g@amazon.com
Abstract
This paper studies joint models for selecting correct answer sentences among the top kprovided by answer sentence selection ( AS2 ) modules , which are core components of retrievalbased Question Answering ( QA ) systems .
Our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers .
For this purpose , we build a three - way multiclassiﬁer , which decides if an answer supports , refutes , or is neutral with respect to another one .
More speciﬁcally , our neural architecture integrates a state - of - the - art AS2 module with the multi - classiﬁer , and a joint layer connecting all components .
We tested our models on WikiQA , TREC - QA , and a real - world dataset .
The results show that our models obtain the new state of the art in AS2 .
1 Introduction Automated Question Answering ( QA ) research has received a renewed attention thanks to the diffusion of Virtual Assistants .
Among the different types of methods to implement QA systems , we focus on Answer Sentence Selection ( AS2 ) research , originated from TREC - QA track ( V oorhees and Tice , 1999 ) , as it proposes efﬁcient models that are more suitable for a production setting , e.g. , they are more efﬁcient than those developed in machine reading ( MR ) work ( Chen et al . , 2017 ) .
Garg et
al .
( 2020 ) proposed the T AND A approach based on pre - trained Transformer models , obtaining impressive improvement over the state of the art for AS2 , measured on the two most used datasets , WikiQA ( Yang et al . , 2015 ) and TRECQA ( Wang et al . , 2007 ) .
However , T AND A was applied only to pointwise rerankers ( PR ) , e.g. , simple binary classiﬁers .
Bonadiman and Moschitti Work done while the author was an intern at Amazon AlexaClaim : Joe Walsh was inducted in 2001 .
Ev1 : As a member of the Eagles , Walsh was inducted into the Rock and Roll Hall of Fame in 1998 , and into the V ocal Group Hall of Fame in 2001 .
Ev2 : Joseph Fidler Walsh ( born November 20 , 1947 ) is an American singer songwriter , composer , multiinstrumentalist and record producer .
Ev3 : Walsh was awarded with the Vocal Group Hall of Fame in 2001 .
Table 1 : A claim veriﬁcation example from FEVER .
( 2020 ) tried to improve this model by jointly modeling all answer candidates with listwise methods , e.g. , ( Bian et al . , 2017 ) .
Unfortunately , merging the embeddings from all candidates with standard approaches , e.g. , CNN or LSTM , did not improve over TANDA .
A more structured approach to building joint models over sentences can instead be observed in Fact Veriﬁcation Systems , e.g. , the methods developed in the FEVER challenge ( Thorne et al . , 2018a ) .
Such systems take a claim , e.g. , Joe Walsh was inducted in 2001 , as input ( see Tab . 1 ) , and verify if it is valid , using related sentences called evidences ( typically retrieved by a search engine ) .
For example , Ev1,As a member of the Eagles , Walsh was inducted into the Rock and Roll Hall of Fame in 1998 , and into the Vocal Group Hall of Fame in 2001 , andEv3,Walsh was awarded with the Vocal Group Hall of Fame in 2001 , support the veracity of the claim .
In contrast , Ev2is neutral as it describes who Joe Walsh is but does not contribute to establish the induction .
We conjecture that supporting evidence for answer correctness in AS2 task can be modeled with a similar rationale .
In this paper , we design joint models for AS2 based on the assumption that , given qand a target answer candidate t , the other answer candidates , ( c1;::ck ) can provide positive , negative , or neutral support to decide the correctness of t. Our ﬁrst approach exploits Fact Checking research : we adapted a state - of - the - art FEVER system , KGAT ( Liu et al . , 2020 ) , for AS2 .
We deﬁned a claim as
3253a pair constituted of the question and one target answer , while considering all the other answers as evidences .
We re - trained and rebuilt all its embeddings for the AS2 task .
Our second method , Answer Support - based Reranker ( ASR ) , is completely new , it is based on the representation of the pair , ( q , t ) , generated by state - of - the - art AS2 models , concatenated with the representation of all the pairs ( t;ci ) .
The latter summarizes the contribution of each citotusing a maxpooling operation .
cican be unrelated to ( q;t ) since the candidates are automatically retrieved , thus it may introduce just noise .
To mitigate this problem , we use an Answer Support Classiﬁer ( ASC ) to learn the relatedness between tandciby classifying their embedding , which we obtain by applying a transformer network to their concatenated text .
ASC tunes the ( t;ci ) embedding parameters according to the evidence that ciprovides tot .
Our Answer Support - based Reranker ( ASR ) signiﬁcantly improves the state of the art , and is also simpler than our approach based on KGAT .
Our third method is an extension of ASR .
It should be noted that , although ASR exploits the information from the kcandidates , it still produces a score for a target twithout knowing the scores produced for the other target answers .
Thus , we jointly model the representation obtained for each target in a multi - ASR ( MASR ) architecture , which can then carry out a complete global reasoning over all target answers .
We experimented with our models over three datasets , WikiQA , TREC - QA and WQA , where the latter is an internal dataset built on anonymized customer questions .
The results show that : •ASR improves the best current model for AS2 , i.e. , T AND A by3 % , corresponding to an error reduction of 10 % in Accuracy , on both WikiQA and TREC - QA .
•We also obtain a relative improvement of 3 % over T AND A on WQA , conﬁrming that ASR is a general solution to design accurate QA systems .
•Most interestingly , MASR improves ASR by additional 2 % , conﬁrming the beneﬁt of joint modeling .
Finally , it is interesting to mention that MASR improvement is also due to the use of FEVER data forpre-ﬁne - tuning ASC , suggesting that the fact veriﬁcation inference and the answer support inference are similar .
2 Problem deﬁnition and related work We consider retrieval - based QA systems , which are mainly constituted by ( i ) a search engine , retrieving documents related to the questions ; and ( ii ) an AS2 model , which reranks passages / sentences extracted from the documents .
The top sentence is typically used as ﬁnal answer for the users .
2.1 Answer Sentence Selection ( AS2 )
The task of reranking answer - sentence candidates provided by a retrieval engine can be modeled with a classiﬁer scoring the candidates .
Let qbe an element of the question set , Q , andA = fc1;:::;c ng be a set of candidates for q , a reranker can be deﬁned asR : Q(A)!(A ) , where (A)is the set of all permutations of A. Previous work targeting ranking problems in the text domain has classiﬁed reranking functions into three buckets : pointwise , pairwise , and listwise methods .
Pointwise reranking : This approach learns p(q;ci ) , which is the probability of cicorrectly answeringq , using a standard binary classiﬁcation setting .
The ﬁnal rank is simply obtained sorting ci , based onp(q;ci ) .
Previous work estimates p(q;ci ) with neural models ( Severyn and Moschitti , 2015 ) , also using attention mechanisms , e.g. , CompareAggregate ( Yoon et al . , 2019 ) , inter - weighted alignment networks ( Shen et al . , 2017 ) , and pre - trained Transformer models , which are the state of the art .
Garg et
al .
( 2020 ) proposed T AND A , which is the current most accurate model on WikiQA and TREC - QA .
Pairwise reranking : The method considers binary classiﬁers of the form  ( q;ci;cj)for determining the partial rank between ciandcj , then the scoring function p(q;ci)is obtained by summing up all the contributions with respect to the target candidatet = ci , e.g. ,p(q;ci )
= P j  ( q;ci;cj ) .
There has been a large body of work preceding Transformer models , e.g. , ( Laskar et al . , 2020 ; Tayyar Madabushi et al . , 2018 ; Rao et
al . , 2016 ) .
However , these methods are largely outperformed by the pointwise TANDA model .
Listwise reranking : This approach , e.g. , ( Bian et al . , 2017 ; Cao et al . , 2007 ;
Ai et al . , 2018 ) , aims at learningp(q;);2(A ) , using the information on the entire set of candidates .
The loss function for training such networks is constituted by the
3254contribution of all elements of its ranked items .
The closest work to our research is by Bonadiman and Moschitti ( 2020 ) , who designed several joint models .
These improved early neural networks based on CNN and LSTM for AS2 , but failed to improve the state of the art using pre - trained Transformer models .
2.2 Joint Models in Question Answering MR is a popular QA task that identiﬁes an answer string in a paragraph or a text of limited size for a question .
Its application to retrieval scenario has also been studied ( Chen et al . , 2017 ;
Hu et al . , 2019 ; Kratzwald and Feuerriegel , 2018 ) .
However , the large volume of retrieved content makes their use not practical yet .
Moreover , the joint modeling aspect of MR regards sentences from the same paragraphs .
Jin et al .
( 2020 ) use the relation between candidates in Multi - task learning approach for AS2 .
However , they do not exploit transformer models , thus their results are rather below the state of the art .
In contrast with the work above , our modeling is driven by an answer support strategy , where the pieces of information are taken from different documents .
This makes our model even more unique ; it allows us to design innovative joint models , which are still not designed in any MR systems .
2.3 Fact Veriﬁcation for Question Answering Fact veriﬁcation has become a social need given the massive amount of information generated daily .
The problem is , therefore , becoming increasingly important in NLP context ( Mihaylova et al . , 2018 ) .
In QA , answer veriﬁcation is directly relevant due to its nature of content delivery ( Mihaylova et al . , 2019 ) .
The problem has been explored in MR setting ( Wang et al . , 2018 ) .
Zhang et al .
( 2020a ) also proposed to fact check for product questions using additional associated evidence sentences .
The latter are retrieved based on similarity scores computed with both TF - IDF and sentence - embeddings from pre - trained BERT models .
While the process is technically sound , the retrieval of evidence is an expensive process , which is prohibitive to scale in production .
We instead address this problem by leveraging the top answer candidates .
3 Baseline Models for AS2 In this section , we describe our baseline models , which are constituted by pointwise , pairwise , andlistwise strategies .
3.1 Pointwise Models One simple and effective method to build an answer selector is to use a pre - trained Transformer model , adding a simple classiﬁcation layer to it , and ﬁne - tuning the model on the AS2 task .
Specifically , q = Tokq 1, ... ,Tokq Nandc = Tokc 1, ... ,Tokc M are encoded in the input of the Transformer by delimiting them using three tags :
[ CLS ] , [ SEP ] and [ EOS ] , inserted at the beginning , as separator , and at the end , respectively .
This input is encoded as three embeddings based on tokens , segments and their positions , which are fed as input to several layers ( up to 24 ) .
Each of them contains sublayers for multi - head attention , normalization and feed forward processing .
The result of this transformation is an embedding , E , representing ( q;c ) , which models the dependencies between words and segments of the two sentences .
For the downstream task , Eis fed ( after applying a non - linearity function ) to a fully connected layer having weights : WandB. The output layer can be used to implement the task function .
For example , a softmax can be used to model the probability of the question / candidate pair classiﬁcation , as : p(q;c ) = softmax ( Wtanh(E(q;c ) )
+ B ) .
We can train this model with log cross - entropy loss : L= P l2f0;1gyllog(^yl)on pairs of texts , whereylis the correct and incorrect answer label,^y1 = p(q;c ) , and ^y0= 1 p(q;c ) .
Training the Transformer from scratch requires a large amount of labeled data , but it can be pre - trained using a masked language model , and the next sentence prediction tasks , for which labels can be automatically generated .
Several methods for pretraining Transformer - based language models have been proposed , e.g. , BERT ( Devlin et al . , 2018 ) , RoBERTa ( Liu et al . , 2019 ) , XLNet ( Yang et al . , 2019 ) , AlBERT ( Lan et al . , 2020 ) .
3.2 Our joint model baselines To better show the potential of our approach and the complexity of the task , we designed three joint model baselines based on : ( i ) a multiclassiﬁer approach ( a listwise method ) , and ( ii ) a pairwise joint model operating over k+ 1 candidates , and our adaptation of KGAT model ( a pairwise method ) .
Joint Model Multi
- classiﬁer
The ﬁrst baseline is also a Transformer - based architecture : we concatenate the question with the top k+1answer can-
3255didates , i.e. , ( q[SEP ] c1[SEP ] c2:::[SEP ] ck+1 ) , and provide this input to the same Transformer model used for pointwise reranking .
We use the ﬁnal hidden vector Ecorresponding to the ﬁrst input token
[ CLS ] generated by the Transformer , and a classiﬁcation layer with weights W2R(k+1)jEj , and train the model using a standard cross - entropy classiﬁcation loss : ylog(softmax ( EWT ) ) , whereyis a one - hot vector representing labels for thek+ 1candidates , i.e. ,jyj = k+ 1 .
We use a transformer model ﬁne - tuned with the TANDARoBERTa - base or large models , i.e. , RoBERTa models ﬁne - tuned on ASNQ ( Garg et al . , 2020 ) .
The scores for the candidate answers are calculated as  p(c1);::;p(ck+1) = softmax ( EWT ) .
Then , we rerankciaccording their probability .
Joint Model Pairwise Our second baseline is similar to the ﬁrst .
We concatenate the question with eachcito constitute the ( q;ci)pairs , which are input to the Transformer , and we use the ﬁrst input token
[ CLS ] as the representation of each ( q;ci)pair .
Then , we concatenate the embedding of the pair containing the target candidate , ( q;t)with the embedding of all the other candidates ’
[ CLS ] .
( q;t)is always in the ﬁrst position .
We train the model using a standard classiﬁcation loss .
At classiﬁcation time , we select one target candidate at a time , and set it in the ﬁrst position , followed by all the others .
We classify all k+ 1candidates and use their score for reranking them .
It should be noted that to qualify for a pairwise approach , Joint Model Pairwise should use a ranking loss .
However , we always use standard cross - entropy loss as it is more efﬁcient and the different is performance is negligible .
Joint Model with KGAT Liu et al .
( 2020 ) presented an interesting model , Kernel Graph Attention Network ( KGAT ) , for fact veriﬁcation : given a claimed fact f , and a set of evidencesEv= fev1;ev 2;:::;ev mg , their model carries out joint reasoning over Ev , e.g. , aggregating information to estimate the probability of fto be true or false , p(yjf;Ev ) , wherey2ftrue , falseg .
The approach is based on a fully connected graph , G , whose nodes are the ni= ( f;ev i)pairs , andp(yjf;Ev )
= p(yjf;ev i;Ev)p(evijf;Ev ) , wherep(yjf;ev i;Ev )
= p(yjni;G)is the label probability in each node iconditioned on the whole graph , andp(evijf;Ev )
= p(nijG)is the probability of selecting the most informative evidence .
KGAT uses an edge kernel to perform a hierarchi - cal attention mechanism , which propagates information between nodes and aggregate evidences .
We built a KGAT model for AS2 as follows : we replace ( i)eviwith the set of candidate answers ci , and ( ii ) the claim fwith the question and a target answer pair , ( q;t ) .
KGAT constructs the evidence graph Gby using each claim - evidence pair as a node , which , in our case , is ( ( q;t);ci ) , and connects all node pairs with edges , making it a fully - connected evidence graph .
This way , sentence and token attention operate over the triplets , ( q;t;c i ) , establishing semantic links , which can help to support or undermine the correctness of t.
The original KGAT aggregates all the pieces of information we built , based on their relevance , to determine the probability of t. As we use AS2 data , the probability will be about the correctness oft .
More in detail , we initialize the node representation using the contextual embeddings obtained with two TANDA - RoBERTa - base models1 : the ﬁrst produces the embedding of ( q;t ) , while the second outputs the embedding of ( q;ci ) .
Then , we apply a max - pooling operation on these two to get the ﬁnal node representation .
The rest of the architecture is identical to the original KGAT .
Finally , at test time , we select one ciat a time , as the target t , and compute its probability , which ranks ci .
4 Joint Answer Support Models for AS2 We proposed the Answer Support Reranker ( ASR ) , which uses an answer pair classiﬁer to provide evidence to a target answer t. Given a question q , and a subset of its top- k+1 ranked answer candidates , A(reranked by an AS2 model ) , we build a function , :QCCk!Rsuch that(q;t;Anftg ) provides the probability of tto be correct , where C is the set of sentence - candidates .
We also design a multi - classiﬁer MASR , which combines kASR models , one for each different target answer .
4.1 Answer Support - based Reranker ( ASR ) We developed ASR architecture described in Figure 1c .
This consists of three main components : 1.aPointwise Reranker ( PR ) , which provides the embedding of the input ( q;t ) , described in Figure 1a .
This is essentially the state - of - the - art AS2 model based on the T AND A approach applied to RoBERTa pre - trained transformer .
1https://github.com/alexa/wqa tanda
3256 ( a ) Baseline Reranker using Transformers .
( b ) PairWise Representation using Transformers .
( c ) Answer Support Reranker ( d ) Multi Answer Support Reranker Figure 1 : Multi - Answer Support Reranker and its building blocks .
2.To reduce the noise that may be introduced by irrelevantci , we use the Answer Support Classiﬁer(ASC ) , which classiﬁes each ( t;ci)in one of the following four classes : 0 : tandciare both correct , 1 : tis correct while ciis not , 2 : vice versa , and 3 : both incorrect .
This multi - classiﬁer , described in Figure 1b , is built on top a RoBERTa Transformer , which produced a PairWise Representation ( PWR ) .
ASC is trained end - to - end with the rest of the network in a multi - task learning fashion , using its speciﬁc cross - entropy loss , computed with the labels above .
3.The ASR ( see Figure 1c ) uses the joint representation of ( q;t)with(t;ci),i= 1;::;k , wheretandciare the top - candidates reranked by PR .
Thekrepresentations are summarized by applying a max - pooling operation , which will aggregate all the supporting or not supporting properties of the candidates with respect to the target answer .
The concatenation of the PR embedding with the max - pooling embedding is given as input to the ﬁnal classiﬁcation layer , which scorestwith respect to q , also using the information from the other candidates .
For training and testing , we select a tfrom thek+
1candidates ofqat a time , and compute its score .
This way , we can rerank all the k+ 1candidates with their scores .
Implementation details : ASR is a PR that also exploits the relation between tandAnftg .
We use RoBERTa to generate the [ CLS ] 2Rdembedding of(q;t ) = Et .
We denote with ^Ejthe[CLS ] output by another RoBERTa Transformer applied to answer pairs , i.e. , ( t;cj ) .
Then , we concatenate Etto the max - pooling tensor from ^E1;::;^Ek : V=
[ Et : Maxpool ( [ ^E1;::;^Ek ] ) ] ; ( 1 ) whereV2R2dis the ﬁnal representation of the target answer t.
Then , we use a standard feedforward network to implement a binary classiﬁcation layer : p(yijq;t;C k ) = softmax ( VWT+B ) , whereW2R22dandBare parameters to transform the representation of the target answer tfrom dimension 2dto dimension 2 , which represents correct or incorrect labels .
ASC labels There can be different interpretations when attempting to deﬁne labels for answer pairs .
An alternative to the deﬁnition illustrated above is to use the following FEVER compatible encoding : 0 : tis correct , while cican be any value , as also an incorrect cimay provide important context ( corresponding to FEVER Support label ) ; 1 : tis incorrect , cicorrect , since cican provide evidence that tis not similar to a correct answer ( corresponding to FEVER Refutal label ) ; and 2 : both are incorrect , in this case , nothing can be told ( corresponding to FEVER Neutral label ) .
4.2 Multi - Answer Support Reranker ( MASR ) ASR still selects answers with a pointwise approach2 .
This means that we can improve it by 2Again , using ranking loss did not provide a signiﬁcant improvment .
3257DatasetTrain Dev Test # Q # A+ # A- # Q # A+ # A- # Q # A+ # AWikiQA 873 1,040 7,632 121 140 990 237 293 2,058 TREC - QA 1,229 6,403 47,014 65 205 912 68 248 1,194 WQA 5,000 42,962 163,289 905 8,179 28,096 1,000 8,256 30,123 Table 2 : AS2 dataset statistics building a listwise model , to select the best answer for each question , by utilizing the information from all target answers .
In particular , the architecture of MASR shown in Figure 1d is made up of two parts : ( i ) a list of ASR containing k+ 1ASR blocks , in which each ASR block provides the representation of a target answer t. ( ii ) A ﬁnal multiclassiﬁer and a softmax function , which scores each tfromk+ 1 embedding concatenation and selects the one with highest score .
For training and testing , we select thetfrom thek+ 1 candidates of qbased on a softmax output at a time .
Implementation details : The goal of MASR is to measure the relation between k+ 1target answers , t0 , .. , tk .
The representation of each target answer is the embedding V2R2dfrom
Equation 1 in ASR .
Then , we concatenate the hidden vectors ofk+ 1target answers to form a matrix V(q;k+1)2R(k+1)2d .
We use this matrix and a classiﬁcation layer weights W2R2d , and compute a standard multi - class classiﬁcation loss : LMASR = ylog(softmax ( V(q;k+1)WT);(2 ) whereyis a one - hot - vector , and jyj = jk+ 1j .
5 Experiments In these experiments , we compare our models : KGAT , ASR and MASR with pointwise models , which are the state of the art for AS2 .
We also compare them with our joint model baselines ( pairwise and listwise ) .
Finally , we provide an error analysis .
5.1 Datasets We used two most popular AS2 datasets , and one real world application dataset we built to test the generality of our approach .
WikiQA is a QA dataset ( Yang et al . , 2015 ) containing a sample of questions and answer - sentence candidates from Bing query logs over Wikipedia .
The answers are manually labeled .
We follow the most used setting : training with all the questions that have at least one correct answer , and validating and testing with all the questions having at least one correct and one incorrect answer .
Data split Supported Refuted Not Enough Info Train 80,035 29,775 35,639 Dev 6,666 6,666 6,666 Test 6,666 6,666 6,666 Table 3 : FEVER dataset statistics
TREC - QA is another popular QA benchmark by Wang et al .
( 2007 ) .
We use the same splits of the original data , following the common setting of previous work , e.g. , ( Garg et al . , 2020 ) .
WQA The Web - based Question Answering is a dataset built by Alexa AI as part of the effort to improve understanding and benchmarking in QA systems .
The creation process includes the following steps : ( i ) given a set of questions we collected from the web , a search engine is used to retrieve up to 1,000 web pages from an index containing hundreds of millions pages .
( ii ) From the set of retrieved documents , all candidate sentences are extracted and ranked using AS2 models from ( Garg et al . , 2020 ) .
Finally , ( iii ) top candidates for each question are manually assessed as correct or incorrect by human judges .
This allowed us to obtain a richer variety of answers from multiple sources with a higher average number of answers .
Table 2 reports the corpus statistics of WikiQA , TREC - QA , and WQA3 .
FEVER is a large - scale public corpus , proposed by Thorne et
al . ( 2018a ) for fact veriﬁcation task , consisting of 185,455 annotated claims from 5,416,537 documents from the Wikipedia dump in June 2017 .
All claims are labelled as Supported , Refuted or Not Enough Info by annotators .
Table 3 shows the statistics of the dataset , which remains the same as in ( Thorne et al . , 2018b ) .
5.2 Training and testing details Metrics The performance of QA systems is typically measured with Accuracy in providing correct answers , i.e. , the percentage of correct responses .
This is also referred to Precision - at-1 ( P@1 ) in the context of reranking , while standard Precision and Recall are not essential in our case as we assume the system does not abstain from providing answers .
We also use Mean Average Precision ( MAP ) and Mean Reciprocal Recall ( MRR ) evaluated on the test set , using the entire set of candidates for each 3The public version of WQA will be released in the short - term future .
Please search for a publication with titleWQA : A Dataset for Web - based Question Answering Tasks on arXiv.org .
3258RoBERTa Base WikiQA TREC - QA WQA P@1 MAP MRR P@1 MAP MRR P@1 MAP MRR Reranker by Garg et
al . , 2020 – 0.8890 0.9010 – 0.9140 0.9520 – – – Our Reranker 0.8189y0.8860 0.8983 0.9118 0.9043 0.9498 – – – Joint Model Multi - classiﬁer ( k=5 ) 0.7819y0.8542 0.8684 0.8971 0.9052 0.9424 -2.29y% -1.00 % -1.23 % Joint Model Pairwise ( k=3 ) 0.8272y0.8927 0.9045 0.9559 0.9196 0.9743 2.67y% 0.39 % 1.39 % KGAT ( k=2 ) 0.8436 0.8991 0.9120 0.9412 0.9155 0.9645 2.10 % 0.39 % 0.93 % ASR ( k=3)y0.8436 0.9014 0.9123 0.9706 0.9257 0.9816y2.86 % 0.86 % 1.39 % MASR ( k=3 )
0.8230 0.8891
0.9017 0.9265 0.9200 0.9632 3.82 % 0.70 % 1.67 % MASR - F ( k=3 ) 0.8272 0.8918 0.9031 0.9412 0.9222 0.9706 2.67 % 0.55 % 1.47 % MASR - FP ( k=3 ) 0.8436 0.8998 0.9113 0.9559 0.9191 0.9743 4.96 % 0.94 % 2.43 % Table 4 : Results on WikiQA , TREC - QA and WQA , using RoBERTa base Transformer.yis used to indicate that the difference in P@1 between ASR and the other marked systems is statistically signiﬁcant at 95 % .
JOINT -MULTI CLASSIFER JOINT -PAIR KGAT ASR 1 2 3 4 5 3 2 1012345 kImprovement ( % ) Figure 2 : Impact of kon the WQA dev .
set question ( this varies according to the dataset ) , to have a direct comparison with the state of the art .
Models We use the pre - trained RoBERTa - Base ( 12 layer ) and RoBERTa - Large - MNLI ( 24 layer ) models , which were released as checkpoints for use in downstream tasks4 .
Reranker training We adopt Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 2e-5 for the transfer step on the ASNQ dataset ( Garg et al . , 2020 ) , and a learning rate of 1e-6 for the adapt step on the target dataset .
We apply early stopping on the development set of the target corpus for both ﬁne - tuning steps based on the highest MAP score .
We set the max number of epochs equal to 3 and 9 for the adapt and transfer steps , respectively .
We set the maximum sequence length for RoBERTa to 128 tokens .
KGAT and ASR training Again , we use the Adam optimizer with a learning rate of 2e-6 for training the ASR model on the target dataset .
We utilize 1 Tesla V100 GPU with 32 GB memory and a train batch size of eight .
We set the maximum sequence length for RoBERTa Base / Large to 130 tokens and the number of training epochs to 20 .
The other training conﬁgurations are the same of the original KGAT model from ( Liu et al . , 2020 ) .
We use two transformer models for ASR : a RoBERTa 4https://github.com/pytorch/fairseqBase/Large for PR , and one for ASC .
We set the maximum sequence length for RoBERTa to 128 tokens and the number of epochs to 20 .
MASR training We use the same conﬁguration of the ASR training , including the optimizer type , learning rate , the number of epochs , GPU type , maximum sequence length , etc .
Additionally , we design two different models MASR - F , using an ASC classiﬁer targeting the FEVER labels , and MASR - FP , which initializes ASC with the data from FEVER .
This is possible as the labels are compatible .
5.3 Choosing the best k
The selection of the hyper - parameter k , i.e. , the number of candidates to consider for supporting a target answer is rather tricky .
Indeed , the standard validation set is typically used for tuning PR .
This means that the candidates PR moves to the top k+1 positions are optimistically accurate .
Thus , when selecting also the optimal kon the same validation set , there is high risk to overﬁt the model .
We solved this problem by running a PR version not heavily optimized on the dev .
set , i.e. , we randomly choose a checkpoint after the standard three epochs of ﬁne - tuning of RoBERTa transformer .
Additionally , we tuned konly using the WQA dev .
set , which contains36;000Q / A pairs .
WikiQA and TREC - QA dev .
sets are too small to be used ( 121 and 65 questions , respectively ) .
Fig .
2 plots the improvement of four different models , Joint Model Multi - classiﬁer , Joint Model Pairwise , KGAT , and ASR , when using different kvalues .
Their best results are reached for 5 , 3 , 2 , and 3 , respectively .
We note that the most reliable curve shape ( convex ) is the one of ASR and Joint Model Pairwise .
32595.4 Comparative Results Table 4 reports the P@1 , MAP and MRR of the rerankers , and different answer supporting models on WikiQA , TREC - QA and WQA datasets .
As WQA is an internal dataset , we only report the improvement over PR in the tables .
All models use RoBERTa - Base pre - trained checkpoint and start from the same set of kcandidates reranked by PR ( state - of - the - art model ) .
The table shows that : •PR replicates the MAP and MRR of the stateof - the - art reranker by Garg et al .
( 2020 ) on WikiQA .
•Joint Model Multi - classiﬁer performs lower than PR for all measures and all datasets .
This is in line with the ﬁndings of Bonadiman and Moschitti ( 2020 ) , who also did not obtain improvement when jointly used all the candidates altogether in a representation .
•Joint Model Pairwise differs from ASR as it concatenates the embeddings of the ( q;ci ) , instead of using max - pooling , and does not use any Answer Support Classiﬁer ( ASC ) .
Still , it exploits the idea of aggregating the information of all pairs ( q;ci)with respect to a target answer t , which proves to be effective , as the model improves on PR over all measures and datasets .
•Our KGAT version for AS2 also improves PR over all datasets and almost all measures , conﬁrming that the idea of using candidates as support of the target answer is generally valid .
However , it is not superior to Joint Model Pairwise .
•ASR achieves the highest performance among all models ( but MASR - FP on WQA ) , all datasets , and all measures .
For example , it outperforms PR by almost 3 absolute percent points in P@1 on WikiQA , and by almost 6 points on TREC from 91.18 % to 97.06 % , which corresponds to an error reduction of 60 % .
•MASR and MASR - F do not achieve better performance than Joint Model Pairwise on WikiQA and TREC , although MASR outperforms all baselines and even ASR on WQA .
This suggests that the signiﬁcantly higher number of parameters of MASR can not be trained on small corpus , while WQA has a sufﬁcient number of examples .
RoBERTa LargeWikiQA TREC - QA P@1 MAP MRR P@1 MAP MRR Garg et al . , – 0.9200 0.9330 – 0.9430 0.9740 Our Reranker 0.8724 0.9151 0.9266 0.9706 0.9481 0.9816 KGAT ( K=2 )
0.8642 0.9094 0.9218 0.9559 0.9407 0.9743 ASR ( K=3 ) 0.8971 0.9280 0.9399 0.9706 0.9488 0.9816 Table 5 : Results on WikiQA and TREC - QA , using RoBERTa Large Transformer .
•MASR - FP exploiting FEVER for the initialization of ASC performs better than MASR and MASR - F on WikiQA and TREC .
Interestingly , it signiﬁcantly outperforms ASR by 2 % on WQA .
This conﬁrms the potential of the model when enough training data is available .
•We perform randomization test ( Yeh , 2000 ) to verify if the models signiﬁcantly differ in terms of prediction outcome .
We use 100,000 trials for each calculation .
The results conﬁrm the statistically signiﬁcant difference between ASR and all the baselines , with p < 0.05 for WikiQA , and between ASR and all models ( i.e. , including also KGAT ) on WQA .
5.5 Ofﬁcial State of the art As the state of the art for AS2 is obtained using RoBERTa Large , we trained KGAT and ASR using this pre - trained language model .
Table 5 also reports the comparison with PR , which is the ofﬁcial state of the art .
Again , our PR replicates the results of Garg et al .
( 2020 ) , obtaining slightly lower performance on WikiQA but higher on TREC - QA .
KGAT performs lower than PR on both datasets .
ASR establishes the new state of the art on WikiQA with an MAP of 92.80 vs. 92.00 .
The P@1 also signiﬁcantly improves by 2 % , i.e. , achieving 89.71 , which is impressively high .
Also , on TRECQA , ASR outperforms all models , being on par with PR regarding P@1 .
The latter is 97.06 , which corresponds to mistaking the answers of only two questions .
We manually checked these and found out that these were two annotation errors : ASR achieves perfect accuracy while PR only mistakes one answer .
Of course , this just provides evidence that PR based on RoBERTa - Large solves the task of selecting the best answers ( i.e. , measuring P@1 on this dataset is not meaningful anymore ) .
5.6 Model Discussion Table 6 reports the accuracy of ASC inside to different models .
In ASR , it uses 4 - way categories , while in MASR - based models , it uses the three
3260WikiQA TREC - QA WQA ACC F1 ACC F1 ACC F1 ASR 0.59 0.00 0.56 0.80 0.58 0.64 MASR 0.46 0.00 0.45 0.62 0.53 0.61 MASR - F 0.46 0.00 0.64 0.78 0.58 0.68 MASR - FP 0.49 0.37 0.65 0.73 0.59 0.69 Table 6 : The Accuracy and F1 of category 0 for ASC FEVER labels ( see Sec . 4.1 ) .
ACC is the overall accuracy while F1 refers to the category 0 .
We note that ASC in MASR - FP achieves the highest accuracy with respect to the average over all datasets .
This happens since we pre-ﬁne - tuned it with the FEVER data .
We analyzed examples for which ASR is correct and PR is not .
Tab .
7 shows that , given qandk= 3 candidates , PR chooses c1 , a suitable but wrong answer .
This probably happens since the answer best matches the syntactic / semantic pattern of the question , which asks for a type of color , indeed , the answer offers such type , primary colors .
PR does not rely on any background information that can support the set of colors in the answer .
In contrast , ASR selects c2as it can rely on the support of other answers .
Its ASC provides an average score for the category 0(both members are correct ) of c2 , i.e. ,1 kP i6=2ASC(c2;ci ) = 0:653 , while forc1the average score is signiﬁcant lower , i.e. , 0.522 .
This provides higher support for c2 , which is used by ASR to rerank the output of PR .
Tab . 8 shows an interesting case where all the sentences contain the required information , i.e. , February .
However , PR and ASR both choose answerc0 , which is correct but not natural , as it provides the requested information indirectly .
Also , it contains a lot of ancillary information .
In contrast , MASR is able to rerank the best answer , c1 , in the top position .
6 Conclusion We have proposed new joint models for AS2 .
ASR encodes the relation between the target answer and all the other candidates , using an additional Transformer model , and an Answer Support Classiﬁer , while MASR jointly models the ASR representations for all target answers .
We extensively tested KGAT , ASR , MASR , and other joint model baselines we designed .
The results show that our models can outperform the state of the art .
Most interestingly , ASR constantly outperforms all the models ( but MASR - FP ) , on all datasets , through all measures , and for both base and large transformers .
For example , ASRq : What kind of colors are in the rainbow ?
c1 :
Red , yellow , and blue are called the primary colors .
c2 : The order of the colors in the rainbow goes : red , orange , yellow , green , blue , indigo and violet .
c3 : The colors in all rainbows are present in the same order : red , orange , yellow , green , blue , indigo , and violet .
c4 : A rainbow occurs when white light bends and separates into red , orange , yellow , green blue , indigo and violet .
Table 7 : A question with answer candidates ranked by PR ; ASR chose c2 .
q : What ’s the month of Valentine ’s day ?
c0 : Celebrated on February 14 every year , saint Valentine ’s day or Valentine ’s day is the traditional day on which lovers convey their love to each other by sending Valentine ’s cards , sometimes even anonymously .
c1 : February is historically chosen to be the month of love and romance and the month to celebrate Valentine ’s day .
c2 : In order for today to be Valentine ’s day , it ’s necessary that today is in the month of February .
c3 : Every year , Valentine ’s day is celebrated on February 14 in many countries around the world .
Table 8 : A question with answer candidates fc0;c1;c2;c3granked by PR ; ASR reranks as fc0;c3;c2;c1 g ; and MASR reranks as fc1;c3;c0;c2 g ; c1is the natural correct answer .
achieves the best reported results , i.e. , MAP values of 92.80 % and 94.88 , on WikiQA and TRECQA , respectively .
MASR improves ASR by 2 % on WQA , since this contains enough data to train the ASR representations jointly .
References Qingyao Ai , Keping Bi , Jiafeng Guo , and W. Bruce Croft .
2018 .
Learning a deep listwise context model for ranking reﬁnement .
CoRR , abs/1804.05936 .
Weijie Bian , Si Li , Zhao Yang , Guang Chen , and Zhiqing Lin . 2017 .
A compare - aggregate model with dynamic - clip attention for answer selection .
In CIKM , pages 1987–1990 . ACM .
Daniele Bonadiman and Alessandro Moschitti . 2020 .
A study on efﬁciency , accuracy and document structure for answer sentence selection .
CoRR , abs/2003.02349 .
Zhe Cao , Tao Qin , Tie - Yan Liu , Ming - Feng Tsai , and Hang Li . 2007 .
Learning to rank : from pairwise approach to listwise approach .
In Proceedings of the 24th international conference on Machine learning , ICML ’ 07 , pages 129–136 , New York , NY , USA . ACM .
Danqi Chen , Adam Fisch , Jason Weston , and Antoine Bordes . 2017 .
Reading wikipedia to answer opendomain questions .
CoRR , abs/1704.00051 .
Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .
BERT : pre - training of
3261deep bidirectional transformers for language understanding .
CoRR , abs/1810.04805 .
Siddhant Garg , Thuy Vu , and Alessandro Moschitti .
2020 .
TANDA : transfer and adapt pre - trained transformer models for answer sentence selection .
In The Thirty - Fourth AAAI Conference on Artiﬁcial Intelligence , AAAI 2020 , The Thirty - Second Innovative Applications of Artiﬁcial Intelligence Conference , IAAI 2020 , The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence , EAAI 2020 , New York , NY , USA , February 7 - 12 , 2020 , pages 7780 – 7788 .
AAAI Press .
Minghao Hu , Yuxing Peng , Zhen Huang , and Dongsheng Li . 2019 .
Retrieve , read , rerank : Towards end - to - end multi - document reading comprehension .
CoRR , abs/1906.04618 .
Zan - Xia Jin , Bo - Wen Zhang , Fang Zhou , Jingyan Qin , and Xu - Cheng Yin .
2020 .
Ranking via partial ordering for answer selection .
Information Sciences .
Diederik P. Kingma and Jimmy Ba . 2014 .
Adam : A method for stochastic optimization .
CoRR , abs/1412.6980 .
Bernhard Kratzwald and Stefan Feuerriegel .
2018 .
Adaptive document retrieval for deep question answering .
In EMNLP’18 , pages 576–581 .
Zhenzhong Lan , Mingda Chen , Sebastian Goodman , Kevin Gimpel , Piyush Sharma , and Radu Soricut .
2020 .
Albert :
A lite bert for self - supervised learning of language representations .
In ICLR 2020 .
Md Tahmid Rahman Laskar , Jimmy Xiangji Huang , and Enamul Hoque .
2020 .
Contextualized embeddings based transformer encoder for sentence similarity modeling in answer selection task .
In Proceedings of The 12th Language Resources and Evaluation Conference , pages 5505–5514 , Marseille , France .
European Language Resources Association .
Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .
2019 .
Roberta : A robustly optimized BERT pretraining approach .
CoRR , abs/1907.11692 .
Zhenghao Liu , Chenyan Xiong , Maosong Sun , and Zhiyuan Liu . 2020 .
Fine - grained fact veriﬁcation with kernel graph attention network .
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 7342–7351 , Online .
Association for Computational Linguistics .
Tsvetomila Mihaylova , Georgi Karadzhov , Pepa Atanasova , Ramy Baly , Mitra Mohtarami , and Preslav Nakov .
2019 .
SemEval-2019 task 8 : Fact checking in community question answering forums .
InProceedings of the 13th International Workshop on Semantic Evaluation , pages 860–869 , Minneapolis , Minnesota , USA . Association for Computational Linguistics .
Tsvetomila Mihaylova , Preslav Nakov , Llu ´ ıs M`arquez , Alberto Barr ´ on - Cede ˜no , Mitra Mohtarami , Georgi Karadzhov , and James R. Glass .
2018 .
Fact checking in community forums .
AAAI-2018 .
Jinfeng Rao , Hua He , and Jimmy J. Lin . 2016 .
Noisecontrastive estimation for answer selection with deep neural networks .
In CIKM , pages 1913–1916 .
ACM .
Aliaksei Severyn and Alessandro Moschitti . 2015 .
Learning to rank short text pairs with convolutional deep neural networks .
In SIGIR’15 .
Gehui Shen , Yunlun Yang , and Zhi - Hong Deng . 2017 .
Inter - weighted alignment network for sentence pair modeling .
In EMNLP’17 , pages 1179–1189 , Copenhagen , Denmark .
Harish Tayyar Madabushi , Mark Lee , and John Barnden .
2018 .
Integrating question classiﬁcation and deep learning for improved answer selection .
In COLING’18 , pages 3283–3294 .
James Thorne , Andreas Vlachos , Christos Christodoulopoulos , and Arpit Mittal .
2018a .
FEVER : a large - scale dataset for fact extraction and VERiﬁcation .
In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long Papers ) , pages 809–819 , New Orleans , Louisiana .
Association for Computational Linguistics .
James Thorne , Andreas Vlachos , Oana Cocarascu , Christos Christodoulopoulos , and Arpit Mittal .
2018b .
The fact extraction and VERiﬁcation ( FEVER ) shared task .
In Proceedings of the First Workshop on Fact Extraction and VERiﬁcation ( FEVER ) , pages 1–9 , Brussels , Belgium .
Association for Computational Linguistics .
E. V oorhees and D. Tice .
1999 .
The TREC-8 Question Answering Track Evaluation , pages 77–82 . Department of Commerce , National Institute of Standards and Technology .
Mengqiu Wang , Noah A. Smith , and Teruko Mitamura .
2007 .
What is the Jeopardy model ?
a quasi - synchronous grammar for QA .
In EMNLPCoNLL’07 , pages 22–32 , Prague , Czech Republic .
Association for Computational Linguistics .
Yizhong Wang , Kai Liu , Jing Liu , Wei He , Yajuan Lyu , Hua Wu , Sujian Li , and Haifeng Wang .
2018 .
Multipassage machine reading comprehension with crosspassage answer veriﬁcation .
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1918–1927 , Melbourne , Australia .
Association for Computational Linguistics .
Yi Yang , Wen - tau Yih , and Christopher Meek . 2015 .
Wikiqa : A challenge dataset for open - domain question answering .
In Proceedings of the 2015 conference on empirical methods in natural language processing , pages 2013–2018 .
3262Zhilin Yang , Zihang Dai , Yiming Yang , Jaime G. Carbonell , Ruslan Salakhutdinov , and Quoc V .
Le . 2019 .
Xlnet :
Generalized autoregressive pretraining for language understanding .
CoRR , abs/1906.08237 .
Alexander S. Yeh .
2000 .
More accurate tests for the statistical signiﬁcance of result differences .
CoRR , cs . CL/0008005 .
Seunghyun Yoon , Franck Dernoncourt , Doo Soon Kim , Trung Bui , and Kyomin Jung .
2019 .
A compareaggregate model with latent clustering for answer selection .
CoRR , abs/1905.12897 .
Wenxuan Zhang , Yang Deng , Jing Ma , and Wai Lam . 2020a .
AnswerFact :
Fact checking in product question answering .
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 2407–2417 , Online .
Association for Computational Linguistics .
Yingxue Zhang , Fandong Meng , Peng Li , Ping Jian , and Jie Zhou .
2020b .
Ms - ranker : Accumulating evidence from potentially correct candidates for answer selection .
CoRR , abs/2010.04970 .
