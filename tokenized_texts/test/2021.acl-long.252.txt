Joint Models for Answer Veriﬁcation in Question Answering Systems
Zeyu Zhang∗
Thuy Vu Alessandro Moschitti School of Information , The University of Arizona , Tucson , AZ , USA Amazon Alexa AI , Manhattan Beach , CA , USA zeyuzhang@email.arizona.edu , { thuyvu , amosch}@amazon.com
Abstract
This paper studies joint models for selecting correct answer sentences among the top k provided by answer sentence selection ( AS2 ) modules , which are core components of retrievalbased Question Answering ( QA ) systems .
Our work shows that a critical step to effectively exploiting an answer set regards modeling the interrelated information between pair of answers .
For this purpose , we build a three - way multiclassiﬁer , which decides if an answer supports , refutes , or is neutral with respect to another one .
More speciﬁcally , our neural architecture integrates a state - of - the - art AS2 module with the multi - classiﬁer , and a joint layer connecting all components .
We tested our models on WikiQA , TREC - QA , and a real - world dataset .
The results show that our models obtain the new state of the art in AS2 .
1
Introduction
Automated Question Answering ( QA ) research has received a renewed attention thanks to the diffusion of Virtual Assistants .
Among the different types of methods to implement QA systems , we focus on Answer Sentence Selection ( AS2 ) research , originated from TREC - QA track ( Voorhees and Tice , 1999 ) , as it proposes efﬁcient models that are more suitable for a production setting , e.g. , they are more efﬁcient than those developed in machine reading ( MR ) work ( Chen et al , 2017 ) .
Garg et al ( 2020 ) proposed the TANDA approach based on pre - trained Transformer models , obtaining impressive improvement over the state of the art for AS2 , measured on the two most used datasets , WikiQA ( Yang et al , 2015 ) and TRECQA ( Wang et al , 2007 ) .
However , TANDA was applied only to pointwise rerankers ( PR ) , e.g. , simple binary classiﬁers .
Bonadiman and Moschitti
∗Work done while the author was an intern at Amazon
Alexa
Claim : Ev1 :
Ev2 :
Ev3 :
Joe Walsh was inducted in 2001 .
As a member of the Eagles , Walsh was inducted into the Rock and Roll Hall of Fame in 1998 , and into the Vocal Group Hall of Fame in 2001 .
Joseph Fidler Walsh ( born November 20 , 1947 ) is an American singer songwriter , composer , multiinstrumentalist and record producer .
Walsh was awarded with the Vocal Group Hall of Fame in 2001 .
Table 1 : A claim veriﬁcation example from FEVER .
( 2020 ) tried to improve this model by jointly modeling all answer candidates with listwise methods , e.g. , ( Bian et al , 2017 ) .
Unfortunately , merging the embeddings from all candidates with standard approaches , e.g. , CNN or LSTM , did not improve over TANDA .
A more structured approach to building joint models over sentences can instead be observed in Fact Veriﬁcation Systems , e.g. , the methods developed in the FEVER challenge ( Thorne et al , 2018a ) .
Such systems take a claim , e.g. , Joe Walsh was inducted in 2001 , as input ( see Tab . 1 ) , and verify if it is valid , using related sentences called evidences ( typically retrieved by a search engine ) .
For example , Ev1 , As a member of the Eagles , Walsh was inducted into the Rock and Roll Hall of Fame in 1998 , and into the Vocal Group Hall of Fame in 2001 , and Ev3 , Walsh was awarded with the Vocal Group Hall of Fame in 2001 , support the veracity of the claim .
In contrast , Ev2 is neutral as it describes who Joe Walsh is but does not contribute to establish the induction .
We conjecture that supporting evidence for answer correctness in AS2 task can be modeled with a similar rationale .
In this paper , we design joint models for AS2 based on the assumption that , given q and a target answer candidate t , the other answer candidates , ( c1 , .. ck ) can provide positive , negative , or neutral support to decide the correctness of t. Our ﬁrst approach exploits Fact Checking research : we adapted a state - of - the - art FEVER system , KGAT ( Liu et al , 2020 ) , for AS2 .
We deﬁned a claim as
Proceedingsofthe59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing , pages3252–3262August1–6,2021. © 2021AssociationforComputationalLinguistics3252  a pair constituted of the question and one target answer , while considering all the other answers as evidences .
We re - trained and rebuilt all its embeddings for the AS2 task .
Our second method , Answer Support - based Reranker ( ASR ) , is completely new , it is based on the representation of the pair , ( q , t ) , generated by state - of - the - art AS2 models , concatenated with the representation of all the pairs ( t , ci ) .
The latter summarizes the contribution of each ci to t using a maxpooling operation .
ci can be unrelated to ( q , t ) since the candidates are automatically retrieved , thus it may introduce just noise .
To mitigate this problem , we use an Answer Support Classiﬁer ( ASC ) to learn the relatedness between t and ci by classifying their embedding , which we obtain by applying a transformer network to their concatenated text .
ASC tunes the ( t , ci ) embedding parameters according to the evidence that ci provides to t. Our Answer Support - based Reranker ( ASR ) signiﬁcantly improves the state of the art , and is also simpler than our approach based on KGAT .
Our third method is an extension of ASR .
It should be noted that , although ASR exploits the information from the k candidates , it still produces a score for a target t without knowing the scores produced for the other target answers .
Thus , we jointly model the representation obtained for each target in a multi - ASR ( MASR ) architecture , which can then carry out a complete global reasoning over all target answers .
We experimented with our models over three datasets , WikiQA , TREC - QA and WQA , where the latter is an internal dataset built on anonymized customer questions .
The results show that :
• ASR improves the best current model for AS2 , i.e. , TANDA by ∼3 % , corresponding to an error reduction of 10 % in Accuracy , on both WikiQA and TREC - QA .
• We also obtain a relative improvement of ∼3 % over TANDA on WQA , conﬁrming that ASR is a general solution to design accurate QA systems .
• Most interestingly , MASR improves ASR by additional 2 % , conﬁrming the beneﬁt of joint modeling .
Finally , it is interesting to mention that MASR improvement is also due to the use of FEVER data for
pre-ﬁne - tuning ASC , suggesting that the fact veriﬁcation inference and the answer support inference are similar .
2 Problem deﬁnition and related work We consider retrieval - based QA systems , which are mainly constituted by ( i ) a search engine , retrieving documents related to the questions ; and ( ii ) an AS2 model , which reranks passages / sentences extracted from the documents .
The top sentence is typically used as ﬁnal answer for the users .
2.1 Answer Sentence Selection ( AS2 )
The task of reranking answer - sentence candidates provided by a retrieval engine can be modeled with a classiﬁer scoring the candidates .
Let q be an element of the question set , Q , and A = { c1 , . . .
, cn } be a set of candidates for q , a reranker can be deﬁned as R : Q × Π(A ) → Π(A ) , where Π(A ) is the set of all permutations of A. Previous work targeting ranking problems in the text domain has classiﬁed reranking functions into three buckets : pointwise , pairwise , and listwise methods .
Pointwise reranking : This approach learns p(q , ci ) , which is the probability of ci correctly answering q , using a standard binary classiﬁcation setting .
The ﬁnal rank is simply obtained sorting ci , based on p(q , ci ) .
Previous work estimates p(q , ci ) with neural models ( Severyn and Moschitti , 2015 ) , also using attention mechanisms , e.g. , CompareAggregate ( Yoon et al , 2019 ) , inter - weighted alignment networks ( Shen et al , 2017 ) , and pre - trained Transformer models , which are the state of the art .
Garg et al ( 2020 ) proposed TANDA , which is the current most accurate model on WikiQA and TREC - QA .
Pairwise reranking : The method considers binary classiﬁers of the form χ(q , ci , cj ) for determining the partial rank between ci and cj , then the scoring function p(q , ci ) is obtained by summing up all the contributions with respect to the target candidate t = ci , e.g. , p(q , ci ) = ( cid:80 ) j χ(q , ci , cj ) .
There has been a large body of work preceding Transformer models , e.g. , ( Laskar et al , 2020 ; Tayyar Madabushi et al , 2018 ;
Rao et
al , 2016 ) .
However , these methods are largely outperformed by the pointwise TANDA model .
Listwise reranking : This approach , e.g. , ( Bian et al , 2017 ; Cao et al , 2007 ; Ai et al , 2018 ) , aims at learning p(q , π ) , π ∈ Π(A ) , using the information on the entire set of candidates .
The loss function for training such networks is constituted by the
3253  contribution of all elements of its ranked items .
The closest work to our research is by Bonadiman and Moschitti ( 2020 ) , who designed several joint models .
These improved early neural networks based on CNN and LSTM for AS2 , but failed to improve the state of the art using pre - trained Transformer models .
2.2
Joint Models in Question Answering
MR is a popular QA task that identiﬁes an answer string in a paragraph or a text of limited size for a question .
Its application to retrieval scenario has also been studied ( Chen et al , 2017 ; Hu et al , 2019 ; Kratzwald and Feuerriegel , 2018 ) .
However , the large volume of retrieved content makes their use not practical yet .
Moreover , the joint modeling aspect of MR regards sentences from the same paragraphs .
Jin et al ( 2020 ) use the relation between candidates in Multi - task learning approach for AS2 .
However , they do not exploit transformer models , thus their results are rather below the state of the art .
In contrast with the work above , our modeling is driven by an answer support strategy , where the pieces of information are taken from different documents .
This makes our model even more unique ; it allows us to design innovative joint models , which are still not designed in any MR systems .
2.3 Fact Veriﬁcation for Question Answering
Fact veriﬁcation has become a social need given the massive amount of information generated daily .
The problem is , therefore , becoming increasingly important in NLP context ( Mihaylova et al , 2018 ) .
In QA , answer veriﬁcation is directly relevant due to its nature of content delivery ( Mihaylova et al , 2019 ) .
The problem has been explored in MR setting ( Wang et al , 2018 ) .
Zhang et al ( 2020a ) also proposed to fact check for product questions using additional associated evidence sentences .
The latter are retrieved based on similarity scores computed with both TF - IDF and sentence - embeddings from pre - trained BERT models .
While the process is technically sound , the retrieval of evidence is an expensive process , which is prohibitive to scale in production .
We instead address this problem by leveraging the top answer candidates .
3 Baseline Models for AS2
In this section , we describe our baseline models , which are constituted by pointwise , pairwise , and
listwise strategies .
3.1 Pointwise Models
1, ... ,Tokq
N and c = Tokc
One simple and effective method to build an answer selector is to use a pre - trained Transformer model , adding a simple classiﬁcation layer to it , and ﬁne - tuning the model on the AS2 task .
Specifically , q = Tokq 1, ... ,Tokc M are encoded in the input of the Transformer by delimiting them using three tags :
[ CLS ] , [ SEP ] and [ EOS ] , inserted at the beginning , as separator , and at the end , respectively .
This input is encoded as three embeddings based on tokens , segments and their positions , which are fed as input to several layers ( up to 24 ) .
Each of them contains sublayers for multi - head attention , normalization and feed forward processing .
The result of this transformation is an embedding , E , representing ( q , c ) , which models the dependencies between words and segments of the two sentences .
For the downstream task , E is fed ( after applying a non - linearity function ) to a fully connected layer having weights : W and B.
The output layer can be used to implement the task function .
For example , a softmax can be used to model the probability of the question / candidate pair classiﬁcation , as : p(q , c ) = sof tmax(W × tanh(E(q , c ) ) + B ) .
We can train this model with log cross - entropy loss : L = − ( cid:80 ) l∈{0,1 } yl × log(ˆyl ) on pairs of texts , where yl is the correct and incorrect answer label , ˆy1 = p(q , c ) , and
ˆy0 = 1 − p(q , c ) .
Training the Transformer from scratch requires a large amount of labeled data , but it can be pre - trained using a masked language model , and the next sentence prediction tasks , for which labels can be automatically generated .
Several methods for pretraining Transformer - based language models have been proposed , e.g. , BERT ( Devlin et al , 2018 ) , RoBERTa ( Liu et al , 2019 ) , XLNet ( Yang et al , 2019 ) , AlBERT ( Lan et al , 2020 ) .
3.2 Our joint model baselines
To better show the potential of our approach and the complexity of the task , we designed three joint model baselines based on : ( i ) a multiclassiﬁer approach ( a listwise method ) , and ( ii ) a pairwise joint model operating over k + 1 candidates , and our adaptation of KGAT model ( a pairwise method ) .
Joint Model Multi
- classiﬁer
The ﬁrst baseline is also a Transformer - based architecture : we concatenate the question with the top k + 1 answer can3254  didates , i.e. , ( q[SEP ] c1[SEP ] c2 . . .
[ SEP ] ck+1 ) , and provide this input to the same Transformer model used for pointwise reranking .
We use the ﬁnal hidden vector E corresponding to the ﬁrst input token
[ CLS ] generated by the Transformer , and a classiﬁcation layer with weights W ∈ R(k+1)×|E| , and train the model using a standard cross - entropy classiﬁcation loss : y × log(sof tmax(EW T ) ) , where y is a one - hot vector representing labels for the k + 1 candidates , i.e. , |y| = k
+ 1 .
We use a transformer model ﬁne - tuned with the TANDARoBERTa - base or large models , i.e. , RoBERTa models ﬁne - tuned on ASNQ ( Garg et al , 2020 ) .
The scores for the candidate answers are calculated as ( cid:0)p(c1 ) , .. , p(ck+1)(cid:1 )
= sof tmax(EW T ) .
Then , we rerank ci according their probability .
Joint Model Pairwise Our second baseline is similar to the ﬁrst .
We concatenate the question with each ci to constitute the ( q , ci ) pairs , which are input to the Transformer , and we use the ﬁrst input token
[ CLS ] as the representation of each ( q , ci ) pair .
Then , we concatenate the embedding of the pair containing the target candidate , ( q , t ) with the embedding of all the other candidates ’
[ CLS ] .
( q , t ) is always in the ﬁrst position .
We train the model using a standard classiﬁcation loss .
At classiﬁcation time , we select one target candidate at a time , and set it in the ﬁrst position , followed by all the others .
We classify all k + 1 candidates and use their score for reranking them .
It should be noted that to qualify for a pairwise approach , Joint Model Pairwise should use a ranking loss .
However , we always use standard cross - entropy loss as it is more efﬁcient and the different is performance is negligible .
Joint Model with KGAT Liu et al ( 2020 ) presented an interesting model , Kernel Graph Attention Network ( KGAT ) , for fact veriﬁcation : given a claimed fact f , and a set of evidences Ev = { ev1 , ev2 , . . .
, evm } , their model carries out joint reasoning over Ev , e.g. , aggregating information to estimate the probability of f to be true or false , p(y|f , Ev ) , where y ∈{true , false } .
The approach is based on a fully connected graph , G , whose nodes are the ni = ( f , evi ) pairs , and p(y|f , Ev ) = p(y|f , evi , Ev)p(evi|f , Ev ) , where p(y|f , evi , Ev ) = p(y|ni , G ) is the label probability in each node i conditioned on the whole graph , and p(evi|f , Ev ) = p(ni|G ) is the probability of selecting the most informative evidence .
KGAT uses an edge kernel to perform a hierarchical attention mechanism , which propagates information between nodes and aggregate evidences .
We built a KGAT model for AS2 as follows : we replace ( i ) evi with the set of candidate answers ci , and ( ii ) the claim f with the question and a target answer pair , ( q , t ) .
KGAT constructs the evidence graph G by using each claim - evidence pair as a node , which , in our case , is ( ( q , t ) , ci ) , and connects all node pairs with edges , making it a fully - connected evidence graph .
This way , sentence and token attention operate over the triplets , ( q , t , ci ) , establishing semantic links , which can help to support or undermine the correctness of t.
The original KGAT aggregates all the pieces of information we built , based on their relevance , to determine the probability of t. As we use AS2 data , the probability will be about the correctness of t.
More in detail , we initialize the node representation using the contextual embeddings obtained with two TANDA - RoBERTa - base models 1 : the ﬁrst produces the embedding of ( q , t ) , while the second outputs the embedding of ( q , ci ) .
Then , we apply a max - pooling operation on these two to get the ﬁnal node representation .
The rest of the architecture is identical to the original KGAT .
Finally , at test time , we select one ci at a time , as the target t , and compute its probability , which ranks ci .
4
Joint Answer Support Models for AS2
We proposed the Answer Support Reranker ( ASR ) , which uses an answer pair classiﬁer to provide evidence to a target answer t. Given a question q , and a subset of its top - k+1 ranked answer candidates , A ( reranked by an AS2 model ) , we build a function , σ : Q × C × Ck → R such that σ(q , t , A \ { t } ) provides the probability of t to be correct , where C is the set of sentence - candidates .
We also design a multi - classiﬁer MASR , which combines k ASR models , one for each different target answer .
4.1 Answer Support - based Reranker ( ASR )
We developed ASR architecture described in Figure 1c .
This consists of three main components :
1 . a Pointwise Reranker ( PR ) , which provides the embedding of the input ( q , t ) , described in Figure 1a .
This is essentially the state - of - the - art AS2 model based on the TANDA approach applied to RoBERTa pre - trained transformer .
1https://github.com/alexa/wqa tanda
3255  ( a ) Baseline Reranker using Transformers .
( b ) PairWise Representation using Transformers .
( c ) Answer Support Reranker
( d ) Multi Answer Support Reranker
Figure 1 : Multi - Answer Support Reranker and its building blocks .
2 . To reduce the noise that may be introduced by irrelevant ci , we use the Answer Support Classiﬁer ( ASC ) , which classiﬁes each ( t , ci ) in one of the following four classes :
0 : t and ci are both correct , 1 : t is correct while ci is not , 2 : vice versa , and 3 : both incorrect .
This multi - classiﬁer , described in Figure 1b , is built on top a RoBERTa Transformer , which produced a PairWise Representation ( PWR ) .
ASC is trained end - to - end with the rest of the network in a multi - task learning fashion , using its speciﬁc cross - entropy loss , computed with the labels above .
3 . The ASR ( see Figure 1c ) uses the joint representation of ( q , t ) with ( t , ci ) , i = 1 , .. , k , where t
improvment .
and ci are the top - candidates reranked by PR .
The k representations are summarized by applying a max - pooling operation , which will aggregate all the supporting or not supporting properties of the candidates with respect to the target answer .
The concatenation of the PR embedding with the max - pooling embedding is given as input to the ﬁnal classiﬁcation layer , which scores t with respect to q , also using the information from the other candidates .
For training and testing , we select a t from the k + 1 candidates of q at a time , and compute its score .
This way , we can rerank all the k + 1 candidates with their scores .
Implementation details : ASR is a PR that also exploits the relation between t and A \ { t } .
We use RoBERTa to generate the [ CLS ] ∈ Rd embedding of ( q , t ) = Et .
We denote with ˆEj the [ CLS ] output by another RoBERTa Transformer applied to answer pairs , i.e. , ( t , cj ) .
Then , we concatenate Et to the max - pooling tensor from ˆE1 , .. , ˆEk :
V
=
[ Et : Maxpool ( [ ˆE1 , .. , ˆEk ] ) ] , ( 1 ) where V ∈ R2d is the ﬁnal representation of the target answer t.
Then , we use a standard feedforward network to implement a binary classiﬁcation layer : p(yi|q , t , Ck ) = sof tmax(V W T + B ) , where W ∈ R2×2d and B are parameters to transform the representation of the target answer t from dimension 2d to dimension 2 , which represents correct or incorrect labels .
ASC labels There can be different interpretations when attempting to deﬁne labels for answer pairs .
An alternative to the deﬁnition illustrated above is to use the following FEVER compatible encoding :
0 : t is correct , while ci can be any value , as also an incorrect ci may provide important context ( corresponding to FEVER Support label ) ;
1 : t is incorrect , ci correct , since ci can provide evidence that t is not similar to a correct answer ( corresponding to FEVER Refutal label ) ; and
2 : both are incorrect , in this case , nothing can be told ( corresponding to FEVER Neutral label ) .
4.2 Multi - Answer Support Reranker ( MASR )
ASR still selects answers with a pointwise approach2 .
This means that we can improve it by
2Again , using ranking loss did not provide a signiﬁcant
3256  Dataset
WikiQA
# Q 873 TREC - QA 1,229 WQA 5,000
Train # A+ 1,040 6,403 42,962
# A7,632 47,014 163,289
# Q 121 65 905
Dev
# A+ 140 205 8,179
# A990 912 28,096
# Q 237 68 1,000
Test # A+ 293 248 8,256
# A2,058 1,194 30,123
Data split Train Dev Test
Supported 80,035 6,666 6,666
Refuted Not Enough Info 35,639 29,775 6,666 6,666 6,666 6,666
Table 2 : AS2 dataset statistics
Table 3 : FEVER dataset statistics
building a listwise model , to select the best answer for each question , by utilizing the information from all target answers .
In particular , the architecture of MASR shown in Figure 1d is made up of two parts : ( i ) a list of ASR containing k + 1 ASR blocks , in which each ASR block provides the representation of a target answer t. ( ii ) A ﬁnal multiclassiﬁer and a softmax function , which scores each t from k + 1 embedding concatenation and selects the one with highest score .
For training and testing , we select the t from the k + 1 candidates of q based on a softmax output at a time .
Implementation details : The goal of MASR is to measure the relation between k + 1 target answers , t0 , .. , tk .
The representation of each target answer is the embedding V ∈ R2d from Equation 1 in ASR .
Then , we concatenate the hidden vectors of k + 1 target answers to form a matrix V(q , k+1 ) ∈ R(k+1)×2d .
We use this matrix and a classiﬁcation layer weights W ∈ R2d , and compute a standard multi - class classiﬁcation loss :
LM ASR = y ∗ log(sof tmax(V(q , k+1)W T ) , ( 2 )
where y is a one - hot - vector , and |y| = |k + 1| .
5 Experiments
In these experiments , we compare our models : KGAT , ASR and MASR with pointwise models , which are the state of the art for AS2 .
We also compare them with our joint model baselines ( pairwise and listwise ) .
Finally , we provide an error analysis .
5.1 Datasets
We used two most popular AS2 datasets , and one real world application dataset we built to test the generality of our approach .
WikiQA is a QA dataset ( Yang et al , 2015 ) containing a sample of questions and answer - sentence candidates from Bing query logs over Wikipedia .
The answers are manually labeled .
We follow the most used setting : training with all the questions that have at least one correct answer , and validating and testing with all the questions having at least one correct and one incorrect answer .
TREC - QA is another popular QA benchmark by Wang et al ( 2007 ) .
We use the same splits of the original data , following the common setting of previous work , e.g. , ( Garg et al , 2020 ) .
WQA The Web - based Question Answering is a dataset built by Alexa AI as part of the effort to improve understanding and benchmarking in QA systems .
The creation process includes the following steps : ( i ) given a set of questions we collected from the web , a search engine is used to retrieve up to 1,000 web pages from an index containing hundreds of millions pages .
( ii ) From the set of retrieved documents , all candidate sentences are extracted and ranked using AS2 models from ( Garg et al , 2020 ) .
Finally , ( iii ) top candidates for each question are manually assessed as correct or incorrect by human judges .
This allowed us to obtain a richer variety of answers from multiple sources with a higher average number of answers .
Table 2 reports the corpus statistics of WikiQA ,
TREC - QA , and WQA3 .
FEVER is a large - scale public corpus , proposed by Thorne et al ( 2018a ) for fact veriﬁcation task , consisting of 185,455 annotated claims from 5,416,537 documents from the Wikipedia dump in June 2017 .
All claims are labelled as Supported , Refuted or Not Enough Info by annotators .
Table 3 shows the statistics of the dataset , which remains the same as in ( Thorne et al , 2018b ) .
5.2 Training and testing details
Metrics The performance of QA systems is typically measured with Accuracy in providing correct answers , i.e. , the percentage of correct responses .
This is also referred to Precision - at-1 ( P@1 ) in the context of reranking , while standard Precision and Recall are not essential in our case as we assume the system does not abstain from providing answers .
We also use Mean Average Precision ( MAP ) and Mean Reciprocal Recall ( MRR ) evaluated on the test set , using the entire set of candidates for each
3The public version of WQA will be released in the short - term future .
Please search for a publication with title WQA : A Dataset for Web - based Question Answering Tasks on arXiv.org .
3257  RoBERTa Base
WikiQA
TREC - QA
Reranker by Garg et al , 2020 Our Reranker Joint Model Multi - classiﬁer ( k=5 ) Joint Model Pairwise ( k=3 ) KGAT ( k=2 ) ASR ( k=3 ) MASR ( k=3 ) MASR - F ( k=3 ) MASR - FP ( k=3 )
P@1
– 0.8189† 0.7819† 0.8272† 0.8436 †0.8436 0.8230 0.8272 0.8436
MAP MRR 0.8890 0.8860 0.8542 0.8927 0.8991 0.9014 0.8891 0.8918 0.8998
0.9010 0.8983 0.8684 0.9045 0.9120 0.9123 0.9017 0.9031 0.9113
P@1
– 0.9118 0.8971 0.9559 0.9412 0.9706 0.9265 0.9412 0.9559
MAP MRR 0.9140 0.9043 0.9052 0.9196 0.9155 0.9257 0.9200 0.9222 0.9191
0.9520 0.9498 0.9424 0.9743 0.9645 0.9816 0.9632 0.9706 0.9743
P@1
WQA MAP
MRR
– –
– –
– – -2.29†% -1.00 % -1.23 % 2.67†% 0.39 % 1.39 % 2.10 % 0.39 % 0.93 % †2.86 % 0.86 % 1.39 % 3.82 % 0.70 % 1.67 % 2.67 % 0.55 % 1.47 % 4.96 % 0.94 % 2.43 %
Table 4 : Results on WikiQA , TREC - QA and WQA , using RoBERTa base Transformer .
† is used to indicate that the difference in P@1 between ASR and the other marked systems is statistically signiﬁcant at 95 % .
JOINT - MULTICLASSIFER
JOINT - PAIR
KGAT
ASR
)
%
(
t n e m e v o r p m
I
5
4
3
2
1
0
−1
−2
−3
1
2
4
5
3 k
Figure 2 : Impact of k on the WQA dev .
set
question ( this varies according to the dataset ) , to have a direct comparison with the state of the art .
Models We use the pre - trained RoBERTa - Base ( 12 layer ) and RoBERTa - Large - MNLI ( 24 layer ) models , which were released as checkpoints for use in downstream tasks4 .
Reranker training We adopt Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 2e-5 for the transfer step on the ASNQ dataset ( Garg et al , 2020 ) , and a learning rate of 1e-6 for the adapt step on the target dataset .
We apply early stopping on the development set of the target corpus for both ﬁne - tuning steps based on the highest MAP score .
We set the max number of epochs equal to 3 and 9 for the adapt and transfer steps , respectively .
We set the maximum sequence length for RoBERTa to 128 tokens .
KGAT and ASR training Again , we use the Adam optimizer with a learning rate of 2e-6 for training the ASR model on the target dataset .
We utilize 1 Tesla V100 GPU with 32 GB memory and a train batch size of eight .
We set the maximum sequence length for RoBERTa Base / Large to 130 tokens and the number of training epochs to 20 .
The other training conﬁgurations are the same of the original KGAT model from ( Liu et al , 2020 ) .
We use two transformer models for ASR : a RoBERTa
4https://github.com/pytorch/fairseq
Base / Large for PR , and one for ASC .
We set the maximum sequence length for RoBERTa to 128 tokens and the number of epochs to 20 .
MASR training We use the same conﬁguration of the ASR training , including the optimizer type , learning rate , the number of epochs , GPU type , maximum sequence length , etc .
Additionally , we design two different models MASR - F , using an ASC classiﬁer targeting the FEVER labels , and MASR - FP , which initializes ASC with the data from FEVER .
This is possible as the labels are compatible .
5.3 Choosing the best k
The selection of the hyper - parameter k , i.e. , the number of candidates to consider for supporting a target answer is rather tricky .
Indeed , the standard validation set is typically used for tuning PR .
This means that the candidates PR moves to the top k +1 positions are optimistically accurate .
Thus , when selecting also the optimal k on the same validation set , there is high risk to overﬁt the model .
We solved this problem by running a PR version not heavily optimized on the dev .
set , i.e. , we randomly choose a checkpoint after the standard three epochs of ﬁne - tuning of RoBERTa transformer .
Additionally , we tuned k only using the WQA dev .
set , which contains ∼ 36 , 000 Q / A pairs .
WikiQA and TREC - QA dev .
sets are too small to be used ( 121 and 65 questions , respectively ) .
Fig .
2 plots the improvement of four different models , Joint Model Multi - classiﬁer , Joint Model Pairwise , KGAT , and ASR , when using different k values .
Their best results are reached for 5 , 3 , 2 , and 3 , respectively .
We note that the most reliable curve shape ( convex ) is the one of ASR and Joint Model Pairwise .
3258  5.4 Comparative Results
Table 4 reports the P@1 , MAP and MRR of the rerankers , and different answer supporting models on WikiQA , TREC - QA and WQA datasets .
As WQA is an internal dataset , we only report the improvement over PR in the tables .
All models use RoBERTa - Base pre - trained checkpoint and start from the same set of k candidates reranked by PR ( state - of - the - art model ) .
The table shows that :
• PR replicates the MAP and MRR of the stateof - the - art reranker by Garg et al ( 2020 ) on WikiQA .
• Joint Model Multi - classiﬁer performs lower than PR for all measures and all datasets .
This is in line with the ﬁndings of Bonadiman and Moschitti ( 2020 ) , who also did not obtain improvement when jointly used all the candidates altogether in a representation .
• Joint Model Pairwise differs from ASR as it concatenates the embeddings of the ( q , ci ) , instead of using max - pooling , and does not use any Answer Support Classiﬁer ( ASC ) .
Still , it exploits the idea of aggregating the information of all pairs ( q , ci ) with respect to a target answer t , which proves to be effective , as the model improves on PR over all measures and datasets .
• Our KGAT version for AS2 also improves PR over all datasets and almost all measures , conﬁrming that the idea of using candidates as support of the target answer is generally valid .
However , it is not superior to Joint Model Pairwise .
• ASR achieves the highest performance among all models ( but MASR - FP on WQA ) , all datasets , and all measures .
For example , it outperforms PR by almost 3 absolute percent points in P@1 on WikiQA , and by almost 6 points on TREC from 91.18 % to 97.06 % , which corresponds to an error reduction of 60 % .
• MASR and MASR - F do not achieve better performance than Joint Model Pairwise on WikiQA and TREC , although MASR outperforms all baselines and even ASR on WQA .
This suggests that the signiﬁcantly higher number of parameters of MASR can not be trained on small corpus , while WQA has a sufﬁcient number of examples .
RoBERTa Large
Garg et al , Our Reranker KGAT ( K=2 ) ASR ( K=3 )
WikiQA
TREC - QA
P@1 – 0.8724 0.8642 0.8971
MAP MRR 0.9200 0.9151 0.9094 0.9280
0.9330 0.9266 0.9218 0.9399
P@1
– 0.9706 0.9559 0.9706
MAP MRR 0.9430 0.9481 0.9407 0.9488
0.9740 0.9816 0.9743 0.9816
Table 5 : Results on WikiQA and TREC - QA , using RoBERTa Large Transformer .
• MASR - FP exploiting FEVER for the initialization of ASC performs better than MASR and MASR - F on WikiQA and TREC .
Interestingly , it signiﬁcantly outperforms ASR by 2 % on WQA .
This conﬁrms the potential of the model when enough training data is available .
• We perform randomization test ( Yeh , 2000 ) to verify if the models signiﬁcantly differ in terms of prediction outcome .
We use 100,000 trials for each calculation .
The results conﬁrm the statistically signiﬁcant difference between ASR and all the baselines , with p < 0.05 for WikiQA , and between ASR and all models ( i.e. , including also KGAT ) on WQA .
5.5 Ofﬁcial State of the art
As the state of the art for AS2 is obtained using RoBERTa Large , we trained KGAT and ASR using this pre - trained language model .
Table 5 also reports the comparison with PR , which is the ofﬁcial state of the art .
Again , our PR replicates the results of Garg et al ( 2020 ) , obtaining slightly lower performance on WikiQA but higher on TREC - QA .
KGAT performs lower than PR on both datasets .
ASR establishes the new state of the art on WikiQA with an MAP of 92.80 vs. 92.00 .
The P@1 also signiﬁcantly improves by 2 % , i.e. , achieving 89.71 , which is impressively high .
Also , on TRECQA , ASR outperforms all models , being on par with PR regarding P@1 .
The latter is 97.06 , which corresponds to mistaking the answers of only two questions .
We manually checked these and found out that these were two annotation errors : ASR achieves perfect accuracy while PR only mistakes one answer .
Of course , this just provides evidence that PR based on RoBERTa - Large solves the task of selecting the best answers ( i.e. , measuring P@1 on this dataset is not meaningful anymore ) .
5.6 Model Discussion
Table 6 reports the accuracy of ASC inside to different models .
In ASR , it uses 4 - way categories , while in MASR - based models , it uses the three
3259  WikiQA ACC F1 0.59 0.46 0.46 0.49
0.00 0.00 0.00 0.37
TREC - QA ACC F1 0.56 0.45 0.64 0.65
0.80 0.62 0.78 0.73
WQA ACC F1 0.58 0.53 0.58 0.59
0.64 0.61 0.68 0.69
ASR MASR MASR - F MASR - FP
Table 6 : The Accuracy and F1 of category 0 for ASC
FEVER labels ( see Sec . 4.1 ) .
ACC is the overall accuracy while F1 refers to the category 0 .
We note that ASC in MASR - FP achieves the highest accuracy with respect to the average over all datasets .
This happens since we pre-ﬁne - tuned it with the FEVER data .
We analyzed examples for which ASR is correct and PR is not .
Tab .
7 shows that , given q and k = 3 candidates , PR chooses c1 , a suitable but wrong answer .
This probably happens since the answer best matches the syntactic / semantic pattern of the question , which asks for a type of color , indeed , the answer offers such type , primary colors .
PR does not rely on any background information that can support the set of colors in the answer .
In contrast , ASR selects c2 as it can rely on the support of other answers .
Its ASC provides an average score for the category 0
( both members are correct ) of c2 , i.e. , 1 i(cid:54)=2 ASC(c2 , ci ) = 0.653 , while for c1 the k average score is signiﬁcant lower , i.e. , 0.522 .
This provides higher support for c2 , which is used by ASR to rerank the output of PR .
( cid:80 )
Tab . 8 shows an interesting case where all the sentences contain the required information , i.e. , February .
However , PR and ASR both choose answer c0 , which is correct but not natural , as it provides the requested information indirectly .
Also , it contains a lot of ancillary information .
In contrast , MASR is able to rerank the best answer , c1 , in the top position .
6 Conclusion
We have proposed new joint models for AS2 .
ASR encodes the relation between the target answer and all the other candidates , using an additional Transformer model , and an Answer Support Classiﬁer , while MASR jointly models the ASR representations for all target answers .
We extensively tested KGAT , ASR , MASR , and other joint model baselines we designed .
The results show that our models can outperform the state of the art .
Most interestingly , ASR constantly outperforms all the models ( but MASR - FP ) , on all datasets , through all measures , and for both base and large transformers .
For example , ASR
q : What kind of colors are in the rainbow ?
c1 :
Red , yellow , and blue are called the primary colors .
c2 :
The order of the colors in the rainbow goes : red , orange , yellow , green , blue , indigo and violet .
The colors in all rainbows are present in the same order : red , orange , yellow , green , blue , indigo , and violet .
c4 : A rainbow occurs when white light bends and separates
c3 :
into red , orange , yellow , green blue , indigo and violet .
Table 7 : A question with answer candidates ranked by PR ; ASR chose c2 .
c1 :
q : What ’s the month of Valentine ’s day ?
c0 : Celebrated on February 14 every year , saint Valentine ’s day or Valentine ’s day is the traditional day on which lovers convey their love to each other by sending Valentine ’s cards , sometimes even anonymously .
February is historically chosen to be the month of love and romance and the month to celebrate Valentine ’s day .
In order for today to be Valentine ’s day , it ’s necessary that today is in the month of February .
Every year , Valentine ’s day is celebrated on February 14 in many countries around the world .
c3 :
c2 :
Table 8 : A question with answer candidates { c0 , c1 , c2 , c3 } ranked by PR ; ASR reranks as { c0 , c3 , c2 , c1 } ; and MASR reranks as { c1 , c3 , c0 , c2 } ; c1 is the natural correct answer .
achieves the best reported results , i.e. , MAP values of 92.80 % and 94.88 , on WikiQA and TRECQA , respectively .
MASR improves ASR by 2 % on WQA , since this contains enough data to train the ASR representations jointly .
