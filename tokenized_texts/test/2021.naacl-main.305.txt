Double Perturbation : On the Robustness of Robustness and Counterfactual Bias Evaluation
Chong Zhang
Jieyu Zhao Huan Zhang Kai - Wei Chang Cho - Jui Hsieh
Department of Computer Science , UCLA { chongz , jyzhao , kwchang , chohsieh}@cs.ucla.edu , huan@huan-zhang.com
Abstract
Xtest
abundant natural
Robustness and counterfactual bias are usually evaluated on a test dataset .
However , are these evaluations robust ?
If the test dataset is perturbed slightly , will the evaluation results keep the same ?
In this paper , we propose a “ double perturbation ” framework to uncover model weaknesses beyond the test dataset .
The framework ﬁrst perturbs the test dataset to construct sentences similar to the test data , and then diagnoses the prediction change regarding a single - word substitution .
We apply this framework to study two perturbation - based approaches that are used to analyze models ’ robustness and counterfactual bias in English .
( 1 ) For robustness , we focus on synonym substitutions and identify vulnerable examples where prediction can be altered .
Our proposed attack attains high success rates ( 96.0%–99.8 % ) in ﬁnding vulnerable examples on both original and robustly trained CNNs and Transformers .
( 2 ) For counterfactual bias , we focus on substituting demographic tokens ( e.g. , gender , race ) and measure the shift of the expected prediction among constructed sentences .
Our method is able to reveal the hidden model biases not directly shown in the test dataset .
Our code is available https://github.com/chong-z/ at nlp - second - order - attack .
1
Introduction
Recent studies show that NLP models are vulnerable to adversarial perturbations .
A seemingly “ invariance transformation ” ( a.k.a . adversarial perturbation ) such as synonym substitutions ( Alzantot et al , 2018 ; Zang et al , 2020 ) or syntax - guided paraphrasing ( Iyyer et al , 2018 ; Huang and Chang , 2021 ) can alter the prediction .
To mitigate the model vulnerability , robust training methods have been proposed and shown effective ( Miyato et al , 2017 ;
Jia et al , 2019 ; Huang et al , 2019 ; Zhou et al , 2020 ) .
x0 = " a deep and meaningful ﬁlm ( movie ) . "
99 % positive ( 99 % positive )
perturb
˜x0 = " a short and moving ﬁlm ( movie ) . "
73 % positive
( 70 % negative )
Figure 1 : A vulnerable example beyond the test dataset .
Numbers on the bottom right are the sentiment predictions for film and movie .
Blue x0 comes from the test dataset and its prediction can not be altered by the substitution film → movie ( robust ) .
Yellow example ˜x0 is slightly perturbed but remains natural .
Its prediction can be altered by the substitution ( vulnerable ) .
In most studies , model robustness is evaluated based on a given test dataset or synthetic sentences constructed from templates ( Ribeiro et al , 2020 ) .
Speciﬁcally , the robustness of a model is often evaluated by the ratio of test examples where the model prediction can not be altered by semantic - invariant perturbation .
We refer to this type of evaluations as the ﬁrst - order robustness evaluation .
However , even if a model is ﬁrst - order robust on an input sentence x0 , it is possible that the model is not robust on a natural sentence ˜x0 that is slightly modiﬁed from x0 .
In that case , adversarial examples still exist even if ﬁrst - order attacks can not ﬁnd any of them from the given test dataset .
Throughout this paper , we call ˜x0 a vulnerable example .
The existence of such examples exposes weaknesses in models ’ understanding and presents challenges for model deployment .
Fig .
1 illustrates an example .
In this paper , we propose the double perturbation framework for evaluating a stronger notion of second - order robustness .
Given a test dataset , we consider a model to be second - order robust if there is no vulnerable example that can be identiﬁed in the neighborhood of given test instances
Proceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics : HumanLanguageTechnologies , pages3899–3916June6–11,2021. © 2021AssociationforComputationalLinguistics3899  ( § 2.2 ) .
In particular , our framework ﬁrst perturbs the test set to construct the neighborhood , and then diagnoses the robustness regarding a single - word synonym substitution .
Taking Fig .
2 as an example , the model is ﬁrst - order robust on the input sentence x0 ( the prediction can not be altered ) , but it is not second - order robust due to the existence of the vulnerable example ˜x0 .
Our framework is designed to identify ˜x0 .
We apply the proposed framework and quantify second - order robustness through two second - order attacks ( § 3 ) .
We experiment with English sentiment classiﬁcation on the SST-2 dataset ( Socher et al , 2013 ) across various model architectures .
Surprisingly , although robustly trained CNN ( Jia et al , 2019 ) and Transformer ( Xu et
al , 2020 ) can achieve high robustness under strong attacks ( Alzantot et al , 2018 ; Garg and Ramakrishnan , 2020 ) ( 23.0%–71.6 % success rates ) , for around 96.0 % of the test examples our attacks can ﬁnd a vulnerable example by perturbing 1.3 words on average .
This ﬁnding indicates that these robustly trained models , despite being ﬁrst - order robust , are not second - order robust .
Furthermore , we extend the double perturbation framework to evaluate counterfactual biases ( Kusner et al , 2017 ) ( § 4 ) in English .
When the test dataset is small , our framework can help improve the evaluation robustness by revealing the hidden biases not directly shown in the test dataset .
Intuitively , a fair model should make the same prediction for nearly identical examples referencing different groups ( Garg et al , 2019 ) with different protected attributes ( e.g. , gender , race ) .
In our evaluation , we consider a model biased if substituting tokens associated with protected attributes changes the expected prediction , which is the average prediction among all examples within the neighborhood .
For instance , a toxicity classiﬁer is biased if it tends to increase the toxicity if we substitute straight → gay in an input sentence ( Dixon et al , 2018 ) .
In the experiments , we evaluate the expected sentiment predictions on pairs of protected tokens ( e.g. , ( he , she ) , ( gay , straight ) ) , and demonstrate that our method is able to reveal the hidden model biases .
Our main contributions are : ( 1 ) We propose the double perturbation framework to diagnose the robustness of existing robustness and fairness evaluation methods .
( 2 ) We propose two second - order attacks to quantify the stronger notion of secondpositive
negative
˜x1
x0
˜x(cid:48 ) 0
˜x0
Figure 2 : An illustration of the decision boundary .
Diamond area denotes invariance transformations .
Blue x0 is a robust input example ( the entire diamond is green ) .
Yellow ˜x0 is a vulnerable example in the neighborhood of x0 .
Red ˜x(cid:48 ) 0 is an adversarial example to ˜x0 .
Note : ˜x(cid:48 ) 0 is not an adversarial example to x0 since they have different meanings to human ( outside the diamond ) .
order robustness and reveal the models ’ vulnerabilities that can not be identiﬁed by previous attacks .
( 3 ) We propose a counterfactual bias evaluation method to reveal the hidden model bias based on our double perturbation framework .
2 The Double Perturbation Framework
In this section , we describe the double perturbation framework which focuses on identifying vulnerable examples within a small neighborhood of the test dataset .
The framework consists of a neighborhood perturbation and a word substitution .
We start with deﬁning word substitutions .
2.1 Existing Word Substitution Strategy
We focus our study on word - level substitution , where existing works evaluate robustness and counterfactual bias by directly perturbing the test dataset .
For instance , adversarial attacks alter the prediction by making synonym substitutions , and the fairness literature evaluates counterfactual fairness by substituting protected tokens .
We integrate the word substitution strategy into our framework as the component for evaluating robustness and fairness .
For simplicity , we consider a single - word substitution and denote it with the operator ⊕. Let X ⊆ V l be the input space where V is the vocabulary and l is the sentence length , p = ( p(1 ) , p(2 ) )
∈ V 2 be a pair of synonyms ( called patch words ) , Xp ⊆ X denotes sentences with a single occurrence of p(1 ) ( for simplicity we skip other sentences ) , x0 ∈ Xp be an input sentence , then x0 ⊕ p means “ substitute p(1 ) → p(2 ) in x0 ” .
The result after substitution is :
x(cid:48 ) 0
= x0 ⊕ p.
3900  Taking Fig .
1 as an example , where p = ( film , movie ) and x0 = a deep and meaningful film , the perturbed sentence is x(cid:48 ) 0
= a deep and meaningful movie .
Now we introduce other components in our framework .
2.2 Proposed Neighborhood Perturbation
Instead of applying the aforementioned word substitutions directly to the original test dataset , our framework perturbs the test dataset within a small neighborhood to construct similar natural sentences .
This is to identify vulnerable examples with respect to the model .
Note that examples in the neighborhood are not required to have the same meaning as the original example , since we only study the prediction difference caused by applying synonym substitution p ( § 2.1 ) .
Constraints on the neighborhood .
We limit the neighborhood sentences within a small ( cid:96)0 norm ball ( regarding the test instance ) to ensure syntactic similarity , and empirically ensure the naturalness through a language model .
The neighborhood of an input sentence x0 ∈ X is :
where Ballk(x0 ) = { x | ( cid:107)x − x0(cid:107)0
≤ k , x ∈ X } is the ( cid:96)0 norm ball around x0 ( i.e. , at most k different tokens ) , and Xnatural denotes natural sentences that satisfy a certain language model score which will be discussed next .
Construction with masked language model .
We construct neighborhood sentences from x0 by substituting at most k tokens .
As shown in Algorithm 1 , the construction employs a recursive approach and replaces one token at a time .
For each recursion , the algorithm ﬁrst masks each token of the input sentence ( may be the original x0 or the ˜x from last recursion ) separately and predicts likely replacements with a masked language model ( e.g. , DistilBERT , Sanh et al 2019 ) .
To ensure the naturalness , we keep the top 20 tokens for each mask with the largest logit ( subject to a threshold , Line 9 ) .
Then , the algorithm constructs neighborhood sentences by replacing the mask with found tokens .
We use the notation ˜x in the following sections to denote the constructed sentences within the neighborhood .
Algorithm 1 : Neighborhood construction Data : Input sentence x0 , masked language model
LM , max distance k.
1 Function Neighbork(x0 ): 2
if k = 0 then return { x0 } ; if k ≥ 2 then return ( cid:83 ) Xneighbor ← ∅ ; for i ← 0 , . . .
, len(x0 ) − 1 do
˜x∈Neighbor1(x0 ) Neighbork−1(˜x ) ;
T , L ← LM.ﬁllmask(x0 , i ) ; ( cid:46 )
Mask ith token and return candidate tokens and corresponding logits .
L ← SortDecreasing(L ) ; lmin ← max{L(κ ) , L(0 ) − δ } ; ( cid:46 ) L(i ) denotes the ith element .
We
empirically set κ ← 20 and δ ← 3 .
Tnew ← { t | l > lmin , ( t , l ) ∈ T × L } ; Xnew ← { x0 | x(i ) ( cid:46 ) Construct new sentences by replacing the ith token .
0 ← t , t ∈ Tnew } ;
Xneighbor ← Xneighbor ∪ Xnew ;
return Xneighbor ;
3
4
5
6
7
8
9
10
11
12
13
3 Evaluating Second - Order Robustness
With the proposed double perturbation framework , we design two black - box attacks1 to identify vulnerable examples within the neighborhood of the test set .
We aim at evaluating the robustness for inputs beyond the test set .
Adversarial attacks search for small and invariant perturbations on the model input that can alter the prediction .
To simplify the discussion , in the following , we take a binary classiﬁer f ( x ) : X → { 0 , 1 } as an example to describe our framework .
Let x0 be the sentence from the test set with label y0 , then the smallest perturbation δ∗ under ( cid:96)0 norm distance is:2
δ∗ : = argmin
( cid:107)δ(cid:107)0 s.t . f ( x0 ⊕ δ ) ( cid:54)= y0 .
δ
Here δ = p1 ⊕ · · · ⊕ pl denotes a series of substitutions .
In contrast , our second - order attacks ﬁx δ = p and search for the vulnerable x0 .
3.2 Proposed Second - Order Attacks
Second - order attacks study the prediction difference caused by applying p.
For notation convenience we deﬁne the prediction difference F ( x ; p ) :
1Black - box attacks only observe the model outputs and do
not know the model parameters or the gradient .
2For simplicity , we use ( cid:96)0 norm distance to measure the
similarity , but other distance metrics can be applied .
Neighbork(x0 ) ⊆ Ballk(x0 ) ∩ Xnatural ,
( 1 )
3.1 Previous First - Order Attacks
3901  Find p for x0 .
x0 = a deep and meaningful ﬁlm .
Find vulnerable example through beam search .
p alters the prediction .
x ( i = 1 ) a deep and disturbing ﬁlm ( movie ) .
a deep and moving ﬁlm ( movie ) .
a dramatic and meaningful ﬁlm ( movie ) .
fsoft(x ) .990 ( .989 ) .999 ( .999 ) .999 ( .999 )
˜x0 = " a short and moving ﬁlm ( movie ) . "
73 % positive
( 70 % negative )
Synonyms ﬁlm , movie story , tale
fool , silly · · ·
p = ﬁlm , movie
x ( i = 2 )
a short and moving ﬁlm ( movie ) .
a slow and moving ﬁlm ( movie ) .
a dramatic or meaningful ﬁlm ( movie ) .
fsoft(x ) .730 ( .303 ) .519 ( .151 ) .487 ( .168 )
· ·
·
· ·
·
Figure 3 : The attack ﬂow for SO - Beam ( Algorithm 2 ) .
Blue x0 is the input sentence and yellow ˜x0 is our constructed vulnerable example ( the prediction can be altered by substituting film → movie ) .
Green boxes in the middle show intermediate sentences , and fsoft(x ) denotes the probability outputs for film and movie .
X × V 2 → { −1 , 0 , 1 } by:3
F ( x ; p ) : = f ( x ⊕ p ) − f ( x ) .
( 2 )
Taking Fig .
1 as an example , the prediction difference for ˜x0 on p is F ( ˜x0 ; p ) =
f ( ... moving movie . )
− f ( ... moving film . )
= −1 .
Given an input sentence x0 , we want to ﬁnd patch words p and a vulnerable example ˜x0 such that f ( ˜x0 ⊕ p ) ( cid:54)= f ( ˜x0 ) .
Follow Alzantot et al ( 2018 ) , we choose p from a predeﬁned list of counter-ﬁtted synonyms ( Mrkši´c et al , 2016 ) that maximizes |fsoft(p(2 ) )
− fsoft(p(1))| .
Here fsoft(x ) : X → [ 0 , 1 ] denotes probability output ( e.g. , after the softmax layer but before the ﬁnal argmax ) , fsoft(p(1 ) ) and fsoft(p(2 ) ) denote the predictions for the single word , and we enumerate through all possible p for x0 .
Let k be the neighborhood distance , then the attack is equivalent to solving :
˜x0 = argmax
|F ( x ; p)| .
( 3 )
x∈Neighbork(x0 )
Brute - force attack ( SO - Enum ) .
A naive approach for solving Eq .
( 3 ) is to enumerate through Neighbork(x0 ) .
The enumeration ﬁnds the smallest perturbation , but is only applicable for small k ( e.g. , k ≤ 2 ) given the exponential complexity .
Beam - search attack ( SO - Beam )
.
The efﬁciency can be improved by utilizing the probability output , where we solve Eq .
( 3 ) by minimizing the crossentropy loss with regard to x ∈ Neighbork(x0 ):
L(x ; p ) :
= − log(1 − fmin ) − log(fmax ) ,
( 4 )
where fmin and fmax are the smaller and the larger output probability between fsoft(x ) and fsoft(x ⊕
3We assume a binary classiﬁcation task , but our framework
is general and can be extended to multi - class classiﬁcation .
p ) , respectively .
Minimizing Eq .
( 4 ) effectively leads to fmin → 0 and fmax → 1 , and we use a beam search to ﬁnd the best x. At each iteration , we construct sentences through Neighbor1(x ) and only keep the top 20 sentences with the smallest L(x ; p ) .
We run at most k iterations , and stop earlier if we ﬁnd a vulnerable example .
We provide the detailed implementation in Algorithm 2 and a ﬂowchart in Fig .
3 .
Algorithm 2 : Beam - search attack ( SOBeam )
Data : Input sentence x0 , synonyms P , model
functions F and fsoft , loss L , max distance k.
1 Function SO - Beamk(x0 ): p ← argmax 2
p∈P s.t .
x0∈Xp
|fsoft(p(2 ) )
− fsoft(p(1))| ;
3
4
5
6
7
8
9
˜x∈Xbeam
Xbeam ← { x0 } ; for i ← 1 , . . .
, k do Xnew ← ( cid:83 ) Neighbor1(˜x ) ; ˜x0
← argmaxx∈Xnew |F ( x ; p)| ; if F ( ˜x0 ; p ) ( cid:54)= 0 then return ˜x0 ; Xnew ← SortIncreasing(Xnew , L ) ; Xbeam ← { X ( 0 ) ( cid:46 )
Keep the best beam .
new , . . .
, X ( β−1 )
} ;
new
10
return None ;
We set β ← 20 .
3.3 Experimental Results
In this section , we evaluate the second - order robustness of existing models and show the quality of our constructed vulnerable examples .
3.3.1 Setup
We follow the setup from the robust training literature ( Jia et al , 2019 ; Xu et
al , 2020 ) and experiment with both the base ( non - robust ) and robustly trained models .
We train the binary sentiment classiﬁers on the SST-2 dataset with bag - ofwords ( BoW ) , CNN , LSTM , and attention - based
3902  Original : 70 % Negative Input Example :
Genetic : 56 % Positive Adversarial Example :
BAE : 56 % Positive Adversarial Example :
in its best moments , resembles a bad high school production of grease , without beneﬁt of song .
in its best moment , recalling a naughty high school production of lubrication , unless beneﬁt of song .
in its best moments , resembles a great high school production of grease , without beneﬁt of song .
SO - Enum and SO - Beam ( ours ): 60 % Negative Vulnerable Example :
( 67 % Positive )
in its best moments , resembles a bad ( unhealthy ) high school production of musicals , without beneﬁt of song .
Table 1 : Sampled attack results on the robust BoW.
For Genetic and BAE the goal is to ﬁnd an adversarial example that alters the original prediction , whereas for SO - Enum and SO - Beam the goal is to ﬁnd a vulnerable example beyond the test set such that the prediction can be altered by substituting bad → unhealthy .
models .
Base models .
For BoW , CNN , and LSTM , all models use pre - trained GloVe embeddings ( Pennington et al , 2014 ) , and have one hidden layer of the corresponding type with 100 hidden size .
Similar to the baseline performance reported in GLUE ( Wang et al , 2019 ) , our trained models have an evaluation accuracy of 81.4 % , 82.5 % , and 81.7 % , respectively .
For attention - based models , we train a 3 - layer Transformer ( the largest size in Shi et al 2020 ) and ﬁne - tune a pre - trained bertbase - uncased from HuggingFace ( Wolf et al , 2020 ) .
The Transformer uses 4 attention heads and 64 hidden size , and obtains 82.1 % accuracy .
The BERT - base uses the default conﬁguration and obtains 92.7 % accuracy .
Robust models ( ﬁrst - order ) .
With the same setup as base models , we apply robust training methods to improve the resistance to word substitution attacks .
Jia et al ( 2019 ) provide a provably robust training method through Interval Bound Propagation ( IBP , Dvijotham et al 2018 ) for all word substitutions on BoW , CNN and LSTM .
Xu et al ( 2020 ) provide a provably robust training method on general computational graphs through a combination of forward and backward linear bound propagation , and the resulting 3 - layer Transformer is robust to up to 6 word substitutions .
For both works we use the same set of counter-ﬁtted synonyms provided in Jia et al ( 2019 ) .
We skip BERT - base due to the lack of an effective robust training method .
Attack success rate ( ﬁrst - order ) .
We quantify ﬁrst - order robustness through attack success rate , which measures the ratio of test examples that an adversarial example can be found .
We use ﬁrstorder attacks as a reference due to the lack of a direct baseline .
We experiment with two black - box attacks : ( 1 ) The Genetic attack ( Alzantot et al , 2018 ;
Jia et al , 2019 ) uses a population - based optimization algorithm that generates both syntactically and semantically similar adversarial examples , by replacing words within the list of counterﬁtted synonyms .
( 2 ) The BAE attack ( Garg and Ramakrishnan , 2020 ) generates coherent adversarial examples by masking and replacing words using BERT .
For both methods we use the implementation provided by TextAttack ( Morris et al , 2020 ) .
Attack success rate ( second - order ) .
We also quantify second - order robustness through attack success rate , which measures the ratio of test examples that a vulnerable example can be found .
To evaluate the impact of neighborhood size , we experiment with two conﬁgurations : ( 1 ) For the small neighborhood ( k = 2 ) , we use SO - Enum that ﬁnds the most similar vulnerable example .
( 2 ) For the large neighborhood ( k = 6 ) , SO - Enum is not applicable and we use SO - Beam to ﬁnd vulnerable examples .
We consider the most challenging setup and use patch words p from the same set of counter-ﬁtted synonyms as robust models ( they are provably robust to these synonyms on the test set ) .
We also provide a random baseline to validate the effectiveness of minimizing Eq .
( 4 ) ( Appendix A.1 ) .
Quality metrics ( perplexity and similarity ) .
We quantify the quality of our constructed vulnerable examples through two metrics : ( 1 ) GPT-2 ( Radford et al , 2019 ) perplexity quantiﬁes the naturalness of a sentence ( smaller is better ) .
We report the perplexity for both the original input examples and the constructed vulnerable examples .
( 2 ) ( cid:96)0 norm distance quantiﬁes the disparity between two sentences ( smaller is better ) .
We report the distance between the input and the vulnerable example .
Note that ﬁrst - order attacks have different objectives and thus can not be compared directly .
3903  Attack Success Rate ( % ) Genetic BAE SO - Enum SO - Beam
Base Models : BoW CNN LSTM Transformer BERT - base Robust Models : BoW CNN LSTM Transformer
57.0 62.0 60.0 73.0 41.0
28.0 23.0 24.0 56.0
69.7 71.0 68.3 74.3 61.5
63.1 64.4 61.0 71.6
95.3 95.3 95.8 95.4 94.3
81.5 91.0 62.9 91.2
99.7 99.8 99.5 98.0 98.7
88.4 96.0 77.5 96.2
SO - Enum Perturb PPL
Original PPL
SO - Beam Perturb PPL
Original PPL
( cid:96)0
1.1 1.1 1.1 1.0 1.3
1.2 1.2 1.3 1.2
202 204 204 193 229
212 209 251 213
( cid:96)0
1.2 1.2 1.2 1.1 1.4
1.4 1.3 1.8 1.3
166 166 166 165 168
171 168 185 165
202 201 204 195 222
222 210 260 208
Base Models : BoW CNN LSTM Transformer BERT - base Robust Models : BoW CNN LSTM Transformer
168 170 168 165 170
170 166 194 170
Table 2 : The average rates over 872 examples ( 100 for Genetic due to long running time ) .
Second - order attacks achieve higher successful rate since they are able to search beyond the test set .
Table 3 : The quality metrics for second - order methods .
We report the median perplexity ( PPL ) and average ( cid:96)0 norm distance .
The original PPL may differ across models since we only count successful attacks .
3.3.2 Results
We experiment with the validation split ( 872 examples ) on a single RTX 3090 .
The average running time per example ( in seconds ) on base LSTM is 31.9 for Genetic , 1.1 for BAE , 7.0 for SO - Enum ( k = 2 ) , and 1.9 for SO - Beam ( k = 6 ) .
We provide additional running time results in Appendix A.3 .
Table 1 provides an example of the attack result where all attacks are successful ( additional examples in Appendix A.5 ) .
As shown , our secondorder attacks ﬁnd a vulnerable example by replacing grease → musicals , and the vulnerable example has different predictions for bad and unhealthy .
Note that , Genetic and BAE have different objectives from second - order attacks and focus on ﬁnding the adversarial example .
Next we discuss the results from two perspectives .
Second - order robustness .
We observe that existing robustly trained models are not second - order robust .
As shown in Table 2 , our second - order attacks attain high success rates not only on the base models but also on the robustly trained models .
For instance , on the robustly trained CNN and Transformer , SO - Beam ﬁnds vulnerable examples within a small neighborhood for around 96.0 % of the test examples , even though these models have improved resistance to strong ﬁrst - order attacks ( success rates drop from 62.0%–74.3 % to 23.0%–71.6 % for Genetic and BAE).4
This phenomenon can be explained by the fact that both ﬁrstorder attacks and robust training methods focus on synonym substitutions on the test set , whereas our attacks , due to their second - order nature , ﬁnd vul4BAE is more effective on robust models as it may use
replacement words outside the counter-ﬁtted synonyms .
nerable examples beyond the test set , and the search is not required to maintain semantic similarity .
Our methods provide a way to further investigate the robustness ( or ﬁnd vulnerable and adversarial examples ) even when the model is robust to the test set .
Quality of constructed vulnerable examples .
As shown in Table 3 , second - order attacks are able to construct vulnerable examples by perturbing 1.3 words on average , with a slightly increased perplexity .
For instance , on the robustly trained CNN and Transformer , SO - Beam constructs vulnerable examples by perturbing 1.3 words on average , with the median5 perplexity increased from around 165 to around 210 .
We provide metrics for ﬁrst - order attacks in Appendix A.5 as they have different objectives and are not directly comparable .
Furthermore , applying existing attacks on the vulnerable examples constructed by our method will lead to much smaller perturbations .
As a reference , on the robustly trained CNN , Genetic attack constructs adversarial examples by perturbing 2.7 words on average ( starting from the input examples ) .
However , if Genetic starts from our vulnerable examples , it would only need to perturb a single word ( i.e. , the patch words p ) to alter the prediction .
These results demonstrate the weakness of the models ( even robustly trained ) for those inputs beyond the test set .
3.3.3 Human Evaluation We perform human evaluation on the examples constructed by SO - Beam .
Speciﬁcally , we randomly
5We report median due to the unreasonably large perplexity on certain sentences .
e.g. , 395 for that ’s a cheat .
but 6740 for that proves perfect cheat .
3904  Naturalness ( 1 - 5 ) Semantic Similarity ( % ) Original Perturb Original
Perturb
3.87
3.63
85
71
Table 4 : The quality metrics from human evaluation .
select 100 successful attacks and evaluate both the original examples and the vulnerable examples .
To evaluate the naturalness of the constructed examples , we ask the annotators to score the likelihood ( on a Likert scale of 1 - 5 , 5 to be the most likely ) of being an original example based on the grammar correctness .
To evaluate the semantic similarity after applying the synonym substitution p , we ask the annotators to predict the sentiment of each example , and calculate the ratio of examples that maintain the same sentiment prediction after the synonym substitution .
For both metrics , we take the median from 3 independent annotations .
We use US - based annotators on Amazon ’s Mechanical Turk6 and pay $ 0.03 per annotation , and expect each annotation to take 10 seconds on average ( effectively , the hourly rate is about $ 11 ) .
See Appendix A.2 for more details .
As shown in Table 4 , the naturalness score only drop slightly after the perturbation , indicating that our constructed vulnerable examples have similar naturalness as the original examples .
As for the semantic similarity , we observe that 85 % of the original examples maintain the same meaning after the synonym substitution , and the corresponding ratio is 71 % for vulnerable examples .
This indicates that the synonym substitution is an invariance transformation for most examples .
4 Evaluating Counterfactual Bias
In addition to evaluating second - order robustness , we further extend the double perturbation framework ( § 2 ) to evaluate counterfactual biases by setting p to pairs of protected tokens .
We show that our method can reveal the hidden model bias .
4.1 Counterfactual Bias
In contrast to second - order robustness , where we consider the model vulnerable as long as there exists one vulnerable example , counterfactual bias focuses on the expected prediction , which is the average prediction among all examples within the neighborhood .
We consider a model biased if the
6https://www.mturk.com
x
x ⊕ p
x
x ⊕ p
Figure 4 : An illustration of an unbiased model vs. a biased model .
Green and gray indicate the probability of positive and negative predictions , respectively .
Left : An unbiased model where the ( x , x ⊕ p ) pair ( yellowred dots ) is relatively parallel to the decision boundary .
Right : A biased model where the predictions for x ⊕ p ( red ) are usually more negative ( gray ) than x ( yellow ) .
expected predictions for protected groups are different ( assuming the model is not intended to discriminate between these groups ) .
For instance , a sentiment classiﬁer is biased if the expected prediction for inputs containing woman is more positive ( or negative ) than inputs containing man .
Such bias is harmful as they may make unfair decisions based on protected attributes , for example in situations such as hiring and college admission .
Counterfactual token bias .
We study a narrow case of counterfactual bias , where counterfactual examples are constructed by substituting protected tokens in the input .
A naive approach of measuring this bias is to construct counterfactual examples directly from the test set , however such evaluation may not be robust since test examples are only a small subset of natural sentences .
Formally , let p be a pair of protected tokens such as ( he , she ) or ( Asian , American ) , Xtest ⊆
Xp be a test set ( as in § 2.1 ) , we deﬁne counterfactual token bias by :
Bp , k : =
E x∈Neighbork(Xtest )
Fsoft(x ; p ) .
( 5 )
We calculate Eq .
( 5 ) through an enumeration across all natural sentences within the neighborhood.7 Here Neighbork(Xtest ) = ( cid:83 ) Neighbork(x ) denotes the union of neighborhood examples ( of distance k ) around the test set , and Fsoft(x ; p ) : X × V 2 → [ −1 , 1 ] denotes the difference between probability outputs fsoft ( similar to Eq .
( 2 ) ):
x∈Xtest
Fsoft(x ; p ) : = fsoft(x ⊕ p ) − fsoft(x ) .
( 6 )
7For gender bias , we employ a blacklist to avoid adding gendered tokens during the neighborhood construction .
This is to avoid semantic shift when , for example , p = ( he , she ) such that it may refer to different tokens after the substitution .
3905  Patch Words
# Original
# Perturbed
he , she his , her him , her men , women man , woman actor , actress . . .
Total
5 4 4 3 3 2
325,401 255,245 233,803 192,504 222,981 141,780
34
2,317,635
Table 5 : The number of original examples ( k = 0 ) and the number of perturbed examples ( k = 3 ) in Xﬁlter .
The model is unbiased on p if Bp , k ≈ 0 , whereas a positive or negative Bp , k indicates that the model shows preference or against to p(2 ) , respectively .
Fig .
4 illustrates the distribution of ( x , x ⊕ p ) for both an unbiased model and a biased model .
The aforementioned neighborhood construction does not introduce additional bias .
For instance , let x0 be a sentence containing he , even though it is possible for Neighbor1(x0 ) to contain many stereotyping sentences ( e.g. , contains tokens such as doctor and driving ) that affect the distribution of fsoft(x ) , but it does not bias Eq .
( 6 ) as we only care about the prediction difference of replacing he → she .
The construction has no information about the model objective , thus it would be difﬁcult to bias fsoft(x ) and fsoft(x ⊕ p ) differently .
Figure 5 : Our proposed Bp , k measured on Xﬁlter .
Here “ original ” is equivalent to k = 0 , “ perturbed ” is equivalent to k = 3 , p is in the form of ( male , female ) .
Metrics .
We evaluate model bias through the proposed Bp , k for k = 0 , . . .
, 3 .
Here the bias for k = 0 is effectively measured on the original test set , and the bias for k ≥ 1 is measured on our constructed neighborhood .
We randomly sample a subset of constructed examples when k = 3 due to the exponential complexity .
Filtered test set .
To investigate whether our method is able to reveal model bias that was hidden in the test set , we construct a ﬁltered test set on which the bias can not be observed directly .
Let Xtest be the original validation split , we construct Xﬁlter by the equation below and empirically set ( cid:15 ) = 0.005 .
We provide statistics in Table 5 .
4.2 Experimental Results
Xﬁlter : = { x | |Fsoft(x ; p)| < ( cid:15 ) , x ∈ Xtest } .
In this section , we use gender bias as a running example , and demonstrate the effectiveness of our method by revealing the hidden model bias .
We provide additional results in Appendix A.4 .
4.2.1
Setup We evaluate counterfactual token bias on the SST-2 dataset with both the base and debiased models .
We focus on binary gender bias and set p to pairs of gendered pronouns from Zhao et al ( 2018a ) .
Base Model .
We train a single layer LSTM with pre - trained GloVe embeddings and 75 hidden size ( from TextAttack , Morris et al 2020 ) .
The model has 82.9 % accuracy similar to the baseline performance reported in GLUE .
Debiased Model .
Data - augmentation with gender swapping has been shown effective in mitigating gender bias ( Zhao et al , 2018a , 2019 ) .
We augment the training split by swapping all male entities with the corresponding female entities and vice - versa .
We use the same setup as the base LSTM and attain 82.45 % accuracy .
4.2.2 Results Our method is able to reveal the hidden model bias on Xﬁlter , which is not visible with naive measurements .
In Fig .
5 , the naive approach ( k = 0 ) observes very small biases on most tokens ( as constructed ) .
In contrast , when evaluated by our double perturbation framework ( k = 3 ) , we are able to observe noticeable bias , where most p has a positive bias on the base model .
This observed bias is in line with the measurements on the original Xtest ( Appendix A.4 ) , indicating that we reveal the correct model bias .
Furthermore , we observe mitigated biases in the debiased model , which demonstrates the effectiveness of data augmentation .
To demonstrate how our method reveals hidden bias , we conduct a case study with p = ( actor , actress ) and show the relationship between the bias Bp , k and the neighborhood distance k.
We present the histograms for Fsoft(x ; p ) in Fig .
6 and plot the corresponding Bp , k vs. k in the right - most panel .
Surprisingly , for the base model , the bias is
3906  Figure 6 : Left and Middle : Histograms for Fsoft(x ; p ) ( x - axis ) with p = ( actor , actress ) .
Right :
The plot for the average Fsoft(x ; p ) ( i.e. , counterfactual token bias ) vs. neighborhood distance k. Results show that the counterfactual bias on p can be revealed when increasing k.
negative when k = 0 , but becomes positive when k = 3 .
This is because the naive approach only has two test examples ( Table 5 ) thus the measurement is not robust .
In contrast , our method is able to construct 141,780 similar natural sentences when k = 3 and shifts the distribution to the right ( positive ) .
As shown in the right - most panel , the bias is small when k = 1 , and becomes more signiﬁcant as k increases ( larger neighborhood ) .
As discussed in § 4.1 , the neighborhood construction does not introduce additional bias , and these results demonstrate the effectiveness of our method in revealing hidden model bias .
5 Related Work
First - order robustness evaluation .
A line of work has been proposed to study the vulnerability of natural language models , through transformations such as character - level perturbations ( Ebrahimi et al , 2018 ) , word - level perturbations ( Jin et al , 2019 ;
Ren et al , 2019 ; Yang et al , 2020 ; Hsieh et al , 2019 ; Cheng et al , 2020 ; Li et al , 2020 ) , prepending or appending a sequence ( Jia and Liang , 2017 ; Wallace et al , 2019a ) , and generative models ( Zhao et al , 2018b ) .
They focus on constructing adversarial examples from the test set that alter the prediction , whereas our methods focus on ﬁnding vulnerable examples beyond the test set whose prediction can be altered .
Robustness beyond the test set .
Several works have studied model robustness beyond test sets but mostly focused on computer vision tasks .
Zhang et al ( 2019 ) demonstrate that a robustly trained model could still be vulnerable to small perturbations if the input comes from a distribution only slightly different than a normal test set ( e.g. , images with slightly different contrasts ) .
Hendrycks and Dietterich ( 2019 ) study more sources of common corruptions such as brightness , motion blur and fog .
Unlike in computer vision where simple
image transformations can be used , in our natural language setting , generating a valid example beyond test set is more challenging because language semantics and grammar must be maintained .
Counterfactual fairness .
Kusner et al ( 2017 ) propose counterfactual fairness and consider a model fair if changing the protected attributes does not affect the distribution of prediction .
We follow the deﬁnition and focus on evaluating the counterfactual bias between pairs of protected tokens .
Existing literature quantiﬁes fairness on a test dataset or through templates ( Feldman et al , 2015 ; Kiritchenko and Mohammad , 2018 ; May et al , 2019 ; Huang et al , 2020 ) .
For instance , Garg et al ( 2019 ) quantify the absolute counterfactual token fairness gap on the test set ; Prabhakaran et al ( 2019 ) study perturbation sensitivity for named entities on a given set of corpus .
Wallace et
al ( 2019b ) ; Sheng et al ( 2019 , 2020 ) study how language generation models respond differently to prompt sentences containing mentions of different demographic groups .
In contrast , our method quantiﬁes the bias on the constructed neighborhood .
6 Conclusion
This work proposes the double perturbation framework to identify model weaknesses beyond the test dataset , and study a stronger notion of robustness and counterfactual bias .
We hope that our work can stimulate the research on further improving the robustness and fairness of natural language models .
Acknowledgments
We thank anonymous reviewers for their helpful feedback .
We thank UCLA - NLP group for the valuable discussions and comments .
The research is supported NSF # 1927554 , # 1901527 , # 2008173 and # 2048280 and an Amazon Research Award .
3907  Ethical Considerations
Intended use .
One primary goal of NLP models is the generalization to real - world inputs .
However , existing test datasets and templates are often not comprehensive , and thus it is difﬁcult to evaluate real - world performance ( Recht et al , 2019 ;
Ribeiro et al , 2020 ) .
Our work sheds a light on quantifying performance for inputs beyond the test dataset and help uncover model weaknesses prior to the realworld deployment .
Misuse potential .
Similar to other existing adversarial attack methods ( Ebrahimi et al , 2018 ; Jin et al , 2019 ; Zhao et al , 2018b ) , our second - order attacks can be used for ﬁnding vulnerable examples to a NLP system .
Therefore , it is essential to study how to improve the robustness of NLP models against second - order attacks .
Limitations .
While the core idea about the double perturbation framework is general , in § 4 , we consider only binary gender in the analysis of counterfactual fairness due to the restriction of the English corpus we used , which only have words associated with binary gender such as he / she , waiter / waitress , etc .
