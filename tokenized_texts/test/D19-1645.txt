Cross - Sentence N -ary Relation Extraction using Lower - Arity Universal Schemas
Kosuke Akimoto1 , Takuya Hiraoka1 , Kunihiko Sadamasa1 , Mathias Niepert2 1 Security Research Laboratories , NEC Corporation 2 NEC Laboratories Europe
k - akimoto@ab , t - hiraoka@ce , k - sadamasa@az }
mathias.niepert@neclab.eu
{
.jp.nec.com
Abstract
Most existing relation extraction approaches exclusively target binary relations , and n - ary relation extraction is relatively unexplored .
Current state - of - the - art n - ary relation extraction method is based on a supervised learning approach and , therefore , may suffer from the lack of sufﬁcient relation labels .
In this paper , we propose a novel approach to cross - sentence n - ary relation extraction based on universal schemas .
To alleviate the sparsity problem and to leverage inherent decomposability of n - ary relations , we propose to learn relation representations of lower - arity facts that result from decomposing higher - arity facts .
The proposed method computes a score of a new nary fact by aggregating scores of its decomposed lower - arity facts .
We conduct experiments with datasets for ternary relation extraction and empirically show that our method improves the n - ary relation extraction performance compared to previous methods .
1
Introduction
Relation extraction is a core natural language processing task which is concerned with the extraction of relations between entities from text .
It has numerous applications ranging from question answering ( Xu et al , 2016 ) to automated knowledge base construction ( Dong et al , 2014 ) .
While the vast majority of existing research focuses on extracting binary relations , there exists only few recent approaches to extract n - ary relations , that is , relations among n 2 entities ( Li et al , 2015 ; Ernst et al , 2018 ) .
In n - ary relation extraction , relation mentions tend to span multiple sentences more frequently as n increases .
Thus , Peng et al ( 2017 ) recently extended the problem to cross - sentence n - ary relation extraction in which n - ary relations are extracted from multiple sentences .
As a motivating example , consider the following text from Wikipedia : “ Revis
≥
started off the 2009 season matched up against some of football ’s best wide receivers .
In Week 1 , he helped limit Houston Texans Pro - bowler Andre Johnson to four receptions for 35 yards . ”
In this example , two sentences collectively describes that Andre Johnson is a player of the football team the Texans during 2009 season , and thus we need cross - sentence information to correctly extract this ternary interaction among the three entities , i.e. Player(Andre Johnson , Texans , 2009 season ) .
Previous methods ( Peng et al , 2017 ; Song et al , 2018 ) capture cross - sentence n - ary relation mentions by representing texts with a document graph which consists of both intra- and cross - sentence links between words .
With this graphical representation , they applied graph neural networks to predict ternary relations in the medical domain .
However , these methods train the neural networks in a supervised manner using distant supervision ( Mintz et al , 2009 ) and , therefore , may suffer from the lack of sufﬁcient positive labels when a well - populated knowledge base is not available .
On the other hand , for binary relation extraction , the problem of insufﬁcient positive labels can be mitigated with universal schemas ( Riedel et al , 2013 ) .
In a universal schema approach , textual representations ( surface patterns ) of entities and their relations are encoded into the same vector space as the canonical knowledge base relations .
Thus , semantically similar surface patterns can share information of relation labels in a semisupervised manner .
This reduces the amount of required labeled training data .
Applying the universal schema approach to n - ary ( n > 2 ) relation extraction is , however , not straight - forward due to the sparsity of higher - order relation mentions among a speciﬁc set of n > 2 entities.1 This is be1In our Wiki-90k dataset ( see § 4.1 ) , only 12.5 % of ternary entity tuples have at least two relations among the entities , while 77.3 % of entities appear at least twice .
Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing , pages6225–6231,HongKong , China , November3–7,2019.c(cid:13)2019AssociationforComputationalLinguistics6225  cause the universal schema approach ( Riedel et al , 2013 ) and its extensions ( Toutanova et al , 2015 ; Verga et al , 2016 , 2017 ) utilize co - occurring patterns of relation types between speciﬁc pair of entities .
Also , prior work has only addressed binary relations , and it is not trivial to deﬁne surface patterns among n > 2 entities and to encode these patterns into a vector representation .
To mitigate the aforementioned sparsity problem and utilize existing encoders for binary and unary surface patterns , we propose to train universal schema models on more dense lower - arity ( unary and binary ) facts instead of original sparse n - ary facts .
Since most n - ary relations can be decomposed into a set of k - ary relations ( k = 1 , 2 ) which are implied by the n - ary relation,2 we can easily acquire lower - arity facts by decomposing n - ary facts .
Our model learns representations of these lower - arity relations using the universal schema framework , and predicts new n - ary facts by aggregating scores of lower - arity facts .
To evaluate the proposed method , we create new cross - sentence n - ary relation extraction datasets with multiple ternary relations.3 The new datasets contain more entity tuples with known relational facts appeared in a knowledge base than the existing dataset ( Peng et al , 2017 ) , and , therefore , these datasets can be used to more effectively evaluate methods which predict relation labels for each individual entity tuple .
We show empirically that by jointly training lower - arity models and an nary score aggregation model , the proposed method improves the performance of n - ary relation extraction .
To the best of our knowledge , this is the ﬁrst attempt to apply universal schemas to n - ary relation extraction , taking advantage of the compositionality of higher - arity facts .
2 Task Deﬁnition and Notation
O
R
{ ( cid:104 )
KB =
r , ( e1 , ... , en )
The cross - sentence n - ary relation extraction task ( Peng et al , 2017 ) is deﬁned as follows .
Let KB be a set of relation , and , r
E types of an external knowledge base : r(e1 , ... , en )
be a set of entities ,
∈ relation AwardedFor(director , movie , award ) can be decomposed into the binary relations DirectorOf(director , movie ) and WonAward(director , award ) .
Note that a similar idea is introduced in ( Ernst et al , 2018 ) as partial facts or partial patterns .
3Our
KB ∈ KB
availhttps://github.com/aurtg/
able nary - relation - extraction - decomposed .
codes at
example ,
datasets
ternary
2For
and
the
are
( cid:105 )
Figure 1 : An overview of the proposed method .
Figure 2 : An example of decomposed textual facts .
O
{ ( cid:104 )
KB
∈ E
KB , ei
∈ E }
∈ R
is mentioned in T
T , ( e1 , ... , en ) ( cid:105 )
.
We colbe the set of facts in R lect a set of candidate entity tuples among which KB possibly holds.4 Here , KB relation r all entities in each candidate tuple ( e1 , ... , en ) are mentioned in the same text section T in a given set of documents .
We deﬁne a set of these : text = entity mentions as ei .
Here , text section T } is a ( short ) span in a document which can deIn the scribes relational facts among entities .
cross - sentence n - ary relation extraction task , text section T can contain multiple sentences .
In this paper , following ( Peng et al , 2017 ) , we deﬁne 1 ) which conM consecutive sentences ( M tain n target entities as a text section in the crosssentence n - ary relation extraction task .
We use the term “ relation ” to refer to both relations r and sections T .
∈ R
≥
KB
The goal of the cross - sentence n - ary relation extraction task is to predict new facts / r , ( e1 , ... , en ) KB given ∈ O ( cid:105 ) ( cid:104 ) text ,
where n KB O ∪ O 3 Proposed Method
KB for relation r
∈ R
O
≥
=
2 .
3.1 Lower - Arity Facts
To alleviate the sparsity problem of facts among n entities ( n > 2 ) and to utilize well - studied encoders for binary and unary surface patterns ,
we
4It is allowed that multiple KB relations hold among the
same set of entities .
6226nmodcompoundaclcompound  decompose a set of original n - ary facts , set of unary facts ( Figure 1 ) .
1 and a set of binary facts
O
O
, into a
2
O
( cid:105 )
Given
( cid:105 ) ∈ O
an n - ary
: k = 1 , ... , n
Unary Facts :
fact r , ( e1 , ... , ek ) , we decompose it into a ( cid:104 ) r(k ) , ek set of n unary facts , } { ( cid:104 ) where r(k ) is a tentative unary relation w.r.t .
the k - th argument of the original relation r.
If r is a KB relation , we deﬁne unary relation r(k ) as a new canonicalized relation .
If r is section T , we deﬁne unary relation r(k ) as a tuple r(k ) = ( T , pos(ek ) ) , where pos(ek ) is a set of word position indices of entity ek in section T ( Figure 2 ) .
We denote a set of all decomposed unary facts by 1 .
Intuitively , these unary relations represent semantic roles or types of corresponding arguments of the original relation r ( Yao et al , 2013 ) .
O
O
= l
Given
, we
an n - ary
Binary Facts :
− : k , l = 1 , ... , n , k
r , ( e1 , ... , ek ) ( cid:105 ) ( cid:104 ) ∈ set of n(n into a it r(k , l ) , ( ek , el ) ( cid:105 )
fact decompose 1 ) binary facts , { ( cid:104 ) } where r(k , l ) is a tentative binary relation between the k - th and l - th argument of the original relation r.
If r is a KB relation , we deﬁne binary relation r(k , l ) as a new canonicalized relation .
If r is a section T , we represent it by the shortest path between ek and el on the document graph ( Quirk and Poon , 2017 ) of T ( Figure 2 ) , and denote it by path(T ; ek , el ) .
We denote the set of all decomposed binary facts by
2 .
O
3.2 Lower - Arity Relation Representations
∈ 1 or
Rdr for We learn a vector representation v(r ) each unary or binary relation in 2 .
For r(k ) or r(k , l ) derived from a KB relation , we represent it by a trainable parameter vector .
On the other hand , for the one derived from a textual relation , we use the following encoders to compute its representations .
O
O
Unary encoder : For an unary textual relation r(k ) = ( T , pos(ek ) ) , we represent each section T by a sequence of word vectors and use a bidirectional LSTM ( Bi - LSTM )
( Schuster and Paliwal , 1997 ) to compute a hidden representation Rdr at each word position l. Following rehl cent works ( Zhang et al , 2018 ; He et al , 2018 ; Lee et al , 2017 ) , we aggregate hl within a phrase of entity ek to compute v(T ( k ) ) .
We use elementwise mean as aggregation function :
∈
r(k , l ) = path(T ; ek , el ) , we represent each token ( word or edge label ) in path(T ; ek , el ) by an embedding vector ( Toutanova et al , 2015 ; Verga et al , 2016 ) .
We use a Bi - LSTM to compute a Rdr at each token pohidden representation h(cid:48 ) l ∈ sition l , and max - pool along the path to compute the relation representation :
v(T ( k , l ) )
= max (
l : l = 1 , ... , L
)
.
( 2 )
} 3.3 Learning Relation Representations
h(cid:48 ) {
( cid:104 )
∪ O
( cid:105 ) ∈ O
( cid:105 ) ∈ O
1 ) or an entity tuple ( if
We follow Verga et al ( 2017 ) to train relation rep3.2 ) .
We deﬁne a score θ(cid:104)r , p(cid:105 ) for resentations ( § r , p each lower - arity fact 2 , and mini1 mize the following loss ( 3 ) for each arity i = 1 , 2 .
Here , placeholder p refers to either an entity ( if r , p 2 ) , ( cid:104 ) and we simply refer to both as entity tuple .
The loss functions contrast a score of an original fact r , p+ i and those of K sampled negative ( cid:104 ) /
i. We sample negative facts by facts ∈ O randomly replacing entity tuple p+ in the original fact by different entity tuples p− k .
exp(θ(cid:104)r , p+(cid:105 ) )
( cid:105 ) ∈ O r , p− k ( cid:105 ) ( cid:104 )
( cid:105 ) ∈ O
r , p
( cid:104 )
log (
exp(θ(cid:104)r , p+(cid:105))+(cid:80 )
k exp(θ
)
) ]
.
( cid:104)r , p− k
( cid:105 )
i =
L
E [ − ( cid:104)r , p+(cid:105)∈Oi ( cid:104)r , p− k ( cid:105 ) /∈Oi
The score of fact
( 3 ) is deﬁned as θ(cid:104)r , p(cid:105 )
= r , p ( cid:105 ) ( cid:104 ) v(r)T v(p ; r ) .
Entity tuple representations v(p ; r ) are computed with a weighted average of the representations as shown in ( 4 ) ∈ and ( 5 ) where a(r(cid:48 ) , r ; V ( p ) ) is the attention weight for each relation r(cid:48 ) V ( p).5
v(r(cid:48 ) ) : r(cid:48 ) {
V ( p ) }
v(p ; r ) =
a(r(cid:48 ) , r ; V ( p))v(r(cid:48 ) ) ,
∈
( cid:88 )
r(cid:48)∈V ( p )
a(r(cid:48 ) , r ; V ( p ) )
=
exp(v(r(cid:48))T v(r ) )
r(cid:48)(cid:48)∈V ( p ) exp(v(r(cid:48)(cid:48))T v(r ) )
( cid:80 )
( 4 ) .
V ( p ) =
( cid:40 ) { {
r : r :
( cid:104 ) ( cid:104 )
r , ek 1 ( cid:105 ) ∈
O r , ( ek , el )
} 2 ( cid:105 ) ∈ O
}
( if p = ek ) ( if p = ( ek , el ) )
.
( 5 )
3.4 Aggregating Lower - Arity Scores
To predict n - ary facts of KB relation r KB , we compute its score θ(cid:104)r,(e1, ... ,en)(cid:105 ) by aggregating lower - arity scores as in ( 6 ) , where w ( · ) is a positive r scalar weight deﬁned for each KB relation which sum to one : ( cid:80 )
r + ( cid:80 )
r = 1 .
k(cid:54)=l w(k , l )
k w(k )
∈ R
v(r(k ) )
= mean ( {
hl : l
pos(ek ) }
)
.
∈
( 1 )
Binary encoder :
For a binary textual relation
5During training , as in ( Verga et al , 2017 ) , we aggregate sub - sampled M relations from V ( p ) if |V ( p)| > M .
We set M = 2 for all experiments .
6227(cid:54 )  ( 6 )
4.2 Baselines
r
and w(k , l ) r
We can set al weights w(k )
to 1 / n2 , or train these weights to give higher scores to positive n - ary facts by minimizing additional loss n directly contrasts n - ary function scores associated with KB relations r KB in a 2.6 more supervised manner than both
n. Note that
∈ R 1 and
L
L
L
L
θ(cid:104)r,(e1, ... ,en)(cid:105 )
=
n ( cid:88 )
k=1
w(k )
r θ(cid:104)r(k),ek(cid:105 ) +
w(k , l ) r
θ(cid:104)r(k , l),(ek , el)(cid:105 ) .
( cid:88 )
k=1, ... ,n l=1, ... ,n k(cid:54)=l
n =
L
E ( cid:104)r , p+(cid:105)∈OKB ( cid:104)r , p−(cid:105 ) /∈OKB
[ max(0 , 1
θ(cid:104)r , p+(cid:105 ) + θ(cid:104)r , p−(cid:105 ) ) ]
−
( 7 ) 2 + The overall loss function is now α n.
By changing α , we can balance the semisupervised effect of lower - arity universal schemas 2 ) and that of the supervision with n - ary re ( L L lation labels (
1 +
1 ,
=
L
L
L
L
n ) .
L
4 Experiments
4.1 Dataset
The cross - sentence n - ary relation extraction dataset from Peng et al ( 2017 ) contains only 59 distinct ternary KB facts including the train and test set .
Since our proposed method and universal schemas baselines predict KB relations for each entity tuple instead of each surface pattern , the number of known facts of KB relations is crucial to reliably evaluate and compare these methods .
Thus , we created two new n - ary cross - sentence relation extraction datasets ( dubbed with Wiki-90k and WF-20k ) that contain more known facts retrieved from public knowledge bases .
To create the Wiki-90k and WF-20k datasets , we used Wikidata and Freebase respectively as external knowledge bases .
Since these knowledge bases store only binary relational facts , we deﬁned multiple ternary relations by combining a few binary relations.7,8 For both datasets , we collected paragraphs from the English Wikipedia , and used Stanford CoreNLP ( Manning et al , 2014 ) to
extract dependency and co - reference links .
Entity mentions are detected using DBpedia Spotlight ( Daiber et al , 2013 ) .
We followed ( Peng et al , 2017 ) to extract co - occurring entity tuples and their surface patterns , that is , we selected tuples which occurred in a minimal span within at most M 3 consecutive sentences .
Entity tuples without a known KB relation are subsampled , since the number of such tuples are too large .
We randomly partitioned all entity tuples into train , development ( dev ) , and test sets .
≤
( Song et al , 2018 ): The state - of - the - art crosssentence n - ary relation extraction method proposed by Song et al ( 2018 ) represents each surface pattern by the concatenation of entity vectors from the last layer of a Graph State LSTM , a variant of a graph neural network .
The concatenated vector is then fed into a classiﬁer to predict the relation label .
Since their method directly predicts a relation label for each surface pattern , it is more robust to the sparsity of surface patterns among a speciﬁc higher arity entity tuple .
However , due to their purely supervised training objective , its performance may degrade if the number of available training labels is small .
Universal schemas : We compared our method with semi - supervised methods based on universal schemas ( Toutanova et al , 2015 ; Verga et al , 2017 ) .
In our experiments , we used the same encoder as ( Song et al , 2018 ) to encode each surface pattern.9 We tested two types of scoring functions , Model F and Model E , as in ( Toutanova et al , 2015).10,11
4.3 Evaluation
We compared the methods in the held - out evaluation as in ( Mintz et al , 2009 ) and report ( weighted ) mean average precision ( MAP ) ( Riedel et al , 2013 ) .
Unless otherwise noted , reported values are average values over six experiments , in which network parameters are randomly initialized .
All reported p - values are calculated based on Wilcoxon rank sum test ( Wilcoxon , 1945 ) with
6The loss function ( 7 ) performs better than a loglikelihood based loss , − log σ(θ(cid:104)r , p+(cid:105 ) − θ(cid:104)r , p−(cid:105 ) ) .
7See the appendix A for details about deﬁned ternary relations .
8For about half of the deﬁned ternary relations , combined original binary relations in KB are different from decomposed binary relations of the ternary relations in the proposed method ( § 3.1 ) .
9Using linear projection instead of simple concatenation did not improve performance in our preliminary experiments .
10It is not trivial to apply Model E scoring function with Verga et al ( 2017 ) method , since their aggregation method calculates a representation for each row , i.e. entity tuple .
11DistMult scoring function in ( Toutanova et al , 2015 ) showed poor performance compared to the other two scoring functions in our preliminary experiments .
6228  Method
Wiki-90k average weighted 0.584 0.471
0.634 0.536
average 0.821 0.639
WF-20k
weighted 0.842 0.680
Proposed ( Song et al , 2018 ) ( Toutanova et al , 2015 ) with Graph State LSTM ( Song et al , 2018 ) encoder
Model F Model E
0.240 0.399
0.262 0.414
0.341 0.725
( Verga et al , 2017 ) with Graph State LSTM ( Song et al , 2018 ) encoder
Model F
0.443
0.482
0.610
0.380 0.752
0.653
bold : p ≤ 0.01 Table 1 : Mean average precisions ( MAPs ) on test data .
multiple - test adjustment using Holm ’s method ( Holm , 1979 ) .
4.4 Results
Table 1 illustrates the performance of each method.12 Compared to the baseline methods , our proposed method achieves higher weighted MAP for both datasets .
Interestingly , Model F performs well in Verga et al ( 2017 ) baseline , while it shows low performance in Toutanova et al ( 2015 ) baseline .
L
L
2 , and α
n respectively .
Ablation Study : Table 2 illustrates the performance of various settings of our proposed method .
U , B , and N stand for using the loss functions 1 , In the result , U+B L performs signiﬁcantly better ( p < 0.005 ) than U and B , and this shows effectiveness of combining scores of both binary facts and unary facts .
On the other hand , there was no signiﬁcant difference between U+B+N and N ( p > 0.9 ) .
Note that we used all positive labels in this experiment , that is , sufﬁcient amount of positive labels are used for calculating the loss N.
L
1 +
∞ 2 + α
n instead of
Data efﬁciency : Furthermore , we also investigated the inﬂuence of the training data size ( the number of positive labels ) of our proposed method and baseline methods.13 Here , α = stands for optimizing n. As shown in Figure 3 , α = 1 achieved higher performance than α = , showing that introducing lower - arity semi - supervised loss ( 2 ) improves the performance for dataset with few positive labels .
On the other hand , the lower performance of α = 0 compared to α = 0.1 , 1 suggests that information of higher - arity facts introduced n is beneﬁtial for n - ary relation extraction .
from
1 +
∞
L
L
L
L
L
L
5 Conclusion and Future Works
We proposed a new method for cross - sentence nary relation extraction that decomposes sparse n12For the proposed method , we set α = 10 .
13In
this experiment , we conducted four experiments per
each setting and set K = 10 .
Setting U B N U+B U+B+N U+B+N ( K = 10 ) U+B+N ( K = 5 ) U+B+N ( ﬁx wr = 1 )
average weighted 0.363 0.411 0.646 0.521 0.646
0.404 0.452 0.685 0.552 0.682
0.650
0.689
0.592
0.640
0.645
0.679
Default hyperparameter : K = 20 Table 2 : Ablation study : mean average precisions ( MAPs ) on dev data ( Wiki-90k ) .
Figure 3 : Weighted MAP on test data with missing labels ( Wiki-90k ) .
ary facts into dense unary and binary facts .
Experiments on two datasets with multiple ternary relations show that our proposed method can statistically signiﬁcantly improve over previous works , which suggests the effectiveness of using unary and binary interaction among entities in surface patterns .
However , as Fatemi et al ( 2019 ) suggests , there exists cases in which reconstructing n - ary facts from decomposed binary facts induces false positives .
Tackling this issue is one important future research direction .
