Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 219–228 , Hong Kong , China , November 3–7 , 2019 .
c  2019 Association for Computational Linguistics219Open Relation Extraction : Relational Knowledge Transfer from Supervised Data to Unsupervised Data Ruidong Wu1∗ ,
Yuan Yao1∗,X uH a n1 , Ruobing Xie2 , Zhiyuan Liu1† , Fen Lin2 , Leyu Lin2 , Maosong Sun1 1Department of Computer Science and Technology , Tsinghua University , Beijing , China Institute for Artiﬁcial Intelligence , Tsinghua University , Beijing , China State Key Lab on Intelligent Technology and Systems , Tsinghua University , Beijing , China 2Search Product Center , WeChat Search Application Department , Tencent , China mooninsiderain@gmail.com
Abstract Open relation extraction ( OpenRE ) aims to extract relational facts from the open - domaincorpus .
To this end , it discovers relation pat - terns between named entities and then clustersthose semantically equivalent patterns into aunited relation cluster .
Most OpenRE meth - ods typically conﬁne themselves to unsuper - vised paradigms , without taking advantage ofexisting relational facts in knowledge bases ( KBs ) and their high - quality labeled instances .
To address this issue , we propose RelationalSiamese Networks ( RSNs ) to learn similar - ity metrics of relations from labeled data ofpre - deﬁned relations , and then transfer therelational knowledge to identify novel relations in unlabeled data .
Experiment results on two real - world datasets show that our frame - work can achieve signiﬁcant improvements ascompared with other state - of - the - art methods .
Our code is available at https://github . com / thunlp / RSN .
1 Introduction Relation extraction ( RE ) aims to extract relational facts between two entities from plain texts .
Forexample , with the sentence “ Hayao Miyazaki is the director of the ﬁlm ‘ The Wind Rises ’ " , we can extract a relation “ director_of " between two entities “ Hayao Miyazaki " and “ The Wind Rises " .
Recent progress in supervised methods to RE has achieved great successes .
Supervised meth - ods can effectively learn signiﬁcant relation se - mantic patterns based on existing labeled data , but the data constructions are time - consuming andhuman - intensive .
To lower the level of super - vision , several semi - supervised approaches havebeen developed , including bootstrapping , activelearning , label propagation ( Pawar et al . , 2017 ) .
∗indicates equal contribution †Corresponding author : Z.Liu(liuzy@tsinghua.edu.cn)Relational Siamese NetworkNovel Relations ( Unlabeled ) Pre - defined Relations ( Auto - labeled / Labeled ) 1 … 2 … n … … Testing Instances Novel Relations ( Extracted ) n+1 … n+2 … n+m … … clustering … every rectangle indicates an unlabeled instance of a   novel relation … metric learning every rectangle indicates   a pre - defined relation   instancerelational   knowledge transfer Figure 1 : A ﬂowchart of our framework .
Our model RSN learns from both labeled instances of pre - deﬁnedrelations and unlabeled instances of new relations , andtries to cluster testing instances of new relations .
1 Mintz ( 2009 ) also proposes distant supervision to generate training data automatically .
It assumesthat if two entities have a relation in KBs , all sen - tences that contain these two entities will expressthis relation .
Still , all these approaches can onlyextract pre - deﬁned relations that have already ap - peared either in human - annotated datasets or KBs .
It is hard for them to cover the great variety ofnovel relational facts in the open - domain corpora .
Open relation extraction ( OpenRE ) aims to extract relational facts on the open - domain cor - pus , where the relation types may not be pre - deﬁned .
There are some efforts concentrating onextracting triples with new relation types .
Banko(2008 ) directly extracts words or phrases in sentences to represent new relation types .
However , some relations can not be explicitly represented with tokens in sentences , and it is hardto align different relational tokens that exactlyhave the same meanings .
Yao ( 2011 ) consid1To highlight our model ’s ability to extract new relations , testing instances only contain new relations .
220ers OpenRE as a clustering task for extracting triples with new relation types .
However , previ - ous clustering - based OpenRE methods ( Yao et al . , 2011 , 2012 ; Marcheggiani and Titov , 2016 ; Elsahar et al . , 2017 ) are mostly unsupervised , and can not effectively select meaningful relation patternsand discard irrelevant information .
In this paper , we propose to take advantage of high - quality supervised data of pre - deﬁned rela - tions for OpenRE .
The approach is non - trivial , however , due to the considerable gap between thepre - deﬁned relations and novel relations of inter - est in open domain .
To bridge the gap , we proposeRelational Siamese Networks ( RSNs ) to learn transferable relational knowledge from superviseddata for OpenRE .
Speciﬁcally , RSNs learn re - lational similarity metrics from labeled data ofpre - deﬁned relations , and then transfer the metrics to measure the similarity of unlabeled sentences for open relation clustering .
We describethe ﬂowchart of our framework in Figure 1 .
Moreover , we show that RSNs can also be generalized to various weakly - supervised scenar - ios .
We propose Semi - supervised RSN to learn from both supervised data of pre - deﬁned rela - tions and unsupervised data with novel relations , and Distantly - supervised RSN to learn from distantly - supervised data and unsupervised data .
We conduct experiments on real - world RE datasets , FewRel and FewRel - distant , by split - ting relations into seen and unseen set , and eval - uate our models in supervised , semi - supervised , and distantly - supervised scenarios .
The resultsdemonstrate that our models signiﬁcantly outper - form state - of - the - art baseline methods in all sce - narios without using external linguistic tools .
Tosummarize , the main contributions of this work areas follows : ( 1 ) We develop a novel relational knowledge transfer framework RSN for OpenRE , which caneffectively transfer existing relational knowledgeto novel - relation data and accurately identifynovel relations .
To the best of our knowledge , RSN is the ﬁrst model to consider knowledgetransfer in clustering - based OpenRE task .
( 2 ) We further propose Semi - supervised RSNs and Distantly - supervised RSNs that can learnfrom various weakly supervised scenarios .
Theexperimental results show that all these RSN mod - els achieve signiﬁcant improvements in F - measurecompared with state - of - the - art baselines.2 Related Work Open Relation Extraction .
Relation extraction ( RE ) is an important task in NLP .
Traditional REmethods mainly concentrate on classifying rela - tional facts into pre - deﬁned relation types ( Mintz et al . , 2009 ; Y u et al . , 2017 ) .
Zeng ( 2014 ) utilizes CNN encoders to build sentence representationswith the help of position embeddings .
Lin ( 2016 ) further improves RE performance on distantly - supervised data via instance - level attention .
Thesemethods take advantage of supervised or distantly - supervised data to learn neural sentence encodersfor distributed representations , and have achievedpromising results .
However , these methods can - not handle the open - ended growth of new relationtypes in the open - domain corpora .
To solve this problem , recently many efforts have been invested in exploring methods foropen relation extraction ( OpenRE ) , which aimsto discover new relation types from unsupervisedopen - domain corpora .
OpenRE methods can beroughly divided into two categories : tagging - based and clustering - based .
Tagging - based meth - ods cast OpenRE as a sequence labeling prob - lem , and extract relational phrases consisting ofwords from sentences in unsupervised ( Banko et al . , 2007 ; Banko and Etzioni , 2008 ) or supervised paradigms ( Jia et al . , 2018 ; Cui et
al . , 2018 ; Stanovsky et al . , 2018 ) .
However , tagging - based methods often extract multiple overly - speciﬁc re - lational phrases for the same relation type , and can not be readily utilized for downstream tasks .
In comparison , conventional clustering - based OpenRE methods extract rich features for relationinstances via external linguistic tools , and clus - ter semantic patterns into several relation types(Lin and Pantel , 2001 ; Yao et al . , 2011 , 2012 ) .
Marcheggiani ( 2016 ) proposes a reconstructionbased model discrete - state variational autoencoder for OpenRE via unlabeled instances .
Elsahar(2017 ) utilizes a clustering algorithm over linguistic features .
In this paper , we focus on theclustering - based OpenRE methods , which have the advantage of discovering highly distinguishable relation types .
Few - shot Learning .
Few - shot learning aims to classify instances with a handful of labeled sam - ples .
Many efforts are devoted to few - shot image classiﬁcation ( Koch et al . , 2015 ) and relation classiﬁcation ( Y uan et al . , 2017 ; Han et al . , 2018 ) .
Notably , ( Koch et al . , 2015 ) introduces Convolu-
221Twain    was   a   writer   of   America   FC   distance classifier   0.7 vl   vr vd p max   word   embeddings position   embeddings FC max Kenji    was   a   poet   of   Japan    Figure 2 : The architecture of Relational Siamese Networks .
The output is the similarity between two rela - tional instances .
tional Siamese Neural Network for image metric learning , which inspires us to learn relational sim - ilarity metrics for OpenRE .
Semi - supervised Clustering .
Semi - supervised clustering aims to cluster semantic patterns giveninstance seeds of target categories ( Bair , 2013 ; Hongtao Lin , 2019 ) .
Differently , our proposed Semi - supervised RSN only leverages labeled in - stances of pre - deﬁned relations , and does not needany seed of new relations .
3 Methodology Our OpenRE framework mainly consists of twomodules , the relation similarity calculation mod - ule and the relation clustering module .
For rela - tion similarity calculation , we propose RelationalSiamese Networks ( RSNs ) , which learn to pre - dict whether two sentences mention the same re - lation .
To utilize large - scale unsupervised dataand distantly - supervised data , we further proposeSemi - supervised RSN and Distantly - supervisedRSN .
Finally , in the relation clustering module , with the learned relation metric , we utilize hierar - chical agglomerative clustering ( HAC ) and Lou - vain clustering algorithms to cluster target relationinstances of new relation types .
3.1 Relational Siamese Network ( RSN ) The architecture of our Relational Siamese Networks is shown in Figure 2 .
CNN modules encode a pair of relational instances into vectors , and several shared layers compute their similarity .
Sentence Encoder .
We use a CNN module as the sentence encoder .
The CNN module in - cludes an embedding layer , a convolutional layer , a max - pooling layer , and a fully - connected ( FC ) layer .
The embedding layer transforms the words in a sentence xand the positions of entities e head andetail into pre - trained word embeddings and random - initialized position embeddings .
Follow - ing ( Zeng et al . , 2014 ) , we concatenate these embeddings to form a vector sequence .
Next , a one - dimensional convolutional layer and a maxpooling layer transform the vector sequence intofeatures .
Finally , an FC layer with sigmoid ac - tivation maps features into a relational vector v. To summarize , we obtain a vector representationvfor a relational sentence with our CNN module : v= CNN(s ) , ( 1 ) in which we denote the joint information of a sentencexand two entities in it ehead andetailas a data sample s.
And with paired input relational instances , we have : vl= CNN(s l),vr= CNN(s r ) , ( 2 ) in which two CNN modules are identical and share all the parameters .
Similarity Computation .
Next , to measure the similarity of two relational vectors , we calculatetheir absolute distance and transform it into a real - number similarity p∈[0,1 ] .
First , a distance layer computes the element - wise absolute distance oftwo vectors : vd=|vl−vr| .
( 3 ) Then , a classiﬁer layer calculates a metric pfor relation similarity .
The layer is a one - dimensional - output FC layer with sigmoid activation : p = σ(kvd+b ) , ( 4 ) in which σdenotes the sigmoid function , kandb denote the weights and bias .
To summarize , we obtain a good similarity metric pof relational instances .
Cross Entropy Loss .
The output of RSN pcan also be explained as the probability of two sen - tences mentioning two different relations .
Thus , we can use binary labels qand binary cross entropy loss to train our RSN : Ll = Edl∼D l[qln(pθ(dl ) )
+ ( 1 −q)l n ( 1 −pθ(dl))],(5 ) in whichθindicates all the parameters in the RSN .
222labeled data dl     p = 0.7 q = 0   Cross Entropy   Relational   Siamese   Network …   ( a ) Supervised RSN ( auto)-labeled data
dl     q = 0   Cross Entropy    ( + V AT ) Relational   Siamese   Network …     unlabeled data du … p =
0.7 p = 0.6   Conditional Entropy   + V AT   ( b ) Weakly - supervised RSNs Figure 3 : The comparison of ( a ) Supervised RSN and ( b ) Weakly - supervised RSNs .
Weakly - supervised RSNs , including Semi - supervised RSN and Distantlysupervised RSN , further learn from unlabeled data withconditional entropy minimization and virtual adversar - ial training ( V A T ) .
In ﬁgures , pindicates the predicted similarity of two relational sentences , while qindicates the ground - truth label between them .
3.2 Semi - supervised RSN To discover relation clusters in the open - domain corpus , it is beneﬁcial to not only learn from la - beled data , but also capture the manifold of unla - beled data in the semantic space .
To this end , weneed to push the decision boundaries away fromhigh - density areas , which is known as the clusterassumption ( Chapelle and Zien , 2005 ) .
We try to achieve this goal with several additional loss functions .
In the following paragraphs , we denote the labeled training dataset as D land a couple of labeled relational instances as dl .
Similarly , we denote the unlabeled training dataset as Duand a couple of unlabeled instances as du .
Conditional Entropy Loss .
In classiﬁcation problems , a well - classiﬁed embedding space usu - ally reserves large margins between different clas - siﬁed clusters , and optimizing margin can be apromising way to facilitate training .
However , inclustering problems , type labels are not availableduring training .
To optimize margin without ex - plicit supervision , we can push the data pointsaway from the decision boundaries .
Intuitively , when the distance similarity pbetween two relational instances equals 0.5 , there is a high prob - ability that at least one of two instances is near the decision boundary between relation clusters .
Thus , we use the conditional entropy loss ( Grandvalet and Bengio , 2005 ) , which reaches the maximum when p=0.5 , to penalize close - boundary distribution of data points :
Lu = Edu∼D u[pθ(du)l n ( pθ(du))+ ( 1−pθ(du ) ) ln(1 −pθ(du))].(6 )
Virtual Adversarial Loss .
Despite its theoretical promise , conditional entropy minimizationsuffers from shortcomings in practice .
Due to neu - ral networks ’ strong ﬁtting ability , a very complexdecision hyperplane might be learned so as to keepaway from all the training samples , which lacksgeneralizability .
As a solution , we can smooththe relational representation space with locally - Lipschitz constraint .
To satisfy this constraint , we introduce virtual adversarial training ( Miyato et al . , 2016 ) on both branches of RSN .
Virtual adversarial training cansearch through data point neighborhoods , and pe - nalize most sharp changes in distance prediction .
For labeled data , we have Lvl = Edl∼D l[DKL(pθ(dl)||pθ(dl , t1,t2 ) ) ] , ( 7 ) in which D KLindicates the Kullback - Leibler divergence , pθ(dl , t1,t2)indicates a new distance estimation with perturbations t1andt2on both input instances respectively .
Speciﬁcally , t1andt2 are worst - case perturbations that maximize the KL divergence between pθ(dl)and pθ(dl , t1,t2)with a limited length .
Empirically , we approximate theperturbations the same as the original paper ( Miyato et al . , 2016 ) .
Speciﬁcally , we ﬁrst add a random noise to the input , and calculate the gradientof the KL - divergence between the outputs of theoriginal input and the noisy input .
We then addthe normalized gradient to the original input andget the perturbed input .
And for unlabeled data , we have Lvu = Edu∼D u[DKL(pθ(du)||pθ(du , t1,t2 ) ) ] , ( 8) in which the perturbations t1andt2are added to word embeddings rather than the words them - selves .
To summarize , we use the following loss function to train Semi - supervised RSN , which learnsfrom both labeled and unlabeled data :
Lall = Ll+λvLvl+λu(Lu+λvLvu ) , ( 9 ) in whichλvandλuare two hyperparameters .
2233.3 Distantly - supervised RSN To alleviate the intensive human labor for annotation , the topic of distantly - supervised learning hasattracted much attention in RE .
Here , we proposeDistantly - supervised RSN , which can learn fromboth distantly - supervised data and unsuperviseddata for relational knowledge transfer .
Speciﬁ-cally , we use the following loss function :
Lall = Ll+λu(Lu+λvLvu ) , ( 10 ) which treats auto - labeled data as labeled data but removes the virtual adversarial loss on the auto - labeled data .
The reason to remove the loss is simple : virtual adversarial training on auto - labeled data canamplify the noise from false labels .
Indeed , wedo ﬁnd that the virtual adversarial loss on auto - labeled data can harm our model ’s performance inexperiments .
We do not use more denoising methods , since we think RSN has some inherent advantages oftolerating such noise .
Firstly , the noise will beoverwhelmed by the large proportion of negativesampling during training .
Secondly , during clus - tering , the prediction of a new relation cluster isbased on areas where the density of relational in - stances is high .
Outliers from noise , as a result , will not inﬂuence the prediction process so much .
3.4 Open Relation Clustering After RSN is learned , we can use RSN to calculate the similarity matrix of testing instances .
With thismatrix , several clustering methods can be appliedto extract new relation clusters .
Hierarchical Agglomerative Clustering .
The ﬁrst clustering method we adopt is hierarchical ag - glomerative clustering ( HAC ) .
HAC is a bottom - up clustering algorithm .
At the start , every testing instance is regarded as a cluster .
For every step , itagglomerates two closest instances .
There are sev - eral criteria to evaluate the distance between twoclusters .
Here , we adopt the complete - linkage cri - terion , which is more robust to extreme instances .
However , there is a signiﬁcant shortcoming of HAC : it needs the exact number of clusters in ad - vance .
A potential solution is to stop agglomerat - ing according to an empirical distance threshold , but it is hard to determine such a threshold .
Thisproblem leads us to consider another clustering al - gorithm Louvain ( Blondel et al . , 2008 ) .Louvain .
Louvain is a graph - based clustering algorithm traditionally used for detecting commu - nities .
To construct the graph , we use the binaryapproximation of RSN ’s output , with 0indicating an edge between two nodes .
The advantageof Louvain is that it does not need the number ofpotential clusters beforehand .
It will automaticallyﬁnd proper sizes of clusters by optimizing commu - nity modularity .
According to the experiments weconduct , Louvain performs better than HAC .
After running , Louvain might produce a number of singleton clusters with few instances .
It is notproper to call these clusters new relation types , sowe label these instances the same as their closestlabeled neighbors .
Finally , we want to explain the reason why we do not use some other common clustering methodslike K - Means , Mean - Shift and Ward ’s ( Ward Jr , 1963 ) method of HAC : these methods calculate the centroid of several points during clustering by merely averaging them .
However , the relation vectors in our model are high - dimensional , and thedistance metric described by RSN is non - linear .
Consequently , it is not proper to calculate the cen - troid by simply averaging the vectors .
4 Experiments In this section , we conduct several experiments onreal - world RE datasets to show the effectivenessof our models , and give a detailed analysis to showits advantages .
4.1 Dataset In experiments , we use FewRel ( Han et al . , 2018 ) as our ﬁrst dataset .
FewRel is a human - annotateddataset containing 80types of relations , each with 700 instances .
An advantage of FewRel is that every instance contains a unique entity pair , so REmodels can not choose the easy way to memorizethe entities .
We use the original train set of FewRel , which contains 64relations , as labeled set with predeﬁned relations , and the original validation set ofFewRel , which contains 16new relations , as the unlabeled set with novel relations to extract .
Wethen randomly choose 1,600 instances from the unlabeled set as the test set , with the rest labeledand unlabeled instances considered as the train set .
The second dataset we use is FewRel - distant , which contains the distantly - supervised data ob - tained by the authors of FewRel before human an-
224notation .
We follow the split of FewRel to obtain the auto - labeled train set and unlabeled train set .
For evaluation , we use the human - annotated test set of FewRel with 1,600 instances .
Unlabeled instances already existing in this test set are removedfrom the unlabeled train set of FewRel - distant .
Fi - nally , the auto - labeled train set contains 323,549 relational instances , and the unlabeled train set contains 60,581 instances .
A previous OpenRE work reports performance on an unpublic dataset called NYT - FB ( Marcheggiani and Titov , 2016 ) .
However , it has several shortcomings compared with FewRel - distant .
First , NTY -FB ’s test set is distantly - supervisedand is noisy for instance - level RE .
Moreover , in - stances in NYT - FB often share entity pairs or re - lational phrases , which makes it much easier forrelation clustering .
Therefore , we think the re - sults on FewRel - distant are convincing enough forDistantly - supervised OpenRE .
4.2 Implementation Details Data Sampling .
The input of RSN should be a pair of sampled instances .
For the unlabeledset , the only possible sampling method is to selecttwo instances randomly .
For the labeled set , how - ever , random selection would result in too manydifferent - relation pairs , and cause severe biasesfor RSN .
To solve this problem , we use down - sampling .
In our experiments , we ﬁx the percent - age of same - relation pairs in every labeled databatch as 6 % .
Let us denote this percentage number as the sample ratio for convenience .
Experimental re - sults show that the sample ratio decides RSN’stendency to predict larger or smaller clusters .
Inother words , it controls the granularity of the pre - dicted relation types .
This phenomenon suggests apotential application of our model in hierarchicalrelation extraction .
However , we leave any seriousdiscussion to future work .
Hyperparameter Settings .
Following ( Lin et al . , 2016 ) and ( Zeng et al . , 2014 ) , we ﬁx the less inﬂuencing hyperparameters for sentence en - coding as their reported optimal values .
For wordembeddings , we use pre - trained 50 - dimensional Glove ( Pennington et al . , 2014 ) word embeddings .
For position embeddings , we use randominitialized 5 - dimensional position embeddings .
During training , all the embeddings are trainable .
For the neural network , the number of featuremaps in the convolutional layer is 230 .
The ﬁlter length is 3 .
The activation function after the max - pooling layer is ReLU , and the activationfunctions after FC layers are sigmoid .
Besides , we adopt two regularization methods in the CNNmodule .
We put a dropout layer right after theembedding layer as ( Miyato et al . , 2016 ) .
The dropout rate is 0.2 .
We also impose L2 regularization on the convolutional layer and the FC layer , with parameters of 0.0002 and0.001 respectively .
Hyperparameters for virtual adversarial trainingare just the same as ( Miyato et al . , 2016 ) proposed .
At the same time , major hyperparameters are selected with grid search according to the modelperformance on a validation set .
Speciﬁcally , thevalidation set contains 10,000 randomly chosensentence pairs from the unlabeled set ( i.e. 16 novelrelations ) and does not overlap with the test set .
The model is evaluated according to the precisionof binary classiﬁcation of sentence pairs on thevalidation set , which is an estimation for models’clustering ability .
We do not use F1 during modelvalidation because the clustering steps are time - consuming .
For optimization , we use Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 0.0001 , which is selected from{0.1,0 .01,0.001,0.0001,0.00001 } .
The batch size is 100 selected from { 25,50,100 } .
For hyperparameters in Equation 9and Equation 10,λv is1.0selected from { 0.1,0.5,1.0,2.0}andλuis 0.03selected from { 0.01,0.02,0.03,0.04,0.05 } .
For baseline models , original papers do grid search for all possible hyperparameters and reportthe best result during testing .
We follow their set - tings and do grid search directly on the test set .
4.3 Experiment Results on OpenRE In this section , we demonstrate the effectiveness of our RSN models by comparing our mod - els with state - of - the - art clustering - based OpenREmethods .
We also conduct ablation experimentsto detailedly investigate the contributions of dif - ferent mechanisms of Semi - supervised RSN andDistantly - supervised RSN .
Baselines .
Conventional clustering - based OpenRE models usually cluster instances by either clustering their linguistic features ( Lin and Pantel , 2001 ;
Yao et al . , 2012 ; Elsahar et al . , 2017 ) or imposing reconstruction constraints ( Yao et al . , 2011 ; Marcheggiani and Titov , 2016 ) .
To demonstrate
225the effectiveness of our RSN models , we compare our models with two state - of - the - art models : ( 1 ) HAC with re - weighted word embeddings ( RW - HAC ) ( Elsahar et al . , 2017 ): RW - HAC is the state - of - the - art feature clustering model forOpenRE .
The model ﬁrst extracts KB types andNER tags of entities as well as re - weighted wordembeddings from sentences , then adopts principalcomponent analysis ( PCA ) to reduce feature di - mensionality , and ﬁnally uses HAC to cluster theconcatenation of reduced feature representations .
( 2 ) Discrete - state variational autoencoder ( V AE ) ( Marcheggiani and Titov , 2016 ): V AE is the state - of - the - art reconstruction - based modelfor OpenRE via unlabeled instances .
It optimizesa relation classiﬁer by reconstructing entities frompairing entities and predicted relation types .
Richfeatures including entity words , context words , trigger words , dependency paths , and contextPOS tags are used to predict the relation type .
RW - HAC and V AE both rely on external linguistic tools to extract rich features from plaintexts .
Speciﬁcally , we ﬁrst align entities to Wiki - data and get their KB types .
Next , we preprocessthe instances with part - of - speech ( POS ) tagging , named - entity recognition ( NER ) , and dependencyparsing with Stanford CoreNLP ( Manning et al . , 2014 ) .
It is worth noting that these features are only used by baseline models .
Our models , in con - trast , only use sentences and entity pairs as inputs .
Evaluation Protocol .
In evaluation , we use B 3 metric ( Bagga and Baldwin , 1998 ) as the scoring function .
B3metric is a standard measure to balance the precision and recall of clusteringtasks , and is commonly used in previous OpenREworks ( Marcheggiani and Titov , 2016 ; Elsahar et al . , 2017 ) .
To be speciﬁc , we use F 1measure , the harmonic mean of precision and recall .
First , we report the result of supervised RSN with different clustering methods .
Speciﬁcally , SN represents the original RSN structure , HAC and Lindicate HAC and Louvain clustering introduced in Sec .
3.3 .
The result shows that Louvain performs better than HAC , so in the following ex - periments we focus on using Louvain clustering .
Next , for Semi - supervised and Distantlysupervised RSN , we conduct various combina - tions of different mechanisms to verify the contri - bution of each part .
( + C ) indicates that the model is powered up with conditional entropy minimiza - tion , while ( + V ) indicates that the model is pow - FewRel FewRel - distant Approach PRF 1PRF 1 V AE 17.9
69.7 28.5 17.9 69.7 28.5 RW - HAC 31.8 46.0 37.6 31.8 46.0 37.6 SN - HAC 36.2 53.3 43.1 34.5 53.3 41.5SN - L 36.5 69.2 47.8 34.6 59.8 43.9SN - L+V 46.1 77.3 57.8 40.7 52.4 45.8SN - L+C 47.1 78.1 58.8 42.3 66.0 51.5 SN - L+CV 148.9 77.5 59.9 40.8 74.0 52.6 Table 1 : Precision , recall and F1 results ( % ) for different models .
The ﬁrst two models are baselines .
The next ﬁve models are different variants of our model .
ered up with virtual adversarial training .
Experimental Result Analysis .
Table 1shows the experimental results , from which we can ob - serve that : ( 1 ) RSN models outperform all baseline models on precision , recall , and F1 - score , among whichWeakly - supervised RSN ( SN - L+CV ) achievesstate - of - the - art performances .
This indicates thatRSN is capable of understanding new relations’semantic meanings within sentences .
( 2 ) Supervised and distantly - supervised relational representations improve clustering performances .
Compared with RW - HAC , SN - HAC achieves better clustering results because of itssupervised relational representation and similar - ity metric .
Speciﬁcally , unsupervised baselinesmainly use sparse one - hot features .
RW - HAC usesword embeddings , but integrates them in a rule - based way .
In contrast , RSN uses distributed fea - ture representations , and can optimize informationintegration process according to supervision .
( 3 ) Louvain outperforms HAC for clustering with RSN , comparing SN - HAC with SN - L. Oneexplanation is that our model does not put addi - tional constraints on the prior distribution of rela - tional vectors , and therefore the relation clustersmight have odd shapes in violation of HAC ’s as - sumption .
Moreover , when representations are notdistinguishable enough , forcing HAC to ﬁnd ﬁne - grained clusters may harm recall while contributing minimally to precision .
In practice , we do observe that the number of relations SN - L extracts isconstantly less than the true number 16 .
( 4 ) Both SN - L+V and SN - L+C improve the performance of supervised or distantly - supervised 1Here for FewRel - distant we use Equation 10rather than Equation 9as loss , which corresponds to Distantlysupervised RSN , and this brings a minor improvement on F1 from 52.0%to52.6 % .
226 ( a ) RSN ( b ) Semi - supervised RSN ( c ) Supervised CNN Figure 4 : The t - SNE visualization of the output vectors of CNN modules in our ( a ) OpenRE model RSN , ( b ) Semi - supervised RSN facilitated by unlabeled novel - relation data and in ( c ) a classical RE baseline trained withlabeled novel - relation data .
All ﬁgures visualize the clustering result for 402 instances of 4novel relations .
40 48 56 64 Number of Pre - defined Training Relations40.042.545.047.550.052.555.057.560.0F1 Score SN - L+CV SN - L+V SN - L+C
SN - L Figure 5 : The clustering results with different numbers of pre - deﬁned training relations .
RSN by further utilizing unsupervised corpora .
Both semi - supervised approaches bring signiﬁcantimprovements for F 1scores by increasing the precision and recall , and combining both can furtherincrease the F 1score .
( 5 ) One interesting observation is that SN - L+V does not outperform SN - L so much on FewRel - distant .
This is probably because V A T on the noisy data might amplify the noise .
In further experiments , we perform V A T only on unlabeled setand observe improvements on F 1 , with SN - L+V from 45.8%to49.2%and SN - L+CV from 52.0 % to52.6 % , which proves this conjecture .
4.4
The Inﬂuence of Pre - deﬁned Relation Diversity on Generalizability In this subsection , we mainly focus on analyzing the inﬂuence of pre - deﬁned relation diversity , i.e. ,the number of relations in the labeled train set .
Tostudy this inﬂuence , we use FewRel for evaluationand change the number of relations in the labeledtrain set from 40to64while ﬁxing the total num - ber of labeled instances to 25,000 , and report the clustering results in Figure 5 .
Several conclusions can be drawn according to Figure 5 .
Firstly , a rich variety of labeled relations do improve the performance of our models , especially RSN .
The models trained on 64 rela - tions perform better than those trained on 40relations constantly .
Secondly , while the performanceof supervised RSN is very sensitive to pre - deﬁnedrelation diversity , its semi - supervised counterpartssuffer much less from the relation number limit .
This phenomenon suggests that Semi - supervisedRSNs succeed in learning from unlabeled novel - relation data and are more generalizable to novelrelations .
4.5 Relational Knowledge Representation Visualization To intuitively evaluate the knowledge transfer effects of RSN and Semi - supervised RSN , we visualize their relational knowledge representation spaces in the last layer of CNN encoders with t - SNE ( Maaten and Hinton , 2008 ) in Figure 4.W e also compare with a supervised CNN trained on9,600 labeled instances of novel relations , which suggests the optimal relational knowledge representation .
In each ﬁgure , we plot 402 relation instances of 4randomly - chosen relation types in the test set , and points are colored according to theirground - truth labels .
As we can see from Figure 4 , RSN is able to roughly distinguish different relations , andSemi - supervised RSN further facilitated knowl - edge transfer by optimizing the margin betweenpotential relation clusters during training .
As a re - sult , Semi - supervised RSN can extract more dis - tinguishable novel relations , and gains comparable
227relational knowledge representation ability with supervised CNN .
5 Conclusions and Future Work
In this paper , we propose a new model Rela - tional Siamese Network ( RSN ) for OpenRE .
Dif - ferent from conventional unsupervised models , our model learns to measure relational similarityfrom supervised / distantly - supervised data of pre - deﬁned relations , as well as unsupervised data ofnovel relations .
There are mainly two innovativepoints in our model .
First , we propose to transferrelational similarity knowledge with RSN struc - ture .
To the best of our knowledge , we are the ﬁrstto propose knowledge transfer for OpenRE .
Sec - ond , we propose Semi / Distantly - supervised RSN , to further perform semi - supervised and distantly - supervised transfer learning .
Experiments show that our models signiﬁcantly surpass conventionalOpenRE models and achieve new state - of - the - artperformance .
For future research , we plan to explore the following directions : ( 1 ) Besides CNN , thereare some other popular sentence encoder struc - tures like piecewise convolutional neural network(PCNN ) and Long Short - Term Memory ( LSTM)for RE .
In the future , we can try different sentenceencoders in our model .
( 2 ) As mentioned above , our model has the potential ability to discover thehierarchical structure of relations .
In the future , we will try to explore this application with addi - tional experiments .
6 Acknowledgement This work is supported by the National Key Re - search and Development Program of China ( No.2018YFB1004503 ) and the National Natural Sci - ence Foundation of China ( NSFC No . 61572273,61661146007 ) .
Ruidong Wu is also supported byTsinghua University Initiative Scientiﬁc ResearchProgram .
References Amit Bagga and Breck Baldwin .
1998 .
Algorithms for scoring coreference chains .
In Proceedings of the First Iternational Conference on Language Re - sources and Evaluation Workshop on LinguisticsCoreference .
Eric Bair .
2013 .
Semi - supervised clustering methods .
Wiley Interdisciplinary Reviews : Computational Statistics .Michele
Banko , Michael J Cafarella , Stephen Soderland , Matthew Broadhead , and Oren Etzioni . 2007.Open
information extraction from the web .
In Proceedings of IJCAI .
Michele Banko and Oren Etzioni . 2008 .
The tradeoffs between open and traditional relation extraction .
In Proceedings of ACL - HLT .
Vincent D Blondel , Jean Loup Guillaume , Renaud Lambiotte , and Etienne Lefebvre . 2008 .
Fast un - folding of communities in large networks .
Journal of Statistical Mechanics .
Olivier Chapelle and Alexander Zien .
2005 .
Semisupervised classiﬁcation by low density separation .
InProceedings of AISTATS .
Lei Cui , Furu Wei , and Ming Zhou .
2018 .
Neural open information extraction .
arXiv .
Hady Elsahar , Elena Demidova , Simon Gottschalk , Christophe Gravier , and Frederique Laforest .
2017.Unsupervised open relation extraction .
In Proceedings of European Semantic Web Conference .
Yves Grandvalet and Y oshua Bengio .
2005 .
Semisupervised learning by entropy minimization .
In Proceedings of NIPS .
Xu Han , Hao Zhu , Pengfei Y u , Ziyun Wang , Y uan Yao , Zhiyuan Liu , and Maosong Sun .
2018 .
Fewrel : Alarge - scale supervised few - shot relation classiﬁcation dataset with state - of - the - art evaluation .
In Proceedings of EMNLP .
Meng Qu Xiang Ren Hongtao Lin , Jun Yan . 2019 .
Learning dual retrieval module for semi - supervisedrelation extraction .
In Proceedings of WWW .
Shengbin Jia , Yang Xiang , and Xiaojun Chen .
2018 .
Supervised neural models revitalize the open rela - tion extraction .
arXiv .
Diederik P Kingma and Jimmy Ba . 2014 .
Adam : A method for stochastic optimization .
arXiv .
Gregory Koch , Richard Zemel , and Ruslan Salakhutdinov .
2015 .
Siamese neural networks for one - shotimage recognition .
In Proceedings of ICML Deep Learning Workshop , volume 2 .
Dekang Lin and Patrick Pantel . 2001 .
Dirt - discovery of inference rules from text .
In Proceedings of KDD .
Yankai Lin , Shiqi Shen , Zhiyuan Liu , Huanbo Luan , and Maosong Sun . 2016 .
Neural relation extractionwith selective attention over instances .
In Proceedings of ACL .
Laurens van der Maaten and Geoffrey Hinton .
2008 .
Visualizing data using t - sne .
JMLRJournal of Statistical Mechanics .
Christopher D. Manning , Mihai Surdeanu , John Bauer , Jenny Finkel , Steven J. Bethard , and David Mc - Closky .
2014 .
The Stanford CoreNLP natural language processing toolkit .
In Proceedings of ACL .
228Diego Marcheggiani and Ivan Titov .
2016 .
Discretestate variational autoencoders for joint discovery andfactorization of relations .
Transactions of ACL .
Mike Mintz , Steven Bills , Rion Snow , and Daniel Jurafsky . 2009 .
Distant supervision for relation extrac - tion without labeled data .
In Proceedings of ACLIJCNLP .
Takeru Miyato , Andrew M Dai , and Ian Goodfellow .
2016 .
Adversarial training methods for semi - supervised text classiﬁcation .
arXiv .
Sachin Pawar , Girish K. Palshikar , and Pushpak Bhattacharyya .
2017 .
Relation extraction : A survey.arXiv .
Jeffrey Pennington , Richard Socher , and Christoper Manning . 2014 .
Glove : Global vectors for wordrepresentation .
In Proceedings of EMNLP .
Gabriel Stanovsky , Julian Michael , Luke Zettlemoyer , and Ido Dagan .
2018 .
Supervised open informationextraction .
In Proceedings of NAACL .
Joe H Ward Jr. 1963 .
Hierarchical grouping to optimize an objective function .
Journal of the American Statistical Association .
Limin Yao , Aria Haghighi , Sebastian Riedel , and Andrew Mccallum .
2011 .
Structured relation discov - ery using generative models .
In Proceedings of EMNLP .
Limin Yao , Sebastian Riedel , and Andrew McCallum .
2012 .
Unsupervised relation discovery with sensedisambiguation .
In Proceedings of ACL .
Dian Y u , Lifu Huang , and Heng Ji . 2017 .
Open relation extraction and grounding .
In Proceedings of IJCNLP .
Jianbo Y uan , Han Guo , Zhiwei Jin , Hongxia Jin , Xianchao Zhang , and Jiebo Luo . 2017 .
One - shot learn - ing for ﬁne - grained relation extraction via convolu - tional siamese neural network .
In Proceedings of BigData .
Daojian Zeng , Kang Liu , Siwei Lai , Guangyou Zhou , Jun Zhao , et al . 2014 .
Relation classiﬁcation viaconvolutional deep neural network .
In Proceedings of COLING .
