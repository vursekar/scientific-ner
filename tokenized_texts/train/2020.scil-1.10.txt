Interpreting Verbal Irony : Linguistic Strategies and the Connection to the Type of Semantic Incongruity
Debanjan Ghosh ⇤
1 , Elena Musi2 , Kartikeya Upasani3 , Smaranda Muresan4
1McGovern Institute for Brain Research , MIT , Cambridge , MA 2University of Liverpool , Liverpool , UK 3Facebook Conversational AI , CA 4 Data Science Institute , Columbia University , New York , NY dg513@mit.edu , elena.musi@liverpool.ac.uk , kart@fb.com , smara@columbia.edu
Abstract
Human communication often involves the use of verbal irony or sarcasm , where the speakers usually mean the opposite of what they say .
To better understand how verbal irony is expressed by the speaker and interpreted by the hearer we conduct a crowdsourcing task : given an utterance expressing verbal irony , users are asked to verbalize their interpretation of the speaker ’s ironic message .
We propose a typology of linguistic strategies for verbal irony interpretation and link it to various theoretical linguistic frameworks .
We design computational models to capture these strategies and present empirical studies aimed to answer three questions : ( 1 ) what is the distribution of linguistic strategies used by hearers to interpret ironic messages ? ; ( 2 ) do hearers adopt similar strategies for interpreting the speaker ’s ironic intent ? ; and ( 3 ) does the type of semantic incongruity in the ironic message ( explicit vs. implicit ) inﬂuence the choice of interpretation strategies by the hearers ?
1
Introduction
It is well understood that recognizing whether a speaker is ironic or sarcastic is essential to understanding their actual sentiments and beliefs .
For instance , the utterance “ pictures of holding animal carcasses are so ﬂattering ” is an expression of verbal irony , where the speaker has a negative sentiment towards “ pictures of holding animal carcasses ” , but uses the positive sentiment word “ ﬂattering ” .
This inherent characteristic of verbal irony is called semantic incongruity — incongruity between the literal evaluation and the context ( e.g. , between the positive sentiment words and the negative situation in this example ) .
Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating
⇤ Part of the research was carried out while Debanjan was
a Ph.D. candidate at Rutgers University .
it as a binary classiﬁcation task using either the utterance in isolation or adding contextual information such as conversation context , author context , visual context , or cognitive features ( Davidov et al , 2010 ; Maynard and Greenwood , 2014 ; Wallace et al , 2014 ; Joshi et al , 2015 ; Bamman and Smith , 2015 ; Muresan et al , 2016 ; Amir et al , 2016 ; Mishra et al , 2016 ; Ghosh and Veale , 2017 ; Felbo et al , 2017 ; Ghosh et al , 2017 ; Hazarika et al , 2018 ;
Tay et al , 2018 ; Ghosh et al , 2018 ; Oprea and Magdy , 2019 ) .
Such approaches have focused their analysis on the speakers ’ beliefs and intentions for using irony ( Attardo , 2000 ) .
However , sarcasm and verbal irony are types of interactional phenomena with speciﬁc perlocutionary effects on the hearer ( Haverkate , 1990 ) .
Thus , we argue that , besides recognizing the speaker ’s sarcastic / ironic intent , it is equally important to understand how the hearer interprets the speaker ’s sarcastic / ironic message .
For the above utterance , the strength of negative sentiment perceived by the hearer depends on whether they interpret the speaker ’s actual meaning as “ picture . .
. are not ﬂattering ” vs. “ pictures . .
. are so gross ” ( Table 1 ) .
The intensity of negative sentiment is higher in the latter interpretation than in the former .
Kreuz ( 2000 ) noted that most studies in linguistics and psychology have conducted experiments analyzing reaction times ( Gibbs , 1986 ; Katz et al , 2004 ) or situational context ( Ivanko and Pexman , 2003 ) , featuring a setup with in vitro data aimed at testing the validity of speciﬁc theories of irony .
Instead , our study adopts a naturalistic approach to understand hearers ’ reception of irony looking at what linguistic strategies are recurrently used by hearers to interpret the non - literal meaning underlying ironic utterances .
We leverage the crowdsourcing task introduced by Ghosh et al ( 2015 ) for their work on detecting whether a word has a literal or sarcastic in  terpretation , later adopted by Peled and Reichart ( 2017 ) .
The task is framed as follows : given a speaker ’s ironic message , ﬁve annotators ( e.g. , Turkers on Amazon Mechanical Turk ( MTurk ) ) are asked to verbalize their interpretation of the speaker ’s ironic message ( i.e. , their understanding of the speaker ’s intended meaning ) ( see Table 1 ; Sim denotes the speaker ’s ironic message , while Hint denotes the hearer ’s interpretation of that ironic message ) .
The crowdsourcing experiments are reported in Section 2 .
The paper makes three contributions .
First , we propose a data - driven typology of linguistic strategies that hearers use to interpret ironic messages and discuss its relevance in verifying theoretical frameworks of irony ( Section 4 ) .
Second , we propose computational models to capture these strategies ( Section 5 ) .
Third , we present two studies that aim to answer two questions : ( 1 ) does the type of semantic incongruity in the ironic message ( explicit vs. implicit ; see Section 3 ) inﬂuence the choice of interpretation strategies by the hearers ?
( Section 6.2 ) ; ( 2 ) do interpretation strategies of verbal irony vary by hearers ?
We make all datasets and code available.1
2 Datasets of Speakers ’ Ironic Messages
and Hearers ’ Interpretations
To generate a parallel dataset of speakers ’ ironic messages and hearers ’ interpretations we conduct a crowdsourcing experiment .
Given a speaker ’s ironic message ( Sim ) , ﬁve Turkers ( hearers ) on MTurk are asked to verbalize their interpretation their unof the speaker ’s ironic message ( i.e. , derstanding of the speaker ’s intended meaning ) ( Hint ) .
The design of the MTurk task was ﬁrst introduced by Ghosh et al ( 2015 ) , who use the resulting dataset to identify words that can have both a literal and a sarcastic sense .
Peled and Reichart ( 2017 ) employed similar design to generate a parallel dataset to use for generating interpretations of sarcastic messages using machine translation approaches .
They use skilled annotators in comedy writing and literature paraphrasing and give them the option not to rephrase ( we refer to Peled and Reichart ( 2017 ) ’s dataset as SIGN ) .
We perform this new crowdsourcing task and do not rely entirely on the above two datasets for two reasons : ( 1 ) we focus on verbal irony , and ( 2 ) we always require an interpretation from the Turkers .
Un1https://github.com/debanjanghosh/interpreting verbal irony
like the above two studies , the main goal of our research is to analyze the linguistics strategies employed by hearers in interpreting verbal irony .
We collected messages that express verbal irony from Twitter using the hashtags # irony , # sarcastic , and # sarcasm .
We chose Twitter as a source since the presence of the hashtags allows us to select sentences where the speaker ’s intention was to be ironic .
Furthermore , even though Twitter users can not be considered representative of the entire population , they are unlikely to be skewed with respect to topics or gender .
We manually checked and kept 1,000 tweets that express verbal irony .
We do not draw any theoretical distinction between sarcasm and irony since we can not assume that Twitter users also differentiate between # irony and # sarcasm , blurred even in scholarly literature .
The Turkers were provided with detailed instructions and examples of the task including the standard deﬁnition of verbal irony taken from the Merriam - Webster dictionary ( “ use of words to express something other than and especially the opposite of the literal meaning ” ) .
We decided to suggest them a guiding deﬁnition for two reasons .
First , hearers do not usually focus on literal vs. non literal meaning , as shown by studies measuring processing times for both types of statements ( Inhoff et al , 1984 ) .
Therefore , when asked to rephrase the speakers ’ intended meaning , hearers would have probably come up with sentences expressing the speaker ’s imagined discursive goals , rather than disclosing their perceived literal meaning .
Second , it is reasonable to assume that Turkers would have looked up the standard meaning of ironic utterance given by an online dictionary to ease up their task , possibly coming up with biased deﬁnitions .
The Turkers were instructed to consider the entire message in their verbalization to avoid asymmetry in length between the Sim and Hint .
We obtained a dataset of 5,000 Sim - Hint pairs where ﬁve Turkers rephrase each Sim .
A total of 184 Turkers participated in the rephrasing task .
Table 1 shows examples of speaker ’s ironic messages ( Sim ) and their corresponding hearers ’ interpretations ( Hi int ) .
Next , we ran a second MTurk task to verify whether the generated Hint messages are plausible interpretations of the ironic messages .
This time we employ three Turkers per task and only Turkers who were not involved in the content generation task were allowed to perform this
Sim 1 .
Ed Davey is such a passionate , inspiring speaker 2 . ca n’t believe how much captain America looks like me 3 .
Pictures of you holding dead animal carcasses are so ﬂattering
int
H1 Ed Davey is a boring , uninspiring speaker I wish I looked like Captain America .
I need to lose weights Hate hunting season and the pictures of you holding dead animal are so gross
int
H2 Ed Davey is such a dull , monotonous speaker ca n’t believe how much captain America looks different from me Pictures of you holding dead animal carcasses is an unﬂattering look
int
H3 Ed Davey is not a passionate , inspiring speaker I do n’t , but I wish I looked like Captain America
Pictures of you holding dead animal carcasses are not ﬂattering
Table 1 : Examples of speaker ’s ironic messages ( Sim ) and interpretations given by 3 Turkers ( Hi
int ) .
task .
We observe that Turkers labeled 5 % ( i.e. , 238 verbalizations ) of Hints as invalid and low quality ( e.g. , wrong interpretation ) .
For both tasks , we allowed only qualiﬁed Turkers ( i.e. , at least 95 % approval rate and 5,000 approved HITs ) , paid 7 cents / task and gave sixty minutes to complete each task .
The ﬁnal dataset contains 4,762 pairs SimHint .
3 Semantic Incongruity in Ironic Messages : Explicit vs. Implicit
Attardo ( 2000 ) and later Burgers ( 2010 ) distinguish between two theoretical aspects of irony : irony markers and irony factors .
Irony markers are meta - communicative signals , such as interjections or emoticons that alert the reader that an utterance might be ironic .
In contrast , irony factors can not be removed without destroying the irony , such as the incongruity between the literal evaluation and its context ( “ semantic incongruity ” ) .
Incongruity expresses the contrast between the conveyed sentiment ( usually , positive ) and the targeted situation ( usually , negative ) .
This contrast can be explicitly or implicitly expressed in the ironic message .
Following Karoui et al ( 2017 ) , we consider that semantic incongruity is explicit , when it is lexicalized in the utterance itself ( e.g. , both the positive sentiment word(s ) and the negative situation are available to the reader explicitly ) .
On Twitter , beside sentiment words , users often make use of hashtags ( e.g. , “ Studying 5 subjects . . .
# worstsaturdaynight ” ) or an image ( e.g. , “ Encouraging how Police feel they ’re above the law .
URL ”
; the URL shows a police car not paying parking ) to express their sentiment .
We consider these cases as explicit , since the incongruity is present in the utterance even if via hashtags or other media .
For implicit incongruity , we consider cases where one of the two incongruent terms ( “ propositions ” in Karoui et al ( 2017 ) ) is not lexicalized and has to be reconstructed from the context ( either outside word knowledge or a larger conversational context ) .
For example “ You are such a nice friend ! ! ! ” , or “ Driving in Detroit is fun ;) ” are cases of ironic messages where the semantic incongruity is implicit .
Based on these definitions of explicit and implicit incongruity , two expert annotators annotated the Sim - Hint dataset ( 1000 ironic messages ) as containing explicit or implicit semantic incongruity .
The inter - annotator agreement was =0.7 , which denotes good agreement similar to Karoui et al ( 2017 ) .
The annotation showed that 38.7 % of the ironic messages are explicit , while 61.3 % are implicit .
In the following section we propose a typology of linguistic strategies used in hearers ’ interpretations of speakers ’ ironic messages and in section 6.2 we discuss the correlation of linguistic strategies with the type of semantic incongruity .
4
Interpreting Verbal Irony : A Typology of Linguistic Strategies
Given the deﬁnition of verbal irony , we would expect that Turkers ’ interpretation of speaker ’s ironic message will contain some degree of opposite meaning with respect to what the speaker has said .
However , it is unclear what linguistic strategies the Turkers will use to express that .
To build our typology , from the total set of Sim - Hint pairs obtained through crowdsourcing ( i.e. , 4,762 pairs ; see Section 2 ) we selected a dev set of 500 Sim - Hint pairs .
Our approach does not assume any speciﬁc theory or irony , but it is data - driven : a linguist expert in semantics and pragmatics analyzed the dev set to formulate the lexical and pragmatic phenomena attested in the data .
The assembled typology is , thus , the result of a bottom - up procedure .
A Sim - Hint pair can be annotated with more than one strategy .
The core linguistic strategies are explained below and synthesized in Table 2 .
Distribution ( % )
Typology Antonyms - lexical antonyms - antonym phrases Negation - simple negation Antonyms OR Negation - weakening sentiment - interrogative - desiderative constructions Pragmatic inference
!
declarative
( 42.2 ) ( 6.0 )
( 28.4 )
( 23.2 ) ( 5.2 ) ( 2.8 ) ( 3.2 )
Table 2 : Typology of linguistic strategies and their distribution ( in % ) over the dev set
4.1 Linguistic Strategies
$
$
Lexical and phrasal antonyms :
This category “ hate ” , contains lexical antonyms ( e.g. , “ love ” “ great ” “ terrible ” ) as well as indirect antonyms ( Fellbaum , 1998 ) , where the opposite meaning can only be interpreted in context ( e.g. , “ passionate “ boring speaker ” ; Table 1 ) .
Although speaker ” the typical antonym of “ passionate ” is “ unpassionate ” , “ boring ” works in this context as a lexical opposite since a speaker who is passionate entails that he is not boring .
Besides lexical antonyms , Turkers sometimes use antonym phrases ( e.g. , “ I ca n’t wait ” “ not looking forward ” , “ I like ( to visit ER ) ”
“ I am upset ( to visit ER ) ” ) .
!
!
!
Negation : Here , Turkers negate the main predicate .
This strategy is used in the presence of copulative constructions where the predicative expression is an adjective / noun expressing sentiment “ is not great ” ) and of verbs ex(e.g . , “ is great ” “ do not love ” ) pressing sentiment ( e.g. , “ love ” “ I or propositional attitudes ( e.g. , “ I wonder ” do n’t wonder ” ) .
!
!
!
Weakening the intensity of sentiment : The use of negation and antonyms is sometimes accompanied by two strategies that reﬂect a weakening of sentiment intensity .
First , when Sim contains words expressing a high degree of positive sentiment , the hearer ’s interpretation replaces them with more neutral ones ( e.g. , “ I love it ” “ I do n’t like it ” ) .
Second , when Sim contains an intensiﬁer , it is eliminated in the Turkers ’ interpretation .
Intensiﬁers specify the degree of value / quality expressed by the words they modify ( M´endez - Naya , 2008 ) ( e.g. , “ cake for breakfast .
so healthy ” “ cake for breakfast .
not healthy ” ) .
!
!
Interrogative to Declarative Transformation ( + Antonym / Negation ): This strategy , used
mostly in conjunction with the negation or antonym strategies , consists in replacing the interrogative form with a declarative form , when Sim is a rhetorical question ( for brevity , RQ ) ( e.g. , “ do n’t you love ﬁghting ? ”
“ I hate ﬁghting ” ) .
!
Counterfactual Desiderative Constructions : When the ironic utterance expresses a positive / negative sentiment towards a past event ( e.g. , “ glad you relayed this news ” ) or an expressive speech act ( e.g. , “ thanks X that picture needed more copy ” )
the hearer ’s interpretation of intended meaning is expressed through the counterfactual desiderative constructions I wish ( that ) p ( “ I wish you had n’t relayed . . .
” , “ I wish X did n’t copy . . . ” ) .
Differently from antonymic phrases , this strategy stresses on the failure of the speaker ’s expectation more than on their commitment to the opposite meaning .
In addition to the above Pragmatic Inference : strategies , there are cases where the interpretation calls for an inferential process to be recognized .
For instance , “ made 174 this month . . .
I ’m gon na buy a yacht ! ”
“ made 174 this month . . .
I am so poor ” .
The distribution of the strategies on the dev set is represented in Table 2 .
!
4.2 Links to Theoretical Frameworks
In linguistic literature many different approaches to irony have been provided .
Here we focus on the three accounts ( w.r.t . examples from Sim - Hint corpus ) that bear a different views on pragmatic factors .
According to Grice ( 1975 ) , ironic messages are uttered to convey a meaning opposite to that literally expressed , ﬂouting the conversational maxim of quality “ do not say what you believe to be false ” .
In verbal irony , the violation of the maxim is frequently signaled by “ the opposite ” of what is said literally ( e.g. , intended meaning of “ carcasses are ﬂattering ” is they are gross ; Table 1 ) .
The linguistic strategies of antonyms ( e.g. “ worst day of my life ” ) and simple negation ( “ yeap we totally do nt drink alcohol every single day ” [ ... ] )
cover the majority of the SimHint corpus and seem to ﬁt the Gricean ( Grice , 1975 ) account of irony , since the hearer seems to have primarily recognized the presence of semantic incongruity .
However , as touched upon by Giora ( 1995 ) , antonyms and direct negation are not always semantically equivalent strategies , since the second sometimes allows a graded interpretation : if “ x is not encouraging ” , it is not nec  essarily bad , but simply “ x < encouraging ” .
Such an implicature is available exclusively with items allowing mediated contraries , such as sentiment words ( Horn , 1989 ) .
Direct negation with sentiment words implies that just one value in a set is negated , while the others are potentially afﬁrmed .
The spectrum of interpretations allowed by negation as a rephrasing strategy indicates that hearers recognize that the relevance of the ironic utterance in itself plays a role next to what the utterances refers to ( if the rephrased utterance is intended as “ x is not encouraging at all ” , the perceived irrelevance of the corresponding ironic utterance is more prominent than in “ x is not very encouraging ” ) .
The fact that the interpretation of irony has a propositional scope is even clearer when the ironic sentence in interrogative form ( “ and they all lived happily ever after ? ” ) is rephrased as a declarative ( e.g. “ I doubt they all lived happily ever after ” ): the hearers recognizes that the question has a rhetoric value since otherwise contextually irrelevant .
The intentional falsehood of Gricean analysis is also not deemed by Sperber and Wilson ( 1986 ) ; Wilson and Sperber ( 2012 ) as a necessary and sufﬁcient condition for irony .
According to their theory of echoic mentioning , irony presupposes the mention to the inappropriateness of the entire sentence : in asserting “ awesome weather in Scotland today ” the speaker does not simply want to express that the weather was horrible but he signals that assuming that the weather would be nice was irrelevant and , thus , ridiculous .
Kreuz and Glucksberg ( 1989 ) expand the Relevance Theory approach talking about echoic reminding to account for cases such as “ could you be just a little louder , please ?
My baby is n’t trying to sleep ” where the extreme politeness reminds the hearer that the question is indeed a request and that the mother bears a certain stance and has certain expectations towards the addressee .
Similarly , the use of the pragmatic inference strategy the can not be fully explained in Gricean terms : rephrase “ made 174 this month . . .
I am so poor ” for “ made 174 this month . . .
I am gon na buy a yatch ” more than pointing to the presence of lexical incongruity , show that the hearers knows for background knowledge that the assertion of “ buying a yatch ” is completely irrelevant in the context of a low salary situation .
Rephrasing strategies using counterfactual desiderative constructions ( e.g. “ I really wish my friends and family would check up on my after yesterday ’s near death experience ” ) show , instead , that the interpretation of irony involves an echoic reminding to the speaker ’s ( social ) expectations which failed to be fulﬁlled .
Overall , using the results of our crowdsourcing experiment with main existing theories of irony , it turns out that the theories have a complementary explanatory power .
In Section 6.2 we investigate weather this situation might relate to the presence of explicit / implicit irony .
5 Empirical Analysis of Interpretation
Strategies
Here our goal is to perform a comparative empirical analysis to understand how hearers interpret verbal irony .
To accomplish this , we propose computational models to automatically detect these linguistic strategies in two datasets : ( 1 ) Sim -Hint dataset and ( 2 ) the SIGN dataset .
As stated in Section 2 , albeit for a different purpose , the task designed in Peled and Reichart ( 2017 ) is identical to ours : they used a set of 3,000 sarcastic tweets and collected ﬁve interpretation verbalization , including an option to just copy the original message if it was not deemed ironic .
They used workers skilled in comedy writing and literature paraphrasing .
SIGN contains 14,970 pairs .
To evaluate our models , we asked two annotators to annotate two test sets of 500 pairs each from the Sim -Hint and the SIGN dataset ( i.e. , denoted by SIGNtest ) , respectively .
Note , the test set for the Sim -Hint has no overlap with the dev set of 500 Sim - Hint pairs used to identify the strategies ( Section 4 ) .
Agreement between the annotators for both sets is high with  > 0.9 .
In SIGNtest , 79 instances were just copies of the original message , which we eliminated , thus the SIGNtest contains only 421 instances .
5.1 Computational Methods
Lexical Antonyms .
To detect whether an SimHint pair uses the lexical antonyms strategy , we ﬁrst need to build a resource of lexical antonyms .
We use the MPQA sentiment Lexicon ( Wilson et al , 2005 ) , Hu and Liu ( 2004 ) ’s opinion lexicon , antonym pairs from Mohammad et al ( 2013 ) , antonyms from WordNet , and pairs of opposite verbs from Verbocean ( Chklovski and Pantel , 2004 ) .
Given this lexicon of lexical antonyms , the task is now to detect whether a given Sim - Hint pair
Strategies Lex ant Simple neg AN weaksent ANI D AN desiderative AntPhrase+PragInf
!
dev R P 95.7 89.0 89.4 92.0 87.9 93.6 53.1 65.4 100.0 92.9 53.2 86.2
F1 92.2 90.7 90.7 58.6 96.3 65.8
SIGNtest
test R 89.9 88.3 91.9 0.44
R F1 P 97.9 93.4 97.2 91.2 88.3 88.3 87.5 93.4 95.0 80.0 70.6 57.2 100.0 100.0 100.0 100.0 66.7 68.0 77.4 70.7
P 89.4 93.3 93.3 85.7
89.5
85.3
F1 93.5 92.2 90.3 77.4 80.0 77.3
Table 3 : Evaluation of Computational Methods on dev , test and SIGNtest set ( in % )
!
uses the lexical antonyms strategy .
We use a heuristic approach based on word - alignment and dependency parsing ( similar to contradiction detection ( De Marneffe et al , 2008 ) ) .
Word - to - word alignments between Sim - Hint are extracted using a statistical machine translation ( SMT ) alignment method - IBM Model 4 with HMM alignment from Giza++ ( Och and Ney , 2004 ) .
We consider a lexical antonym strategy if : 1 ) antonym words are aligned ; 2 ) they are the roots of the respective dependency trees or if the nodes modiﬁed by the lexical antonyms are the same in their respective trees ( e.g. , ‘ can you show any more of steelers ” “ show less of steelers ” , the candidate lexical antonyms are more and less and they are the objects of the same predicate in Sim - Hint : show ) .
Out of 211 Sim - Hint pairs that are marked as having lexical antonym strategy ( dev set )
, 12 instances are identiﬁed by only the dependency parses , 67 instances by the word - alignments , and 100 instances by both ( P / R / F1 scores are 92.1 % , 77.7 % and 84.3 % ) , respectively on dev dataset .
However , sometimes both dependency and wordalignment methods fail .
In “ circling down the bowl .
Yay ” “ circling down the bowl .
awful ” , although the lexical antonyms yay and awful exist , neither the alignment nor the dependency trees can detect it ( 25 such instances in the dev set ) .
To account for this , after having run the dependency and alignment methods , we also just search whether a Sim - Hint pair contains a lexical antonym pair .
This improves the ﬁnal recall and on the dev set we achieve 89.0 % precision , 95.7 % recall , and 92.2 % F1 on dev dataset ( Lex ant Strategy ; Table 3 show results both on dev and the test sets ) .
Note , just searching whether a lexical antonym pair is present in a Sim - Hint pair results in low precision ( 58.6 % ) but high recall ( 80 % ) .
!
Simple negation .
This strategy ( denoted as Simple neg in Table 3 and Table 4 ) involves identifying the presence of negation and its scope .
the scope of negation is conHere , however ,
strained since generally Turkers negated only a single word ( i.e. , “ love ” “ not love ” ) .
Thus our ! problem is easier than the general problem of ﬁnding the scope of negation ( Li and Lu , 2018 ;
Qian et
al , 2016 ; Fancellu et al , 2016 ) .
We use 30 negation markers from Reitan et al ( 2015 ) to ﬁnd negation scope in tweets .
We ﬁrst detect whether a negation marker appears in either Hint or Sim , but not in both ( negation can appear in Sim for ironic blame )
If the marker is used , we extract its parent node from the dependency tree , and if this node is also present in the other utterance , then Negation strategy is selected .
For instance , in “ looks just like me ” “ does not look like me ” , the negation not is modifying the main predicate looks in Hint , which is also the main predicate in Sim ( words are lemmatized ) .
In the next section , we discuss if the parent nodes are not the same but similar and with different sentiment strength .
!
!
Weakening the intensity of sentiment .
The ﬁrst strategy — replacing words expressing a high degree of positive / negative sentiment with more neutral ones ( ‘ I love being sick ” “ I do n’t like being sick ) — , is applied only in conjunction with the negation strategy .
We measure the difference in strength using the Dictionary of Affect ( Whissell et al , 1986 ) .
Out of 31 Sim - Hint pairs in the dev set , we automatically identify 28 interpretations that use this approach .
For the second strategy — removing the intensiﬁer ( I am really happy ” “ I am disappointed ’ ) — , we ﬁrst determine whether the intensiﬁer exists in Sim and is eliminated from Hint .
We use only adjective and adverb intensiﬁers from Taboada et al ( 2011 ) , primarily to discard conjunctions such as “ so ” ( “ no water so I ca n’t wash . . . ” ) .
This strategy is used together with both lexical antonyms and Simple negation strategies .
For a candidate Sim - Hint pair , if the lexical antonym strategy is selected and aS and aH are the lexical antonyms , we determine whether any intensiﬁer modiﬁes aS and no intensiﬁer modiﬁes aH .
If the Negation strategy is se !
lected , we identify the negated term in the Hint and then search its aligned node from the Sim using the word - word alignment .
Next , we search in the Sim if any intensiﬁer is intensifying the aligned term .
The strategies are denoted as AN weaksent in Table 3 and Table 4 .
Interrogative to Declarative Transformation ( + Antonym / Neg ) .
To capture this strategy we need to determine ﬁrst if the verbal irony was expressed as a rhetorical question .
To build a classiﬁer to detect RQ , we collect two categories of tweets ( 4 K each ) ( 1 ) tweets labeled with # sarcasm or # irony that also contain “ ? ” , and ( 2 ) information seeking tweets containing “ ? ” .
We train a binary classiﬁer using SVM RBF Kernel with default parameters .
The features are Twitter - trained word embeddings ( Ghosh et al , 2015 ) , modal verbs , pronouns , interrogative words , negations , and position of “ ? ” in a tweet .
We evaluate the training model on the dev data and the P / R / F1 are 53.2 % , 65.4 % , and 58.6 % , respectively ( in future work we plan to develop more accurate models for RQ detection ) .
Once we detect the ironic message was expressed as a RQ , we identify the speciﬁc interpretation strategy accompanying the transformation from interrogative to declarative form : antonym or negation .
These combined strategies are denoted as ANI
D in Table 3 and Table 4 .
!
Desiderative Constructions : Currently , we use wish ” to capa simple regular expression “ I [ w ] ⇤ ture counterfactual cases ( AN desiderative in Tables 3 and Table 4 ) .
Note , when the Simple negation and lexical antonyms strategies are combined with other strategy ( e.g. , removing of intensiﬁer ) , we consider this combined strategy for the interpretation of verbal irony and not the simple negation or lexical antonym strategy ( i.e. , we do not double count ) .
Phrasal antonyms and pragmatic inference : Identifying phrasal antonyms and pragmatic inference is a complex task , and thus we propose a method of phrase matching based on phrase extraction via unsupervised alignment technique in SMT .
We use IBM Model 4 with HMM ( Giza++ ; ( Och and Ney , 2000 ) ) , phrase extraction via Moses ( Koehn et al , 2007 ) and the IRST tool to build the required language models .
As postprocessing , we ﬁrst remove phrase pairs obtained from the Sim - Hint bitext that are also present in the set of extracted phrases from the Hint - Hint
Strategies Lex ant Simple neg AN weaksent ANI D AN desiderative AntPhrase+PragInf
!
Sim - Hint 2,198 ( 40.0 ) 1,596 ( 29.1 ) 895 ( 16.3 ) 329 ( 6.0 ) 92 ( 1.7 ) 357 ( 6.5 )
SIGN 9,691 ( 51.8 ) 3,827 ( 20.5 ) 2,160 ( 11.6 ) 933 ( 5.0 ) 86 ( 0.5 ) 1912 ( 10.1 )
Table 4 : Distribution of interpretation strategies on two datasets ( in % )
1
bitext .
This increases the likelihood of retaining semantically opposite phrases , since phrases extracted from the Hint - Hint bitext are more likely to be paraphrastic .
Second , based on the translation probability scores   , for phrase e if we have a set of aligned phrases fset we reject phrases that have   scores less than size(fset ) .
Finally , 11,200 phrases are extracted from the Sim - Hint bitext .
The low recall for this strategy is expected since there are too many ways that users can employ pragmatic inference or rephrase the utterance without directly using any antonym or In future , we will explore neural MT negation .
( Cho et al , 2014 ) and use external data to generate more phrases .
Since we have not manually evaluated these phrase pairs , we only use this strategy after we have tried all the remaining strategies ( AntPhrase+PragInf in Table 3 and Table 4 ) .
5.2 Results and Distribution of Linguistic
Strategies
!
The performance of the models is similar on both test and SIGNtest sets , showing consistently good performance ( Table 3 ; 90 % F1 for all strategies , except the AntPhrase+PragInf and ANI D ) .
Given these results , we can now apply these models to study the distribution of these strategies in the entire datasets ( Table 4 ) .
The strategy distribution between our dataset Sim - Hint and SIGN dataset is similar and matches the distribution on the manual annotations on the dev dataset in Table 2 .
The sum of the strategies can exceed the total number of the pairs since a tweet can contain several ironic sentences that are interpreted by Turkers .
For instance , in “ Dave too nice .
. .
a nice fella ” “ Dave not nice . . .
a mean fella ” we observe the application of two strategies , lexical antonyms ( e.g. , nice mean ) and negation ( e.g. , nice
not nice ) .
!
!
!
6 Discussion
6.1 Hearer - dependent Interpretation
Strategies
We investigate how hearers adopt strategies for interpreting the speaker ’s ironic intent .
To implement this study , we selected three Turkers ( e.g. , H1 , H2 , and H3 ;
In Table 1 , Hi int are generated by the correspondent Turker Hi ) , from our crowdsourced data , who were able to rephrase at least ﬁve hundred identical Sim messages .
Note , we can not carry this experiment on the SIGN dataset ( Peled and Reichart , 2017 ) because the annotators ’ information is absent there .
Although the three Turkers choose lexical antonym and simple negation as two top choices , there is some variation among them .
H1 and H2 choose antonyms more frequently than negation while in contrary Turker H3 choose negation more than antonyms , sometime combined with the weakening of sentiment strategy .
As we mentioned in Section 4.2 , antonyms and direct negation are not semantically equivalent strategies since the latter , allows a graded interpretation : if “ x is not inspiring ” , it is not necessarily bad , but simply “ x < inspiring ” ( Giora , 1995 ) .
In Table 1 , the Sim - Hint pair “ passionate ” !
“ gross ” ( interpretation of H1 ) have more contrast than the pair “ passionate ” “ not passionate ” and “ so ﬂattering ” “ not ﬂattering ” ( interpretation of H3 ) .
This suggests that H1 perceive the intensity of negative sentiment towards the target of irony ( “ Ed Davey ” and “ picture of dead animals ” , respectively ) higher than Turker H3 .
All three Turkers have chosen the remaining strategies with similar frequencies .
“ boring ” and “ ﬂattering ”
!
!
!
6.2 Message - dependent Interpretation
Strategies
Interpretation Strategies and the Type of Semantic Incongruity : We investigate whether the type of semantic incongruity in the ironic message ( explicit vs. implicit ; see Section 3 ) inﬂuences the choice of interpretation strategies by the hearers .
To do this , we looked at Sim - level distribution of interpretation strategies used by the hearers for the same ironic message Sim .
Table 5 represents the correlation of linguistic strategies with the type of semantic incongruity ( explicit vs. implicit ) as well as the presence and absence of irony markers .
We notice that Turkers use lexical antonyms
Strategies
Lex ant Simple neg AN weaksent ANI D AN desiderative AntPhrase+PragInf
!
marker
incongruity Exp .
48.5 24.9 14.3 5.9 1.3 5.2
Imp .
+ 34.8 32.3 17.6 6.1 1.9 7.1
35.7 28.9 15.7 12.3 0.9 6.2
  42.2 30.0 16.8 3.1 2.0 6.6
Table 5 : Rephrasing Strategies against Incongruency and Irony Markers on Sim - Hint dataset ( in % )
Figure 1 : Strategies selected per message ( in % )
as interpretation strategy more when the semantic incongruity is explicit than implicit ( 48.5 % vs. the presence of explicit sentiment trig34.8 % ): gered the use of the antonym strategy .
In contrary they use simple negation more when the semantic incongruity is implicit than explicit .
 
We also analyze the interpretation strategies w.r.t .
to the presence ( + ) or absence ( ) of irony markers .
We implement various morpho - syntactic as well as typographic markers ( similar to ( Ghosh and Muresan , 2018 ) ) to identify the presence of markers .
We observe that Lex ant strategy is used more in cases where the markers are abIn Sim - Hint , markers are present twice as sent .
much in the case of implicit ( 21 % ) than explicit incongruity ( 10 % ) .
This ﬁnding validates ( Burgers et al , 2012 ) who argued speakers will likely use markers to signal their ironic intent in implicit incongruity .
Message interpreted the same by all hearers : In Figure 1 , the vertical columns ( purple : SimHint and grey : SIGN ) depict the distribution ( in % ) of tweets strategy - wise .
In Sim - Hint dataset , for 17 % of messages ( 124 Sims ) all ﬁve Turkers use the same strategy to interpret the Sims ( labeled as 5 on the X - axis ) , whereas for 26 % ( 188 Sims ) , 4 Turkers used same strategy ( labeled as 4,1 on Xaxis ) and so on .
We observe when the Sims are marked by strong subjective words e.g. , “ great ” , “ best ” , etc . ,
they
!
have been replaced in 90 % of cases as lexical “ terrible ” ) .
In addition , antonyms ( e.g. , “ great ” !
the majority of adjectives are used in attributive position ( i.e. , “ lovely neighbor is vacuuming at night ” ) , thus blocking paraphrases involving predicate negation .
However , not all strong subjective words guarantee the use of direct opposites in the Hints ( e.g. , “ ﬂattering ” “ not ﬂattering ” ; See Table 1 ) .
The choice of strategies may also depend upon the target of ironic situation ( Ivanko and Pexman , 2003 ) .
We implement the bootstrapping algorithm from Riloff et al ( 2013 ) to identify ironic situations in Sims that are rephrased by Lexical antonym strategy .
We ﬁnd utterances containing stereotypical negative situations regarding health issues ( e.g. , “ having migraines ” , “ getting killed by chemicals ” ) and other undesirable negative states such as “ oversleeping ” , “ luggage lost ” , “ stress in life ” are almost always interpreted via lexical antonym strategy .
Utterances where all ﬁve Turkers used simple negation , if negative particles are positioned in the ironic message with a sentential scope ( e.g. , “ not a biggie ” , “ not awkward ” ) then they are simply omitted in the interpretations .
This trend can be explained according to the inter - subjective account of negation types ( Verhagen , 2005 ) .
Sentential negation leads the addressee to open up an alternative mental space where an opposite predication is at stake .
7 Related Work
Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating it as a binary classiﬁcation task using either the utterance in isolation or adding contextual information such as conversation context , author context , visual context , or cognitive features ( Gonz´alezIb´a˜nez et al , 2011 ; Liebrecht et al , 2013 ; Wallace et
al , 2014 ; Zhang et al , 2016 ; Ghosh and Veale , 2016 ; Schifanella et al , 2016 ; Xiong et al , 2019 ; Castro et al , 2019 ) .
Unlike this line of work , our research focuses on how the hearer interprets an ironic message .
The ﬁndings from our study could have multiple impacts on the sarcasm detection task .
First , interpretation strategies open up a scope of “ graded interpretation ” of irony instead of only a binary decision ( i.e. , predicting the strength of irony ) .
Second , nature of semantic incongruence and stereotype irony situations can be useful features in irony detection .
Recently , Peled and Reichart ( 2017 ) proposed a computational model based on SMT to generate interpretations of sarcastic messages .
We aim to deepen our understanding of such interpretations by introducing a typology of linguistic strategies .
We study the distribution of these strategies via both hearer - dependent and messagedependent interpretations .
Psycholinguistics studies that have dealt with the hearers ’ perception , have mainly focused on how ironic messages are processed : through the analysis of reaction times ( Gibbs , 1986 ; Katz et al , 2004 ) , the role of situational context ( Ivanko and Pexman , 2003 ) and in tackling speaker - hearer social relations by annotating ironic texts from different genres ( Burgers , 2010 ) .
However , no attention has been paid to correlations between how ironic message is expressed and how it is interpreted by the hearer , including what linguistic strategies the hearers employ .
8 Conclusions
We leveraged a crowdsourcing task to obtain a dataset of ironic utterances paired with the hearer ’s verbalization of their interpretation .
We proposed a typology of linguistic strategies for verbal irony interpretation and designed computational models to capture these strategies with good performance .
Our study shows ( 1 ) Turkers mostly adopt lexical antonym and negation strategies to interpret speaker ’s irony , ( 2 ) interpretations are correlated to stereotype ironic situations , and ( 3 ) irony expression ( explicit vs. implicit incongruity and absence or presence of markers ) inﬂuences the choice of interpretation strategies and match with different explanatory theories ( the Gricean approach links up better with explicit incongruity , while Relevance Theory with the implicit one ) .
The latter can have an impact on irony detection by bringing out more discriminative semantic and pragmatic features .
