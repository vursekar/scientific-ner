Higher - order Comparisons of Sentence Encoder Representations
Mostafa Abdou† ∗ Artur Kulmizev ♣ Felix Hill ♦
Daniel M. Low ♠
Anders Søgaard† †Department of Computer Science , University of Copenhagen ♣ Department of Linguistics and Philology , Uppsala University ♦ DeepMind ♠ Program in Speech and Hearing Bioscience and Technology , Harvard Medical School - MIT
Abstract
Representational Similarity Analysis ( RSA ) is a technique developed by neuroscientists for comparing activity patterns of different measurement modalities ( e.g. , fMRI , electrophysiology , behavior ) .
As a framework , RSA has several advantages over existing approaches to interpretation of language encoders based on probing or diagnostic classiﬁcation : namely , it does not require large training samples , is not prone to overﬁtting , and it enables a more transparent comparison between the representational geometries of different models and modalities .
We demonstrate the utility of RSA by establishing a previously unknown correspondence between widely - employed pretrained language encoders and human processing difﬁculty via eye - tracking data , showcasing its potential in the interpretability toolbox for neural models .
1
Introduction
Examining the parallels between human and machine learning is a natural way for us to better understand the former and track our progress in the latter .
The “ black box ” aspect of neural networks has recently inspired a large body of work related to interpretability , i.e. understanding of representations that such models learn .
In NLP , this push has been largely motivated by linguistic questions , such as : what linguistic properties are captured by neural networks ?
and to what extent do decisions made by neural models reﬂect established linguistic theories ?
Given the relative recency of such questions , much work in the domain so far has been focused on the context of models in isolation ( e.g. what does model X learn about linguistic phenomenon Y ? )
In order to more broadly understand models ’ representational tendencies , however , it is vital that such questions be formed not only with other models in mind , but also other representational methods and modalities ( e.g. behavioral data , fMRI measurements , etc . ) .
In context of the latter concern , the present - day interpretability toolkit has not yet been able to afford a practical way of reconciling this .
In this work , we employ Representational Similarity Analysis ( RSA ) as a simple method of interpreting neural models ’ representational spaces as they relate to other models and modalities .
In particular , we conduct an experiment wherein we investigate the correspondence between human processing difﬁculty ( as reﬂected by gaze ﬁxation measurements ) and the representations induced by popular pretrained language models .
In our experiments , we hypothesize that there exists an overlap between the sentences which are difﬁcult for humans to process and those for which per - layer encoder representations are least correlated .
Our intuition is that such sentences may exhibit factors such as low - frequency vocabulary , lexical ambiguity , and syntactic complexity ( e.g. multiple embedded clauses ) , etc . that are uncommon in both standard language and , relatedly , the corpora employed in training large - scale language models .
In the case of a human reader , encountering such a sentence may result in a number of processing delays , e.g. longer aggregate gaze duration .
In the case of a sentence encoder , an uncommon sentence may lead to a degradation of representations in the encoder ’s layers , wherein a lower layer might learn to encode vastly different information than a higher one .
Similarly , different models ’ representations may emphasize different aspects of these more complex sentences and therefore diverge from each other .
With this in mind , our hypothesis is that sentences which are difﬁcult for humans to process are likely to have divergent representations within models ’ internal layers and between different models ’ layers .
Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing , pages5838–5845,HongKong , China , November3–7,2019.c(cid:13)2019AssociationforComputationalLinguistics5838  Understanding and analysing language encoders In recent years , some prominent efforts towards interpreting neural networks for NLP have included : developing suites that evaluate network representations through performance on downstream tasks ( Conneau et al , 2017a ; Wang et al , 2018 ; McCann et al , 2018 ) ; analyzing network predictions on carefully curated datasets ( Linzen et al , 2016 ; Marvin and Linzen , 2018 ; Gulordava et al , 2018 ; Loula et al , 2018 ;
Dasgupta et al , 2018 ; Tenney et al , 2018 ) ; and employing diagnostic classiﬁers to assess whether certain classes of information are encoded in a model ’s ( intermediate ) representations ( Adi et al , 2016 ; Chrupała et al , 2017 ; Hupkes et al , 2017 ; Belinkov et al , 2017 ) .
While these approaches provide valuable insights into how neural networks process a large variety of phenomena , they rely on decoding accuracy as a probe for encoded linguistic information .
If properly biased , this means that they can detect whether information is encoded in a representation or not .
However , they do not allow for a direct comparison of representational structure between models .
Consider a toy dataset of ﬁve sentences of interest and three encodings derived from quite different processing models ; a hidden state of a trained neural language model , a tf - idf weighted bag - of - words representation , and measurements of ﬁxation duration from an eyetracking device .
Probing methods do not allow us to quantify or visualise , for each of these encoding strategies , how the encoder ’s responses to the ﬁve sentences relate to each other .
Moreover , probing methods would not directly reveal whether the ﬁxations from the eye - tracking device aligned more closely with the tf - idf representation or the states In short , while of the neural language model .
probing classiﬁer methods can establish if phenomena are separable based on the provided representations , they do not tell us about the overall geometry of the representational spaces .
RSA , on the other hand , provides a basis for higher - order comparisons between spaces of representations , and a way to visualise and quantify the extent to which they are isomorphic .
Indeed , RSA has seen a modest introduction within interpretable NLP in recent years .
For example , Chrupała et al ( 2017 ) employed RSA as a means of correlating encoder representations of speech , text , and images in a post - hoc analysis of a
multi - task neural pipeline .
Similarly , Bouchacourt and Baroni ( 2018 ) used the framework to measure the similarity between input image embeddings and the representations of the same image by an agent in an language game setting .
More recently , Chrupała and Alishahi ( 2019 ) correlated activation patterns of sentence encoders with symbolic representations , such as syntax trees .
Lastly , similar to our work here , Abnar et al ( 2019 ) proposed an extension to RSA that enables the comparison of a single model in the face of isolated , changing parameters , and employed this metric along with RSA to correlate NLP models ’ and human brains ’ respective representations of language .
We hope to position our work among this brief survey and further demonstrate the ﬂexibility of RSA across several levels of abstraction .
2 Representational Similarity Analysis
RSA was proposed by Kriegeskorte et al ( 2008 ) as a method of relating the different representational modalities employed in neuroscientiﬁc studies .
Due to the lack of correspondence between the activity patterns of disparate measurement modalities ( e.g. brain activity via fMRI , behavioural responses ) , RSA aims to abstract away from the activity patterns themselves and instead compute representational dissimilarity matrices ( RDMs ) , which characterize the information carried by a given representation method through dissimilarity structure .
Given a set of representational methods ( e.g. , pretrained encoders ) M and a set of experimental conditions ( sentences )
N , we can construct RDMs for each method in M .
Each cell in an RDM corresponds to the dissimilarity between the activity patterns associated with pairs of experimental conditions ni , nj ∈ N , say , a pair of sentences .
When ni = nj , the dissimilarity between an experimental condition and itself is intuitively 0 , thus making the N × N RDM symmetric along a diagonal of zeros ( Kriegeskorte et al , 2008 ) .
The RDMs of the different representational methods in M can then be directly compared in a Representational Similarity Matrix ( RSM ) .
This comparison of RDMs is known as second - order analysis , which is broadly based on the idea of a second - order isomorphism ( Shepard and ChipIn such an analysis , the principal man , 1970 ) .
point of comparison is the match between the dissimilarity structure of the different representa5839  Figure 1 : An example of ﬁrst- and second - order analyses , where N = # of experimental conditions , M = # of models , and H = # of activity patterns observed for a given model ( i.e. dimensionality ) .
The right - most side of the ﬁgure depicts a representational similairty matrix ( RSM ) of correlations between RDMs .
tional methods .
Intuitively , this can be expressed through the notion of distance between distances , and is thus related to Earth Mover ’s Distance ( Rubner et al , 2000).1 Figure 1 shows an illustration of the ﬁrst and second order analyses for pretrained language encoders .
Note that RSA is meaningfully different from , and complementary to , methods that employ saturating functions of representation distances ( e.g. decoding accuracy , mutual information ) , which suffer from ( a ) a ceiling effect : being able to distinguish experimental phenomenon A from B with with an accuracy of 100 % and experimental phenomenon C from D with an accuracy of 100 % does not mean that the distance between A and B is the same as that between C and D ; and ( b ) discretization ( Nili et al , 2014 ) .
We follow Kriegeskorte et al ( 2008 ) in using the correlation distance of experimental condition pairs ni , nj ∈ N as a dissimilarity measure , where ¯ni is the mean of ni ’s elements , · is the dot product , and ( cid:107 ) is the l2 norm : corr(x )
= 1 − ( ni− ¯ni)·(nj − ¯nj ) .
Compared to other measures , correlation distance is preferable as it normalizes both the mean and variance of activity patterns over experimental conditions .
Other popular measures include the Euclidean distance and the Malahanobis distance ( Kriegeskorte et al , 2006 ) .
( cid:107)(ni− ¯ni(cid:107)2(cid:107)(nj − ¯nj ( cid:107)2
3 Fixation Duration and Encoder
Disagreement
Gaze ﬁxation patterns have been shown to strongly reﬂect the online cognitive processing demands of
1More precisely , our measure of dissimilarity between experimental conditions is analogous to ground distance and dissimilarity between RDMs to earth mover ’s distance .
human readers ( Raney et al , 2014 ; Ashby et al , 2005 ) and to be dependent upon a number of linguistic factors ( Van Gompel , 2007 ) .
Speciﬁcally , it has been demonstrated that word frequency , syntactic complexity , and lexical ambiguity play a strong part in determining which sentences are difﬁcult for humans to process ( Rayner and Duffy , 1986 ; Duffy et al , 1988 ; Levy , 2008 ) .
Using the RSA framework , we aim to explore how gaze ﬁxation patterns and the linguistic factors associated with sentence processing difﬁculty relate to the representational spaces of popular language encoders .
Namely , we hypothesize that , for a given sentence , disagreement between hidden layers corresponds to processing difﬁculty .
Because layer disagreement for a sentence measures the extent to which two layers ( e.g. within BERT ) disagree with each other about the pairwise similarity of the sentence ( with other sentences in the corpus ) , a sentence with high layer disagreement will have unstable similarity relationships to other sentences in the corpus .
This indicates that it has a degraded encoder representation .
Going further , we also hypothesize that models ’ representations of said sentences may be confounded , in part , by factors that are known to inﬂuence humans .
Eye - tracking data For our experiments
, we make use of the Dundee eye - tracking corpus ( Kennedy et al , 2003 ) , the English part of which consists of eye - movement data recorded as 10 native participants read 2,368 sentences from 20 newspaper articles .
We consider the following ﬁxation features : TOTAL FIXATION DURATION and FIRST PASS DURATION .
For each of the features , we ﬁrst take the average of the measurements recorded for all 10 participants per word , then ob5840  tain sentence - level annotations by summing the measurements of all words in a sentence and dividing by its length .
The result of this is two vectors Vtotf ix and Vf irstpass of length 2 , 368 , where each cell in the vector corresponds to a sentence ’s average total ﬁxation and average ﬁrst pass duration , respectively .
Syntactic complexity , word frequency , and lexical ambiguity We also consider the three following linguistic features which affect processing difﬁculty .
For each of the following the result is also a vector of length 2 , 368 where each cell corresponds to a sentence :
a. the average word log frequency per sentence extracted from the British National Corpus ( Leech , 1992 ) , VlogF req ..
b. the average number of senses per word per sentence extracted from WordNet ( Miller , 1995 ) , VwordSense .
c. Yngve scores , a standard measure of syntactic complexity based on cognitive load ( Yngve , 1960 ) , VY ngve .
Pretrained encoders We conduct our analysis on pretrained BERT - large ( Devlin et al , 2018 ) and ELMo ( Peters et al , 2018 ) , two widely employed contextual sentence encoders .
To obtain a representation of a sentence from a given layer L , we perform mean - pooling over the time - steps which correspond to the words of a sentence , obtaining a vector representation of the sentence .
Meanpooling is a common approach for obtaining vector representations of sentences for downstream tasks ( Peters et al , 2018 ; Conneau et al , 2017b ) .
We refer to ELMo ’s lowest layer as E1 , BERT ’s 11th layer as B11 , etc .
RDMs We construct an RDM ( see § 2 ) for each contextual encoder ’s layers .
Each RDM is a 2 , 368 × 2 , 368 matrix which represents the dissimilarity structure of the layer , ( i.e. , each row vector in the matrix contains the dissimilarity of a given sentence to every other sentence ) .
We then compute the correlations between the two different RDMs .
For our evaluation of how well the representational geometry of a layer correlates to another , we employ Kendall ’s τA as suggested in Nili et al ( 2014 ) , computing the pairwise correlation for each two corresponding rows in two RDMs .
This second - order analysis gives us a pairwise relational similarity vector VCorrLi−Lj of
length 2 , 368 , which has the correlations between two layers Li and Lj ’s RDMs for each of the sentences .
Third - order analysis The ﬁnal part of our analysis involves computing correlations ( Spearman ’s ρ ) of { VCorrLi−Lj , VlogF req , VY ngve , VwordSense } with each of Vtotf ix and Vf irstpass .
The results from this are shown in Table 1 .
The top section of the table shows correlations when Li and Lj are the three ﬁnal adjacent layers in BERT and ELMo .
The middle section shows the results for top three BERT layer pairs Li and Lj which maximize the correlation scores .
The ﬁnal section shows correlation with the linguistic features .
Finally , Figure 2 shows Spearman ’s ρ correlations between VCorrLi−Lj and each of Vtotf ix , and VY ngve for all combinations of the 24 BERT layers .
4 Discussion
Our results show highly signiﬁcant negative correlations between VCorrLi−Lj and sentence gaze ﬁxation times .
These ﬁndings conﬁrm the hypothesis that the sentences that are most challenging for humans to process , are the sentences ( a ) the layers of BERT disagree most on among themselves ; and ( b ) that ELMo and BERT disagree most on , indicating that there may be common factors which affect human processing difﬁculty and result in disagreement between layers .
By Layer disagreement we refer to the expression 1 − VCorrLi−Lj .
It is important to note that these encoders are trained with a language modelling objective , unlike models where reading behaviour is explicitly modelled ( Hahn and Keller , 2016 ) or predicted ( Matthies and Søgaard , 2013 ) .
Indeed , the similarities here emerge naturally as a function of the task being performed .
This can be seen as analogous to the case of similarities observed between neural networks trained to perform object recognition and spatio - temporal cortical dynamics ( Cichy et al , 2016 ) .
Syntactic complexity Figure 2 shows that , for all combinations of BERT layers , total ﬁxation time and Yngve scores have strong negative and positive correlations ( respectively ) with layer disagreement .
Furthermore , we observe that disagreement between middle layers seems to show the strongest correlation with Yngve scores .
To conﬁrm this , we split the correlations into four groups : “ low ” ( i , j ∈ [ 1 , 8 ] ) , “ middle ” ( i , j ∈
5841  Figure 2 : RSMs showing ( Spearman ’s ρ ) correlation between disagreement among layers i and j ( VCorrLi−Lj and Vtotf ix ( left ) and VY ngve ( Right ) .
BERT layers are denoted with numbers from 1 ( topmost ) to 24 ( lowest ) .
)
Layer Disagreement Total Fixation First Pass Duration
E1 - B22 E2 - B23 E3 - B24
B11 - B12 B12 - B13 B10 - B21
Linguistic Features
Log Freq .
Avg . Senses per Word Yngve Score
-0.46 -0.66 -0.22
-0.88 -0.87 -0.87
-0.20 -0.007 * 0.66
-0.46 -0.67 -0.23
-0.87 -0.85 -0.86
-0.19 -0.004 * 0.66
Table 1 : Spearman ’s ρ between VCorrLi−Lj , VlogF req . , VwordSense , VY ngve and each of Vtotf ix and Vf irstpass .
All correlations signiﬁcant with p < 0.0001 after Bonferroni correction unless marked with * .
[ 9 , 16 ] ) , “ high ” ( i , j ∈ [ 17 , 24 ] ) , and “ out ” ( |i − j| > 7 ) , with the latter representing out - ofgroup correlations ( e.g. CorrL1−L24 ) .
To account for correlations between disagreeing adja|i − j| = 1 ) and Yngve scores cent layers ( e.g. being higher ( as a possible confounding factor ) , we also distinguish layers as either “ adjacent ” or “ non - adjacent ” .
Considering these two factors as three- and two - leveled independent variables respectively , we conduct a two - way analysis of variance .
The analysis reveals that the effect of group is signiﬁcant at F ( 3 , 275 ) = 78.47 , p < 0.0001 , with “ low ” ( µ = 0.65 , σ = 0.08 ) , “ middle ” ( µ = 0.84 , σ = 0.03 ) , “ high ” ( µ = 0.80 , σ = 0.05 ) , and “ out ” ( µ = 0.80 , σ = 0.05 ) .
Neither the effect of adjacency nor its interaction with group proved to be signiﬁcant .
This can be seen as ( modest ) support for the ﬁndings of previous work ( Blevins et al , 2018 ; Tenney et al , 2019 ): namely , that the intermediate layers of neural language models encode the
most syntax , and are therefore possibly more sensitive towards syntactic complexity .
A very similar pattern is observed for total ﬁxation time .
When considered together with the correlation between VY ngve and ﬁxation times , this indicates a tripartite afﬁnity between layer disagreement , syntactic complexity , and ﬁxation .
Lexical Ambiguity and Word Frequency Finally , we observe that VlogF req . has a moderate correlation with both ﬁxation time and layer disagreement and that VwordSense is nearly uncorrelated to both .
Detailed plots of the latter can be found in Appendix A.
5 Conclusion
We presented a framework for analyzing neural network representations ( RSA ) that allowed us to relate human sentence processing data with lanIn experiments guage encoder representations .
conducted on two widely used encoders , our ﬁndings show that sentences which are difﬁcult for humans to process have more divergent representations both intra - encoder and between different encoders .
Furthermore , we lend modest support to the intuition that a model ’s middle layers encode comparatively more syntax .
Our framework offers insight that is complimentary to decoding or probing approaches , and is particularly useful to compare representations from across modalities .
