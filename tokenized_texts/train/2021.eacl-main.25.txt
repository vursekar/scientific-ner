Evaluating the Evaluation of Diversity in Natural Language Generation
Guy Tevet1,2
Jonathan Berant1,3
1School of Computer Science , Tel - Aviv University 2Department of Electrical Engineering , Tel - Aviv University 3Allen Institute for AI { guytevet@mail , joberant@cs}.tau.ac.il
Abstract
Despite growing interest in natural language generation ( NLG ) models that produce diverse outputs , there is currently no principled method for evaluating the diversity of an NLG system .
In this work , we propose a framework for evaluating diversity metrics .
The framework measures the correlation between a proposed diversity metric and a diversity parameter , a single parameter that controls some aspect of diversity in generated text .
For example , a diversity parameter might be a binary variable used to instruct crowdsourcing workers to generate text with either low or high content diversity .
We demonstrate the utility of our framework by : ( a ) establishing best practices for eliciting diversity judgments from humans , ( b ) showing that humans substantially outperform automatic metrics in estimating content diversity , and ( c ) demonstrating that existing methods for controlling diversity by tuning a “ decoding parameter ” mostly affect form but not meaning .
Our framework can advance the understanding of different diversity metrics , an essential step on the road towards better NLG systems .
1
Introduction
language An important desideratum of natural generation ( NLG ) systems is to produce outputs that are not only correct , but also diverse .
For example , a dialog system ( Adiwardana et al , 2020 ) should permit many responses for the prompt “ How are you today ? ” .
Similarly , we expect diverse responses in tasks such as story generation ( Li et al , 2018 ) , question generation ( Pan et al , 2019 ) and question answering ( Fan et al , 2019 ) .
Despite growing effort to produce more diverse models ( Li et al , 2016c , a ; Holtzman et al , 2019 ; Du and Black , 2019 ) , there is no standard evaluation metric for measuring diversity .
Thus , different papers evaluate diversity differently ( if at
Question : So what did I miss in the ﬁrst 20 minutes ?
Set A • Pretty much everything .
• Nothing , really .
• You wo n’t believe what happened !
• Why do you even care ?
•
What were you doing that was more important than this ?
Set B • Not much .
•
It was pretty dull .
• Blah , you did n’t miss anything .
• Not anything that important .
• Very little , it was uneventful .
Figure 1 : Diversity metric evaluation : we show two sets of responses to the same question , generated by crowdsourcing workers .
While both sets are diverse in terms of form , only set A is diverse in terms of content .
Each graph presents the distribution over a diversity metric for sets with high content diversity ( blue ) and low content diversity ( orange ) .
Distributions are approximated over 200 sets .
We observe that the human score metric ( absDHS ) separates the two distributions , while an n - gram based metric ( distinct - n ) fails , illustrating that it does not capture content diversity .
The dotted lines correspond to the speciﬁc sets A and B presented above .
all )
, making it difﬁcult to compare competing approaches ( Hashimoto et al , 2019 ) .
Having a principled and consensual diversity evaluation metric is hence fundamental for the ﬁeld of NLG .
A key challenge in developing diversity evaluation metrics , is the difﬁculty in determining their efﬁcacy .
Unlike metrics for evaluating the quality of generated text , where one can measure correlation between a metric ( such as BLEU ( Papineni et al , 2002 ) ) and human judgement ( Zhang et al , 2019a ; Sagarkar et al , 2018 ) , it is unknown if huProceedingsofthe16thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics , pages326–346April19 - 23,2021. © 2021AssociationforComputationalLinguistics3260.60.81.0Metric Valuesdistinct - n ( averaged)Set ASet B234Metric ValuesabsHDS  mans can reliably estimate diversity .
In this paper , we propose a framework for evaluating diversity metrics ( Figure 2 ) .
We assume that a tester ( human or model ) is generating sets of sentences , conditioned on some diversity parameter that controls the diversity of the output sentences .
We evaluate the diversity of the sentences using a proposed metric , and measure correlation between the metric and the diversity parameter .
High correlation indicates that the metric captures how the diversity parameter affects the model output .
We instantiate this framework with two tests .
As a preliminary step , we introduce the decoding test : the tester is a neural generation model and the diversity parameter is a decoding parameter , such as softmax temperature ( Ackley et al , 1985 ) .
This parameter controls the skewness of the distribution in every generated token , and has been shown to affect model diversity ( Holtzman et al , 2019 ; Caccia et al , 2018 ) .
Then , we turn the focus to content diversity , introducing the content test ( Figure 1 ) .
Here , the tester is a human , and the diversity parameter is a binary variable , where the human is instructed to generate sets of sentences with either high or low diversity in content .
We evaluate three families of popular diversity metrics with these tests : ( a ) n - gram - based metrics that estimate diversity based on surface patterns in a set of generated sentences , ( b ) neural metrics : we propose a reduction from evaluating sentence similarity to evaluating diversity , then evaluate diversity using state - of - the - art sentence similarity models , and ( c ) human evaluation : we explore multiple ways in which humans can be asked to estimate diversity , resulting in multiple Human Diversity Score ( HDS ) variations .
Applying our tests leads to several ﬁndings : ( i ) In the decoding test , n - gram - based metrics correlate well with decoding parameters , such as softmax temperature .
While the goal of our framework is to evaluate diversity metrics , this result lets us reﬂect back on the tester itself and conclude that decoding parameters predominantly control the form of text rather than content .
( ii ) Conversely , n - gram - based metrics perform poorly in the content test .
While neural metrics outperform n - gram - based metrics , humans are substantially better than any automatic metric at detecting content diversity .
This is illustrated in Figure 1 , where a human clearly distinguishes between sets that have high ( blue ) and low ( orange ) content diversity , while n - gram - based metrics fail to do so .
Due to this gap , we construct a large dataset focused on content - diversity metrics .
We release the Metrics for content Diversity ( McDiv ) benchmark , a challenge for research in diversity evaluation .
To conclude , our main contributions are : • A framework for evaluating diversity metrics .
• Tests instantiating this framework , measuring the sensitivity of metrics to diversity , with a focus on content diversity .
• Best practices for obtaining diversity evaluations
from crowdsourcing workers .
• Establishing that humans outperform current automatic metrics in detecting content diversity .
•
The McDiv dataset - a benchmark for content
diversity aware metrics .
• The collected data , test scores and code are publicly available,1 and can be used to easily compare new diversity metrics to existing results in our framework .
2 Background : Diversity Evaluation
Recently , interest in diversity has increased ( Du and Black , 2019 ; Holtzman et al , 2019 ) , resulting in multiple proposals for its evaluation .
We describe recent approaches , highlighting the need for a standard way to evaluate metrics .
Perplexity is the standard metric in language modeling , measuring the proximity of a language model ( LM ) , PLM , to the true distribution , Pref , by approximating the cross - entropy H(Pref , PLM ) with held - out data from Pref .
Thus , perplexity captures to some extent diversity .
For example , a dialog model that puts all probability mass on the output “ I do n’t know ” for any given context will obtain inﬁnite perplexity once it encounters any other response .
This property makes perplexity popular in LM - based NLG models , and often it is the only reported measure for diversity ( Lewis et al , 2017 ; Fan et al , 2018 ; Wang et al , 2019 ; Li et al , 2019 ) .
However , perplexity does not purely measure diversity , and high perplexity does not entail low diversity .
For example , a LM with a uniform distribution over the vocabulary for each decoded token has high diversity , but its perplexity will be extremely high , due to its low quality .
Moreover , perplexity evaluates a LM , while the diversity of a NLG system is also strongly affected by the decoding procedure .
For example , Top - k and nucleus
1https://github.com/GuyTevet/
diversity - eval
327  sampling are popular decoding schemes that tradeoff quality and diversity by ignoring some of the LM probability mass ( Holtzman et al , 2019 ) .
Last , some NLG models , such as Generative Adversarial Networks ( GANs ) ( Yu et al , 2017 ) are not language models .
While one can approximate perplexity for such models ( Tevet et al , 2019 ) , ideally , a metric should not be tied to a model .
N - gram - based metrics A popular metric is distinct n - grams ( Li et al , 2016b ) , which computes the proportion of unique n - grams out of the total number of n - grams in a set of generated sentences .
Duˇsek et al ( 2020 ) calculated Shannon entropy ( Manning et al , 1999 ) based on different n - grams as a measure of lexical diversity .
SelfBLEU
( Zhu et al , 2018 ; Shu et al , 2019 ) measures the BLEU score of a generated sentence with respect to another generated sentence ( rather than a gold reference ) .
High average Self - BLEU indicates high similarity between generated sentences and low diversity .
In § 5 we expand this idea and suggest a reduction from any similarity metric to a diversity metric .
By design , n - gram based metrics are sensitive to diversity in the form of language , rather than its meaning .
Embedding - based metrics A new line of metrics suggests to embed generated sentences in latent space , then evaluate them in this space .
Du and Black ( 2019 ) suggest to cluster the embedded sentences with k - means , then use its inertia as a measure for diversity .
Recently , Lai et al ( 2020 ) suggested to consider the volume induced by the embedded sentences as a diversity metric .
Human evaluation Yang et al ( 2019 ) asked humans to evaluate the internal diversity of a generated essay .
Ghandeharioun et al ( 2019 ) let crowdsourcing workers interact with a dialog chat - bot , then asked them to evaluate the diversity of a single conversation .
In contrast , this paper focuses on the diversity of different responses given a context , as in Zhang et al ( 2019b ) .
To conclude ,
increasing interest in diversity resulted in multiple proposed diversity metrics .
However , there is no consensus on how to evaluate diversity and what each metric actually measures .
3 Evaluating Diversity Metrics
Diversity Parameter d
“ How are you today ? ” c
Tester
/
Gd(c )
“ Very good ! ”
“ Fine thank you . ”
“ Could n’t be better . ”
Diversity Metric mdiv(Sc , d )
Test Score ρ(mdiv , d )
Figure 2 : An overview of our diversity metrics evaluation framework .
The tester ( machine or human ) generates a response set ( Sc , d ) given a diversity parameter ( d ) and a context ( c ) .
The test score of a metric mdiv is the correlation between the metric score for Sc , d and d.
stance , a set of sentences can be diverse in terms of their content , while another may have similar content , but diverse form ( Figure 1 ) .
Our framework provides a way to evaluate metrics for different aspects of diversity under moderate assumptions .
We deﬁne a diversity metric mdiv(Sc ) ∈ R as a function that takes a set of generated responses Sc as an input , and outputs a diversity score .
Each response s ∈ Sc is generated for the same input context c , hence Sc is a sample from a generative distribution Pgen(s | c ) .
The overall diversity score of a generative model can be obtained by averaging mdiv over sets Sc sampled from the model given multiple contexts c ∈ C.
To evaluate mdiv ( · ) , we assume access to some deterministic diversity parameter d that controls an aspect of diversity in Sc .
We test the relation between mdiv and the parameter d.
By varying d and measuring mdiv , we can compute the correlation ρ between mdiv and an aspect of diversity represented by d. Because our goal is to have metrics that rank the diversity of generated texts , we use Spearman ’s ρ rank correlation as our test score .
Figure 2 illustrates the ﬂow of a test in our framework .
In practice , to control the diversity level of Sc using d , we use a tester : a generative model that takes a context c and a diversity parameter d as input , and outputs a response set Sc , d. We stress that the tester can be either a neural model or a human .
A good tester should reliably represent the diversity level quantiﬁed by d.
We now describe our framework for evaluating diversity metrics .
Diversity has many facets : for inAs a hypothetical example , c can be a movie name and d represent sentiment diversity , that is ,
328  the number of different sentiments in a collection of reviews Sc .
A human tester can observe c and d , and produce reviews accordingly ( such data can be easily mined from IMDB ) .
A collection of such ( d , Sc , d ) makes a test , in which the correlation between mdiv(Sc , d ) and d measures the sensitivity of mdiv to sentiment diversity .
We now describe two tests that instantiate this roughly corresponding to the two framework , main aspects of diversity : form diversity and content diversity .
3.1 Decoding Test
The diversity of a NLG system constructed from a LM depends on both the LM but also the decoding algorithm on top of it .
For example , beam search approximates the most probable output , and dramatically reduces diversity .
Conversely , sampling from the LM leads to high diversity , but low quality output ( Holtzman et al , 2019 ) .
A popular method to control diversity in NLG systems is to vary some decoding parameter .
Variations include ( a ) softmax temperature ( Ackley et al , 1985 ) , where a parameter τ controls the skewness of the softmax distribution at each step , ( b ) Nucleus ( Top - p ) sampling ( Holtzman et al , 2019 ) , where one samples at each step from the minimal set of most probable tokens whose cumulative probability is at least p , and ( c ) Top - k sampling , which samples from the top - k most probable tokens at each step .
All methods skew the LM distribution in a way that avoids low - probability tokens and leads to higher quality ( Holtzman et al , 2019 ) , providing a decoding parameter that trades off quality and diversity ( Caccia et al , 2018 ) .
In the decoding test ( decTest ) , we deﬁne the tester to be a LM , such as GPT-2 ( Radford et al , 2019 ) , and the diversity parameter d to be a decoding parameter such as temperature .
We check how different diversity metrics mdiv correlate with decoding parameters .
This can shed light on the quality of the metrics , but also on how decoding parameters affect the output of a NLG system .
The decoding test uses automatically - generated data that is cheap to produce , and decoding parameters that are well - known to control diversity .
Thus , we view this test as a warm - up test to explore the strengths of our framework .
of content diversity
.
Measuring content diversity requires deep understanding of the semantics of responses in Sc .
To isolate content from form diversity , we aim to generate response sets with a similar level of form diversity , but where the level of content diversity is controlled by the diversity parameter d.
Thus , we use crowdsourcing workers as testers , and a binary parameter d ∈ { 0 , 1 } , corresponding to low or high content diversity .
A worker observes a context c and produces a set of responses Sc based on the value of d. We encourage workers to use different words and phrases in different responses regardless of the value of d , such that form diversity is high in all examples .
Examples from this data are in Figure 1 and Appendix B.
In § 6 , we will focus on whether automatic diversity metrics can perform as well as humans on the task of estimating content diversity .
4 Human Diversity Score
One of the core questions we tackle is : Can humans evaluate diversity reliably ?
Although a few papers ( Ghandeharioun et al , 2019 ; Yang et al , 2019 ; Zhang et al , 2019b ) asked humans to evaluate diversity , to the best of our knowledge no work thoroughly investigated this question .
The importance of this question is clear when comparing to quality evaluation .
There , human judgment is the gold standard , and automatic quality metrics are established by showing high correlation with human score .
Thus , understanding if humans can judge diversity is important for improving diversity metrics .
We use crowdsourcing workers2 to compute a human diversity score : we show workers a context followed by a set of responses , and ask them to rate the diversity of the set .
To establish best practices , we experiment with multiple variations of HDS ( detailed in § 6.2 ) , asking humans to rate the diversity of a response set , and evaluating each practice with our framework .
We focus on the following questions : • Should humans rate diversity of a set or similarity between pairs in the set , from which diversity can be inferred ?
( tl;dr : diversity )
• Can humans evaluate different aspects of diversity well ?
( tl;dr : not effectively )
• Should humans rate the absolute diversity score of a set of sentences or rank whether one set is
2Native English speakers ,
for more details see Ap3.2 Content Test
In the content test ( conTest ) , our goal is to evaluate how different diversity metrics capture the notion
pendix A.
329  more diverse than another ?
Here , we did not reach a conclusive result , and describe this experiment in the Appendix C.
As a preliminary step , we conducted pilot experiments among a group of NLP graduate students .
The main insights were : ( a ) humans are biased by quality : if a generated set has high diversity but low quality , humans will rate diversity low .
To neutralize this , we explicitly ask workers to evaluate the quality of one of the responses in the set Sc , and then instruct them to ignore quality in diversity questions ; ( b ) To make sure a worker reads the context c , we ask them to generate a sentence s before they rate diversity ; ( c ) It is difﬁcult for workers to evaluate the diversity of a set with more than 10 responses .
Our crowdsourcing tasks are provided in Appendix A.
5 Diversity to Similarity Reduction
We expand the idea from Zhu et al ( 2018 ) and suggest a method to construct a diversity metric from any 2 - sentence similarity metric .
Given msim(s1 , s2 ) ∈ R , a symmetric similarity metric that gets a pair of input sentences ( s1 , s2 ) and returns a similarity score , we can deﬁne a diversity metric ˜mdiv as the negation of the mean similarity score across all ( unordered ) pairs of Sc :
˜mdiv(Sc )
= −
1 ( cid:0)|Sc| 2
( cid:1 )
( cid:88 )
msim(si , sj ) .
si , sj ∈Sc , i > j
This reduction allows us to easily deﬁne new diversity metrics based on past work on sentence similarity ( Gomaa et al , 2013 ; Devlin et al , 2019 ; Zhang et al , 2019a ; Reimers and Gurevych , 2019 ) .
In § 6 we show that both n - gram - based similarity metrics and neural semantic similarity metrics provide useful diversity metrics .
6 Experiments
6.1 NLG Tasks
We apply our evaluation procedure on three different English NLG tasks that require diversity .
•
Story completion ( storyGen ) ; We use the ROC Stories dataset ( Mostafazadeh et al , 2016 ) , in which the context c is the ﬁrst four sentences of a story , and the response s is a single sentence that ends the story .
We use the contexts C from this data and generate response sets Sc for each context using our testers .
The long contexts characterizing this data narrow down the space of
possible responses , making this a “ low - entropy ” generation task , where the output is constrained , but diversity is still essential .
• Dialog response generation ( respGen ) ; A comment - response pairs dataset extracted from the website reddit.com and pre - processed by Hashimoto et al ( 2019 ) .
We use the comments from their data as contexts C and generate response sets Sc for each context using our testers .
Since comments are single sentences the response is less constrained , making this a “ medium - entropy ” generation task .
• 3 - words prompt completion ( promptGen ) ; Contexts C are 3 - words prompts , extracted from the Cornell Movie - Dialogs Corpus ( DanescuNiculescu - Mizil and Lee , 2011 ) by taking the ﬁrst three words from each original context .
The response sets Sc are completions of the prompts , generated by our testers .
This context provides minimal constraints , making this a “ highentropy ” generation task .
Samples of the contexts extracted for each task , along with generated response sets , are presented in Appendix B.
We intentionally avoid NLG tasks where diversity is not necessarily desired , such as summarization and machine translation .
6.2 Evaluated Metrics
N - gram - based metrics We evaluate distinct ngrams ( distinct - n ) , as described in § 2 .
We also evaluate n - grams cosine similarity ( cos - sim ): a similarity measure computing the cosine between the vectors representing two sentences , where each vector is a count vector over the n - grams that appear in the response .
We use the reduction from § 5 to convert this to a diversity measure .
In both metrics , rather than choosing the order of the ngrams , we average over n ∈ { 1 , . .
.
, 5 } , which we found to outperform any single choice of n. Neural metrics We exploit existing BERT - based models ( Devlin et al , 2019 ) ﬁne - tuned for estimating similarity between two sentences ( applying the reduction from § 5 ) .
BERT - STS ; A BERT model ﬁne - tuned on Semantic Textual Similarity ( Cer et al , 2017 ): a collection of sentence pairs annotated with scores from 1 - 5 denoting their semantic similarity.3 BERT - Score ( Zhang et al , 2019a ) ; Originally a quality metric , BERT - Score uses BERT ’s embeddings to measure similarity between two sen3https://github.com/swen128/bert-sts
330  ( Reimers
tences .
We used RoBERTa - large ( Liu et al , 2019 ) , as suggested by the authors.4 Sentence - BERT ( sent - BERT ) and Gurevych , 2019 ) is a sentence - level embedding model based on BERT .
We use the cosine similarity between the embeddings of two responses as a similarity metric .
In our experiments we used bert - large - nli - stsb - mean - tokens.5 Human Metrics We examine four methods for evaluating diversity with humans ( see § 4 ) , to investigate best practices for obtaining diversity judgment from humans .
In all metrics ( except ranking ) , ratings are from 5 ( highest diversity / similarity ) to 1 ( lowest ) .
The original tasks presented to workers are in Appendix A. Absolute HDS ( absHDS ) ; Given a context c and a set of generated responses Sc , rate the level of diversity of Sc .
Ranking HDS ( rnkHDS ) ; Given a context c and two sets Sc , d1 , Sc , d2 generated with different values of the diversity parameter d , rate which set is more diverse .
Since this metric did not clearly outperform absHDS , we provide results in Appendix C only .
Similarity HDS ( simHDS ) ; Given a context c and a set of generated responses Sc , rate the similarity of each two sentences in Sc , and then apply the reduction from § 5 .
Aspect HDS ( aspHDS ) ;
Identical to absHDS , except we explicitly ask about a speciﬁc aspect of diversity , namely form and content.6
6.3 Decoding Test
In decTest we measure the correlation between diversity metrics ( mdiv ) and the softmax temperature decoding parameter ( d ) .
The tester generating the response sets ( Sc ) is a neural NLG model .
Data and settings For each task , we generated sets of 10 responses per context , using a linear temperature sweep with 100 values in the range [ 0.2 , 1.2 ] ( Caccia et al , 2018 ) .
We generated 1 K sets in total for each of 1 K contexts ( 10 per temperature ) and evaluated 200 ( 2 random sets per temperature ) .
For automatic metrics , we repeat this 100 times ( randomly sampling 200 out of 1 K sets each time ) , to present the mean and standard
4https://github.com/Tiiiger/bert_score 5https://github.com/UKPLab/
sentence - transformers
6We note that perplexity can not be evaluated as a diversity metric in our framework , because it requires a sample from Pref , while we assume a response set sampled from Pgen .
Context
Fire next door .
John woke up smelling like something was burning .
He went outside .
He saw the ﬁre next door .
He called the authorities .
Response set ( τ = 0.25 )
• It was a minor ﬁre and they put it out .
•
It was a ﬁre .
•
It was a ﬁre .
•
It was a ﬁre .
•
It was a ﬁre .
Response set ( τ = 0.8 )
• They arrived and put out the ﬁre .
•
It was a ﬁre .
•
It was a ﬁre .
•
It turned out to be a ﬁre .
•
It was a minor ﬁre night .
Response set ( τ = 1.1 )
• It turned out to be a mechanic .
•
Before the ﬁre was put out it was a ﬁre .
•
It was a ﬁre .
•
They co - worker matter how bad the ﬁre was .
• Several shells , the ﬁre department came just in time .
Table 1 : An example of the effect of temperature on the response set Sc for a context c from ROC Stories .
deviation .
HDS metrics are computed over one experiment of 200 sets , due to their high cost .
Data for storyGen and respGen was generated by the MASS model ( Song et al , 2019 ) , ﬁne - tuned on each dataset .
Data for promptGen was generated by GPT-2 - large ( Radford et al , 2019 ) without ﬁne - tuning .
We provide examples for how story endings change as a function of temperature in Table 1 .
Examples for all tasks along with additional reproducibility details are in the Appendix B. For each HDS metric , we collected 10 ratings per query from Amazon Mechanical Turk ( AMT ) workers .
While absHDS demands one query per response set , in order to perform simHDS at a reasonable cost , we chose |Sc| = 5 , ( cid:1 ) = 10 crowdsourcing queries inresulting in ( cid:0)5 2 stead of ( cid:0)10 ( cid:1 ) = 45 per set .
We evaluate simHDS 2 only for respGen due to the metric ’s high cost and low performance .
Results Table 2 presents results of absHDS , simHDS , and all automatic metrics .
In general , ngram based metrics capture the diversity induced by a temperature sweep , beating HDS and neural metrics .
Figure 3 provides a more detailed analysis .
Each point represents a single set of responses generated at some temperature .
While rank correlation for cosine similarity is high , it is
331  Figure 3 : decTest : Scatter plot of n - gram - based ( cosine similarity ) , neural ( BERT - STS ) and human ( absHDS ) metrics as a function of temperature for respGen .
Each point corresponds to a single generated set .
Error bars of HDS represent the standard deviation over 10 annotator ratings .
storyGen
respGen
promptGen
Temperature
Top - p
Top - k
distinct - n cos - sim
0.76 ( 0.03 ) 0.71 ( 0.04 )
0.89 ( 0.01 ) 0.89 ( 0.01 )
BERT - STS sent - BERT BERT - score
0.64 ( 0.04 ) 0.65 ( 0.03 ) 0.69 ( 0.04 )
0.81 ( 0.02 ) 0.80 ( 0.02 ) 0.87 ( 0.01 )
absHDS simHDS
0.69 0.81 0.74
0.91 ( 0.01 ) 0.87 ( 0.02 )
0.84 ( 0.02 ) 0.74 ( 0.03 ) 0.88 ( 0.02 )
0.79 Table 2 : decTest results : Spearman ’s ρ correlation between temperature and each metric score ( mean and standard deviation ) .
simHDS was tested only on respGen .
far from linear and reaches high values even at low temperatures , scoring 0.6 Pearson correlation .
Conversely , the correlation for BERT - STS and absHDS is more linear , scoring 0.75 and 0.77 Pearson correlation respectively .
Thus , Pearson and Spearman correlations disagree on the quality of the different metrics in this case .
While our framework is meant to evaluate diversity metrics , the results of the test let us reﬂect on the decoding parameters themselves .
This result shows that humans perform worse than automatic metrics in this experimental setup , hinting that temperature mostly controls superﬁcial changes to the generated text .
Additionally , simHDS performs worse than absHDS although it is 3x more expensive , showing that rating the entire set rather than averaging over pairs is useful .
Other decoding parameters To compare the robustness of our conclusions to other decoding parameters , we repeat it with two additional decoding methods : ( a ) in Nucleus ( Top - p ) sampling we swept linearly over 100 values of p in the range [ 0.1 , 1.0 ] ; ( b ) In Top - k sampling we swept k in logarithmic scale over 100 values in the range [ 1 , 30 K ] and present the correlation between the
distinct - n cos - sim
BERT - STS sent - BERT BERT - score
0.91 ( 0.01 ) 0.87 ( 0.02 )
0.84 ( 0.02 ) 0.74 ( 0.03 ) 0.88 ( 0.02 )
0.84 ( 0.02 ) 0.78 ( 0.03 )
0.61 ( 0.05 ) 0.48 ( 0.05 )
0.74 ( 0.03 ) 0.63 ( 0.05 ) 0.77 ( 0.03 )
0.55 ( 0.05 ) 0.51 ( 0.05 ) 0.57 ( 0.05 )
Table 3 : decTest results for different decoding parameters : Spearman ’s ρ ( mean and standard deviation ) of automatic metrics for promptGen .
metrics and log10(k )
.
While softmax temperature enables skewing PLM to a more diverse Pgen using τ >
1 , both Top - p and Top - k enable only skewing PLM to a more sharp ( hence less diverse ) Pgen .
Table 3 presents results for all automatic metrics using the three decoding methods over promptGen .
Results for other tasks are in Appendix C.
We ﬁnd that Top - p correlates well with temperature along all three generation tasks , whereas Topk does not correlate with any of them .
6.4 Content Test
In conTest , we measure the correlation between diversity metrics ( mdiv ) and content diversity , represented by a binary parameter d ∈ { 0 , 1 } .
The testers are AMT workers , guided to create sets with high level of form diversity and high or low content diversity according to d.
Data and settings For each task , we collected 200 sets of 5 responses each ( 100 sets per class ) .
For high content diversity class , we asked workers to give 5 responses per context , with as different content and structure as possible .
Then we asked the same workers to choose a single response they wrote , and rephrase it 5 times such that the original content will be preserved , while changing the form – this set is used for the low content diversity class .
3320.20.40.60.81.01.2Temperature0.00.20.40.60.81.0Metric ScoreCosine Similarity0.20.40.60.81.01.2Temperature0.00.10.20.30.4BERT - STS0.20.40.60.81.01.2Temperature12345absHDS  Figure 4 : conTest : histograms of metric values of n - gram ( distinct n - grams ) , neural ( BERT - Score ) and human ( absHDS ) metrics for promptGen .
The orange histogram represents the distribution of the low content diversity class , the blue histogram represents the distribution of the high content diversity class and brown is the intersection between the two .
Pointing down triangles represent the threshold η of the optimal classiﬁers .
The histograms show how each metric separates the two classes .
A sample from this data is in Figure 1 and more samples in Appendix B. For each HDS metric , we collected 10 ratings from crowdsourcing workers , different than the ones who composed the sets .
In addition to Spearman ’s ρ , we report Results the optimal single - threshold classiﬁer accuracy ( OCA ) , i.e. , the best achievable accuracy in predicting the class of a response set ( high or low content diversity ) for any threshold η on mdiv , such that if mdiv(Sc ) >
η the classiﬁer predicts high diversity , and otherwise predicts low diversity .
Table 4 shows the results .
N - gram - based metrics perform poorly , indicating they do not measure content diversity well .
Neural models perform better than n - gram - based metrics ( especially sent - BERT ) , but there is still a clear gap between automatic metrics and humans .
Figure 4 illustrates the typical distributions of n - gram , neural and human metrics .
Clearly , HDS separates high and low content diversity better than neural metrics .
In addition , n - gram - based metrics saturate both classes to near maximal values , similarly to decTest .
Since conTest
isolates content diversity , we used aspHDS to directly rate content and form diversity .
Content aspHDS gets similar scores to absHDS , suggesting little gain in asking directly on the tested aspect .
Form aspHDS gets low scores compared to absHDS , validating that the form diversity of the two classes is similar .
Content Diversity Benchmark We construct the Metrics for content Diversity ( McDiv ) benchmark , focusing on metrics for content diversity .
McDiv is a dataset containing 6 K { c , Sc } pairs , ( 2 K for each storyGen , respGen and promptGen ) collected as described in this section .
McstoryGen ρ
OCA
respGen ρ
OCA
promptGen OCA
ρ
distinct - n cos - sim
BERT - STS sent - BERT BERT - score
absHDS aspHDSform aspHDScontent
0.57 0.56
0.6 0.77 0.59
0.85 0.35 0.84
0.77 0.77
0.78 0.90 0.77
0.95 0.65 0.94
0.34 0.33
0.46 0.59 0.49
0.63 0.56 0.67
0.67 0.66
0.72 0.79 0.74
0.81 0.79 0.83
0.33 0.36
0.65 0.68 0.4
0.78 0.4 0.75
0.68 0.67
0.82 0.81 0.69
0.89 0.68 0.88
Table 4 : conTest results : Spearman ’s ( ρ ) correlation between a set ’s class and each metric score .
Div contains a subset of 3 K examples , termed McDivnuggets , in which form diversity was neutralized , providing a difﬁcult meta - evaluation challenge .
McDivnuggets was sampled to ensure that the correlation of distinct - n ( a form diversity metric ) is zero over this subset .
Applying conTest over the data shows that n - gram based metrics obtain near - zero values on McDivnuggets as expected , and all neural metrics perform substantially worse on McDivnuggets than on McDiv .
On conTest , we obtain absHDS annotations for more than 200 random samples from McDivnuggets and obtain 0.7 Spearman ’s ρ for the respGen task , substantially higher than the best performing neural metric ( sent - BERT ) score at 0.6 .
Details and conTest results can be found in Appendix C.
HDS Stability :
Picking Parameter Values HDS experiments demand expensive human labor .
Thus , we need to carefully choose the number of sets and different ratings we ask per set , to get reliable results in a reasonable budget .
To this end , we conducted two series of experiments , once increasing the number of sets , and again increasing the number of ratings per sets .
By observing results along those two series , we chose to use 200
3330.40.50.60.70.80.91.0Metric Valuesdistinct - n ( averaged)high content diversitylow content diversity0.40.60.81.0Metric ValuesBERT Score2.53.03.54.04.5Metric ValuesabsHDS  Figure 5 : conTest absHDS results depends on the number of ratings per set and the number of sets .
