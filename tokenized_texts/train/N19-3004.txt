The Strength of the Weakest Supervision : Topic Classiﬁcation Using Class Labels Jiatong Li1 , Kai Zheng2 , Hua Xu3 , Qiaozhu Mei4 , Yue Wang5 1Department of Computer Science , Rutgers University 2Department of Informatics , University of California , Irvine 3School of Biomedical Informatics , The University of Texas Health Science Center at Houston 4School of Information , University of Michigan , Ann Arbor 5School of Information and Library Science , University of North Carolina at Chapel Hill 1jiatong.li@rutgers.edu , 2zhengkai@uci.edu , 3hua.xu@uth.tmc.edu , 4qmei@umich.edu , 5wangyue@email.unc.edu
Abstract
When developing topic classiﬁers for realworld applications , we begin by deﬁning a set of meaningful topic labels .
Ideally , an intelligent classiﬁer can understand these labels right away and start classifying documents .
Indeed , a human can conﬁdently tell if a news article is about science , politics , sports , or none of the above , after knowing just the class labels .
We study the problem of training an initial topic classiﬁer using only class labels .
We investigate existing techniques for solving this problem and propose a simple but effective approach .
Experiments on a variety of topic classiﬁcation data sets show that learning from class labels can save signiﬁcant initial labeling effort , essentially providing a “ free ” warm start to the topic classiﬁer .
1
Introduction
When developing topic classiﬁers for real - world tasks , such as news categorization , query intent detection , and user - generated content analysis , practitioners often begin by crafting a succinct deﬁnition , or a class label , to deﬁne each class .
Unfortunately , these carefully written class labels are completely ignored by supervised topic classiﬁcation models .
Given a new task , these models typically require a signiﬁcant amount of labeled documents to reach even a modest initial In contrast , a human can readperformance .
ily understand new topic categories by reading the class deﬁnitions and making connections to prior knowledge .
Labeling initial examples for every new task can be time - consuming and laborintensive , especially in resource - constrained domains like medicine and law .
Therefore it is desirable if a topic classiﬁer can proactively interpret class labels before the training starts , giving itself a “ warm start ” .
An imperfect initial model can always be ﬁne - tuned with more labeled documents .
Figure 1 : Learning from class labels can give “ warm start ” to a classiﬁer , accelerating the learning process .
As conceptually shown in Figure 1 , a warm start can reduce the total number of training labels for a classiﬁer to reach certain performance level .
In this work , we study algorithms that can initialize a topic classiﬁer using class labels only .
Since class labels are the starting point of any topic classiﬁcation task , they can be viewed as the earliest hence weakest supervision signal .
We propose a simple and effective approach that combines word embedding and naive Bayes classiﬁcation .
On six topic classiﬁcation data sets , we evaluate a suite of existing approaches and the proposed approach .
Experimental results show that class labels can train a topic classiﬁer that generalizes as well as a classiﬁer trained on hundreds to thousands of labeled documents .
2 Related Work
Text retrieval .
Classifying documents by short labels can be viewed as evaluating textual similarity between a document and a label .
Baeza - Yates et al ( 2011 ) called this approach “ naive text classiﬁcation ” .
Treating labels as search queries , we can classify a document into a class if it best matches the label of that class .
Well - studied text retrieval methods , such as vector space models and probabilistic models ( Croft et al , 2010 ) , can produce matching scores .
To mitigate vocabulary mismatch , such a classiﬁer can be further enhanced
Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics : StudentResearchWorkshop , pages22–28Minneapolis , Minnesota , June3 - 5,2019.c(cid:13)2017AssociationforComputationalLinguistics22#oflabelsclassificationperformancelearningfromclasslabelsignoringclasslabelscoldstartwarmstart0  by self - training : the classiﬁer assigns pseudo labels to top - ranked documents as done in pseudo relevance feedback ( Rocchio , 1965 ) , and updates itself using those labels .
Semi - supervised learning .
Our problem setting can be seen as an extreme case of weak supervision : we only use class labels as the ( noisy ) supervision signal , and nothing else .
If we view class labels as “ labeled documents ” , one from each class , and to - be - classiﬁed documents as unlabeled documents , then we cast the problem as semisupervised learning ( Zhu , 2006 ) .
Self - training is one such technique : a generative classiﬁer is trained using only class labels , and then teaches itself using its own predictions on unlabeled data .
If we view class labels as “ labeled features ” , then we expect the classiﬁer to predict a class when a document contains the class label words .
For instance , Druck et al ( 2008 ) proposed generalized expectation criteria that uses feature words ( class labels ) to train a discriminative classiﬁer .
Jagarlamudi et al ( 2012 ) and Hingmire and Chakraborti ( 2014 ) proposed Seeded LDA to incorporate labeled words / topics into statistical topic modeling .
The inferred document - topic mixture probabilities can be used to classify documents .
Zero - shot learning aims to classify visual objects from a new class using only word descriptions of that class ( Socher et al , 2013 ) .
It ﬁrst learns visual features and their correspondence with word descriptions , and then constructs a new classiﬁer by composing learned features .
Most research on zero - shot learning focuses on image classiﬁcation , but the same principle applies to text classiﬁcation as well ( Pushp and Srivastava , 2017 ) .
Our proposed method constructs a new classiﬁer by composing learned word embeddings in a probabilistic manner .
Since the new classiﬁer transfers semantic knowledge in word embedding to topic classiﬁcation tasks , it is broadly related to transfer learning ( Pan and Yang , 2010 ) .
The main difference is that in transfer learning the information about the new task is in the form of labeled data , not class deﬁnition words .
3 Proposed Method
ity p(y|x ) ∝ p(x|y)p(y ) .
Generative approaches tends to perform well when training data is scarce , which is the case in our setting .
We assume there exists weak prior knowledge on which classes are popular and which are rare .
We can then construct rough estimates ˆp(y ) using simple heuristics as described in ( Schapire et
al , It distributes probability mass q evenly 2002 ) .
among majority classes , and 1 − q evenly among minority classes .
We treat the most frequent class as the majority class , the rest as minority classes , and q = 0.7 in our experiments .
By interpreting class topic description as words , we obtain ˆp(x|y ) = p(x|dy ) .
We assume that the dy expresses a noisy - OR relation of the words it contains ( Oni´sko et al , 2001 ) .
Up to ﬁrst - order approximation :
p(x|dy )
= 1 −
( 1 − p(x|wy ) )
( cid:89 )
wy∈dy
( cid:88 )
≈
wy∈dy
p(x|wy ) ,
( 1 )
where each wy is a word in the class topic description dy .
Further , we assume that words in document x are conditionally independent given a label word wy ( na¨ıve Bayes assumption ):
p(x|wy )
=
p(wj|wy ) .
( 2 )
( cid:89 )
wj ∈x
Combining ( 1 ) and ( 2 ) , the document likelihood is
ˆp(x|y )
=
p(wj|wy ) .
( 3 )
( cid:88 )
( cid:89 )
wy∈dy
wj ∈x
To this end , we need a word association model p(w1|w2 ) , ∀w1 , w2 ∈ V .
It can be efﬁciently learned by word embedding algorithms .
The skipgram algorithm ( Mikolov et al , 2013 ) learns vector representations of words , such that for words w1 , w2 , their vectors uw1 , vw2 approximate the conditional probability1
p(w1|w2 )
=
( cid:80 )
( cid:1 )
exp ( cid:0)u(cid:62 ) w1vw2 w∈V exp ( u(cid:62 )
wvw2 )
.
( 4 )
Let a test document x be a sequence of words ( w1 , · · · , wj , · · · ) , and a class topic description y be a sequence of words dy = ( w1 , · · · , wy , · · · ) .
All words are in vocabulary V .
We propose a generative approach , where the predictive probabil1The two sets of word vectors { uw : w ∈ V } and { vw : w ∈ V } produced by skip - gram correspond to the input and output parameters of a two - layer neural network .
Typically , only the output parameters are used as the “ learned word vectors ” .
Here we need both input and output parameters to compute p(w1|w2 ) .
23  n ( cid:88 )
( cid:88 )
i=1
y∈Y
m ( cid:88 )
( cid:88 )
j=1
y∈Y
Combining ( 3 ) with ( 4 ) , the document likelihood becomes
ˆp(x|y )
=
( cid:88 )

exp

( cid:88 )
( cid:16 )
wy∈dy
wj ∈x
u(cid:62 )
wj vwy − Cwy

( cid:17 )
 ,
w∈V exp ( cid:0)u(cid:62 )
( cid:1 ) is indewhere
Cwy = log ( cid:80 ) pendent of document x and only related to label word wy , therefore can be precomputed and stored to save computation .
wvwy
Finally , we construct an generative classiﬁer as ˆp(y|x ) ∝ ˆp(x|y)ˆp(y ) .
We call this method word embedding na¨ıve Bayes ( WENB ) .
3.1 Continued Training
The proposed method produces pseudo labels ˆp(y|xj ) for unlabeled documents { xj}m j=1 .
When true labels { ( xi , yi)}n i=1 are available , we can train a new discriminative logistic regression classiﬁer pθ(y|x ) using both true and pseudo labels ( θ is the model parameter ):
J(θ )
=
−1{yi = y } log pθ(y|xi )
+ λ ( cid:107)θ(cid:107)2
+ µ
−ˆp(y|xj ) log pθ(y|xj ) .
( 5 )
To ﬁnd the balance of pseudo vs.
true labels in ( 5 ) , we search the hyperparameter µ on a 5point grid { 10−2 , 10−1 , 0.4 , 0.7 , 1 } .
We expect pseudo labels to have comparable importance as true labels when n is small ( ﬁne granularity for µ ∈ [ 10−1 , 1 ] ) , and their importance will diminish as n gets large ( µ = 10−2 ) .
µ is automatically selected such that it gives the best 5 - fold crossvalidation accuracy on n true labels .
4 Experiments
We compare a variety of methods on six topic classiﬁcation data sets .
The goals are ( 1 ) to study the best classiﬁcation performance achievable using class labels only , and ( 2 ) to estimate the equivalent amount of true labels needed to achieve the same warm - start performance .
4.1 Compared Methods
Retrieval - based methods .
We use language modeling retrieval function with Dirichlet smoothing ( Zhai and Lafferty , 2001 ) ( µ = 2500 ) to match a document to class labels ( IR ) .
The top 10 results
are then used as pseudo - labeled documents to retrain three classiﬁers : IR+Roc : a Rocchio classiﬁer ( α = 1 , β = 0.5 , γ = 0 ) ; IR+NB : a multinomial naive Bayes classiﬁer ( Laplace smoothing , α = 0.01 ) ; IR+LR a logistic regression classiﬁer ( linear kernel , C = 1 ) .
ST-0 :
Semi - supervised methods .
the initial self - training classiﬁer using class labels as “ training documents ” ( multinomial na¨ıve Bayes , Laplace smoothing α = 0.01 ) .
ST-1 : ST-0 retrained on 10 most conﬁdent documents predicted by itself .
GE : a logistic regression classiﬁer trained using generalized expectation criteria ( Druck et al , 2008 ) .
Class labels are used as labeled features .
sLDA : a supervised topic model trained using seeded LDA ( Jagarlamudi et al , 2012 ) .
Besides k seeded topics ( k is the number of classes ) , we use an extra topic to account for other content in the corpus .
Word embedding - based methods .
Cosine : a centroid - based classiﬁer , where class deﬁnitions and documents are represented as average of word vectors .
WENB :
The proposed method ( Section 3 ) .
WENB+LR : a logistic regression classiﬁer trained only on pseudo labels produced by WENB ( Section 3.1 , n = 0 ) .
For general domain tasks , we take raw text from English Wikipedia , English news crawl ( WMT , 2014 ) , and 1 billion word news corpus ( Chelba et al , 2013 ) to train word vectors .
For medical domain tasks , we take raw text from MEDLINE abstracts ( NLM , 2018 ) to train word vectors .
We ﬁnd 50 - dimensional skip - gram word vectors perform reasonably well in the experiments .
4.2 Data Sets
We consider six topic classiﬁcation data sets with different document lengths and application domains .
Table 1 summarizes basic statistics of these data sets .
Table 4 and 5 in the appendix show actual class labels used in each data set .
Data set Wiki Titles News Titles Y Questions 20 News Reuters Med WSD
Avg word / doc 3.1 ( 1.1 ) 6.7 ( 9.5 ) 5.0 ( 2.6 ) 101.6 ( 438.5 ) 76.5 ( 117.3 ) 202.8 ( 46.6 )
# classes 15 4 10 20 10 2 / task
# docs 30,000 422,937 1,460,000 18,846 8,246 190 / task
Table 1 : Statistics of topic classiﬁcation data sets .
Numbers in column “ Avg word / doc ” are “ mean ( standard deviation ) ” .
24  Majority guess IR IR+Roc IR+NB IR+LR ST-0 ST-1 GE sLDA Cosine WENB WENB+LR
Wiki Titles News Titles Y Questions .83 3.14 ( .25 ) 2.93 ( .24 ) 5.44 ( .53 ) 3.26 ( .30 ) 3.16 ( .32 ) 5.62 ( .29 ) 9.55 ( .90 ) 7.07 ( 0.97 ) 27.67 ( .59 ) 26.70 ( .48 ) 24.88 ( .39 )
13.26 14.20 ( .06 ) 14.20 ( .06 ) 32.98 ( 2.13 ) 13.44 ( .10 ) 16.03 ( .16 ) 24.34 ( .36 ) 14.54 ( .08 ) 51.16 ( 8.10 ) 33.49 ( .11 ) 63.02 ( .10 ) 63.76 ( .11 )
1.82 6.15 ( .06 ) 8.35 ( 1.12 ) 14.45 ( .45 ) 7.38 ( 2.08 ) 6.15 ( .02 ) 10.02 ( .49 ) 31.72 ( .05 ) 40.98 ( 2.61 ) 31.16 ( .03 ) 44.89 ( .06 ) 45.69 ( .09 )
20 News .48 19.57 ( .95 ) 25.09 ( .93 ) 30.45 ( 1.46 ) 34.76 ( 1.50 ) 19.49 ( .98 ) 22.91 ( 1.29 ) 48.71 ( .41 ) 24.80 ( 4.98 ) 26.19 ( .75 ) 32.23 ( .48 ) 30.57 ( .71 )
Reuters 6.47 8.37 ( .55 ) 19.33 ( 1.87 ) 62.59 ( 2.43 ) 6.48 ( .07 ) 6.79 ( .17 ) 55.77 ( 1.62 ) 21.65 ( 27.36 ) 30.61 ( 4.80 ) 6.56 ( .16 ) 34.99 ( 1.99 ) 32.04 ( 1.44 )
Med WSD 34.20 52.99 ( .64 ) 59.89 ( .54 ) 82.12 ( .41 ) 68.35 ( .38 ) 69.11 ( .26 ) 82.97 ( .56 ) 62.63 ( .37 ) 69.81 ( 1.09 ) 32.65 ( .19 ) 68.27 ( .20 ) 62.57 ( .19 )
Table 2 : Macro - averaged F1 ( % ) of compared methods on different data sets .
The numbers are “ mean ( standard deviation ) ” of 5 - fold cross validation .
Top two numbers in each column are highlighted in boldface .
Data set Wiki Titles News Titles Y Questions 20 News Reuters Med WSD
# of labels 1500 200 1500 - 2000 100 - 200 100 - 200 20 / task × 198 tasks
Table 3 : Number of true labels needed for a logistic regression classiﬁer to achieve the same performance as “ WENB+LR ” .
Three short text data sets are ( 1 ) Wiki Titles : Wikipedia article titles sampled from 15 main categories ( Wikipedia Main Topic ) .
( 2 ) News Titles : The UCI news title data set ( Lichman , 2013 ) .
( 3 ) Y Questions : User - posted questions in Yahoo Answers ( Yahoo Language Data , 2007 ) .
Three long text data sets are ( 1 ) 20 News : The well - known 20 newsgroup data set .
( 2 ) Reuters .
The Reuters-21578 data set ( Lewis ) .
We take the articles from the 10 largest topics .
( 3 ) Med WSD : The MeSH word sense disambiguation ( WSD ) data set ( Jimeno - Yepes et al , 2011 ) .
Each WSD task aims to tell the sense ( meaning ) of an ambiguous term in a MEDLINE abstract .
For instance , the term “ cold ” may refer to Low Temperature , Common Cold , or Chronic Obstructive Lung Disease , depending on its context .
These senses are used as the class labels .
We use 198 ambiguous words with at least 100 labeled abstracts in the data set , and report the average statistics over 198 independent classiﬁcation tasks .
Although no true labels are used for training , some methods require unlabeled data for retrieval , pseudo - labeling , and re - training .
We split unlabeled data into 5 folds , using 4 folds to “ train ” a classiﬁer and 1 fold for test .
We use macroaveraged F1 as the performance metric because not all data sets have a balanced class distribution .
4.3 Results and Discussion
Label savings .
Table 2 shows that overall , class labels can train text classiﬁers remarkably better than majority guess .
This is no small feat considering that the classiﬁer has not seen any labeled documents yet .
Such performance gain essentially comes “ for free ” , as any text classiﬁcation task has to start by deﬁning classes .
In Table 3 , we report the number of true labels needed for a logistic regression model to achieve the same performance as WENB+LR .
The most signiﬁcant savings happen on short documents : class labels are equivalent to hundreds to thousands of labeled documents at the beginning of the training process .
Effect of document length .
On short documents ( Wiki Titles , News Titles , Y Questions ) , leveraging unlabeled data does not help with most semi - supervised methods due to severe vocabulary mismatch .
The proposed methods ( WENB and WENB+LR ) show robust performance , because pretrained word vectors can capture semantic similarity even without any word overlap between a class label and a document .
This prior knowledge is essential when documents are short .
On long documents ( 20 News , Reuters , Med WSD ) , leveraging unlabeled data helps , since long documents have richer content and are more likely to contain not only label words themselves , but also other topic - speciﬁc words .
Retrieval - based and semi - supervised methods are able to learn these words by exploiting intra - document word co - occurrences .
Performance of other methods .
Learning from class labels themselves provides very limited help ( IR and ST-0 ) .
Using class labels as search queries and labeled documents are closely related : IR and ST-0 perform similarly ; so do IR+NB and ST-1 .
When using class labels as search queries ,
25  Figure 2 : Continued training behavior : Atheism vs. Autos .
Colored band : ±1 standard deviation .
Figure 3 : Continued training behavior : Medical vs. Mideast .
Colored band : ±1 standard deviation .
re - ranking ( IR+Roc ) is less useful than training classiﬁers ( IR+NB and IR+LR ) .
After initial retrieval , training a na¨ıve Bayes classiﬁer is almost always better than a logistic regression classiﬁer ( IR+NB vs. IR+LR ) , demonstrating the power of generative models when supervision signal is sparse .
Using class labels as labeled features ( GE and sLDA ) performs well occasionally ( GE on 20 News ; sLDA on Y Questions ) , but not consistently .
The Cosine method performs well only on Wiki Titles , the shortest documents , because without supervision , representing a long document as an average of word vectors causes signiﬁcant information loss .
Finally , it is encouraging to see WENB+LR sometimes outperform WENB , as WENB+LR is much smaller than WENB+LR in terms of model size .
4.4 Continued Training and Error Analysis
Figure 2 and 3 compare logistic regression classiﬁers trained with and without pseudo labels generated by WENB .
Note that the classiﬁer trained with pseudo labels ( cont . train ) has a much lower performance variance than the logistic regression classiﬁer trained only on true labels ( LR ) .
The warm - started classiﬁer can serve as a good starting point for further training .
Figure 2 shows a salient warm - start effect on a balanced binary classiﬁcation task in 20 News .
The weight µ of pseudo labels increases when true labels are few ( initial classiﬁer as an informative prior ) .
As expected , µ decreases when true labels become abundant .
Figure 3 shows another binary classiﬁcation task in 20 News where the warm - start effect is limited .
Correspondingly , µ quickly diminishes as more true labels are available .
With 100 or more true labels , pseudo labels have a negligible weight ( µ = 10−2 ) .
In machine learning terms , these pseudo labels specify an incorrect prior that the model should quickly forget , so that it will not hinder the overall learning process .
A closer investigation reveals that the word vector for mideast ( the class label of one topic in Figure 3 ) is not well - trained .
This is because in general text corpus , the word mideast is rather infrequent compared to commonly used alternatives , such as middle east .
The word vector of mideast is surrounded by other infrequent words or misspellings ( such as hizballah , jubeir , saudis , isreal ) as opposed to more frequent and relevant ones ( such as israel , israeli , saudi , arab ) .
Since WENB uses the semantic knowledge in word vectors to infer pseudo labels , the quality of class label word vectors will affect the pseudo label accuracy .
5 Conclusion and Future Directions
We studied the problem of training topic classiﬁers using only class labels .
Experiments on six data sets show that class labels can save a signiﬁcant amount of labeled examples in the beginning .
Retrieval - based and semi - supervised methods tend to perform better on long documents , while the proposed method performs better on short documents .
This study opens up many interesting avenues for future work .
First , we introduce a new perspective on text classiﬁcation : can we build a text classiﬁer by just providing a short description of each class ?
This is a more challenging ( but more user - friendly ) setup than standard supervised classiﬁcation .
Second , future work can investigate tasks such as sentiment and emotion classiﬁcation , which are more challenging than topic classiﬁcation tasks .
Third , the two approaches – leveraging unlabeled data ( retrievalbased and semi - supervised methods ) and leveraging pretrained models ( the proposed method ) – could be combined to give robust performance on both short and long documents .
Finally , we can invite users into the training loop : in addition to labeling documents , users can also revise the class deﬁnitions to improve the classiﬁer .
26101102103 # true labels00.20.40.60.81101102103 # true labels00.20.40.60.81  Acknowledgments
We thank the anonymous reviewers for their helpful comments .
This work was in part supported by the National Library of Medicine under grant number 2R01LM010681 - 05 .
Qiaozhu Mei ’s work was supported in part by the National Science Foundation under grant numbers 1633370 and 1620319 .
Yue Wang would like to thank the support of the Eleanor M. and Frederick G. Kilgour Research Grant Award by the UNC - CH School of Information and Library Science .
