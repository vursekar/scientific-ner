Multilingual Code - Switching for Zero - Shot Cross - Lingual Intent Prediction and Slot Filling
Jitin Krishnan Antonios Anastasopoulos Hemant Purohit Huzefa Rangwala George Mason University Fairfax , VA , USA { jkrishn2,antonis,hpurohit,rangwala}@gmu.edu
Abstract
Predicting user intent and detecting the corresponding slots from text are two key problems in Natural Language Understanding ( NLU ) .
Since annotated datasets are only available for a handful of languages , our work focuses particularly on a zero - shot scenario where the target language is unseen during training .
In the context of zero - shot learning , this task is typically approached using representations from pre - trained multilingual language models such as mBERT or by fine - tuning on data automatically translated into the target language .
We propose a novel method which augments monolingual source data using multilingual code - switching via random translations , to enhance generalizability of large multilingual language models when fine - tuning them for downstream tasks .
Experiments on the MultiATIS++ benchmark show that our method leads to an average improvement of +4.2 % in accuracy for the intent task and +1.8 % in F1 for the slot - filling task over the state - of - the - art across 8 typologically diverse languages .
We also study the impact of code - switching into different families of languages on downstream performance .
Furthermore , we present an application of our method for crisis informatics using a new human - annotated tweet dataset of slot filling in English and Haitian Creole , collected during the Haiti earthquake.1
1
Introduction
A cross - lingual setting is typically described as a scenario in which a model trained for a particular task in one source language ( e.g. English ) should be able to generalize well to a different target language ( e.g. Japanese ) .
While semi - supervised solutions ( Muis et al , 2018 ; FitzGerald , 2020 , inter alia ) assume some target language data or translators are available , a zero - shot solution ( Eriguchi
1Implementation
and https://github.com/jitinkrishnan/ Multilingual - ZeroShot - SlotFilling .
dataset
are
available
at
et al , 2018 ; Srivastava et al , 2018 ; Xu et
al , 2020 ) assumes none is available at training time .
Having models that generalize well even to unseen languages is crucial for tackling real world problems such as extracting relevant information during a new disaster ( Nguyen et al , 2017 ; Krishnan et al , 2020 ) or detecting hate speech ( Pamungkas and Patti , 2019 ; Stappen et al , 2020 ) , where the target language might be of low - resource or unknown .
Intent prediction and slot filling are two NLU tasks , usually solved jointly , which learn to model the intent ( sentence - level ) and slot ( word - level ) labels .
Such models are currently used extensively for goal - oriented dialogue systems , such as Amazon ’s Alexa , Apple ’s Siri , Google Assistant , and Microsoft ’s Cortana .
Finding the ‘ intent ’ behind the user ’s query and identifying relevant ‘ slots ’ in the sentence to engage in a dialogue are essential for effective conversational assistance .
For example , users might want to ‘ play music ’ given the slot labels ‘ year ’ and ‘ artist ’ ( Coucke et al , 2018 ) , or they may want to ‘ book a flight ’ given the ‘ airport ’ and ‘ locations ’ slot labels ( Price , 1990 ) .
A strong correlation between the two tasks has made jointly trained models successful ( Goo et al , 2018 ; Haihong et al , 2019 ; Hardalov et al , 2020 ; Chen et al , 2019 ) .
In a cross - lingual setting , the model should be able to learn this joint task in one language and transfer knowledge to another ( Upadhyay et al , 2018 ; Schuster et al , 2019 ; Xu et
al , 2020 ) .
This is the premise of our work .
Highly effective transformer - based multilingual models such as mBERT ( Devlin et al , 2019 ) and XLM - R ( Conneau et al , 2020a ) have found success across several multilingual tasks in recent years .
In the zero - shot cross - lingual transfer setting with an unknown target language , a typical solution is to use pre - trained transformer models and fine - tune to the downstream task using the monolingual source data ( Xu et al , 2020 ) .
However , Pires et al ( 2019 ) showed that existing transformer - based represenProceedingsofthe1stWorkshoponMultilingualRepresentationLearning , pages211–223November11,2021. © 2021AssociationforComputationalLinguistics211  Figure 1 : t - SNE plot of embeddings across the 12 multi - head attention layers of multilingual BERT .
Parallelly translated sentences of MutiATIS++ dataset are still clustered according to the languages : English ( black ) , Chinese ( cyan ) , French ( blue ) , German ( green ) , and Japanese ( red ) .
Figure 2 : An original example in English from MultiATIS++ dataset and its multilingually code - switched version .
In the above code - switching example , the chunks are in Chinese , Punjabi , Spanish , English , Arabic , and Russian .
‘ atis_airfare ’ represents an intent class where the user seeks price of a ticket .
tations may exhibit systematic deficiencies for certain language pairs .
Figure 1 also verifies that the representations across the 12 multi - head attention layers of mBERT are still not shared across languages , instead forming clearly distinguishable clusters per language .
This leads to a fundamental challenge that we address in this work : enhancing the language neutrality so that the fine - tuned model is generalizable across languages for the downstream task .
To this goal , we introduce a data augmentation method via multilingual codeswitching , where the original sentence in English is code - switched into randomly selected languages .
For example , chunk - level code - switching creates sentences with phrases in multiple languages as shown in Figure 2 .
We show that mBERT can be fine - tuned for many languages starting only with monolingual source - language data , leading to better performance in zero - shot settings .
Further , we show how code - switching with languages from different language families impacts the model ’s performance on individual target languages , even finding some counter - intuitive results .
For instance , training on data code - switched between English and Sino - Tibetan languages is as helpful for Hindi ( an Indo - Aryan Indo - European language ) as code - switching with other Indo - Aryan languages , and Turkic languages can be helpful for both Chinese and Japanese .
a ) We present a data augmentaContributions : tion method via multilingual code - switching to enhance the language neutrality of transformerbased language models such as mBERT for finetuning to a downstream NLU task of intent prediction and slot filling .
b ) By studying different language families , we show how code - switching can be used to aid zero - shot cross - lingual learning for low - resource languages .
c ) We release a new human - annotated tweet dataset , collected during Haiti earthquake disaster , for intent prediction and slot filling in English and Haitian Creole .
2 Methodology
This section describes our problem definition , codeswitching algorithm , language families , and the training methodology .
2.1 Problem Definition
Given a source ( S ) and a set of target ( T ) languages , the goal is to train a classifier using data only in the source language and predict examples from the completely unseen target languages .
We assume the target language is unknown during training ( fine - tuning ) time , which makes direct translation to target infeasible .
In this context , we use code - switching ( cs ) to augment the monolingual source data .
Thus , the input , augmented input , and output of our problem can be defined as :
212  Algorithm 1 : Data Augmentation via Multilingual Code - Switching ( Chunk - Level ) ut , yen , yen Input : X en sl , lT Output : X cs ut , ycs , ycs sl ut ← ∅ , ycs ← ∅ , ycs X cs sl ← ∅ lset = googletrans.languages − lT for i ∈ 1 ..
k do
for j ∈ 1 .. len(X en
ut )
do Gcs ←
∅
, Lcs ← ∅ chunks = slot_chunks(X en for c ∈ chunks do
ut
[
j ] , yen
sl [ j ] )
l ← random.choice(lset ) t ← translate(c , l ) Gcs ← Gcs ∪ t Lcs ← Lcs ∪ align_label(c , t )
ut ← X cs
end X cs ut ∪ Gcs ycs ← ycs ∪ ycs[j ] sl ← ycs ycs
sl ∪ Lcs
end
end
ut , yS , yS Input : X S Code - Switched Input : X cs Output : yT , yT
ut , ycs , ycs sl sl ←
predict(X
T ut )
sl , lT
where Xut represents sentences , y their ground truth intent classes , ysl the slot labels for the words in those sentences , and lT the set of target languages .
An example sentence , its intent class , and slot labels are shown in Figure 2 .
2.2 Multilingual Code - Switching
Multilingual masked language models , such as mBERT ( Devlin et al , 2019 ) , are trained using large datasets of publicly available unlabeled corpora such as Wikipedia .
Such corpora largely remain monolingual at the sentence level because the presence of intra - sentence code - switched data in written texts is likely scarce .
The masked words that needed to be predicted usually are in the same language as their surrounding words .
We study how code - switching can enhance the language neutrality of such language models by augmenting it with artificially code - switched data for fine - tuning it to a downstream task .
Algorithm 1 explains this codeswitching process at the chunk - level .
When using slot filling datasets , slot labels that are grouped by BIO ( Ramshaw and Marcus , 1999 ) tags constitute natural chunks , as shown in Figure 2 .
To summarize the algorithm , we take a sentence , take each chunk from that sentence , perform a translation into a random language using Google ’s NMT system ( Wu et al , 2016 ) , and align the slot labels to fit the translation , i.e. , label propagation through alignment as the translated sentence do not preserve the
Group Name
Afro - Asiatic
Germanic
Indo - Aryan
Romance
Sino - Tibetan , Koreanic , & Japonic Turkic
Languages
Arabic ( ar ) , Amharic ( am ) , Hebrew ( he ) , Somali ( so ) German ( de ) , Dutch ( nl ) , Danish ( da ) , Swedish ( sv ) , Norwegian ( no ) Hindi ( hi ) , Bengali ( bn ) , Marathi ( mr ) , Nepali ( ne ) , Gujarati ( gu ) , Punjabi ( pa ) Spanish ( es ) , Portuguese ( pt ) , French ( fr ) , Italian ( it ) , Romanian ( ro ) Chinese ( zh - cn ) , Japanese ( ja ) , Korean ( ko ) Turkish Uyghur ( ug ) , Kazakh ( kk )
( tr ) , Azerbaijani
( az ) ,
Table 1 : Selected language families to evaluate their impact on a target language .
number and order of words in the original sentence .
At the chunk - level , we use a direct alignment .
The BIO - tagged labels are recreated for the translated phrase based on the word tokens .
More complex methods could be applied here to improve the alignment of the slot labels such as fast - align ( Dyer et al , 2013 ) or soft - align ( Xu et al , 2020 ) , but we leave this for future work .
Code - Switching at the word - level essentially translates every word randomly , while at the sentence - level translates the entire sentence .
During the experimental evaluation process , to build a language - neutral model using monolingual source ( English ) data , all eight target languages are excluded from the code - switching procedure to avoid unfair model comparisons , i.e. removing target languages ( lT ) from lset in Algorithm 1 .
Complexity .
The augmentation process is repeated k times per sentence producing a new augmented dataset of size k × n , where n is the size of the original dataset , i.e. space complexity of O(k × n ) .
For T translations per sentence , Algorithm 1 has a runtime complexity of O(k × n × T ) assuming constant time for alignment .
Word - level requires as many translations as the number of words but sentence - level requires only one .
An increase in the dataset size also increases the training time , but the advantage is one model appropriate for many languages .
2.3 Language Families
A language family is defined as a group of related languages that likely share a common ancestor .
For example , Portuguese , Spanish , French , Italian , and Romanian are all derived from Latin ( Rowe and Levine , 2017 ) .
We use language families to study their impact on the target languages .
We augment the source language with code - switching to
213  a particular language family .
For instance , codeswitching the English dataset with Turkic language family and testing on Japanese can reveal how closely the two are aligned in the vector space of a pre - trained multilingual model .
We work with 6 language groups : Afro - Asiatic ( Voegelin and Voegelin , 1976 ) , Germanic ( Harbert , 2006 ) , Indo - Aryan ( Masica , 1993 ) , Romance ( Elcock and Green , 1960 ) , and Turkic ( Johanson and Johanson , 2015 ) , also grouping Sino - Tibetan , Koreanic and Japonic ( Shafer , 1955 ; Miller , 1967).2 Germanic , Romance , and Indo - Aryan are genera of the Indo - European family .
Language groups and corresponding languages are shown in Table 1 .
Each group is selected based on a target language in the dataset , and the Afro - Asiatic family is added as an extra group .
In experiments , lset in Algorithm 1 will be assigned languages from a specific family .
2.4
Joint Training
Joint training is traditionally used for intent prediction and slot filling to exploit the correlation between the two tasks .
This is done by feeding the feature vectors of one model to another or by sharing layers of a neural network followed by training the tasks together .
So , a standard joint model loss can be defined as a combination of intent ( Li ) and slot ( Lsl ) losses .
i.e. , L = αLi + βLsl , where α and β are corresponding task weights .
Prior works ( Goo et al , 2018 ; Schuster et al , 2019 ; Liu and Lane , 2016 ; Haihong et al , 2019 ) that use BiLSTM or RNN are now modified to BERT - based implementations explored in more recent works ( Chen et al , 2019 ; Hardalov et al , 2020 ;
Xu et
al , 2020 ) .
A standard Joint model consists of BERT outputs from the final hidden state ( classification ( CLS ) token for intent and m word tokens for slots ) fed to linear layers to get intent and slot predictions .
Assuming hcls represents the CLS token and hm represents a token from the remaining word - level tokens , the BERT model outputs are defined as ( Chen et al , 2019 ; Xu et al , 2020 ):
pi = sof tmax(W ihcls + bi ) m = sof tmax(W slhm + bsl ) ∀m psl
( 1 )
with a multi - class cross - entropy loss3 for both intent ( Li ) and slots ( Lsl ) .
We will use this model as 2Each of the Sino - Tibetan , Koreanic , and Japonic families have a single high - resource member ( Chinese , Korean , Japanese respectively ) .
We only group them as an additional interesting data point , not because we ascribe to any theories that link them typologically .
i=1[y log yˆ ]
3L = − 1 n
∑︁n
our baseline for joint training .
Our goal will be to show that code - switching on top of joint training improves the performance .
The output of Algorithm 1 will be the input used for joint training on BERT for code - switched experiments .
3 Datasets
Benchmark Dataset .
We use the latest multilingual benchmark dataset of MultiATIS++ ( Xu et al , 2020 ) , which was created by manually translating the original ATIS ( Price , 1990 ) dataset from English ( en ) to 8 other languages : Spanish ( es ) , Portuguese ( pt ) , German ( de ) , French ( fr ) , Chinese ( zh ) , Japanese ( ja ) , Hindi ( hi ) , and Turkish ( tr ) .
The dataset consists of utterances for each language with an ‘ intent ’ label for ‘ flight intent ’ and ‘ slot ’ labels for the word tokens in BIO format .
A sample datapoint in English is shown in Figure 2 .
Table 2 presents the dataset statistics for the benchmark dataset of MultiATIS++ as well as for the newly constructed dataset for disaster NLU .
New Dataset for Disaster NLU .
We construct a new intent and slot filling dataset of tweets collected during natural disasters , in two languages : English ( en ) and Haitian Creole ( ht ) .
The tweets originally were released by Appen.4 For English , a language expert labeled the tweets , and for Haitian Creole , we used Amazon Mechanical Turk with five annotators .
Intent classes include : ‘ request ’ and ‘ others ’ .
Slot filling consists of 5 labels : ‘ medical_help ’ , ‘ food ’ , ‘ water ’ , ‘ shelter ’ , and ‘ other_aid ’ .
Table 2 provides the dataset statistics .
4 Experimental Setup
We use the traditional cross - lingual task setting where each experiment consists of a source language and a target language .
A model is trained on the source data ( English ) and evaluated on the target data ( 8 other languages ) .
For code - switching experiments , the English dataset is augmented with multilingual code - switching before training .
Our implementation is in PyTorch ( Paszke et al , 2019 ) and we use the pre - trained bert - base - multilingualuncased with BertForSequenceClassification ( Wolf et al , 2020 ) model .
Maximum epochs is set to 25 with an early stopping patience of 5 , batch size of 32 , and Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 5e−5 .
We select the best
4https://appen.com/datasets/ combined - disaster - response - data/
214  Language
English Spanish Portuguese German French Chinese Japanese Hindi Turkish
train
4488 4488 4488 4488 4488 4488 4488 1440 578
test
train
Tokens dev
Utterances dev MultiATIS++ ( Xu et al , 2020 ) 490 490 490 490 490 490 490 160 60 Disaster Tweets ( New Dataset ) 490 50755 55197 55052 51111 55909 88194 133890 16422 6132
5445 5927 5909 5517 5769 9652 14416 1753
686
893 893 893 893 893 893 893 893 715
16369 4242 520
test
9164 10338 10228 9383 10511 16710 25939 9755 7683
2834
Intents Slots
18 18 18 18 18 18 18 17 17
2 2
84 84 84 84 84 84 84 75 71
5 5
English Haitian Creole
3518 Table 2 : Datasets & Statistics .
model on the validation set .
Consistent with the metrics reported for intent prediction and slot filling evaluation in the past , we also accuracy for intent and micro F15 to measure slot performance .
4.1 Baselines & Upper Bound
Since we assume that target language is not known before hand , Translate - Train ( TT ) ( Xu et al , 2020 ) method is not a suitable baseline .
Rather , we set this to be an upper bound , i.e. translating to the target language and fine - tuning the model should intuitively outperform a generic model .
Additionally , we add code - switching to this TT model to assess if augmentation negatively impacts its performance .
The zero - shot baselines for the codeswitching experiments use an English - Only ( Xu et al , 2020 ) model , which is fine - tuned over the pre - trained mBERT separately for each task and an English - only Joint model ( Chen et al , 2019 ) .
5 Results & Discussion
Effect of Multilingual Code - Switching .
Table 3 describes performance evaluation on the MultiATIS++ dataset .
When compared to the state - ofthe - art jointly trained English - only baseline , we see a +4.2 % boost in intent accuracy and +1.8 % boost in slot F1 scores on average by augmenting the dataset via multilingual code - switching without requiring the target language .
From the significance tests , except for Spanish and German , all
5To address class imbalance for slots , we use Micro F1 instead of Macro F1 , which is why our F1 scores are inflated when compared to scores in ( Xu et al , 2020 ) .
other languages were helped by code - switching for intent detection .
For slot filling , improvement on Portuguese and French is insignificant .
This suggests that code - switching primarily helped languages that are morphologically more different from the source language ( English ) .
For example , Hindi and Turkish have the highest intent performance improvement of +16.1 % and +9.8 % respectively .
And for slots , Hindi and Chinese with +6.0 % and +4.3 % respectively .
Japanese showed +4 % improvement for intent and +3.4 % for slots .
The runtime of the models in Table 5 ( Appendix B ) shows that code - switching is expensive , taking up to five hours for five augmentation rounds ( k = 5 ) .
This is because there are k times more data compared to the monolingual source data .
Increasing the number of code - switchings ( k ) for a sentence from 5 to 50 improves the performance by +1 % , while increasing the run - time by a large margin .
Hence , such tradeoffs should be considered when picking k for real - world applications where time to deployment might be of the essense .
In the translate - train ( upper bound ) scenario , it is not immediately clear if augmentation helps , since data in the same language as the target are always preferable to other language or code - switched data .
At a minimum , augmentation does not hinder upper - bound performance ( Table 3 ) .
For both intent and slot performance , the chunklevel model remained robust across the languages .
For intent , the difference between word - level and sentence - level was insignificant .
For slot , sentencelevel was in par with chunk - level on average .
Thus ,
215  Intent Acc .
m
es
de
zh
ja
pt
fr
hi
tr
Slot F1
m
es
de
English - Only Baseline * Jointen−only Baseline * Word - level CS† Sentence - level CS Chunk - level CS ( CCS ) Jointen−only * + CCS
Translate - Train ( TT ) *
JointT T * JointT T
*
+
CCS
English - Only Baseline * Jointen−only Baseline * Word - level CS† Sentence - level CS Chunk - level CS ( CCS ) Jointen−only * + CCS
1 1 1 1 1 1
8 8 8
1 1 1 1 1 1
8 8 8
Upper Bound
94.42 95.03 94.18 94.60 95.12 95.48
94.29 94.51 93.92 93.53 95.27 94.51
94.02 94.16 95.48
93.84 94.24 95.41
96.16 96.12 95.81 96.57 96.68 96.09
96.73 96.76 96.33 96.92 96.82 96.56
79.53 80.54 81.67 81.21 83.88 84.43 ♠
90.21 91.56 91.60
zh
83.12 84.95 85.46 86.32 87.10 88.61 ♠
73.75 73.57 75.48 75.01 74.27 76.48 ♠
84.19 85.98 87.17
ja
78.81 79.60 79.33 79.52 80.00 82.28 ♠
Upper Bound
92.90 93.48 92.54 93.10 94.20 94.15 ♠
93.86 93.33 94.18 93.24 93.48 94.89 ♠
95.66 95.75 95.34
pt
95.63 95.76 96.27 96.65 96.46 96.01
96.35 96.11 95.88
94.54 95.01 94.60
fr
95.40 95.76 95.08 95.84 96.31 95.94
96.02 95.95 95.44
67.06 73.53 81.19 82.37 82.73 85.37 ♠
85.08 86.45 87.94
hi
77.05 77.63 79.10 81.94 80.95 82.28 ♠
69.71 71.05 74.22 75.11 77.51 78.04 ♠
85.79 84.95 85.93
tr
88.09 88.92 86.86 89.84 91.60 90.45 ♠
AVG
83.19 84.38 85.92 86.02 87.06 87.92
90.42 91.01 91.68
AVG
88.87 89.44 89.28 90.45 90.51 91.03
Translate - Train ( TT ) *
JointT T * JointT T
*
+
CCS
96.89 96.92 96.98
96.04 95.66 96.27
93.48 93.64 93.37
85.29 87.84 85.87
82.03 82.98 82.00
91.21 91.15 91.31
92.16 92.53 92.14
Table 3 : Performance evaluation of code - switching with setting k = 5 .
CS : Code - Switching .
Reported scores are average of 5 independent runs ( including a separate code - switched data for each run ) .
m = number of distinct models to be trained .
* : modified BERT - based implementations ( Chen et al , 2019 ; Xu et
al , 2020 ) .
† :
Similar to Qin et
al , 2020 but modified for slot - filling task and also excluding target language from randomized switching .
♠ : The difference is significant with p < 0.05 using Tukey HSD ( conducted between Jointen−only + CCS versus Jointen−only Baseline for each language ) .
Intent Acc .
English - Only Baseline * Translate - Train ( TT ) Chunk - level CS ( CCS ) Jointen−only * + CCS
Slot F1
English - Only Baseline * Translate - Train ( TT ) Chunk - level CS ( CCS ) Jointen−only * + CCS
ht
56.12 62.58 63.15 63.73
ht
68.72 69.96 70.27 70.02
Table 4 : Performance on disaster data in Haitian Creole ( ht ) .
CS = Code - Switching .
Reported scores are average of 5 independent runs ( * : modified BERT - based ) .
we think that code - switching at chunk - level is safer for avoiding semantic discrepancies ( which are a danger in the word - level ) while also better encouraging intra - sentence language neutrality .
Evaluation on Disaster Dataset .
We found that disaster data is more challenging than the ATIS dataset for transfer learning in NLU .
The predictive performance is shown in Table 4 .
Code - Switching improved intent accuracy by +12.5 % and slot F1 by +2.3 % , which is quite promising considering the domain mismatch ( tweets vs airline guides ) .
Joint training added +0.9 % improvement to intent accuracy , however did not seem to help slot F1 .
This might imply a weaker correlation between the two tasks in real - world data , i.e. a mention of ‘ food ’ or ‘ shelter ’ in a tweet may not always mean that there is a ‘ request ’ or vice - versa .
The upper
bound of translate - train method did not perform any better than the randomly code - switched model which seemed counter - intuitive .
This might be due to the lack of strong representation for Haitian Creole in the pre - trained model , although it is similar to French , or due to the limitation of the machine translation system .
Impact of Language Families .
Results of language family analysis are shown in Figure 3 for the 4 languages that showed significant improvements for both intent and slots in Table 3 .
The input in English is independently code - switched using 6 different language families .
Note that the target language is always excluded from the group when evaluating on the same , i.e. Hindi is excluded from Indo - Aryan family when that family is being evaluated on it .
Translate - train model is provided as a frame of reference and upper bound .
Generally , as expected , we found that language families helped their corresponding languages , i.e. Romance helped Spanish , Germanic helped German , and so on .
An exception is our loose group of SinoTibetan , Koreanic , and Japonic languages – for both Chinese and Japanese , languages from the Turkic language family helped more than others .
On the other hand , the Sino - tibetan , Japonic , and Koreanic group helps Hindi more than other Indo - European languages .
We believe this highlights the necessity
216  the augmentation having just one copy of codeswitching ( without the original non - code - switched data ) , as compared to k = 0 .
Adding the original data to one round of code - switched data ( k = 2 ) leads to big improvements .
Overall , we see improvement for both tasks , with Slot F1 plateauing earlier .
Table 5 and Figure 10 in Appendix B show the impact of code - switching on training runtime , which increases as k increases .
Thus , finding an optimal value of k and specific language groups are essential for downstream applications .
mBERT versus XLM - R. Additional performance evaluations and benefits of code - switching on XLM - R ( Conneau et al , 2020a ) , a stronger multilingual language model , are provided in
Appendix A. Note that XLM - R is trained using Common - Crawl and is likely to be exposed to some code - switched data .
Thus , we focus primarily on mBERT which largely remains monolingual at the sentence - level to identify the unbiased impact of code - switching during fine - tuning .
Furthermore , runtime and hyperparameter tuning along with insights into layers to freeze before training are shown in Appendix B.
Error Analysis .
Selecting intent classes with support > 10 , Figure 5 shows how each class is positively or negatively impacted by code - switching .
Improvement was primarily on ‘ airfare ’ , ‘ distance ’ ‘ capacity ’ , ‘ airline ’ , and ‘ ground_service ’ which had longer sentences such as ‘ Please tell me which airline has the most departures from Atlanta ’ when compared to ‘ abbreviations ’ and ‘ airport ’ classes that included very short phrases like ‘ What does EA mean ? ’
However , note that Spanish and German did not improve much , aligning with our results in Table 3 .
For slot labels in Figure 6 , we selected the ones with support > 50 and that have different characteristics , e.g. ‘ name ’ , ‘ code ’ , etc .
The overall trend in slot performance shows improvements for labels such as ‘ day_name ’ , ‘ airport_code ’ , and ‘ city_name ’ and slight variations in labels such as ‘ fight_number ’ and ‘ period_of_day ’ , implying textual slots benefiting over numeric ones .
6 Related Work
Cross - Lingual Transfer .
Researchers have studied cross - lingual tasks in various settings such as sentiment / sequence classification ( Wan , 2009 ; Eriguchi et al , 2018 ; Yu et al , 2018 ) , named entity recognition ( Zirikly and Hagiwara , 2015 ; Tsai
Figure 3 : Impact of different language groups on the target languages .
Figure 4 : Performance as k ( augmentation rounds ) increases ( on mBERT ) .
for methods like the one of Xia et al ( 2020 ) that can a priori identify the best helper language or group of languages that can benefit downstream tasks for low resource languages .
Control Experiments on k. Hyperparameter k controls the amount of code - switched data .
k = 0 represents original size with no code - switching , k = 1 represents original size with code - switching , and k = 10 means 10 - times more code - switched data than the original .
The main experiments in Table 3 use k = 5 .
Figure 4 shows how varying k affects performance .
For this analysis , we consider 4 target languages on which code - switching produced significant results in Table 3 on both Intent Accuracy and Slot F1 : Chinese , Japanese , Hindi , and Turkish .
Intuitively , we observe that as k increases , too much code - switching becomes expensive in terms of runtime , while performance improvement slowly plateaus .
For Slot F1 performance in all four cases , unlike Intent , we observe an interesting dip when k = 1 , which represents
217  ( Conneau et al , 2018 ) .
Monolingual models for joint slot filling and intent prediction have used attention - based RNN ( Liu and Lane , 2016 ) and attention - based BiLSTM with a slot gate ( Goo et al , 2018 ) on benchmark datasets ( Price , 1990 ; Coucke et al , 2018 ) .
These methods have shown that a joint method can enhance both tasks and slot filling can be conditioned on the learned intent .
A related approach iteratively learns the relationship between the two tasks ( Haihong et al , 2019 ) .
Recently , BERT - based approaches ( Hardalov et al , 2020 ; Chen et al , 2019 ) have improved results .
On the other hand , cross - lingual versions of this joint task include a low - supervision based approach for Hindi and Turkish ( Upadhyay et al , 2018 ) , new datasets for Spanish and Thai ( Schuster et al , 2019 ) , and recently Xu et al ( 2020 ) creating MultiATIS++ , a comprehensive dataset in 9 languages .
The joint task mentioned above in a pure zero - shot setting is one of the motivations for our work .
A Zero - shot is the setting where the model sees a new distribution of examples only during test ( prediction ) time ( Xian et al , 2017 ; Srivastava et al , 2018 ; Romera - Paredes and Torr , 2015 ) .
Thus , in our setting , we assume that target language is unknown during training , so that our model is generalizable across multiple languages .
Code - Switching .
Linguistic code - switching is a phenomenon where multilingual speakers alternate between languages .
Recently , monolingual models have been adapted to code - switched text in entity recognition ( Aguilar and Solorio , 2019 ) , part - ofspeech tagging ( Soto and Hirschberg , 2018 ; Ball and Garrette , 2018 ) , sentiment analysis ( Joshi et al , 2016 ) and language identification ( Mave et al , 2018 ; Yirmibe¸so˘glu and Eryi˘git , 2018 ; Mager et al , 2019 ) .
Recently , KhudaBukhsh et al , 2020 have proposed an approach to sample code - mixed documents using minimal supervision .
Qin et
al , 2020 allows randomized code - switching to include the In target language , as shown in their Figure 3 .
our context for example , if the target language is German , we ensure that there is no code - switching to German during training .
We consider this distinction essential to evaluate a true zero - shot learning scenario and prevent any bias when comparing with translate - and - train .
Yang et al ( 2020 ) present a non - zero - shot approach that performs code - switching to target languages , and Jiang et al ( 2020 ) present a code - switching based method to improve the ability of multilingual language modFigure 5 : Impact of code - switching on intent classes .
Figure 6 : Impact of code - switching on slot labels .
et al , 2016 ; Xie et al , 2018 ) , parts - of - speech tagging ( Yarowsky et al , 2001 ; Täckström et al , 2013 ; Plank and Agi´c , 2018 ) , and natural language understanding ( He et al , 2013 ; Upadhyay et al , 2018 ; Xu et
al , 2020 ) .
The methodology for most of the current approaches for cross - lingual tasks can be categorizes as : a ) multilingual representations from pre - trained or fine - tuned models such as mBERT ( Devlin et al , 2019 ) or XLM - R ( Conneau et al , 2020a ) , b ) machine translation followed by alignment ( Shah et al , 2010 ; Yarowsky et al , 2001 ; Ni et al , 2017 ) , or c ) a combination of both ( Xu et al , 2020 ) .
Before transformer models , effective approaches included domain adversarial training to extract language - agnostic features ( Ganin et al , 2016 ; Chen et al , 2018 ) and word alignment methods such as MUSE ( Conneau et al , 2017 ) to align fastText word vectors ( Bojanowski et al , 2017 ) .
Recently , Conneau et al , 2020b show that having shared parameters in the top layers of the multilingual encoders can be used to align different languages quite effectively on tasks such as XNLI
218  els for factual knowledge retrieval .
Contemporary work by Tan and Joty , 2021 makes use of both word and phrase - level code - mixing to switch to a set of languages to perform adversarial training for XNLI .
Code - switching and other data augmentation techniques have been applied to the pre - training stage in recent works ( Chaudhary et al , 2020 ; Kale and Siddhant , 2021 ; Dufter and Schütze , 2020 ) .
However , pre - training is outside the scope of this work .
In addition to studying cross - lingual slot filling and language families , another key distinction of our method is that we completely ignore the target language during training to represent a fully zero - shot scenario .
The main advantage is that with enhanced cross - lingual generalizability , it can be deployed out - of - the - box , as our training is conducted independently of the target language .
7 Conclusion & Future Work
Our study shows that augmenting the monolingual input data with multilingual code - switching via random translations at the chunk - level helps a zeroshot model to be language neutral when evaluated on unseen languages .
This approach enhanced the generalizability of pre - trained language models such as mBERT when fine - tuning for downstream tasks of intent detection and slot filling .
Additionally , we presented an application of this method using a new annotated dataset of disaster tweets .
Further , we studied code - switching with language families and their impact on specific target languages .
Addressing code - switching with language families during the pre - training phase and releasing a larger dataset of annotated disaster tweets in more languages are planned for future work .
8 Ethical Considerations
The tweet dataset that we constructed for disaster NLU was originally released by Appen6 , and we use it to construct slot labels in two languages : English ( en ) and Haitian Creole ( ht ) .
Data statement that includes annotator guidelines for the labeling jobs and other dataset information will be provided with the implementation .
From a broader impact perspective , our code and developed models are open - source and allows NLP technology to be accessible to information systems for emergency services and social scientists in quickly deploying model during disaster events .
