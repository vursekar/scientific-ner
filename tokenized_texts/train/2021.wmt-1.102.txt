IST - Unbabel 2021 Submission for the Quality Estimation Shared Task
Chrysoula Zerva1,2,∗
Daan van Stigt3,∗ Ricardo Rei2,3,4,∗ Ana C. Farinha3 José G. C. de Souza3 Taisiya Glushkova1,2
Miguel Vera3 Pedro G. Ramos3 Fábio Kepler3 André F. T. Martins1,2,3
1Instituto Superior Técnico
2Instituto de Telecomunicações
3Unbabel
4INESC - ID
2,3,4Lisbon , Portugal { chrysoula.zerva , ricardo.rei , taisiya.glushkova , andre.t.martins}@tecnico.ulisboa.pt
{ daan.stigt , catarina.farinha , pedro.ramos , jose.souza , miguel.vera , fabio.kepler}@unbabel.com
Abstract
We present the joint contribution of IST and Unbabel to the WMT 2021 Shared Task on Quality Estimation .
Our team participated on two tasks : Direct Assessment and PostEditing Effort , encompassing a total of 35 submissions .
For all submissions , our efforts focused on training multilingual models on top of OpenKiwi predictor - estimator architecture , using pre - trained multilingual encoders combined with adapters .
We further experiment with and uncertainty - related objectives and features as well as training on out - ofdomain direct assessment data .
1
Introduction
Quality estimation ( QE ) is the task of evaluating a translation system ’s quality without access to reference translations ( Blatz et al , 2004 ; Specia et al , 2018 ) .
This paper describes the joint contribution of Instituto Superior Técnico ( IST ) and Unbabel to the WMT21 Quality Estimation shared task ( Specia et al , 2021 ) , where systems were submitted to two tasks : 1 ) sentence - level direct assessment ; 2 ) word- and sentence - level post - editing effort .
This year ’s submission combines several ideas built on top of the OpenKiwi framework .
Motivated by the mixture of blind and seen language pairs in the test sets , we experimented with extensions that would allow us to train multilingual models that maintain good generalization ability and are robust to the presence of epistemic and aleatoric uncertainty .
For both tasks we trained and submitted an ensemble of multilingual models .
All submitted models follow the predictor - estimator architecture ( Kim and Lee , 2016 ; Kim et al , 2017 ) and use pretrained models for feature extraction .
Also , we ﬁne - tune all models on the provided QE data using stacked adapter layers ( Pfeiffer et al , 2020 ) .
∗ The ﬁrst three authors have equal contribution .
We show that we can thus achieve comparable performance across language pairs while minimising the number of trainable parameters ( see Table 1 ) .
Furthermore , we experimented with different types of uncertainty - related information to leverage it ’s beneﬁts , improving performance and robustness of the submitted systems ( see § 3.1.1 ) .
All related code extensions will be publicly available .
Our main contributions are :
• We build on our OpenKiwi architecture by exploring adapter layers ( Houlsby et al , 2019 ; Pfeiffer et al , 2020 ) for quality estimation as these demonstrated to be less amenable to overﬁtting while presenting the same or superior quality performance than ﬁne - tuning the whole base pre - trained model for different NLP tasks ( He et al , 2021 ) .
• We incorporate different types of uncertainty into our architectures .
We make use of the glass - box features ( Fomicheva et al , 2020 ) extracted from the NMT models , the aleatoric ( data ) uncertainty derived from the human annotations and the epistemic ( model ) uncertainty ( Hora , 1996 ; Kiureghian and Ditlevsen , 2009 ; Huellermeier and Waegeman , 2021 ) that originates from the QE model .
• We show that training the QE models on additional out - of - domain direct assessment ( DA ) data gives considerable gains in performance for the new language pairs from the blind test sets .
2 Quality Estimation Tasks
In this year ’s shared task edition we submitted models for the ﬁrst two tasks :
1 . Task 1 : sentence - level direct assessment
2 . Task 2 : word- and sentence level post - editing effort , comprising of two subtasks : a ) predicting the HTER score of the translated sentence
ProceedingsoftheSixthConferenceonMachineTranslation(WMT),pages961–972November10–11,2021. © 2021AssociationforComputationalLinguistics961  ( hypothesis ) ; and b ) predicting OK / BAD tags for the words and gaps ( both in source and translation )
We note that this year , both tasks 1 and 2 provided additional blind test sets with language pairs that were not included in the data made available for training / development , providing an interesting challenge and motivating multilingual and generalisable approaches .
3
Implemented Systems
3.1 Task 1
For Task 1 our ﬁnal submission consisted of an ensemble of two different multilingual models , that differ in the way they process the input source ( original sentence ) and hypothesis ( machine translation ) .
Both models are based on the predictor - estimator architecture , using different pre - trained models to extract features and different training approaches to optimise for the QE task .
The key idea explored with our ﬁrst model ( denoted by M1 variations in the experiments ) , revolved around pursuing highly generalisable multilingual models , robust to overﬁtting .
To this end , we train a cross - lingual transformer ( XLMRoBERTa ( Conneau et al , 2020 ) ) on large , multilingual data with direct assessments and then use adapters ( Houlsby et al , 2019 ; Pfeiffer et al , 2020 ) to adapt to the domain speciﬁc data of the QE In line with task with minimal training effort .
our efforts for good generalisation , we use only task - speciﬁc adapters and refrain from using speciﬁc adapters for each language pair .
For these experiments we build on the OpenKiwi architecture ( Kepler et al , 2019 ) , using a pre - trained xlm - roberta - large encoder as a feature predictor .
The source and hypothesis sentences are jointly encoded with hypothesis ﬁrst .
Then , source and hypothesis features are generated using average pooling over the hypothesis embeddings and forwarded to the estimator module which corresponds to a feed - forward layer .
Figure 1 provides the general architecture1
The model was ﬁrst trained on the direct assessment data provided in the Metrics shared tasks ( Mathur et al , 2020 ) , as described in § 3.1.2 .
Upon training , the XML - R encoder is frozen and the the model is ﬁne - tuned on sentence regression with
1Note that glass - box features are integrated but not used in this submission as they did not signiﬁcantly improve performance .
Figure 1 : General architecture of M1 model variations .
Word tag prediction is used only for Task 2 .
the task - speciﬁc data , using stacked adapters .
We hence manage to maintain a low number of trainable parameters during ﬁne - tuning and minimize training time while learning to predict task - speciﬁc sentence scores .
For the second model ( denoted by M2 - KL - GMCD ) we aimed to explore the potential of a large pre - trained multilingual model ( trained with MT objectives ) .
We use the mBART ( Liu et al , 2020 ) encoder - decoder architecture to encode the source and force - decode the hypothesis .
We speciﬁcally use the mBART50 model ( Tang et al , 2020 ) which is trained with multilingual ﬁnetuning on 50 languages , including all languages of interest for the QE 2021 task .
We obtain the features by averaging the decoder embeddings and concatenating with the < eos > token of the sequence .
The estimator part of the model consists of a bottleneck feed - forward layer that reduces the dimensionality of the decoder output , and is concatenated with a vector with additional glass - box features from the NMT models ( see § 3.1.1 ) .
The combined vector is then forwarded to a feed - forward estimator and the full model is ﬁne - tuned on the task speciﬁc QE data .
Apart from the glass - box features we experimented further with methods that allow the model to be
962  • Softmax - Ent sentence average of softmax
output distribution entropy
• Sent - Std sentence standard deviation of
word probabilities
• D - TP average TP across N(N = 30 ) stochastic
forward - passes
forward - passes
• D - Var variance of TP across N stochastic
• D - Combo combination of D - TP and D - Var
deﬁned by 1 − D − T P / D − V ar
• D - Lex - Sim lexical similarity - measured by METEOR score ( Lavie and Denkowski , 2009 ) - of MT output generated in different stochastic passes .
Aleatoric uncertainty The noise and complexity of the training data is a source of predictive uncertainty in itself , referred to as data or aleatoric uncertainty ( Kiureghian and Ditlevsen , 2009 ) .
This uncertainty is often reﬂected in the disagreement between human annotations for the same sourcehypothesis segment ( Cohn and Specia , 2013 ; Fornaciari et al , 2021 ) .
We hypothesize that the direct assessments can be better modelled as normally distributed scores rather than a single score , and that a model trained to predict this distribution ( mean and standard deviation ) could provide better quality estimates 2 .
We formalise this as a KL divergence objective , using the closed form solution to estimate the KL divergence between the target distribution p(x ) = N ( µ1 , σ1 ) and the predicted distribution q(x ) =
N ( µ2 , σ2 ) , as shown in Eq .
1 .
KL(p||q )
= log
+
σ2 σ1
1 + ( µ1 − µ2)2 σ2 2σ2 2
−
( 1 )
1 2
where we take the mean and standard deviation ( std ) of the direct assessment z_scores as the target ( ground truth proxy ) values p.
This way , we account for the annotator disagreement ( reﬂected in the std value ) during learning .
QE epistemic uncertainty We use MC dropout ( Gal and Ghahramani , 2016 ) to account for the uncertainty of the QE model .
Speciﬁcally , we enable dropout during inference and run multiple forward runs over each test instance .
Thus we obtain a distribution of quality predictions for each instance
2Note that for this task ’s data we only had access to 3 scores per segment
so the mean and std values are calculated over these numbers .
Figure 2 : General architecture of M2 model variations .
more robust towards the underlying uncertainty of its predictions .
We elaborate that in the next section .
Figure 2 provides a general architecture of the M2 model variations .
3.1.1 Learning from uncertainty
Multiple neural models are involved in the process of obtaining and scoring machine translations , which naturally leads to several sources of uncertainty .
These sources can be very informative and useful for MT evaluation .
In this work we try to consider three types of uncertainty : ( 1 ) uncertainty of the NMT models used to obtain the hypotheses , ( 2 ) data ( aleatoric ) uncertainty for which we use the inter - annotator disagreement as a proxy , and ( 3 ) uncertainty of the MT evaluation model itself .
NMT model uncertainty The idea of extracting uncertainty - related features from the MT systems in order to estimate the quality of their predictions , was originally introduced by Fomicheva et al ( 2020 ) .
This glass - box approach to QE is mostly focusing on capturing epistemic uncertainty , and the proposed features are extracted either using Monte Carlo ( MC ) dropout on the NMT or using the output probability distributions obtained from a standard deterministic MT system .
In our last year ’s submission ( Moura et al , 2020 ) the integration of such features proved to be effective , thus we decided to incorporate it into our new model as well .
We list the extracted features below :
• TP sentence average of word translation probability
963  instead of a single point estimate .
We use the estimated mean of the distribution as our predicted quality estimate .
MC dropout has been shown to improve predictive accuracy and perform on par or even better compared to deep ensembles for MT evaluation tasks ( Glushkova et al , 2021 ) .
It thus allows us to simulate ensembling in a cheap and effective way , without the need to train multiple checkpoints .
3.1.2 Out - of - domain direct assessment data
The QE data is relatively limited , making it harder to train multilingual models with a large number of parameters without over-ﬁtting .
Thus , as explained in § 3.1 we aimed to investigate whether we could obtain models that generalise better and are more robust to noise and out - of - distribution data by training the XLM - RoBERTa model ﬁrst on a larger – yet noisier and out - of - domain dataset .
To that end we leverage the data provided for the past Metrics shared tasks , which covers the language pairs used in this year ’s QE task , including the blind tests for which we had no in - domain data available .
Altogether , it encompasses 30 language pairs from the news domain ( versus 7 in the QE dataset ) .
We provide more detailed statistics for each language pair of the Metrics data in Appendix C.
We refer to experiments using the model initially trained on the Metrics data as M1M- .
We also show that using the trained XLM - RoBERTa encoder from the M1 M model can prove beneﬁcial for the predictions on post - edited data of Task 2 ( see Table 3 ) .
3.2 Task 2
For Task 2 we submitted an ensemble of two variations of the ﬁrst model ( M1 - ADAPT and M1MIn ADAPT ) presented for Task 1 ( see § 3.1 ) .
both cases , we use multi - task training and a feedforward for each output types : hypothesis word tags , hypothesis gap tags , source word tags , and sentence regression ( on HTER scores ) .
Both variations use a pre - trained XLM - RoBERTa ( large ) encoder to extract features as described for Task 1 , but differ in the training of the encoder .
In the ﬁrst case we use the pre - trained model 3 and ﬁnetune on the QE data using stacked adapters .
In the second variation we swap the original pre - trained model with the XLM - RoBERTa model that has been trained on the Metrics data as described in
3https://huggingface.co/transformers/
model_doc / xlmroberta.html
§ 3.1.2 .
We note that the two variations favor different language pairs , hence we combine multiple checkpoints from each variation ( ranging training steps ) .
We use the test-20 split of the data to optimise the hyper - parameters and following this approach we use the estimated top-3 checkpoints from each variation using the combined dataset 4 and the top checkpoint for the non - augmented model trained exclusively on the train set , resulting in total 7 checkpoints in our ﬁnal ensemble .
4 Experimental Results
We present the performance of the implemented models on the test-20 dataset .
4.1 Task 1
The results can be seen in Tables 1 and 2 .
In line with the shared task guidelines we treat Pearson r as the primary performance metric and select the submitted models accordingly .
We can observe , that while on average the M1 model and its variations outperform the M2 model , their performance is comparable , and M2 - KL - G - MCD can even outperform M1M - ADAPT for speciﬁc language pairs , hence it made sense to combine them in the ﬁnal ensemble .
We can also see that ﬁne - tuning the M1 model on the Metrics data , results in performance gains for the majority of the language pairs .
Specifically , even applying the M1 M directly , without further ﬁne - tuning on QE data , achieves competitive performance for most pairs , which further improves upon ﬁne - tuning .
It helps in increasing the performance on the blind sets ( denoted as zeroshot in the Appendix B ) .
The performance gains concern mostly the correlation performance indicators ( Pearson and Spearman correlations ) , since especially for M1 the error - based indicators ( MAE and RMSE ) seem to favor the versions of the model that have not seen the Metrics data .
One possible explanation for this discrepancy could lie in the differences between the range and distribution of DA scores for the two datasets .
Indicatively , the range of scores on the train - dev - test-20 concatenation of the QE data is [ −7.542 , 3.178 ] and for the Metrics data [ −8.624 , 4.332 ] .
The target DA scores in both datasets are calculated via standardizing ( taking the z score ) the direct assessments for each annotator and then averaging all standardized
4The combined dataset in this case refers to the concatenation of the train / dev / test20 annotated data splits provided for the shared task
964  Pears↑
Spear↑ MAE↓ RMSE↓
Pears↑
Spear↑ MAE↓ RMSE↓
E D
N E
H Z N E
N E T E
N E E N
N E O R
N E U R
N E
I
S
L M
M1 BASE M1 - ADAPT M1 M M1M - ADAPT
M1 BASE M1 - ADAPT M1 M M1M - ADAPT
M1 BASE M1 - ADAPT M1 M M1M - ADAPT
M1 BASE M1 - ADAPT M1 M M1M - ADAPT
M1 BASE M1 - ADAPT M1 M M1M - ADAPT
M1 BASE M1 - ADAPT M1 M M1M - ADAPT
M1 BASE M1 - ADAPT M1 M M1M - ADAPT
M1 BASE M1 - ADAPT M1 M M1M - ADAPT
0.4534 0.5092 0.5288 0.5695
0.4429 0.4723 0.4447 0.4815
0.7939 0.7948 0.7580 0.7956
0.7805 0.7609 0.7477 0.7888
0.8718 0.8923 0.8345 0.8889
0.7587 0.7736 0.6703 0.7425
0.6456 0.6613 0.6308 0.6649
0.6781 0.6949 0.6593 0.7045
0.4532 0.4825 0.4872 0.5131
0.4362 0.4755 0.4400 0.4872
0.8076 0.8061 0.7611 0.8110
0.7592 0.7475 0.7324 0.7556
0.8360 0.8533 0.8132 0.8488
0.6919 0.7142 0.6535 0.7159
0.6112 0.6172 0.6535 0.6225
0.6565 0.6709 0.5131 0.6791
0.4482 0.4868 0.4485 0.4127
0.5364 0.5228 0.4772 0.5502
0.5388 0.4518 0.5820 0.5358
0.4278 0.4075 0.4499 0.4192
0.3598 0.3068 0.4585 0.3142
0.4885 0.4138 0.5606 0.4989
0.5060 0.4742 0.4742 0.4863
0.4722 0.4377 0.4127 0.4596
0.6371 0.6288 0.6327 0.6095
0.6867 0.6714 0.6110 0.7017
0.6928 0.5810 0.7134 0.6921
0.5461 0.5393 0.6161 0.5332
0.4878 0.4201 0.5863 0.4437
0.6949 0.6082 0.7583 0.7250
0.6481 0.5939 0.5786 0.6064
0.6276 0.5775 0.6095 0.6160
Table 1 : Results for Task 1 with the M1 predictorestimator ( XLM - RoBERTa ) and different training/ﬁnetuning approaches .
M1 M is the M1 model trained on the Metrics dataset and M#-ADAPT signiﬁes a model ﬁne - tuned on the QE data with adapters .
ML stands for MULTILINGUAL , showing the performance averaged over all language pairs .
Underlined numbers indicate the best result for each language pair and evaluation metric .
Bold systems were selected for the ﬁnal ensemble .
assessments for each segment .
Thus , the difference in target score range and distribution could affect the magnitude of predicted scores and the distance to the ground truth values , which is reﬂected in the MAE and RMSE metrics .
These ﬁndings , further supported by the results on Task 2 , is a ﬁrst step in exploring the underlying connection and bridging the gap between the Metrics and Quality Estimation shared tasks .
4.2 Task 2
The results can be seen in Table 3 .
Similarly to Task 1 , the primary evaluation metric for the sentence level sub - task of Task 2 is the Pearson r coefﬁcient ,
E D
N E
H Z N E
N E T E
N E E N
N E O R
N E U R
N E
I
S
L M
M2 BASE M2 - KL M2 - KL - G
0.4889 0.4971 0.5110 M2 - KL - G - MCD 0.5093
M2 BASE M2 - KL M2 - KL - G
0.4484 0.4574 0.4566 M2 - KL - G - MCD 0.4628
M2 BASE M2 - KL M2 - KL - G
0.7792 0.7833 0.7847 M2 - KL - G - MCD 0.7868
M2 BASE M2 - KL M2 - KL - G
0.7333 0.7638 0.7529 M2 - KL - G - MCD 0.7596
M2 BASE M2 - KL M2 - KL - G
0.8780 0.8826 0.8728 M2 - KL - G - MCD 0.8777
M2 BASE M2 - KL M2 - KL - G
0.7406 0.7532 0.7485 M2 - KL - G - MCD 0.7509
M2 BASE M2 - KL M2 - KL - G
0.6243 0.6373 0.6506 M2 - KL - G - MCD 0.6545
M2 BASE M2 - KL M2 - KL - G
0.6704 0.6821 0.6825 M2 - KL - G - MCD 0.6859
0.4645 0.4769 0.4738 0.4754
0.4355 0.4471 0.4543 0.4584
0.7842 0.7896 0.7962 0.7951
0.7154 0.7393 0.7228 0.7269
0.8407 0.8406 0.8397 0.8429
0.6874 0.7123 0.7191 0.7204
0.5899 0.6000 0.6168 0.6199
0.6454 0.6580 0.6604 0.6627
0.4608 0.4549 0.4396 0.4495
0.4940 0.5042 0.5278 0.4973
0.4581 0.4684 0.4643 0.4539
0.4347 0.4040 0.4194 0.4125
0.3403 0.3199 0.3314 0.3209
0.4696 0.4558 0.4630 0.4492
0.4709 0.4572 0.4586 0.4495
0.4469 0.4378 0.4434 0.4333
0.6180 0.6191 0.6133 0.6128
0.6374 0.6485 0.6751 0.6390
0.5624 0.5824 0.5924 0.5674
0.5531 0.5247 0.5353 0.5313
0.4514 0.4305 0.4635 0.4426
0.6381 0.6299 0.6612 0.6358
0.5939 0.5726 0.5796 0.5697
0.5792 0.5725 0.5886 0.5712
Table 2 : Results for Task 1 with the M2 predictorestimator ( mBART ) and different uncertainty handling additions .
“ KL ” signiﬁes the incorporation of KL loss , “ G”the incorporation of glass - box features and MCD the addition of MC dropout .
ML stands for MULTILINGUAL , showing the performance averaged over all language pairs .
Underlined numbers indicate the best result for each language pair and evaluation metric .
Bold systems were selected for the ﬁnal ensemble .
while the word level sub - task is evaluated using the Matthews correlation coefﬁcient ( MCC , ( Matthews , 1975 ) ) as the primary performance indicator .
We can see that while HTER scores do not always correlate highly with DAs ( see Table 4 ) , the use of the M1 M model encoder that was trained on large data with direct assessments can still prove useful .
Indeed , when ﬁne - tuning on the Task2 data , the model using the M1 M encoder ( M1MADAPT in the table 3 ) provides a performance boost for the Pearson correlation in most language pairs , and competitive performance for the rest .
Based on these results , we deem it worthwhile to include checkpoints trained with this conﬁguration in the ensemble estimating that they will contribute in higher performance , especially on the blind test sets .
This can be further conﬁrmed when
965  Pearson↑
SRC - MCC↑ TGT - MCC↑
E D
N E
H Z N E
N E T E
N E E N
N E O R
N E U R
N E
I
S
L M
M1 BASE M1 - ADAPT M1M - ADAPT
M1 BASE M1 - ADAPT M1M - ADAPT
M1 BASE M1 - ADAPT M1M - ADAPT
M1 BASE M1 - ADAPT M1M - ADAPT
M1 BASE M1 - ADAPT M1M - ADAPT
M1 BASE M1 - ADAPT M1M - ADAPT
M1 BASE M1 - ADAPT M1M - ADAPT
M1 BASE M1 - ADAPT M1 M ADAPT
0.5256 0.5573 0.5499
0.3786 0.3711 0.3721
0.7319 0.7360 0.7498
0.5898 0.5987 0.6252
0.8531 0.8282 0.8280
0.4899 0.4811 0.5060
0.6659 0.6698 0.6935
0.6050 0.6061 0.6178
0.3331 0.4211 0.3647
0.3253 0.4346 0.4255
0.4537 0.5545 0.4929
0.5198 0.6884 0.4244
0.5727 0.5984 0.5682
0.2766 0.341 0.2927
0.4653 0.6776 0.3872
0.4209 0.5323 0.4222
0.4092 0.36454 0.4239
0.3589 0.3288 0.3643
0.5110 0.4978 0.5513
0.4386 0.5426 0.4682
0.6190 0.5653 0.5813
0.3213 0.3071 0.3421
0.4776 0.5057 0.4937
0.4479 0.4445 0.4607
Table 3 : Results for Task 2 with the M1 predictorestimator ( XLM - RoBERTa ) and different training/ﬁnetuning approaches .
M1 M is the M1 model trained on the Metrics dataset and M#-ADAPT signiﬁes a model ﬁne - tuned on the QE data with adapters .
ML stands for MULTILINGUAL , showing the performance averaged over all language pairs .
Underlined numbers indicate the best result for each language pair and evaluation metric .
Bold systems were selected for the ﬁnal ensemble .
inspecting the results for the blind sets ( en - cs , en - ja , km - en and ps - en ) in the ofﬁcial results on test-21 as shown in Appendix B.
lp
TRAIN
DEV
TEST-20
EN - DE -0.1654 EN - ZH -0.2947 ET - EN -0.5464
NE - EN -0.4527 RO - EN -0.5887 RU - EN -0.5358 -0.3916
SI - EN
-0.4032
-0.1895 -0.5850
-0.5004 -0.7932 -0.5055 -0.4384
-0.3850 -0.1932
-0.5995 -0.4558
-0.7880
-0.5152 -0.4125
Table 4 : Pearson correlation between the z_mean of the direct assessments for the QE Task 1 data and the HTER score for the post edits in QE Task 2 data .
5 Conclusions
We presented a joint contribution of IST and Unbabel to the WMT 2021 QE shared task .
Our
submissions are ensembles of multilingual checkpoints extending the OpenKiwi framework .
We found adapter - tuning to be suitable for ﬁne - tuning OpenKiwi on the QE tasks data and less prone to overﬁtting .
We showed that pre - training on large , out - of - domain annotated data can prove beneﬁcial both for the direct assessment and the postediting QE tasks .
We also demonstrated that handling uncertainty - related sources of information improves the performance when integrated into the QE system .
For Task 2 we do multi - task training based on the models from the previous task and use multiple checkpoints to create the submitted ensemble .
