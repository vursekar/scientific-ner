Multi - Task Neural Model for Agglutinative Language Translation     Yirong Pan1,2,3 , Xiao Li1,2,3 , Yating Yang1,2,3 , and Rui Dong1,2,3   1 Xinjiang Technical Institute of Physics & Chemistry , Chinese Academy of Sciences , China   2 University of Chinese Academy of Sciences , China   3 Xinjiang Laboratory of Minority Speech and Language Information Processing , China   panyirong15@mails.ucas.ac.cn   { xiaoli , yangyt , dongrui}@ms.xjb.ac.cn
Abstract
improve
Neural   machine   translation   ( NMT )   has   achieved impressive performance recently   by   using   large - scale   parallel   corpora .  
However ,   it   struggles   in   the   low - resource   and   morphologically - rich   scenarios   of   agglutinative   language   translation   task .  
Inspired   by   the   finding   that   monolingual   the   NMT   data   can   greatly   performance ,   we   propose   a   multi - task   neural model that jointly learns to perform   bi - directional translation and agglutinative   language stemming .
Our approach employs   the shared encoder and decoder to train a   single model without changing the standard   NMT   architecture   but   instead   adding   a   token before   each   source - side sentence to   specify the desired target outputs of the two   different   tasks .  
Experimental   results   on   Turkish - English   and   Uyghur - Chinese   show   that   our   proposed   approach   can   significantly   translation   performance on agglutinative languages by   using a small amount of monolingual data .
improve
the
1
Introduction
Neural   machine   translation   ( NMT )   has   achieved   impressive   performance   on   many   high - resource   machine translation tasks ( Bahdanau et al , 2015 ;   Luong   et   al . ,   2015a ;   Vaswani   et   al . ,   2017 ) .  
The   standard NMT model uses the encoder to map the   source   sentence   to   a   continuous   representation   vector , and then it feeds the resulting vector to the   decoder to produce the target sentence .
However , the NMT model still suffers from the   low - resource and   morphologically - rich scenarios   of agglutinative language translation tasks , such as   Turkish - English   and   Uyghur - Chinese .  
Both   Turkish   and   Uyghur   are   agglutinative   languages   with   complex   morphology .  
The   morpheme   structure of the word can be denoted as : prefix1  
+ … + prefixN + stem + suffix1 +
… +
suffixN
( Ablimit   et   al . ,   2010 ) .  
Since   the   suffixes   have   many   inflected   and   morphological   variants ,   the   vocabulary   size   of   an   agglutinative   language   is   considerable   even   in   small - scale   training   data .  
Moreover , many words have different morphemes   and meanings in different context , which leads to   inaccurate translation results .
the
Recently , researchers show   their great interest   in   utilizing   monolingual   data   to   further   improve   the NMT model performance ( Cheng et al , 2016 ;  
Ramachandran et
al , 2017 ; Currey et al , 2017 ) .  
Sennrich   et   al .   ( 2016 )   pair   target - side   monolingual data with automatic back - translation   as additional training data to train the NMT model .  
Zhang   and   Zong   ( 2016 )   use   the   source - side   monolingual   data   and   employ   the   multi - task   learning   framework   for   translation   and   source   sentence reordering .  
Domhan   and   Hieber   ( 2017 )   modify the decoder to enable multi - task learning   for translation and language modeling .
However ,   the   above   works   mainly   focus   on   boosting   the   translation fluency , and lack the consideration of   morphological and linguistic knowledge .
Stemming is a morphological analysis method ,   which is widely used for information retrieval tasks   ( Kishida ,   2005 ) .  
By   removing   the   suffixes   in the   word ,   stemming   allows   the   variants   of   the   same   word   to   share   representations   and   reduces   data   sparseness .
We consider that stemming can lead to   better   generalization   on   agglutinative   languages ,   which helps NMT to capture the in - depth semantic   information .
Thus we use stemming as an auxiliary   task for agglutinative language translation .
In this paper , we investigate a method to exploit   the monolingual data of the agglutinative language   to enhance the representation ability of the encoder .  
This   is   achieved   by   training   a   multi - task   neural   model to jointly perform bi - directional translation   and   agglutinative   language   stemming ,   which   utilizes the shared encoder and decoder .
We treat   stemming as a sequence generation task .
1
Proceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics : StudentResearchWorkshop , pages103–110July5 - July10,2020.c(cid:13)2020AssociationforComputationalLinguistics103
2
The   method   requires   no   changes   to   the   standard   NMT   architecture   but   instead   requires   adding   a   token at the beginning of each source sentence to   specify   the   desired   target   sentence .  
Inspired   by   their   work ,   we   employ   the   standard   NMT   model   with   one   encoder   and   one decoder   for   parameter   sharing and model generalization .
In addition , we   build a joint vocabulary on the concatenation of the   source - side and target - side words .
Several   works   on   morphologically - rich   NMT   have   focused   on   using   morphological   analysis to   pre - process the training data ( Luong et al , 2016 ;  
Huck et al , 2017 ; Tawfik et al , 2019 ) .
Gulcehre et   al .   ( 2015 )   segment   each   Turkish   sentence   into   a   sequence of morpheme units and remove any nonsurface morphemes for Turkish - English translation .  
Ataman   et   al .   ( 2017 )   propose   a   vocabulary   reduction method that considers the morphological   properties of the agglutinative language , which is   based   on   the   unsupervised   morphology   learning .  
This   work   takes   inspiration   from   our   previously   proposed segmentation method ( Pan et al , 2020 )   that   segments   the   word   into   a   sequence   of   subword   units   with   morpheme   structure ,   which   can   effectively reduce language complexity .
3   Multi - Task Neural Model
3.1   Overview
We propose a multi - task neural model for machine   translation   from   and   into   a   low - resource   and   morphologically - rich   agglutinative   language .  
We   train the model to jointly learn to perform both the   bi - directional   translation   task   and   the   stemming   task   on   an   agglutinative   language   by   using   the   standard   NMT   framework .  
Moreover ,   we   add   an   artificial   token   before   each   source   sentence   to   specify the desired target outputs for different tasks .  
The architecture of the proposed model is shown in   Figure 1 .
We take the Turkish - English translation   task as example .
The “ < MT > ” token denotes the   bilingual   translation   task   and   the   “ < ST > ”   token   denotes the stemming task on Turkish sentence .
3.2   Neural Machine Translation ( NMT )
Our proposed multi - task neural model on using the   source - side   monolingual   data   for   agglutinative   language   translation   task   can   be   applied   in   any   NMT structures with encoder - decoder framework .  
In this work , we follow the NMT model proposed   by Vaswani et al ( 2017 ) , which is implemented as   Transformer .
We will briefly summarize it here .
Figure 1 : The architecture of the multi - task neural model   that   jointly   learns   to   perform   bi - directional   translation   between Turkish and English , and stemming for Turkish   sentence .
2   Related Work
Multi - task   learning   ( MTL )   aims   to   improve   the   generalization performance of a main task by using   the other related tasks , which has been successfully   applied   to   various   research   fields   ranging   from   language   ( Liu   et   al . ,   2015 ;   Luong   et   al . ,   2015a ) ,   vision ( Yim et al , 2015 ;
Misra et al , 2016 ) , and   speech ( Chen and Mak , 2015 ; Kim et al , 2016 ) .  
Many   natural   language   processing   ( NLP )   tasks   have been chosen as auxiliary task to deal with the   increasingly complex tasks .
Luong et
al ( 2015b )   employ a small amount of data of syntactic parsing   and image caption for English - German translation .  
Hashimoto et al ( 2017 ) present a joint MTL model   to handle the tasks of part - of - speech ( POS ) tagging ,   dependency   parsing ,   semantic   relatedness ,   and   textual   entailment   for   English .  
Kiperwasser   and   Ballesteros   ( 2018 )   utilize   the   POS   tagging   and   dependency parsing for English - German machine   translation .
To the best of our knowledge , we are   the   first   to   incorporate   stemming   task   into   MTL   framework   to   further   improve   the   translation   performance on agglutinative languages .
Recently ,   several   works   have   combined   the   MTL   method   with   sequence - to - sequence   NMT   model   for   machine   translation   tasks .  
Dong   et   al .   ( 2015 ) follow a one - to - many setting that utilizes a   shared   encoder   for   all   the   source   languages   with   respective   attention   mechanisms   and   multiple   decoders for the different target languages .
Luong   et al ( 2015b ) follow a many - to - many setting that   uses   multiple   encoders   and   decoders   with   two   separate   unsupervised   objective   functions .  
Zoph   and   Knight   ( 2016 )   follow   a   many - to - one   setting   that employs multiple encoders for all the source   languages   and   one   decoder   for   the   desired   target   language .  
Johnson   et   al .   ( 2017 )   propose   a   more   simple method in one - to - one setting , which trains   a single NMT model with the shared encoder and   decoder in order to enable multilingual translation .
104Encoder - Decoder FrameworkBilingual DataMonolingual DataTraining data < MT > + English sentence < MT > +
Turkish sentence < ST > +
Turkish sentenceTurkish sentenceEnglish sentencestem sequence  # Src
# Trg
355,251   6,356,767   8,021,161
Task   Data   # Sent   train   Tr - En   valid   2,455   4,962   test   333,097   6,026,953   5,748,298   train   valid   700   test
17,821   20,580
17,085   18,179
37,153   69,006
52,125   96,291
UyCh
1,000
Table 1 :   The statistics of the training , validation , and   test datasets on Turkish - English and Uyghur - Chinese   machine   translation   tasks .  
The   “ #   Src ”   denotes   the   number of the source tokens , and the “ # Trg ” denotes   the numbers of the target tokens .
Figure 2 : The example of morphological segmentation   method for the word in Turkish .
Firstly , the Transformer model maps the source   sequence 𝒙 = ( 𝑥1 , … , 𝑥𝑚 ) and the target sentence   𝒚 = ( 𝑦1 , … , 𝑦𝑛 )   into   a   word   embedding   matrix ,   respectively .
Secondly , in order to make use of the   word   order   in   the   sequence ,   the   above   word   embedding   matrices   sum   with   their   positional   encoding matrices to generate the source - side and   target - side   positional   embedding   matrices .  
The   encoder   is   composed   of   a   stack   of   N   identical   layers .
Each layer has two sub - layers consisting of   the   multi - head   self - attention   and   fully   connected feed - forward network , which maps the   source - side   positional   embedding   matrix   into   a   representation vector .
the
The   decoder is   also   composed   of   a   stack   of   N   identical   layers .  
Each   layer   has   three   sub - layers :   the   multi - head   self - attention ,   the   multi - head   attention ,   and   the   fully   connected   feed - forward   network .  
The   multi - head   attention   attends   to   the   outputs of the encoder and decoder to generate a   context vector .
The feed - forward network followed   by   a   linear   layer   maps   the   context   vector   into   a   vector with the original space dimension .
Finally ,   the   softmax   function   is   applied   on   the   vector   to   predict the target word sequence .
1 https://wit3.fbk.eu/archive/2018-01/additional_TED_xml/   2 http://data.statmt.org/wmt18/translation-task/   3 http://uy.ts.cn/
3
4   Experiment
4.1   Dataset
The   statistics   of   the   training ,   validation ,   and   test   datasets   on Turkish - English   and   Uyghur - Chinese   machine translation tasks are shown in Table 1 .
For   the   Turkish - English   machine   translation ,   following ( Sennrich et al , 2015a ) , we use the WIT   corpus   ( Cettolo   et   al . ,   2012 )   and   the   SETimes   corpus   ( Tyers   and Alperen ,   2010 )   as   the   training   dataset ,   merge   the   dev2010   and   tst2010   as   the   validation dataset , and use tst2011 , tst2012 , tst2013 ,   tst2014   from   the   IWSLT   as   the   test   datasets .
We   also use the talks data from the IWSLT evaluation   campaign1 in 2018 and the news data from News   Crawl   corpora2   in   2017   as   external   monolingual   data for the stemming task on Turkish sentences .
For the Uyghur - Chinese machine translation , we   use   the   news   data   from   the   China   Workshop   on   Machine Translation in 2017 ( CWMT2017 ) as the   training   dataset   and   validation   dataset ,   use   the   news   data   from   CWMT2015   as   the   test   dataset .  
Each Uyghur sentence has four Chinese reference   sentences .
Moreover ,   we   use   the   news   data   from   the Tianshan website3 as external monolingual data   for the stemming task on Uyghur sentences .
4.2   Data Preprocessing
We normalize and tokenize the experimental data .  
We utilize the jieba toolkit4 to segment the Chinese   sentences ,   we   utilize   the   Zemberek   toolkit5   with   morphological   disambiguation   ( Sak   et   al . ,   2007 )   and the morphological analysis tool ( Tursun et al ,   2016 )   to   annotate   the   morpheme   structure   of   the   words in Turkish and Uyghur , respectively .
We use our previously proposed morphological   segmentation   method   ( Pan   et   al . ,   2020 ) ,   which   segments the word into smaller subword units with   morpheme   structure .  
Since   Turkish   and   Uyghur   only have a few prefixes , we combine the prefixes   with stem into the stem unit .
As shown in Figure 2 ,   the   morpheme   structure   of   the   Turkish   word   “ hecelerini ” ( syllables ) is : hece + lerini .
Then the   byte   pair   encoding   ( BPE )   technique   ( Sennrich   et   al . ,   2015b )   is   applied   on   the   stem   unit   “ hece ”   to   segment   it   into   “ he@@ ”   and   “ ce@@ ” .  
Thus   the   Turkish word is segmented into a sequence of subword units : he@@ + ce@@ + lerini .
4 https://github.com/fxsjy/jieba   5 https://github.com/ahmetaa/zemberek-nlp
105bir dilin son hecelerini kendisiyle birlikte mezaraMorpheme Segmentationhece+ler+i+nihece+lerinihe@@+ce@@+leriniApply BPE on StemStem+Combined Suffix  Task   En - Tr   Translation
Tr - En   Translation
Turkish   Stemming
lerini
ritüel@@
Training Sentence Samples   < MT >  
We   go   through   initiation   rit@@ es .   Başla@@   ma   yaş@@ ıyoruz .  
< MT >   Başla@@   ma   ritüel@@   lerini yaş@@ ıyoruz .  
We go through initiation rit@@ es .  
< ST > Başla@@ ma ritüel@@ lerini   yaş@@ ıyoruz .  
Başla@@ ritüel@@ yaş@@
Table 2 :   The training sentence samples for multi - task   neural model on Turkish - English machine translation   task .
We add “ < MT > ” and “ < ST > ” before each source   sentence   to   specify   the   desired   target   outputs   for   different tasks .
Lang   Method   # Merge   Vocab   Avg . Len  
Tr   Tr   En   Uy   Uy   Ch
36,468   28   36,040   22   31,306   25   38,164   28   38,292   21   40,835   19
Morph   BPE   BPE   Morph   BPE   BPE
15 K   36 K   32 K   10 K   38 K   32 K
Table 3 :   The detailed statistics of using different word   segmentation   methods   on   Turkish ,   English ,   Uyghur ,   and Chinese .
In this paper , we utilize the above morphological   segmentation   method   for   our   experiments   by   applying BPE on the stem units with 15 K merge   operations for the Turkish words and 10 K merge   operations   for   the   Uyghur   words .  
The   standard   NMT   model   trained   on   this   experimental   data   is   denoted as “ baseline NMT model ” .
Moreover , we   employ BPE to segment the words in English and   Chinese by learning separate vocabulary with 32 K   merge   operations .  
Table   2   shows   the   training   sentence   samples   for   multi - task   neural   model   on   Turkish - English machine translation task .
In   addition ,   to   certify   the   effectiveness   of   the   morphological   segmentation   method ,   we   employ   the pure BPE to segment the words in Turkish and   Uyghur   by   learning   a   separate   vocabulary   with   36 K and 38 K merge operations , respectively .
The   standard NMT model trained on this experimental   data is denoted as “ general NMT model ” .
Table 3   shows the detailed statistics of using different word   segmentation   methods   on   Turkish ,   English ,   Uyghur , and Chinese .
The “ Vocab ” token denotes   the vocabulary size after data preprocessing .
The   “ Avg . Len ”   token   denotes   the   average   sentence   length .
4.3   Training and Evaluation Details
We employ the Transformer model implemented in   the   Sockeye   toolkit   ( Hieber   et   al . ,   2017 ) .  
The   number of layer in both the encoder and decoder is   set to N=6 , the number of head is set to 8 , and the   number of hidden unit in the feed - forward network   is set to 1024 .
We use an embedding size of both   the source and target words of 512 dimension , and   use a batch size of 128 sentences .
The maximum   sentence length is set to 100 tokens with 0.1 label   smoothing .
We apply layer normalization and add   dropout to the embedding   and transformer layers   with 0.1 probability .
Moreover , we use the Adam   optimizer   ( Kingma   and   Ba ,   2015 )   with   an   initial   learning   rate   of   0.0002 ,   and   save   the   checkpoint   every 1500 updates .
Model training process stops after 8 checkpoints   without improvements on the validation perplexity .  
Following Niu et al ( 2018a ) , we select the 4 best   checkpoint   based   on   the   validation   perplexity   values and combine them in a linear ensemble for   decoding .  
Decoding   is   performed   by   using   beam   search   with   a   beam   size   of   5 .  
We   evaluate   the   machine   translation   performance   by   using   the   case - sensitive BLEU score ( Papineni et al , 2002 )   with standard tokenization .
4.4   Neural Translation Models
segmentation .  
The
In this paper , we select 4 neural translation models   for comparison .
More details about the models are   shown below :   General NMT Model : The standard NMT model   trained   on   the   experimental   data   segmented   by   BPE .  
Baseline NMT Model : The standard NMT model   trained   on   the   experimental   data   segmented   by   morphological   following   models also use this word segmentation method .  
Bi - Directional NMT Model : Following Niu et al   ( 2018b ) , we train a single NMT model to perform   bi - directional machine translation .
We concatenate   the bilingual parallel sentences in both directions .  
Since the source and target sentences come from   the same language pairs , we share the source and   target   vocabulary ,   and   tie   their   word   embedding   during model training .  
Multi - Task   Neural   Model :   We   simply   use   the   monolingual   data   of   the   agglutinative   language   from the bilingual parallel sentences .
We use a joint   vocabulary , tie the word embedding as well as the   output layer ’s weight matrix .
4
106  Task   Model   general   TrEn   baseline   general   EnTr   baseline
tst12
tst13
tst14   tst11   25.92   26.55   27.34   26.35   26.48   27.02   27.91   26.33   13.73   14.68   13.84   14.65   14.85   15.93   15.45   15.93
Table 4 :   The BLEU scores of the general NMT model   and   baseline   NMT   model   on   the   machine   translation   task between Turkish and English .
Task   Model   TrEn
tst12
tst13
tst11   tst14   26.48   27.02   27.91   26.33   26.21   27.17   28.68   26.90
baseline   bidirectional   multi - task   baseline   bidirectional   multi - task
EnTr
26.82   27.96   29.16   27.98   14.85   15.93   15.45   15.93   15.08   16.20   16.25   16.56
15.65   17.10   16.35   16.41
Table 5 :   The BLEU scores of the baseline NMT model ,   bi - directional   NMT   model ,   and   multi - task   neural   model on the machine translation task between Turkish   and English .
5   Results and Discussion
Table   4   shows   the   BLEU   scores   of   the   general   NMT model and baseline NMT model on machine   translation task .
We can observe that the baseline   NMT   model   is   comparable   to   the   general   NMT   model , and it achieves the highest BLEU scores on   almost all the test datasets in both directions , which   indicates   that   the   NMT   baseline   based   on   our   proposed segmentation method is competitive .
5.1   Using Original Monolingual Data
Table   5   shows   the   BLEU   scores   of   the   baseline   NMT   model ,   bi - directional   NMT   model ,   and   multi - task neural model on the machine translation   task between Turkish and English .
The table shows   that the multi - task neural model outperforms both   the baseline NMT model and bi - directional NMT   model , and it achieves the highest BLEU scores on   almost all the test datasets in both directions , which   suggests that the multi - task neural model is capable   of improving the bi - directional translation quality   on agglutinative languages .
The main reason is that   compared with the bi - directional NMT model , our   proposed   multi - task   neural   model   additionally   employs   the   stemming   task   for   the   agglutinative   language , which is effective for the NMT model to   learn   both   the   source - side   semantic   information   and the target - side language modeling .
5
Figure 3 : The function of epochs ( x - axis ) and perplexity   ( y - axis )   values   on   the   validation   dataset   in   different   neural translation models for the translation task .
Translation Examples   source   reference   baseline   bidirectional   multi - task   The university was imitating life .
üniversite hayatı taklit ediyordu .  
College was imitating life .  
It was emulating a university life .  
The   university   was   emulating   its   lives .
Table 6 :   A translation example for the different NMT   models on Turkish - English .
The function of epochs and perplexity values on   the validation dataset in different neural translation   models are shown in Figure 3 .
We can see that the   perplexity   values   are   consistently   lower   on   the   multi - task neural model , and it converges rapidly .  
Table   6   shows   a   translation   example   for   the   different   models   on Turkish - English .  
We   can   see   that the translation result of the multi - task neural   model is more accurate .
The Turkish word “ taklit ”   means “ imitate ” in English , both the baseline NMT   and bi - directional NMT translate it into a synonym   “ emulate ” .
However , they are not able to express   the   meaning   of   the   sentence   correctly .  
The   main   reason is that the auxiliary task of stemming forces   the proposed model to focus more strongly on the   core   meaning   of   each   word   ( or   stem ) ,   therefore   helping the model make the correct lexical choices   and capture the in - depth semantic information .
5.2   Using External Monolingual Data
Moreover , we evaluate the multi - task neural model   on   using   external   monolingual   data   for   Turkish   stemming task .
We employ the parallel sentences   and the monolingual data in a 1 - 1 ratio , and shuffle   them randomly before each training epoch .
More   details about the data are shown below :
107  Task   Data   Tr - En
tst11
tst13
tst14   tst12   original   26.82   27.96   29.16   27.98   26.55   27.94   29.13   28.02   talks   26.47   28.18   28.89   27.40   news   26.60   27.93   29.58   27.32   mixed   original   15.65   17.10   16.35   16.41   15.57   16.97   16.22   16.91   talks   15.67   17.19   16.26   16.69   news   15.96   17.35   16.55   16.89   mixed
En - Tr
We also evaluate the translation performance of   the general NMT model , baseline NMT model , and   multi - task neural model with external news data on   the machine translation task between Uyghur and   Chinese .  
The   experimental   results   are   shown   in   Table   8 .  
The   results   indicate   that   the   multi - task   neural model achieves the highest BLEU scores on   the   test   dataset   by   utilizing   external   monolingual   data for the stemming task on Uyghur sentences .
Table   7 :    
The   BLEU   scores   of   the   multi - task   neural   model on using external monolingual data of talks data ,   news data , and mixed data .
6   Conclusions
Task   Uy - Ch
Ch - Uy
Model   general NMT model   baseline NMT model   multi - task   neural   model   with   external monolingual data   general NMT model   baseline NMT model   multi - task neural model with   external monolingual data
BLEU   35.12   35.46   36.47
21.00   21.57   23.02
Table 8 :   The BLEU scores of the general NMT model ,   baseline NMT model , and the multi - task neural model   with   external   monolingual   data   on   Uyghur - Chinese   and Chinese - Uyghur machine translation tasks .
Original Data : The monolingual data comes from   the original bilingual parallel sentences .  
Talks Data : The monolingual data contains talks .  
News Data : The monolingual data contains news .  
Talks   and   News   Mixed   Data :   The   monolingual   data contains talks and news in a 3:4 ratio as the   same with the original bilingual parallel sentences .  
Table 7 shows the BLEU scores of the proposed   multi - task neural model on using different external   monolingual   data .  
We   can   see   that   there   is   no   obvious difference on Turkish - English translation   performance by using different monolingual data ,   whether the data is in - domain or out - of - domain to   the test dataset .
However , for the English - Turkish   machine   translation   task ,   which   can   be   seen   as   agglutinative   language   generation   task ,   using   the   mixed   data   of   talks   and   news   achieves   further   improvements   of   the   BLEU   scores   on   almost   all   the   test   datasets .  
The   main   reason   is   that   the   proposed   multi - task   neural   model   incorporates   many morphological and linguistic information of   Turkish rather than that of English , which mainly   pays   attention   to   the   source - side   representation   ability on agglutinative languages rather than the   target - side language modeling .
In this paper , we propose a multi - task neural model   for   translation task   from   and   into   a   low - resource   and   morphologically - rich   agglutinative   language .  
The model jointly learns to perform bi - directional   translation   and   agglutinative   language   stemming   by utilizing the shared encoder and decoder under   standard NMT framework .
Extensive experimental   results show that the proposed model is beneficial   for the agglutinative language machine translation ,   and only a small amount of the agglutinative data   can   improve   the   translation   performance   in   both   directions .
Moreover , the proposed approach with   external   monolingual   data   is   more   useful   for   translating into the agglutinative language , which   achieves an improvement   of +1.42 BLEU points   for translation from English into Turkish and +1.45   BLEU points from Chinese into Uyghur .
In   future   work ,   we   plan   to   utilize   other   word   segmentation methods for model training .
We also   plan   to   combine   the   proposed   multi - task   neural   model with back - translation method to enhance the   ability of the NMT model on target - side language   modeling .
