CompLx@SMM4H’22 : In - domain pretrained language models for detection of adverse drug reaction mentions in English tweets
Orest Xherija Independent Researcher xherija.orest@gmail.com
Hojoon Choi Nielsen hojoon.choi@nielsen.com
Abstract
The paper describes the system that team CompLx developed for sub - task 1a of the Social Media Mining for Health 2022 ( # SMM4H ) Shared Task .
We finetune a RoBERTa model , a pretrained , transformer - based language model , on a provided dataset to classify English tweets for mentions of Adverse Drug Reactions ( ADRs ) , i.e. negative side effects related to medication intake .
With only a simple finetuning , our approach achieves competitive results , significantly outperforming the average score across submitted systems .
We make the model checkpoints1 and code2 publicly available .
We also create a web application3 to provide a userfriendly , readily accessible interface for anyone interested in exploring the model ’s capabilities .
1
Introduction
The Shared Task ( Weissenbacher et al , 2022 ) of the 2022 Social Media Mining for Health Applications ( # SMM4H ) workshop proposed ten sub - tasks in the domain of social media mining for health monitoring and surveillance .
From the perspective of Natural Language Processing ( NLP ) , these tasks present a considerable challenge since the nature of social media posts requires dealing with both a significant level of language variation ( informal and colloquial expressions , ambiguity , multilingual posts ) and data sparsity , as well as a widespread presence of noise such as misspellings of clinical concepts and syntactic errors .
In the 2022 instantiation of the # SMM4H Shared Task , our team participated in : ( i ) sub - task 1a , the classification of English tweets containing mentions of Adverse Drug Reactions ( ADRs ) ( Magge et al , 2021 ) , ( ii ) sub - task 3 , the classification of English tweets ( 3a ) and WebMD reviews ( 3b ) contain1https://huggingface.co/orestxherija/roberta-base-adr2https://github.com/orestxherija/CompLx-SMM4H2022 3https://huggingface.co/spaces/orestxherija/adr-mentionsmm4h2022
classifier
ing mentions of changes in medication treatments , and ( iii ) sub - task 8 , the classification of English tweets self - reporting chronic stress .
In this paper we primarily describe our approach for task 1a , as that constituted the major focus of our efforts .
To address these challenges , we finetune a variant of a RoBERTa ( Liu et al , 2019 ) model , a transformer - based ( Vaswani et al , 2017 ) language model pretrained on approximately 128 million tweets ( Loureiro et al , 2022 ) on each sub - task ’s provided dataset .
Without any domain adaptation efforts ( apart from standard finetuning on the downstream task ) or hyperparameter optimizations , the model outperforms the average of all submissions for sub - task 1a by a 9 % absolute difference in F1score .
In the following sections , we introduce the subtasks ’ datasets , describe the model architecture and training setup , report our results , and conclude with a discussion of related research and potential avenues for future work .
2 Datasets
In Section [ 1 ] we provided a brief summary of each sub - task in which we participated .
For each of them , participants were given access to a labeled training and validation set , as well as an unlabeled evaluation set that was used to determined the final performance of the submitted systems .
Table
[ 1 ] summarizes the number of samples per dataset per task .
Additionally , Table [ 2 ] provides representative samples from sub - task 1a .
As can be noted upon quick inspection , merely depending
training validation evaluation
1a 17174 909 10969
3a 5898 1572 15360
3b 10378 1297 13132
8 2936 420 839
Table 1 : Number of samples per split per task .
Proceedingsofthe29thInternationalConferenceonComputationalLinguistic , pages176–181October12–17,2022.176  Label Sentence vyvanse make me so hyper and creative and i think of so many tweets ADR feed an ocd vyvanse and cover him in crayons trazodone has screwed up my sleep schedule .
its helping tho .
No ADR ADR
Table 2 : Selection of samples from training set of sub - task 1a .
on medication - related keywords for label assignment is going to be problematic : both the first and the second example contain the medication term “ vyvanse " but they have been assigned different labels , “ ADR " and “ No ADR " respectively .
This motivates the use of a modeling approach that leverages the overall semantic content of the sentence , rather than keyword matching with individual constituents .
3 Modeling Approach
3.1 Model Architecture
The establishment of language modeling as the pretraining step in the transfer learning pipeline revolutionized modern NLP with models such as ULMFiT ( Howard and Ruder , 2018 ) , ELMo ( Peters et al , 2018 ) and , most notably , transformerbased language models such as GPT ( Radford et al , 2018 ) and BERT ( Devlin et al , 2019 ) .
In recent years , there have been intensive efforts in the research community to produce ever - larger transformer - based pretrained language models that are trained using a variety of datasets , transformermodel architectures , training objectives and optimization techniques .
This should come as no surprise , since such language models have dominated virtually all NLP leaderboards , most notably GLUE ( Wang et al , 2018 ) and SuperGLUE ( Wang et al , 2019 ) .
Considering this overwhelming success , we opt for a RoBERTa ( Liu et al , 2019 ) model4 that has been trained on approximately 128 million tweets ( Loureiro et al , 2022 ) .
Our exact modeling approach is depicted in Figure [ 1 ] .
We opt for a model that has been trained on an in - domain corpus , namely tweets , as transfer learning has been shown to yield improved results when there is indomain pretraining ( Gururangan et al , 2020 ) .
We do not use any text normalization steps .
4https://huggingface.co/cardiffnlp/twitter-roberta-basemar2022
3.2 Training Regime
We train the model to minimize the negative log - likelihood loss using back - propagation with stochastic gradient descent and a mini - batch size of 16 .
To monitor model performance , we use the train / validation split provided by the organizers .
For optimization , we use the AdamW optimizer ( Loshchilov and Hutter , 2019 ) with gradient clipping ( Pascanu et al , 2013 ) and a linear scheduler with no warm - up .
We use FP-16 mixed precision ( Micikevicius et al , 2018 ) training ( and inference ) in order to afford a larger batch size and increased training speed .
To optimize GPU use by minimizing the amount of memory allocated for padding tokens , we use dynamic padding and length - based batching in the sense of ( Skinner , 2018 ) .
Finally , we employ label smoothing ( Szegedy et al , 2016 ) with a smoothing factor of 0.1 .
3.3 Hyperparameters
As mentioned in Section [ 1 ] , we do not experiment with hyperparameter tuning but rather keep the default parameters of the Trainer API in the Hugging Face transformers library .
More specifically , we use β1 = 0.9 , β2 = 0.999 and ϵ = 10−8 for the AdamW optimizer parameter values and a learning rate of 0.00005 .
We train the models for a maximum of 25 epochs with an early stopping patience level set to 0.001 for 3 epochs .
Finally , we set a maximum sequence length of 128 since input sentences are generally short and we would like to avoid consuming GPU memory for padding tokens .
4 Experiments and Results
In this section , we give a brief description of the system we used to conduct our experiments , share our results and provide a brief discussion .
4.1 Setup
The model was developed using the PyTorch ( Paszke et al , 2019 ) implementation of the Hugging Face transformers ( Wolf et al , 2020 ) library .
The experiments were executed on a
177  Figure 1 : Illustration of modeling approach ( inference step ): the string input is tokenized and the tokens are passed through the language model backbone so as to obtain contextualized ( vector ) representations of the tokens .
The vector associated with the [ CLS ] token is passed through a feed - forward layer and the logit outputs are used to decide the sample ’s class label , " ADR " or " No ADR " .
machine with an Intel Core i9 - 9820X CPU @ 3.30GHz and a NVIDIA GeForce RTX 2080
Ti GPU with 11 GB of memory .
4.2 Results
Table [ 3 ] summarizes the performance of our approach in the validation set for each sub - task .
Note that in this set of experiments , the validation set was used both during training ( e.g. for early stopping or selection of batch size ) as well as for the reporting of the systems ’ performance .
Table
[ 4 ] summarizes the performance of our approach in the evaluation set for each sub - task .
The organizers chose to disclose to each team only their respective score along with the average score of all submitted systems .
Our system performed considerably better than the average in sub - task 1a and surpassed the existing state - of - the - art F1 - score of 0.63 reported in ( Magge et al , 2021 ) .
Performance was considerably poorer for sub - tasks 3 and 8 .
As mentioned in
P 0.769 0.030 0.571 0.372
R 0.769 0.312 0.995 1.0
F1 0.769 0.055 0.725 0.543
1a 3a 3b 8
Table 3 : Validation set results for sub - tasks 1a , 3 and 8 .
Subtask-1a
Subtask-3a
Subtask-3b
Subtask-8
P 0.737 ( 0.646 ) 0.034 ( 0.535 ) 0.567 ( 0.778 ) 0.372 ( 0.720 )
R 0.585 ( 0.497 ) 0.341 ( 0.458 ) 1.0 ( 0.888 ) 1.0 ( 0.760 )
F1 0.652 ( 0.562 ) 0.061 ( 0.456 ) 0.723 ( 0.818 ) 0.542 ( 0.750 )
Table 4 : Results on the evaluation set for sub - tasks 1a , 3 and 8 .
Average score of all participating systems in parentheses .
Metric is F1 - score for class 1 .
Section [ 1 ] , our main efforts were dedicated to subtask 1a and the system developed did not transfer well to the remaining sub - tasks .
5 Conclusion and Future Directions
We demonstrated that a RoBERTa model ( Liu et al , 2019 ) pretrained on approximately 128 million tweets performs very competitively when finetuned on English tweet classification for ADRs .
Using only a standard finetuning approach , our model obtained competitive results , outperforming the average of all submissions for sub - task 1 by a 9 % absolute difference in F1 - score .
This constitutes yet another testament of the fact that large pre1780[CLS]2362no5840Ġtaste13Ġfor47Ġyou111Ġ-11454vy9965van1090se2[SEP]Finetuned RoBERTa - Base Language Model BackboneFeed Forward Layerno taste for you -vyvanse1.53 - 0.68ADRNo ADRADRModel OutputClassificationFeaturizationTokenization  trained language models have rightfully become the default approach in virtually all NLP tasks .
With respect to potential future work , there is a large collection of available options .
Text classification and , more generally , binary classification is one of the oldest and most widely researched topics in NLP .
Most approaches aiming to improve performance of classification models can be broadly categorized into three groups , depending on the segment of the machine learning workflow that they are targeting .
Data augmentation methods typically target the initial part of the workflow , the data , aiming to increase the quantity , quality and diversity of the training dataset to ensure that model performance is robust to small syntactic or semantic perturbations in the inputs .
Transformations acting directly on strings , such random token insertions or deletions , synonym / antonym replacements and related techniques ( Wei and Zou , 2019 ; Karimi et al , 2021 , inter alia ) have shown significant performance improvements , especially in lowresource scenarios much like the one in this shared task .
A second approach , evidently a natural extension of the previous technique , would be to target vector encodings of the tokens and/or documents that are produced by the various layers of the neural networks .
We can distinguish two different approaches here : ( i ) improve the language model backbone during the pretraining phase , or ( ii ) improve the weights of the language model backbone during finetuning .
The research community has devoted intensive efforts in the former approach , as can be observed by the ever - increasing list of transformerbased pretrained language models ( Devlin et al , 2019 ; Joshi et al , 2020 ; Kitaev et al , 2020 ; Raffel et al , 2020 ; Brown et al , 2020 , inter multi alia ) released .
Model size , in terms of total number of trainable parameters , has been consistently shown to correlate strongly with downstream performance , so opting for a larger pretrained model would be a reasonable first steps towards more transferable vector representations ( and hence improved performance ) in the downstream task .
The latter approach would include domain adaptation techniques , such as continued self - supervised pretraining followed by supervised finetuning , which has been shown ( Gururangan et al , 2020 ) to consistently lead to superior results relative to direct finetuning .
( Hui and Belkin , 2021 ) , in an extensive series of experiments , show that the established practice of using a cross - entropy loss for classification is not well - founded and show through a variety of diverse experiments that a square loss can , in many cases , significantly improve performance .
