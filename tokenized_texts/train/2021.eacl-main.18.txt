Non - Autoregressive Text Generation with Pre - trained Language Models Yixuan Su ♦ Deng Cai ♥ Yan Wang ♠ David Vandyke ♣ Simon Baker ♦
Piji Li ♠ Nigel Collier ♦
♦
Language Technology Lab , University of Cambridge ♥ The Chinese University of Hong Kong ♠ Tencent AI
Lab ♣ Apple { ys484,sb895,nhc30}@cam.ac.uk thisisjcykcd@gmail.com , dvandyke@apple.com { brandenwang,pijili}@tencent.com
Abstract
Non - autoregressive generation ( NAG ) has recently attracted great attention due to its fast inference speed .
However , the generation quality of existing NAG models still lags behind their autoregressive counterparts .
In this work , we show that BERT can be employed as the backbone of a NAG model to greatly improve performance .
Additionally , we devise mechanisms to alleviate the two common problems of vanilla NAG models : the inﬂexibility of preﬁxed output length and the conditional independence of individual token predictions .
Lastly , to further increase the speed advantage of the proposed model , we propose a new decoding strategy , ratio-ﬁrst , for applications where the output lengths can be approximately estimated beforehand .
For a comprehensive evaluation , we test the proposed model on three text generation tasks , including text summarization , sentence compression and machine translation .
Experimental results show that our model signiﬁcantly outperforms existing non - autoregressive baselines and achieves competitive performance with many strong autoregressive models .
In addition , we also conduct extensive analysis experiments to reveal the effect of each proposed component.1
1
Introduction
Autoregressive generation ( AG ) models achieve state - of - the - art performance on a wide range of text generation tasks , such as machine translation ( Vaswani et al , 2017 ) and text summarization ( Rush et al , 2015 ) .
Such models generate a token sequence in a left - to - right , token - by - token fashion .
The prediction for the next token is conditioned on all previously generated tokens .
This characteristic makes it impossible to parallelize the computational overhead for token predictions in different
1All related code , data , and models can be found in https://github.com/yxuansu/NAG-BERT .
positions , which leads to a relatively high latency in inference .
On the other hand , non - autoregressive generation ( NAG ) models ( Gu et al , 2018 ) have emerged as a promising alternative due to their fast inference speed .
NAG models omit the sequential dependencies within the output - side sequence and predict tokens in all positions simultaneously once the output length has been determined beforehand .
While NAG models enjoy full parallelism and faster inference , the generation quality of NAG models often lags behind their autoregressive counterparts .
In this work , we explore the potential of largescale pre - trained language models for improving the performance of non - autoregressive generation .
Speciﬁcally , we utilize BERT ( Devlin et al , 2019 ) as the backbone for NAG modelling and extend the architecture of BERT with a CRF output layer ( Lafferty et al , 2001 ; Sun et al , 2019 ) for better capturing the output - side dependencies .
In addition , we analyze two signiﬁcant limitations that NAG models currently suffer from : ( 1 ) the inﬂexibility of preﬁxed output length , and ( 2 ) the conditional independence of individual token predictions .
Accordingly , we devise two solutions to these two problems .
First , prior NAG models require the output length to be determined before token generation , thus an extra module for output length prediction is always required .
Nevertheless , the most likely length from the prediction module is not necessarily the best - suited one for the token generation model .
To this end , previous works ( Gu et al , 2018 ; Ma et al , 2019 ) usually rely on length - parallel decoding ( LPD ) ( Wei et al , 2019 ) for performance enhancement ; that is , generating and re - ranking the results from different output length candidates .
In this work , we propose a simple and elegant decoding mechanism that lets the model determine the output length on - the-ﬂy .
Speciﬁcally , our model dynamically adjusts the output sequence length via
Proceedingsofthe16thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics , pages234–243April19 - 23,2021. © 2021AssociationforComputationalLinguistics234  emitting an [ eos ] token at any output position to indicate the ending of the generated sequence .
Therefore , we can avoid the additional efforts of output length prediction and results re - ranking .
Second , most existing NAG models assume the token predictions in different positions are conditionally independent .
As a consequence , they often tend to generate results that are ungrammatical with repetitions ( Wang et al , 2019b ) .
To alleviate this problem , we propose a context - aware learning objective which impels the model to output different tokens at adjacent positions , thereby reducing the possibility of repetitive generation .
Furthermore , for tasks like text summarization , the output sequence ( summary ) is known to be shorter than the source sequence ( article ) .
In such cases , to further improve the model ’s inference efﬁciency , we introduce a new ratio-ﬁrst decoding strategy .
Speciﬁcally , instead of performing inference on all source - side hidden states , ratio-ﬁrst generates the result only based on a subset of source hidden states .
The subset size is jointly determined by the source length T and a predeﬁned ratio α that is set based on our prior knowledge from the data statistics .
In the experiments , we show that ratio-ﬁrst can signiﬁcantly improve the inference speed while maintaining the generation quality .
We evaluate the proposed model on three typical text generation tasks , including text summarization , sentence compression and machine translation .
Experimental results show that our model signiﬁcantly outperforms many strong non - autoregressive baselines , and even performs competitively with several strong autoregressive models .
In addition , we conduct extensive analysis experiments to study the effect of individual proposed components .
In summary , our contributions are : ( 1 ) We propose a novel framework that utilizes BERT for text generation under the non - autoregressive generation paradigm ; ( 2 ) We propose a decoding mechanism that allows the model to dynamically determine the output length , and a new context - aware learning objective that reduces errors stemming from the output - side conditional independence assumption ; ( 3 ) We introduce a ratio-ﬁrst decoding strategy that further improve the model ’s inference efﬁciency .
2 Background
Autoregressive generation ( AG ) models generate sequences based on a left - to - right factorization .
As shown in Figure 1 , given the source sequence X ,
Figure 1 : ( a ) Autoregressive ; ( b ) Non - Autoregressive
the target sequence Y with length T ( cid:48 ) is generated via a chain of conditional probabilities based on the left - to - right sequential dependencies as :
T ( cid:48 ) ( cid:89 )
i=1
T ( cid:48 ) ( cid:89 )
i=1
p(Y|X ) =
p(yi|y < i , X ) ,
( 1 )
where y < i denotes the tokens before the i - th step .
This property of autoregressive factorization makes the generation process hard to be parallelized as the result is generated token by token .
Unlike AG models , non - autoregressive ( NAG ) models generate sequences without modelling the output - side dependencies .
As shown in Figure 1 , given the prespeciﬁed output length T ( cid:48 ) , the probability of the target sequence Y is then modelled as :
p(Y|X ) =
p(yi|X , i , T ( cid:48 ) ) .
( 2 )
With this conditional independence assumption , NAG models can fully parallelize their generation process , which signiﬁcantly improves the inference speed .
However , it has been shown that , the choice of the prespeciﬁed output length has a notable impact on the model ’s generation quality ( Gu et al , 2018 ) .
In addition , the removal of output - side sequential dependency also causes the generation quality of NAG models to be inferior to their autoregressive counterparts ( Wang et al , 2019b ) .
3 Proposed Model
In this section , we give a detailed explanation of the proposed model .
First , we describe how to utilize BERT as a non - autoregressive generation model .
Then we discuss the decoding mechanism which allows the model to determine the output length dynamically .
Finally , we introduce the new ratioﬁrst decoding strategy which further improves the model ’s decoding efﬁciency .
235  Figure 2 : The overall illustration of the proposed model : During training , the model parameters are only updated on the positions of the target sequence .
During inference , once the decoded trajectory ( colored in red ) gets into the [ eos ] state , it will only transit to the [ eos ] state in the remaining steps .
The ﬁnal result is obtained by removing the generated [ eos ] tokens from the entire decoded trajectory .
3.1 Model Architecture
The architecture of the proposed model is presented in Figure 2 , in which the embedding layer and the stack of transformer layers are initialized with BERT ( Devlin et al , 2019 ) .
Input Representation Following the setup of BERT , we ﬁrst append a [ cls ] and a [ sep ] token on both sides of the source sequence .
Then we attach a number of [ pad ] tokens at the end of source sequence to make its length equal to the predeﬁned maximum size ( e.g. , 256 ) .
Thus we can make sure the source length is longer than or equal to the output length .
As a special case , for tasks like text summarization where the source is known to be longer than the target , we do not attach the [ pad ] tokens when constructing the input .
Transformer Layers Given the source sequence X , it is processed by a stack of N transformer ( Vaswani et al , 2017 ) layers .
Formally , the MultiHead Attention is deﬁned as MultiHead(Q , K , V ) , where Q , K , V denotes the query , key and value respectively .
The computation of the ﬁrst transformer layer is then deﬁned as :
V(1 ) = MultiHead(E(X ) , E(X ) , E(X ) ) , O(1 ) = FFN(V(1 ) ) , FFN(x ) = max(0 , xW1 +
b1)W2 + b2 ,
( 3 )
( 4 )
( 5 )
where E(X ) = T E(X ) + P E(X ) in which T E ( · ) denotes the token embedding and P E ( · ) denotes the position embedding .
For other layers :
V(n ) = MultiHead(O(n−1 ) , O(n−1 ) , O(n−1 ) ) ,
O(n )
= FFN(V(n ) ) ,
( 6 )
( 7 )
where n = 2 , ... , N and N is the total number of transformer layers .
The ﬁnal sequence representation H ∈ RT ×dmodel is the output states of BERT from the last layer , where T is the source sequence length and dmodel is the model size .
CRF Layer Then , H is passed through a linearchain CRF ( Lafferty et al , 2001 ) .
Under the CRF framework , the likelihood of the target sequence Y with length T ( cid:48 ) is then modelled as :
PCRF(Y|X )
=
eS(X , Y ) Y(cid:48 ) eS(X , Y(cid:48 ) )
=
1 Z(X )
exp (
Φyi(hi )
+
t(yi−1 , yi ) ) ,
T ( cid:48 ) ( cid:88 )
i=2
( cid:80 )
T ( cid:48 ) ( cid:88 )
i=1
( 8)
where Z(X ) is the normalizing factor and Φyi(hi ) denotes the label score of yi at position i.
In practice , Φ is parameterized by a neural network that maps the BERT output state hi into the label ( vocabulary ) space .
The t(yi−1 , yi )
= Tyi−1,yi denotes the transition score from label yi−1 to yi where T ∈ R|V |×|V | is the transition matrix .
Approximation
In the context of text generation , the size of the label space ( vocabulary size ) |V | is typically large , e.g. , 32k .
Therefore , it is intractable to directly model the transition matrix T and the normalizing factor Z(X ) .
To this end , we adopt the techniques proposed by Sun et al ( 2019 ) to approximate these two terms .
Speciﬁcally , the full transition matrix is approximated by the product of two low - rank matrices
T = E1ET 2 , where E1 , E2 ∈ R|V |×d and d is much smaller than |V | .
To compute the normalizing factor Z(X ) , at each
236  time step , instead of searching through all possible paths , the number of candidates is heuristically truncated to a predeﬁned beam size k. We refer readers to the original paper for further details .
3.2 Output Length Determination
In this section , we describe how to let the model determine the output sequence length by itself .
Our basic idea is that we want the model to dynamically stop generation via emitting a special [ eos ] token .
To achieve this , during training , we manually append two consecutive [ eos ] tokens to the end of the target sequence , as shown in the top left part of Figure 2 .
In this way , the model can learn a deterministic transition behaviour between two [ eos ] states , meaning that t([eos ] , [ eos ] )
= maxv∈V t([eos ] , v ) .
This is because , during training , the model never sees a transition ( [ eos ] , v ) , where v ( cid:54)=
[ eos ] .
During inference , the result ˜Y is acquired as ˜Y = arg maxY(cid:48 ) S(X , Y(cid:48 ) ) , where the CRF scoring function S(X , Y(cid:48 ) ) in Equation ( 8) can be decomposed as :
S(X , Y(cid:48 ) )
=
Φy(cid:48 )
i
( hi ) +
t(y(cid:48 )
i−1 , y(cid:48 ) i )
T ( cid:88 )
i=1
T ( cid:88 )
i=2
= Φy(cid:48 )
( h1 ) 1 ( cid:124 ) ( cid:123)(cid:122 ) ( cid:125 ) initial state
T ( cid:88 ) {
+
i=2
label score ( cid:122 ) ( cid:125)(cid:124 ) ( cid:123 ) Φy(cid:48 ) i ( cid:124 )
transition score ( cid:122 ) ( cid:123 ) ( cid:125)(cid:124 ) t(y(cid:48 ) i−1 , y(cid:48 ) } .
i ) ( cid:125 ) ( cid:123)(cid:122 ) state transition
( hi ) +
( 9 )
Once the decoded trajectory enters the [ eos ] the state transition term in S(X , Y(cid:48 ) ) state , will be dominated by the transition score term t([eos ] , [ eos ] ) .
As a result , the model will keep transitioning to [ eos ] in the remaining steps .
An example is provided in the right part of Figure 2 , from which we can see that , at step 5 , the decoded trajectory enters the [ eos ] state and remains at it in the rest of the generation process .
In this way , our model can dynamically control the length of output sequence by entering the [ eos ] state during the generation process .
After the entire generation process is completed , the ﬁnal output sequence can be obtained by removing all generated [ eos ] tokens .
3.3 Ratio - First Decoding
We note that the outputs of BERT can be divided into two subsets .
The ﬁrst subset ranges from the beginning to the position where the ﬁrst [ eos ] is emitted , and the second subset is the rest .
For example , in Figure 2 , the ﬁrst subset are those corresponding to the output sequence “ y(1 ) y(2 ) y(3 )
y(4 )
[ eos ] ” .
As for the second part , we can see that it has little effect on the ﬁnal output and removing it should not change the result .
This indicates that it sufﬁces to only consider the beginning part of BERT outputs for improving the inference speed .
Especially , for tasks like summarization where the target is known to be shorter than the source sequence , we are safe to only use the ﬁrst [ α · T ] outputs of BERT to perform inference .
Here T denotes the source length , α ∈ ( 0.0 , 1.0 ) is set based on the data statistics and [ · ] is the integer rounding operation .
Formally , given the source sequence X , the ratio-ﬁrst decoding is deﬁned as
˜Y
= arg max
F(X , Y(cid:48 ) , α ) ,
Y(cid:48 )
= arg max
{
Y(cid:48 )
[ α·T ] ( cid:88 )
i=1
Φy(cid:48 )
i
( hi ) +
t(y(cid:48 )
i−1 , y(cid:48 )
i ) } .
[ α·T ] ( cid:88 )
i=2
( 10 )
When α = 1.0 , ratio-ﬁrst degenerates to the standard decoding strategy in CRF - based models .
It should be noted that , [ α · T ] only constrains the maximum length of the generated result , and the actual output length ( after removing the generated [ eos ] tokens ) is still decided by the model itself .
In the experiment section , we demonstrate that ratio-ﬁrst can notably improve the inference speed whilst maintaining the generation quality .
4 Learning
Due to the conditional independence approximation on output tokens , NAG models often tend to generate repeated tokens ( Wang et al , 2019b ) .
One way to alleviate this problem is to introduce implicit dependencies on the output side .
In this work , we propose to use the unlikelihood formulation of Welleck et al ( 2020 ) in the context of NAG , where we deﬁne the set of negative candidate as the surrounding tokens within a predeﬁned context window c. Formally , given the source sequence X and the target sequence Y with length T ( cid:48 ) , the proposed context - aware objective is deﬁned as :
LCA(Y|X )
= −
{ log pθ(yi|hi ; X ) + lCA(i ) } ,
lCA(i )
=
log(1.0 − pθ(yj|hi ; X ) ) ,
( 11 )
T ( cid:48 ) ( cid:88 )
i=1
j = i+c ( cid:88 )
j = i−c , yj ( cid:54)=yi
237  where hi is the model output state at position i.
At position i , the proposed objective maximizes the probability of token yi while minimizing the probabilities of the surrounding tokens .
In this way , it discourages the model from generating repetitive tokens at different time steps .
The overall learning objective is then deﬁned as
LCRF = − log PCRF(Y|X ) , L = LCRF + λ · LCA ,
( 12 )
where λ controls the importance of different loss terms and PCRF(Y|X ) is described in Equation ( 8) .
matrix in the CRF layer , we set the dimension d of matrices E1 and E2 as 32 .
For the normalizing factor Z(X ) , we set the predeﬁned beam size k as 256 .
As for the overall learning objective , we set the window size c as 3 and λ as 1.0 .
In training , we use Adam optimizer ( Kingma and Ba , 2015 ) .
To measure the relative speedup , we follow the standard setup which runs inference for each individual example separately .
The model ’s inference speed is computed by averaging the results of test cases .
For a fair comparison , we measure the inference speed of all models on the same platform .
5 Related Work
6.2 Text Summarization
Non - Autoregressive generation was ﬁrst introduced by Gu et al ( 2018 ) to reduce the inference latency in machine translation .
Recent works in this area have investigated ways to mitigate the tradeoff between the decoding speed and generation quality .
Gu et al ( 2018 ) utilized fertility as latent variables for better translation performance .
Wang et al ( 2019b ) proposed two auxiliary objectives for better modelling the output states and solving the under - translation problem .
To better model the intermediate alignments between source and target sides , Ma et al ( 2019 ) proposed a model based on the generative ﬂow framework .
Ghazvininejad et al ( 2019 ) proposed to use a masked language objective to train the NAG model .
During inference , starting from a fully masked sequence , the output is generated in an iterative reﬁnement manner .
Recently , Sun et al ( 2019 ) proposed to incorporate a conditional random ﬁeld into the decoder of a NAG model for better modelling the outputside dependencies .
Our work is different from prior works in two aspects : ( 1 ) we directly utilize a pretrained language model ( BERT ) to perform nonautoregressive generation ; ( 2 ) our model can dynamically generate the output sequence without the need of prespeciﬁed output length .
6 Experiments
We evaluate the proposed model on three typical text generation tasks : ( 1 ) text summarization ; ( 2 ) sentence compression and ( 3 ) machine translation .
6.1 Experimental Setup
We implement the proposed model with PyTorch ( Paszke et al , 2017 ) .
The BERT model we use is the Huggingface implementation ( Wolf et al , 2019 ) ( bert - base - uncased ) .
To approximate the transition
Text summarization aims to automatically generate a compact summary that retains the most important content of the original text document ( Nenkova and McKeown , 2012 ) .
In this experiment , we use the Gigawords dataset ( Rush et al , 2015 ) as our benchmark .
For evaluation , standard metrics including ROUGE-1 ( R-1 ) , ROUGE-2 ( R-2 ) and ROUGE - L ( R - L ) ( Lin , 2004 ) are reported .
We compare our model with several representative and the latest NAG models , including NAGNMT ( Gu et al , 2018 ) , NAR - REG ( Wang et al , 2019b ) and NAG - CRF ( Sun et al , 2019 ) .
Following previous works , during training , we train a length predictor to predict the output length .
During inference , for each NAG baseline , we adopt the length - parallel decoding strategy ( LPD - k ) ( Wei et al , 2019 ) , that is , generating k results using the top - k possible output length predictions from the length predictor .
The results are then re - ranked by a transformer model to get the ﬁnal ouput .
In the experiment , we report the results of different NAG baselines using LPD-9 decoding .
In addition , to better examine the effect of using BERT in NAG models , we add a BNAG - CRF baseline which adopts the same structure of the NAG - CRF model but using BERT as the encoder .
We also compare our model with several strong autoregressive models , which are Luong - NMT ( Luong et al , 2015 ) , Pointer - Generator ( See et al , 2017 ) , DRGD ( Li et al , 2017 ) and Concept Pointer ( Wang et al , 2019a ) .
To measure the relative inference speedup , we include transformer as a baseline model .
The results are shown in Table 1 , from which we can see that , by using length - parallel decoding , the performance of all NAG baselines can be notably improved .
However , such procedure significantly increases the inference latency .
In contrast ,
238  Models
R - L
Speedup
Models
R-2
R - L
Speedup
R-1
R-2 Autoregressive 14.45 15.99 17.61 16.40 16.97
33.10 35.98 36.25 36.62 35.74
Non - Autoregressive
27.20 29.76 28.56 31.23 30.29 32.91 32.63 34.56 34.67 35.05
8.96 10.03 9.79 11.14 12.61 14.31 14.32 16.10 16.13 16.48
30.71 33.33 33.55 33.98 33.43
25.58 28.04 26.83 29.55 28.71 31.03 30.82 32.76 32.81 33.28
Luong - NMT Pointer - Generator DRGD Concept Pointer Transformer ( b = 4 )
NAG - NMT + LPD-9 NAR - REG + LPD-9 NAG - CRF + LPD-9 BNAG - CRF + LPD-9 Ours ( α = 0.3 ) Ours ( α = 1.0 )
1.00×
9.31× 5.28× 8.64× 4.74× 8.07× 4.32× 6.13× 3.21× 9.31× 6.72×
Table 1 : Results on Gigawords dataset , where b in the transformer baseline stands for beam search size .
our model can self - determine the output length without any re - ranking process .
As shown in the results , our model outperforms the best NAG baseline ( with LPD ) and achieves performances that are comparable with several strong AG models .
Comparing the results of BNAG - CRF and NAGCRF , we can see that incorporating BERT as encoder helps to improve the model performance .
Nonetheless , our model still outperforms BNAGCRF with LPD-9 decoding .
This is because the dynamic length decoding mechanism allows our model to generate results with optimal length , leading to stronger model performances .
Finally , we analyze the proposed ratio-ﬁrst decoding .
From the results , we observe a moderate performance drop when using ratio-ﬁrst ( α = 0.3 ) .
It comes from the fact that , for some input documents with length T , the reference summary is longer than [ α · T ] .
In such cases , ratio-ﬁrst fails to generate the complete reference summary , leading to the drop of performance .
On the other hand , we can see that , ratio-ﬁrst can notably improve the inference speedup .
With α = 0.3 , our model achieves the highest inference speedup while still outperforms all compared NAG models .
6.3 Sentence Compression
Sentence compression aims at compressing a long sentence into a short one by deleting redundant words .
In this experiment , we use the Google sentence compression dataset ( Filippova and Altun , 2013 ) as our benchmark .
For evaluation , we use
Bi - LSTM - Dep Tagger Tagger+ILP HiSAN - Dep HiSAN Transformer ( b = 4 )
R-1
F1 Autoregressive 82.3
82.8 79.0 82.7 83.2 82.4
81.5 81.1 76.1 82.1 82.9 82.0
Non - Autoregressive
NAG - NMT + LPD-9 NAG - REG + LPD-9 NAG - CRF + LPD-9 BNAG - CRF + LPD-9 Ours ( α = 0.7 ) Ours ( α = 1.0 )
72.5 73.8 73.7 75.6 75.1 77.3 77.1 79.3 79.5 80.7
72.1 73.6 73.1 75.1 74.4 76.5 76.2 78.5 79.0 80.3
74.1 72.4 64.6 74.9 75.8 74.6
59.9 61.0 61.5 63.4 66.8 69.0 68.9 71.7 72.1 73.6
81.3 80.9 75.8 81.9 82.7 81.8
71.8 73.1 73.0 74.9 74.2 76.3 76.0 78.2 78.7 80.1
1.00×
10.71× 6.09× 10.00× 5.49× 9.41× 5.04× 7.21× 3.91× 10.00× 8.42×
Table 2 : Results on sentence compression task
the standard token - kept - F1 ( F1 ) score .
In addition , We also report the results of other standard metrics including ROUGE-1 , ROUGE-2 and
ROUGE - L. We compare the proposed model with the same NAG baselines as in the previous experiment .
We also compare our model with several strong autoregressive models , including Bi - LSTM - Dep ( Filippova et al , 2015 ) , Tagger and Tagger+ILP ( Wang et al , 2017 ) , HiSAN - Dep and HiSAN ( Kamigaito et al , 2018 ) .
To measure the inference speedup , we include transformer as a baseline model .
The results are presented in Table 2 , from which we see that our model outperforms the best reported NAG baseline ( with LPD ) in terms of both the generation quality and inference speed .
Comparing with the strong autoregressive models , our model can achieve competitive performance with a over 8.42× inference speed up .
We also report the results of our model using the ratio-ﬁrst decoding strategy .
By setting α as 0.7 , it achieves a 10.00× inference speedup while still outperforming other compared NAG baselines .
6.4 Machine Translation
Machine translation aims at translating text from the source language to the target language .
In this task , we use the IWSLT14 German - to - English ( DEEN ) dataset as our benchmark .
Following previous works , we use the sequence - level knowledge distillation ( Gu et al , 2018 ) during training .
For evaluation , we report results in BLEU scores ( Papineni et al , 2002 ) .
In this experiment , we use the BERT model in German language .
We compare our model with a range of strong
239  Models
Speedup(× )
BLEU Autoregressive 28.53 32.84 33.31
LSTM - based CNN - based Transformer ( b = 4 )
ENAG - E ENAG - P NAG - REG NAG - NMT NAG - CRF BNAG - CRF Ours ( α = 0.8 ) Ours ( α = 1.0 )
Non - Autoregressive 24.13 ( 27.30 ) 25.09 ( 28.60 ) 23.89 ( 28.04 ) 23.04 ( 26.79 ) 26.39 ( 29.21 ) 26.73 ( 29.67 ) 29.71 30.45
1.00
15.08 ( 7.39 ) 14.48 ( 7.24 ) 16.45 ( 9.05 ) 13.92 ( 7.24 ) 11.74 ( 6.03 ) 9.42 ( 5.01 ) 13.92 11.31
Table 3 : Results on IWSLT14
De - En dataset .
The numbers in ( ) are results using length - parallel decoding .
BERT CRF
( cid:88 )
× ( cid:88 )
×
R-1 35.05 32.41 32.16 27.02
R-2 16.48 14.19 11.33 8.81
R - L 33.28 30.53 30.34 25.25
( cid:88 ) ( cid:88 )
× ×
Table 4 : Ablation study on Gigawords dataset .
NAG models , including NAG - NMT ( Gu et al , 2018 ) , ENAG - E and ENAG - P ( Guo et al , 2019 ) , NAG - REG ( Wang et al , 2019b ) , NAG - CRF ( Sun et al , 2019 ) and BNAG - CRF .
For each NAG baseline , we also report the results using LPD9 decoding .
In addition , we compare our model with several strong autoregressive models , including LSTM - based ( Wu et al , 2016 ) , CNN - based ( Gehring et al , 2017 ) and transformer model .
The results are shown in Table 3 , from which we see that our model outperforms the best NAG baseline ( with LPD ) in terms of both the generation quality and inference speedup .
Additionally , we also report the results using the ratio-ﬁrst decoding .
By setting α as 0.8 , the inference speedup can be further boosted to 13.92× while the generation quality is still higher than the best NAG baseline .
6.5 Further Analysis
In this section , we present further discussions and empirical analysis of the proposed model .
BERT & CRF To quantify the importance of each component ( BERT & CRF ) of our model , we evaluate the performance on Gigawords dataset by removing each component iteratively .
The results are shown in Table 4 , from which we can see that by removing any of these compoModels w/o CA Ours Transformer
rep-1 6.897 5.786 4.329
rep-2 2.640 1.978 1.348
rep-3 0.741 0.427 0.267
rep-4 0.295 0.106 0.089
R - L 32.89 33.28 33.43
Table 5 : Evaluation results on n - gram repetitions .
nents , the overall performance decreases .
By removing BERT from the model , we observe notable drop across all metrics .
This shows that the knowledge of BERT is an important factor of the model ’s strong performance .
Comparing with results in Table 1 , it still outperforms vanilla NAG - CRF and performs comparably with NAG - CRF using LPD decoding , which demonstrates the merit of the proposed dynamic length decoding mechanism .
Another interesting ﬁnding is that , by only removing the CRF layer , the most notable drop is observed on the bigram - level metric ( ROUGE-2 ) .
This shows that the bigram - level dependencies on the output side are mainly captured by the CRF module .
In addition , by removing both BERT and CRF , all metrics further decrease .
This conﬁrms that each of these two components positively contributes to the model ’s overall performance .
Context - Aware Objective
In this part , we study the effect of the context - aware objective .
As described in Equation ( 11 ) , it aims at alleviating the problem of repetitive generation .
To give a quantitative analysis , we use the measurement of sentencelevel repetition ( Welleck et al , 2020 ) to compute the ratio of duplicate n - grams ( rep - n ) in the generated result .
This metric is deﬁned as
rep - n(Y ) = 100 × ( 1.0 −
|unique n - grams(Y)| |n - grams(Y)|
)
.
( 13 ) For each generated result , rep - n is 0.0 when it has no repeating n - grams .
The ﬁnal result is computed by averaging over the entire evaluation set .
We conduct experiments on Gigawords dataset to evaluate the n - gram repetitions ranging from uni - gram to 4 - gram .
The results are shown in Table 5 , where w/o CA means the model is trained without using context - aware objective and
R - L denotes the model ’s ROUGE - L score .
Additionally , we also show the results from transformer model for a direct comparison .
Comparing the two variants of our model , we see that training with context - aware objective leads to a 42 % drop on rep-3 metric ( 0.427 vs 0.741 ) and a 64 % drop on rep-4 metric ( 0.106 vs 0.295 ) .
The ROUGE - L results also indicate that
240  Models
BLEU Speedup(× )
Ours ( α = 1.0 ) 30.45 11.31
Length - Parallel Decoding LPD-5 29.62 8.92
LPD-10 30.37 6.01
LPD-1 27.15 11.84
Table 6 : Results comparison on IWSLT14 dataset
the reduction in token repetition can effectively improve the model generation quality .
Dynamic Length Determination Next , we examine the importance of the model ’s ability to dynamically determine the length of the generated output .
To this end , we train another model variant by removing the two [ eos ] tokens from the target sequence .
In this way , the model is not able to self - determine the output length throughout the generation process .
To perform inference , we use length - parallel decoding ( LPD ) with different number of length candidates .
Formally , for each length candidate l , the model generates the result ˜Y as
˜Y
= arg max
{
Y(cid:48 )
l ( cid:88 )
i=1
Φy(cid:48 )
i
( hi ) +
t(y(cid:48 )
i−1 , y(cid:48 )
i ) } .
l ( cid:88 )
i=2
( 14 ) The ﬁnal result is acquired by re - ranking the generated results with a transformer model .
We conduct experiments on the IWSLT14 DEEN dataset in which we try a different number of length candidates , including top-1 , top-5 and top10 .
The results are shown in Table 6 , from which we can see , as the number of length candidates increases , the model performance increases as well .
The reason is that a larger candidates set is more likely to contain the best - suited length for the generation model , leading to better performance .
However , such decoding procedure inevitably increases the required computation overhead .
We can see that , when setting k as 10 , the inference speedup decreases from 11.84× to 6.01×.
In contrast , our proposed model is able to determine the optimal output length by itself .
Without any re - ranking process , it outperforms the model with LPD-10 decoding and achieves the inference speedup that is comparable with the model using LPD-1 decoding .
Ratio - First Decoding We are also interested in the effect of the ratio-ﬁrst decoding strategy .
To provide a quantitative analysis , we perform inference on the Gigawords dataset using ratio-ﬁrst with different α .
The experimental results with different α are presented in Figure 3 .
It can be observed that , when α reaches 0.3 , the model approximately
Figure 3 : Experiment results on Gigawords dataset using ratio-ﬁrst decoding with different α .
Figure 4 : The distribution of target / source length ratio of the training and test set in Gigawords dataset .
achieves its optimal performance .
At the same time , a notable improvement can be observed in terms of the inference speedup ( 6.72× → 9.31× ) .
Now we illustrate why the near optimal performance can be achieved when α reaches 0.3 .
In Figure 4 , we present the distribution of the target / source length ratio of every data instance in the Gigawords dataset .
We can see that , for most cases , the ratio between the target length T ( cid:48 ) and source length T is less than 0.3 .
Recall the deﬁnition of ratio-ﬁrst decoding in Equation ( 10 ) , the [ α · T ] constrains the maximum length of the generated result .
Therefore , once we have a prior knowledge on the data statistic , we can easily choose a proper α that both improves the inference speed whilst maintaining the generation quality .
In this case , a proper α could be 0.3 which is demonstrated by the results in Figure 3 and 4 .
By setting different α , ratio-ﬁrst provides us an explicit way to control the balance between the inference speed and the generation quality .
This property of ratio-ﬁrst is especially favorable in real - life scenarios where the inference speed is the highest concern .
241  7 Conclusion
In this work , we explored the potential of BERT in various text generation tasks under the NAG framework .
To address problems from NAG models previously having a preﬁxed output length , we devised a decoding mechanism which enables the model to determine the output length dynamically .
To reduce errors stemming from the assumption of conditional independence of output tokens , we proposed a context - aware objective as well as using a CRF decoding .
Furthermore , to maximize the inference speed advantage of our model , we introduced a ratio-ﬁrst decoding strategy .
We evaluated our model on three benchmark datasets and the results show that our model signiﬁcantly outperforms many strong NAG baselines and performs comparably to many strong AG models .
Acknowledgments
The authors wish to thank Jialu Xu , Guanlin Li , Xing Wang for their insightful discussions and support .
Many thanks to our anonymous reviewers for their suggestions and comments .
