De - Biased Court â€™s View Generation with Causality
Yiquan Wu1 , Kun Kuang1âˆ— , Yating Zhang2âˆ— , Xiaozhong Liu3
Changlong Sun2
, Jun Xiao1 , Yueting Zhuang1 , Luo Si2 , Fei Wu1âˆ— 1Zhejiang University , Hangzhou , China 2Alibaba Group , Hangzhou , China 3Indiana University Bloomington , USA { wuyiquan , kunkuang , yzhuang}@zju.edu.cn yatingz89@gmail.com , changlong.scl@taobao.com , liu237@indiana.edu
{ junx , wufei}@cs.zju.edu.cn , luo.si@alibaba-inc.com
Abstract
Court â€™s view generation is a novel but essential task for legal AI , aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation .
While prior text - to - text natural language generation ( NLG ) approaches can be used to address this problem , neglecting the confounding bias from the data generation mechanism can limit the model performance , and the bias may pollute the learning outcomes .
In this paper , we propose a novel Attentional and Counterfactual based Natural Language Generation ( ACNLG ) method , consisting of an attentional encoder and a pair of innovative counterfactual decoders .
The attentional encoder leverages the plaintiff â€™s claim and fact description as input to learn a claim - aware encoder from which the claim - related information in fact description can be emphasized .
The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgmentdiscriminative court â€™s views ( both supportive and non - supportive views ) by incorporating with a synergistic judgment predictive model .
Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics .
1
Introduction
Owing to the prosperity of machine learning , especially the natural language processing ( NLP ) techniques , many legal assistant systems have been proposed to improve the effectiveness and efï¬ciency of a judge from different aspects , such as relevant case retrieval ( Chen et al , 2013 ) , applicable law articles recommendation ( Chen et al , 2019 ) , controversy focus mining ( Duan et al , 2019 ) , and judgment prediction ( Lin et al , 2012 ; Zhong et al , 2018 ; Hu et al , 2018 ; Jiang et al , 2018 ; Chalkidis et al ,
âˆ—Corresponding Authors .
Figure 1 : Confounding bias from the data generation mechanism .
u refers to the unobserved mechanism ( i.e. , plaintiffs sue when they have a high probability to be supported ) that causes the judgment in dataset D(J ) to be imbalanced .
D(J ) â†’ I denotes that the imbalanced data D(J ) has a causal effect on the representation of input
I ( i.e. , plaintiff â€™s claim and fact description ) , and D(J ) â†’ V denotes that D(J ) has a causal effect on the representation of court â€™s view V .
Such imbalance in D(J ) leads to the confounding bias that the representations of I and V tend to be supportive , and blind the conventional training on P ( V |I ) .
2019 )
.
Court â€™s view can be regarded as the interpretation for the sentence of a case .
Being an important portion of verdict , court â€™s view is difï¬cult to generate due to its logic reasoning in the content .
Therefore court â€™s view generation is regarded as one of the most critical functions in a legal assistant system .
Court â€™s view consists of two main parts , including the judgment and the rationales , where the judgment is a response to the plaintiff â€™s claims in civil cases or charges in criminal cases , and the rationales are summarized from the fact description to derive and explain the judgment .
Recently , Ye et al ( 2018 ) investigated the problem of court â€™s view generation for the criminal cases , but it focused on the generation of rationales in the court â€™s view based on the given criminal charge and fact description of a case .
Such an experimental scenario is not applicable to the practical situation since the rationales should be concluded before reaching the ï¬nal judgment .
Moreover , difProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing , pages763â€“780,November16â€“20,2020.c(cid:13)2020AssociationforComputationalLinguistics763ğ‘«(ğ‘±)ğ‘°ğ‘½ğ’–  Figure 2 : An example of plaintiff â€™s claim , fact description , and court â€™s view from a legal document in a civil case .
The judgment is non - support since there exists a rejection on one of the plaintiff â€™s claims in the court â€™s view .
ferent from the criminal cases , in civil cases , the judgment depends not only on the facts recognized but also on the claims that the plaintiff declared .
of the model by focusing on the supported cases while ignoring the non - supported cases , leading to incorrect judgment generation of court â€™s view .
In this paper , we focus on the problem of automatically generating the court â€™s view in civil cases by injecting the plaintiff â€™s claim and fact description , as shown in Fig .
2 . In such a context , the problem of the court â€™s view generation can be formulated as a text - to - text natural language generation ( NLG ) problem , where the input is the plaintiff â€™s claim and the fact description , and the output is the corresponding court â€™s view that contains the judgment and the rationales1 .
Although classical text generative models ( e.g. , sequence - tosequence model Sutskever et al , 2014 , attentionbased model , and pointer - generator networks See et al , 2017 ) have been applied to many text generation tasks , yet , in the task of the court â€™s view generation , such techniques can not be simply applied for the following reasons : ( 1 ) There exists â€œ no claim , no trial â€ principle in civil legal systems : The judgment in the real court â€™s view is the response to the claims declared by the plaintiff , where its rationales summarize the corresponding facts .
In other words , there exists a correspondence relationship between the input ( claims and facts ) and the generated text ( court â€™s view ) .
For example , the plaintiff â€™s claims shown in Fig .
2 mentioned the principal and the interest , respectively .
Hence , the court â€™s view of this case would and might only focus on the facts about the principal and the interest .
( 2 ) The imbalance of judgment in civil cases : The distribution of judgment results of civil cases is very imbalanced .
For example , over 76 % of cases were supported in private lending , which is the most frequent category in civil cases .
Such an imbalance of judgment would blind the training
1Since the claims are various , for simpliï¬cation , the judgment of a civil case is deï¬ned as supported if all its claims are accepted , otherwise , deï¬ned as non - supported .
From the perspective of causality ( Pearl , 2009 ; Kuang et al , 2020 ) , the imbalance of judgment reveals the confounding bias induced by the data generation mechanism that plaintiffs sue when they have a high probability to be supported .
Such imbalanced data would cause the learned representation of both inputs ( claims and recognized facts ) and output ( court â€™s view ) to be supported , leading to confounding bias between inputs and output , and blinding the training process of conventional NLG models as we demonstrated in Fig .
1 .
To address these challenges , we propose an Attentional and Counterfactual based Natural Language Generation ( AC - NLG ) method by jointly optimizing a claim - aware encoder , a pair of counterfactual decoders to generate judgmentdiscriminative court â€™s views ( both supportive and non - supportive views ) and a synergistic judgment predictive model .
Speciï¬cally , the claim - aware encoder is designed to represent the fact description which emphasizes on the declared claims .
The counterfactual decoder is inspired by the backdoor adjustment in causal inference ( Pearl et al , 2016 ; Kuang et al , 2020 ) to address the confounding bias and the imbalance problem in judgment .
To determine the judgment result of each case , a judgment predictive model is jointly learned with the two decoders and decides which output to be selected as the ï¬nal generated court â€™s view .
We validate the effectiveness of our AC - NLG method with extensive experiments on real legal documents .
Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics .
Since legal AI is a sensitive ï¬eld , we make ethical discussion in the penultimate section(Sec . 6 ) .
The main contributions of this paper can be sum764PLAINTIFFâ€™SCLAIMTheplaintiffAclaimedthatthedefendantBshouldreturntheloanof$29,500PrincipleClaimandthecorrespondinginterestInterestClaim .
FACTDESCRIPTIONAfterthehearing , thecourtheldthefactsasfollows : ThedefendantBborrowed$29,500fromtheplaintiffA , andagreedtoreturnafteronemonth .
Aftertheloanexpired , thedefendantfailedtoreturnFact . COURTâ€™SVIEWThecourtconcludedthattheloanrelationshipbetweentheplaintiffAandthedefendantBisvalid .
ThedefendantfailedtoreturnthemoneyontimeRationale .
Therefore , theplaintiffâ€™sclaimonprinciplewassupportedAcceptanceaccordingtolaw . Thecourtdidnotsupporttheplaintiffâ€™sclaimoninterestRejectionbecausetheevidencewasinsufficientRationale .  marized as follows : â€¢ We investigate the problem of de - biased court â€™s view generation in civil cases from a causal perspective , considering the issue of confounding bias from judgment imbalance .
â€¢ We propose a novel AC - NLG model to jointly optimize a claim - aware encoder and a pair of counterfactual decoders for generating a judgment - discriminative court â€™s view by incorporating with a judgment predictive model .
â€¢ We construct a dataset based on raw civil legal documents , where each case is objectively split into three parts : plaintiff â€™s claim , fact description , and court â€™s view with human annotation on the judgment .
To motivate other scholars to investigate this novel but important problem , we make the experiment dataset publicly available2 .
â€¢ We validate the superior performance of the proposed method with extensive experiments .
Our method can be applied to other natural language generation tasks with confounding bias or data imbalance .
2 Related Work
2.1 Legal Assistant
In recent years , many researchers from both law and computer science ï¬elds have been exploring the potential and methods to perform judicial decisions and auxiliary tasks , aiming at helping lawyers and lower court judges .
In recent work , judicial intelligence is also applied to various tasks of natural language processing .
Since most of the legal documents appear in textual form , many NLP technologies have been brought into the legal ï¬eld to improve the efï¬ciency of legal work .
Charge prediction is a common task of judgment prediction , considered as a text classiï¬cation problem ( Lin et al , 2012 ; Zhong et al , 2018 ; Hu et al , 2018 ; Jiang et al , 2018 ; Chalkidis et al , 2019 ) .
Besides , there are also works on legal questions classiï¬cation ( Xiao et al , 2017 ) , law articles recommendation ( Chen et al , 2019 ) , controversy focus mining ( Duan et al , 2019 ) and relevant case retrieval ( Chen et al , 2013 ) .
Ye et al ( 2018 ) explored the court â€™s view generation in criminal cases , where the input is only fact description , and the court â€™s view generation is conditioned on the known judgment results , which is not applicable in real cases .
2https://github.com/wuyiquan/AC-NLG
2.2 Natural Language Generation
Our task aims at generating the court â€™s view based on the plaintiff â€™s claim and the fact description , which can be regarded as a NLG task .
NLG has been widely studied and applied to many tasks , such as machine translation ( Wu et al , 2016 ) , question answering ( McCann et al , 2018 ; Bagchi and Wynter , 2013 ) and text summarization ( Rush et al , 2015 ) .
The recent success of sequence - to - sequence models ( Sutskever et al , 2014 ) , in which recurrent neural networks ( RNNs ) reading and generating text simultaneously , has made the generation task feasible .
Bahdanau et al ( 2014 ) ï¬rstly applied the attention mechanism into the NLG task .
See et al ( 2017 ) proposed a Pointer - Generator Networks ( PGN ) , which can effectively solve the OutOf - Vocabulary ( OOV ) problem .
Although the previous work on NLG can produce ï¬‚uent sentences , they are struggling to be directly applied to our task since a good court â€™s view considers not only the text ï¬‚uency but also the logical correctness .
2.3 Causal Inference
Causal Inference ( Pearl , 2009 ; Kuang et al , 2020 ) is a powerful statistical modeling tool for explanatory analysis by removing confounding bias in data .
That bias might bring a spurious correlation or confounding effect among variables .
Recently , many methods have been proposed to remove confounding bias in the literature of causal inference , including do - operation based on structure causal model ( Pearl , 2009 ) and counterfactual outcome prediction based on potential outcome framework ( Imbens and Rubin , 2015 ) .
With dooperation , the backdoor adjustment ( Pearl et al , 2016 ) have been proposed for data de - bias .
In this paper , we sketch the causal structure model of our problem , as shown in Fig .
1 , and adopt backdoor for confounding bias reduction .
3 Problem Formulation
In this work , we focus on the problem of the court â€™s view generation in civil cases , where the input is the plaintiff â€™s claim and the fact description , and the output is the corresponding court â€™s view .
We formulate our problem with the deï¬nition of the plaintiff â€™s claim , the fact description , and the court â€™s view , as shown in Fig .
2 .
Plaintiff â€™s claim ( C ) is a descriptive sentence that depicts the claims from the plaintiff .
In a civil case , it often appears multiple claims from the
765  Figure 3 : The architecture of AC - NLG , which consists of a claim - aware encoder , a pair of counterfactual decoders , and a judgment predictor .
plaintiff .
For example , the plaintiff â€™s claim demonstrated in Fig .
2 contains the principal claim and the interest claim .
Here , we denote the plaintiff â€™s claim in a case as a sentence form c = { wc t=1 , where wc t represents one word , and m is the number of words in plaintiff â€™s claim .
t } m
Fact description ( F ) is also a descriptive sentence , which describes the identiï¬ed facts ( relevant events that have happened ) in a case , as Fig .
2 shows .
Here , we denote the fact description in a case as f = { wf
t=1 , where n is the length .
t } n
Court â€™s view ( V ) contains two main components , judgment and rationales , where the judgment is to respond the plaintiff â€™s claims , and the rationales are the claim - related summarization on the fact description to determine and interpret the judgment .
Here , we denote the court â€™s view as v = { wv t=1 , where l is the length .
Moreover , we use a variable j to denote the judgment in the court â€™s view .
For simplicity , we set j = 1 to denote supported judgment ( all the claims are judged to be accepted ) , and j = 0 to denote non - supported judgment .
t } l
Then , the problem of court â€™s view generation
can be denoted as follow :
Problem 1 ( Court â€™s View Generation ) .
Given the plaintiff â€™s claim c = { wc t=1 and the fact description f = { wf t=1 , our task is to generate the court â€™s view v = { wv
t } m
t } n
t } l
t=1 .
4 Method
In this section , we ï¬rst introduce the effect of mechanism confounding bias on the court â€™s view generation and propose a backdoor - inspired method to eliminate that bias .
Then , we describe our Attentional and Counterfactual based Natural Language
Generation ( AC - NLG ) model in detail .
Fig .
3 shows the overall framework .
4.1 Backdoor Adjustment
As shown in Fig .
1 , the confounding bias from the data generation mechanism would blind the conventional training on P ( V |I ) , and current sequenceto - sequence models struggle to solve this problem .
Here , we see through why these models fail mathematically .
For a certain case , given the input I = ( c , f ) , using Bayes rule , we would train the model to generate the court â€™s view V as follow :
P ( V |I ) =
P ( V |I , j)P ( j|I )
( 1 )
( cid:88 )
j
If the supported cases dominate our training data , e.g. , P ( j = 1|I ) â‰ˆ 1 .
Thus , P ( V |I ) degrades to P ( V |I , j = 1 ) , which would ignore the representation of non - supported cases , leading to the learned representations of inputs I and output V tend to be supported .
Thus , the model tends to build a strong connection between inputs and the supported court â€™s view , even for the cases that are non - supported .
In this way , the representation of input I is contaminated by the confounding bias from I â† D(J ) â†’ V .
Backdoor adjustment is a main de - confounding technique in causal inference ( Pearl et al , 2016 ; Pearl , 2009 ) .
De - confounding seeks the exact causal effect of one variable on another , which appeals for our court â€™s view generation task since the court â€™s view should be faithful only to the content of the plaintiff â€™s claims and fact descriptions .
The backdoor adjustment makes a do - operation on I , which promotes the posterior probability from passive observation to active intervention .
766Claim - aware Encoderğ‘—Attention LayerAttention LayerCounterfactual Decoderğ¯ğ§(j=0)ğ¯ğ¬(j=1)JudgmentPredictor = addc = claimsf = factsğ¡ğ±=hidden states of xğ¯ğ¬ 	 = supported viewğ¯ğ§=non - supported viewğ¬ğ¯ğ±=decode states of ğ¯ğ± 	 j = judgmentFC LayerPointer GeneratorPointer Generatorğ‘¤#ğ’—ğ’ğ‘¤%ğ’—ğ’ğ‘¤&ğ’—ğ’ğ‘¤#ğ’—ğ’”ğ‘¤%ğ’—ğ’”ğ‘¤&ğ’—ğ’”</s>ğ‘¤%ğ’—ğ’Sigmoid</s>ğ‘¤&ğ’—ğ’” â€¦ â€¦ ğ¬ğ¯ğ§ğ¬ğ¯ğ¬s%ğ¯ğ¬s#ğ¯ğ§ğ‘¤#(ğ‘¤%(ğ‘¤&(ğ‘¤)(ğ‘¤*(Attention Layerğ‘¤#+ğ‘¤%+ğ‘¤&+ğ‘¤,+ğŸğœğ¡ğŸğ¡ğœâ„% ( â€¦â€¦  The backdoor adjustment addresses the confounding bias by computing the interventional posterior P ( V |do(I ) ) and controlling the confounder as : ( cid:88 )
P ( V |do(I ) )
=
P ( V |I , j)P ( j )
j
In our problem , the variable j is a binary variable ( support or non - support ) , hence ,
P ( V |do(I ) )
= P ( V |I , j = 0)P
( j = 0 )
+ P ( V |I , j = 1)P ( j = 1 )
( 2 )
( 3 )
The main difference between traditional posterior in Eq . 1 and interventional posterior in Eq . 2 is that P ( j|I ) is changed to P ( j ) .
Since the backdooor adjustment help to cut the dependence between D(J ) and I , we can eliminate the confounding bias from data generation mechanism and learn a interventional model for de - biased court â€™s view generation .
4.2 Backdoor In Implementation
As shown in Fig .
3 , to optimize Eq .
3 , we use a pair of counterfactual decoders to learn the likelihood P ( V |I , j ) for each j. At inference , we propose to use a predictor to approximate P ( j ) .
Note that our implementation on backdoor - adjustment can be easily applied for multi - valued confounding with multiple counterfactual decoders .
4.3 Model Architecture
Our model is conducted in a multi - task learning manner which consists of a shared encoder , a predictor , and a pair of counterfactual decoders .
The predictor and the decoders take the output of the encoder as input .
Our model looks like SHAPED(Zhang et al , 2018 ) ( several decoders with a classiï¬er ) , but the motivations and mechanisms behind the model are different .
Claim - aware Encoder Intuitively , the plaintiff â€™s claim c and the fact description f are sequences of words .
Therefore , the encoder ï¬rstly transforms the words to embeddings .
Then the embedding sequences are fed to the Bi - LSTM , producing two sequences of hidden states hc , hf corresponding to the plaintiff â€™s claim and the fact description respectively .
After that , we use a claim - aware attention mechanism to fuse hc and hf .
For each hidden state hf k is its attention weight on hc i in hf , ei k , and the attention distribution qi is calculated as follow :
k = vT tanh(Wchc ei
k + Wf hf
i + battn )
( 4 )
qi = sof tmax(ei )
( 5 )
where v , Wc , Wf , battn are learnable parameters .
The attention distribution can be regarded as the importance of each word in the plaintiff â€™s claim for a word in fact description .
Next , the new representation of fact description is produced as follows :
i = hf hf âˆ—
i +
( cid:88 )
khc qi k
k
( 6 )
After feeding to another Bi - LSTM layer , we get
the claim - aware representation of fact h.
Judgment Predictor Given the claim - aware representation of fact h , the judgment predictor produces the probability of support Psup through a fully connected layer and a sigmoid operation .
The prediction result j is obtained as follow :
( cid:40 )
j =
1 Psup > 0.5 0
Psup < = 0.5
( 7 )
where 1 means support , and 0 means non - support .
Counterfactual Decoder To eliminate the effect of data bias , here we use a pair of counterfactual decoders , which contains two decoders , one is for supported cases , and the other is for non - supported cases .
The two decoders have the same structure but aim to generate the court â€™s view with different judgments .
We name them as counterfactual decoders because every time there is only one of the two generated court â€™s views correct .
Still , we apply the attention - mechanism .
At each step t , given the encoder â€™s output h , and the decode state st , the attention distribution at is calculated the same way as qi in Eq . 5 , but with different parameters .
The context vector hâˆ—
t is then a weighted sum of h :
hâˆ— t =
( cid:88 )
at ihi
i
( 8)
The context vector hâˆ— t , which can be regarded as a representation of the input for this step , is concatenated with the decode state st and fed to linear layers to produce the vocabulary distribution pvocab :
pvocab = sof tmax(V ( cid:48)(V [ st , hâˆ—
t ] )
+ b ) + b(cid:48 ) )
( 9 )
where V , V ( cid:48 ) , b , b(cid:48 ) are all learnable parameters .
Then we add a generation probability ( See et al , 2017 ) to solve the OOV problem .
Given the context
767  hâˆ— t , the decode state st and the decoder â€™s input ( the word embedding of the previous word ) xt , the generation probability pgen can be calculated :
Pgen = Ïƒ(wT
hâˆ—hâˆ—
t + wT
s st + wT
x xt + bptr ) ( 10 )
where whâˆ— , ws , wx and bptr are learnable , and Ïƒ is the sigmoid function .
The ï¬nal probability for a word w in time step is obtained :
Table 1 : Statistics of private lending dataset
Type
Result
51087(76 % ) # Supported case 15817(24 % )
#
Non - supported case 77.9 Avg . # tokens in claim Avg . # tokens in fact 158.0 Avg . # tokens in court â€™s view 194.4
P ( w ) = Pgen âˆ— pvocab(w ) + ( 1 âˆ’ Pgen )
5 Experiments
( cid:88 )
at i
i : wi = w
( 11 ) We introduce how to alienate the two decoders
5.1 Data Construction
in the training part .
Training For predictor , we use cross - entropy as the loss function :
Lpred = âˆ’Ë†jlog(Psup ) âˆ’ ( 1 âˆ’ Ë†j)log(1 âˆ’ Psup ) ( 12 )
where Ë†j is the real judgment .
For decoders , the previous word in training is the word in real court â€™s view , and the loss for timestep t is the negative log - likelihood of the target word wâˆ— t :
Lt = âˆ’logP ( wâˆ— t )
and the overall generation loss is :
( 13 )
( 14 )
Lgen =
1 T
T ( cid:88 )
t=0
Lt
where T is the length of real court â€™s view .
Since we aim to make the two decoders generate two different court â€™s views , we take a mask operation when calculating the loss of each decoder .
The exact loss for the support decoder is :
Lsup =
( cid:40 )
Lgen 0
Ë†j = 1 Ë†j = 0
( 15 )
the loss for the non - support decoder Lnsup is obtained by the opposite way .
Thus , the total loss is :
Ltotal = Lsup + Lnsup + Î»Lpred
( 16 )
where we set Î» to 0.1 in our model .
Inference In inference , the counterfactual decoders apply beam search to generate two court â€™s views , and one of them will be selected as the ï¬nal output , depending on the result of the predictor j.
Since there is no publicly available court â€™s view generation dataset in civil cases , we build a dataset based on raw civil legal documents3 .
Speciï¬cally , we choose private lending , which is the most frequent category in civil cases , to construct the dataset .
We process the legal documents as following steps : 1 ) Split legal documents into three parts : plaintiff â€™s claim , facts description , and court â€™s view , which can be objectively split by keywords ( subtitles ) .
2 ) Human annotation .
We employ experts with legal backgrounds to annotate the judgment ( deï¬ned in Sec . 3 ) on the court â€™s view .
3 ) Annotation veriï¬cation .
We use random sampling test to ensure that the annotation accuracy is over 95 % .
After that , we get the dataset as shown in Tab .
1 .
We randomly separate the dataset into a training set , a validation set , and a test set according to a ratio of 8 : 1 : 1 , the ratio of supported cases is about 76 % in each set .
5.2 Baselines
We implement the following baselines for comparison :
â€¢ S2S Sequence - to - sequence model ( Sutskever et al , 2014 ) is a classic model for NLG task .
We concatenate the plaintiff claims and facts descriptions as input .
â€¢ PGN Pointer Generator Networks ( See et al , 2017 ) utilizes a pointer network to solve the outof - vocabulary ( OOV ) problem , which is essential for the court â€™s view generation since many nouns occur there .
Oversampling is a common method to alleviate data imbalance .
We oversample the non - supported cases so that the ratio between supported cases and non - supported cases become 1 : 1 .
â€¢ S2SwS
Apply oversampling to S2S.
3https://wenshu.court.gov.cn/
768  Table 2 : Results on court â€™s view generation .
Method
S2S S2SwS PGN PGNwS
R-1 54.0 51.5 53.3 53.2 AC - NLGw / oBA 54.1 AC - NLGw / oCA 53.7 53.7 55.1
AC - NLGwS AC - NLG
ROUGE R-2 35.7 32.0 37.1 36.0 38.1 36.7 36.4 38.6
R - L 48.3 45.0 48.8 48.0 49.9 49.1 48.5 50.8
BLEU B-2 57.6 55.6 56.1 56.7 55.9 56.0 56.5 57.1
B-1 65.0 63.3 62.0 63.1 61.8 62.1 62.8 63.2
B - N 50.5 47.9 50.0 50.2 49.9 49.7 50.0 51.0
BERT SCORE f1 r p 89.6 89.5 89.6 86.2 88.8 83.8 92.6 91.2 94.0 94.8 94.0 95.7 92.8 91.9 93.6 93.5 92.6 94.5 93.0 92.1 94.0 95.5 94.6 96.5
Table 3 : Results on judgment prediction .
Prediction Acc .
Method
p 72.1 92.0 86.0 AC - NLG 93.4
w / oD w / oCA wS
Support r 81.0 97.2 94.3 95.9
f1 76.3 94.5 90.0 94.6
Non - support r 44.3 66.0 38.6 72.9
f1 49.8 74.5 47.8 76.9
p 56.9 85.6 62.8 81.5
Table 4 : Results of human evaluation .
Method
PGN AC - NLG
Judgment Support Non
-
support
3.34 3.52
1.78 3.24
Rational
3.11 3.25
Flu .
3.41 3.50
â€¢ PGNwS Apply oversampling to PGN .
â€¢
AC - NLGwS
Apply oversampling to ACNLG .
We do ablation experiments as follows : â€¢ AC - NLGw / oD
We remove the decoder and train the remaining model ( encoder and predictor ) as a classiï¬cation task for judgment prediction .
â€¢ AC - NLGw / oBA
We remove the backdoor adjustment by replacing the pair of counterfactual decoders and predictor with a single decoder , but keep the claim - aware attention mechanism .
â€¢ AC - NLGw / oCA We remove the claim - aware attention , and concatenate the claims and the facts instead .
5.3 Metrics ROUGE4 is a set of metrics used in the NLP task .
We keep the results of ROUGE-1 , ROUGE-2 , and ROUGE - L. ROUGE-1 and ROUGE-2 refer to the overlap of unigram and bigram between the generated and reference documents , respectively .
ROUGE - L is a Longest Common Subsequence ( LCS ) based statistics .
BLEU5 ( Papineni et al , 2002 ) is a method of au4https://pypi.org/project/rouge/ 5http://www.nltk.org/api/nltk.test .
unit.translate.html
tomatic text - generation evaluation that highly correlates with human evaluation .
We use BLEU-1 , BLEU-2 to evaluate from the perspectives of unigram , bigram .
BLEU - N is an average of BLEU-1 , BLEU2 , BLEU-3 , BLEU-4 .
BERT SCORE6 ( Zhang et al , 2019 ) computes a similarity score by using contextual embedding of the tokens .
We calculate the precision ( p ) , recall ( r ) and f1 - score to evaluate the information matching degree .
Accuracy of judgment prediction To evaluate the performance of the predictor , we calculate the precision ( p ) , recall ( r ) and , f1 - score of supported and non - supported cases , respectively .
Human Evaluation We conduct a human evaluation to better analyze the quality of the generated court â€™s view .
First , we randomly sample 500 test cases , where the ratio of the supported and nonsupported cases are 1:1 .
For each case , we present the generated court â€™s views from each method7 with the ground truth to 5 human annotators with legal backgrounds .
The evaluation is conducted following three perspectives : ( 1 ) Judgment level .
Annotators are asked to give a score ( 1 - 5 ) on the judgment in the generated court â€™s view .
1 for totally wrong and 5 for totally correct .
( 2 ) Rational level .
Annotators are asked to give a score ( 1 - 5 ) on the rationals in the generated court â€™s view .
1 for the worst and 5 for the best .
( 3 ) Fluency level .
Annotators are asked to give a score ( 1 - 5 ) on the ï¬‚uency of the generated court â€™s view .
1 for the worst and 5 for the best .
5.4 Experimental Results
Tab . 2 demonstrates the results of court â€™s view generation with ROUGE , BLEU , and BERT SCORE .
Also , we report the results on the judgment prediction of our predictor component with precision
6https://github.com/Tiiiger/bert_score 7We shufï¬‚e all the results to be fair for all the methods
769  Figure 4 : Case study .
( p ) , recall ( r ) , and f1 - score ( f1 ) in Tab .
3 .
To demonstrate that our method is de - biased on judgment generation , we report the result of human evaluation in Tab .
4 .
Results of court â€™s view generation : From Tab .
2 , we can conclude that : ( 1 ) S2S tends to repeat words , which makes it get high BLEU but low BERT SCORE .
( 2 ) Oversampling strategy does nâ€™t beneï¬t the models , hence , it can not address the confounding bias .
( 3 ) With claim - aware encoder and backdoor - inspired counterfactual decoders , our AC - NLG achieves better performance on court â€™s view generation compared with baselines .
( 4 ) The performance gap between AC - NLGw / oCA and AC - NLG demonstrates the effectiveness of our proposed claim - aware encoder , and the gap between AC - NLGw / oBA and AC - NLG illustrates the superiority of our counterfactual decoders .
Results of judgment prediction : From Tab . 3 , we have the following observations : ( 1 ) The counterfactual decoders in our model can signiï¬cantly eliminate the confounding bias , hence , achieve remarkable improvement on the non - supported cases , for example boosting f 1 from 49.8 % to 76.9 % .
( 2 ) The proposed claim - aware encoder has a limited effect on judgment prediction since it â€™s designed for improving the quality of generation as shown in Tab .
2 . ( 3 ) Still , oversampling brings no improvement to the model .
Results of human evaluation : From Tab . 4 , we have the following observations : ( 1 ) due to the confounding bias in data , the performance of judgment generation in PGN is poor for non - supported cases , and its performance gap between supported and non - supported cases is huge ( 1.56 ) .
( 2 ) By
debiasing with backdoor - inspired counterfactual decoders , our AC - NLG signiï¬cantly improves the performance of judgment generation , especially for non - supported cases , and achieves a smaller performance gap ( only 0.28 ) between the supported and non - supported cases .
( 3 ) With a claim - aware encoder , our AC - NLG also achieves better performance on the generation of rational and generated court â€™s view ï¬‚uency .
( 4 ) Kappa coefï¬cient Îº is more than 0.8 between any two judges , which proves the validation of human evaluation .
Overall , thanks to the proposed claim - aware encoder , counterfactual decoders , and a synergistic judgment predictor , our model achieves better performance than single - task baselines on the task of judgment prediction , judgment generation in court â€™s view and court â€™s view generation .
5.5 Experiment Details We use Gensim ( Ë‡RehËšuË‡rek and Sojka , 2010 ) with a large - scale generic corpus to train a language model as the pre - trained model , then use it to initialize the word embeddings , which is in the dimension of 300.8
5.6 Case Study
Figure 4 shows three court â€™s views for a certain case : the court â€™s view generated by PGN , by the proposed AC - NLG method , and the real court â€™s view .
We ï¬nd that the one generated by PGN accepts the claim for principal , but ignores other claims such as issue related to guarantee .
Compared with the real court â€™s view , our model accu8Source code , data , more experiment details and results
can be found in supplementary materials .
770PLAINTIFFâ€™SCLAIMThedefendantBreturntheloanof$495,000.ThedefendantCreturntheloantogether .
FACTDESCRIPTIONAfterthehearing , thecourtheldthefactsasfollows : FromNovember20,2010toMarch23,2011,thedefendantBsuccessivelyborrowedatotalof$495,000fromtheplaintiffAandissuedfourseparateborrowings . ThedefendantBhasnotrepaidtheaboveloan .
PGNThecourtconcludedthat : TheprivatelendingrelationshipbetweenPlaintiffAandDefendantB , wherethesubjectwasappropriate , thecontentwaslegitimate , andthemeaningwastrue , shouldbeconfirmedtobelegalandvalid .
Thetwosidesdidnotagreedinwritingonaloanperiod , sothedefendantshouldreturntheplaintiff'sloaninatimelymannerwithinareasonableperiodaftertheplaintiffurged . Thedefendantâ€™sfailuretoreturntheloantimelyconstitutedabreachofcontractandshouldbearcorrespondingcivilliabilities .
Therefore , the plaintiff 's claim was reasonable and legal , and the court supported it Acceptance .
AC - NLGThecourtconcludedthatthesubjectoftheprivatelendingrelationshipbetweenPlaintiffAandDefendantBwasqualified , thecontentwaslegal , andthemeaningwastrue .
Itshouldbedeemedvalid .
Thetwosidesdidnotagreedinwritingonaloanperiod , thedefendantshallreturntheloanwithinareasonableperiodaftertheplaintiffurged .
The plaintiff â€™s claim requesting the defendant to return the loan of $ 495,000 was in compliance with the law and the court supported it Acceptance .
However , the court did not support the claim requesting the defendant C to bear the guarantee liability because   the evidence was insufficient Rejection .
REALThecourtconcludedthat : ThesubjectoftheprivatelendingrelationshipbetweenPlaintiffAandDefendantBwasqualified , thecontentwaslegal , andthemeaningwastrue .
Itshouldbedeemedvalid .
Defendantshouldrepaytheplaintiff'sloanwithinareasonableperiodaftertheplaintiffurged .
Therefore , Defendant B should bear the civil liability of returning the plaintiff 's loan of $ 495,000 and paying overdue interest Acceptance .
The court did not support the plaintiff â€™s claim requesting the defendant C to return the loan together because the evidence was insufficientRejection .
DefendantBfailedtoappearincourtafterbeinglegallysummonedbythecourt .  rately responds to both claims and produces the correct judgment .
6 Ethical Discussion
While AI is gaining adoption in legal justice(Lin et al , 2012 ; Zhong et al , 2018 ; Hu et al , 2018 ; Jiang et al , 2018 ; Chalkidis et al , 2019 ) , any subtle statistical miscalculation may trigger serious consequences .
From a fairness perspective , prior studies suggested that global ( statistical ) optimization ( cid:54)= individual ( demographic ) fairness ( Zemel et al , 2013 ) , and this ethical concern should be further investigated .
In this section , we explore the following ethical issues .
Target User :
According to the report of statistics , a typical active trial judge closed around 250 cases in a year .
Trial judges suffering from â€˜ daunting workload â€™ is becoming an critical issue(Duan et al , 2019 ) .
The proposed algorithm is designed for generating the court â€™s view draft for assisting the trial judges for decision making .
This work is an algorithmic investigation , but such algorithm should never â€˜ replace â€™ human judges .
Human knowledge / judgment should be the ï¬nal safeguard to protect social justice and individual fairness .
Potential Error : The potential error would be as follows : a ) generating a wrong judgment and b ) generating a wrong rationale .
The goal of our algorithm is to generate a draft of court â€™s view for trail judge as a reference , and judges need to proofread the content generated from algorithm .
Demographic Bias : In this paper , we focus on addressing the bias problem from the data generation by treating the variable of data generation as confounder in back - door adjustment .
The model adoption can face potential demographic bias / unfairness challenges , such as gender and race bias in the training data .
To further ensure the model fairness , in the future , algorithm adoption should be empowered with de - biased legal content pretraining , which could avoid potential demographic bias .
For instance , in order to remove gender / race bias , system could use ( Bolukbasi et al , 2016 ) algorithm to debias the sensitive gender / race information , e.g. , replace â€˜ he / she â€™ and â€˜ asian / hispanic â€™ with gender / race neutral words for pretraining , which can be vital for legal domain .
7 Conclusion and Future Work
In this paper , we propose a novel Attentional and Counterfactual based Natural Language Generation ( AC - NLG ) method to solve the task of court â€™s view generation in civil cases and ensure the fairness of the judgment .
We design a claim - aware encoder to represent the fact description which emphasizes on the plaintiff â€™s claim , as well as a pair of backdoor - inspired counterfactual decoders to generate judgment - discriminative court â€™s views ( both supportive and non - supportive views ) and to eliminate the bias that arose from the data generation mechanism by connecting with a synergistic judgment predictive model .
The experimental results show the effectiveness of our method .
Based on the AC - NLG method , in the future , we can explore the following directions : ( 1 ) Improve the accuracy of judgment on a claim - level .
( 2 ) Add external knowledge ( e.g. a logic graph ) to the predictor for the interpretability of the model .
Acknowledgments
This work was supported by National Natural Science Foundation of China ( No . 62006207 , 61625107 ) , National Key R&D Program of China ( No . 2018AAA0101900 , 2018YFC0830200 , 2018YFC0830206 , 2020YFC0832500 ) , the Fundamental Research Funds for the Central Universities .
Finally , we would like to thank the anonymous reviewers for their helpful feedback and suggestions .
