De - Biased Court ‚Äôs View Generation with Causality
Yiquan Wu1 , Kun Kuang1‚àó , Yating Zhang2‚àó , Xiaozhong Liu3
Changlong Sun2
, Jun Xiao1 , Yueting Zhuang1 , Luo Si2 , Fei Wu1‚àó 1Zhejiang University , Hangzhou , China 2Alibaba Group , Hangzhou , China 3Indiana University Bloomington , USA { wuyiquan , kunkuang , yzhuang}@zju.edu.cn yatingz89@gmail.com , changlong.scl@taobao.com , liu237@indiana.edu
{ junx , wufei}@cs.zju.edu.cn , luo.si@alibaba-inc.com
Abstract
Court ‚Äôs view generation is a novel but essential task for legal AI , aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation .
While prior text - to - text natural language generation ( NLG ) approaches can be used to address this problem , neglecting the confounding bias from the data generation mechanism can limit the model performance , and the bias may pollute the learning outcomes .
In this paper , we propose a novel Attentional and Counterfactual based Natural Language Generation ( ACNLG ) method , consisting of an attentional encoder and a pair of innovative counterfactual decoders .
The attentional encoder leverages the plaintiff ‚Äôs claim and fact description as input to learn a claim - aware encoder from which the claim - related information in fact description can be emphasized .
The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgmentdiscriminative court ‚Äôs views ( both supportive and non - supportive views ) by incorporating with a synergistic judgment predictive model .
Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics .
1
Introduction
Owing to the prosperity of machine learning , especially the natural language processing ( NLP ) techniques , many legal assistant systems have been proposed to improve the effectiveness and efÔ¨Åciency of a judge from different aspects , such as relevant case retrieval ( Chen et al , 2013 ) , applicable law articles recommendation ( Chen et al , 2019 ) , controversy focus mining ( Duan et al , 2019 ) , and judgment prediction ( Lin et al , 2012 ; Zhong et al , 2018 ; Hu et al , 2018 ; Jiang et al , 2018 ; Chalkidis et al ,
‚àóCorresponding Authors .
Figure 1 : Confounding bias from the data generation mechanism .
u refers to the unobserved mechanism ( i.e. , plaintiffs sue when they have a high probability to be supported ) that causes the judgment in dataset D(J ) to be imbalanced .
D(J ) ‚Üí I denotes that the imbalanced data D(J ) has a causal effect on the representation of input
I ( i.e. , plaintiff ‚Äôs claim and fact description ) , and D(J ) ‚Üí V denotes that D(J ) has a causal effect on the representation of court ‚Äôs view V .
Such imbalance in D(J ) leads to the confounding bias that the representations of I and V tend to be supportive , and blind the conventional training on P ( V |I ) .
2019 )
.
Court ‚Äôs view can be regarded as the interpretation for the sentence of a case .
Being an important portion of verdict , court ‚Äôs view is difÔ¨Åcult to generate due to its logic reasoning in the content .
Therefore court ‚Äôs view generation is regarded as one of the most critical functions in a legal assistant system .
Court ‚Äôs view consists of two main parts , including the judgment and the rationales , where the judgment is a response to the plaintiff ‚Äôs claims in civil cases or charges in criminal cases , and the rationales are summarized from the fact description to derive and explain the judgment .
Recently , Ye et al ( 2018 ) investigated the problem of court ‚Äôs view generation for the criminal cases , but it focused on the generation of rationales in the court ‚Äôs view based on the given criminal charge and fact description of a case .
Such an experimental scenario is not applicable to the practical situation since the rationales should be concluded before reaching the Ô¨Ånal judgment .
Moreover , difProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing , pages763‚Äì780,November16‚Äì20,2020.c(cid:13)2020AssociationforComputationalLinguistics763ùë´(ùë±)ùë∞ùëΩùíñ  Figure 2 : An example of plaintiff ‚Äôs claim , fact description , and court ‚Äôs view from a legal document in a civil case .
The judgment is non - support since there exists a rejection on one of the plaintiff ‚Äôs claims in the court ‚Äôs view .
ferent from the criminal cases , in civil cases , the judgment depends not only on the facts recognized but also on the claims that the plaintiff declared .
of the model by focusing on the supported cases while ignoring the non - supported cases , leading to incorrect judgment generation of court ‚Äôs view .
In this paper , we focus on the problem of automatically generating the court ‚Äôs view in civil cases by injecting the plaintiff ‚Äôs claim and fact description , as shown in Fig .
2 . In such a context , the problem of the court ‚Äôs view generation can be formulated as a text - to - text natural language generation ( NLG ) problem , where the input is the plaintiff ‚Äôs claim and the fact description , and the output is the corresponding court ‚Äôs view that contains the judgment and the rationales1 .
Although classical text generative models ( e.g. , sequence - tosequence model Sutskever et al , 2014 , attentionbased model , and pointer - generator networks See et al , 2017 ) have been applied to many text generation tasks , yet , in the task of the court ‚Äôs view generation , such techniques can not be simply applied for the following reasons : ( 1 ) There exists ‚Äú no claim , no trial ‚Äù principle in civil legal systems : The judgment in the real court ‚Äôs view is the response to the claims declared by the plaintiff , where its rationales summarize the corresponding facts .
In other words , there exists a correspondence relationship between the input ( claims and facts ) and the generated text ( court ‚Äôs view ) .
For example , the plaintiff ‚Äôs claims shown in Fig .
2 mentioned the principal and the interest , respectively .
Hence , the court ‚Äôs view of this case would and might only focus on the facts about the principal and the interest .
( 2 ) The imbalance of judgment in civil cases : The distribution of judgment results of civil cases is very imbalanced .
For example , over 76 % of cases were supported in private lending , which is the most frequent category in civil cases .
Such an imbalance of judgment would blind the training
1Since the claims are various , for simpliÔ¨Åcation , the judgment of a civil case is deÔ¨Åned as supported if all its claims are accepted , otherwise , deÔ¨Åned as non - supported .
From the perspective of causality ( Pearl , 2009 ; Kuang et al , 2020 ) , the imbalance of judgment reveals the confounding bias induced by the data generation mechanism that plaintiffs sue when they have a high probability to be supported .
Such imbalanced data would cause the learned representation of both inputs ( claims and recognized facts ) and output ( court ‚Äôs view ) to be supported , leading to confounding bias between inputs and output , and blinding the training process of conventional NLG models as we demonstrated in Fig .
1 .
To address these challenges , we propose an Attentional and Counterfactual based Natural Language Generation ( AC - NLG ) method by jointly optimizing a claim - aware encoder , a pair of counterfactual decoders to generate judgmentdiscriminative court ‚Äôs views ( both supportive and non - supportive views ) and a synergistic judgment predictive model .
SpeciÔ¨Åcally , the claim - aware encoder is designed to represent the fact description which emphasizes on the declared claims .
The counterfactual decoder is inspired by the backdoor adjustment in causal inference ( Pearl et al , 2016 ; Kuang et al , 2020 ) to address the confounding bias and the imbalance problem in judgment .
To determine the judgment result of each case , a judgment predictive model is jointly learned with the two decoders and decides which output to be selected as the Ô¨Ånal generated court ‚Äôs view .
We validate the effectiveness of our AC - NLG method with extensive experiments on real legal documents .
Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics .
Since legal AI is a sensitive Ô¨Åeld , we make ethical discussion in the penultimate section(Sec . 6 ) .
The main contributions of this paper can be sum764PLAINTIFF‚ÄôSCLAIMTheplaintiffAclaimedthatthedefendantBshouldreturntheloanof$29,500PrincipleClaimandthecorrespondinginterestInterestClaim .
FACTDESCRIPTIONAfterthehearing , thecourtheldthefactsasfollows : ThedefendantBborrowed$29,500fromtheplaintiffA , andagreedtoreturnafteronemonth .
Aftertheloanexpired , thedefendantfailedtoreturnFact . COURT‚ÄôSVIEWThecourtconcludedthattheloanrelationshipbetweentheplaintiffAandthedefendantBisvalid .
ThedefendantfailedtoreturnthemoneyontimeRationale .
Therefore , theplaintiff‚ÄôsclaimonprinciplewassupportedAcceptanceaccordingtolaw . Thecourtdidnotsupporttheplaintiff‚ÄôsclaimoninterestRejectionbecausetheevidencewasinsufficientRationale .  marized as follows : ‚Ä¢ We investigate the problem of de - biased court ‚Äôs view generation in civil cases from a causal perspective , considering the issue of confounding bias from judgment imbalance .
‚Ä¢ We propose a novel AC - NLG model to jointly optimize a claim - aware encoder and a pair of counterfactual decoders for generating a judgment - discriminative court ‚Äôs view by incorporating with a judgment predictive model .
‚Ä¢ We construct a dataset based on raw civil legal documents , where each case is objectively split into three parts : plaintiff ‚Äôs claim , fact description , and court ‚Äôs view with human annotation on the judgment .
To motivate other scholars to investigate this novel but important problem , we make the experiment dataset publicly available2 .
‚Ä¢ We validate the superior performance of the proposed method with extensive experiments .
Our method can be applied to other natural language generation tasks with confounding bias or data imbalance .
2 Related Work
2.1 Legal Assistant
In recent years , many researchers from both law and computer science Ô¨Åelds have been exploring the potential and methods to perform judicial decisions and auxiliary tasks , aiming at helping lawyers and lower court judges .
In recent work , judicial intelligence is also applied to various tasks of natural language processing .
Since most of the legal documents appear in textual form , many NLP technologies have been brought into the legal Ô¨Åeld to improve the efÔ¨Åciency of legal work .
Charge prediction is a common task of judgment prediction , considered as a text classiÔ¨Åcation problem ( Lin et al , 2012 ; Zhong et al , 2018 ; Hu et al , 2018 ; Jiang et al , 2018 ; Chalkidis et al , 2019 ) .
Besides , there are also works on legal questions classiÔ¨Åcation ( Xiao et al , 2017 ) , law articles recommendation ( Chen et al , 2019 ) , controversy focus mining ( Duan et al , 2019 ) and relevant case retrieval ( Chen et al , 2013 ) .
Ye et al ( 2018 ) explored the court ‚Äôs view generation in criminal cases , where the input is only fact description , and the court ‚Äôs view generation is conditioned on the known judgment results , which is not applicable in real cases .
2https://github.com/wuyiquan/AC-NLG
2.2 Natural Language Generation
Our task aims at generating the court ‚Äôs view based on the plaintiff ‚Äôs claim and the fact description , which can be regarded as a NLG task .
NLG has been widely studied and applied to many tasks , such as machine translation ( Wu et al , 2016 ) , question answering ( McCann et al , 2018 ; Bagchi and Wynter , 2013 ) and text summarization ( Rush et al , 2015 ) .
The recent success of sequence - to - sequence models ( Sutskever et al , 2014 ) , in which recurrent neural networks ( RNNs ) reading and generating text simultaneously , has made the generation task feasible .
Bahdanau et al ( 2014 ) Ô¨Årstly applied the attention mechanism into the NLG task .
See et al ( 2017 ) proposed a Pointer - Generator Networks ( PGN ) , which can effectively solve the OutOf - Vocabulary ( OOV ) problem .
Although the previous work on NLG can produce Ô¨Çuent sentences , they are struggling to be directly applied to our task since a good court ‚Äôs view considers not only the text Ô¨Çuency but also the logical correctness .
2.3 Causal Inference
Causal Inference ( Pearl , 2009 ; Kuang et al , 2020 ) is a powerful statistical modeling tool for explanatory analysis by removing confounding bias in data .
That bias might bring a spurious correlation or confounding effect among variables .
Recently , many methods have been proposed to remove confounding bias in the literature of causal inference , including do - operation based on structure causal model ( Pearl , 2009 ) and counterfactual outcome prediction based on potential outcome framework ( Imbens and Rubin , 2015 ) .
With dooperation , the backdoor adjustment ( Pearl et al , 2016 ) have been proposed for data de - bias .
In this paper , we sketch the causal structure model of our problem , as shown in Fig .
1 , and adopt backdoor for confounding bias reduction .
3 Problem Formulation
In this work , we focus on the problem of the court ‚Äôs view generation in civil cases , where the input is the plaintiff ‚Äôs claim and the fact description , and the output is the corresponding court ‚Äôs view .
We formulate our problem with the deÔ¨Ånition of the plaintiff ‚Äôs claim , the fact description , and the court ‚Äôs view , as shown in Fig .
2 .
Plaintiff ‚Äôs claim ( C ) is a descriptive sentence that depicts the claims from the plaintiff .
In a civil case , it often appears multiple claims from the
765  Figure 3 : The architecture of AC - NLG , which consists of a claim - aware encoder , a pair of counterfactual decoders , and a judgment predictor .
plaintiff .
For example , the plaintiff ‚Äôs claim demonstrated in Fig .
2 contains the principal claim and the interest claim .
Here , we denote the plaintiff ‚Äôs claim in a case as a sentence form c = { wc t=1 , where wc t represents one word , and m is the number of words in plaintiff ‚Äôs claim .
t } m
Fact description ( F ) is also a descriptive sentence , which describes the identiÔ¨Åed facts ( relevant events that have happened ) in a case , as Fig .
2 shows .
Here , we denote the fact description in a case as f = { wf
t=1 , where n is the length .
t } n
Court ‚Äôs view ( V ) contains two main components , judgment and rationales , where the judgment is to respond the plaintiff ‚Äôs claims , and the rationales are the claim - related summarization on the fact description to determine and interpret the judgment .
Here , we denote the court ‚Äôs view as v = { wv t=1 , where l is the length .
Moreover , we use a variable j to denote the judgment in the court ‚Äôs view .
For simplicity , we set j = 1 to denote supported judgment ( all the claims are judged to be accepted ) , and j = 0 to denote non - supported judgment .
t } l
Then , the problem of court ‚Äôs view generation
can be denoted as follow :
Problem 1 ( Court ‚Äôs View Generation ) .
Given the plaintiff ‚Äôs claim c = { wc t=1 and the fact description f = { wf t=1 , our task is to generate the court ‚Äôs view v = { wv
t } m
t } n
t } l
t=1 .
4 Method
In this section , we Ô¨Årst introduce the effect of mechanism confounding bias on the court ‚Äôs view generation and propose a backdoor - inspired method to eliminate that bias .
Then , we describe our Attentional and Counterfactual based Natural Language
Generation ( AC - NLG ) model in detail .
Fig .
3 shows the overall framework .
4.1 Backdoor Adjustment
As shown in Fig .
1 , the confounding bias from the data generation mechanism would blind the conventional training on P ( V |I ) , and current sequenceto - sequence models struggle to solve this problem .
Here , we see through why these models fail mathematically .
For a certain case , given the input I = ( c , f ) , using Bayes rule , we would train the model to generate the court ‚Äôs view V as follow :
P ( V |I ) =
P ( V |I , j)P ( j|I )
( 1 )
( cid:88 )
j
If the supported cases dominate our training data , e.g. , P ( j = 1|I ) ‚âà 1 .
Thus , P ( V |I ) degrades to P ( V |I , j = 1 ) , which would ignore the representation of non - supported cases , leading to the learned representations of inputs I and output V tend to be supported .
Thus , the model tends to build a strong connection between inputs and the supported court ‚Äôs view , even for the cases that are non - supported .
In this way , the representation of input I is contaminated by the confounding bias from I ‚Üê D(J ) ‚Üí V .
Backdoor adjustment is a main de - confounding technique in causal inference ( Pearl et al , 2016 ; Pearl , 2009 ) .
De - confounding seeks the exact causal effect of one variable on another , which appeals for our court ‚Äôs view generation task since the court ‚Äôs view should be faithful only to the content of the plaintiff ‚Äôs claims and fact descriptions .
The backdoor adjustment makes a do - operation on I , which promotes the posterior probability from passive observation to active intervention .
766Claim - aware EncoderùëóAttention LayerAttention LayerCounterfactual DecoderùêØùêß(j=0)ùêØùê¨(j=1)JudgmentPredictor = addc = claimsf = factsùê°ùê±=hidden states of xùêØùê¨ 	 = supported viewùêØùêß=non - supported viewùê¨ùêØùê±=decode states of ùêØùê± 	 j = judgmentFC LayerPointer GeneratorPointer Generatorùë§#ùíóùíèùë§%ùíóùíèùë§&ùíóùíèùë§#ùíóùíîùë§%ùíóùíîùë§&ùíóùíî</s>ùë§%ùíóùíèSigmoid</s>ùë§&ùíóùíî ‚Ä¶ ‚Ä¶ ùê¨ùêØùêßùê¨ùêØùê¨s%ùêØùê¨s#ùêØùêßùë§#(ùë§%(ùë§&(ùë§)(ùë§*(Attention Layerùë§#+ùë§%+ùë§&+ùë§,+ùêüùêúùê°ùêüùê°ùêú‚Ñé% ( ‚Ä¶‚Ä¶  The backdoor adjustment addresses the confounding bias by computing the interventional posterior P ( V |do(I ) ) and controlling the confounder as : ( cid:88 )
P ( V |do(I ) )
=
P ( V |I , j)P ( j )
j
In our problem , the variable j is a binary variable ( support or non - support ) , hence ,
P ( V |do(I ) )
= P ( V |I , j = 0)P
( j = 0 )
+ P ( V |I , j = 1)P ( j = 1 )
( 2 )
( 3 )
The main difference between traditional posterior in Eq . 1 and interventional posterior in Eq . 2 is that P ( j|I ) is changed to P ( j ) .
Since the backdooor adjustment help to cut the dependence between D(J ) and I , we can eliminate the confounding bias from data generation mechanism and learn a interventional model for de - biased court ‚Äôs view generation .
4.2 Backdoor In Implementation
As shown in Fig .
3 , to optimize Eq .
3 , we use a pair of counterfactual decoders to learn the likelihood P ( V |I , j ) for each j. At inference , we propose to use a predictor to approximate P ( j ) .
Note that our implementation on backdoor - adjustment can be easily applied for multi - valued confounding with multiple counterfactual decoders .
4.3 Model Architecture
Our model is conducted in a multi - task learning manner which consists of a shared encoder , a predictor , and a pair of counterfactual decoders .
The predictor and the decoders take the output of the encoder as input .
Our model looks like SHAPED(Zhang et al , 2018 ) ( several decoders with a classiÔ¨Åer ) , but the motivations and mechanisms behind the model are different .
Claim - aware Encoder Intuitively , the plaintiff ‚Äôs claim c and the fact description f are sequences of words .
Therefore , the encoder Ô¨Årstly transforms the words to embeddings .
Then the embedding sequences are fed to the Bi - LSTM , producing two sequences of hidden states hc , hf corresponding to the plaintiff ‚Äôs claim and the fact description respectively .
After that , we use a claim - aware attention mechanism to fuse hc and hf .
For each hidden state hf k is its attention weight on hc i in hf , ei k , and the attention distribution qi is calculated as follow :
k = vT tanh(Wchc ei
k + Wf hf
i + battn )
( 4 )
qi = sof tmax(ei )
( 5 )
where v , Wc , Wf , battn are learnable parameters .
The attention distribution can be regarded as the importance of each word in the plaintiff ‚Äôs claim for a word in fact description .
Next , the new representation of fact description is produced as follows :
i = hf hf ‚àó
i +
( cid:88 )
khc qi k
k
( 6 )
After feeding to another Bi - LSTM layer , we get
the claim - aware representation of fact h.
Judgment Predictor Given the claim - aware representation of fact h , the judgment predictor produces the probability of support Psup through a fully connected layer and a sigmoid operation .
The prediction result j is obtained as follow :
( cid:40 )
j =
1 Psup > 0.5 0
Psup < = 0.5
( 7 )
where 1 means support , and 0 means non - support .
Counterfactual Decoder To eliminate the effect of data bias , here we use a pair of counterfactual decoders , which contains two decoders , one is for supported cases , and the other is for non - supported cases .
The two decoders have the same structure but aim to generate the court ‚Äôs view with different judgments .
We name them as counterfactual decoders because every time there is only one of the two generated court ‚Äôs views correct .
Still , we apply the attention - mechanism .
At each step t , given the encoder ‚Äôs output h , and the decode state st , the attention distribution at is calculated the same way as qi in Eq . 5 , but with different parameters .
The context vector h‚àó
t is then a weighted sum of h :
h‚àó t =
( cid:88 )
at ihi
i
( 8)
The context vector h‚àó t , which can be regarded as a representation of the input for this step , is concatenated with the decode state st and fed to linear layers to produce the vocabulary distribution pvocab :
pvocab = sof tmax(V ( cid:48)(V [ st , h‚àó
t ] )
+ b ) + b(cid:48 ) )
( 9 )
where V , V ( cid:48 ) , b , b(cid:48 ) are all learnable parameters .
Then we add a generation probability ( See et al , 2017 ) to solve the OOV problem .
Given the context
767  h‚àó t , the decode state st and the decoder ‚Äôs input ( the word embedding of the previous word ) xt , the generation probability pgen can be calculated :
Pgen = œÉ(wT
h‚àóh‚àó
t + wT
s st + wT
x xt + bptr ) ( 10 )
where wh‚àó , ws , wx and bptr are learnable , and œÉ is the sigmoid function .
The Ô¨Ånal probability for a word w in time step is obtained :
Table 1 : Statistics of private lending dataset
Type
Result
51087(76 % ) # Supported case 15817(24 % )
#
Non - supported case 77.9 Avg . # tokens in claim Avg . # tokens in fact 158.0 Avg . # tokens in court ‚Äôs view 194.4
P ( w ) = Pgen ‚àó pvocab(w ) + ( 1 ‚àí Pgen )
5 Experiments
( cid:88 )
at i
i : wi = w
( 11 ) We introduce how to alienate the two decoders
5.1 Data Construction
in the training part .
Training For predictor , we use cross - entropy as the loss function :
Lpred = ‚àíÀÜjlog(Psup ) ‚àí ( 1 ‚àí ÀÜj)log(1 ‚àí Psup ) ( 12 )
where ÀÜj is the real judgment .
For decoders , the previous word in training is the word in real court ‚Äôs view , and the loss for timestep t is the negative log - likelihood of the target word w‚àó t :
Lt = ‚àílogP ( w‚àó t )
and the overall generation loss is :
( 13 )
( 14 )
Lgen =
1 T
T ( cid:88 )
t=0
Lt
where T is the length of real court ‚Äôs view .
Since we aim to make the two decoders generate two different court ‚Äôs views , we take a mask operation when calculating the loss of each decoder .
The exact loss for the support decoder is :
Lsup =
( cid:40 )
Lgen 0
ÀÜj = 1 ÀÜj = 0
( 15 )
the loss for the non - support decoder Lnsup is obtained by the opposite way .
Thus , the total loss is :
Ltotal = Lsup + Lnsup + ŒªLpred
( 16 )
where we set Œª to 0.1 in our model .
Inference In inference , the counterfactual decoders apply beam search to generate two court ‚Äôs views , and one of them will be selected as the Ô¨Ånal output , depending on the result of the predictor j.
Since there is no publicly available court ‚Äôs view generation dataset in civil cases , we build a dataset based on raw civil legal documents3 .
SpeciÔ¨Åcally , we choose private lending , which is the most frequent category in civil cases , to construct the dataset .
We process the legal documents as following steps : 1 ) Split legal documents into three parts : plaintiff ‚Äôs claim , facts description , and court ‚Äôs view , which can be objectively split by keywords ( subtitles ) .
2 ) Human annotation .
We employ experts with legal backgrounds to annotate the judgment ( deÔ¨Åned in Sec . 3 ) on the court ‚Äôs view .
3 ) Annotation veriÔ¨Åcation .
We use random sampling test to ensure that the annotation accuracy is over 95 % .
After that , we get the dataset as shown in Tab .
1 .
We randomly separate the dataset into a training set , a validation set , and a test set according to a ratio of 8 : 1 : 1 , the ratio of supported cases is about 76 % in each set .
5.2 Baselines
We implement the following baselines for comparison :
‚Ä¢ S2S Sequence - to - sequence model ( Sutskever et al , 2014 ) is a classic model for NLG task .
We concatenate the plaintiff claims and facts descriptions as input .
‚Ä¢ PGN Pointer Generator Networks ( See et al , 2017 ) utilizes a pointer network to solve the outof - vocabulary ( OOV ) problem , which is essential for the court ‚Äôs view generation since many nouns occur there .
Oversampling is a common method to alleviate data imbalance .
We oversample the non - supported cases so that the ratio between supported cases and non - supported cases become 1 : 1 .
‚Ä¢ S2SwS
Apply oversampling to S2S.
3https://wenshu.court.gov.cn/
768  Table 2 : Results on court ‚Äôs view generation .
Method
S2S S2SwS PGN PGNwS
R-1 54.0 51.5 53.3 53.2 AC - NLGw / oBA 54.1 AC - NLGw / oCA 53.7 53.7 55.1
AC - NLGwS AC - NLG
ROUGE R-2 35.7 32.0 37.1 36.0 38.1 36.7 36.4 38.6
R - L 48.3 45.0 48.8 48.0 49.9 49.1 48.5 50.8
BLEU B-2 57.6 55.6 56.1 56.7 55.9 56.0 56.5 57.1
B-1 65.0 63.3 62.0 63.1 61.8 62.1 62.8 63.2
B - N 50.5 47.9 50.0 50.2 49.9 49.7 50.0 51.0
BERT SCORE f1 r p 89.6 89.5 89.6 86.2 88.8 83.8 92.6 91.2 94.0 94.8 94.0 95.7 92.8 91.9 93.6 93.5 92.6 94.5 93.0 92.1 94.0 95.5 94.6 96.5
Table 3 : Results on judgment prediction .
Prediction Acc .
Method
p 72.1 92.0 86.0 AC - NLG 93.4
w / oD w / oCA wS
Support r 81.0 97.2 94.3 95.9
f1 76.3 94.5 90.0 94.6
Non - support r 44.3 66.0 38.6 72.9
f1 49.8 74.5 47.8 76.9
p 56.9 85.6 62.8 81.5
Table 4 : Results of human evaluation .
Method
PGN AC - NLG
Judgment Support Non
-
support
3.34 3.52
1.78 3.24
Rational
3.11 3.25
Flu .
3.41 3.50
‚Ä¢ PGNwS Apply oversampling to PGN .
‚Ä¢
AC - NLGwS
Apply oversampling to ACNLG .
We do ablation experiments as follows : ‚Ä¢ AC - NLGw / oD
We remove the decoder and train the remaining model ( encoder and predictor ) as a classiÔ¨Åcation task for judgment prediction .
‚Ä¢ AC - NLGw / oBA
We remove the backdoor adjustment by replacing the pair of counterfactual decoders and predictor with a single decoder , but keep the claim - aware attention mechanism .
‚Ä¢ AC - NLGw / oCA We remove the claim - aware attention , and concatenate the claims and the facts instead .
5.3 Metrics ROUGE4 is a set of metrics used in the NLP task .
We keep the results of ROUGE-1 , ROUGE-2 , and ROUGE - L. ROUGE-1 and ROUGE-2 refer to the overlap of unigram and bigram between the generated and reference documents , respectively .
ROUGE - L is a Longest Common Subsequence ( LCS ) based statistics .
BLEU5 ( Papineni et al , 2002 ) is a method of au4https://pypi.org/project/rouge/ 5http://www.nltk.org/api/nltk.test .
unit.translate.html
tomatic text - generation evaluation that highly correlates with human evaluation .
We use BLEU-1 , BLEU-2 to evaluate from the perspectives of unigram , bigram .
BLEU - N is an average of BLEU-1 , BLEU2 , BLEU-3 , BLEU-4 .
BERT SCORE6 ( Zhang et al , 2019 ) computes a similarity score by using contextual embedding of the tokens .
We calculate the precision ( p ) , recall ( r ) and f1 - score to evaluate the information matching degree .
Accuracy of judgment prediction To evaluate the performance of the predictor , we calculate the precision ( p ) , recall ( r ) and , f1 - score of supported and non - supported cases , respectively .
Human Evaluation We conduct a human evaluation to better analyze the quality of the generated court ‚Äôs view .
First , we randomly sample 500 test cases , where the ratio of the supported and nonsupported cases are 1:1 .
For each case , we present the generated court ‚Äôs views from each method7 with the ground truth to 5 human annotators with legal backgrounds .
The evaluation is conducted following three perspectives : ( 1 ) Judgment level .
Annotators are asked to give a score ( 1 - 5 ) on the judgment in the generated court ‚Äôs view .
1 for totally wrong and 5 for totally correct .
( 2 ) Rational level .
Annotators are asked to give a score ( 1 - 5 ) on the rationals in the generated court ‚Äôs view .
1 for the worst and 5 for the best .
( 3 ) Fluency level .
Annotators are asked to give a score ( 1 - 5 ) on the Ô¨Çuency of the generated court ‚Äôs view .
1 for the worst and 5 for the best .
5.4 Experimental Results
Tab . 2 demonstrates the results of court ‚Äôs view generation with ROUGE , BLEU , and BERT SCORE .
Also , we report the results on the judgment prediction of our predictor component with precision
6https://github.com/Tiiiger/bert_score 7We shufÔ¨Çe all the results to be fair for all the methods
769  Figure 4 : Case study .
( p ) , recall ( r ) , and f1 - score ( f1 ) in Tab .
3 .
To demonstrate that our method is de - biased on judgment generation , we report the result of human evaluation in Tab .
4 .
Results of court ‚Äôs view generation : From Tab .
2 , we can conclude that : ( 1 ) S2S tends to repeat words , which makes it get high BLEU but low BERT SCORE .
( 2 ) Oversampling strategy does n‚Äôt beneÔ¨Åt the models , hence , it can not address the confounding bias .
( 3 ) With claim - aware encoder and backdoor - inspired counterfactual decoders , our AC - NLG achieves better performance on court ‚Äôs view generation compared with baselines .
( 4 ) The performance gap between AC - NLGw / oCA and AC - NLG demonstrates the effectiveness of our proposed claim - aware encoder , and the gap between AC - NLGw / oBA and AC - NLG illustrates the superiority of our counterfactual decoders .
Results of judgment prediction : From Tab . 3 , we have the following observations : ( 1 ) The counterfactual decoders in our model can signiÔ¨Åcantly eliminate the confounding bias , hence , achieve remarkable improvement on the non - supported cases , for example boosting f 1 from 49.8 % to 76.9 % .
( 2 ) The proposed claim - aware encoder has a limited effect on judgment prediction since it ‚Äôs designed for improving the quality of generation as shown in Tab .
2 . ( 3 ) Still , oversampling brings no improvement to the model .
Results of human evaluation : From Tab . 4 , we have the following observations : ( 1 ) due to the confounding bias in data , the performance of judgment generation in PGN is poor for non - supported cases , and its performance gap between supported and non - supported cases is huge ( 1.56 ) .
( 2 ) By
debiasing with backdoor - inspired counterfactual decoders , our AC - NLG signiÔ¨Åcantly improves the performance of judgment generation , especially for non - supported cases , and achieves a smaller performance gap ( only 0.28 ) between the supported and non - supported cases .
( 3 ) With a claim - aware encoder , our AC - NLG also achieves better performance on the generation of rational and generated court ‚Äôs view Ô¨Çuency .
( 4 ) Kappa coefÔ¨Åcient Œ∫ is more than 0.8 between any two judges , which proves the validation of human evaluation .
Overall , thanks to the proposed claim - aware encoder , counterfactual decoders , and a synergistic judgment predictor , our model achieves better performance than single - task baselines on the task of judgment prediction , judgment generation in court ‚Äôs view and court ‚Äôs view generation .
5.5 Experiment Details We use Gensim ( ÀáRehÀöuÀárek and Sojka , 2010 ) with a large - scale generic corpus to train a language model as the pre - trained model , then use it to initialize the word embeddings , which is in the dimension of 300.8
5.6 Case Study
Figure 4 shows three court ‚Äôs views for a certain case : the court ‚Äôs view generated by PGN , by the proposed AC - NLG method , and the real court ‚Äôs view .
We Ô¨Ånd that the one generated by PGN accepts the claim for principal , but ignores other claims such as issue related to guarantee .
Compared with the real court ‚Äôs view , our model accu8Source code , data , more experiment details and results
can be found in supplementary materials .
770PLAINTIFF‚ÄôSCLAIMThedefendantBreturntheloanof$495,000.ThedefendantCreturntheloantogether .
FACTDESCRIPTIONAfterthehearing , thecourtheldthefactsasfollows : FromNovember20,2010toMarch23,2011,thedefendantBsuccessivelyborrowedatotalof$495,000fromtheplaintiffAandissuedfourseparateborrowings . ThedefendantBhasnotrepaidtheaboveloan .
PGNThecourtconcludedthat : TheprivatelendingrelationshipbetweenPlaintiffAandDefendantB , wherethesubjectwasappropriate , thecontentwaslegitimate , andthemeaningwastrue , shouldbeconfirmedtobelegalandvalid .
Thetwosidesdidnotagreedinwritingonaloanperiod , sothedefendantshouldreturntheplaintiff'sloaninatimelymannerwithinareasonableperiodaftertheplaintiffurged . Thedefendant‚Äôsfailuretoreturntheloantimelyconstitutedabreachofcontractandshouldbearcorrespondingcivilliabilities .
Therefore , the plaintiff 's claim was reasonable and legal , and the court supported it Acceptance .
AC - NLGThecourtconcludedthatthesubjectoftheprivatelendingrelationshipbetweenPlaintiffAandDefendantBwasqualified , thecontentwaslegal , andthemeaningwastrue .
Itshouldbedeemedvalid .
Thetwosidesdidnotagreedinwritingonaloanperiod , thedefendantshallreturntheloanwithinareasonableperiodaftertheplaintiffurged .
The plaintiff ‚Äôs claim requesting the defendant to return the loan of $ 495,000 was in compliance with the law and the court supported it Acceptance .
However , the court did not support the claim requesting the defendant C to bear the guarantee liability because   the evidence was insufficient Rejection .
REALThecourtconcludedthat : ThesubjectoftheprivatelendingrelationshipbetweenPlaintiffAandDefendantBwasqualified , thecontentwaslegal , andthemeaningwastrue .
Itshouldbedeemedvalid .
Defendantshouldrepaytheplaintiff'sloanwithinareasonableperiodaftertheplaintiffurged .
Therefore , Defendant B should bear the civil liability of returning the plaintiff 's loan of $ 495,000 and paying overdue interest Acceptance .
The court did not support the plaintiff ‚Äôs claim requesting the defendant C to return the loan together because the evidence was insufficientRejection .
DefendantBfailedtoappearincourtafterbeinglegallysummonedbythecourt .  rately responds to both claims and produces the correct judgment .
6 Ethical Discussion
While AI is gaining adoption in legal justice(Lin et al , 2012 ; Zhong et al , 2018 ; Hu et al , 2018 ; Jiang et al , 2018 ; Chalkidis et al , 2019 ) , any subtle statistical miscalculation may trigger serious consequences .
From a fairness perspective , prior studies suggested that global ( statistical ) optimization ( cid:54)= individual ( demographic ) fairness ( Zemel et al , 2013 ) , and this ethical concern should be further investigated .
In this section , we explore the following ethical issues .
Target User :
According to the report of statistics , a typical active trial judge closed around 250 cases in a year .
Trial judges suffering from ‚Äò daunting workload ‚Äô is becoming an critical issue(Duan et al , 2019 ) .
The proposed algorithm is designed for generating the court ‚Äôs view draft for assisting the trial judges for decision making .
This work is an algorithmic investigation , but such algorithm should never ‚Äò replace ‚Äô human judges .
Human knowledge / judgment should be the Ô¨Ånal safeguard to protect social justice and individual fairness .
Potential Error : The potential error would be as follows : a ) generating a wrong judgment and b ) generating a wrong rationale .
The goal of our algorithm is to generate a draft of court ‚Äôs view for trail judge as a reference , and judges need to proofread the content generated from algorithm .
Demographic Bias : In this paper , we focus on addressing the bias problem from the data generation by treating the variable of data generation as confounder in back - door adjustment .
The model adoption can face potential demographic bias / unfairness challenges , such as gender and race bias in the training data .
To further ensure the model fairness , in the future , algorithm adoption should be empowered with de - biased legal content pretraining , which could avoid potential demographic bias .
For instance , in order to remove gender / race bias , system could use ( Bolukbasi et al , 2016 ) algorithm to debias the sensitive gender / race information , e.g. , replace ‚Äò he / she ‚Äô and ‚Äò asian / hispanic ‚Äô with gender / race neutral words for pretraining , which can be vital for legal domain .
7 Conclusion and Future Work
In this paper , we propose a novel Attentional and Counterfactual based Natural Language Generation ( AC - NLG ) method to solve the task of court ‚Äôs view generation in civil cases and ensure the fairness of the judgment .
We design a claim - aware encoder to represent the fact description which emphasizes on the plaintiff ‚Äôs claim , as well as a pair of backdoor - inspired counterfactual decoders to generate judgment - discriminative court ‚Äôs views ( both supportive and non - supportive views ) and to eliminate the bias that arose from the data generation mechanism by connecting with a synergistic judgment predictive model .
The experimental results show the effectiveness of our method .
Based on the AC - NLG method , in the future , we can explore the following directions : ( 1 ) Improve the accuracy of judgment on a claim - level .
( 2 ) Add external knowledge ( e.g. a logic graph ) to the predictor for the interpretability of the model .
Acknowledgments
This work was supported by National Natural Science Foundation of China ( No . 62006207 , 61625107 ) , National Key R&D Program of China ( No . 2018AAA0101900 , 2018YFC0830200 , 2018YFC0830206 , 2020YFC0832500 ) , the Fundamental Research Funds for the Central Universities .
Finally , we would like to thank the anonymous reviewers for their helpful feedback and suggestions .
