Evaluating Multiple System Summary Lengths : A Case Study
Ori Shapira1 , David Gabay1 , Hadar Ronen2 , Judit Bar - Ilan2 , Yael Amsterdamer1 , Ani Nenkova3 , and Ido Dagan1
1Computer Science Department , Bar - Ilan University , Ramat - Gan , Israel 2Information Science Department , Bar - Ilan University , Ramat - Gan , Israel 3University of Pennsylvania , Philadelphia , PA { obspp18 , dawid.gabay , hadarg , juditb}@gmail.com amstery@cs.biu.ac.il , nenkova@seas.upenn.edu , dagan@cs.biu.ac.il
Abstract
Practical summarization systems are expected to produce summaries of varying lengths , per user needs .
While a couple of early summarization benchmarks tested systems across multiple summary lengths , this practice was mostly abandoned due to the assumed cost of producing reference summaries of multiple In this paper , we raise the research lengths .
question of whether reference summaries of a single length can be used to reliably evaluate system summaries of multiple lengths .
For that , we have analyzed a couple of datasets as a case study , using several variants of the ROUGE metric that are standard in summarization evaluation .
Our ﬁndings indicate that the evaluation protocol in question is indeed competitive .
This result paves the way to practically evaluating varying - length summaries with simple , possibly existing , summarization benchmarks .
1
Introduction
Automated summarization systems typically proIn duce a text that mimics a manual summary .
these systems , an important aspect is the output summary length , which may vary according to user needs .
Consequently , output length has been a common tunable parameter in pre - neural summarization systems and has been incorporated recently in few neural models as well ( Kikuchi et al , 2016 ;
Fan et al , 2017 ; Ficler and Goldberg , 2017 ) .
It was originally assumed that summarization systems should be assessed across multiple summary lengths .
For that , the earliest Document Understand Conference ( DUC ) ( NIST , 2011 ) benchmarks , in 2001 and 2002 , deﬁned several target summary lengths and evaluated each summary against ( manually written ) reference summaries of the same length .
However , due to the high cost incurred , subsequent DUC and TAC ( NIST , 2018 ) benchmarks
( 2003 - 2014 ) , as well as the more recently popular datasets CNN / Daily Mail ( Nallapati et al , 2016 ) and Gigaword ( Graff et al , 2003 ) , included references and evaluation for just one summary length per input text .
Accordingly , systems were asked to produce a single summary , of corresponding length .
This decision was partly supported by an observation that system rankings tended to correlate across different summary lengths ( Over et al , 2007 ) , even though , as we show in Section 2 , this correlation is limited .
In this paper , we propose that the summarization community should consider resuming evaluating summarization systems over multiple length outputs , as it would allow better assessment of length - related performance within and across systems ( illustrated in Section 3 ) .
To avoid the need in multiple - length reference summaries we raise the following research question : can reference summaries of a single length be used to evaluate system summaries of multiple lengths , as reliably as when using references of multiple lengths , with respect to different standard evaluation metrics ?
Recently , Kikuchi et al ( 2016 ) evaluated system summaries of three different lengths against reference summaries of a single length .
Yet , their evaluation methodology was not assessed through correlation to human judgment , as has been commonly done for other automatic evaluation protocols .
Here , we provide a closer look into this methodology , given its potential value .
As a ﬁrst accessible case study , we test our research question over the DUC 2001 and 2002 data ( Section 2 ) .
To the best of our knowledge , these are the only two datasets that include multiple length reference and submitted system summaries , as well as manual assessment of the latter .
Our analysis reveals that , for this data and with respect to various highly utilized automatic ROUGE metrics , the answer to our question is afﬁrmative , in
Proceedingsofthe2018ConferenceonEmpiricalMethodsinNaturalLanguageProcessing , pages774–778Brussels , Belgium , October31 - November4,2018.c(cid:13)2018AssociationforComputationalLinguistics774  # refs 3 2
ref lengths ( # words ) 50 , 100 , 200 , 400 10 , 50 , 100 , 200
# clusters 30 59
# systems 14 10
2001 2002
Table 1 : DUC 2001 and 2002 .
Number of reference summaries per length for each text cluster , reference lengths , number of clusters and number of evaluated systems .
terms of correlation with human judgment .
Our promising results suggest repeating the assessment methodology presented here in future work , to test our question over more recent and broader summarization datasets and human evaluation schemes .
This , in turn , would allow the community to feasibly resume proper evaluation and deliberate development of systems that target effective summaries across a range of lengths .
2 Case Study Analysis
Here , we ﬁrst examine the relevance of our proposal to reinstitute summarization evaluation over multiple summary lengths .
Then , we investigate our research question of whether using reference summaries of a single length sufﬁces for evaluating system summaries of multiple lengths .
We turn to the DUC 2001 and 2002 multi - document summarization datasets , which , to the best of our knowledge , are the only available datasets that provide the necessary requirements for this analysis ( see Table 1 ) .
The importance of evaluating and comparing systems at several lengths is demonstrated with the observation that system rankings can change quite signiﬁcantly at different summary lengths .
In 2001 , the Spearman correlation between the available human rankings of systems at the 50word and 400 - word lengths is 0.61 .
For example , the system ranked ﬁrst at length 50 ranks sixth at lengths 200 and 400 .
Even for the human system ranking at the 100 - word length , which deviates the least from human rankings at the other lengths , the correlation with system ranking at the 400 length is only 0.73 .
Generally , the larger the difference between a pair of summary lengths , the greater the ﬂuctuation in system rankings .
Similar trends were observed for DUC 2002 , and when comparing system rankings by automatic ROUGE scoring ( both rankings are elaborated below ) .
Obviously , such performance differences are overlooked when evaluating systems over summaries of a single length .
Next , we turn to investigate our research question .
In this paper , we examine it with respect to automatic summary evaluation , which has become most common for system development and evaluation , thanks to its speed and low cost .
Speciﬁcally , we use several variants of the ROUGE metric ( Lin , 2004 ) , which is almost exclusively utilized as an automatic evaluation metric class for summarization .
ROUGE variants are based on word sequence overlap between a system summary and a reference summary , where each variant measures a different aspect of text comparison .
Despite its pitfalls , ROUGE has shown reasonable correlation of its system scores to those obtained by manual evaluation methods ( Lin , 2004 ; Over and James , 2004 ;
Over et al , 2007 ; Nenkova et al , 2007 ; Louis and Nenkova , 2013 ; Peyrard et al , 2017 ) , such as SEE ( Lin , 2001 ) , responsiveness ( NIST , 2006 ) and Pyramid ( Nenkova et al , 2007 ) .
We follow the same methodology of assessing the reliability of automatic evaluation scores by measuring their correlation to human evaluation In our case , DUC 2001 and 2002 apscores .
plied the SEE manual evaluation method .
NIST assessors compared systems ’ summaries to reference summaries , which were all decomposed into a list of elementary discourse units ( EDUs ) .
Each reference EDU was marked throughout the system EDUs and was scored for how well it was expressed .
The ﬁnal manually evaluated scores , called the human mean content coverage scores , are provided in the DUC datasets .
We can then correlate the human - based system ranking , attained from these provided scores , to the system ranking attained from the automatic scores that we calculate using our proposed methodology .
As a baseline , we consider the ROUGE Recall scores obtained by the standard reference summary conﬁguration ( Standard , ﬁrst row in Table 2 ) , that is , when system summaries of each length ( table columns ) are evaluated against reference summaries of the same length .
This is the same conﬁguration used by Lin ( 2004 ) when introducing and assessing ROUGE .
Then , looking into our research question , we consider reference summary conﬁgurations in which system summaries of all lengths are evaluated against reference summaries of a single chosen length ( OnlyNNN , subIn each conﬁguration sequent rows of Table 2 ) .
( each row ) , we repeat the evaluation twice : once using the complete set of available reference sum775  System Summary Length
50
100
200
400
3refs
0.72
0
-0.01 -0.09 -0.06
1ref
0.65
0 +0.04 -0.09 +0.02
3refs
0.88
+0.02 0
-0.06 -0.09
1ref
0.85
0 0 -0.08 -0.09
3refs
0.9
+0.01 +0.01 0
-0.01
1ref
0.86
+0.04 -0.01 0 +0.03
3refs
0.95
+0.01 +0.02 +0.01 0
1ref
0.94
+0.02 0
-0.01 0
Avg . across lengths 3refs
1ref
0.86
+0.010 +0.005 -0.035 -0.040
0.83
+0.015 +0.008 -0.0045 -0.010
t Standard e S e c n e r e f e R
Only50 Only100 Only200 Only400
Table 2 : Pearson correlations between ROUGE-1 and human scores over DUC 2001 for different system summary lengths ( column pairs ) and different reference summary conﬁgurations ( rows ) , when using one reference or three .
The ﬁrst baseline row presents absolute correlations while successive rows show relative differences to the baseline .
R-1
3refs
0.86
+0.010 +0.005 -0.035 -0.040
1ref
0.83
+0.015 +0.008 -0.045 -0.010
2001 R-2
3refs
0.79
-0.013 -0.010 -0.055 -0.075
1ref
0.77
-0.038 -0.003 -0.053 -0.075
R - L
R-1
3refs
0.87
+0.008 +0.005 -0.033 -0.038
1ref
0.83
+0.010 +0.013 -0.04 0
2refs
0.78
0
+0.035 +0.023 +0.013 1ref
0.75
-0.015 +0.053 +0.048 +0.023 2002 R-2
2refs
0.86
-0.100 -0.050 -0.035 -0.068 1ref
0.82
-0.178 -0.038 0 -0.025 R - L
2refs
0.82
-0.003 +0.020 -0.008 -0.028 1ref
0.77
-0.045 +0.080 +0.040 +0.005 t Standard e S e
c n e r e f e R
Only10 Only50 Only100 Only200 Only400
Table 3 : Averaged correlations ( across system summary lengths , equivalent to the rightmost columns in Table 2 ) for different ROUGE variants ( column pairs ) and reference summary conﬁgurations ( rows ) , when using 1 reference or multiple .
The ﬁrst row presents absolute correlations , with relative differences in subsequent rows .
maries of the utilized reference length , and once with just one randomly chosen reference summary from that set ( the 3refs and 1ref sub - columns ) .
For each reference summary conﬁguration , we compute ROUGE Recall system scores1 for the three common ROUGE variants R-1 , R-2 and R - L , which compare unigrams , bigrams and the longest common subsequence , respectively .
System scores , per summary length , are obtained by averaging across all summarized texts .
We then calculate their Pearson correlation2 with the available human mean content coverage scores for the systems .
The ﬁrst row of Table 2 shows these correlations , considering the R-1 scores for the DUC 2001 systems , per summary length .
The subsequent rows show the corresponding ﬁgures for the single - reference - length conﬁgurations .
For readability , we present in these rows the relative differences to the Standard baseline row .
Hence , positive values indicate a conﬁguration that is at least as good as the standard conﬁguration .
Table 3 presents correlations averaged over all summary lengths , for the three ROUGE variants
1Omitting stop words .
2Following Lin ( 2004 ) .
Spearman ranking correlations
provide similar results .
over both datasets .
We see in the tables that evaluating system summaries of all lengths against references of a single length often performs on par with the standard conﬁguration .
In particular , the single ﬁxed set of 50 - word reference summaries performs overall as well as the standard approach , and , although not substantially , is the most effective conﬁguration within the data analyzed .
In other words , in this dataset , the 50 - word reference summaries provide a “ test sample ” for evaluating the longer system summaries , which is as effective as the same length references used by the standard method .
We note that even when a single reference summary is available , reasonable correlations with human scores are obtained for the 50 word reference .
This suggests that it may be possible to compare system summaries of multiple lengths even against a single reference summary , of a relatively short length .
This observation seems to deserve further assessment over recent large scale datasets , such as CNN / DailyMail , which provide a single relatively short reference for each summarized text .
In addition to correlation to human assessment , we computed the correlations between system rankings calculated by Standard and those calcu776  1 E
G U O R
0.5
0.4
0.3
0.2
0.1
ICSI S
R T
100
200
300
400
System Summary Length
Figure 1 : R-1 scores of a few systems , evaluated against the 50 - word reference set of DUC 01 .
Systems R , S and T are from DUC 01 ; ICSISumm is a later competitive system ( Gillick et al , 2008 ) .
lated by Only50 , at each system summary length .
We ﬁnd very high correlations ( above 0.95 for all system summary lengths , in both datasets ) when using multiple references and slightly lower ( 0.85 to 0.9 ) with one reference summary .
These ﬁgures show that the Only50 conﬁguration ranks systems very similarly to Standard .
To further verify our results , we computed correlations in two additional settings .
First , we conducted the same analysis , excluding 2 - 3 of the worst systems , which might artiﬁcially boost the correlation ( Rankel et al , 2013 ) .
Second , we computed score differences between all pairs of systems , for both human and ROUGE scores , and computed the correlation between these two sets of differences ( Rankel et al , 2011 ) .
In both cases we observed rather consistent results , assessing that a single set of short reference summaries evaluates system summaries of different lengths just as well as the standard conﬁguration .
3 Cross - length Summary Evaluation
This section illustrates how system performances can be measured and compared when evaluating them on outputs of varying lengths against a single reference point .
Figure 1 presents the ROUGE scores of the Only50 conﬁguration for three DUC01 submitted systems , and for ICSISumm ( Gillick et al , 2008 ) , a later competitive system .
As expected when measuring ROUGE Recall against a ﬁxed reference length , longer system summaries typically cover more of the reference summaries content than shorter ones , yielding higher scores .
Yet , it can be noted , for example , that the value of the 400 - word summary of system R in the ﬁgure is lower than that of the 200 - word summaries of the other systems .
Such a comparison is impossible in the standard setup , as each system length is evaluated against different reference summaries .
We note that similar comparisons are embedded in the evaluations of Steinberger and Jezek ( 2004 ) and Kikuchi et al ( 2016 ) , who also evaluated multiple summary lengths .
Further , one can deﬁne the marginal value of longer summaries of a given system as the ROUGE score increase per number of additional words , namely the graph slope .
This denotation allows measuring the effectiveness of producing longer summaries .
For example , deploying system R , we might decide to output only summaries no longer than 200 words , since the marginal value of longer summaries becomes too small .
The other systems , on the other hand , seem marginally effective also in 400 word summaries .
4 Discussion
We proposed the potential value of evaluating summarization systems at different summary lengths .
Such evaluations would allow proper trackevaluation of systems ’ “ length knob ” , ing how their ranking changes across summary lengths as well as tracking the cross - length behavior of individual systems .
Given that reference summaries of a single length are usually available in practice , we analyzed the potential use of reference summaries of a single length for evaluating system summaries of multiple lengths .
We found , on the only two datasets readily available for such analysis , that this conﬁguration is as reliable as the standard conﬁguration , which evaluates each system summary against a reference of a matching length .
To broadly substantiate our ﬁndings , we propose future work that would follow our assessment methodology over test samples from current datasets ( e.g. CNN / DailyMail ) , judging performance of current systems and utilizing current manual evaluation protocols .
This would require preparing , for limited samples , additional manually crafted summaries of several lengths and manually evaluating system summaries of corresponding lengths .
Using such data , it will be possible to repeat our analysis and test the broader validity of the single - reference - length conﬁguration .
If broadly assessed , it will be possible to start evaluating system summaries of multiple lengths over most currently available datasets , leveraging the available single - length reference summaries .
Fu777  ture benchmarks could require systems to produce different length outputs , while feasibly evaluating them using the existing , single length , reference summaries .
This , in turn , is likely to drive research to better address the need for producing high quality summaries ﬂexibly across a range of summary lengths , a dimension that has been disregarded for long .
Acknowledgments
We would like to thank the anonymous reviewers for their constructive comments .
We thank Yinfei Yang for his assistance in producing the ICSISumm summaries that we utilized in our analysis .
This work was supported in part by grants from the MAGNET program of the Israel Innovation Authority ; by the German Research Foundation through the German - Israeli Project Cooperation ( DIP , grant DA 1600/1 - 1 ) ; by the BIU Center for Research in Applied Cryptography and Cyber Security in conjunction with the Israel National Cyber Bureau in the Prime Ministers Ofﬁce ; and by the Israel Science Foundation ( grants 1157/16 and 1951/17 ) .
