HacRED : A Large - Scale Relation Extraction Dataset Toward Hard Cases in Practical Applications
Qiao Cheng1‚àó , Juntao Liu1‚àó ,   Xiaoye Qu2 ,   Jin Zhao1 ,   Jiaqing Liang1 ,   Zhefeng Wang2 , Baoxing Huai2 , Nicholas Jing Yuan2 , Yanghua Xiao1,3‚Ä† 1Shanghai Key Laboratory of Data Science , School of Computer Science , Fudan University , China 2Huawei Cloud , China 3Fudan - Aishu Cognitive Intelligence Joint Research Center , Shanghai , China { cq.qiaojim , l.j.q.light}@gmail.com , { jtliu19 , shawyh , jinzhao20}@fudan.edu.cn ,   { quxiaoye , wangzhefeng , huaibaoxing , nicholas.yuan}@huawei.com
Abstract
Relation extraction ( RE ) is an essential topic in natural language processing and has attracted extensive attention .
Current RE approaches achieve fantastic results on common datasets , while they still struggle on practical applications .
In this paper , we analyze the above performance gap , the underlying reason of which is that practical applications intrinsically have more hard cases .
To make RE models more robust on such practical hard cases , we propose a case - oriented construction framework to build a Hard Case Relation Extraction Dataset ( HacRED ) .
The proposed HacRED consists of 65,225 relational facts annotated from 9,231 documents with sufficient and diverse hard cases .
Notably , HacRED is one of the largest Chinese document - level RE datasets and achieves a high 96 % F1 score on data quality .
Furthermore , we apply the stateof - the - art RE models on this dataset and conduct a thorough evaluation .
The results show that the performance of these models is far lower than humans , and RE applying on practical hard cases still requires further efforts .
HacRED is publicly available at https://github . com / qiaojiim / HacRED .
1
Introduction
Relation extraction ( RE ) is one of the core NLP tasks and plays an increasingly important role in knowledge graph completion ( Bordes et al , 2013 ) and question answering ( Dong et al , 2015 ) .
RE aims to extract structured relational facts , i.e. , triples such as ( Bill Gates , founder_of , Microsoft ) from plain texts .
Recently , various models ( Zeng et al , 2018 ; Takanobu et al , 2019 ; Fu et al , 2019 ; Wei et al , 2020 ) have been proposed to identify the relational facts and achieved state - of - theart ( SOTA ) performance , among which the latest
‚àóEqually contributed .
‚Ä†Corresponding author .
Figure 1 : Cases and corresponding triples in WebNLG and practical applications .
method CasRel achieves notable 91.8 % F1 score on WebNLG ( Gardent et al , 2017 ) and 89.6 % on NYT ( Riedel et al , 2010 ) .
However , can these seemingly fantastic results prove that the current RE models are powerful enough to perform well in practical applications .
To answer the question , we employ CasRel on 300 randomly selected samples of WebNLG and the same number of data from practical DuIE1 .
The F1 scores under these scenarios drop significantly from 89.3 % to 62.8 % .
As illustrated in Figure 1 , CasRel extracts correct triples ( Elliot See , place_of_birth , Dallas ) and ( Elliot See , place_of_death , St. Louis ) in WebNLG where keywords such as born and died explicitly express the relation information .
In contrast , CasRel fails to extract triples such as ( Yang Jima , graduate_from , Communication University of China ) where no keywords like graduate are mentioned .
The most significant reason why CasRel performs well on WebNLG but struggles on practical data is that more challenging instances which
1http://lic2019.ccf.org.cn/kg
FindingsoftheAssociationforComputationalLinguistics : ACL - IJCNLP2021,pages2819‚Äì2831August1‚Äì6,2021. ¬© 2021AssociationforComputationalLinguistics2819Elliot See was born on July 23rd , 1927 in Dallas , and died in St. Louison February 28th , 1966 .Case in WebNLGElliot See , place_of_birth , DallasElliot See , place_of_death , St. LouisTriplesYang Jima ( 1986 - ) , ... , is a student of 2005 in the Department of ‚Ä¶ , Communication University of China ...
In the semi - final of the Chinese Idol Show , Yangexcellentlyperformed the Lhasa Ballad , which was recognized by the judges and the audience .
As a result , she got to the final competition .
Case in PracticeYang Jima(Yang ) , graduate_from , Communication University of ChinaLhasa Ballad , singer , Yang JimaYang Jima(Yang ) , invited_guest_of , Chinese Idol ShowTriples  we refer to as hard cases exist in the practical applications .
Moreover , according to the statistics of entity description documents in CN - DBpedia ( Xu et al , 2017 ) , at least 40.1 % relational facts can only be extracted from hard cases .
Therefore , relation extraction from hard cases can not be neglected and demands more attention .
Although many datasets ( Li et al , 2016 ; Yao et al , 2019 ) have been proposed for RE , they rarely analyze the performance gap and focus on the hard cases .
In order to make models robust on hard cases and more fit practical scenarios , in this paper , we aim to build a RE dataset with sufficient hard cases .
To this end , we propose a case - oriented construction framework based on the challenging instances and build a Hard Case Relation Extraction Dataset ( HacRED ) .
Specifically , we first obtain general , massive , and various contexts as well as relational facts from CN - DBpedia to construct a distantly supervised dataset .
The crucial part is to distinguish hard cases from abundant data .
Therefore , we formulate nine indicators through systematic analysis of hard cases to quantify them .
Then , we conduct feature engineering based on the valid indicators .
Afterwards , a classifier is trained for distinguishing the desired hard cases .
Finally , we develop a crowdsourcing platform with a novel three - stage annotation strategy and effective aggregation method CrowdTruth2.0 ( Dumitrache et al , 2018 ) to guarantee the data size and quality .
In total , HacRED consists of 9,231 instances with 26 predefined relations and 9 types of entities .
To the best of our knowledge , it is one of the largest document - level RE benchmark .
Moreover , HacRED contains sufficient and diverse hard cases in line with practice .
We conduct extensive experiments and systematic error analysis of SOTA models on HacRED .
A sharp performance drop on HacRED compared to the existing benchmarks proves that RE in practical applications remains an open problem and still requires further research .
To recap , our main contributions are three - fold :
‚Ä¢ We first analyze the performance gap between popular datasets and practical applications , and therefore construct one of the largest Chinese document - level RE dataset which contains sufficient and diverse hard cases to improve the evaluation for complex RE tasks .
‚Ä¢ We propose a case - oriented construction framework to build RE dataset toward special cases .
Meanwhile , we design a novel three - stage annotation method applicable for crowdsourcing of complex RE .
‚Ä¢ We systematically evaluate the current mainstream RE models on HacRED and justify its effectiveness in depth .
2 Related Work
2.1 Datasets for Relation Extraction
A series of datasets have been built for RE as of late , which have extraordinarily advanced the improvement of RE systems .
RE datasets such as SemEval-2010 Task 8 ( Hendrickx et al , 2009 ) and ACE05 are constructed through human annotation with relatively limited relation types and size .
A large - scale dataset TACRED ( Zhang et al , 2017 ) is obtained via crowdsourcing to satisfy the training of data - hungry models .
As RE applications differ much in various scenarios , constructing datasets aimed at specific targets is a popular trend in RE .
DocRED ( Yao et al , 2019 ) is constructed to accelerate the research on document - level RE .
To meet the challenges of fewshot RE , FewRel ( Han et al , 2018 ) as well as FewRel 2.0 ( Gao et al , 2019 ) have been presented .
RELX ( Koksal and Ozgur , 2020 ) is a benchmark for cross - lingual RE .
Jia et al ( 2020 ) propose the task of interpersonal RE in dyadic dialogues and further construct a corresponding dataset called DDRel .
Compared with previous RE datasets , HacRED is derived from the analysis of the performance gap between popular datasets and practical applications .
It targets towards promoting the RE models to extract information from the complex contexts .
2.2 Models for Relation Extraction
Recently , many exciting works have been pro(1)Joint Model : posed to solve the RE tasks .
NovelTagging ( Zheng et al , 2017 ) first formulates the task as a sequence labeling problem and presents a novel tagging schema to jointly extract entities and relations .
CopyRE ( Zeng et al , 2018 ) extracts triples based on a sequence - to - sequence structure and integrates the copy mechanism for entity generation .
GraphRel ( Fu et al , 2019 ) uses graph convolutional network ( GCN ) to capture features of words and text .
CasRel ( Wei et al , 2020 ) is different from the past and is able to extract more triples by learning relationspecific entity taggers .
( 2)Pipeline Model : PURE
2820  ( Zhong and Chen , 2020 ) is a simple pipelined approach which learns an entity model and a relation model independently .
DGCNN - BERT is a powerful pipeline method that first identifies multiple relations and then labels the head and tail entities given a relation .
It achieves 89.3 F1 scores and has won the champion in the Competition of DuIE held by Baidu Inc. ( 3)Document - level Relation Classification Models : LSR ( Nan et al , 2020 ) is a model that empowers the relational reasoning across sentences by automatically inducing the latent document - level graph .
GAIN ( Zeng et al , 2020 ) introduces a path reasoning mechanism based on a heterogeneous mention - level graph and an entity - level graph .
ATLOP ( Zhou et al , 2020 ) proposes two techniques , adaptive thresholding and localized context pooling .
SSAN ( Xu et
al , 2021 ) designs several transformations to incorporate mention structural dependencies for document - level relation classification ( DocRC ) .
3 Easy Cases vs. Hard Cases
To analyze where models struggle in practical instances and distinguish the hard cases , we conduct a manual exploratory analysis on the errorprone instances of SOTA models ( CGCN , CasRel , DGCNN - BERT ) on NYT , DuIE and industry data .
Then we formulate the potential causes of the errors with nine indicators illustrated as follows : Text Length .
We notice that models tend to fail on instances with longer text .
The experiments of Alt et al ( 2020 ) also reflect that RE models get a relatively higher error rate with the length of sentence greater than 30 in TACRED .
Argument Distance .
We observe that the performance of the models declines when the arguments ( i.e. , head and tail entity mentions ) are far away , especially in inter - sentence RE .
Distractors .
Extracting triples in contexts with linguistic distractors is tough for current models .
For example , drop out will contribute to wrong relation graduate_from between entity mentions with PERSON and SCHOOL type .
Reasoning .
Reasoning is needed to extract the relation mentioned implicitly in the text .
Recent work suggests that future researchers consider incorporating common sense knowledge or improved causal modules in RE tasks ( Han et al , 2018 ) .
Homogeneous Entities .
The context contains multiple homogeneous entity mentions with idenText 1 : ‚Äú ... ‚Äù said Joseph Bastianich , who owns Del Posto with his mother , Lidia Bastianich , and the chef , Mario Batali .
Annotation : NA Prediction : children_of Indicators : Distractor , Homogeneous Entities Interpretation : Three entity mentions with the same type of PERSON are mentioned in the text and the word mother may lead to wrong prediction children .
Text 2 : ... Lieberman , who was defeated by the political upstart Ned Lamont in Connecticut ‚Äôs Democratic primary earlier this month .
Annotation : place_lived Prediction : place_of_birth Indicators : Similar Relations Interpretation : The relation place_lived and place_of_birth are similar in semantics .
Text 3 : One of the most brutal tyrants of recent history , Saddam Hussein unleashed devastating regional wars and reduced oil - rich Iraq to a claustrophobic police state .
Annotation : nationality Prediction : place_of_death Indicators : Reasoning Interpretation : Reasoning is required to get the relation nationality based on the context that Hussein is the tyrants of Iraq .
Table 1 : Examples of hard cases in NYT .
The head and tail mentions are colored accordingly .
tical types .
We observe the high error rate in relations like children and parents when the text mentions different entities with type PERSON .
Similar Relations .
Models struggle to identify the correct relation among those semantically similar ones concurrently mentioned in context .
A sharp decrease is also found in few - shot RE when selecting N similar relations on N - way K - shot settings ( Han et al , 2020 ) .
Long - tail Relations .
Only a handful instances are available for long - tail relations in common datasets .
Current data - hungry models struggle to learn the semantic patterns on these relations .
Multiple Triples .
Models always get a poor performance on the instances with numerous triples .
Overlapping Triples .
Different triples involve the identical entity mentions .
Many existing models can not well handle the EntityPairOverlap and SingleEntityOverlap ( Zeng et al , 2018 ) instances .
Table 1 provides various examples from NYT and corresponding hard case indicators .
In Table 2 , the proportion growing on the error instances reflects the gap between existing datasets and practical data , which also proves the effectiveness of these indicators .
2821  Figure 2 : The case - oriented construction framework of building HacRED which consists of four stages .
The right part correspondingly describes each stage .
Through the construction , the texts and triples are established .
Indicator
WebNLG
DuIE
Text Length Argument Distance Distractors Reasoning Homogeneous Ent .
Similar Rel .
Long - tail Rel .
Multiple Triples Overlapping Triples
original 18 12 1 2 9 1 17 25
error 39 30 5 3 34 54 5 59 64
original 3 5 4 1 19 27 8 16
error 32 17 13 9 21 17 2 93 33
Table 2 : The proportion of indicators in randomly selected samples of original test set and error - prone instances .
Note that one case may fit multiple indicators .
4 HacRED Dataset Construction
The overall architecture of the proposed caseoriented construction framework is illustrated in Figure 2 .
Different from previous works ( Zhang et al , 2017 , Zaporojets et al , 2020 ) which start crowdsourcing annotation straight after the data collection stage , we introduce additional stages of hard case feature engineering and target instance prediction .
Moreover , we design a novel three - stage annotation method and employ CrowdTruth2.0 .
4.1 Data Collection
To avoid data bias to high - frequency entities and relations , we first obtain about 5 million plain texts and 800 thousand triples from CN - DBpedia .
The abundant texts and triples contribute to a more reasonable distribution .
We use fine - grained named entity recognition ( NER ) toolkit TexSmart ( Zhang et al , 2020 ) and entity linking ( Chen et al , 2018 ) to align mentioned entities in texts to those in triples .
Finally , we construct a distantly supervised dataset Dds with 1.6 million instances , where we select
challenging instances in the following steps .
4.2 Hard Case Feature Engineering and Seed
Selection
To build a dataset toward practical hard cases , we systematically formulate the nine indicators of hard cases ( refer to Section 3 ) and introduce measurements to quantify them .
For example , we calculate the Argument Distance as the number of tokens between the head and tail entity mentions in the text .
More details of feature engineering are described in Appendix A.
After hard case oriented feature engineering , we discard the instances in Dds without any indicator of hard cases .
The remaining part forms a hard case candidate dataset D with about 108 thousand instances .
We randomly sample 3,500 instances from D and ask experts to select the hard cases given the context and features .
Specifically , if an instance with multiple hard case indicators or with only one indicator but selected by all three experts based on their expertise , it is regarded as a hard case .
To further evaluate the quality of selected hard cases , we utilize DGCNN - BERT to test the selected and unselected data .
If the F1 score drops Œ¥=10 % on the hard cases , we reserve the data to constitute the high quality seeds of hard case Dp .
The remaining data is easy case Dn .
In total , we obtain 1,431 seeds of hard cases .
4.3 Classifier Training and Hard Case
Prediction
It is impossible to manually select all instances to construct a large - scale dataset .
So we utilize a classifier to recall more hard cases similar to the seed
2822TextTriplesMicrosoft   ‚Ä¶ .. , founder_of , .... , found_time , .. ‚Ä¶ ‚Ä¶ Hard Case IndicatorsFilter Data CollectionHard Case FeatureEngineering   and Seed Selection Target Instances Prediction AnnotationCN - DBpedia‚ë†easy casescandidate   hard casestext ; triples ‚Ä¶ ùë´ùë´ùíÖùíÖùíÖùíÖtext ; triplestext ; triplesexploratoryanalysisDeep Neural Network Decision Treebinary   cross entropy lossnnPUlossweighted averagePatterns Matchingùë´ùë´ùíèùíèùë´ùë´ùíñùíñùë´ùë´ùíëùíëManual Selectionseed of hard casesseed of easy casesrandomlysamplingunlabeleddataùë´ùë´ùíâùíâùíâùíâCrowdTruth2.0‚ë°‚ë¢Knowledge BaseHard CaseExploratoryAnalysis‚ë£‚ë£‚ë¢‚ë°‚ë†Bill Gates , founder_of , MicrosoftMicrosoft   was founded in 1975 byBill
Gates , ‚Ä¶ triple 2text 2distant supervisionùë´ùë´ùíÖùíÖùíÖùíÖPU learningclassical   learningEnsemble Classifier ùë´ùë´Deep Neural Network Triple Schemarecommendation Triple AnnotationCrowdTruth2.0CrowdTruth2.0NER Toolkitrecommendation Entity AnnotationRelational   Pattern recommendation Relation Annotation  samples selected by experts .
The classifiers consist of three categories : ( 1 ) Decision tree ( Quinlan , 1986 ) ; ( 2 ) Deep classifiers by positive negative ( PN ) learning ( Rakhlin , 2016 ) ; ( 3 ) Deep classifiers by positive unlabeled ( PU ) learning ( Kiryo et al , 2017 ; du Plessis et al , 2015 ) .
First of all , we adopt the decision tree to make the classifier aware of the indicators explicitly .
Then , we form the representation vector as recommended in Baldini Soares et al ( 2019 ) and utilize classical PN learning on Dp and Dn to train the basic classifiers .
Since the easy cases are extremely diverse and Dn can not represent the entire distribution of easy cases , we leverage the massive unlabeled data in Dds by introducing PU learning to improve the generalization of hard cases classification .
Besides , we train deep models based on different neural network structures , including CNN ( LeCun et al , 1998 ) and BiLSTM ( Hochreiter and Schmidhuber , 1997 ) , to capture the context information .
More training details can be found in Appendix B.
We ensemble multiple classifiers by weighted average and distinguish hard cases with high confidence in the original massive unlabeled dataset .
Besides , we directly select instances by implicit semantic patterns to explore more hard cases fitting the indicator of Reasoning which is not well quantified by the auxiliary features .
Finally , we obtain the dataset Dhc ready for annotation .
4.4 Crowdsourcing
To make instances in Dhc fully and accurately labeled , we develop a novel three - stage RE annotation platform taking the following two aspects into consideration : ( 1 ) Heavy workload of annotating all information at once results in growing negative feedback as the task goes on ; ( 2 ) Aggregated method , such as majority vote ( Dumitrache et al , 2018 ) , is insufficient for complicated and openended tasks .
To relieve the pressure of workers , we divide the whole task into three partitions consisting of Relation Annotation , Entity Annotation , and Triple Annotation .
Moreover , we utilize patterns and toolkits to provide high - quality recommendations in each stage for higher recall .
To capture the label disagreement more thoroughly among workers , we employ CrowdTruth2.0 ( Dumitrache et al , 2018 ) , which models the quality of workers , documents , and annotations .
In short , in the Relation Annotation , workers select the missed relations or delete wrong recommended ones .
When all relations are annotated , NER toolkit recommends multiple entity mentions with the corresponding type based on schema information .
Workers also need to append new entity mentions or delete incorrect ones in the Entity Annotation .
As for Triple Annotation , workers verify the correctness of a candidate triples automatically generated by permutation of entity arguments and relations based on schema .
Note that every input data in the three stage is assigned to three different annotators and aggregated by CrowdTruth2.0 .
Detailed annotation process is in Appendix D.
5 Experiments
In this section , we first compare our HacRED with existing datasets .
Then we re - evaluate the SOTA RE models on HacRED and systematically analyze their abilities on different experiment settings .
At last , we demonstrate the effectiveness of HacRED via a case study .
5.1 Data Analysis
In this section , we analyze various aspects of common RE datasets and HacRED .
Data Size .
As shown in Table 3 , HacRED has a greater average number of words , entities , and triples in each text than all of the sentencelevel datasets .
Thus we regard HacRED as a document - level RE dataset .
Compared with the document - level datasets , DocRED aims at common document - level RE but not consider performance gaps and various hard cases in practical scenarios .
BC5CDR is specially designed for biomedical domain .
By contrast , we are the first to analyze the performance gap between popular datasets and practical applications , and propose HacRED which focuses on different kinds of hard cases in general domain .
Besides , HacRED is larger in scale and contains much more various relational facts than BC5CDR and DocRED but with lower duplicated triples ratio .
Data Distribution .
We calculate three global statistic metrics about data distribution of common datasets and HacRED .
Table 4 show the results .
Specifically , 84.29 % of the triples in NYT and 91.20 % in WebNLG are duplicate , which results in a bias to high - frequency triples of same entity pairs ( known as semantic bias for models ) .
For example , ( Beijing , capital_of , China ) occurs frequently in corpus and models still extract this triple from Beijing is a historic city in China .
Mean2823  Dataset
# Text
# Relation
# Triple
# Fact
Avg . Sent .
Avg . Word‚Ä°
Avg . Ent .
Avg .
Triple
SemEval10 NYT WebNLG TACRED
13,434 66,194 6,222 106,264
BC5CDR DocRED HacRED
1,500 5,053 9,231
sentence - level dataset 10,251 16,387 1,275 5,976
13,434 104,339 14,485 21,773
document - level dataset
3,116 63,427 67,047
2,434 56,354 65,225
10 24 171 41
1 96 26
1.0 2.1 2.5 1.0
7.4 8.0 5.0
17.4 37.8 24.0 33.2
188.0 198.3 126.6
2.0 2.2 3.15 2.0
19.5 26.2 10.8
1.0 1.6 2.3 1.0
2.1 12.5 7.4
Table 3 : Statistics of common RE datasets and HacRED .
Note that the Avg . Word is computed at word - level vocabulary , which means ‚Äú ‰∏≠ÂõΩ‚Äù(China ) , two characters in Chinese , is regarded as one word .
The average length of documents at character - level is 204.2 in HacRED .
Dataset
Duplicated Triples
Biased Relations
Top 20 % Relation Triples
SemEval10 NYT WebNLG TACRED
BC5CDR DocRED HacRED
sentence - level dataset
0.00 % 58.33 % 94.74 % 9.52 %
23.69 % 84.29 % 91.20 % 72.55 % document - level dataset 21.89 % 11.15 % 2.72 %
12.50 % 0.00 %
44.92 % 98.93 % 77.57 % 91.33 %
71.46 % 49.96 %
Table 4 : Data distributions of common RE datasets and HacRED .
The ratio of duplicate triples , biased relations , and top 20 % relation triples is calculated as 1‚àí # F acts , respectively .
If the highest - frequency mention is involved in more than 10 % triples of the given relation , we regard it as a biased relation .
, # T riples of top20 % Rel # T riples
# T riples , # Biased Rel
# Rel
Dataset
Relation Example
Highest - frequency Mention ( Ratio )
WebNLG NYT DocRED HacRED
county_seat person_profession sister_city dynasty
Texas ( 72.73 % ) Bavetta ( 50.00 % ) Chipilo ( 35.29 % ) Tang ( 4.20 % )
Table 5 : Example of relations which could lead to selection bias in WebNLG , NYT , and DocRED .
In HacRED , the ratio of the highest - frequency mention in all relations is only 4.20 % .
while , the top 20 % relations in NYT nearly cover the entire relation triples .
The numbers of top and last 20 % relation triples in WebNLG , TACRED and DocRED also vary greatly .
As a result , models perform well on popular relations but fail on longtail ones .
The experiments in the Section 5.4 prove this and we regard it as relation bias .
In addition , 94.74 % relations in WebNLG , 58.33 % in NYT and 12.5 % in DocRED contribute to the selection bias .
In WebNLG , 72.73 % triples with relation county_seat involve the mention Texas , as illustrated in Table 5 .
Models could memorize the cooccurrence between high - frequency mentions
Indicators Text Length & Argument Distance Distractors & Reasoning Homo .
Entities & Similar Relations Long - tail Relations
1 - 3 4 - 9 10 - 15 16 +
Multiple Triples
Overlapping Triples
Ratio 25.40 % 21.20 % 9.67 % 13.66 % 38.87 % 36.67 % 14.27 % 10.20 % 13.20 %
Table 6 : Statistics about the proportion of instances fitting different hard case indicators on HacRED .
CrowdTruth 2.0
Human ( % )
Avg . UQS ‚Üë Avg . AQS ‚Üë Avg . WQS ‚Üë
0.9373 0.9446 0.9557
Precision Recall F1
97.29 94.64 95.94
Table 7 : Results of different quality metrics on HacRED .
and the relation while low - frequency mentions are neglected .
All these three aspects reveal the unreasonable data distribution of common datasets .
In comparison , we observe a more reasonable data distribution in HacRED from Table 4 and Table 5 .
HacRED has a low ratio of duplicate triples and contains various relational facts , which addresses semantic bias .
No biased relation existing in HacRED reduces the risk of selection bias .
The proportion of top 20 % relations promotes the alleviation of relation bias on HacRED .
The more comparison of overall data distribution can be found in Appendix E. Data Quality .
We evaluate the quality of HacRED through both automatic metrics and human evaluation .
Specifically , we first compute the average unit quality score ( UQS ) , annotation quality score ( AQS ) , and worker quality score ( WQS ) of the whole 9,231 instances .
UQS , AQS and WQS are proposed by CrowdTruth2.0 ( Appendix F provides more calculation details ) .
The closer these
2824  Model Joint NovelTagging CopyRE GraphRel CasRel
NovelTagging CopyRE GraphRel CasRel
Pipeline PURE
PURE Doc .
Level LSR GAIN ATLOP SSAN
Precision Recall
F1
46.77 75.04 85.14 75.43
30.51 13.11 30.13 55.24
72.23
NER‚Ä°
35.07 51.38 69.69 62.88 End - to - end
2.91 9.64 35.62 43.78
NER‚Ä°
63.45 End - to - end 66.09
40.08 61.00 76.64 68.59
5.31 11.12 32.65 48.85
67.56
60.12
55.14 Relation Classification 69.70 72.04 77.89 60.01
67.17 80.62 76.55 62.03
68.41 76.09 77.21 61.00
Table 8 : Model performance on HacRED test set(% ) .
NER results are computed based on the entities involved in the gold triples of each instance .
scores are to 1 , the higher quality of the crowdsourcing is .
Meanwhile , we randomly sample 400 instances from HacRED and compute the precision , recall , and F1 score with annotations based on the revision of humans .
The evaluation scores are reported in Table 7 .
From this table , our HacRED achieves a considerable annotation quality .
As a comparison , NYT contains about 31 % noise instances ( Riedel et al , 2010 ) and TACRED has poor annotation quality ( Alt et al , 2020 ) .
Hard Case Types .
We group the randomly sampled 400 instances into nine categories as shown in Table 6 .
The proportions of different kinds of instances reflect that HacRED contains a various range of hard cases , which evaluates models comprehensively for practical applications .
5.2 Model Evaluation
As DGCNN - BERT has been used in the main process of construction , we evaluate other strong RE models including joint RE models , pipeline RE models , and DocRC models on HacRED .
First , we limit the relation set within 20 types both in HacRED and DuIE , and then separate a part of instances in DuIE to form the contrastive easy case dataset Dec. We carry out the equivalent substitution of hard cases in HacRED for easy ones in Dec in different proportions .
Figure 3 shows the the proporF1 curve of the performances w.r.t . tion of substitution .
As the ratio of replacement increases , models generally have a growing trend
Model
CasRel PURE Human
ATLOP Human
F1
Precision Recall End - to - end 45.43 65.15 84.59
58.76 56.52 90.21 Relation Classification 78.33 96.21
76.70 93.03
51.24 60.53 87.31
77.51 94.59
Table 9 : Human performance ( % ) .
in performance .
The SOTA model CasRel still outperforms other joint models and achieves great F1 on 100 % Dec.
However , the performance drops on data with more complex instances .
We notice that F1 value of easy cases is generally greater than that of hard cases in different substitution ratio settings , which illustrates that RE models indeed struggle when tackling hard cases .
Note that by combining HacRED with easy cases in existing datasets , it is easy to simulate diverse practical scenarios .
In addition , we split HacRED into train , dev , and test sets with 6231 , 1500 , 1500 instances respectively .
The precision , recall , and F1 score of the three major categories of models are shown in Table 8 .
The joint and pipeline learning strategies do not contribute to a great F1 on triple extraction .
For the NER task , PURE has a separate entity model but results in a 30.61 % F1 when all entities in a document are considered , including entities with no positive relation labels .
This also reflects the challenge to obtain complete entity information in practical scenarios .
On the other hand , the relation classification performances of DocRC models are far from satisfactory .
The results suggest that existing models have remarkably poor performance on HacRED compared with humans ( Table 9 ) , which indicates that RE applicable for practical hard cases still requires further research .
5.3 Human Performance
We randomly select 200 contexts from test set and ask three volunteers to extract relational facts in an end - to - end manner .
Schema information like entity type set as well as relation set is provided but no entity mentions .
As for relation classification task , three volunteers select the relation , including NA regarded as negative , of the given entity pair .
As demonstrated in Table 9 , humans fulfill excellent results which indicate the possible ceiling performance on HacRED .
2825  Figure 3 : The F1 curve of the model performance on different mix ratios of hard and easy cases .
Model
NovelTagging CopyRE GraphRel CasRel
Text Length Argument Distance
Homo .
Ent .
Similar Rel .
Long - tail Rel .
Overlapping Triples
Distractor Reasoning
Overall
4.99 5.47 30.15 45.34
4.33 3.90 27.82 45.60
1.72 1.28 0.08 13.54
3.99 6.59 34.67 53.34
9.23 7.30 29.81 44.00
5.31 11.12 32.65 48.85
Table 10 : F1 score on HacRED instances with different indicators of hard cases ( % ) .
5.4 Detailed Analysis
In this section , we give insight into the abilities of current mainstream joint models when tackling different kinds of hard cases and propose some research indications as well .
As it is hard to obtain complete entity information in practical scenarios , we do not consider DocRC models in this section that entity information is provided as input .
Multiple Triples .
Table 11 shows the F1 score of existing models when extracting from texts with different number of triples .
The performance of NovelTagging and CopyRE decreases as the number of triples increases , which indicates that the novel tagging schema and multiple decoder mechanism are not able to address the challenge of Multiple Triples .
Since GraphRel predicts relations for all word pairs and CasRel learns separate entity tagger for different relations , these two models alleviate this problem .
An interesting point is that the performance of GraphRel and CasRel rises as the number of triples increases when the triples number is less than 16 , indicating that these two models work well in texts with number of triples nearing the average .
However , all models get F1 score below average when text mentions have more than 16 triples .
Text Length and Argument Distance .
To assess the abilities of models in capturing the long - distance context , we provide the evaluation on instances with indicators of Text Length and Argument Distance in Table 10 .
The GCNbased models ( i.e. , GraphRel ) outperforms the
Model
Number of triples
NovelTagging CopyRE GraphRel CasRel
1 - 3 17.92 12.69 29.49 43.42
4 - 9 12.18 10.58 35.23 51.05
10 - 15 8.60 8.82 37.04 54.90
16 + 3.29 3.38 29.24 43.18
Table 11 : F1 score on HacRED test set with different number of triples ( % ) .
BiLSTM - based neural models like NovelTagging and CopyRE .
The performance improvement on CasRel suggests the powerfulness of BERT encoder in the long - distance context .
Homogeneous Entities and Similar Relations .
Since the text mentions multiple homogeneous entities and semantically similar relations , models are required to distinguish the fine - grained difference of the context to extract the correct triples .
The first two columns in Table 10 have similar results , which indicates that the contexts with homogeneous entities and similar relations are as challenging as the long - distance contexts .
Long - tail Relations .
We observe a dramatic decrease on the instances with long - tail relational triples .
As long - tail relations are common in realworld scenarios , a more efficient learning method is required to make RE models applicable for practical applications .
Overlapping Triples .
CasRel achieves a better performance on extracting overlapping triples .
This proves the effectiveness of cascade binary tagging strategy by first identifying the head mention and then extract the corresponding tail mention given a relation .
Specifically , the F1 scores of
28260.00.20.40.60.81.0Substitution Ratio0102030405060708090F1 Score5.6811.531.3348.786.5714.4827.6148.079.1816.6226.7951.114.4122.6226.3953.6523.0528.5733.0854.8644.8345.4356.4564.07F1 curve on entire mixed datasetNovelTaggingCopyREGraphRelCasRel0.00.20.40.60.81.0Substitution Ratio010203040506070809031.334.8728.5458.133.3731.1463.5839.7736.563.5642.3941.4864.7945.4356.4564.07F1 curve on easy casesNovelTaggingCopyREGraphRelCasRel0.00.20.40.60.81.0Substitution Ratio01020304050607080905.6811.531.3348.785.1812.9927.5648.895.2713.3626.2148.466.4712.9222.7547.089.5513.9820.6942.77F1 curve on hard casesNovelTaggingCopyREGraphRelCasRel  6 Conclusion
In order to effectively evaluate the RE models and accelerate the research of practical RE , we first analyze the performance gap between popular datasets and practical applications .
Therefore , we construct a large - scale and high - quality HacRED with reasonable data distribution and sufficient hard cases .
To focus on the practical challenging cases , we propose a case - oriented construction framework .
We also design a novel annotation method to guarantee the quality of HacRED .
Finally , we conduct extensive experiments and analyze the abilities of SOTA models from various aspects , which provides a deeper understanding of RE models and inspiration for further improvement .
