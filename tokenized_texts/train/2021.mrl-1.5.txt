Specializing Multilingual Language Models : An Empirical Study
Ethan C. Chau† Noah A. Smith†⋆ †Paul G. Allen School of Computer Science & Engineering , University of Washington ⋆Allen Institute for Artificial Intelligence { echau18,nasmith}@cs.washington.edu
Abstract
Pretrained multilingual language models have become a common tool in transferring NLP capabilities to low - resource languages , often with adaptations .
In this work , we study the performance , extensibility , and interaction of two such adaptations : vocabulary augmentation and script transliteration .
Our evaluations on part - of - speech tagging , universal dependency parsing , and named entity recognition in nine diverse low - resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low - resource settings .
1
Introduction
Research in natural language processing is increasingly carried out in languages beyond English .
This includes high - resource languages with abundant data , as well as low - resource languages , for which labeled ( and unlabeled ) data is scarce .
In fact , many of the world ’s languages fall into the latter category , even some with a high number of speakers .
This presents unique challenges compared to high - resource languages : effectively modeling low - resource languages involves both accurately tokenizing text in such languages and maximally leveraging the limited available data .
One common approach to low - resource NLP is the multilingual paradigm , in which methods that have shown success in English are applied to the union of many languages ’ data,1 enabling transfer between languages .
For instance , multilingual contextual word representations ( CWRs ) from language models ( Devlin et al , 2019 ; Huang et al , 2019 ; Lample and Conneau , 2019 , inter alia ) are conventionally “ pretrained ” on large multilingual
corpora before being “ finetuned ” directly on supervised tasks ; this pretraining - finetuning approach is derived from analogous monolingual models ( Devlin et al , 2019 ; Liu et al , 2019 ; Peters et al , 2018 ) .
However , considering the diversity of the world ’s languages and the great data imbalance among them , it is natural to question whether the current multilingual paradigm can be improved upon for low - resource languages .
Indeed , past work has demonstrated that it can .
For instance , Wu and Dredze ( 2020 ) find that multilingual models often lag behind non - contextualized baselines for the lowest - resource languages in their training data , drawing into question their utility in such settings .
Conneau et al ( 2020a ) posit that this phenomenon is a result of limited model capacity , which proves to be a bottleneck for sufficient transfer to low - resource languages .
In fact , with multilingual models only being pretrained on a limited set of languages , most of the world ’s languages are unseen by the model .
For such languages , the performance of such models is even worse ( Chau et al , 2020 ) , due in part to the diversity of scripts across the world ’s languages ( Muller et al , 2021 ; Pfeiffer et al , 2021b ;
Rust et al , 2021 ) as compared to the models ’ Latin - centricity ( Ács , 2019 ) .
Nonetheless , there have been multiple attempts to remedy this discrepancy by specializing2 a multilingual model to a given target low - resource language , from which we take inspiration .
Among them , Chau et al ( 2020 ) augment the model ’s vocabulary to more effectively tokenize text , then pretrain on a small amount of data in the target language ; they report significant performance improvements on a small set of low - resource languages .
In a similar vein , Muller et al ( 2021 ) propose to transliterate text in the target language
1Within the multilingual paradigm , a distinction is sometimes made between massively multilingual methods , which consider tens or hundreds of languages ; and polyglot methods , which use only a handful .
In this paper , all mentions of “ multilingual ” refer to the former .
2We use specialization to denote preparing a model for use on a specific target language , to the exclusion of others .
This is a subset of adaptation , which includes all techniques that adjust a model for use on target languages , regardless of their resulting universality .
to Latin script to be better tokenized by the existing model , followed by additional pretraining ; they observe mixed results and note that transliteration quality may be a confounding factor .
We hypothesize that these two methods can serve as the basis for improvements in modeling a broad set of low - resource languages .
In this work , we study the effectiveness , extensibility , and interaction of these two approaches to specialization : the vocabulary augmentation technique of Chau et al ( 2020 ) and the script transliteration method of Muller et al ( 2021 ) .
We verify the performance of vocabulary augmentation on three tasks in a diverse set of nine low - resource languages across three different scripts , especially on non - Latin scripts ( § 2 ) and find that these gains are associated with improved vocabulary coverage of the target language .
We further observe a negative interaction between vocabulary augmentation and transliteration in light of a broader framework for specializing multilingual models , while noting that vocabulary augmentation offers an appealing balance of performance and cost ( § 3 ) .
Overall , our results highlight several possible directions for future study in the low - resource setting .
Our code , data , and hyperparameters are publicly available.3
2 Revisiting Vocabulary Augmentation
We begin by revisiting the Vocabulary Augmentation method of Chau et al ( 2020 ) , which we recast more generally in light of recent work ( § 2.1 ) .
We evaluate their claims on three different tasks , using a diverse set of languages in multiple scripts ( § 2.2 ) , and find that the results hold to an even more pronounced degree in unseen low - resource languages with non - Latin scripts ( § 2.3 ) .
2.1 Method Overview
Following Chau et al ( 2020 ) , we consider how to apply the pretrained multilingual BERT model ( MBERT ; Devlin et al , 2019 ) to a target lowresource language , for which both labeled and unlabeled data is scarce .
This model has produced strong CWRs for many languages ( Kondratyuk and Straka , 2019 , inter alia ) and has been the starting model for many studies on low - resource languages ( Muller et al , 2021 ;
Pfeiffer et al , 2020 ; Wang et al , 2020 ) .
MBERT covers the languages with the 104 largest Wikipedias , and it uses this data to con3https://github.com/ethch18/
specializing - multilingual
struct a wordpiece vocabulary ( Wu et al , 2016 ) and train its transformer - based architecture ( Vaswani et al , 2017 ) .
Although low - resource languages are slightly oversampled , high - resource languages still dominate both the final pretraining data and the vocabulary ( Ács , 2019 ; Devlin et al , 2019 ) .
Chau et al ( 2020 ) note that target low - resource languages fall into three categories with respect to MBERT ’s pretraining data : the lowest - resource languages in the data ( Type 1 ) , completely unseen low - resource languages ( Type 2 ) , and low - resouce languages with more representation ( Type 0).4 Due to their poor representation in the vocabulary , Type 1 and Type 2 languages achieve suboptimal tokenization and higher rates of the “ unknown ” wordpiece5 when using MBERT out of the box .
This hinders the model ’s ability to capture meaningful patterns in the data , resulting in reduced data efficiency and degraded performance .
We note that this challenge is exacerbated when modeling languages written in non - Latin scripts .
MBERT ’s vocabulary is heavily Latin - centric ( Ács , 2019 ; Muller et al , 2021 ) , resulting in a significantly larger portion of non - Latin scripts being represented with “ unknown ” tokens ( Pfeiffer et al , 2021b ) and further limiting the model ’s ability to generalize .
In effect , MBERT ’s low initial performance on such languages can be attributed to its inability to represent the script itself .
To alleviate the problem of poor tokenization , Chau et al ( 2020 ) propose to specialize MBERT using Vocabulary Augmentation ( VA ) .
Given unlabeled data in the target language , they train a new wordpiece vocabulary on the data , then select the 99 most common wordpieces in the new vocabulary that replace “ unknown ” tokens under the original vocabulary .
They then add these 99 wordpieces to the original vocabulary and continue pretraining MBERT on the unlabeled data for additional steps .
They further describe a tiered variant ( TVA ) , in which a larger learning rate is used for the embeddings of these 99 new wordpieces .
VA yields strong gains over unadapted multilingual language models on dependency parsing in four low - resource languages with Latin scripts .
How4Muller et al ( 2021 ) further subdivide Type 2 into Easy , Medium , and Hard languages , based on the performance of MBERT after being exposed to these languages .
However , this categorization can not be determined a priori for a given language .
5The “ unknown ” wordpiece is inserted when the wordpiece algorithm is unable to segment a word - level token with the current vocabulary .
ever , no evaluation has been performed on other tasks or on languages with non - Latin scripts , which raises our first research question :
RQ1 : Do the conclusions of Chau et al ( 2020 ) hold for other tasks and for languages with nonLatin scripts ?
We can view VA and TVA as an instantation of a more general framework of vocabulary augmentation , shared by other approaches to using MBERT in low - resource settings .
Given a new vocabulary V , number of wordpieces n , and learning rate multiplier a , the n most common wordpieces in V are added to the original vocabulary .
Additional pretraining is then performed , with the embeddings of the n wordpieces taking on a learning rate a times greater than the overall learning rate .
For VA , we set n = 99 and a = 1 , while we treat a as a hyperparameter for TVA .
The related E - MBERT method of Wang et al ( 2020 ) sets n = |V | and a = 1 .
Investigating various other instantiations of this framework is an interesting research direction , though it is out of the scope of this work .
2.2 Experiments
We expand on the dependency parsing evaluations of Chau et al ( 2020 ) by additionally considering named entity recognition and part - of - speech tagging .
We follow Kondratyuk and Straka ( 2019 ) and compute the CWR for each token as a weighted sum of the activations at each MBERT layer .
For dependency parsing , we follow the setup of Chau et al ( 2020 ) and Muller et al ( 2021 ) and use the CWRs as input to the graph - based dependency parser of Dozat and Manning ( 2017 ) .
For named entity recognition , the CWRs are used as input to a CRF layer , while part - of - speech tagging uses a linear projection atop the representations .
In all cases , the underlying CWRs are finetuned during downstream task training , and we do not add an additional encoder layer above the transformer outputs .
We train models on five different random seeds and report average scores and standard errors .
2.2.1 Languages and Datasets
We select a set of nine typologically diverse lowresource languages for evaluation , including three of the original four used by Chau et al ( 2020 ) .
These languages use three different scripts and are chosen based on the availability of labeled datasets and their exemplification of the three language types identified by Chau et al ( 2020 ) .
Of the languages seen by MBERT , all selected Type 0 languages are within the 45 largest Wikipedias , while the remaining Type 1 languages are within the top 100 .
The Type 2 languages , which are excluded from MBERT , are all outside of the top 150.6 Additional information about the evaluation languages is given in Tab .
1 .
Unlabeled Datasets Following Chau et al ( 2020 ) , we use articles from Wikipedia as unlabeled data for additional pretraining in order to reflect the original pretraining data .
We downsample full articles from the largest Wikipedias to be on the order of millions of tokens in order to simulate a low - resource unlabeled setting , and we remove sentences that appear in the labeled validation or test sets .
Labeled Datasets For dependency parsing and part - of - speech tagging , we use datasets and train / test splits from Universal Dependencies ( Nivre et al , 2020 ) , version 2.5 ( Zeman et al , 2019 ) .
POS tagging uses language - specific partof - speech tags ( XPOS ) to evaluate understanding of language - specific syntactic phenomena .
The Belarusian treebank lacks XPOS tags for certain examples , so we use universal part - of - speech tags instead .
Dependency parsers are trained with gold word segmentation and no part - of - speech features .
Experiments with named entity recognition use the WikiAnn dataset ( Pan et al , 2017 ) , following past work ( Muller et al , 2021 ; Pfeiffer et al , 2020 ;
Wu and Dredze , 2020 ) .
Specifically , we use the balanced train / test splits of ( Rahimi et al , 2019 ) .
We note that UD datasets were unavailable for Meadow Mari , and partitioned WikiAnn datasets were missing for Wolof .
2.2.2
Baselines To measure the effectiveness of VA , we benchmark it against unadapted MBERT , as well as directly pretraining MBERT on the unlabeled data without modifying the vocabulary ( Chau et al , 2020 ; Muller et al , 2021 ; Pfeiffer et al , 2020 ) .
Following Chau et al ( 2020 ) , we refer to the latter approach as language - adaptive pretraining ( LAPT ) .
We also evaluate two monolingual baselines that are trained on our unlabeled data : fastText embeddings ( FASTT ; Bojanowski et al , 2017 ) , which represent a static word vector approach ; and a BERT model trained from scratch ( BERT ) .
For
6Based
on
https://meta.wikimedia.org/
wiki / List_of_Wikipedias .
Language Bulgarian ( BG ) Belarusian ( BE ) Meadow Mari ( MHR ) Vietnamese ( VI ) Irish ( GA ) Maltese ( MT ) Wolof ( WO ) Urdu ( UR ) Uyghur ( UG )
Type Script Cyrillic 0 Cyrillic 0 Cyrillic 2 Latin 0
Latin 1 Latin 2 Latin 2 Perso - Arabic 0 Perso - Arabic Turkic 2
Family Slavic Slavic Uralic Viet - Muong Celtic Semitic Niger - Congo Indic
# Sentences 357k
187k 52k 338k 274k 75k 15k 201k 136k
# Tokens Downsample % # WP / Token 1.81 2.25 2.37 1.17 1.83 2.39 1.78 1.58 2.54
5.6 M 2.7 M 512k 6.9 M 5.8 M 1.4 M 396k 3.6 M 2.3 M
10 % 10 % – 5 % – – – 20 % –
Table 1 : Language overview and unlabeled dataset statistics : number of sentences , number of tokens , and average wordpieces per token under the original MBERT vocabulary .
BERT , we follow Muller et al ( 2021 ) and train a six - layer RoBERTa model ( Liu et al , 2019 ) with a language - specific SentencePiece tokenizer ( Kudo and Richardson , 2018 ) .
For a fair comparison to VA , we use the same task - specific architectures and modify only the input representations .
2.2.3
Implementation Details
To pretrain LAPT and VA models , we use the code of Chau et al ( 2020 ) , who modify the pretraining code of Devlin et al ( 2019 ) to only use the masked language modeling ( MLM ) loss .
To generate VA vocabularies , we train a new vocabulary of size 5000 and select the 99 wordpieces that replace the most unknown tokens .
We train with a fixed linear warmup of 1000 steps .
To pretrain BERT models , we use the HuggingFace Transformers library ( Wolf et al , 2020 ) .
Following Muller et al ( 2021 ) , we train a half - sized RoBERTa model with six layers and 12 attention heads .
We use a byte - pair vocabulary of size 52000 and a linear warmup of 1 epoch .
For LAPT , VA , and BERT , we train for up to 20 epochs total , selecting the highest - performing epoch based on validation masked language modeling loss .
FASTT models are trained with the skipgram model for five epochs , with the default hyperparameters of Bojanowski et al ( 2017 ) .
Training of downstream parsers and taggers follows Chau et al ( 2020 ) and Kondratyuk and Straka ( 2019 ) , with an inverse square - root learning rate decay and linear warmup , and layer - wise gradual unfreezing and discriminative finetuning .
Models are trained with AllenNLP , version 2.1.0 ( Gardner et al , 2018 ) , for up to 200 epochs with early stopping based on validation performance .
We choose batch sizes to be the maximum that allows for successful training on one GPU .
2.3 Results
Tab . 2 presents performance of the different input representations on POS tagging , dependency parsing , and named entity recognition .
VA achieves strong results across all languages and tasks and is the top performer in the majority of them , suggesting that augmenting the vocabulary addresses MBERT ’s limited vocabulary coverage of the target language and is beneficial during continued pretraining .
The relative gains that VA provides appear to correlate not only with language type , as in the findings of Chau et al ( 2020 ) , but also with each language ’s script .
For instance , in Vietnamese , which is a Type 0 Latin script language , the improvements from VA are marginal at best , reflecting the Latindominated pretraining data of MBERT .
Irish , the Type 1 Latin script language , is only slightly more receptive .
However , Type 0 languages in Cyrillic and Arabic scripts , which are less represented in MBERT ’s pretraining data , are more receptive to VA , with VA even outperforming all other methods for Urdu .
This trend is amplified in the Type 2 languages , as the improvements for Maltese and Wolof are small but significant .
However , they are dwarfed in magnitude by those of Uyghur , where VA achieves up to a 57 % relative error reduction over LAPT .
This result corroborates the findings of both Chau et al ( 2020 ) and Muller et al ( 2021 ) and answers RQ1 .
Prior to specialization , MBERT is especially poorly equipped to handle unseen lowresource languages and languages in non - Latin scripts due to its inability to model the script itself .
In such cases , specialization via VA is beneficial , providing MBERT with explicit signal about the target language and script while maintaining its language - agnostic insights .
On the other hand , this also motivates additional investigation into reme  Rep. FASTT BERT MBERT LAPT
VA
Rep. FASTT BERT MBERT LAPT
VA
Rep. FASTT BERT MBERT LAPT
VA
BE * ( 0 ) 68.84 ± 7.16 91.00 ± 0.30 94.57 ± 0.45 95.74 ± 0.44 95.28 ± 0.51
BG ( 0 ) 88.86 ± 0.37 94.48 ± 0.10 96.98 ± 0.08 97.15 ± 0.04 97.20 ± 0.06
GA ( 1 ) 86.87 ± 2.55 90.36 ± 0.20 91.91 ± 0.25 93.28 ± 0.19 93.33 ± 0.16
MT ( 2 ) 89.68 ± 2.15 92.61 ± 0.10 94.01 ± 0.17 95.76 ± 0.09 96.33 ± 0.09
UG ( 2 ) 89.45 ± 1.37 90.87 ± 0.13 78.07 ± 0.22 79.88 ± 0.27 91.49 ± 0.13
UR ( 0 ) 90.81 ± 0.31 89.88 ± 0.13 91.77 ± 0.18 92.18 ± 0.16 92.24 ± 0.16
VI ( 0 ) 81.84 ± 1.15 84.73 ± 0.13 88.97 ± 0.10 89.64 ± 0.20 89.49 ± 0.22
WO ( 2 ) 87.48 ± 0.55 87.71 ± 0.31 93.04 ± 0.20 94.58 ± 0.13 94.48 ± 0.20
BE ( 0 ) 35.81 ± 2.24 45.77 ± 1.35 71.83 ± 0.90 72.77 ± 1.12 73.22 ± 1.23
( a ) POS tagging ( accuracy ) .
* Belarusian uses universal POS tags .
BG ( 0 ) 84.03 ± 0.41 84.61 ± 0.27 91.62 ± 0.23 92.08 ± 0.31 91.90 ± 0.20
GA ( 1 ) 65.58 ± 1.21 64.02 ± 0.49 71.68 ± 0.62 74.79 ± 0.12 74.35 ± 0.22
UR ( 0 ) 79.33 ± 0.25 78.07 ± 0.22 81.45 ± 0.26 81.78 ± 0.44 81.88 ± 0.25
MT ( 2 ) 68.45 ± 1.40 65.92 ± 0.45 76.63 ± 0.35 81.53 ± 0.37 82.00 ± 0.31
UG ( 2 ) 54.52 ± 1.02 60.34 ± 0.27 47.70 ± 0.44 50.67 ± 0.34 67.55 ± 0.17
VI ( 0 ) 54.91 ± 0.79 54.70 ± 0.27 64.58 ± 0.42 66.15 ± 0.41 65.64 ± 0.12
WO ( 2 ) 70.39 ± 1.39 60.12 ± 0.39 76.24 ± 0.83 80.34 ± 0.14 80.22 ± 0.41
( b ) UD parsing ( LAS ) .
BE ( 0 ) 84.26 ± 0.86 88.08 ± 0.62 91.13 ± 0.07 91.61 ± 0.74 91.38 ± 0.56
BG ( 0 ) 87.98 ± 0.76 90.31 ± 0.20 92.56 ± 0.09 92.96 ± 0.13 92.70 ± 0.11
GA ( 1 ) 67.21 ± 4.30 76.58 ± 0.98 82.82 ± 0.57 84.13 ± 0.78 84.82 ± 1.00
MT ( 2 ) 33.53 ± 17.89 54.64 ± 3.51 61.86 ± 2.60 81.53 ± 2.33 80.00 ± 2.77
UG ( 2 ) – 61.54 ± 3.70 50.76 ± 1.86 56.76 ± 4.91 68.93 ± 3.30
UR ( 0 ) 92.85 ± 2.04 94.04 ± 0.55 94.60 ± 0.34 95.17 ± 0.29 95.43 ± 0.22
VI ( 0 ) 85.57 ± 1.98 88.08 ± 0.15 92.13 ± 0.27 92.41 ± 0.15 92.43 ± 0.16
MHR ( 2 ) 35.28 ± 13.81 54.17 ± 2.88 61.85 ± 3.25 59.17 ± 5.15 64.23 ± 3.07
Avg . 85.48 90.20 91.16 92.28 93.73
Avg . 64.13 64.19 72.72 75.01 77.09
Avg . 60.84 75.93 78.46 81.72 83.74
( c ) NER ( macro F1 ) . – indicates that a model did not converge .
Table 2 : Results on POS tagging , UD parsing , and NER , with standard deviations from five random initializations .
Bolded results are the maximum for each language , and scores in gray are not significantly worse than the best model ( 1 - sided paired t - test , p = 0.05 with Bonferonni correction ) .
dies for the script imbalance at a larger scale , e.g. , more diverse pretraining data .
2.4 Analysis
We perform further analysis to investigate VA ’s patterns of success .
Concretely , we hypothesize that VA significantly improves the tokenizer ’s coverage of target languages where it is most successful .
Inspired by Ács ( 2019 ) , Chau et al ( 2020 ) , and Rust et al ( 2021 ) , we quantify tokenizer coverage using the percentage of tokens in the raw text that yield unknown wordpieces when tokenized with a given vocabulary ( “ UNK token percentage ” ) .
These are tokens whose representations contain at least partial ambiguity due to the inclusion of the unknown wordpiece .
Tab . 3 presents the UNK token percentage for each dataset using the MBERT vocabulary , averaged over each script and language type .
This vocabulary is used in LAPT and represents the baseline level of vocabulary coverage .
We also include the change in the UNK token percentage between the MBERT and VA vocabularies , which quantifies the coverage improvement .
Both sets of values are juxtaposed against the average change in task - specific performance from LAPT to VA , representing the effect of augmenting the vocabulary on task - specific performance .
We observe that off - the - shelf MBERT already attains relatively high vocabulary coverage for Type 0 and 1 languages , as well as languages written in Latin and Cyrillic scripts .
On the other hand , up to one - fifth of the tokens in Arabic languages and one - sixth of those in Type 2 languages yield an unknown wordpiece .
For these languages , there is great room for increasing tokenizer coverage , and VA indeed addresses this more tangible need .
This aligns with the task - specific performance improvements for each group and helps to explain our results in § 2.3 .
It is notable that VA does not always eliminate the issue of unknown wordpieces , even in languages for which MBERT attains high vocabulary coverage .
This suggests that the remaining unknown wordpieces in these languages are more sparsely distributed ( i.e. , they represent low frequency sequences ) , while the unknown wordpieces in languages with lower vocabulary coverage represent sequences that occur more commonly .
As a result , augmenting the vocabulary in such languages quickly improves coverage while associating these commonly occurring sequences with each other , which benefits the overall tokenization quality .
We further explore the association between the improvements in vocabulary coverage and taskspecific performance in Fig .
1 .
Although we do not find that languages from the same types or scripts form clear clusters , we nonetheless observe a loose
Lang .
Group ( # of Langs . )
All ( 9 ) Type 0 ( 4 ) Type 1 ( 1 ) Type 2 ( 4 ) Latin ( 4 ) Cyrillic ( 3 ) Arabic ( 2 )
Avg . UNK Token % ( MBERT )
Avg . UNK Token % ( ∆ )
Avg . Task Performance ( ∆ )
Unlabeled 5.9 % ( – ) 1.0 % ( ↓ ) 0.3 % ( ↓ ) 12.3 % ( ↑ ) 1.2 % ( ↓ ) 3.6 % ( ↓ ) 19.0 % ( ↑ )
UD WikiAnn 6.2 % ( – ) 1.2 % ( ↓ ) 0.4 % ( ↓ ) 14.8 % ( ↑ ) 2.4 % ( ↓ ) 2.8 % ( ↓ ) 16.9 % ( ↑ )
5.2 % ( – ) 0.3 % ( ↓ ) 0.0 % ( ↓ ) 13.5 % ( ↑ ) 0.6 % ( ↓ ) 0.6 % ( ↓ ) 19.2 % ( ↑ )
Unlabeled – 5.3 % ( – ) – 0.9 % ( ↑ ) – 0.3 % ( ↑ ) – 10.8 % ( ↓ ) – 1.2 % ( ↑ ) – 3.6 % ( ↑ ) – 16.1 % ( ↓ )
UD 4.7 % ( – ) – 0.3 % ( ↑ ) – 0.00 % ( ↑ ) – 12.1 % ( ↓ ) – 0.6 % ( ↑ ) – 0.6 % ( ↑ ) – 17.0 % ( ↓ )
UD
POS
WikiAnn
NER – 5.8 % ( – ) +1.45 ( – ) +2.08 ( – ) +2.02 ( – ) – 0.04 ( ↓ ) – 1.2 % ( ↑ ) – 0.05 ( ↓ ) – 0.13 ( ↓ ) – 0.4 % ( ↑ ) +0.05 ( ↓ ) – 0.44 ( ↓ ) +0.69 ( ↓ ) – 13.7 % ( ↓ ) +4.03 ( ↑ ) +5.74 ( ↑ ) +5.23 ( ↑ ) – 0.15 ( ↓ ) – 2.3 % ( ↑ ) +0.09 ( ↓ ) – 0.27 ( ↓ ) – 2.7 % ( ↑ ) – 0.21 ( ↓ ) +0.14 ( ↓ ) +1.52 ( ↓ ) – 15.5 % ( ↓ ) +5.84 ( ↑ ) +8.49 ( ↑ ) +6.22 ( ↑ )
Table 3 : Average UNK token percentage under the MBERT vocabulary ( left ) ; change in UNK token percentage from MBERT to VA vocabularies ( center ) ; and average task performance change from LAPT to VA ( right ) .
Averages are computed overall and within each script and language type , with comparisons to the overall average ; all UNK token percentages are computed on the respective training sets for illustration .
Note that Uyghur accounts for a large portion of the behavior of the Type 2 / Arabic rows .
correlation between the two factors in question and see that VA delivers greater performance gains on Type 2 and Arabic - script languages compared to their Type 0/1 and Latin - script counterparts , respectively .
To quantify the strength of this association , we also compute the language - level Spearman correlation between the change in UNK token percentage on the unlabeled dataset7 from the MBERT to VA vocabulary and the task - specific performance improvements from LAPT to VA .
The resulting ρ - values – 0.29 for NER , 0.56 for POS tagging , and 0.81 for UD parsing – suggest that this set of factors is meaningful for some tasks , though additional and more fine - grained analysis in future work should give a more complete explanation .
3 Mix - in Specialization : VA and
Transliteration
We now expand on the observation made in § 2.3 regarding the difficulties that MBERT encounters when faced with unseen low - resource languages in non - Latin scripts because of its inability to model the script .
Having observed that VA is beneficial in such cases , we now investigate the interaction between this method and another specialization approach that targets this problem .
Specifically , we consider the transliteration methods of Muller et al ( 2021 ) , in which unseen low - resource languages in non - Latin scripts are transliterated into the Latin script , often using transliteration schemes inspired by the Latin orthographies of languages related to the target language .
They hypothesize that the increased similarity in the languages ’ writing systems , combined with MBERT ’s overall Latin - centricity , provides increased opportunity for crosslingual transfer .
We can view transliteration as a inverted form of vocabulary augmentation : instead of adapting the model to the needs of the data , the data is adjusted to meet the assumptions of the model .
Furthermore , the transliteration step is performed prior to pretraining MBERT on additional unlabeled data in the target language , the same stage at which VA is performed .
In both cases , the ultimate goal is identical : improving tokenization and more effectively using available data .
We can thus view transliteration and VA as two instantiations of a more general mix - in paradigm for model specialization , whereby various transformations ( mix - ins ) are applied to the data and/or model prior to performing additional pretraining .
These mix - ins target different components of the experimental pipeline , which naturally raises our second research question :
RQ2 : How do the VA and transliteration mix - ins
for MBERT compare and interact ?
3.1 Method and Experiments
To test this research question , we apply transliteration and VA in succession and evaluate their compatibility .
Given unlabeled data in the target language , we first transliterate it into Latin script , which decreases but does not fully eliminate the issue of unseen wordpieces .
We then perform VA , generating the vocabulary for augmentation based on the transliterated data .
We evaluate on Meadow Mari and Uyghur , which are Type 2 languages where transliteration was successfully applied by Muller et al ( 2021 ) .
To transliterate the data , we use the same methods as Muller et al ( 2021 ): Meadow Mari uses the transliterate8 package , while Uyghur uses
7We benchmark against the unlabeled dataset instead of
8https://pypi.org/project/
task - specific ones for comparability .
transliterate
( a ) POS tagging .
( b ) UD parsing .
( c ) NER .
Figure 1 : Relationship between the change in UNK token percentage on task data and the change in task performance , from ( MBERT / LAPT to VA ) , with a 1 - degree line of best fit .
All vocabulary values are computed on the respective training sets .
a linguistically - motivated transliteration scheme9 aimed at associating Uyghur with Turkish .
We use the same training scheme , model architectures , and baselines as in § 2.2 , the only difference being the use of transliterated data .
This includes directly pretraining on the unlabeled data ( LAPT ) , which is comparable to the highest - performing transliteration models of Muller et al ( 2021 ) .
Although our initial investigation of VA in § 2 also included non - Type 2 languages of other scripts , we omit them from our investigation based on the finding of Muller et al ( 2021 ) that transliterating higherresource languages into Latin scripts is not beneficial .
3.2 Results
Tab . 4 gives the results of our transliteration mix - in experiments .
For the MBERT - based models , both VA and transliteration provide strong improvements over their respective baselines .
Specifically , the improvements from LAPT to VA and LAPT to LAPT with transliteration are most pronounced .
This verifies the independent results of Chau et al ( 2020 ) and Muller et al ( 2021 ) and suggests that in the non - Latin low - resource setting , unadapted additional pretraining is insufficient , but that the mix - in stage between initial and additional pretraining is amenable to performance - improving modifications .
Unsurprisingly , transliteration provides no consistent improvement to the monolingual baselines , since the noisy transliteration process removes information without improving crosslingual alignment .
However , VA and transliteration appear to interact negatively .
Although VA with transliteration im9https://github.com/benjamin-mlr/
mbert - unseen - languages
proves over plain VA for Uyghur POS tagging and dependency parsing , it still slightly underperforms LAPT with transliteration for the latter .
For the two NER experiments , VA with transliteration lags both methods independently .
One possible explanation is that transliteration into Latin script serves as implicit vocabulary augmentation , with embeddings that have already been updated during the initial pretraining stage ; as a result , the two sources of augmentation conflict .
Alternatively , since the transliteration process merges certain characters that are distinct in the original script , VA may augment the vocabulary with misleading character clusters .
Either way , additional vocabulary augmentation is generally not as useful when combined with transliteration , answering RQ2 .
Nonetheless , additional investigation into the optimal amount of vocabulary augmentation might yield a configuration that is consistently complementary to transliteration and is an interesting direction for future work .
Furthermore , designing linguistically - informed transliteration schemes like those devised by Muller et al ( 2021 ) for Uyghur requires large amounts of time and domain knowledge .
VA ’s fully data - driven nature and relatively comparable performance suggest that it achieves an appealing balance between performance gain and implementation difficulty .
4 Related Work
Our work follows a long line of studies investigating the performance of multilingual language models like MBERT in various settings .
The exact source of such models ’ crosslingual ability is contested : early studies attributed MBERT ’s success to vocabulary overlap between languages ( Cao et al , 2020 ;
Pires et al , 2019 ; Wu and Dredze , 2019 ) ,
Rep. FASTT BERT MBERT LAPT
VA
MHR ( NER )
35.28 → 41.32 54.17 → 48.45 61.85 → 63.84 59.17 → 63.68 64.23 → 63.19
( +6.04 )
( – 5.72 )
( +1.99 )
( +4.51 )
( – 1.04 )
UG ( NER ) –
61.54 → 63.05 50.76 → 56.80 56.76 → 67.57 68.93 → 67.10
( +1.51 )
( +6.04 )
( +10.81 )
( – 1.83 )
UG ( POS )
UG ( UD )
89.45 → 89.03 90.87 → 90.76 78.07 → 91.34 79.88 → 92.59 91.49 → 92.64
( – 0.42 )
( – 0.09 )
( +13.27 )
( +12.71 )
( +1.15 )
54.52 → 54.45 60.34 → 60.08 47.70 → 65.85 50.67 → 69.39 67.55 → 68.58
( – 0.07 )
( – 0.26 )
( +18.15 )
( +18.72 )
( +1.03 )
Table 4 : Comparison of model performance before and after transliteration .
Bolded results are the maximum for each language - task pair . – indicates that a model did not converge .
but subsequent studies find typological similarity and parameter sharing to be better explanations ( Conneau et al , 2020b ; K et al , 2020 ) .
Nonetheless , past work has consistently highlighted the limitations of multilingual models in the context of low - resource languages .
Conneau et al ( 2020a ) highlight the tension between crosslingual transfer and per - language model capacity , which poses a challenge for low - resource languages that require Indeed , Wu and Dredze ( 2020 ) find that both .
MBERT is unable to outperform baselines in the lowest - resource seen languages .
Our experiments build off these insights , which motivate the development of methods for adapting MBERT to target low - resource languages .
Adapting Language Models Several prior studies have proposed methods for adapting pretrained models to a downstream task .
The simplest of these is to perform additional pretraining on unlabeled data in the target language ( Chau et al , 2020 ; Muller et al , 2021 ; Pfeiffer et al , 2020 ) , which in turn builds off similar approaches for domain adaptation ( Gururangan et al , 2020 ;
Han and Eisenstein , 2019 ) .
Recent work uses one or more of these additional pretraining stages to specifically train modular adapter layers for specific tasks or languages , with the goal of maintaining a language - agnostic model while improving performance on individual languages ( Pfeiffer et al , 2020 , 2021a ; Vidoni et al , 2020 ) .
However , as Muller et al ( 2021 ) note , the typological diversity of the world ’s languages ultimately limits the viability of this approach .
On the other hand , many adaptation techniques have focused on improving representation of the target language by modifying the model ’s vocabulary or tokenization schemes ( Chung et al , 2020 ; Clark et al , 2021 ; Wang et al , 2021 ) .
This is wellmotivated : Artetxe et al ( 2020 ) emphasize representation in the vocabulary as a key factor for effective crosslingual transfer , while Rust et al ( 2021 ) find that MBERT ’s tokenization scheme for many languages is subpar .
Pfeiffer et al ( 2021b ) further
observe that for languages with unseen scripts , a large proportion of the language is mapped to the generic “ unknown ” wordpiece , and they propose a matrix factorization - based approach to improve script representation .
Wang et al ( 2020 ) extend MBERT ’s vocabulary with an entire new vocabulary in the target language to facilitate zero - shot transfer to low - resource languages from English .
The present study most closely derives from Chau et al ( 2020 ) , who select 99 wordpieces with the greatest amount of coverage to augment MBERT ’s vocabulary while preserving the remainder ; and Muller et al ( 2021 ) , who transliterate target language data into Latin script to improve vocabulary coverage .
We deliver new insights on the effectiveness and applicability of these methods .
5 Conclusion
We explore the interactions between vocabulary augmentation and script transliteration for specializing multilingual contextual word representations in low - resource settings .
We confirm vocabulary augmentation ’s effectiveness on multiple languages , scripts , and tasks ; identify the mix - in stage as amenable to specialization ; and observe a negative interaction between vocabulary augmentation and script transliteration .
Our findings highlight several open questions in model specialization and low - resource natural language processing at large , motivating further study in this area .
Future directions for investigation are manifold .
In particular , our results in this work unify the separate findings of past works , which use MBERT as a case study ; a natural continuation would extend these methods to a broader set of multilingual models , such as mT5 ( Xue et al , 2021 ) and XLM - R ( Conneau et al , 2020a ) , in order to obtain a clearer understanding of the factors behind specialization methods ’ patterns of success .
While we intentionally choose a set of small unlabeled datasets to evaluate on a setting applicable to the vast majority of the world ’s low - resource languages , we acknowl  edge great variation in the amount of unlabeled data available in different languages .
Continued study on the applicability of these methods to datasets of different sizes is an important future step .
An interesting direction of work is to train multilingual models on data where script respresentation is more balanced , which might also allow for different output scripts for transliteration .
Given that the mix - in stage is an effective opportunity to specialize models to target languages , constructing mix - ins at both the data and model level that are complementary by design has potential to be beneficial .
Finally , future work might shed light on the interaction between different configurations of the adaptations studied here ( e.g. , the number of wordpiece types used in vocabulary augmentation ) .
Acknowledgments
We thank Jungo Kasai , Phoebe Mulcaire , and members of UW NLP for their helpful comments on preliminary versions of this paper .
We also thank Benjamin Muller for insightful discussions and providing details about transliteration methods and baselines .
Finally , we thank the anonymous reviewers for their helpful remarks .
