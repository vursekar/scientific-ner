Adversarial Multi - lingual Neural Relation Extraction
Xiaozhi Wang1‚àó , Xu Han1‚àó , Yankai Lin1 , Zhiyuan Liu1‚Ä† , Maosong Sun1,2 1Department of Computer Science and Technology , State Key Lab on Intelligent Technology and Systems , Beijing National Research Center for Information Science and Technology , Tsinghua University , Beijing , China 2Beijing Advanced Innovation Center for Imaging Technology , Capital Normal University , Beijing , China
Abstract
Multi - lingual relation extraction aims to Ô¨Ånd unknown relational facts from text in various languages .
Existing models can not well capture the consistency and diversity of relation patterns in different languages .
To address these issues , we propose an adversarial multi - lingual neural relation extraction ( AMNRE ) model , which builds both consistent and individual representations for each sentence to consider the consistency and diversity among languages .
Further , we adopt an adversarial training strategy to ensure those consistent sentence representations could effectively extract the language - consistent relation patterns .
The experimental results on real - world datasets demonstrate that our AMNRE model signiÔ¨Åcantly outperforms the state - of - the - art models .
The source code of this paper can be obtained from https://github.com/thunlp/AMNRE .
1
Introduction
Relation extraction ( RE ) is a crucial task in NLP , which aims to extract semantic relations between entity pairs from the sentences containing them .
For example , given an entity pair ( Bill Gates , Microsoft ) and a sentence ‚Äú Bill Gates is the co - founder and CEO of Microsoft ‚Äù , we want to Ô¨Ågure out the relation Founder between the two entities .
RE can potentially beneÔ¨Åt many applications , such as knowledge base construction ( Zhong et al , 2015 ; Han et al , 2018 ) and question answering ( Xiang et al , 2017 ) .
Recently , neural models have shown their great abilities in RE .
Zeng et al ( 2014 ) introduce a convolutional neural network ( CNN ) to extract relational facts with automatically learning features from text .
To address the issue of lack of data , Zeng et al ( 2015 ) incorporate multi - instance learning with a piece - wise convolutional neural network ( PCNN ) to extract relations in distantly supervised data .
Because distant supervision suffer from wrong labeling problems , Lin et al ( 2016 ) further employ a sentence - level selective attention to Ô¨Ålter out those noisy sentences in distantly supervised data and achieve state - of - the - art performance .
All these neural relation extraction ( NRE ) models merely focus on extracting relational facts from mono - lingual data , ignoring the rich information in multi - lingual data .
Lin et al ( 2017 ) propose a multi - lingual attention - based neural relation extraction ( MNRE ) model , which considers the consistency and complementarity in multi - lingual data .
MNRE builds a sentence representation for each sentence in various languages and employs a multi - lingual attention to capture the pattern consistency and complementarity among languages .
Although MNRE achieves great success in multi - lingual RE , it still has some problems .
MNRE learns a single representation for each sentence in various languages , which can not well capture both the consistency and diversity of relation patterns in different languages .
Moreover , MNRE simply utilizes a multi - lingual attention mechanism and a global relation predictor to capture the consistent relation patterns among multiple languages .
From the experimental data , we Ô¨Ånd that the sentence representations in different languages are still far from each other and linearly separable .
Therefore , it is hard for the multi‚àó indicates equal contribution
‚Ä†
Corresponding author : Zhiyuan Liu ( liuzy@tsinghua.edu.cn ) .
This work is http://creativecommons.org/licenses/by/4.0/
licensed under a Creative Commons Attribution 4.0 International License .
License details :
Proceedingsofthe27thInternationalConferenceonComputationalLinguistics , pages1156‚Äì1166SantaFe , NewMexico , USA , August20 - 26,2018.1156  Figure 1 : Overall architecture of our adversarial multi - lingual neural relation extraction ( AMNRE ) which contains two languages .
lingual attention mechanism and global relation predictor to extract relation consistency from distinct sentence representations .
To address these issues , we propose an adversarial multi - lingual NRE ( AMNRE ) model .
As shown in Figure 1 , for an entity pair , we encode its corresponding sentences in various languages through neural sentence encoders .
For each sentence , we build an individual representation to grasp its individual language features and a consistent representation to encode its substantially consistent features among languages .
Further , we adopt an adversarial training strategy to ensure AMNRE can extract the language - consistent relation patterns from the consistent representations .
Orthogonality constraints are also adopted to enhance differences between individual representations and consistent representations for each language .
In experiments , we take Chinese and English to show the effectiveness of AMNRE .
The experimental results show that AMNRE outperforms all baseline models signiÔ¨Åcantly by explicitly encoding the consistency and diversity among languages .
And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language - consistent relation patterns .
2 Related Works
2.1 Relation Extraction
Traditional supervised RE models ( Zelenko et al , 2003 ; Socher et al , 2012 ; Santos et al , 2015 ) heavily rely on abundant amounts of high - quality annotated data .
Hence , Mintz et al ( 2009 ) propose a distantly supervised model for RE .
Distant supervision aligns knowledge bases ( KBs ) and text to automatically annotate data , and thus distantly supervised models inevitably suffer from wrong labeling problems .
To alleviate the noise issue , Riedel et al ( 2010 ) and Hoffmann et al ( 2011 ) propose multi - instance learning ( MIL ) mechanisms for single - label and multi - label problems respectively .
Then , Zeng et al ( 2015 ) attempt to integrate neural models into distant supervision .
Lin et al ( 2016 ) further propose a sentence - level attention to jointly consider all sentences containing same entity pairs for RE .
The attention - based neural relation extraction ( NRE ) model has become a foundation for some recent works ( Ji et al , 2017 ; Zeng et al , 2017 ; Liu et al , 2017b ; Wu et al , 2017 ; Feng et al , 2018 ; Zeng et al , 2018 ) .
Most existing RE models are devoted to extracting relations from mono - lingual data and ignore information lying in text of multiple languages .
Faruqui and Kumar ( 2015 ) and Verga et al ( 2016 ) Ô¨Årst attempt to adopt multi - lingual transfer learning for RE .
However , both of these works learn predictive
1157x1x2x1x1          1 EI       1 EI       1 EC       1 EC   x2x2x1x2     EI       2 EC       2 EC       2 EI       2   ùíö 12   ùíö11   ùíö12   ùíö22   ùíö21      s1 ùíî    s2Language 1Language 2Individual Semantic Space ( Language1)Individual Semantic Space ( Language2)Consistent Semantic Space Textual Relation RepresentationMulti - lingual AttentionSentence EmbeddingSentence EncoderInput EmbeddingDùíö 11   ùíö 22   ùíö 21   models on a new language for existing KBs , without fully leveraging semantic information in text .
Then , Lin et al ( 2017 ) construct a multi - lingual NRE ( MNRE ) model to jointly represent text of multiple languages to enhance RE .
In this paper , we propose a novel multi - lingual NRE framework to explicitly encode language consistency and diversity into different semantic spaces , which can achieve more effective representations for RE .
2.2 Adversarial Training
Goodfellow et al ( 2015 ) propose adversarial training for image classiÔ¨Åcation tasks .
Afterwards , Goodfellow et al ( 2014 ) propose a mature adversarial training framework and use the framework to train generative models .
Adversarial networks have recently been used as methods to narrow probability distributions and proven effective in some tasks .
In domain adaptation , Ganin et al ( 2016 ) and Bousmalis et al ( 2016 ) adopt adverarial training strategies to transfer the features of one source domain to its corresponding target domain .
Inspired by Ganin et al ( 2016 ) , adversarial training has also been explored in some typical NLP tasks for multi - feature fusion .
Park and I m ( 2016 ) propose a multi - modal representation learning model based on adversarial training .
Then , Liu et al ( 2017a ) employ adversarial training to construct a multi - task learning model for text classiÔ¨Åcation by extending the original binary adversarial training to the multiclass version .
And a similar adversarial framework is also adapted by Chen et al ( 2017 ) to learn features from different datasets for chinese word segmentation .
In this paper , we adopt adversarial training to boost feature fusion to grasp the consistency among different languages .
3 Methodology
In this section , we introduce the overall framework of our proposed AMNRE in detail .
As shown in Figure 1 , for each entity pair , AMNRE encodes its corresponding sentences in different languages into several semantic spaces to grasp their individual language patterns .
Meanwhile , a uniÔ¨Åed space is also set up to encode consistent features among languages .
By explicitly encoding the consistency and diversity among languages , AMNRE can achieve better extraction results in the multi - lingual scenario .
For each given entity pair , we deÔ¨Åne its corresponding sentences in n different languages as T = { S1 , . . .
, Sn } , where Sj = { x1 j } denotes the sentence set in the j - th language .
All these sentences are labeled with the relation r ‚àà R by heuristical labeling algorithms in distant supervision ( Mintz et al , 2009 ) .
Our model aims to learn a relation extractor by maximizing the conditional probability p(r|T ) with the following three components :
j , . . . ,
x|Sj |
Sentence Encoder .
Given a sentence and its target entity pair , we employ neural networks to encode the sentence into a embedding .
In this paper , we implement the sentence encoder with both convolutional ( CNN ) and recurrent ( RNN ) architectures .
SpeciÔ¨Åcally , we set the encoders EI to encode each sentence in the j - th language into its individual and consistent embeddings respectively , and expect these embeddings to capture the diversity and consistency among languages .
j and EC j
Multi - lingual Attention .
Since not all sentences are labeled correctly in distant supervision , we adopt multi - lingual attention mechanisms to capture those informative sentences .
In practice , we apply language - individual and language - consistent attentions to compute local and global textual relation representations respectively for Ô¨Ånal prediction .
Adversarial Training .
Under the framework of AMNRE , we encode the sentences in various languages into a uniÔ¨Åed consistent semantic space .
We further adopt adversarial training to ensure these sentences are well fused in the uniÔ¨Åed space after encoding so that our model can effectively extract the language - consistent relation patterns .
We will introduce the three components in detail as follows .
3.1 Sentence Encoder
Given a sentence x = { w1 , w2 , . . . } containing two entities , we apply neural architectures including both CNN and RNN to encode the sentence into a continuous low - dimensional space to capture its implicit semantics .
1158  Input Layer
3.1.1
The input layer transforms all input words in the sentence into corresponding input embeddings by concatenating their word embeddings and position embeddings .
The word embeddings are pre - trained by Skip - Gram ( Mikolov et al , 2013 ) .
The position embeddings are a widely - used technique in RE proposed by Zeng et al ( 2014 ) , representing each word ‚Äôs relative distances to the two entities into two kp - dimensional vectors .
The input layer represents the input sentence as a ki - dimensional embedding sequence x = { w1 , w2 , . . .
} , where ki = kw + kp √ó2 , kw and kp are the dimensions of word embeddings and position embeddings respectively .
3.1.2 Encoding Layer After representing the input sentence as a ki - dimensional embedding sequence , we select both CNN ( Zeng et al , 2014 ) and RNN ( Zhang and Wang , 2015 ) to encode the input embedding sequence x = { w1 , w2 , . .
. } to its sentence embedding .
CNN slides a convolution kernel with the window size m to extract the kh - dimensional local features ,
A max - pooling is then adopted to obtain the Ô¨Ånal sentence embedding y as follows ,
hi = CNN(cid:0)wi‚àí m‚àí1
, .
. .
, wi+ m‚àí1
2
2
( cid:1 ) .
[ y]j = max{[h1]j , . . .
, [ hn]j } .
RNN is mainly designed for modeling sequential data .
In this paper , we adopt bidirectional RNN
( Bi - RNN ) to incorporate information from both sides of the sentence sequence as follows ,
‚àí‚Üí h i = RNNf ( xi ,
‚àí‚Üí h i‚àí1 ) ,
‚Üê‚àí
h i = RNNb(xi ,
‚Üê‚àí h i+1 ) ,
‚àí‚Üí h i and
‚Üê‚àí h i are the kh - dimensional hidden states at the position i of the forward and backward where RNN respectively .
RNN ( ¬∑ ) is the recurrent unit and we select gated recurrent unit ( GRU ) ( Cho et al , 2014 ) as the recurrent unit in this paper .
We concatenate both the forward and backward hidden states as the sentence embedding y ,
For simplicity , we denote such a sentence encoding operation as the following equation ,
y
=
[
‚àí‚Üí h n ;
‚Üê‚àí h 1 ] .
y = E(x ) .
For each sentence xi encoder EC
j ‚àà Sj , we adopt the individual sentence encoder EI
j and the consistent sentence
j to embed the sentence into its individual and consistent representations respectively ,
{ y1
j , y2
j , . . . }
= { EI
j ( x1
j ) , EI
j ( x2
j ) , . .
. } ,
{ ¬Øy1
j , ¬Øy2
j , . . . }
= { EC
j ( x1
j ) , EC
j ( x2
j ) , . . .
} .
3.2 Multi - lingual Selective Attention
For each given entity pair , AMNRE adopts multi - lingual selective attention mechanisms to exploit informative sentences in T .
We explicitly encode languages ‚Äô consistency and diversity into individual and consistent representations , thus our attentions are more simple than those proposed in Lin et al ( 2017 ) .
3.2.1 Language - individual Attention Since it is intuitive that each language has its own characteristic , we set language - individual attention mechanisms for different languages .
In the individual semantic space of the j - th language , we assign a query vector rj to each relation r ‚àà R. The attention score for each sentence in Sj = { x1 j , . .
. } is deÔ¨Åned as follows ,
j , x2
The attention scores can be used to compute language - individual textual relation representations ,
( 1 )
( 2 )
( 3 )
( 4 )
( 5 )
( 6 )
( 7 )
( 8)
Œ±i
j =
exp(r(cid:62 )
j yi j ) j yk k=1 exp(r(cid:62 ) j )
( cid:80)|Sj |
.
sj =
j yk Œ±k j .
|Sj | ( cid:88 )
k=1
1159 
Œ≤i j =
exp(¬Ør(cid:62)¬Øyi j ) ( cid:80)|Sl| k=1 exp(¬Ør(cid:62)¬Øyk l )
.
( cid:80)n
l=1
¬Øs =
n ( cid:88 )
|Sl| ( cid:88 )
l=1
k=1
l ¬Øyk Œ≤k l .
p(r|T )
= p(r|¬Øs )
p(r|sj ) .
n ( cid:89 )
j=1
3.2.2 Language - consistent Attention Besides language - individual attention mechanisms , we also adopt a language - consistent attention to take all sentences in all languages into consideration .
In the consistent semantic space , we also assign a query vector ¬Ør to each relation r ‚àà R and the attention score for each sentence is deÔ¨Åned as follows ,
The attention scores can be used to compute language - consistent textual relation representations ,
3.3 Relation Prediction
With the language - individual textual relation representations { s1 , s2 , . . . }
and the language - consistent textual relation representation ¬Øs , we can estimate the probability p(r|T ) over each relation r ‚àà R ,
p(r|¬Øs ) and p(r|sj ) can be deÔ¨Åned as follows ,
p(r|sj )
= softmax[Rjsj + dj ] ,
p(r|¬Øs )
= softmax [ ¬ØR¬Øs + ¬Ød ] ,
where dj and ¬Ød are bias vectors , Rj is the speciÔ¨Åc relation matrix of the j - th language , and ¬ØR is the consistent relation matrix .
We deÔ¨Åne the objective function to train the relation extractor as follows ,
min Œ∏
Lnre(Œ∏ )
= ‚àí
log p(rl|Tl ) ,
( cid:88 )
l
where Œ∏ is all parameters in the framework .
In the training phase , p(r|T ) is computed using the labeled relations as the attention queries .
In the test phase , we need to use each possible relation as attention queries to compute p(r|T ) for relation prediction since the relations are unknown in advance .
3.4 Adversarial Training
In our framework , we encode sentences of various languages into a consistent semantic space to grasp the consistency among languages .
One possible situation is that sentences of different languages are aggregated in different places of the space and linearly separable .
In this case , our purpose of mining substantially consistent relation patterns in different languages is difÔ¨Åcult to be reached .
Inspired by Ganin et al ( 2016 ) , we adopt adversarial training into our framework to address this problem .
In the adversarial training , we deÔ¨Åne a discriminator to estimate which kind of languages the sentences
from .
The probability distributions over these sentences are formalized as follows ,
D(¬Øsi
j ) = softmax(MLP(¬Øsi
j ) ) ,
where MLP is a two - layer multilayer perceptron network .
Contrary to the discriminator , the consistent sentence encoders are expected to produce sentence embeddings that can not be reliably predicted by the discriminator .
Hence , the adversarial training process is a min - max game and can be formalized as follows ,
min Œ∏C E
max Œ∏D
n ( cid:88 )
|Sj | ( cid:88 )
j=1
i=1
log[D(EC
j ( xi
j))]j ,
where [ ¬∑ ] j is the j - th value of the vector .
The formula means that given a sentence of any language , the corresponding sentence encoder of its language generates the sentence embedding to confuse the discriminator .
Meanwhile , the discriminator
( 9 )
( 10 )
( 11 )
( 12 )
( 13 )
( 14 )
( 15 )
1160  tries its best to predict the language of the sentence according to the sentence embedding .
After sufÔ¨Åcient training , the encoders and the discriminator reach a balance , and sentences of different languages containing similar semantic information can be well encoded into adjacent places of the space .
In training , we optimize the following loss functions instead of Eq . 15 ,
LE
adv(Œ∏C
E )
=
min Œ∏C E
( cid:88 )
( cid:88 )
( cid:88 )
l
Sj ‚ààTl
xi
j ‚ààSj
log[D(EC
j ( xi
j))]j , min Œ∏D
LD
adv(Œ∏D )
= ‚àí
( cid:88 )
( cid:88 )
( cid:88 )
l
Sj ‚ààTl
xi
j ‚ààSj
log[D(EC
j ( xi
j))]j ,
( 16 )
where Œ∏C
E and Œ∏D are all parameters of the consistent sentence encoders and the discriminator .
We notice that language - individual semantics could be wrongly encoded into the consistent semantic space , and may have negative effects on extracting language - consistent features .
Inspired by Bousmalis et al ( 2016 ) , we adopt orthogonality constraints to alleviate this issue .
We minimize the following penalty function :
min Œ∏E
Lpenalty(Œ∏E )
=
n ( cid:88 )
j=1
( cid:13 ) ( cid:13)IT ( cid:13 )
j Cj
( cid:13 ) ( cid:13 ) ( cid:13)F
,
( 17 )
where Ij and Cj are two matrices whose row vectors are the embeddings of sentences in the j - th language encoded by EI respectively .
Œ∏E is parameters of the all encoders .
And ( cid:107)¬∑(cid:107)F is the squared Frobenius norm .
j and EC j
3.5
Implementation Details
During training process , we combine the extraction and adversarial objective functions as follows ,
L = Lnre(Œ∏ ) + Œª1LD
adv(Œ∏D )
+ Œª2LE
adv(Œ∏C
E ) + Œª3Lpenalty(Œ∏E ) ,
( 18 )
where Œª1 , Œª2 , and Œª3 are harmonic factors .
All models are optimized using stochastic gradient descent ( SGD ) .
In practice , we integrate Œª1 and Œª2 into the alternating ratio among the loss functions , and we calibrate a 1:1:5 ratio among Lnre(Œ∏ ) + Œª3Lpenalty(Œ∏E ) , LD
E ) .
Œª3 is set as 0.02 .
adv(Œ∏D ) and LE
adv(Œ∏C
4 Experiments
4.1 Datasets and Evaluation
We evaluate our models on a multi - lingual relation extraction dataset developed by Lin et al ( 2017 ) .
The dataset consists of English and Chinese data , and has 176 relations including a special relation NA indicating that there is no relation between entities .
The whole dataset is divided into three parts for training , validation and test .
The statistics of the dataset are listed in Table 1 .
Dataset
# Rel
# Sent
# Fact
Dataset
# Rel
# Sent
# Fact
English
Training Validation Test
176 176 176
1,022,239 80,191 162,018
47,638 2,192 4,326
Chinese
Training Validation Test
176 176 176
940,595 82,699 167,224
42,536 2,192 4,326
Table 1 : Statistics of the dataset
We evaluate all models by the held - out evaluation following previous works ( Mintz et al , 2009 ; Lin et al , 2017 ) .
In experiments , we report precision - recall curves of recall under 0.3 since we focus more on the performance of those top - ranked results .
To give a complete view of the performance , we also report the area under the curve ( AUC ) .
4.2 Experiment Settings
Following the settings of previous works , we use the pre - trained word embeddings learned by Skip - Gram as the initial word embeddings .
We implement the MNRE framework proposed by Lin et al ( 2017 ) by ourselves .
For fair comparision , we set most of the hyperparameters following Lin et al ( 2017 ) .
We list the best setting of hyperparameters in Table 2 .
1161  Batch Size B Learning Rate Œ± Hidden Layer Dimension kh for CNN Hidden Layer Dimension kh for RNN Hidden Layer Dimension kd for the Discriminator
Convolution Kernel Size m
160 0.002 Dropout Probability p for CNN and RNN 230 200 Word Dimension kw 2048
Dropout Probability pd for the Discriminator
Position Dimension kp
3 0.5 0.1 50 5
Table 2 : Parameter settings .
Figure 2 : The aggregated precision - recall curves for proposed models and various baseline models .
Left : models with CNN as sentence encoders .
Right : models with RNN as sentence encoders .
4.3 Overall Evaluation Results
To evaluate the effectiveness of our proposed models AMNRE - CNN and AMNRE - RNN , we compare the proposed models with various neural methods : MNRE - CNN and MNRE - RNN are multi - lingual attention - based NRE models with CNN and RNN sentence encoders respectively ( Lin et al , 2017 ) ; CNN - EN and RNN - EN are vanilla selective - attention NRE models trained with English data , which are the state - of - the - art models in mono - lingual RE ( Lin et al , 2016 ) ; CNN - CN and RNN - CN are trained with Chinese data ; CNN - Joint and RNN - Joint are naive joint models which predict relations by directly summing up ranking scores of both English and Chinese ; CNN - Share and RNN - Share are another naive joint models which train English and Chinese models with shared relation embeddings .
The results of precision - recall curves are shown in Figure 2 and the results of AUC are shown in Table 3 .
From the results , we have the following observations : ( 1 ) Both for CNN and RNN , the models jointly utilizing English and Chinese sentences outperform the models only using mono - lingual sentences .
This demonstrates that the rich information in multi - lingual data is useful and can signiÔ¨Åcantly enhance existing NRE models .
( 2 ) The -Joint models achieve similar performance with the -Share models , and both of them underperform the MNRE and AMNRE models .
They all beneÔ¨Åt from the multi - lingual information , but the models with multi - lingual attentions can better take advantage of multi - lingual data .
It indicates that designing targeted schemes to extract rich multi - lingual information is crucial .
( 3 ) AMNRE achieves the best results among all the baseline models over the entire range of recall in Figure 2 , even as compared with MNRE .
AMNRE also outperforms MNRE with 3 percentage points increasing in the AUC results .
It indicates our proposed framework which explicitly encodes languageconsistent and language - individual semantics better extract multi - lingual information , and therefore lead to the signiÔ¨Åcant improvement in RE performance .
Models CNN - EN CNN - CN CNN - Joint CNN - Share MNRE - CNN AMNRE - CNN AUC Models RNN - EN RNN - CN RNN - Joint RNN - Share MNRE - RNN AMNRE - RNN AUC
37.0
37.6
36.6
33.2
43.4
46.2
44.2
34.5
34.4
47.3
37.1
36.5
Table 3 : The AUC results of different models ( % ) .
11620.000.050.100.150.200.250.30Recall0.50.60.70.80.91.0PrecisionAMNRE - CNNMNRE - CNNCNN - JointCNN - ShareCNN - CNCNN - EN0.000.050.100.150.200.250.30Recall0.50.60.70.80.91.0PrecisionAMNRE - RNNMNRE - RNNRNN - JointRNN - ShareRNN - CNRNN - EN  Figure 3 : The aggregated precision - recall curves for proposed models and various baseline models in the mono - lingual scenario .
Left : models with CNN as sentence encoders .
Right : models with RNN as sentence encoders .
Models CNN - EN MNRE - EN AMNRE - EN RNN - EN MNRE - EN AMNRE - EN AUC Models CNN - CN MNRE - CN AMNRE - CN RNN - CN MNRE - CN AMNRE - CN AUC
34.5
36.6
42.7
39.6
43.2
33.5
34.6
37.9
36.4
33.2
34.8
42.2
Table 4 : The AUC results of different models in the mono - lingual scenario ( % ) .
4.4 Mono - lingual Evaluation Results
To further verify that every mono - lingual RE models can beneÔ¨Åt from our proposed framework , which explicitly consider language - consistent relation patterns , we train models with multi - lingual data and evaluate the performance of these models in the mono - lingual RE scenario .
To show the results clearly , we report the precision - recall curves in Figure 3 and the AUC results in Table 4 .
From the results , we can observe that : ( 1 ) As compared with the models directly learned with the mono - lingual data , the models exploiting the multi - lingual information perform better in the mono - lingual scenario .
This demonstrates that there is latent consistency among languages , and grasping this consistency from multi - lingual data can provide additional information for models in each language to enhance their results in the mono - lingual scenario .
( 2 ) Our proposed models achieve the best precision over the entire range of recall and also signiÔ¨Åcantly improve the AUC results as compared with both MNRE and mono - lingual RE models .
It indicates that due to the consistent semantic space in our framework , language - consistent information lying in the multi - lingual data is better mined and serve the mono - lingual scenario .
4.5 Effectiveness of Adversarial Training and Orthogonality Constraints
We adopt an adversarial training strategy to fuse the features from different languages to extract consistent relation patterns .
Orthogonality constraints are also adopted to separate the consistent and individual feature spaces .
To measure the effectiveness of them , we conduct an ablation study which compares the proposed models with the similar models but without adversarial training strategy ( AMNRE - noA ) , without orthogonality constraints ( AMNRE - noO ) , and without both of them ( AMNRE - noBoth ) .
The AUC results are shown in Table 5 .
We can observe that both the adversarial training strategy and orthogonality constraints have signiÔ¨Åcant inÔ¨Çuence on the performance of our proposed model .
This demonstrates the effectiveness of adversarial training strategy and orthogonality constraints for multi - lingual RE .
To give a more intuitive picture of the effect of these two mechanisms , we visualize the distribution of sentence feature embeddings encoded by the individual and consistent encoders using t - SNE ( Maaten and Hinton , 2008 ) .
The results are shown in Figure 4 .
Figure 4(a ) shows that there are obvious differences between the feature embeddings encoded from the same sentences by individual and consistent encoders .
It indicates the orthogonality constraints are effective to separate the individual and consistent latent spaces .
From the comparison between Figure
11630.000.050.100.150.200.250.30Recall0.50.60.70.80.91.0PrecisionAMNRE - CNN - ENMNRE - CNN - ENCNN - ENAMNRE - CNN - CNMNRE - CNN - CNCNN - CN0.000.050.100.150.200.250.30Recall0.50.60.70.80.91.0PrecisionAMNRE - RNN - ENMNRE - RNN - ENRNN - ENAMNRE - RNN - CNMNRE - RNN - CNRNN - CN  ( a ) The same English sentences encoded by the consistent encoder ( yellow ) and individual encoder ( blue ) .
( b ) The English sentences ( yellow ) and the Chinese sentences ( blue ) encoded by their own consistent encoders without adversarial training .
( c ) The English sentences ( yellow ) and the Chinese sentences ( blue ) encoded by their own consistent encoders with adversarial training .
Figure 4 : The visualization of sentence feature embeddings with different mechanisms .
Models AMNRE - CNN AMNRE - CNN - noA AMNRE - CNN - noO AMNRE - CNN - noBoth AUC Models AMNRE - RNN AMNRE - RNN - noA AMNRE - RNN - noO AMNRE - RNN - noBoth AUC
41.3
42.2
44.1
46.2
43.9
43.5
47.3
43.5
Table 5 : The AUC results of the proposed models and ablated models.(% )
4(b ) and Figure 4(c ) , we can observe that the feature embeddings from different languages are wellmixed due to the adversarial training strategy .
We can more easily to grasp latent consistency among languages after multi - feature fusion .
5 Case Study
To further show the effectiveness of our proposed model to extract the language - consistent semantic information , we give an example in Table 6 .
We adopt the cosine similarity to measure the similarity between sentence embeddings encoded by consistent encoders .
The Ô¨Årst sentence in the middle column is the standard Chinese translation of the left sentence , thus they share the same semantic information .
We observe that in our proposed model , the feature embedding similarity between these two sentences are signiÔ¨Åcantly higher than the other English sentences sharing entity pair and relational fact but differing in semantics .
It indicates that sentences in different languages containing similar semantics can be indeed encoded into adjacent places of the consistent space in our framework .
There are eighteen small glaciers in the North Island on Mount Ruapehu .
Relation : Located in
ÂåóÂåóÂåóÂ≤õÂ≤õÂ≤õÁöÑÈ≤ÅÈ≤ÅÈ≤ÅÈòøÈòøÈòø‰Ω©‰Ω©‰Ω©ËÉ°ËÉ°ËÉ°Â±±Â±±Â±±‰∏äÊúâÂçÅÂÖ´‰∏™Â∞èÂÜ∞Â∑ù „ÄÇ .
. .
the bottom of the North Island of New Zealand up to the area of Mount Ruapehu .
It is located on the south - eastern North Island volcanic plateau , . . .
south - east of Mount Ruapehu .
Cosine Similarity 0.584
0.3538
0.342
Table 6 : The example highlighting entities for the case study by measuring the cosine similarities between the sentence in the left column and each sentence in the middle column .
6 Conclusion and Future Work
In this paper , we introduce a novel adversarial multi - lingual neural relation extraction model ( AMNRE ) .
AMNRE builds both individual and consistent representations for each sentence to consider the conIt also employs an adversarial training sistency and diversity of relation patterns among languages .
strategy and orthogonality constraints to ensure the consistent representations could extract the languageconsistent features to extract relations .
The experimental results on real - world datasets demonstrate that
1164  our AMNRE could effectively encode the consistency and diversity among languages , and achieves state - of - the - art performance in relation extraction .
We will explore the following directions as our future work : ( 1 ) AMNRE can be also implemented in the scenario of multiple languages , and this paper shows the effectiveness of AMNRE on the dataset with two languages ( English and Chinese ) .
In the future , we will explore AMNRE in much more other languages such as French , Spanish , and so on .
( 2 ) AMNRE simply aligns the sentences with similar semantics in different languages with an adversarial training strategy .
In fact , machine translation is a typical approach to align sentences in various languages .
In the future , we will combine machine translation with our model to further improve the extraction performance .
Acknowledgments
We thank Jiacheng Zhang for his help .
This work is supported by the National Natural Science Foundation of China ( NSFC No . 61621136008 , 61772302 ) and Tsinghua University Initiative ScientiÔ¨Åc Research Program ( 20151080406 ) .
This research is part of the NExT++ project , supported by the National Research Foundation , Prime Minister ‚Äôs OfÔ¨Åce , Singapore under its IRC@Singapore Funding Initiative .
