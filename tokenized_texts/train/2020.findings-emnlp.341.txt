Findings of the Association for Computational Linguistics : EMNLP 2020 , pages 3829‚Äì3839 November 16 - 20 , 2020 .
c  2020 Association for Computational Linguistics3829Reevaluating Adversarial Examples in Natural Language John X. Morris ‚á§ , Eli LiÔ¨Çand ‚á§ , Jack Lanchantin , Yangfeng Ji , Yanjun Qi Department of Computer Science , University of Virginia { jm8wx , edl9cy , jjl5sw , yj3fs , yq2h }
@virginia.edu
Abstract State - of - the - art attacks on NLP models lack a shared deÔ¨Ånition of what constitutes a successful attack .
These differences make the attacks difÔ¨Åcult to compare and hindered the use of adversarial examples to understand and improve NLP models .
We distill ideas from past work into a uniÔ¨Åed framework : a successful natural language adversarial example is a perturbation that fools the model and follows four proposed linguistic constraints .
We categorize previous attacks based on these constraints .
For each constraint , we suggest options for human and automatic evaluation methods .
We use these methods to evaluate two state - of - the - art synonym substitution attacks .
We Ô¨Ånd that perturbations often do not preserve semantics , and 38 % introduce grammatical errors .
Next , we conduct human studies to Ô¨Ånd a threshold for each evaluation method that aligns with human judgment .
Human surveys reveal that to successfully preserve semantics , we need to signiÔ¨Åcantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences .
With constraints adjusted to better preserve semantics and grammaticality , the attack success rate drops by over 70 percentage points.1 1Introduction One way to evaluate the robustness of a machine learning model is to search for inputs that produce incorrect outputs .
Inputs intentionally designed to fool deep learning models are referred to as adversarial examples ( Goodfellow et
al . , 2017 )
.
Adversarial examples have successfully tricked deep neural networks for image classiÔ¨Åcation : two images that look exactly the same to a human receive ‚á§ * Equal contribution 1Our code and datasets are available here .
-$"$ )  ' œ∂&$+/ # !$'(  ) 
  04/ # +#$'$+ " '  .. .*0 )  /-  &   œ¥-  $  /$*)œ∂
"  /$1 ‰ò£  1 - . 
-$  ' œ∂&$+/ # !$'(.  )   
04/ # +#$'$+ " '  .. .*0 )  /-  &   œ¥-  $  /$*)œ∂*.$/$1
”∞Figure 1 : An adversarial example generated by TFA DJUSTED for BERT Ô¨Åne - tuned on the Rotten Tomatoes sentiment analysis dataset .
Swapping a single word causes the prediction to change from positive to negative .
completely different predictions from the classiÔ¨Åer ( Goodfellow et al . , 2014 ) .
While applicable in the image case , the idea of an indistinguishable change lacks a clear analog in text .
Unlike images , two different sequences of text are never entirely indistinguishable .
This raises the question : if indistinguishable perturbations are not possible , what are adversarial examples in text ?
The literature contains many potential answers to this question , proposing varying deÔ¨Ånitions for successful adversarial examples ( Zhang et al . , 2019 ) .
Even attacks with similar deÔ¨Ånitions of success often measure it in different ways .
The lack of a consistent deÔ¨Ånition and standardized evaluation has hindered the use of adversarial examples to understand and improve NLP models.2 Therefore , we propose a uniÔ¨Åed deÔ¨Ånition for successful adversarial examples in natural language : perturbations that both fool the model and fulÔ¨Åll a set of linguistic constraints .
In Section 2 , we present four categories of constraints NLP adversarial examples may follow , depending on the context : semantics , grammaticality , overlap , and non - suspicion to human readers .
2We use ‚Äò adversarial example generation methods ‚Äô and ‚Äò adversarial attacks ‚Äô interchangeably in this paper .
3830By explicitly laying out categories of constraints adversarial examples may follow , we introduce a shared vocabulary for discussing constraints on adversarial attacks .
In Section 4 , we suggest options for human and automatic evaluation methods for each category .
We use these methods to evaluate two SOTA synonym substitution attacks :
GENETIC ATTACK byAlzantot et al .
( 2018 ) and TEXTFOOLER byJin
et al . ( 2019 ) .
Human surveys show that the perturbed examples often fail to fulÔ¨Åll semantics and non - suspicion constraints .
Additionally , a grammar checker detects 39 % more errors in the perturbed examples than in the original inputs , including many types of errors humans almost never make .
In Section 5 , we produce TFA DJUSTED , an attack with the same search process as TEXTFOOLER , but with constraint enforcement tuned to generate higher quality adversarial examples .
To enforce semantic preservation , we tighten the thresholds on the cosine similarity between embeddings of swapped words and between the sentence encodings of original and perturbed sentences .
To enforce grammaticality , we validate perturbations with a grammar checker .
As inTEXTFOOLER , these constraints are applied at each step of the search .
Human evaluation shows thatTFA DJUSTED generates perturbations that better preserve semantics and are less noticeable to human judges .
However , with stricter constraints , the attack success rate decreases from over 80 % to under 20 % .
When used for adversarial training , TEXTFOOLER ‚Äôs examples decreased model accuracy , but TFA DJUSTED ‚Äôs examples did not .
Without a shared vocabulary for discussing constraints , past work has compared the success rate of search methods with differing constraint application techniques .
Jin et
al .
( 2019 ) reported a higher attack success rate for TEXTFOOLER thanAlzantot et al .
( 2018 ) did for GENETIC ATTACK , but it was not clear whether the improvement was due to a better search method3or more lenient constraint application4 .
In Section 6we compare the search methods with constraint application held constant .
We Ô¨Ånd that GENETIC ATTACK ‚Äôs search method is more successful than TEXTFOOLER ‚Äôs , contrary to 3TEXTFOOLER uses a greedy search method with word importance ranking .
GENETIC ATTACK uses a genetic algorithm .
4For example , TEXTFOOLER applies a minimum cosine distance of .5 between embeddings of swapped words .
GENETIC ATTACK uses a threshold of .75.the implications of Jin et al .
( 2019 ) .
The Ô¨Åve main contributions of this paper are : ‚Ä¢A deÔ¨Ånition for constraints on adversarial perturbations in natural language and suggest evaluation methods for each constraint .
‚Ä¢Constraint evaluations of two SOTA synonymsubstitution attacks , revealing that their perturbations often do not preserve semantics , grammaticality , or non - suspicion .
‚Ä¢Evidence that by aligning automatic constraint application with human judgment , it is possible for attacks to produce successful , valid adversarial examples .
‚Ä¢Demonstration that reported differences in attack success between TEXTFOOLER andGENET ICATTACK are the result of more lenient constraint enforcement .
‚Ä¢Our framework enables fair comparison between attacks , by separating effects of search methods from effects of loosened constraints .
2Constraints on Adversarial Examples in Natural Language We deÔ¨Åne F : X!Y as a predictive model , for example , a deep neural network classiÔ¨Åer .
Xis the input space and Yis the output space .
We focus on adversarial perturbations which perturb a correctly predicted input , x2X , into an input xadv .
The boolean goal function G(F , xadv)represents
whether the goal of the attack has been met .
We deÔ¨Åne C1 ... C nas a set of boolean functions indicating whether the perturbation satisÔ¨Åes a certain constraint .
Adversarial attacks search for a perturbation from xtoxadvwhich fools Fby both achieving some goal , as represented by G(F , xadv ) , and fulÔ¨Ålling each constraint Ci(x , xadv ) .
The deÔ¨Ånition of the goal function Gdepends on the purpose of the attack .
Attacks on classiÔ¨Åcation frequently aim to either induce any incorrect classiÔ¨Åcation ( untargeted ) or induce a particular classiÔ¨Åcation ( targeted ) .
Attacks on other types of models may have more sophisticated goals .
For example , attacks on translation may attempt to change every word of a translation , or introduce targeted keywords into the translation ( Cheng et al . , 2018 ) .
In addition to deÔ¨Åning the goal of the attack , the attacker must decide the constraints perturbations must meet .
Different use cases require different
3831Input , x : ‚Äù Shall I compare thee to a summer ‚Äôs day ? ‚Äù ‚Äì William Shakespeare , Sonnet XVIIIConstraintPerturbation , xadvExplanationSemanticsShall I compare thee to awinter‚Äôsday?xadvhas a different meaning thanx .
GrammaticalityShall Icomparesthee to a summer ‚Äôs day?xadvis less grammatically correct thanx .
Edit DistanceSha1li conpp$haaare thee to a5umm3r ‚Äôs day?xandxadvhave a large edit distance .
Non - suspicionAm I gonnacompare thee to a summer ‚Äôs
day?A human reader may suspect thissentence to have been modiÔ¨Åed.11Shakespeare never used the word ‚Äú gon na ‚Äù .
Its Ô¨Årst recorded usage was n‚Äôt until 1806 , and it did n‚Äôt become popular until the 20th century .
Table 1 : Adversarial Constraints and Violations .
For each of the four proposed constraints , we show an example for which violates the speciÔ¨Åed constraint .
constraints .
We build on the categorization of attack spaces introduced by Gilmer et al .
( 2018 ) to introduce a categorization of constraints for adversarial examples in natural language .
In the following , we deÔ¨Åne four categories of constraints on adversarial perturbations in natural language : semantics , grammatically , overlap , and non - suspicion .
Table 1provides examples of adversarial perturbations that violate each constraint .
2.1 Semantics Semantics constraints require the semantics of the input to be preserved between xandxadv .
Many attacks include constraints on semantics as a way to ensure the correct output is preserved ( Zhang et al . , 2019 ) .
As long as the semantics of an input do not change , the correct output will stay the same .
There are exceptions : one could imagine tasks for which preserving semantics does not necessarily preserve the correct output .
For example , consider the task of classifying passages as written in either Modern or Early Modern English .
Perturbing ‚Äú why ‚Äù to ‚Äú wherefore ‚Äù may retain the semantics of the passage , but change the correct label from Modern to Early Modern English5 2.2 Grammaticality Grammaticality constraints place restrictions on the grammaticality of xadv .
For example , an adversary attempting to generate a plagiarised paper which fools a plagiarism checker would need to ensure that the paper remains grammatically correct .
Grammatical errors do n‚Äôt necessarily change semantics , as illustrated in Table 1 . 2.3 Overlap Overlap constraints restrict the similarity between xandxadvat the character level .
This in5Wherefore is a synonym for why , but was used much more often centuries ago.cludes constraints like Levenshtein distance as well as n - gram based measures such as BLEU , METEOR and chRF ( Papineni et al . , 2002 ; Denkowski and Lavie , 2014 ; Popovi ¬¥ c,2015 ) .
Setting a maximum edit distance is useful when the attacker is willing to introduce misspellings .
Additionally , the edit distance constraint is sometimes used when improving the robustness of models .
For example , Huang et al .
( 2019 ) uses Interval Bound Propagation to ensure model robustness to perturbations within some edit distance of the input .
2.4 Non - suspicion Non - suspicion constraints specify that xadvmust appear to be unmodiÔ¨Åed .
Consider the example in Table 1 .
While the perturbation preserves semantics and grammar , it switches between Modern and Early Modern English and thus may seem suspicious to readers .
Note that the deÔ¨Ånition of the non - suspicious constraint is context - dependent .
A sentence that is non - suspicious in the context of a kindergartner ‚Äôs homework assignment might be suspicious in the context of an academic paper .
An attack scenario where non - suspicion constraints do not apply is illegal PDF distribution , similar to a case discussed byGilmer et al .
( 2018 ) .
Consumers of an illegal PDF may tacitly collude with the person uploading it .
They know the document has been altered , but do not care as long as semantics are preserved .
3Review and Categorization of SOTA :
Attacks by Paraphrase : Some studies have generated adversarial examples through paraphrase .
Iyyer et al .
( 2018 ) used neural machine translation systems to generate paraphrases .
Ribeiro et
al .
( 2018 ) proposed semantically - equivalent adversarial rules .
By deÔ¨Ånition , paraphrases preserve semantics .
Since the systems aim to generate perfect
3832paraphrases , they implicitly follow constraints of grammaticality and non - suspicion .
Attacks by Synonym Substitution : Some works focus on an easier way to generate a subset of paraphrases : replacing words from the input with synonyms ( Alzantot et al . , 2018 ; Jin et al . , 2019 ; Kuleshov et al . , 2018 ; Papernot et al . , 2016 ; Ren et al . , 2019 ) .
Each attack applies a search algorithm to determine which words to replace with which synonyms .
Like the general paraphrase case , they aim to create examples that preserve semantics , grammaticality , and non - suspicion .
While not all have an explicit edit distance constraint , some limit the number of words perturbed .
Attacks by Character Substitution : Some studies have proposed to attack natural language classiÔ¨Åcation models by deliberately misspelling words ( Ebrahimi et al . , 2017 ; Gao et al . , 2018 ; Li et al . , 2018 ) .
These attacks use character replacements to change a word into one that the model does n‚Äôt recognize .
The replacements are designed to create character sequences that a human reader would easily correct into the original words .
If there are n‚Äôt many misspellings , non - suspicion may be preserved .
Semantics are preserved as long as human readers can correct the misspellings .
Attacks by Word Insertion or Removal : Liang et al .
( 2017 ) and Samanta and Mehta ( 2017 ) devised a way to determine the most important words in the input and then used heuristics to generate perturbed inputs by adding or removing important words .
In some cases , these strategies are combined with synonym substitution .
These attacks aim to follow all constraints .
Using constraints deÔ¨Åned in Section 2we categorize a sample of current attacks in Table 2 . 4Constraint Evaluation Methods and Case Study For each category of constraints introduced in Section 2 , we discuss best practices for both human and automatic evaluation .
We leave out overlap due to ease of automatic evaluation .
Additionally , we perform a case study , evaluating how well black - box synonym substitution attacks GENETIC ATTACK andTEXTFOOLER fulÔ¨Åll constraints .
Both attacks Ô¨Ånd adversarial examples by swapping out words for their synonyms until the classiÔ¨Åer is fooled .
GENETIC ATTACKuses a genetic algorithm to attack an LSTM trained on the IMDB6document - level sentiment classiÔ¨Åcation dataset .
TEXTFOOLER uses a greedy approach to attack an LSTM , CNN , and BERT trained on Ô¨Åve classiÔ¨Åcation datasets .
We chose these attacks because : ‚Ä¢They claim to create perturbations that preserve semantics , maintain grammaticality , and are not suspicious to readers .
However , our inspection of the perturbations revealed that many violated these constraints .
‚Ä¢They report high attack success rates.7 ‚Ä¢They successfully attack two of the most effective models for text classiÔ¨Åcation : LSTM and BERT .
To generate examples for evaluation , we attacked BERT using TEXTFOOLER and attacked an LSTM using GENETIC ATTACK .
We evaluate both methods on the IMDB dataset .
In addition , we evaluate TEXTFOOLER on the Yelp polarity document - level sentiment classiÔ¨Åcation dataset and the Movie Review ( MR ) sentence - level sentiment classiÔ¨Åcation dataset ( Pang and Lee , 2005 ; Zhang et al . , 2015 ) .
We use 1,000examples from each dataset .
Table 3 shows example violations of each constraint .
4.1 Evaluation of Semantics 4.1.1 Human Evaluation A few past studies of attacks have included human evaluation of semantic preservation ( Ribeiro et al . ,2018 ; Iyyer et al . , 2018 ;
Alzantot et
al . , 2018 ; Jin et al . , 2019 ) .
However , studies often simply ask users to simply rate the ‚Äú similarity ‚Äù of xand xadv .
We believe this phrasing does not generate an accurate measure of semantic preservation , as users may consider two sentences with different semantics ‚Äú similar ‚Äù if they only differ by a few words .
Instead , users should be explicitly asked whether changes between xandxadvpreserve the meaning of the original passage .
We propose to ask human judges to rate if meaning is preserved on a Likert scale of 1 - 5 , where 1 is ‚Äú Strongly Disagree ‚Äù and 5 is ‚Äú Strongly Agree ‚Äù ( Likert,1932 ) .
A perturbation is semantics - preserving if the average score is at least ‚úè sem .
We propose 6https://datasets.imdbws.com/ 7We use ‚Äú attack success rate ‚Äù to mean the percentage of the time that an attack can Ô¨Ånd a successful adversarial example by perturbing a given input .
‚Äú After - attack accuracy ‚Äù or ‚Äú accuracy after attack ‚Äù is the accuracy the model achieves after all successful perturbations have been applied .
3833Selected Attacks Generating Adversarial Examples inNatural LanguageSemantics Grammaticality EditDistanceNon - SuspicionSynonym Substitution.(Alzantot et
al . ,2018;Kuleshovet al . ,2018;Jin et
al . ,2019;Ren et al . ,2019)333
3Character Substitution.(Ebrahimi et
al . ,2017;Gao et
al . ,2018;Li et
al . ,2018)353 3Word Insertion or Removal.(Liang et
al . ,2017;Samantaand
Mehta,2017)333 3General Paraphrase.(Zhao et
al . ,2017;Ribeiro et al . ,2018;Iyyer et
al . ,2018)335 3Table 2 : Summary of Constraints and Attacks .
This table shows a selection of prior work ( rows ) categorized by constraints ( columns ) .
A ‚Äú 3 ‚Äù indicates that the respective attack is supposed to meet the constraint , and a ‚Äú 5 ‚Äù means the attack is not supposed to meet the constraint .
Constraint Violated Input , x Perturbation , xadv Semantics Jagger , Stoppard and director Michael Apted deliver a riveting and surprisingly romantic ride .Jagger ,
Stoppard and director Michael Apted deliver a bafÔ¨Çing and surprisingly sappy motorbike .
Grammaticality Agrating , emaciated Ô¨Çick .
Agrates , lanky Ô¨Çick .
Non - suspicion Great character interaction .
Gargantuan character interaction .
Table 3 : Real World Constraint Violation Examples .
Perturbations by T EXTFOOLER against BERT Ô¨Åne - tuned on the MR dataset .
Each xis classiÔ¨Åed as positive , and each xadvis classiÔ¨Åed as negative .
‚úè sem=4 as a general rule : on average , humans should at least ‚Äú Agree ‚Äù that xandxadvhave the same meaning .
4.1.2 Automatic Evaluation Automatic evaluation of semantic similarity is a well - studied NLP task .
The STS Benchmark is used as a common measurement ( Cer et al . , 2017 ) .
Michel et al .
( 2019 ) explored the use of common evaluation metrics for machine translation as a proxy for semantic similarity in the attack setting .
While n - gram overlap based approaches are computationally cheap and work well in the machine translation setting , they do not correlate with human judgment as well as sentence encoders ( Wieting and Gimpel , 2018 ) .
Some attacks have used sentence encoders to encode two sentences into a pair of Ô¨Åxed - length vectors , then used the cosine distance between the vectors as a proxy for semantic similarity .
TEXTFOOLER uses the Universal Sentence Encoder ( USE ) , which achieved a Pearson correlation score of 0.782on the STS benchmark ( Cer et al . , 2018 ) .
Another option is BERT Ô¨Åne - tuned for semantic similarity , which achieved a score of 0.865 ( Devlin et al . , 2018 ) .
Additionally , synonym substitution methods , including TEXTFOOLER andGENETIC ATTACK , often require that words be substituted only with neighbors in the counter-Ô¨Åtted embedding space , which is designed to push synonyms together and antonyms apart ( Mrksic et al . , 2016 ) .
These automatic metrics of similarity produce a score that represents the similarity between xandxadv .
Attacks depend on a minimum threshold value for each metric to determine whether the changes between xandxadvpreserve semantics .
Human evaluation is needed to Ô¨Ånd threshold values such that people generally ‚Äù agree ‚Äù that semantics is preserved .
4.1.3 Case Study To quantify semantic similarity of xandxadv , we asked users whether they agreed that the changes between the two passages preserved meaning on a scale of 1 ( Strongly Disagree ) to 5 ( Strongly Agree ) .
We averaged scores for each attack method to determine if the method generally preserves semantics .
Perturbations generated by TEXTFOOLER were rated an average of 3.28 , while perturbations generated by GENETIC ATTACK were rated on average 2.70.8The average rating given for both methods was signiÔ¨Åcantly less than our proposed ‚úè semof4 .
Using a clear survey question illustrates that humans , on average , do n‚Äôt assess these perturbations as semantics - preserving .
8We hypothesize that TEXTFOOLER achieved higher scores due to its use of USE .
38344.2 Evaluation of Grammaticality 4.2.1 Human Evaluation Both Jin et al .
( 2019 ) and Iyyer et al .
( 2018 ) reported a human evaluation of grammaticality , but neither study clearly asked if any errors were introduced by a perturbation .
For human evaluation of the grammaticality constraint , we propose presenting xandxadvtogether and asking judges if grammatical errors were introduced by the changes made .
However , due to the rule - based nature of grammar , automatic evaluation is preferred .
4.2.2 Automatic Evaluation The simplest way to automatically evaluate grammatical correctness is with a rule - based grammar checker .
Free grammar checkers are available online in many languages .
One popular checker is LanguageTool , an open - source proofreading tool ( Naber , 2003 ) .
LanguageTool ships with thousands of human - curated rules for the English language and provides an interface for identifying grammatical errors in sentences .
LanguageTool uses rules to detect grammatical errors , statistics to detect uncommon sequences of words , and language model perplexity to detect commonly confused words .
4.2.3 Case Study We ran each of the generated ( x , xadv)pairs through LanguageTool to count grammatical errors .
LanguageTool detected more grammatical errors inxadvthan xfor50 % of perturbations generated byTEXTFOOLER , and 32 % of perturbations generated by G ENETIC ATTACK .
Additionally , perturbations often contain errors that humans rarely make .
LanguageTool detected 6 categories for which errors in the perturbed samples appear at least 10 times more frequently than in the original content .
Details regarding these error categories and examples of violations are shown in Table 4 . 4.3 Evaluation of Non - suspicion 4.3.1 Human Evaluation We propose evaluation of non - suspicion by having judges view a shufÔ¨Çed mix of real and adversarial inputs and guess whether each is real or computer - altered .
This is similar to the human evaluation done by Ren et al .
( 2019 )
, but we formulate it as a binary classiÔ¨Åcation task rather than on a 1 - 5 scale .
A perturbed example xadvis notsuspicious if the percentage of judges who identifyxadvas computer - altered is at most ‚úè ns , where 0Ô£ø ‚úè nsÔ£ø1 .
4.3.2 Automatic Evaluation Automatic evaluation may be used to guess whether or not an adversarial example is suspicious .
Models can be trained to classify passages as real or perturbed , just as human judges do .
For example , Warstadt et al .
( 2018 ) trained sentence encoders on a real / fake task as a proxy for evaluation of linguistic acceptability .
Recently , Zellers et al .
( 2019 ) demonstrated that GROVER , a transformer - based text generation model , could classify its own generated news articles as human or machine - written with high accuracy .
4.3.3 Case Study We presented a shufÔ¨Çed mix of real and perturbed examples to human judges and asked if they were real or computer - altered .
As this is a time - consuming task for long documents , we only evaluated adversarial examples generated by TEXTFOOLER on the sentence - level MR dataset .
If all generated examples were non - suspicious , judges would average 50 % accuracy , as they would not be able to distinguish between real and perturbed examples .
In this case , judges achieved 69.2 % accuracy .
5Producing Higher Quality Adversarial Examples In Section 4 , we evaluated how well generated examples met constraints .
We found that although attacks in NLP aspire to meet linguistic constraints , in practice , they frequently violate them .
Now , we adjust automatic constraints applied during the course of the attack to produce better quality adversarial examples .
We set out to Ô¨Ånd if a set of constraint application methods with appropriate thresholds could produce adversarial examples that are semanticspreserving , grammatical and non - suspicious .
We modiÔ¨Åed TEXTFOOLER to produce TFA DJUSTED , a new attack with stricter constraint application .
To enforce grammaticality , we added LanguageTool .
To enforce semantic preservation , we tuned two thresholds which Ô¨Ålter out invalid word substitutions : ( a ) minimum cosine similarity between counter-Ô¨Åtted word embeddings and ( b ) minimum
3835GrammarRule IDxxadvExplanation ContextTONONBASE2 123
Did you mean ‚Äú know ‚Äù ?
‚Äî ‚Äî Replace with one of [ know ] ... ees at person they do n‚Äôt really want toknewPRPVBG3 112 Did you mean ‚Äú we ‚Äôre wanting ‚Äù , ‚Äú we are wanting ‚Äù , or ‚Äú wewere wanting ‚Äù ? ‚Äî ‚Äî Replace with one of [ we‚Äôrewanting , we are wanting , we were wanting]whilewe wantingmacdowell ‚Äôs character to retrieveher h ...
APLURAL20 294 Do n‚Äôt use indeÔ¨Ånite articles with plural words .
Did youmean ‚Äú a grate ‚Äù , ‚Äú a gratis ‚Äù or simply ‚Äú grates ‚Äù ? ‚Äî ‚Äî Replace with one of [ a grate , a gratis , grates]agrates , lanky Ô¨ÇickDIDBASEFORM25 328The verb ‚Äò ca n‚Äôt ‚Äô requires base form of this verb : ‚Äú compare ‚Äù ‚Äî ‚Äî Replace with one of [ compare] ... Ô¨Årst two cinema in the series , i can‚Äôtcomparesfriday after next to them , but nothing ...
PRPVB6 73 Do not use a noun immediately after the pronoun ‚Äò it ‚Äô .
Usea verb or an adverb , or possibly some other part of speech . ‚Äî ‚Äî Replace game with one of [ ] ... ble of being gravest , so thick with wry itgamelikea readings from bartlett ‚Äôs familia ... PRPMDNN4 46
It seems that a verb or adverb has been misspelled or ismissing here .
‚Äî ‚Äî Replace with one of [ can beappreciative , can have appreciative] ... y bit as awful as borchardt ‚Äôs coven , we canappreciativeit anywayNON3PRSVERB7 78
The pronoun ‚Äô they ‚Äô must be used with a non - third - personform of a verb : ‚Äú do ‚Äù ‚Äî ‚Äî Replace with one of [ do]theydoesa ok operating of painting this family ... Table 4 : Adversarial Examples Contain Uncommon Grammatical Errors .
This table shows grammatical errors detected by LanguageTool that appeared far more often in the perturbed samples .
xandxadvdenote the numbers of errors detected in xandxadvacross 3,115 examples generated by T EXTFOOLER and G ENETIC ATTACK .
cosine similarity between sentence embeddings .
Through human studies , we found threshold values of0.9for ( a ) and 0.98for ( b)9 .
We implemented TFA DJUSTED using TextAttack , a Python framework for implementing adversarial attacks in NLP ( Morris et al . , 2020 ) .
5.1 With Adjusted Constraint Application We tested TFA DJUSTED to determine the effect of tightening constraint application .
We used the IMDB , Yelp , and MR datasets for classifcation as in Section 4 .
We added the SNLI and MNLI entailment datasets ( Bowman et al . , 2015 ; Williams et al . , 2018 ) for the portions not requring human evaluation .
Table 5shows the results .
Semantics .
TEXTFOOLER generates perturbations for which human judges are on average ‚Äú Not sure ‚Äù if semantics are preserved .
With perturbations generated by TFA DJUSTED , human judges on average ‚Äú Agree ‚Äù that semantics are preserved .
Grammaticality .
Since all examples produced by TFA DJUSTED are checked with LanguageTool , no perturbation can introduce
grammatical errors.10 Non
-
suspicion .
We repeated the non - suspicion study from Section 4.3with the examples generated by TFA DJUSTED .
Participants were able to guess with 58.8%accuracy whether inputs were computer - altered .
The accuracy is over 10 % lower than the accuracy on the examples generated by 9Details in the appendix , Section A.2.2 .
10Since the MR dataset is already lowercased and tokenized , it is difÔ¨Åcult for a rule - based grammar checker like LanguageTool to parse some inputs .
TEXTFOOLER .
Attack success .
For each of the three datasets , the attack success rate decreased by at least 71 percentage points ( see last row of Table 5 ) .
5.2 Adversarial Training With Higher Quality Examples Using the 9,595samples in the MR training set as seed inputs , TEXTFOOLER generated 7,382 adversarial examples , while TFA DJUSTED generated just825 .
We append each set of adversarial examples to a copy of the original MR training set and Ô¨Åne - tuned a pre - trained BERT model for 10 epochs .
Figure 2plots the test accuracy over 10 training epochs , averaged over 5 random seeds per dataset .
While neither training method strongly impacts accuracy , the augmentation using TFA DJUSTED has a better impact than that of T EXTFOOLER .
We then re - ran the two attacks using 1000 examples from the MR test set as seeds .
Again averaging over 5 random seeds , we found no signiÔ¨Åcant change in robustness .
That is , models trained on the original MR dataset were approximately as robust as those trained on the datasets augmented with TEXTFOOLER andTFA DJUSTED examples .
This corroborates the Ô¨Åndings of Alzantot et
al .
( 2018 ) and contradicts those of Jin
et al .
( 2019 ) .
We include further analysis along with some hypotheses for the discrepancies in adversarial training results in A.4 .
3836Datasets !IMDB Yelp MR SNLI MNLINoteSemantic Preservation ( before)3.41 3.05 3.37  Semantic Preservation ( after)4.06 3.94 4.18  Higher value : more preservedGrammatical Error % ( before)52.8 61.2 28.3 26.7 20.1Grammatical Error % ( after)00 0 00Lower value : less mistakesNon - suspicion % ( before)  69.2  Non - suspicion % ( after)  58.8  Lower value : less suspiciousAttack Success % ( before)85.0 93.2 86.6 94.5 95.1Attack Success % ( after)13.95.3 10.67.2 14.8Difference ( before - after)71.1 87.9 76.0 87.3 80.3Table 5 : Results from running T EXTFOOLER ( before ) and TFA DJUSTED ( after ) .
Attacks are on BERT classiÔ¨Åcation models Ô¨Åne - tuned for Ô¨Åve respective NLP datasets .
Figure 2 : Accuracy of adversarially trained models on the MR test set .
Augmentation with adversarial examples generated by T EXTFOOLER ( blue ) , although higher in quantity , decreases the overall test accuracy while examples generated by TFA DJUSTED ( orange ) have a small positive effect .
Constraint Removed Yelp IMDB MR MNLI SNLI ( Original - all used )
5.3 13.9 10.6 14.3 7.2 Sentence Encoding 22.9 45.0 28.7 44.4 31.2 Word Embedding 74.6 87.1 52.9 82.7 69.8 Grammar Checking 5.8 15.0 11.6 15.4 9.0 Table 6 : Ablation study : effect of removal of a single constraint on TFA DJUSTED attack success rate .
Attacks against BERT Ô¨Åne - tuned on each dataset .
5.3 Ablation of TFA DJUSTED Constraints TFA DJUSTED generated better quality adversarial examples by constraining its search to exclude examples that fail to meet three constraints : word embedding distance , sentence encoder similarity , and grammaticality .
We performed an ablation study to understand the relative impact of each on attack success rate .
We reran three TFA DJUSTED attacks ( one for each constraint removed ) on each dataset .
Table 6 shows attack success rate after individually removing each constraint .
The word embedding distance constraint was the greatest inhibitor of attack success rate , followed by the sentence encoder.6Comparing Search Methods When an attack ‚Äôs success rate improves , it may be the result of either ( a ) improvement of the search method for Ô¨Ånding adversarial perturbations or ( b ) more lenient constraint deÔ¨Ånitions or constraint application .
TEXTFOOLER achieves a higher success rate than GENETIC ATTACK , but Jin et al .
( 2019 ) did not identify whether the improvement was due to ( a ) or ( b ) .
Since TEXTFOOLER uses both a different search method and different constraint application methods than GENETIC ATTACK , the source of the difference in attack success rates is unclear .
To determine which search method is more effective , we used TextAttack to compose attacks from the search method of GENETIC ATTACK and the constraint application methods of each of TEXTFOOLER andTFA DJUSTED ( Morris et al . , 2020 ) .
With the constraint application held constant , we can identify the source of the difference in attack success rate .
Table 7reveals that the genetic algorithm of GENETIC ATTACK is more successful than the greedy search of TEXTFOOLER at both constraint application levels .
This reveals the source of improvement in attack success rate between GENETIC ATTACK andTEXTFOOLER to be more lenient constraint application .
However , GENETIC ATTACK ‚Äôs genetic algorithm is far more computationally expensive , requiring over 40x more model queries .
7Discussion Tradeoff between attack success and example quality .
TFA DJUSTED made semantic constraints more selective , which helped attacks generate examples that scored above 4 on the Likert scale for preservation of semantics .
However , this led to a steep drop in attack success rate .
This indicates that , when only allowing adversarial perturbations
3837ConstraintsTFADJUSTEDTEXTFOOLERSearch MethodTEXTFOOLERGENETICATTACKTEXTFOOLERGENETICATTACKSemantic Preservation4.06 4.11 - -Grammatical Error % 00 - -Non - suspicion Score58.8 56.9 - -Attack Success % 10.612.091.195.0Perturbed Word % 11.1 11.018.9 17.2Num
Queries27.14431.677.03225.7Table 7 : Comparison of the search methods from G ENETIC ATTACK and T EXTFOOLER with two sets of constraints ( TEXTFOOLER and TFA DJUSTED ) .
Attacks were run on 1000 samples against BERT Ô¨Åne - tuned on the MR dataset .
GENETIC ATTACK ‚Äôs genetic algorithm is more successful than T EXTFOOLER ‚Äôs greedy strategy , albeit much less efÔ¨Åcient .
that preserve semantics and grammaticality , NLP models are relatively robust to current synonym substitution attacks .
Note that our set of constraints is n‚Äôt necessarily optimal for every attack scenario .
Some contexts may require fewer constraints or less strict constraint application .
Decoupling search methods and constraints .
It is critical that researchers decouple new search methods from new constraint evaluation and constraint application methods .
Demonstrating the performance of a new attack that simultaneously introduces a new search method and new constraints makes it unclear whether empirical gains indicate a more effective attack or a more relaxed set of constraints .
This mirrors a broader trend in machine learning where researchers report differences that come from changing multiple independent variables , making the sources of empirical gains unclear ( Lipton and Steinhardt , 2018 ) .
This is especially relevant in adversarial NLP , where each experiment depends on many parameters .
Towards improved methods for generating textual adversarial examples .
As models improve at paraphrasing inputs , we will be able to explore the space of adversarial examples beyond synonym substitutions .
As models improve at measuring semantic similarity , we will be able to more rigorously ensure that adversarial perturbations preserve semantics .
It remains to be seen how robust BERT is when subject to paraphrase attacks that rigorously preserve semantics and grammaticality .
8Related Work The goal of creating adversarial examples that preserve semantics and grammaticality is common in the NLP attack literature ( Zhang et al . , 2019 ) .
However , previous works use different deÔ¨Ånitions of adversarial examples , making it difÔ¨Åcult to compare methods .
We provide a uniÔ¨Åed deÔ¨Ånition ofan adversarial example based on a goal function and a set of linguistic constraints .
Gilmer et al .
( 2018 ) laid out a set of potential constraints for the attack space when generating adversarial examples , which are each useful in different real - world scenarios .
However , they did not discuss NLP attacks in particular .
Michel et al .
( 2019 ) deÔ¨Åned a framework for evaluating attacks on machine translation models , focusing on meaning preservation constraints , but restricted their definitions to sequence - to - sequence models .
Other research on NLP attacks has suggested various constraints but has not introduced a shared vocabulary and categorization that allows for effective comparisons between attacks .
9Conclusion We showed that two state - of - the - art synonym substitution attacks , TEXTFOOLER andGENETI CATTACK , frequently violate the constraints they claim to follow .
We created TFA DJUSTED , which applies constraints that produce adversarial examples judged to preserve semantics and grammaticality .
Due to the lack of a shared vocabulary for discussing NLP attacks , the source of improvement in attack success rate between TEXTFOOLER and GENETIC ATTACK was unclear .
Holding constraint application constant revealed that the source of TEXTFOOLER ‚Äôs improvement was lenient constraint application ( rather than a better search method ) .
With a shared framework for deÔ¨Åning and applying constraints , future research can focus on developing better search methods and better constraint application techniques for preserving semantics and grammaticality .
3838References Moustafa Alzantot , Yash Sharma , Ahmed Elgohary , Bo - Jhang Ho , Mani Srivastava , and Kai - Wei Chang .
2018 .
Generating natural language adversarial examples .
arXiv preprint arXiv:1804.07998 .
Samuel R. Bowman , Gabor Angeli , Christopher Potts , and Christopher D. Manning .
2015 .
A large annotated corpus for learning natural language inference .
CoRR , abs/1508.05326 .
Daniel Cer , Mona Diab , Eneko Agirre , I Àúnigo LopezGazpio , and Lucia Specia .
2017 .
SemEval-2017 task 1 : Semantic textual similarity multilingual and crosslingual focused evaluation .
In Proceedings of the 11th International Workshop on Semantic Evaluation ( SemEval-2017 ) , pages 1‚Äì14 , Vancouver , Canada .
Association for Computational Linguistics .
Daniel Cer , Yinfei Yang , Sheng yi Kong , Nan Hua , Nicole Limtiaco , Rhomni St. John , Noah Constant , Mario Guajardo - Cespedes , Steve Yuan , Chris Tar , Yun - Hsuan Sung , Brian Strope , and Ray Kurzweil .
2018 .
Universal sentence encoder .
ArXiv , abs/1803.11175 .
Minhao Cheng , Jinfeng Yi , Huan Zhang , Pin - Yu Chen , and Cho - Jui Hsieh . 2018 .
Seq2sick : Evaluating the robustness of sequence - to - sequence models with adversarial examples .
arXiv preprint arXiv:1803.01128 .
Michael Denkowski and Alon Lavie .
2014 .
Meteor universal : Language speciÔ¨Åc translation evaluation for any target language .
In Proceedings of the Ninth Workshop on Statistical Machine Translation , pages 376‚Äì380 , Baltimore , Maryland , USA . Association for Computational Linguistics .
Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2018 .
Bert : Pre - training of deep bidirectional transformers for language understanding.arXiv preprint arXiv:1810.04805 .
Javid Ebrahimi , Anyi Rao , Daniel Lowd , and Dejing Dou . 2017 .
HotÔ¨Çip : White - box adversarial examples for text classiÔ¨Åcation .
arXiv preprint arXiv:1712.06751 .
Ji Gao , Jack Lanchantin , Mary Lou Soffa , and Yanjun Qi . 2018 .
Black - box generation of adversarial text sequences to evade deep learning classiÔ¨Åers
.
In IEEE Security and Privacy Workshops ( SPW ) .
Justin Gilmer , Ryan P. Adams , Ian J. Goodfellow , David Andersen , and George E. Dahl . 2018 .
Motivating the rules of the game for adversarial example research .CoRR , abs/1807.06732 .
Ian Goodfellow , Nicolas Papernot , Sandy Huang , Rocky Duan , Pieter Abbeel , and Jack Clark .
Attacking machine learning with adversarial examples [ online ] .
2017.Ian J Goodfellow , Jonathon Shlens , and Christian Szegedy .
2014 .
Explaining and harnessing adversarial examples .
arXiv preprint arXiv:1412.6572 .
Po - Sen Huang , Robert Stanforth , Johannes Welbl , Chris Dyer , Dani Yogatama , Sven Gowal , Krishnamurthy Dvijotham , and Pushmeet Kohli .
2019 .
Achieving veriÔ¨Åed robustness to symbol substitutions via interval bound propagation .
ArXiv , abs/1909.01492 .
Mohit Iyyer , John Wieting , Kevin Gimpel , and Luke Zettlemoyer .
2018 .
Adversarial example generation with syntactically controlled paraphrase networks .
CoRR , abs/1804.06059 .
Di Jin , Zhijing Jin , Joey Tianyi Zhou , and Peter Szolovits . 2019 .
Is BERT Really Robust ?
A Strong Baseline for Natural Language Attack on Text ClassiÔ¨Åcation and Entailment .arXiv e - prints , page arXiv:1907.11932 .
V olodymyr Kuleshov , Shantanu Thakoor , Tingfung Lau , and Stefano Ermon .
2018 .
Adversarial examples for natural language classiÔ¨Åcation problems .
Jinfeng Li , Shouling Ji , Tianyu Du , Bo Li , and Ting Wang .
2018 .
Textbugger : Generating adversarial text against real - world applications .
arXiv preprint arXiv:1812.05271 .
Bin Liang , Hongcheng Li , Miaoqiang Su , Pan Bian , Xirong Li , and Wenchang Shi . 2017 .
Deep text classiÔ¨Åcation can be fooled .
arXiv preprint arXiv:1704.08006 .
R. Likert . 1932 .
A Technique for the Measurement of Attitudes .
Number nos .
136 - 165 in A Technique for the Measurement of Attitudes .
publisher not identiÔ¨Åed .
Zachary Chase Lipton and Jacob Steinhardt .
2018 .
Troubling trends in machine learning scholarship .
ArXiv , abs/1807.03341 .
Paul Michel , Xian Li , Graham Neubig , and Juan Miguel Pino . 2019 .
On evaluation of adversarial perturbations for sequence - to - sequence models .CoRR , abs/1903.06620 .
John X. Morris , Eli LiÔ¨Çand , Jin Yong Yoo , and Yanjun Qi . 2020 .
Textattack : A framework for adversarial attacks in natural language processing .
Nikola Mrksic , Diarmuid ¬¥ OS¬¥eaghdha , Blaise Thomson , Milica Gasic , Lina Maria Rojas - Barahona , Pei hao Su , David Vandyke , Tsung - Hsien Wen , and Steve J. Young .
2016 .
Counter-Ô¨Åtting word vectors to linguistic constraints .
In HLT - NAACL .
Daniel Naber .
2003 .
A rule - based style and grammar checker .
Bo Pang and Lillian Lee . 2005 .
Seeing stars : Exploiting class relationships for sentiment categorization with respect to rating scales .
In Proceedings of the 43rd Annual Meeting of the Association
3839for Computational Linguistics ( ACL‚Äô05 ) , pages 115 ‚Äì 124 , Ann Arbor , Michigan .
Association for Computational Linguistics .
Nicolas Papernot , Patrick McDaniel , Ananthram Swami , and Richard Harang . 2016 .
Crafting adversarial input sequences for recurrent neural networks .
InMilitary Communications Conference , MILCOM 2016 - 2016 IEEE , pages 49‚Äì54 .
IEEE .
Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .
Bleu : a method for automatic evaluation of machine translation .
In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics , pages 311‚Äì318 , Philadelphia , Pennsylvania , USA . Association for Computational Linguistics .
Maja Popovi ¬¥ c. 2015 .
chrF : character n - gram f - score for automatic MT evaluation .
InProceedings of the Tenth Workshop on Statistical Machine Translation , pages 392‚Äì395 , Lisbon , Portugal .
Association for Computational Linguistics .
Nils Reimers and Iryna Gurevych .
2019 .
Sentencebert : Sentence embeddings using siamese bertnetworks .
InProceedings of the 2019 Conference on Empirical Methods in Natural Language Processing .
Association for Computational Linguistics .
Shuhuai Ren , Yihe Deng , Kun He , and Wanxiang Che . 2019 .
Generating natural language adversarial examples through probability weighted word saliency .
InProceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1085‚Äì1097 , Florence , Italy .
Association for Computational Linguistics .
Marco Tulio Ribeiro , Sameer Singh , and Carlos Guestrin .
2018 .
Semantically equivalent adversarial rules for debugging nlp models .
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 856‚Äì865 .
Suranjana Samanta and Sameep Mehta . 2017 .
Towards crafting text adversarial samples .
arXiv preprint arXiv:1707.02812 .
Alex Warstadt , Amanpreet Singh , and Samuel R. Bowman .
2018 .
Neural network acceptability judgments .
CoRR , abs/1805.12471 .
John Wieting and Kevin Gimpel .
2018 .
Paranmt-50 m : Pushing the limits of paraphrastic sentence embeddings with millions of machine translations .
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 451‚Äì462 .
Adina Williams , Nikita Nangia , and Samuel Bowman .
2018 .
A broad - coverage challenge corpus for sentence understanding through inference .
InProceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume1 ( Long Papers ) , pages 1112‚Äì1122 , New Orleans , Louisiana . Association for Computational Linguistics .
Rowan Zellers , Ari Holtzman , Hannah Rashkin , Yonatan Bisk , Ali Farhadi , Franziska Roesner , and Yejin Choi .
2019 .
Defending against neural fake news .CoRR , abs/1905.12616 .
Wei Emma Zhang , Quan Z. Sheng , and Ahoud Abdulrahmn F. Alhazmi .
2019 .
Generating textual adversarial examples for deep learning models : A survey .
CoRR , abs/1901.06796 .
Xiang Zhang , Junbo Zhao , and Yann LeCun . 2015 .
Character - level convolutional networks for text classiÔ¨Åcation .
In Advances in neural information processing systems , pages 649‚Äì657 .
Zhengli Zhao , Dheeru Dua , and Sameer Singh . 2017 .
Generating natural adversarial examples .
arXiv preprint arXiv:1710.11342 .
