Multi - Domain Named Entity Recognition with Genre - Aware and Agnostic Inference
Jing Wang∗ , Mayank Kulkarni∗ , Daniel Preot¸iuc - Pietro Bloomberg New York , New York , USA { jwang1621,mkulkarni24,dpreotiucpie}@bloomberg.net
Abstract
Named entity recognition is a key component of many text processing pipelines and it is thus essential for this component to be robust to different types of input .
However , domain transfer of NER models with data from multiple genres has not been widely studied .
To this end , we conduct NER experiments in three predictive setups on data from : a ) multiple domains ; b ) multiple domains where the genre label is unknown at inference time ; c ) domains not encountered in training .
We introduce a new architecture tailored to this task by using shared and private domain parameters and multi - task learning .
This consistently outperforms all other baseline and competitive methods on all three experimental setups , with differences ranging between +1.95 to +3.11 average F1 across multiple genres when compared to standard approaches .
These results illustrate the challenges that need to be taken into account when building real - world NLP applications that are robust to various types of text and the methods that can help , at least partially , alleviate these issues .
1
Introduction
Accurately identifying named entities and their type in texts is a key processing step for many NLP applications .
Named entity recognition ( NER ) is an important component in several tasks including named entity linking ( Cucerzan , 2007 ) , co - reference resolution ( Ng and Cardie , 2002 ) , question answering ( Krishnamurthy and Mitchell , 2015 ) , relation extraction ( Culotta and Sorensen , 2004 ) and usually sits upstream of analytics such as sentiment ( Pang and Lee , 2004 ) or stance ( Mohammad et al , 2016 ) .
Building robust NER models to accurately tag and adapt to heterogeneous types of text is thus paramount .
Recent research focused on
∗*Equal Contribution
improving the overall performance of NER models on speciﬁc data sets .
Yet NER models show relatively high variance even when trained on the same data ( Reimers and Gurevych , 2017 ) and poorly generalize when tested on data from different genres1 , especially if these contain entity mentions unseen in the test data ( Augenstein et al , 2017 ; Agarwal et al , 2020 ) .
Despite this , research on NER models robust to different types of input is usually limited to the standard domain adaptation scenario : a single source domain rich in training data and a single target domain with limited or no training data ( Lin and Lu , 2018 ) .
We argue that this is an over - simpliﬁed experimental setup that is not typical for how NER models are used in real - world applications .
Ideally , NER models use all available data , regardless of genre , and perform inference on data from any genre , even if this was not encountered in training .
In this scenario , simply pooling all the available data is likely sub - optimal as genre - speciﬁc differences in named entity mentions are useful to model .
Conversely , models limited to only data from the same genre as the test set are likely to underperform , as using more data is usually beneﬁcial .
This work introduces three experimental setups for the NER task where models are trained on data from multiple genres and evaluated as follows : a ) Multi - Domain – evaluation is performed across
multiple genres , all seen in training .
b ) Multi - Domain with Unknown Domain Labels – evaluation is carried out across multiple genres , all seen in training , but the genre label for each document is unknown at inference time .
c ) Zero - shot Domain – evaluation is performed on documents from genres unseen in training .
1Throughout this paper , we refer by genre to a collection of documents with variations in style or structure that might impact modelling ( Santini et al , 2006 ) ; we use domain when referring to modeling concepts .
Proceedingsofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics , pages8476–8488July5 - 10,2020.c(cid:13)2020AssociationforComputationalLinguistics8476  We propose a neural architecture for NER tailored to these three experimental setups , based on the popular BiLSTM - CRF architecture ( Lample et al , 2016 ) .
We augment the base architecture to learn both domain - speciﬁc and independent features through shared and private domain components including projections and CRFs .
Further , we add a multi - task learning objective for domain prediction to guide this separation .
This model can perform inference on a text without knowledge of its corresponding domain label by using the shared components .
We compare this model with several competitive methods that use a similar base architecture while holding the embeddings constant ( i.e. GloVe embeddings ) .
These include models trained on data from each domain independently , models that pool all data and models that use domain identities as features through to source - target domain adaptation methods .
Extensive results on all three experimental setups on a collection of data from a total of twelve genres demonstrate that our proposed architecture outperforms all others by a respectable margin .
Finally , through an error analysis of our results , we aim to understand the contributions of each proposed component and the margins for future improvements .
2 Related Work
Setups for Domain Adaptation Domain adaptation , formulated as learning a single model for the same task across multiple domains , is a wellstudied research area in NLP ( Chelba and Acero , 2004 ; Florian et al , 2004 ; Blitzer et al , 2006 ; Daum´e III , 2007 ) .
The standard setup for domain adaptation is to maximize performance on data from a single low - resource ( target ) domain , by using data from a single high - resource ( source ) domain ( Blitzer et al , 2007 ; Peng and Dredze , 2017 ) .
Extensions consider a single source and multiple different target domains ( Yang and Eisenstein , 2015 ) or multiple sources and a single target domain ( Mansour et al , 2009 ) .
The multi - domain text classiﬁcation task studied in ( Li and Zong , 2008 ; Wu and Huang , 2015 ; Chen and Cardie , 2018 ) is the analogous setup for the text classiﬁcation task to the ﬁrst experimental setup we propose for NER .
Under this setup , training and evaluation is done across data from multiple domains .
Multi - Domain Adaptation Methods for multidomain text classiﬁcation use data fusion either ( Li and Zong , at the feature or classiﬁer level
2008 )
, decomposing the classiﬁer into a shared one and multiple domain - speciﬁc ones ( Wu and Huang , 2015 ) , further guided by a domain discriminator ( Chen and Cardie , 2018 ) which is also used in multi - lingual NER ( Chen et al , 2019 ) .
Further , McClosky et al ( 2010 ) explored sequence tagging tasks on data from unknown domains and Chen and Cardie ( 2018 ) experiment with sentiment classiﬁcation on data from unknown domains , similar to our third experimental setup for NER .
To the best of our knowledge , our second setup where the domain label is not available at inference time was never explicitly studied .
We note that most of these approaches make use of additional unlabeled data from each domain to learn domain - speciﬁc representations .
We do not use these resources in our methods , as we assume the end - user of the model is agnostic to the data used in training and wants to run inference without having to provide entire comparable corpora .
Domain Adaptation for NER Models for domain adaptation in NER using neural architectures were studied recently , albeit mostly for covering the single - source and single - target setup .
The INIT method trains a model using the source domain data , and its parameters are used to initialize a target model which is ﬁne - tuned on the target data ( Mou et al , 2016 ) .
The MULT method trains jointly one model for each domain with shared parameters ( Lee et al , 2018 ) .
For sequence tagging , one CRF for each of the two domains is used to obtain the predictions ( Yang et al , 2017 ) .
Adaptation can also be made at the embeddings stage ( Lin and Lu , 2018 ) or by using additional unlabeled data from the source domain and out - of - domain annotated data ( He and Sun , 2017 ) .
However , as mentioned above , this assumes that unlabeled training data can be provided for each domain , which may not be realistic .
The model adds layers between embeddings and the BiLSTM layers , between the BiLSTM and the CRF for the target domain and separate CRF layers , the latter two of which we adapt to our proposed architecture for multi - domain adaptation .
A hierarchical Bayesian prior approach is used in ( Finkel and Manning , 2009 ) to tie feature weights across domains when information is sparse and also allow the model to take advantage if substantial data is available in one domain .
Their experiments on NER focused only on three data sets : CoNLL , MUC-6 and MUC-7 and only the ﬁrst of our three setups .
A multi - task domain adaptation
8477  method for NER and word segmentation is used in ( Peng and Dredze , 2017 ) .
The proposed architecture learns a shared representation across domains and experiments with linear domain projections for each domain to guide learning of shared representations .
The output of these linear layers is fed to a CRF .
We adopt the linear domain projection method , but extend this to also include a shared projection , followed by domain - speciﬁc CRFs and multi - task learning .
Finally , another type of domain adaptation is temporal adaptation of models tested on data that is more recent than the training data , when each temporal slice can be considered as a different domain ( Rijwhani and Preot¸iuc - Pietro , 2020 ) .
3 Methods
This section describes the proposed NER architecture tailored the architecture to our multi - domain experimental setups , which is independent of input embedding representation .
3.1 Base Architecture
The basic component of our NER models is an architecture which has reached state - of - the - art performance several times over the last few years ( Lample et al , 2016 ; Peters et al , 2018 ; Akbik et al , 2018 ) .
Named entity recognition task is a structured prediction task and earlier statistical approaches are based models like Conditional Random Fields ( Lafferty et al , 2001 ) , which rely on features often designed based on domain - speciﬁc knowledge ( Luo et al , 2015 ) .
The current dominant approach to the NER task consists of neural architectures based on recurrent neural networks with different choices of input representations ( Huang et al , 2015 ; Ma and Hovy , 2016 ; Lample et al , 2016 ; Peters et al , 2018 ; Akbik et al , 2018 , 2019 ) .
The input consists of a concatenation of pretrained word embeddings and character embeddings .
Character embeddings are trained using an LSTM from randomly initialized vectors as in ( Lample et al , 2016 ) .
Word embeddings are derived from a combination GloVe ( Pennington et al , 2014 ) and FastText ( Bojanowski et al , 2017 ) pre - trained word embeddings , as used in ( Ma and Hovy , 2016 ) .
The choice of embeddings is orthogonal to the architecture and thus , we hold these constant in all experiments .
This representation is passed through two LSTM layers that process the input sequence in differFigure 1 : MultDomain – SP – Aux Architecture for 2 domains ( A & B ) and shared layers denoted by Sh
ent directions ( Huang et al , 2015 ) .
The outputs of these layers are concatenated and , in order to map the word representation obtained from the LSTM module into the label distribution , passed to a one - layer feed - forward network .
A Conditional Random Field is applied to the class predictions to jointly assign the sequence tags using a transition matrix .
This CRF layer improves performance of the model ( Lample et al , 2016 ) as it ensures the output sequence takes into account dependencies between the tags and also models the constraints the output sequence adheres to ( e.g. I - PER can not follow B - LOC ) .
3.2 Proposed Architecture ( MultDomain – SP – Aux )
We propose a new architecture based on the BiLSTM – CRF model tailored to the three proposed experimental setups .
Our proposed architecture enhances the base architecture with three components : a ) domain -speciﬁc and -independent feed - forward layers that process the BiLSTM outputs ; b ) domain -speciﬁc and -independent feed forward layers CRFs ; c ) a multi - task learning objective that learns domain labels as an auxiliary task .
The proposed architecture changes are motivated by the aim of capturing commonalities in which named entities are referred to , in any given genre , while still allowing for the model to tease apart and exploit domain - speciﬁc aspects .
The architecture is also designed to capture these commonalities across label relationships , which can vary across domains .
In addition , the multi - task objective further assists the model to leverage domaindependent and -independent components .
The choice of input representation is orthogonal to the proposed architecture and our extensions to the architecture can be combined with any input repre8478  sentation .
The model architecture is presented in Figure 1
and described below :
Private and Shared Layers
We rely on the shared - private paradigm where the model learns both a shared representation across all domains and is useful when the domain of the input is unknown or unseen in training , and a private domain representation that mostly helps tagging in that domain .
We model the shared and private features at both the feature mapping stage connecting the BiLSTM outputs to the CRF(s ) and at the CRF level .
We expect the features extracted by the BiLSTM layers to model the structure of the input across all domains .
The feed - forward layers capture the domainspeciﬁc and -independent information by using private output layers for each domain and one shared output layer .
In training , the BiLSTM outputs are projected to both the shared layer and the private layer based on the domain label provided in training .
The CRF layer is used to make a global decision for the entire tag sequence by modelling label dependencies .
We expect that this decision is , at least partially , dependent on domain - speciﬁc relationships in the label space .
Hence , each feedforward layer feeds into either private CRFs ( one for each domain ) or a shared CRF .
The separation of the shared and private layers could happen before the CRF stage ( late separation ) or before the feed - forward layer stage ( early separation ) .
We investigate the inﬂuence of each individual addition on the multi - domain performance in our analysis section through ablation studies .
Given an input , both the shared and the private parameters are used in learning to predict the output .
The set of private parameters for each domain are only updated by data from the same domain while the set of shared parameters are updated in a pooled way by taking all available data points in the training stage regardless of the domain characteristics .
For a given data point , inference can be run either by : a ) passing it though the private components if the domain label is known ; b ) through the shared components if the domain label in unknown or the domain of the data is unseen in training .
To this end , the objective function for the private and shared layers is :
LN ER SP ( x , y )
= LN ER S(x , y )
+ LN ER P ( x , y )
( 1 )
where LN ER S and LN ER P stand for the shared layer loss and private layer loss respectively .
Multi - Task Learning of Domain Labels Further , to better guide the learning process , we augment our architecture with a multi - task learning objective .
Through this , the model learns to predict the domain label of each sample in training as an auxiliary task .
The architecture uses average pooling on BiLSTM outputs followed by a fully connected layer .
Finally , softmax is applied over the learned domain feature to obtain a probability distribution of all domain labels .
The domain classiﬁcation objective is to minimize the crossentropy loss Ldomain(x , yd ) for an input x with domain label yd .
The global objective function is the combination of the NER loss function and domain loss :
L(x ; y , yd )
= LN ER SP ( x , y ) + Ldomain(x , yd )
( 2 )
4 Experimental setup
4.1 Data
We use a collection of data sets spanning eight genres to evaluate our methods .
In addition , in order to test the feasibility of NER tagging in a zero - shot domain setup , we present additional data covering four other genres .
Each genre of documents is considered a domain in modelling .
4.1.1 Data Sets
The data set collection used in learning the multidomain models ( denoted as ‘ Open Data ’ in the rest of the paper ) includes the following three data sets : CoNLL 2003 We use the data set released as part of CoNLL 2003 shared task for English ( Tjong Kim Sang and De Meulder , 2003 ) , which is arguably the most popular data set for NER and is regularly used as a benchmark for this task .
This data is a collection of news articles from the Reuters Corpus .
Twitter The Twitter data set consists of 22,000 tweets representative of multiple English - speaking locales and a variety of topics that span 11 years of Twitter posts ( 2009–2019 ) .
This data was annotated with Organizations ( ORG ) , Persons ( PER ) and Locations ( LOC ) , using the annotation guidelines used in annotating past data sets ( Tjong Kim Sang and De Meulder , 2003 ) supplemented with examples that are speciﬁc to Twitter data .
OntoNotes ( six genres ) The OntoNotes data set ( Hovy et al , 2006 ) consists for six different genres annotated , amongst others , with named entities and their types .
In this data , each genre refers to a different source , which includes newswire ( NW ) ,
8479  Data Set
# Tokens Density
Entity Distribution
ORG
PER
LOC
CoNLL 2003
302811
14.52 % 33.2 % 38.8 % 28.0 %
Twitter
227019
8.02 % 36.9 % 46.5 % 16.5 %
OntoNotes - NW 490738
8.89 % 55.1 % 21.1 % 23.8 %
OntoNotes - BN
258625
9.06 % 27.5 % 37.2 % 35.3 %
OntoNotes - MZ
197520
7.84 % 28.1 % 41.9 % 30.0 %
OntoNotes - BC
239236
5.49 % 27.5 % 39.8 % 32.8 %
OntoNotes - TC
114463
1.59 % 12.3 % 45.6 % 42.1 %
OntoNotes - WB
490738
2.17 % 25.5 % 44.4 % 30.1 %
Zero - Shot - A
103992
3.10 % 53.3 % 24.4 % 22.2 %
Zero - Shot - B
794199
8.48 % 55.5 % 28.4 % 16.1 %
Zero - Shot - C
156032
10.06 % 64.4 % 14.4 % 21.1 %
Zero - Shot - D
27522
5.84 % 38.8 % 31.9 % 29.4 %
Table 1 : Size of data sets , NE density ( tokens that are named entities ) and distributions across entity types for both open and zero - shot data sets .
broadcast news ( BN ) , broadcast conversation ( BC ) , magazine ( MZ ) , telephone conversation ( TC ) and web data ( WB ) ( Pradhan et al , 2013 ) .
Note that we replace the ‘ LOC ’ , ‘ FAC ’ and ‘ GPE ’ tags in the OntoNotes data with the ‘ LOC ’ type in order to be consistent with the deﬁnition of ‘ LOC ’ in CoNLL 2003 , as also done in ( Augenstein et al , 2017 ) .
Zero Shot Genres Finally , for zero - shot genre NER , we use a collection of internal data sets from four different genres spanning news , closed captions and other documents .
All four genres were annotated with the same entity types and using similar guidelines .
4.1.2 Data Set Statistics
Data set statistics are presented in Table 1 .
This shows that all domains are represented with a substantial number of sentences , although the prevalence of named entities and their distribution across types varies , as expected from data sets collected from different sources and genres .
We also see that the zero - shot domains are signiﬁcantly different in entity type distribution and density than the training data , making them well - suited for this setting .
4.1.3 Data Processing
In order to present comparable results across all different data sets , we limit our experiments to three different types of entities that are present in all the above data sets and annotated using similar guidelines : organizations ( including geo - political entities and facilities ) , persons and locations .
In case other types of entities exist in the data ( e.g. MISC for CoNLL , dates for OntoNotes ) , these are considered to be not an entity , similar to ( Augenstein et al , 2017 ) .
We used the BIO tagging scheme in all our experiments , as this is arguably the most popular and differences in results between this tagging scheme and others , such as the BILOU scheme , are very small in practice ( Ratinov and Roth , 2009 ) .
4.1.4 Data Splits
We train our models using the open data sets from CoNLL , Twitter and OntoNotes .
The training , development and test splits of CoNLL and OntoNotes follows the standard splits .
Similarly , we randomly split the Twitter data set randomly into 70 % for training , 10 % for development and 20 % for testing .
The ﬁnal train , dev and test sets are obtained by joining all the respective splits across the individual data sets .
4.2 Other Methods
introduced in past
We evaluate several baseline methods and other recompetitive methods search and compare to our proposed architecture ( MultDomain – SP – Aux ) described in Section 3.2 .
These methods focus on different variations of the neural model architecture , while holding the input embeddings constant .
InDomain trains an individual NER model using the base architecture for each of the known domains .
In inference , the corresponding in - domain model is used .
This allows us to establish the baseline individual domain performance when no information is shared between the domains in training .
InDomain - DomainClassiﬁer uses the same NER models as the InDomain model .
The InDomain approach is however unable to directly perform inference on sentences where the domain label is unknown at inference time .
We thus build a separate domain classiﬁer using a Bi - LSTM recurrent neural network that feeds the ﬁnal hidden state into a feed - forward network to recognize the domain of a given input sentence and route it to the appropriate InDomain NER model .
PoolDomain naively pools all available data , disregarding the domain information and trains a model using the base architecture .
This model thus ignores the domain information when training , albeit uses all available training data .
Data pooling is the standard baseline in most domain adaptation experiments .
PoolDomain - Init uses all available data and uses the domain information to train models on data from one domain at once .
After training on data from each domain , the model uses the weights as
8480  initialization for training on next domain .
This is similar to the INIT strategy for domain adaptation used in ( Mou et al , 2016 ; Lee et al , 2018 ) .
We perform this weight initialization and ﬁne - tuning process over all the domains consecutively , where the order is deﬁned by the density of entities , starting with the highest one .
PoolDomain - GradRev trains the base architecture using a gradient reversal layer ( Ganin and Lempitsky , 2014 ) .
The gradient reversal technique aims to confuse the domain discriminator while learning NER with the combination of the training data from all domains .
extends
PoolDomain+DomainFeat trains a base architecture model over all available data and , in addition to the text - based features , the domain information is explicitly represented by passing it through a domain embedding .
This is appended to the word - level features that are used as input to the BiLSTM layers .
The domain embeddings are randomly initialized .
MultDomain - SP
the MULT method ( Yang et al , 2017 ) to the multi - domain This method uses a domain - speciﬁc setup .
CRF for each domain and a shared CRF for all domains .
Both the BiLSTM and the feed - forward layers are shared across all domains .
Inference can be done either through the private layer corresponding to the domain of the input – denoted as MultDomain - MultCRF ( P ) – or through the shared layer – denoted as MultDomain - MultCRF ( S ) – in which case this can be used when the domain label is unknown in inference .
4.3
Implementation Details
For our experiments , we largely follow the training and evaluation procedure used in ( Akbik et al , 2018 ) .
As hyperparameters , we follow most suggestions outlined in the in - depth study on model robustness ( Reimers and Gurevych , 2017 ) .
Our training uses 256 hidden states for BiLSTM with mini - batch size of 32 .
The model parameters are updated using back - propagation and Adam optimizer ( Kingma and Ba , 2014 ) .
The learning rate is 1e−3 with weight decay value 1e−5 .
The model is regularized with a locked dropout rate of 0.5 .
We use 300 - dimensional pre - trained word embeddings as described in Section 3.1 , whereas the character LSTM is randomly initialized and has a hidden dimension of 64 .
The embeddings are updated on the training data .
When training the domain features together with the NER ( PoolDomain+DomainFeat ) ,
we set the domain embedding size to 128 .
We train all models for 20 epochs and report the results for the model performing best on the joint development set of the open data set collection .
5 Results
In this section , we present and compare the results of all the methods introduced previously .
Experiments are conducted ﬁrst on the open data collection introduced in Section 4.1 in the Multi - Domain and Multi - Domain with Unknown Label setups .
Following , we evaluate the performance of our model on the data used for zero - shot genre NER .
The goal of these experiments is to examine the NER performance across the three proposed experimental setups which focus on model generalizability across multiple domains .
We note that the results below can not be directly compared to the state - of - the - art results on each data set , as we restrict the entity types to PER , ORG , LOC , such that these types are constant across all data sets .
5.1 Multi - Domain with Known Domain
Labels
First , we compare models when assuming the domain label of each test document is known at inference time .
The results are listed in Table 2 .
Our proposed method – MultDomain - SP - Aux ( P ) – obtains the best results across the entire test collection in both micro - average ( +0.43 ) and macro - average ( +1.94 ) compared to all other approaches and performs best on 7 out of the 8 domains .
The second best method is the PoolDomain+DomainFeat which uses the domain feature as input .
Our method consistently surpasses the in - domain classiﬁers ( InDomain ) on microaverage ( +1.48 ) and macro - average ( +3.11 ) , showing the limitations of naive modeling approaches .
Although increases exist across all domains , these are most prominent in domains like TC ( +5.36 ) that have a low density of named entities and where indomain models have access to limited amounts of data .
However , the in - domain performance is better than the pooled method of training , which shows consistent drops in performance on some domains ( -8.69 on WB , -6.77 on BC , - 1.98 on CoNLL ) , where information from other domains did not beneﬁt the model .
8481  Model
InDomain
InDomain+DomainClassiﬁer
PoolDomain
PoolDomain – Init
PoolDomain – GradRev
PoolDomain+DomainFeat
MultDomain – SP ( P )
MultDomain – SP ( S )
MultDomain – SP – Aux ( P )
MultDomain – SP – Aux ( S )
Works on Unknown Domain Labels ( cid:55 )
CoNLL Twitter NW BN
MZ
BC
TC
WB
µ–Avg M – Avg
89.91
88.92
87.93
31.31
83.49
90.74
87.70
87.41
90.21
88.43
67.36
66.98
66.21
15.74
54.55
67.80
59.16
57.98
69.15
67.13
91.09
91.09
86.90
84.41
77.06
64.74
90.48
90.21
85.63
84.64
76.28
59.62
90.86
92.76
87.73
89.06
70.29
56.05
63.34
67.63
47.30
63.30
33.93
57.55
83.95
86.87
77.46
83.93
77.78
50.88
90.32
92.27
89.12
89.86
78.40
63.37
88.96
93.51
88.52
89.95
77.97
55.51
88.64
93.47
88.39
89.00
55.51
54.39
91.09
93.64
91.38
90.67
82.42
67.44
91.26
93.59
87.67
89.54
78.77
59.63
85.29
83.93
83.94
47.00
77.29
86.34
82.12
81.73
86.77
84.68
81.57
80.35
80.11
47.55
74.86
82.74
80.16
80.08
84.68
82.30
( cid:51 )
( cid:51 )
( cid:51 )
( cid:51 )
( cid:55 )
( cid:55 )
( cid:51 )
( cid:55 )
( cid:51 )
Table 2 : Experimental results on the eight data sets , as well as micro ( µ- ) and macro ( M- ) averaged across data sets .
Performance is measured using micro F1 score .
The rows with ( cid:51 ) indicate methods that can be applied when the domain label is not known at inference time .
( S ) and ( P ) denote if inference is done through the shared ( S ) or private ( P ) layers of the architecture .
Results in bold are the best across all models , those underlined are best across methods that work with unknown domain labels .
5.2 Multi - Domain with Unknown Domain
Models
Labels
Zero - Shot Genres
A
B
C
D
M – Avg
We now focus on the experimental setup where domain labels are unknown for each data point at inference time .
This is akin to a setup where the user is agnostic to the data the model was trained on .
As only a subset of the models can perform inference in this scenario , the results are a subset of those in Table 2 .
Our model – MultDomain - SP - Aux ( S ) – gains the best overall performance in this setup , with 1.95 macro - average F1 increase over the next best method ( InDomain+DomainClassiﬁer ) .
The other standard baseline for domain adaptation ( PoolDomain ) obtains a similar performance ( −2.19 compared to our method ) to the in - domain approach , which shows the beneﬁts of multidomain adaptation .
PoolDomain - Init is performing overall poorly , which shows that the INIT transfer learning strategy that is somewhat effective for source - target domain adaptation does not work well in the multidomain setup .
Our intuition is that this technique is unable to learn robust features sequentially across N domains , as it performs poorly on the initial trained domains .
PoolDomain - GradRev gains relatively weak performance overall , lower than the in - domain baseline .
5.3 Zero - Shot Domain
Finally , we show the results on the experimental setup where the test data is the four ‘ Zero - Shot Genres ’ , which were not used in during training .
Table 3 shows the experimental results of all methods that can run inference with unknown domain
InDomain+DomainClassiﬁer
47.16
60.04
62.00
59.50
PoolDomain
PoolDomain - Init
52.61
62.53
63.53
61.55
24.38
36.92
47.13
19.47
PoolDomain - GradRev
49.48
68.97
67.95
57.41
MultDomain - SP ( S )
50.9
72.27
68.19
61.86
57.17
60.05
31.98
60.95
63.30
MultDomain - SP - Aux ( S )
54.50 Table 3 : Evaluation results on data from genres unseen in training .
64.02
64.15
70.30
67.77
labels , as we assume that in this setup , the end - user does not have knowledge about the domains used in training and which of these are most similar to the test point .
Results show that our proposed method obresults , with a consistains again the best tent margin of 2.24 macro - average F1 improvement over the next method .
Pooling all data ( PoolDomain ) obtains better performance than building in - domain classiﬁers with domain classiﬁcation ( InDomain+DomainClassiﬁer ) unlike in the other setups .
This also shows that the zero - shot domains we used are indeed different to any of the ones in training and pooling all data manages to build a slightly more robust model than individual ones trained on less data .
The in - domain models perform 5.21 F1 points lower than our approach , the largest gap in all experimental setups , highlighting the robustness of the multi - domain modeling approach .
The MultDomain - SP ( S ) model is second best , and as this is the base for our method , we discuss its performance in the ablation study from the next section .
8482  6 Analysis
6.1 Ablation Experiments
We ﬁrst focus on understanding the impact of each component added to our proposed method over the base architecture through an ablation study .
Table 4 shows results using the private layer ( MultDomain - SP - Aux ( P ) ) when each of the three components are alternatively turned off : SharedPrivate Linear layer , Shared - Private CRF and the domain prediction auxiliary task .
Shared vs. Shared - Private CRF With the rest of the architecture ﬁxed , the results show that the shared - private CRF performs close to the shared CRF when the shared linear layer is used ( 80.08 vs. 80.16 ; 82.04 vs. 82.74 ; all comparisons in this section are on macro - average ) .
However , once we use a separate linear layer between the BiLSTM and each CRF , the difference between having the shared and the shared - private CRFs increases drastically ( 81.36 vs. 83.11 ; 82.30 vs. 84.68 ) .
With only this late separation , the inputs to CRF decoders are still domain - independent features , which makes it hard for the linear CRF to adapt .
When the inputs are already domain - dependent , the linear CRF can better use this information in performing the joint inference of the sequence .
We note that only using shared - private CRF with the base architecture is equivalent to the MultDomain - SP method ( Yang et al , 2017 ) .
Shared vs. Shared - Private Linear Projections
The results show that regardless of the other parameters , adding shared and private linear layers between the BiLSTM layers and the CRF(s ) is always beneﬁcial ( 80.08 vs. 81.36 ; 80.16 vs. 83.11 ; 82.04 vs. 82.30 ; 82.74 vs. 84.68 ) .
The improvements are relatively larger when combined with shared and private CRF , as previously seen .
Multi - Task Learning of Domain Labels
Finally , we compare the impact of adding the multi - task learning objective .
We ﬁnd that , similar to the linear layers , adding the domain prediction task is always beneﬁcial for the model with the increase being larger if is only a shared linear layer .
We expect that the two tasks at different levels of granularity rely on shared structure in the original semantic space .
The document - level domain labels can help regularize the training , providing generic information about which low - level features are valuable to entity - level recognition .
6.2
InDomain with Oracle Choice
In order to understand the limitations of the multidomain setup , we study whether the models we can build from the available data could theoretically achieve better overall performance .
We use an oracle - based selection technique on the in - domain models to select , after the prediction and using the gold labels the model which performed best for each test instance , as selected using F1 score or , if there are no entities , the model with most O predictions .
If multiple models are tied , we choose one at random .
The oracle thus provides the counterfactually “ Optimal ” strategy of model selection for each test instance and represents an upper bound on strategies relying on InDomain models .
Table 5 compares the oracle strategy predictions with the InDomain+DomainClassiﬁer and the MultDomain - SP - Aux model .
The results show that even though our model improves substantially over the in - domain models , an oracle selection method would push performance much higher ( +6.73 F1 on the open data ) .
This highlights both the variability of NER models trained on different data sets and that there is potentially more room for improvements in the multi - domain setup .
6.3
InDomain Models
The Supplementary Material shows a breakdown of the domain prediction labels for three methods : domain classiﬁcation , domain prediction in the proposed MultDomain - SP - Aux model and the oracle in - domain choice on gold data .
The oracle strategy selects the predictions from all in - domain models .
Based on this , we analyzed the performance of each individual in - domain model when tested on all domains in Table 6 .
We ﬁnd that although the Oracle strategy uses a mix of models , any model alone is unable to generalize to other domains ( 67.19 vs. 84.68 best InDomain model compared to the best overall model ) .
In the zero - shot genres , the Twitter model performs close to the MultDomain - SP - Aux model ( -0.56 F1 ) , albeit it is 24 F1 lower on the multi - domain setup .
This reinforces that learning shared domain features as opposed to learning individual models helps boost performance and is more robust to different types of inputs .
7 Runtime Comparison
Finally , we compare the runtime difference across various methods listed in the experiment section to test the practical implications of using our pro8483  Auxiliary Task Linear
CRF
CoNLL Twitter NW BN
MZ
BC
TC
WB
µ–Avg M – Avg
( cid:55 )
( cid:55 )
( cid:51 )
( cid:51 )
Shared
Sh - Private
Shared
Sh - Private
Shared
Sh - Private
Shared
Sh - Private
Shared
Sh - Private
Shared
Sh - Private
87.41
87.70
87.65
89.57
89.00
89.48
88.43
90.21
57.98
59.16
64.45
67.78
67.27
67.19
67.13
69.15
88.64
93.47
88.39
89.00
55.51
54.39
81.73
88.96
93.51
88.52
89.95
77.97
55.51
82.12
90.88
92.82
87.92
88.75
80.60
57.81
83.77
90.98
92.45
90.10
88.75
80.86
64.38
85.95
91.10
93.00
89.15
89.00
78.36
59.48
85.69
91.31
93.48
89.99
89.48
79.18
61.84
86.55
91.26
93.59
87.67
89.54
78.77
59.63
91.09
93.64
91.38
90.67
82.42
67.44
84.68
86.77
80.08
80.16
81.36
83.11
82.04
82.74
82.30
84.68
Table 4 : Ablation study comparing the performance ( F1 score ) of models trained with and without : shared - private linear projections of BiLSTM outputs , shared - private CRF heads and multi - task domain classiﬁcation .
Model
Open Data Zero - Shot
InDomain + DomainClassiﬁer
MultDomain - SP - Aux
Oracle with InDomain
80.35
84.68
91.41
57.17
64.15
80.27
posed model to scale to a large number of domains .
This highlights that the proposed MultDomain – SP – Aux model is a viable option for real - world applications .
Table 5 : Performance in macro - average F1 of the InDomain models with an oracle model selection strategy using gold test data compared to selected methods .
Model
CoNLL
Twitter
NW
BN
MZ
BC
TC
Open Data
Zero - Shot
64.26
60.59
67.19
66.08
57.52
59.19
47.25
61.40
63.59
59.00
54.82
48.62
46.30
37.41
WB
25.41
44.09 Table 6 : Results of InDomain models trained on each domain independently on the open data set collection and the zero - shot genres reported in macro average of F1 for each domain .
posed multi - domain modelling approach .
In test phase , we set the batch size as 128 .
Table 7 shows the average time of inference time used for each model .
Our proposed model architecture takes 0.15 ms ( 33 % increase ) longer for inference than InDomain or PoolDomain models , which is a result of more model parameters .
However , our proposed architecture is still 0.19 ms faster than using the InDomain+DomainClassiﬁer approach .
In addition to inference runtime , we also ﬁnd that the training time is not signiﬁcantly more than the combined training time of N in - domain models .
The main additions are that of the shared layers and the auxiliary task to the components of the N in - domain models and is thus a constant addition in the number of parameters to the total of N indomain models .
Hence , the model would scale by a constant with respect to the number of input domains ( N+1 number of components , where N is the number of domains ) .
This should allow our proModel InDomain InDomain+DomainClassiﬁer PoolDomain PoolDomain – Init PoolDomain – GradRev PoolDomain+DomainFeat MultDomain – SP MultDomain – SP – Aux
Runtime ( ms ) 0.45 0.79 0.45 0.43 0.47 0.45 0.56 0.60
Table 7 : Averaged inference time ( in ms ) per sentence query on Open Dataset .
8 Conclusions
Robustness of NLP models is essential to their wider adoption and usability .
Existing NER approaches are widely faced with limited scalability when applied to data that spans multiple domains .
This paper introduced three experimental setups that provide a framework for evaluating the robustness of NER models .
These include learning from data in multiple domains and testing on all domains , when the domain label of the test point is unknown and when this does not belong to a domain seen in training .
Building on past research , we proposed a new neural architecture that achieves substantial improvements of up to 5 F1 points when compared to standard methods .
Future work will focus on domain adaptation at the embedding layer .
