Improving Distantly - supervised Entity Typing with Compact Latent Space Clustering
Bo Chen1 , Xiaotao Gu2 , Yufeng Hu1 , Siliang Tang1∗ , Guoping Hu3 , Yueting Zhuang1 & Xiang Ren4 1Zhejiang University , 2University of Illinois at Urbana Champaign 3iFLYTEK Research , 4University of Southern California , { chenbo123 , xiaofeem , siliang , yzhuang}@zju.edu.cn , xiaotao2@illinois.edu , gphu@iflytek.com , xiangren@usc.edu
Abstract
Recently , distant supervision has gained great success on Fine - grained Entity Typing ( FET ) .
Despite its efﬁciency in reducing manual labeling efforts , it also brings the challenge of dealing with false entity type labels , as distant supervision assigns labels in a contextagnostic manner .
Existing works alleviated this issue with partial - label loss , but usually suffer from conﬁrmation bias , which means the classiﬁer ﬁt a pseudo data distribution given by itself .
In this work , we propose to regularize distantly supervised models with Compact Latent Space Clustering ( CLSC ) to bypass this problem and effectively utilize noisy data yet .
Our proposed method ﬁrst dynamically constructs a similarity graph of different entity mentions ; infer the labels of noisy instances via label propagation .
Based on the inferred labels , mention embeddings are updated accordingly to encourage entity mentions with close semantics to form a compact cluster in the embedding space , thus leading to better classiﬁcation performance .
Extensive experiments on standard benchmarks show that our CLSC model consistently outperforms state - of - the - art distantly supervised entity typing systems by a signiﬁcant margin .
1
Introduction
Recent years have seen a surge of interests in ﬁne - grained entity typing ( FET ) as it serves as an important cornerstone of several nature language processing tasks including relation extraction ( Mintz et al , 2009 ) , entity linking ( Raiman and Raiman , 2018 ) , and knowledge base completion ( Dong et al , 2014 ) .
To reduce manual efforts in labelling training data , distant supervision ( Mintz et al , 2009 ) has been widely adopted by recent FET systems .
With the help of an external knowledge base ( KB ) , an entity mention is ﬁrst
∗Corresponding Author .
Figure 1 : T - SNE visualization of the mention embeddings generated by NFETC ( left ) and CLSC ( right ) on the BBN dataset .
Our model ( CLSC ) clearly groups mentions of the same type into a compact cluster .
linked to an existing entity in KB , and then labeled with all possible types of the KB entity as supervision .
However , despite its efﬁciency , distant supervision also brings the challenge of outof - context noise , as it assigns labels in a context agnostic manner .
Early works usually ignore such noise in supervision ( Ling and Weld , 2012 ; Shimaoka et al , 2016 ) , which dampens the performance of distantly supervised models .
Towards overcoming out - of - context noise , two lines of work have been proposed to distantly supervised FET .
The ﬁrst kind of work try to ﬁlter out noisy labels using heuristic rules ( Gillick et al , 2014 ) .
However , such heuristic pruning signiﬁcantly reduces the amount of training data , and thus can not make full use of distantly annotated data .
In contrast , the other thread of works try to incorporate such imperfect annotation by partiallabel loss ( PLL ) .
The basic assumption is that , for a noisy mention , the maximum score associated with its candidate types should be greater than the scores associated with any other non - candidate types ( Ren et al , 2016a ;
Abhishek et al , 2017 ; Xu and Barbosa , 2018 ) .
Despite their success , PLLbased models still suffer from Conﬁrmation Bias by taking its own prediction as optimization objective in the next step .
Speciﬁcally , given an entity mention , if the typing system selected a wrong
ProceedingsofNAACL - HLT2019,pages2862–2872Minneapolis , Minnesota , June2 - June7,2019.c(cid:13)2019AssociationforComputationalLinguistics2862  Figure 2 : The overall framework of CLSC .
We calculate classiﬁcation loss only on clean data , while regularize the feature extractor with CLSC using both clean and noisy data .
type with the maximum score among all candidates , it will try to further maximize the score of the wrong type in following optimization epoches ( in order to minimize PLL ) , thus amplifying the conﬁrmation bias .
Such bias starts from the early stage of training , when the typing model is still very suboptimal , and can accumulate in training process .
Related discussion can be also found in the setting of semi - supervised learning ( Lee et al , 2006 ; Laine and Aila , 2017 ; Tarvainen and Valpola , 2017 ) .
In this paper , we propose a new method for distantly supervised ﬁne - grained entity typing .
Enlightened by ( Kamnitsas et al , 2018 ) , we propose to effectively utilize imperfect annotation as model regularization via Compact Latent Space Clustering ( CLSC ) .
More speciﬁcally , our model encourages the feature extractor to group mentions of the same type as a compact cluster ( dense region ) in the representation space , which leads to better classiﬁcation performance .
For training data with noisy labels , instead of generating pseudo supervision by the typing model itself , we dynamically construct a similarity - weighted graph between clean and noisy mentions , and apply label propagation on the graph to help the formation of compact clusters .
Figure 1 demonstrates the effectiveness of our method in clustering mentions of different types into dense regions .
In contrast to PLL - based models , we do not force the model to ﬁt pseudo supervision generated by itself , but only use noisy data as part of regularization for our feature extractor layer , thus avoiding bias accumulation .
Extensive experiments on standard benchmarks show that our method consistently outperforms state - of - the - art models .
Further study reveals that , the advantage of our model over the competitors gets even more signiﬁcant as the portion of noisy data rises .
2 Problem Deﬁnition
Fine - grained entity typing takes a corpus and an external knowledge base ( KB ) with a type hierarchy Y as input .
Given an entity mention ( i.e. , a sequence of token spans representing an entity ) in the corpus , our task is to uncover its corresponding type - path in Y based on the context .
By applying distant supervision , each mention is ﬁrst linked to an existing entity in KB , and Forthen labeled with all its possible types .
mally , a labeled corpus can be represented as triples D = { ( mi , ci , Yi)}n i=1 , where mi is the i - th mention , ci is the context of mi , Yi is the set of candidate types of mi .
Note that types in Yi can form one or more type paths .
In addition , we denote all terminal ( leaf ) types of each type path in Yi as the target type set Y t i ( e.g. , for Yi = { artist , teacher , person } , Y t
i = { artist , teacher } ) .
This setting is also adopted by ( Xu and Barbosa , 2018 ) .
As each entity in KB can have several type paths , out - of - context noise may exist when Yi contains type paths that are irrelevant to mi in context ci .
In this work , we argue triples where Yi contains only one type path ( i.e. , |Y t i | = 1 ) as clean data .
Other triples are treated as noisy data , where Yi contains both the true type path and irrel2863 CLSCClassifierKLRegularizationLabel   Clean DataFeatureExtractorFeatureExtractor   Noise DataFeatureExtractorFeatureExtractorDistant SupervisionTraining Datapersonartistrootlocationathlete ... legaldirector ... musicauthoractorKnowledge BaseS1 :   by Defense Secretary William Cohen and Joint Chiefs chairman General Hugh Shelton Labeled CorpusUnlabeled Corpussup       L sup       L sup       L S2 :   Defense Secretary William Cohen says if a formal investigation shows it was aMaine Republican William Cohen said the plan might violate the assassination ban      Candidate Type(cid:966)/person / artist / actor / person / political_figure / person / legalclsc       L clsc       L  Figure 3 : The architecture of feature extractor z((mi , ci ) ; θz )
evant type paths .
Noisy data usually takes a considerable portion of the entire dataset .
The major challenge for distantly supervised typing systems is to incorporate both clean and noisy data to train high - quality type classiﬁers .
3 The Proposed Approach
Overview .
The basic assumptions of our idea are : ( 1 ) all mentions belong to the same type should be close to each other in the representation space because they should have similar context , ( 2 ) similar contexts lead to the same type .
For clean data , we compact the representation space of the same type to comply ( 1 ) .
For noisy data , given assumption ( 2 ) , we infer the their type distributions via label propagation and candidate types constrain .
Figure 2 shows the overall framework of the proposed method .
Clean data is used to train classiﬁer and feature extractor end - to - endly , while noisy data is only used in CLSC regularization .
Formally , given a batch of samples { ( mi , ci , Y t i=1 , we ﬁrst convert each sample ( mi , ci ) into a real - valued vector zi via a feature extractor z((mi , ci ) ; θz ) parameterized by θz .
Then a type classiﬁer g(zi ; θg ) parameterized by θg gives the posterior P ( y|zi ; θg ) .
By incorporating CLSC regularization in the objective function , we encourage the feature extractor z to group mentions of the same type into a compact cluster , which facilitates classiﬁcation as is shown in Figure 1 .
Noisy data enhances the formation of compact clusters with the help of label propagation .
i ) } B
3.1 Feature Extractor
Figure 3 illustrates our feature extractor .
For fair comparison , we adopt the same feature extraction pipeline as used in ( Xu and Barbosa , 2018 ) .
The feature extractor is composed of an embedding layer and two encoders which encode mentions and contexts respectively .
Embedding Layer : The output of this layer is
a concatenation of word embedding and word position embedding .
We use the popular 300dimensional word embedding supplied by ( Pennington et al , 2014 ) to capture the semantic information and random initialized position embedding ( Zeng et al , 2014 ) to acquire information about the relation between words and the mentions .
Formally , Given a word embedding matrix Wword of shape dw × |V | , where V is the vocabulary and dw is the size of word embedding , each column of Wword represents a speciﬁc word w in V .
We map each word wj in ( mi , ci ) to a j ∈ Rdw .
Analogously , we get word embedding wd the word position embedding wp j ∈ Rdp of each word according to the relative distance between the word and the mention , we only use a ﬁxed length context here .
The ﬁnal embedding of the j , wp j - th word is wE j ] .
j =
[ wd
Mention Encoder : To capture lexical level information of mentions , an averaging mention encoder and a LSTM mention encoder ( Hochreiter and Schmidhuber , 1997 ) is applied to encode mentions .
Given mi = ( ws , ws+1 , · · · , we ) , the aver2864formalinvestigationDefense Secretary[WilliamCohen]saysifBi - LSTMWord - levelAttentionSecretary[WilliamCohen]saysLSTM EncoderAverage EncoderFeature Representation[WilliamCohen]EmbeddingContext Encoder ( )  aging mention representation rai ∈ Rdw is :
rai =
1 e − s + 1
e ( cid:88 )
j = s
wd j
( 1 )
By applying a LSTM over an extended mention ( ws−1 , ws , ws+1 , · · · , we , we+1 ) , we get a sequence ( hs−1 , hs , hs+1 , · · · , he , he+1 ) .
We use he+1 as LSTM mention representation rli ∈ Rdl .
The ﬁnal mention representation is rmi =
[ rai , rli ] ∈ Rdw+dl .
Context Encoder : A bidirectional LSTM with dl hidden units is employed to encode embedding sequence ( wE
e+W ):
s−W+1 , · · · , wE −−→ hj−1 , wE ←−− hj−1 , wE
s−W , wE −→ hj = LST M ( ←− hj = LST M ( ←− −→ hj ] hj ⊕
hj =[
j−1 )
j−1 )
where ⊕ denotes element - wise plus .
Then , the word - level attention mechanism computes a score βi , j over different word j in the context ci to get the ﬁnal context representation rci :
( 2 )
( 3 )
αj = wT tanh(hj )
βi , j =
exp(αj )
exp(αk )
rci =
βi , jhi , j
( cid:80 ) k ( cid:88 )
j
We use ri =
[ rmi , rci ] ∈ Rdz = Rdw+dl+dl as the feature representation of ( mi , ci ) and use a Neural Networks q over ri to get the feature vector zi . q has n layers with hn hidden units and use ReLu activation .
3.2 Compact Latent Space Clustering for
Distant Supervision
The overview of CLSC regularization is exhibited in Figure 4 , which includes three steps : dynamic graph construction ( Figure 4c ) , label propagation ( Figure 4d , e ) and Markov chains ( Figure 4 g ) .
The idea of compact clustering for semisupervised learning is ﬁrst proposed by ( Kamnitsas et al , 2018 ) .
The basic idea is to encourage mentions of the same type to be clustered into a dense region in the embedding space .
We introduce more details of CLSC for distantly supervised FET in following sections .
Dynamic Graph Construction : We start by creating a fully connected graph G over the batch
of samples Z = { zi}B i=1 , as shown in Figure 4c1 .
Each node of G is a feature representation zi , while the distance between nodes is deﬁned by a scaled dot - product distance function ( Vaswani et al , 2017 ):
Aij = exp (
) , ∀zi , zj ∈ Z
zT
i zj√
dz ZT Z √ dz
A = exp (
)
( 4 )
Each entry Aij measures the similarity between zi and zj , A ∈ RB×B can be viewed as the weighted adjacency matrix of G.
Label Propagation : The end goal of CLSC is to cluster mentions of the same type to a dense region .
For mentions which have more than one labeled types , we apply label propagation ( LP ) on G to estimate their type distribution .
Formally , we denote Φ ∈ RB×K as the label propagation posterior of a training batch .
The original
label propagation proposed by ( Zhu and Ghahramani , 2002 ) uses a transition matrix H to model the probability of a node i propagating its type posterior φi = P ( yi|xi ) ∈ RK to the other nodes .
Each entry of the transition matrix H ∈ RB×B is deﬁned as :
Hij = Aij/
( cid:88 )
Aib
b
( 5 )
The original label propagation algorithm is deﬁned as :
1 . Propagate the label by transition matrix H ,
Φ(t+1 )
= HΦ(t )
2 . Clamp the labeled data to their true labels .
Repeat from step 1 until Φ converges In this work Φ(0 ) is randomly initialized2 .
Unlike unlabeled data in semi - supervised learning , distantly labeled mentions in FET have a limited set of candidate types .
Based on this observation , We assume that ( mi , ci ) can only transmit and receive probability of types in Y t
i no matter it is noisy data or clean data .
Formally , deﬁne a B × K indicator matrix M ∈ RB×K , where Mij = 1 if type j in Y t i otherwise 0 , where B is the batch size and K
1Z = { zi}B
i=1 is a small subsample of the entire data , we did n’t observe signiﬁcant performance gain when the batch size increases .
2We also explored other initialization ( e.g. uniform initialization ) , but found no essential performance difference between different initialization setups .
2865  Figure 4 : A demonstration of the CLSC process .
( a ) represents the feature extraction step ; ( b)→(h ) shows the traditional type classiﬁcation process ( each color represents one candidate type ) , where suboptimal classiﬁers make predictions for each mention and misclassiﬁes A into the Blue type ; ( c)→(d)→(e)→(f)→(g ) demonstrates the process of CLSC as described in Section 3 .
Through label propagation and compact clustering , our model is able to group mentions of the same type into a dense region and leaves clear separation boundaries in sparse regions .
is the number of types .
Our clamping step relies on M as is shown in Figure 4d :
ij ← Φ(t+1 ) Φ(t+1 )
ij Mij/
Φ(t+1 )
ik Mik
( 6 )
( cid:88 )
k
For convenience , we iterate through these two steps Slp times , Slp is a hyperparameter .
Compact Clustering : The LP posterior Φ = Φ(Slp+1 ) is used to judge the label agreement between samples .
In the desired optimal state , transition probabilities between samples should be uniform inside the same class , while be zero between different classes .
Based on this assumption , the desirable transition matrix T ∈ RB×B is deﬁned as :
Tij =
Φik
, mk =
Φbk
( 7 )
K ( cid:88 )
k=1
Φjk mk
B ( cid:88 )
b=1
mk is a normalization term for class k. Transition matrix H derived from z((mi , ci ) ; θz ) should be in keeping with T .
Thus we minimize the cross entropy between T and H :
L1−step = −
Tijlog(Hij )
( 8)
Lclsc = −
1 B2
B ( cid:88 )
B ( cid:88 )
i=1
j=1
For instance , if Tij is close to 1 , Hij needs to be bigger , which results in the growth of Aij and ﬁnally optimize θz ( Eq.4 ) .
The loss L1−step has largely described the regularization we use in z((mi , ci ) ; θz ) for compression clustering .
In order to keep the structure of existing clusters , ( Kamnitsas et al , 2018 ) proposed an extension of L1−step to the case of Markov chains with multiple transitions between samples , which should remain within a single class .
The extension maximizes probability of paths that only traverse among samples belong to one class .
Deﬁne E ∈ RB×B as :
E = ΦT Φ
( 9 )
Eij measures the label similarities between zi and zj , which is used to mask the transition between different clusters .
The extension is given by :
H ( 1 ) = H H ( s ) =( H ( cid:12 ) E)(s−1)H =( H ( cid:12 ) E)H ( s−1 ) ,
( 10 )
where ( cid:12 ) is Hadamard Product , and H ( s ) is the ij probability of a Markov process to transit from node i to node j after s − 1 steps within the same class .
The extended loss function models paths of different length s between samples on the graph :
1 Sm
1 B2
Sm(cid:88 )
B ( cid:88 )
B ( cid:88 )
s=1
i=1
j=1
Tijlog(H ( s )
ij ) .
( 11 ) For Sm = 1 , Lclsc = L1−step .
By minimizing the cross entropy between T and H ( s ) ( Eq.11 ) , Lclsc compact paths of different length between samples within the same class .
Here , Sm is a hyper - parameter to control the maximum length
2866(c ) Graph(d ) Clamp(f ) Compact(g ) Separate(a )
EmbedNeural Network(cid:258 ) from the Valley on hand because ( cid:258 ) ( e ) Propagate(h ) Suboptimal(b ) PredictZ2Z1ABAB^^B ~ A ~ ABABABAB  of Markov chain .
Lclsc is added to the ﬁnal objective function as regularization to encourage compact cluttering .
3.3 Overall Objective
Given the representation of a mention , the type posterior is given by a standard softmax classiﬁer parameterized by θg :
4.2 Compared Methods
We compare the proposed method with several state - of - the - art FET systems3 :
• Attentive ( Shimaoka et al , 2016 ) uses an attention based feature extractor and does n’t distinguish clean from noisy data ;
• AFET ( Ren et al , 2016a ) trains label embedP ( ˆyi|zi ; θg ) = sof tmax(Wczi + bc ) ,
( 12 )
ding with partial label loss ;
where Wc ∈ RK×dz is a parameter matrix , b ∈ RK is the bias vector , where K is the number of types .
The predicted type is then given by ˆti = argmaxyiP ( ˆyi|zi ; θg ) .
Our loss function consists of two parts .
Lsup is
supervision loss deﬁned by KL divergence :
Lsup = −
yiklog(P ( yi|zi ; θg))k
1 Bc
Bc(cid:88 )
K ( cid:88 )
i=1
k=1
( 13 ) Here Bc is the number of clean data in a training batch , K is the number of target types .
The regularization term is given by Lclsc .
Hence , the overall loss function is :
Lf inal = Lsup + λclsc × Lclsc
( 14 )
λclsc is a hyper parameter to control the inﬂuence of CLSC .
4 Experiments
4.1 Dataset
We evaluate our method on two standard benchmarks : OntoNotes and BBN :
• OntoNotes : The OntoNotes dataset is composed of sentences from the Newswire part of OntoNotes corpus ( Weischedel et al , 2013 ) .
( Gillick et al , 2014 ) annotated the training part with the aid of DBpedia spotlight ( Daiber et al , 2013 ) , while the test data is manually annotated .
• BBN : The BBN dataset is composed of sentences from Wall Street Journal articles and is manually annotated by ( Weischedel and Brunstein , 2005 ) .
( Ren et al , 2016a ) regenerated the training corpus via distant supervision .
• AAA ( Abhishek et al , 2017 ) learns joint representation of mentions and type labels ;
• PLE+HYENA / FIGER ( Ren et al , 2016b ) proposes heterogeneous partial - label embedding for label noise reduction to boost typing systems .
We compare two PLE models with HYENA ( Yogatama et al , 2015 ) and FIGER ( Ling and Weld , 2012 ) as the base typing system respectively ;
• NFETC ( Xu and Barbosa , 2018 )
trains neural ﬁne - grained typing system with hierarchy - aware loss .
We compare the performance of the NFETC model with two different loss functions : partial - label loss and PLL+hierarchical loss .
We denote the two variants as NFETC and NFETChier respectively ;
• NFETC - CLSC is the proposed model in this work .
We use the NFETC model as our base model , based on which we apply Compact Latent Space Clustering Regularization as described in Section 3.2 ; Similarly , we report results produced by using both KLdivergense - based loss ( NFETC - CLSC ) and KL+hierarchical loss ( NFETC - CLSChier ) .
4.3 Evaluation Settings
For evaluation metrics , we adopt strict accuracy , loose macro , and loose micro F - scores widely used in the FET task ( Ling and Weld , 2012 ) .
To ﬁne tuning the hyper - parameters , we randomly sampled 10 % of the test set as a development set for both datasets .
With the ﬁne - tuned hyperparameter as mentioned in 4.4 , we run the model ﬁve times and report the average strict accuracy , macro F1 and micro F1 on the test set .
In this work we use the preprocessed datasets provided by ( Abhishek et al , 2017 ; Xu and Barbosa , 2018 ) .
Table 2 shows detailed statistics of the datasets .
3The baselines result are reported on ( Abhishek et al , 2017 ; Xu and Barbosa , 2018 ) in addition to performance of NFETC on BBN , we search the hyper parameters for it .
( Xu and Barbosa , 2018 ) did n’t report the results on BBN
2867  BBN
74.4 73.6 73.2 73.1 77.7
74.7 75.2 72.4 73.2 75.0
Method
OntoNotes
Strict Acc . Macro F1 Micro F1
Strict Acc . Macro F1 Micro F1
AFET ( Ren et al , 2016a ) AAA ( Abhishek et al , 2017 ) Attentive ( Shimaoka et al , 2016 ) PLE+HYENA ( Ren et al , 2016b ) PLE+FIGER
( Ren et al , 2016b )
55.3 52.2 51.7 54.6 57.2
71.2 68.5 71.0 69.2 71.5
64.6 63.3 64.91 62.5 66.1
68.3 65.5 48.4 69.2 68.5
NFETC
NFETChier
clean
54.4±0.3
71.5±0.4
64.9±0.3
71.2±0.2
77.1±0.3
76.9±0.3
+ noisy
54.8±0.4
71.8±0.4
65.0±0.4
73.8±0.6
78.4±0.6
78.9±0.6
clean
59.6±0.2
76.1±0.2
69.7±0.2
70.3±0.3
76.8±0.3
76.6±0.2
+ noisy
60.2±0.2
76.4±0.1
70.2±0.2
73.9±1.2
78.8±1.2
79.4±1.1
NFETC - CLSC
NFETC - CLSChier
clean
59.1±0.4
75.3±0.3
69.1±0.3
73.0±0.3
79.0±0.3
78.8±0.3
+ noisy
59.6±0.2
75.5±0.4
69.3±0.4
74.7±0.3
80.7±0.2
80.5±0.2
clean
61.5±0.3
77.4±0.3
71.4±0.4
70.5±0.2
78.2±0.2
78.0±0.2
+ noisy
62.8±0.3
77.8±0.4
72.0±0.4
71.9±0.3
79.8±0.4
79.5±0.3
Table 1 : Performance comparision of FET systems on the two datasets .
# types Max hierarchy depth # mentions - train # mentions - test % clean mentions - train % clean mentions - test Average |Y t i |
OntoNotes BBN 89 3 253241 8963 73.13 94.00 1.40
47 2 86078 12845 75.92 100 1.26
Table 2 : Detailed statistics of the two datasets .
4.4 Hyper Parameters
We search the hyper parameter of Ontonotes and BBN respectively via Hyperopt proposed by ( Bergstra et al , 2013 ) .
Hyper parameters are shown in Appendix A.
We optimize the model via Adam Optimizer .
The full hyper parameters includes the learning rate lr , the dimension dp of word position embedding , the dimension dl of the mention encoder ’s output ( equal to the dimension of the context encoder ’s ourput ) , the input dropout keep probability pi and output dropout keep probability po for LSTM layers ( in context encoder and LSTM mention encoder ) , the L2 regularization parameter λ , the factor of hierarchical loss normalization α ( α > 0 means use the normalization ) , BN ( whether using Batch normalization ) , the max step Slp of the label propagation , the max length Sm of Markov chain , the inﬂuence parameter λclsc of CLSC , the batch size B , the number n of hidden layers in q and the number hn of hidden units of the hidden layers .
We implement all models using
Tensorﬂow4 .
4.5 Performance comparison and analysis
Table 1 shows performance comparison between the proposed CLSC model and state - of - the - art FET systems .
On both benchmarks , the CLSC model achieves the best performance in all three metrics .
When focusing on the comparison between NFETC and CLSC , we have following observation :
• Compact Latent Space Clustering shows its effectiveness on both clean data and noisy data .
By applying CLSC regularization on the basic NFETC model , we observe consistent and signiﬁcant performance boost ;
• Hierarchical - aware loss shows signiﬁcant advantage on the OntoNotes dataset , while showing insigniﬁcant performance boost on the BBN dataset .
This is due to different distribution of labels on the test set .
The proportion of terminal types of the test set is 69 % for the BBN dataset , while is only 33 % on the OntoNotes dataset .
Thus , applying hierarchical - aware loss on the BBN dataset brings little improvement ;
• Both algorithms are able to utilize noisy data to improve performance , so we would like to further study their performance in different noisy scenarios in following discussions .
4The code for experiments is available at https://github .
com / herbertchen1 / NFETC - CLSC
2868  4.6 How robust are the methods to the
proportion of noisy data ?
that the use of Markov Chains does bring improvement to the overall performance , which is consistent with the model intuition .
5 Related Work
Named entity Recognition ( NER ) has been excavated for a long time ( Collins and Singer , 1999 ; Manning et
al , 2014 ) , which classiﬁes coarsegrained types ( e.g. person , location )
.
Recently , ( Nagesh and Surdeanu , 2018a , b ) applied ladder network ( Rasmus et al , 2015 ) to coarse - grained entity classiﬁcation in a semi - supervised learning fashion .
( Ling and Weld , 2012 ) proposed FineGrained Entity Recognition ( FET ) .
They used distant supervision to get training corpus for FET .
Embedding techniques was applied to learn feature representations since ( Yogatama et al , 2015 ;
Dong et al , 2015 ) .
( Shimaoka et al , 2016 ) introduced attention mechanism for FET to capture informative words .
( Xin et al , 2018a ) used the TransE entity embeddings ( Bordes et al , 2013 ) as the query vector of attention .
Early works ignore the out - of - context noise , ( Gillick et al , 2014 ) proposed context dependent FET and use three heuristics to clean the noisy labels with the side effect of losing training data .
To utilize noisy data , ( Ren et al , 2016a ) distinguished the loss function of noisy data from clean data via partial label loss ( PLL ) .
( Abhishek et al , 2017 ; Xu and Barbosa , 2018 ) proposed variants of PLL , which still suffer from conﬁrmation bias .
( Xu and Barbosa , 2018 ) proposed hierarchical loss to handle over - speciﬁc noise .
On top of AFET , ( Ren et al , 2016b ) proposed a method PLE to reduce the label noise , which lead to a great success in FET .
Because label noise reduction is separated from the learning of FET , there might be error propagation problem .
Recently , ( Xin et al , 2018b ) proposed utilizing a pretrained language model measures the compatibility between context and type names , and use it to repel the interference of noisy labels .
However , the compatibility got by language model may not be right and type information is deﬁned by corpus and annotation guidelines rather than type names as is mentioned in ( Azad et al , 2018 ) .
In addition , there are some work about entity - level typing which aim to ﬁgure out the types of entities in KB ( Yaghoobzadeh and Sch¨utze , 2015 ; Jin et al , 2018 ) .
Figure 5 : Performance comparison between NFETCCLSC and NFETC by removing 75%-95 % clean data .
By principle , with sufﬁcient amount of clean training data , most typing systems can achieve To further study the satisfying performance .
robustness of the methods to label noise , we compare their performance with the presence of 25 % , 20 % , 15 % , 10 % and 5 % clean training data and all noisy training data .
Figure 5 shows the performance curves as the proportion of clean data drops .
As it reveals , the CLSC model consistently wins in the comparison .
The advantage is especially clear on the BBN dataset , which offers less amount of training data .
Note that , with only 27.9 % of training data ( when only leaving 5 % clean data ) on the BBN dataset , the CLSC model yield a comparable result with the NFETC model trained on full data .
This comparison clearly shows the superiority of our approach in the effectiveness of utilizing noisy data .
4.7 Ablation : Do Markov Chains improve
typing performance ?
Table 3 shows the performance of CLSC with onestep transition ( L1−step ) and with Markov Chains ( Lclsc ) as described in Section 3.2 .
Results show
28690.3000.3500.4000.4500.5000.5507580859095Proportion of Dcleanbeing removed NFETC(c)NFETC(c+n)NFETC - CLSC(c)NFETC - CLSC(c+n)0.5000.5500.6000.6500.7000.7507580859095Proportion of Dcleanbeing removed NFETC(c)NFETC(c+n)NFETC - CLSC(c)NFETC - CLSC(c+n )  CLSC(c)(L1−step ) CLSC(c)(Lclsc ) CLSC(c+n)(L1−step ) CLSC(c+n)(Lclsc )
Strict Acc .
72.0±0.1 73.0±0.3 73.0±0.1 74.7±0.3
Table 3 : The comparison of L1−step and Lclsc on BBN .
6 Conclusion
In this paper , we propose a new method for distantly supervised ﬁne - grained entity typing , which leverages imperfect annotations as model regularization via Compact Latent Space Clustering ( CLSC ) .
Experiments on two standard benchmarks demonstrate that our method consistently outperforms state - of - the - art models .
Further study reveals our method is more robust than the former state - of - the - art approach as the portion of noisy data rises .
The proposed method is general for other tasks with imperfect annotation .
As a part of future investigation , we plan to apply the approach to other distantly supervised tasks , such as relation extraction .
7 Acknowledgments
This work has been supported in part by NSFC ( No.61751209 , U1611461 ) , Zhejiang UniversityiFLYTEK Joint Research Center , Chinese Knowledge Center of Engineering Science and Technology ( CKCEST ) , Engineering Research Center of Digital Library , Ministry of Education .
Xiang Ren ’s research has been supported in part by National Science Foundation SMA 18 - 29268 .
