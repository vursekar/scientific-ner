MLEC - QA : A Chinese Multi - Choice Biomedical Question Answering Dataset
Jing Li , Shangping Zhong and Kaizhi Chen College of Computer and Data Science , Fuzhou University , Fuzhou , China { N190320027 , spzhong , ckz}@fzu.edu.cn
Abstract
Question Answering ( QA ) has been successfully applied in scenarios of human - computer interaction such as chatbots and search engines .
However , for the speciﬁc biomedical domain , QA systems are still immature due to expert - annotated datasets being limited by category and scale .
In this paper , we present MLEC - QA , the largest - scale Chinese multi - choice biomedical QA dataset , collected from the National Medical Licensing Examination in China .
The dataset is composed of ﬁve subsets with 136,236 biomedical multi - choice questions with extra materials ( images or tables ) annotated by human experts , and ﬁrst covers the following biomedical sub-ﬁelds : Clinic , Stomatology , Public Health , Traditional Chinese Medicine , and Traditional Chinese Medicine Combined with Western Medicine .
We implement eight representative control methods and open - domain QA methods as baselines .
Experimental results demonstrate that even the current best model can only achieve accuracies between 40 % to 55 % on ﬁve subsets , especially performing poorly on questions that require sophisticated reasoning ability .
We hope the release of the MLEC - QA dataset can serve as a valuable resource for research and evaluation in open - domain QA , and also make advances for biomedical QA systems.1
1
Introduction
As a branch of the QA task , Biomedical Question Answering ( BQA ) enables effectively perceiving , accessing , and understanding complex biomedical knowledge by innovative applications , which makes BQA an important QA application in the biomedical domain ( Jin et al , 2021 ) .
Such a task has recently attracted considerable attention from the NLP community ( Zweigenbaum , 2003 ; He et al , 2020b ; Jin et al , 2020 ) , but is still confronted with the following three key challenges :
1https://github.com/Judenpech/MLEC-QA
1.html
( 1 ) Most work attempt to build BQA systems with deep learning and neural network techniques ( Ben Abacha et al , 2017 , 2019b ; Pampari et al , 2018 ) and are thus data - hungry .
However , annotating large - scale biomedical question - answer pairs with high quality is prohibitively expensive .
As a result , current expert - annotated BQA datasets are small in size .
( 2 ) Multi - choice QA is a typical format type of BQA dataset .
Most previous work focus on such format type of datasets in which contents are in the ﬁeld of clinical medicine ( Zhang et al , 2018b ; Jin et al , 2020 ) and consumer health ( Zhang et al , 2017 , 2018a ; He et al , 2019 ; Tian et al , 2019 ) .
However , there are many other specialized sub-ﬁelds in biomedicine that have not been studied before ( e.g. , Stomatology ) .
( 3 ) Ideal BQA systems should not only focus on raw text data , but also fully utilize various types of biomedical resources , such as images and tables .
Unfortunately , most BQA datasets are either texts ( Tsatsaronis et al , 2015 ; Pampari et al , 2018 ; Jin et al , 2019 ) or images ( Lau et al , 2018 ; Ben Abacha et al , 2019a ; He et al , 2020a ) ; as a result , BQA datasets that are composed by fusing different biomedical resources are relatively limited .
To push forward the variety of BQA datasets , we present MLEC - QA , the largest - scale Chinese multi - choice BQA dataset .
Questions in MLECQA are collected from the National Medical Licensing Examination in China ( NMLEC)2 , which are carefully designed by human experts to evaluate professional knowledge and skills for those who want to be medical practitioners in China .
The NMLEC has a total number of 24 categories of exams , but only ﬁve of them have the written exams in Chinese .
Every year , only around 18 - 22 % of applicants can pass one of these exams , showing the complexity and difﬁculty of passing
2http://www.nmec.org.cn/Pages/ArticleList-12-0-0Proceedingsofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages8862–8874November7–11,2021.c(cid:13)2021AssociationforComputationalLinguistics8862  them even for skilled humans .
There are three main properties of MLEC - QA : ( 1 ) MLEC - QA is the largest - scale Chinese multichoice BQA dataset , containing 136,236 questions with extra materials ( images or tables ) , Table 1 shows an example .
( 2 ) MLEC - QA ﬁrst covers the following biomedical sub-ﬁelds : Clinic , Stomatology , Public Health , Traditional Chinese Medicine , and Traditional Chinese Medicine Combined with Western Medicine ( denoted as Chinese Western Medicine ) .
Only one ( Clinic ) of them has been studied in previous research .
( 3 ) MLEC - QA provides extra labels of ﬁve question types ( A1 , A2 , A3 / A4 and B1 ) for each question , and an in - depth analysis of the most frequent reasoning types of the questions in MLEC - QA , such as lexical matching , multi - sentence reading and concept summary , etc .
Detailed analysis can be found in Section 3.2 .
Examples of sub-ﬁelds and question types are summarized in Table 2 .
We set each example of ﬁve question types corresponding to one of the subﬁelds due to page limits .
[ Question ] 男，63 岁。3 小时前长跑后头痛伴呕吐 。 查体：查体不合作，嗜睡，双瞳孔对光 反射存在、颈项强直，四肢活动自如 ， 肌张力略高。双侧 Babinski 征明显，头 颅 CT 如图，该患者的诊断是 (
)
Male , 63 years old .
Had headache with vomiting after long - distance running 3 hours ago .
Physical Examination : not cooperative , somnolence , double pupillary light reﬂex exists , neck rigidity , free movement of limbs , muscle tension slightly high .
Bilateral Babinski sign is evident , CT of the head as shown in the ﬁgure .
Which option is the correct diagnosis of the patient :
[ Options ] A：蛛网膜下腔出血 / Subarachnoid hemorrhage B：脑室肿瘤 / Tumor of ventricle C：脑室囊肿 / Ventricular cyst D：脑室出血 / Ventricular hemorrhage E：脉络膜钙化 / Choroidal calciﬁcation
[ Answer ] D
analytics engine , ElasticSearch4 , as the document store and document retriever .
( 2 ) A reader ﬁnding the answer in given documents retrieved by the retriever .
We ﬁne - tune ﬁve pre - trained language models for machine reading comprehension as the reader .
Experimental results show that even the current best model can only achieve accuracies of 53 % , 44 % , 40 % , 55 % , and 50 % on the ﬁve categories of subsets : Clinic , Stomatology , Public Health , Traditional Chinese Medicine , and Chinese Western Medicine , respectively .
The models especially perform poorly on questions that require understanding comprehensive biomedical concepts and handling complex reasoning .
Question Type
Example ( ∗ represents the correct answer )
A1
B1
A2
A3
A4
( Public Health )
What should be used to compare the results of two samples ?
∗E. Rank sum test A. T test B. x2 test C. µ test D. F test
( Chinese Western Medicine ) A. Heart and liver B. Spleen and lungs C. Liver and kidneys D. Heart and kidneys 1 .
What is the viscera that " Yikui " in Yikui homologous refers to ?
( E ) 2 .
What is the viscera that " water and ﬁre " in harmonization of water and ﬁre refers to ?
( D )
E. Spleen and kidneys
( Clinic ) Primipara , 29 years old , at 37 weeks of gestation .
Had jet vomiting once this morning , suddenly convulsed an hour ago and then went to hospital in a coma .
Physical examination : BP 180/120mmhg , urine protein ( + + + ) .
The most likely diagnosis of this patient is : ∗A. Eclampsia B. Hematencephalon C. Hysteria D. Epilepsy E. Cerebral thrombosis
( Traditional Chinese Medicine )
Female , 28 years old .
In the recent month , has oral ulcer attacks repeatedly , upset , difﬁculties to sleep at night , dry stool , defecates every 1 - 2 days , has dry mouth , does not like drinking water , and has yellow urine , red tongue , greasy fur , and rapid pulse .
1 .
The drug of choice for the treatment of the pattern is : A. Mirabilite B. Arctium lappa D. Baikal skullcap E. Gypsum 2 .
The appropriate compatible drug for the treatment of the disease is :
A. Angelica dahurica B. Cassia twig C. Rhizoma Zingiberis ∗D. Rheum ofﬁcinale 3 .
Which of the following drugs should be used with caution during menstruation ?
A. Semen sojae praeparatum ∗B. Rheum ofﬁcinale C. Coptis chinensis D. Lophatherum gracile E. Rhizoma phragmitis
∗C. Bamboo leaf
E. Ash bark
( Stomatology ) A 50 - year - old patient comes to the outpatient clinic 2 years after the end of radiotherapy for nasopharyngeal carcinoma outside hospital .
Examination : full mouth multiple teeth dental surfaces with different degrees of caries , some of the affected teeth have become residual crowns , residual roots , less intraoral saliva , more soft scaling of the dental surfaces and sulci .
1 . The diagnosis of this patient is : A. Acute caries D. Secondary caries 2 .
There are several treatment designs as follows , except : A. Design treatment of full mouth caries B. Endodontic treatment of teeth with hypodontia D. Remineralization adjunctive therapy
∗B. Rampant caries C. Chronic caries
∗C. Filling metal material
E. Smooth surface caries
E. Regular review
Table 1 : An example of questions with additional images in MLEC - QA dataset .
Table 2 : Examples of sub-ﬁelds and question types in MLEC - QA .
The Chinese version is in Appendix D.
As an attempt to solve MLEC - QA and provide strong baselines , we implement eight representative control methods and open - domain QA methods by a two - stage retriever - reader framework : ( 1 ) A retriever ﬁnding documents that ( might ) contain an answer from a large collection of documents .
We adopt Chinese Wikipedia dumps3 as our information sources , and use a distributed search and
In summary , the major contributions of this paper are threefold :
• We present MLEC - QA , the largest - scale Chinese multi - choice BQA dataset with extra materials , and it ﬁrst covers ﬁve biomedical sub-ﬁelds , only one of which has been studied in previous research .
3https://dumps.wikimedia.org/
4https://www.elastic.co/
8863  • We conduct an in - depth analysis on MLECQA , revealing that both comprehensive biomedical knowledge and sophisticated reasoning ability are required to answer questions .
• We implement eight representative methods as baselines and show the performance of existing methods on MLEC - QA , and provide an outlook for future research directions .
2 Related Work
retrieval ,
document / passage
Open - Domain BQA The Text REtrieval Conference ( TREC ) ( Voorhees and Tice , 2000 ) has triggered the open - domain BQA research .
At the time , most traditional BQA systems were employing complex pipelines with question processing , and answer processing modules .
Examples of such systems include EPoCare ( Niu et al , 2003 ) , MedQA ( Yu et al , 2007 ; Terol et al , 2007 ; Wang et al , 2007 ) and AskHERMES ( Cao et al , 2011 ) .
With the introduction of various BQA datasets that are focused on speciﬁc biomedical topics , such as BioASQ ( Tsatsaronis et al , 2015 ) , emrQA ( Pampari et al , 2018 ) and PubMedQA ( Jin et al , 2019 ) , pioneered by Chen et al ( 2017 ) , the modern open - domain BQA systems largely simpliﬁed the traditional BQA pipeline to a two - stage retriever - reader framework by combining information retrieval and machine reading comprehension models ( Ben Abacha et al , 2017 , 2019b ) .
Moreover , the extensive use of medical images ( e.g. , CT ) and tables ( e.g. , laboratory examination ) has improved results in real - world clinical scenarios , making the BQA a task lying at the intersection of Computer Vision ( CV ) and NLP .
However , most BQA models focus on either texts or images ( Lau et al , 2018 ; Ben Abacha et al , 2019a ; He et al , 2020a ) ; as a result , BQA datasets that are composed by fusing different biomedical resources are relatively limited .
Open - Domain Multi - Choice BQA Datasets With rapidly increasing numbers of consumers asking health - related questions on online medical consultation websites , cMedQA ( Zhang et al , 2017 , 2018a ) , webMedQA ( He et al , 2019 ) and ChiMed ( Tian et al , 2019 ) exploit patient - doctor QA data to build consumer health QA datasets .
However , the quality problems in such datasets are that the answers are written by online - doctors and
the data itself has intrinsic noise .
By contrast , medical licensing examinations , which are designed by human medical experts , often take the form of multi - choice questions , and contain a signiﬁcant number of questions that require comprehensive biomedical knowledge and multiple reasoning ability .
Such exams are the perfect data source to push the development of BQA systems .
Several datasets have been released that exploit such naturally existing BQA data , which are summarized in Table 3 .
Collecting from the Spain public healthcare specialization examination , HEAD - QA ( Vilares and Gómez - Rodríguez , 2019 ) contains multichoice questions from six biomedical categories , including Medicine , Pharmacology , Psychology , Nursing , Biology and Chemistry .
NLPEC ( Li et al , 2020 ) collects 21.7k multi - choice questions with human - annotated answers from the National Licensed Pharmacist Examination in China , but only a small number of sample data is available for public use .
Last but not least , clinical medicine , as one of the 24 categories in NMLEC , has been previously studied by MedQA ( Zhang et al , 2018b ) and MEDQA ( Jin et al , 2020 ) .
However , the former did not release any data or code , and the latter only focused on clinical medicine with 34k questions in their cross - lingual studies , questions with images or tables were not included , and none of the remaining categories in MLEC - QA were studied .
Dataset
Size
Content
Metric
Available
Language
Extra
cMedQA v1.0 cMedQA v2.0
webMedQA ChiMed NLPEC
MedQA MEDQA HEAD - QA
54k Consumer Health 108k Consumer Health 63k Consumer Health 24.9k Consumer Health
21k
Pharmacology
235k Clinical Medicine 61k Clinical Medicine 6.8k
Multi - Category
P@1 P@1 P@1 & MAP Acc Acc Acc Acc Acc
MLEC - QA
136k Multi - Category
Acc
Yes
Yes
Yes Yes
No *
No
Yes Yes
Yes
Chinese Chinese Chinese Chinese Chinese Chinese Chinese & English Spanish & English
Chinese
No
No
No
No
No
No
No Yes
Yes
Table 3 : Comparison of MLEC - QA with existing opendomain multi - choice BQA datasets .
No * indicates a small number of sample data is available .
Extra indicates if the dataset provides extra material to answer questions .
3 MLEC - QA Dataset
3.1 Data Collection
We collect 155,429 multi - choice questions from the 2006 to 2020 NMLEC and practice exercises from the Internet .
Except for the categories that do not use Chinese in examinations , all categories are included in MLEC - QA : Clinic ( Cli ) , Stomatology ( Sto ) , Public Health ( PH ) , Traditional Chinese
8864  Medicine ( TCM ) , and Chinese Western Medicine ( CWM ) .
After removing duplicated or incomplete questions ( e.g. , some options missing ) , there are 136,236 questions in MLEC - QA , and each question contains ﬁve candidate options with one correct / best option and four incorrect or partially correct options .
We describe in detail the JSON data structure of MLEC - QA in Appendix B.
MLEC - QA contains 1,286 questions with extra materials that provide additional information to answer correctly .
As shown in Figure 1 , the extra materials are all in a graphical format with various types , such as ECG , table of a patient ’s condition record , formula , CT , line graph , explanatory drawing , etc .
We include these questions with extra materials in MLEC - QA to facilitate future BQA explorations on the crossover studies of CV and NLP , although we will not exploit them in this work due to the various speciﬁcs involved in extra materials .
( a ) ECG
( b ) Table
( c ) Explanatory Drawing
( d ) CT
( e ) Line Graph
( f ) Formula
Figure 1 : Examples of extra materials .
Basically , as shown in Table 2 and Table 4 , the questions in MLEC - QA are divided into ﬁve types including :
• A1 : single statement question ; • B1 : similar to A1 , with a group of options
shared in multiple questions ;
• A2 : questions accompanied by a clinical scenario ;
• A3 : similar to A2 , with information shared
among multiple independent questions ;
• A4 : similar to A3 , with information shared among multiple questions , new information can be gradually added .
We further classify these questions into Knowledge Questions ( KQ ) and Case Questions ( CQ ) , where KQ ( A1+B1 ) focus on the deﬁnition and comprehension of biomedical knowledge , while CQ ( A2+A3 / A4 ) require analysis and practical application for real - world medical scenarios .
Both types of questions require multiple reasoning ability to answer .
Subset
Knowledge Questions
Case Questions
A1 ( Extra )
B1 ( Extra ) A2 ( Extra ) A3 / A4 ( Extra )
Cli 16,996 ( 19 ) Sto 14,796 ( 269 ) 10,413 ( 14 ) PH TCM 15,235 ( 10 ) CWM 14,051 ( 9 )
5,327 ( 21 ) 5,084 ( 84 ) 3,949 ( 2 ) 8,045 ( 14 ) 7,336 ( 22 )
6,823 ( 7 ) 3,528 ( 325 ) 2,469 ( 25 ) 6,044 ( 48 ) 5,370 ( 16 )
4,495 ( 26 ) 3,041 ( 311 ) 1,693 ( 55 ) 1,541 ( 9 ) 0
Total
33,641 26,449 18,524 30,865 26,757
Total
71,491
29,741
24,234
10,770
136,236
Table 4 : Statistics of question types in MLEC - QA , where " Extra " indicates number of questions with extra materials .
Only A1 , A2 , and B1 are used in the examination of Chinese Western Medicine .
For the Train / Dev / Test split , randomly splitting may cause data imbalance because the number of the ﬁve question types are various from each other ( e.g. , A1 is far more than others ) .
To ensure that the subsets have the same distribution of the question types , we split the data based on the question types , with 80 % training , 10 % development , and 10 % test .
The overall statistics of the MLEC - QA dataset are summarized in Table 5 .
We can see that the length of the questions and the vocabulary size in Clinic are larger than the rest of the subsets , explaining that clinical medicine may involve more medical subjects than other specialties .
Metric
Cli
Sto
PH
TCM
CWM
# of options per question Avg./Max .
question len Avg./Max .
option len Vocabulary size # of extra materials
5 46.51 / 332 9.07 / 100 46,175 73
5 37.12 / 341 9.71 / 101 41,178 989
5 36.76 / 352 9.72 / 125 35,790 96
5 32.24 / 340 7.25 / 200 38,904 81
5 30.63 / 280 7.77 / 130 38,187 47
# of questions Train Dev Test Total
26,913 3,365 3,363 33,641
21,159 2,645 2,645 26,449
14,818 1,852 1,854 18,524
24,692 3,086 3,087 30,865
21,406 2,676 2,675 26,757
Table 5 : The overall statistics of MLEC - QA .
Question / option length is calculated in characters .
Vocabulary size is measured by Pkuseg ( Luo et al , 2019 ) in words .
8865  3.2 Reasoning Types of the Questions
Since the annual examination papers are designed by a team of healthcare experts who try to follow the similar reasoning types distribution .
To better understand our dataset , we manually inspected 10 sets of examination papers ( 2 sets for each subﬁeld ) , and summarize the most frequent reasoning types of the questions from MLEC - QA and previous works ( Lai et al , 2017 ; Zhong et al , 2020 ) .
The examples are shown in Table 6 .
Notably , the " Evidence " is well - organized by us to show how models need to handle these reasoning issues to achieve promising performance in MLEC - QA .
The deﬁnition of reasoning types of the questions are as follows :
Lexical Matching This type of question is common and the simplest .
The retrieved documents are highly matched with the question , the correct answer exactly matches a span in the document .
As shown in the example , the model only needs to check which option is matched with .
Multi - Sentence Reading Unlike lexical matching , where questions and correct answers can be found within a single sentence , multi - sentence reading requires models reading multiple sentences to gather enough information to generate answers .
Concept Summary The correct options for this type of question do not appear directly in the documents .
It requires the model to understand and summarize the question relevant concepts after reading the documents .
As shown in the example , the model needs to understand and summarize the relevant mechanism of " Thermoregulation " , and infer that when an obstacle arises in thermoregulation , the body temperature will not be able to maintain a relatively constant level , that is , it will rise with the increase of ambient temperature .
Numerical Calculation This type of question involves logical reasoning and arithmetic operations related to mathematics .
As shown in the example , the model ﬁrst needs to judge the approximate age of month according to the height of the infant , and then reverse calculate the age of months according to the height formula of infants 7~12 months old to obtain the age in months : ( 68 - 65 ) / 1.5 + 6 = 8 .
Multi - Hop Reasoning This type of question requires several steps of logical reasoning over multiple documents to answer .
As shown in the example , the patient ’s hemoglobin ( HB ) value is low , indicating that the patient has anemia , and the supply of iron should be increased in their diet .
The model needs to compare the iron content of each option : the iron content of C , D and E is low and that of A , B is high , but B is not easily absorbed , so the best answer is A.
Reasoning Type
Example ( ∗ represents the correct answer )
Lexical Matching
Multi - Sentence Reading
Concept Summary
Numerical Calculation
Multi - Hop Reasoning
The main hallmark of peritonitis is : A. Signiﬁcant abdominal distension B. Abdominal mobility dullness C. Bowel sounds were reduced or absent D. Severe abdominal cramping ∗E. Peritoneal irritation signs Evidence : The hallmark signs of peritonitis are peritoneal irritation signs , i.e. , tenderness , muscle tension , and rebound tenderness .
Which is wrong in the following narrative relating to the appendix : A. The appendiceal artery is the terminal artery B. Appendiceal tissues contain abundant lymphoid follicles C. Periumbilical pain at appendicitis ∗D. Resection of the appendix in adults will impair the onset visceral pain body ’s immune function
E. There are argyrophilic cells in the deep part of the appendiceal mucosa , which are associated with carcinoid tumorigenesis Evidence : ( 1 ) The appendiceal artery is a branch of the ileocolic artery and is a terminal artery without collaterals ; ( 2 ) The appendix is a lymphoid organ[ ...
]Therefore , resection of the adult appendix does not compromise the body ’s immune function ; ( 3 ) The nerves of the appendix are supplied by sympathetic ﬁbers[ ... ]belonging to visceral pain ; ( 4 ) Argyrophilic cells are found in the appendiceal mucosa and are the histological basis for the development of appendiceal carcinoids .
The main hallmark of thermoregulatory disorders in hyperthermic environments is : A. Developed syncope B. Developed shock C. Dry heat of skin ∗D. Increased body temperature Evidence : The purpose of thermoregulation is to maintain body temperature in the normal range .
In hyperthermic environments , the thermoregulatory center is dysfunctional and can not maintain the body ’s balance of heat production and heat dissipation , so the body temperature is increased by the inﬂuence of ambient temperature .
E. Decreased body temperature
A normal infant , weighing 7.5 kg and measuring 68 cm in length .
Bregma 1.0 cm , head circumference 44 cm .
Teething 4 .
Can sit alone and can pick up pellets with a hallux and foreﬁnger .
The most likely age of the infant is : ∗A. 8 months B. 24 months C. 18 months D. 12 months E. 5 months Evidence : A normal infant measured 65 cm at 6 months and 75 cm at 1 year of age .
The infant ’s 7 to 12 month length is calculated as : length = 65 + ( months of age - 6 ) x 1.5 .
6 - month - old female infant , artiﬁcial feeding mainly , physical examination revealed a low hemoglobin ( HB ) value , the dietary supplement that should be mainly added is : ∗A. Liver paste B. Egg yolk paste C. Tomato paste D. Rice paste E. Apple puree Evidence : ( 1 ) Low HB value indicates anemia tendency .
Iron deﬁciency anemia is the most important and common type of anemia in China .
( 2 ) Iron supply should be increased in diet .
( 3 ) Liver paste is rich in iron .
( 4 ) The iron content of egg yolk paste is lower than that of liver paste , and it is not easy to be absorbed .
( 5 ) The iron content of tomato paste , rice paste and apple puree is lower than that of liver paste .
Table 6 : Examples of reasoning types of the questions in MLEC - QA .
The Chinese version is in Appendix E.
4 Methods
represents
Notation We represent MLEC - QA task as : ith ( D , Q , O , A ) , where Qi Question , Di represents the collection of retrieved question relevant Documents , Oi = { OiA , OiB , OiC , OiD , OiE } are the candidate Options , Ai represents Answer , and we use A′ i to denote the Predicted Answer .
the
8866  4.1 Document Retriever
books
counseling
examination
Both and Wikipedia have been used as the source of supporting materials in previous research ( Zhong et al , 2020 ; Jin et al , 2020 ; Vilares and GómezRodríguez , 2019 ) .
However , because examination counseling books are designed to help examinees pass the examination , knowledge is highly simpliﬁed and summarized ; even the easily confused knowledge points are compared .
Using examination counseling books as information sources may make the retriever - reader more likely to exploit shallow text matching , and complex reasoning is seldom involved .
Therefore ,
to help better understand the improvement coming from future models , we adopt Chinese Wikipedia dumps as our information sources , which contain a wealth of information ( over 1 million articles ) of real - world facts .
Building upon the whole Chinese Wikipedia data , we use a distributed search and analytics engine , ElasticSearch , as the document store and document retriever , which supports very fast full - text searches .
The similarity scoring function used in Elasticsearch is the BM25 algorithm ( Robertson and Zaragoza , 2009 ) , which measures the relevance of documents to a given search query .
As deﬁned in Appendix C , the larger this BM25 score , the stronger the relevance between document and query .
Speciﬁcally , for each question Qi and each candidate option Oij where j ∈ { A , B , C , D , E } , we deﬁne QiOij = Qi + Oij as a search query to Elasticsearch and is repeated for all options .
The document with the highest BM25 score returned by each query is selected as supporting materials for the next stage machine reading comprehension task .
4.2 Control Methods
In general , each option should have the same correct rate for multi - choice questions , but in fact , the order in which the correct options appear is not completely random , and the more the number of options , the lower the degree of randomization ( Poundstone , 2014 ) .
Given the complex nature of multi - choice tasks , we employ three control methods to ensure a fair comparison among various open - domain QA models .
Random A′ = Random(O ) .
For each question , an option is randomly chosen as the answer
from ﬁve candidate options .
We perform this experiment ﬁve times and average the results as the baseline of the Random method .
Constant A′ = Constantj(O ) , where j ∈ { A , B , C , D , E } .
For each question , the jth option is always chosen as the answer to obtain the accuracy distribution of ﬁve candidate options .
Mixed A′ = M ixed(O ) .
Incorporating the previous experiences of NMLEC and multi - choice task work ( Vilares and Gómez - Rodríguez , 2019 ) , the Mixed method simulates how humans solving uncertain questions , and consists of the following three strategies : ( 1 ) the correct rate of choosing " All of the options above is correct / incorrect " is much higher than the other options .
( 2 ) Supposing the length of options is roughly equal , only one option is obviously longer with more detailed and speciﬁc descriptions , or is obviously shorter than the other options , then choose this option .
( 3 ) The correct option tends to appear in the middle of candidate options .
The three strategies are applied in turn .
If any strategy matches , then the option that matches the strategy is chosen as the answer .
4.3 Fine - Tuning Pre - Trained Language
Models
We apply an uniﬁed framework UER - py ( Zhao et al , 2019 ) to ﬁne - tuning pre - trained language models on the machine reading comprehension task as our reader .
We consider the following ﬁve pre - trained language models : Chinese BERT - Base ( denoted as BERT - Base ) and Multilingual Uncased BERT - Base ( denoted as BERT - BaseMultilingual ) ( Devlin et al , 2019 ) , Chinese BERTBase with whole word masking and pre - trained over larger corpora ( denoted as BERT - wwm - ext ) ( Cui et al , 2019 ) , and the robustly optimized BERTs : Chinese RoBERTa - wwm - ext and Chinese RoBERTa - wwm - ext - large ( Cui et al , 2019 ) .
Speciﬁcally , given the ith question Qi , retrieved question relevant documents Di , and a candidate option Oij , where j ∈ { A , B , C , D , E } .
The input sequence for the framework is constructed by concatenating [ CLS ] , tokens in Di , [ SEP ] , tokens in Qi , [ SEP ] , tokens in an option Oij , and [ SEP ] , where [ CLS ] is the classiﬁer token , and [ SEP ] is the sentence separator in pre - trained language models .
We pass each of the ﬁve options in turn , and the model outputs the hidden state representation Sij ∈ R1×H of the input sequence ,
8867  then performs the classiﬁcation and output an unnormalized log probability Pij ∈ R of each option Oij being correct by Pij = SijW T , where W ∈ R1×H is the weight matrix .
Finally , we pass the unnormalized log probabilities of each option through a softmax layer and obtain the option with the highest probability as the predicted answer A′ i.
5 Experiments
5.1 Experimental Settings
We conduct detailed experiments and analyses to investigate the performance of control methods and open - domain QA methods on MLEC - QA .
As shown in Figure 2 , we implement a two - stage ( 1 ) a retriever ﬁrst retriever - reader framework : retrieves question relevant documents from Chinese Wikipedia using ElasticSearch , ( 2 ) and then a reader employs machine reading comprehension models to generate answers in given documents retrieved by the retriever .
For the reader , all machine reading comprehension models are trained with 12 epochs , an initial learning rate of 2e-6 , a maximum sequence length of 512 , a batch size of 5 .
The parameters are selected based on the best performance on the development set , and we keep the default values for the other hyper - parameters ( Devlin et al , 2019 ) .
We use accuracy as the metric to evaluate different methods , and provide baseline results , as well as human pass mark ( 60 % ) instead of human performance due to the wide variations exist in human performance , from almost full marks to can not even pass the exam .
Figure 2 : Overview of the two - stage retriever - reader framework on MLEC - QA .
5.2 Retrieval Performance
The main drawbacks of the Chinese Wikipedia database in biomedicine are that it is not comprehensive and thorough , that is , it may not provide complete coverage of all subjects .
To evaluate whether retrieved documents can cover enough evidence to answer questions , we sampled 5 %
( 681 ) questions from the development sets of ﬁve categories using stratiﬁed random sampling , and manually annotate each question by ﬁve medical experts with 3 labels : ( 1 ) Exactly Match ( EM ): the retrieved documents exactly match the question .
( 2 ) Partial Match ( PM ): the retrieved documents partially match the question , can be confused with the correct options or are incomplete .
( 3 ) Mismatch ( MM ): the retrieved documents do not match the question at all .
Table 7 lists the performance of the retrieval strategy as well as the results of the annotation for KQ and CQ questions on ﬁve subsets .
Subset
EM ( KQ / CQ )
PM ( KQ / CQ )
MM ( KQ / CQ )
Cli 15.63 Sto 20.83 6.25 PH TCM 10.71 CWM 20.83
( 18.75 / 12.5 ) ( 16.67 / 25 ) ( 0 / 12.5 ) ( 14.29 / 7.14 ) ( 8.33 / 33.33 )
75 54.17 43.75 50 54.17
( 68.75 / 81.25 ) ( 50 / 58.33 ) ( 50 / 37.5 ) ( 42.86 / 57.14 ) ( 58.33 / 50 )
9.38 25 50 39.29 25
( 12.5 / 6.25 ) ( 33.33 / 16.67 ) ( 50 / 50 ) ( 42.86 / 35.71 ) ( 33.33 / 16.67 )
Table 7 : Matching rate ( % ) of retrieved documents that exactly match , partial match or mismatch with the questions in the MLEC - QA dataset .
From the table , we make the following observations .
First , most retrieved documents indicate PM with the questions , while the matching rates of EM and MM achieve maximums of 20.83 % ( CWM ) and 50 % ( PH ) , respectively .
Second , the matching rate of CQ is higher than KQ in most subsets as CQ are usually related to simpler concepts , and use more words to describe questions , which leads to easier retrieval .
By contrast , KQ usually involve more complex concepts that may not be included in the Chinese Wikipedia database .
Therefore , the mismatching rate of KQ is signiﬁcantly higher than that of CQ .
Third , among different subsets , the performance in the subset Cli achieves the best as clinical medicine is more " general " to retrieve compare with other specialties .
Whereas the performance in the subset PH achieves the worst as the Public Health is usually related to " confusing concepts " , which leads to poor retrieval performance .
5.3 Baseline Results
Tables 8 and Figure 3 show the performance of baselines as well as the performance on KQ and CQ questions .
As we can see , among control methods , the correct option has a slight tendency to appear in the middle ( C and D ) of candidate options , but the margins are small .
The performance of the Mixed method is slightly better than a random guess , which indicates that the ﬂexible use of
8868  Method
Cli
Sto
PH
TCM
CWM
Dev
Test
Dev
Test
Dev
Test
Dev
Test
Dev
Test
Random
Constant
Mixed
Option [ A ] Option [ B ] Option
[ C ] Option [ D ] Option [ E ]
BERT - Base BERT - wwm - ext BERT - Base - Multilingual RoBERTa - wwm - ext RoBERTa - wwm - ext - large
Human Pass Mark
19.73 17.12 20.30 21.66 22.53 18.40 24.40
47.26 50.27 46.61 49.94 53.25
19.61 18.05 20.93 21.23 21.38 18.41 24.68
48.30 50.89 47.68 51.97 53.22
19.41 16.82 20.95 22.31 20.64 19.28 24.46
40.53 43.26 39.85 41.14 44.92
19.43 17.01 20.53 20.95 22.76 18.75 24.42
40.08 42.05 38.76 40.88 43.75
19.70 16.09 21.49 21.65 22.68 18.09 23.97
38.99 41.75 36.61 38.40 39.10
20.13 17.80 21.36 20.23 21.84 18.77 23.68
37.40 40.04 36.70 38.91 38.75
60
20.17 19.05 22.52 22.29 19.80 16.33 22.07
48.51 54.57 45.50 50.45 47.99
20.11 20.60 20.70 22.35 20.67 15.68 22.38
49.14 54.94 46.61 49.82 48.65
19.69 18.68 20.70 23.39 21.00 16.22 23.62
44.32 49.89 42.26 47.38 50.49
20.16 20.56 21.20 21.27 19.51 17.46 23.25
45.14 50.04 42.86 46.00 50.11
Table 8 : Performance of baselines in accuracy ( % ) on the MLEC - QA dataset .
guessing skills may add wings to the tiger as humans can exclude some certain wrong options , but if the cart before the horse is reversed , it is impossible to pass the exam only through opportunistic guessing .
RoBERTa - wwm - ext - large and BERTwwm - ext perform better than other models on ﬁve subsets .
However , even the best - performing model can only achieve accuracies between 40 % to 55 % on ﬁve subsets , so there is still a gap to pass the exams .
Figure 3 : Performance in accuracy ( % ) on KQ ( solid lines ) and CQ ( dashed lines ) questions .
Comparing the performance between KQ and CQ questions , most models achieve better performance on CQ , which is positively correlated with CQ ’s better retrieval performance .
Among different subsets , the subset TCM is the easiest ( 54.95 % ) one to answer across the board , while the subset PH is the hardest ( 40.04 % ) , which does not totally correspond to their retrieval performance as
shown in Table 7 .
The possible reason is that the diagnosis and treatment of diseases in traditional Chinese medicine are characterized by " Homotherapy for Heteropathy " , that is , treating different diseases with the same method , which may result in some patterns or mechanisms that can be used by the models to reach such results .
5.4 Comparative Analysis
Given that we use the Chinese Wikipedia database as our information sources and apply a two - stage retriever - reader framework , the reason for such poor baseline performance could come from both our information sources and the retriever - reader framework .
books
Information Sources Both and Wikipedia have been used as the information sources in previous research .
One of our subsets , Clinic , has been studied by MEDQA ( Jin et al , 2020 ) as a subset ( MCMLE ) for cross - lingual research .
MEDQA uses 33 medical textbooks as their information sources and the evaluation result shows that their collected text materials can provide enough information to answer all the questions in MCMLE .
We compare the best model ( RoBERTa - wwm - ext - large ) performance on both datasets as shown in Table 9 .
Notably , questions in MCMLE have four candidate options due to one of the wrong options being deleted .
Therefore , the random accuracy on MCMLE is higher than ours .
From the results we can see that even with 100 % covered materials , the best model can only
8869  Dataset
Accuracy ( % ) Test Dev
MEDQA ( MCMLE ) MLEC - QA ( Clinic )
69.30 53.25
70.10 53.22
Table 9 : Comparison of best model ( RoBERTa - wwmext - large ) performance on MEDQA and our MLECQA dataset .
achieve 16.88 % higher accuracy on the test set than ours , which indicates that using Wikipedia as information sources is not that terrible compared with medical books , and the main reason for baseline performance may come from machine reading comprehension models that lack sophisticated reasoning ability .
Retriever - Reader We also perform an experiment that sampled 5 % ( 92 ) questions from the development set of Public Health , and manually annotate each question by a medical expert to determine whether that can exactly or partially match with the top K retrieved documents , as shown in Table 10 .
Notably , the actual number of retrieved documents is 5×K as we deﬁne QiOij = Qi+Oij as a search query and is repeated for all options .
From the results , we can see that more documents even bring more noise instead , as the best match documents have already been fetched in the top 1 documents .
It indicates that the poor performance of machine reading comprehension models is coming from the insufﬁciency of reasoning ability rather than the number of retrieved documents .
Top K
1
2
3
4
5
Match 52.08
42.19
36.25
32.29
29.46
Table 10 : Matching rate ( % ) of Top K retrieved documents that exactly or partial match with the questions in the Public Health subset .
6 Conclusion
specialized domain knowledge and multiple reasoning abilities to be answered .
We implement eight representative control methods and opendomain QA methods by a two - stage retrieverreader framework as baselines .
The experimental results demonstrate that even the current best approaches can not achieve good performance on MLEC - QA .
We hope MLEC - QA can beneﬁt researchers on improving the open - domain QA models , and also make advances for BQA systems .
