An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction
Shun Kiyono 1,2 Jun Suzuki 2,1 Masato Mita 1,2 Tomoya Mizumoto 1,2∗
Kentaro Inui 2,1
1 RIKEN Center for Advanced Intelligence Project
2 Tohoku University
{ shun.kiyono , masato.mita , tomoya.mizumoto}@riken.jp ; { jun.suzuki,inui}@ecei.tohoku.ac.jp
Abstract
The incorporation of pseudo data in the training of grammatical error correction models has been one of the main factors in improving the performance of such models .
However , consensus is lacking on experimental conﬁgurations , namely , choosing how the pseudo data should be generated or used .
In this study , these choices are investigated through extensive experiments , and state - of - the - art performance is achieved on the CoNLL-2014 test set ( F0.5 = 65.0 ) and the ofﬁcial test set of the BEA-2019 shared task ( F0.5 = 70.2 ) without making any modiﬁcations to the model architecture .
1
Introduction
To date , many studies have tackled grammatical error correction ( GEC ) as a machine translation ( MT ) task , in which ungrammatical sentences are regarded as the source language and grammatical sentences are regarded as the target language .
This approach allows cutting - edge neural MT models to be adopted .
For example , the encoder - decoder ( EncDec ) model ( Sutskever et al , 2014 ; Bahdanau et al , 2015 ) , which was originally proposed for MT , has been applied widely to GEC and has achieved remarkable results in the GEC research ﬁeld ( Ji et al , 2017 ; Chollampatt and Ng , 2018 ; JunczysDowmunt et al , 2018 ) .
However , a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data ( Koehn and Knowles , 2017 ) , but the largest set of publicly available parallel data ( Lang-8 ) in GEC has only two million sentence pairs ( Mizumoto et al , 2011 ) .
Consequently , the method of augmenting the data by incorporating pseudo training data has been studied intensively ( Xie et al , 2018 ; Ge et al , 2018 ; Lichtarge et al , 2019 ; Zhao et al , 2019 ) .
When incorporating pseudo data , several decisions must be made about the experimental conﬁgurations , namely , ( i ) the method of generating the pseudo data , ( ii ) the seed corpus for the pseudo data , and ( iii ) the optimization setting ( Section 2 ) .
However , consensus on these decisions in the GEC research ﬁeld is yet to be formulated .
For example , Xie et al ( 2018 ) found that a variant of the backtranslation ( Sennrich et al , 2016b ) method ( BACKTRANS ( NOISY ) ) outperforms the generation of pseudo data from raw grammatical sentences ( DIRECTNOISE ) .
By contrast , the current state of the art model ( Zhao et al , 2019 ) uses the DIRECTNOISE method .
In this study , we investigate these decisions regarding pseudo data , our goal being to provide the research community with an improved understanding of the incorporation of pseudo data .
Through extensive experiments , we determine suitable settings for GEC .
We justify the reliability of the proposed settings by demonstrating their strong performance on benchmark datasets .
Speciﬁcally , without any task - speciﬁc techniques or architecture , our model outperforms not only all previous single - model results but also all ensemble results except for the ensemble result by Grundkiewicz et al ( 2019)1 .
By applying task - speciﬁc techniques , we further improve the performance and achieve state - of - the - art performance on the CoNLL-2014 test set and the ofﬁcial test set of the BEA-2019 shared task .
2 Problem Formulation and Notation
In this section , we formally deﬁne the GEC task discussed in this paper .
Let D be the GEC training data that comprise pairs of an ungrammatical source sentence X and grammatical target sentence
1The paper ( Grundkiewicz et al 2019 ) has not been pub∗ Current afﬁliation : Future Corporation
lished yet at the time of submission .
Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing , pages1236–1242,HongKong , China , November3–7,2019.c(cid:13)2019AssociationforComputationalLinguistics1236  Y , i.e. , D = { ( Xn , Yn)}n .
Here , |D| denotes the number of sentence pairs in the dataset D.
Let Θ represent all trainable parameters of the model .
Our objective is to ﬁnd the optimal parameter set ( cid:98)Θ that minimizes the following objective function L(D , Θ ) for the given training data D :
L(D , Θ ) = −
log(p(Y |X , Θ ) ) , ( 1 )
( cid:88 )
1 |D|
( X , Y ) ∈D
where p(Y |X , Θ ) denotes the conditional probability of Y given X.
In the standard supervised learning setting , the parallel data D comprise only “ genuine ” parallel data Dg ( i.e. , D = Dg ) .
However , in GEC , incorporating pseudo data Dp that are generated from grammatical sentences Y ∈ T , where T represents seed corpus ( i.e. , a set of grammatical sentences ) , is common ( Xie et al , 2018 ; Zhao et al , 2019 ; Grundkiewicz et al , 2019 ) .
Our interest lies in the following three nontrivial aspects of Equation 1 .
Aspect ( i ): multiple methods for generating pseudo data Dp are available ( Section 3 ) .
Aspect ( ii ): options for the seed corpus T are numerous .
To the best of our knowledge , how the seed corpus domain affects the model performance is yet to be shown .
We compare three corpora , namely , Wikipedia , Simple Wikipedia ( SimpleWiki ) and English Gigaword , as a ﬁrst trial .
Wikipedia and SimpleWiki have similar domains , but different grammatical complexities .
Therefore , we can investigate how grammatical complexity affects model performance by comparing these two corpora .
We assume that Gigaword contains the smallest amount of noise among the three corpora .
We can therefore use Gigaword to investigate whether clean text improves model performance .
Aspect ( iii ): at least two major settings for incorporating Dp into the optimization of Equation 1 are available .
One is to use the two datasets jointly by concatenating them as D = Dg ∪ Dp , which hereinafter we refer to as JOINT .
The other is to use Dp for pretraining , namely , minimizing L(Dp , Θ ) to acquire Θ(cid:48 ) , and then ﬁne - tuning the model by minimizing L(Dg , Θ(cid:48 ) ) ; hereinafter , we refer to this setting as PRETRAIN .
We investigate these aspects through our extensive experiments ( Section 4 ) .
3 Methods for Generating Pseudo Data
In this section , we describe three methods for generating pseudo data .
In Section 4 , we experimentally compare these methods .
BACKTRANS ( NOISY ) and BACKTRANS ( SAMPLE ) Backtranslation for the EncDec model was proposed originally by Sennrich et al ( 2016b ) .
In backtranslation , a reverse model , which generates an ungrammatical sentence from a given grammatical sentence , is trained .
The output of the reverse model is paired with the input and then used as pseudo data .
BACKTRANS ( NOISY ) is a variant of backtranslation that was proposed by Xie et al ( 2018)2 .
This method adds rβrandom to the score of each hypothesis in the beam for every time step .
Here , noise r is sampled uniformly from the interval [ 0 , 1 ] , and βrandom ∈ R≥0 is a hyper - parameter that controls the noise scale .
If we set βrandom = 0 , then BACKTRANS ( NOISY ) is identical to standard backtranslation .
BACKTRANS ( SAMPLE ) is another variant of backtranslation , which was proposed by Edunov et al ( 2018 ) for MT .
In BACKTRANS ( SAMPLE ) , sentences are decoded by sampling from the distribution of the reverse model .
DIRECTNOISE Whereas BACKTRANS ( NOISY ) and BACKTRANS ( SAMPLE ) generate ungrammatical sentences with a reverse model , DIRECTNOISE injects noise “ directly ” into grammatical sentences ( Edunov et al , 2018 ; Zhao et al , 2019 ) .
Speciﬁcally , for each token in the given sentence , this method probabilistically chooses one of the following operations : ( i ) masking with a placeholder token ( cid:104)mask(cid:105 ) , ( ii ) deletion , ( iii ) insertion of a random token , and ( iv ) keeping the original3 .
For each token , the choice is made based on the categorical distribution ( µmask , µdeletion , µinsertion , µkeep ) .
4 Experiments
The goal of our experiments is to investigate aspect ( i)–(iii ) introduced in Section 2 .
To ensure that the experimental ﬁndings are applicable to GEC in general , we design our experiments by using the following two strategies : ( i ) we use an off - the - shelf EncDec model without any task - speciﬁc architecture or techniques ; ( ii ) we conduct hyper - parameter tuning , evaluation and comparison of each method or setting on the validation set .
At the end of experiments ( Section 4.5 ) , we summarize our ﬁndings and propose suitable settings .
We then perform a single - shot evaluation of their performance on the test set .
2referred as “ random noising ” in Xie et al ( 2018 ) 3The detailed algorithm is described in Appendix A.
1237  Dataset
# sent ( pairs ) # refs .
Split
Scorer
BEA - train BEA - valid
CoNLL-2014 JFLEG BEA - test
SimpleWiki∗ Wikipedia∗ Gigaword∗
561,410 2,377
1,312 1,951 4,477
train valid
ERRANT
test ERRANT & M 2 scorer test test
GLEU ERRANT
1,369,460 145,883,941 131,864,979
1 1
2 4 5
Table 1 : Summary of datasets used in our experiments .
Dataset marked with “ * ” is a seed corpus T .
Method
Baseline
BACKTRANS ( SAMPLE ) BACKTRANS ( NOISY ) DIRECTNOISE
Prec .
Rec .
F0.5
46.6
44.6 42.5 48.9
23.1
27.4 31.3 25.7
38.8
39.6 39.7 41.4
Table 2 : Performance of models on BEA - valid : a value in bold indicates the best result within the column .
The seed corpus T is SimpleWiki .
4.1 Experimental Conﬁgurations
Dataset The BEA-2019 workshop ofﬁcial dataset4 is the origin of the training and validation data of our experiments .
Hereinafter , we refer to the training data as BEA - train .
We create validation data ( BEA - valid ) by randomly sampling sentence pairs from the ofﬁcial validation split5 .
As a seed corpus T , we use SimpleWiki6 , Wikipedia7 or Gigaword8 .
We apply the noizing methods described in Section 3 to each corpus and generate pseudo data Dp .
The characteristics of each dataset are summarized in Table 1 .
Evaluation We report results on BEA - valid , the ofﬁcial test set of the BEA-2019 shared task ( BEA - test ) , the CoNLL-2014 test set ( CoNLL2014 ) ( Ng et al , 2014 ) , and the JFLEG test set ( JFLEG ) ( Napoles et al , 2017 ) .
All reported results ( except ensemble ) are the average of ﬁve distinct trials using ﬁve different random seeds .
We report the scores measured by ERRANT ( Bryant et al , 2017 ; Felice et al , 2016 ) for BEA - valid , BEA - test , and CoNLL-2014 .
As the reference sentences of BEAtest are publicly unavailable , we evaluate the model outputs on CodaLab9 for BEA - test .
We also report results measured by the M 2 scorer ( Dahlmeier and Ng , 2012 ) on CoNLL-2014 to compare them with those of previous studies .
We use the GLEU metric ( Napoles et al , 2015 , 2016 ) for JFLEG .
the Transformer EncDec Model We adopt model ( Vaswani et al , 2017 ) using the fairseq toolkit ( Ott et al , 2019 ) and use the “ Transformer ( big ) ” settings of Vaswani et al ( 2017 ) .
Optimization
For the JOINT setting , we opti4Details of the dataset is in Appendix B. 5The detailed data preparation process is in Appendix C. 6https://simple.wikipedia.org 7We used 2019 - 02 - 25 dump ﬁle at https://dumps .
wikimedia.org/other/cirrussearch/.
8We used the English Gigaword Fifth Edition ( LDC Catalog No . : LDC2011T07 ) .
competitions/20228
mize the model with Adam ( Kingma and Ba , 2015 ) .
For the PRETRAIN setting , we pretrain the model with Adam and then ﬁne - tune it on BEA - train using Adafactor ( Shazeer and Stern , 2018)10 .
4.2 Aspect ( i ): Pseudo Data Generation
( NOISY ) , BACKTRANS
We compare the effectiveness of the BACK(SAMPLE ) , TRANS and DIRECTNOISE methods generatfor ing pseudo data .
In DIRECTNOISE , we = ( µmask , µdeletion , µinsertion , µkeep ) set ( 0.5 , 0.15 , 0.15 , 0.2)11 .
We use βrandom = 6 for BACKTRANS ( NOISY)12 .
In addition , we use ( i ) the JOINT setting and ( ii ) all of SimpleWiki as the seed corpus T throughout this section .
The results are summarized in Table 2 . BACKTRANS ( NOISY ) and BACKTRANS ( SAMPLE ) show competitive values of F0.5 .
Given this result , we exclusively use BACKTRANS ( NOISY ) and discard BACKTRANS ( SAMPLE ) for the rest of the experiments .
The advantage of BACKTRANS ( NOISY ) is that its effectiveness in GEC has already been demonstrated by Xie et al ( 2018 ) .
In addition , in our preliminary experiment , BACKTRANS ( NOISY ) decoded ungrammatical sentence 1.2 times faster than BACKTRANS ( SAMPLE ) did .
We also use DIRECTNOISE because it achieved the best value of F0.5 among all the methods .
4.3 Aspect ( ii ): Seed Corpus T
We investigate the effectiveness of the seed corpus T for generating pseudo data Dp .
The three corpora ( Wikipedia , SimpleWiki and Gigaword ) are compared in Table 3 .
We set |Dp| = 1.4M.
The difference in F0.5 is small , which implies that the seed corpus T has only a minor effect on the model performance .
Nevertheless , Gigaword consistently outperforms the other two corpora .
In particular ,
10The detailed hyper - parameters are listed in Appendix D. 11These values are derived from preliminary experiments
( Appendix E ) .
experiments ( Appendix F ) .
9https://competitions.codalab.org/
12βrandom = 6 achieved the best F0.5 in our preliminary
1238  Method
Baseline
Seed Corpus T
Prec .
Rec .
F0.5
Optimization Method
N / A
46.6
23.1 38.8
N / A
Baseline
|Dp| Prec .
Rec .
F0.5
0
46.6 23.1 38.8
BACKTRANS ( NOISY ) BACKTRANS ( NOISY ) BACKTRANS ( NOISY )
DIRECTNOISE DIRECTNOISE DIRECTNOISE
Wikipedia SimpleWiki Gigaword
Wikipedia SimpleWiki Gigaword
43.8 42.5 43.1
48.3 48.9 48.3
30.8 40.4 31.3 39.7 33.1 40.6
25.5 41.0 25.7 41.4 26.9 41.7
Table 3 : Performance on BEA - valid when changing the seed corpus T used for generating pseudo data ( |Dp| = 1.4 M ) .
DIRECTNOISE with Gigaword achieves the best value of F0.5 among all the conﬁgurations .
4.4 Aspect ( iii ): Optimization Setting
We compare the JOINT and PRETRAIN optimization settings .
We are interested in how each setting performs when the scale of the pseudo data Dp compared with that of the genuine parallel data Dg is ( i ) approximately the same ( |Dp| = 1.4 M ) and ( ii ) substantially bigger ( |Dp| = 14 M ) .
Here , we use Wikipedia as the seed corpus T instead of SimpleWiki or Gigaword for two reasons .
First , SimpleWiki is too small for the experiment ( b ) ( see Table 1 ) .
Second , the fact that Gigaword is not freely available makes it difﬁcult for other researchers to replicate our results .
( a ) Joint Training or Pretraining Table 4 presents the results .
The most notable result here is that PRETRAIN demonstrates the properties of more pseudo data and better performance , whereas JOINT does not .
For example , in BACKTRANS ( NOISY ) , increasing |Dp| ( 1.4 M → 14 M ) improves F0.5 on PRETRAIN ( 41.1 → 44.5 ) .
By contrast , F0.5 does not improve on JOINT ( 40.4 → 40.3 ) .
An intuitive explanation for this case is that when pseudo data Dp are substantially more than genuine data Dg , the teaching signal from Dp becomes dominant in JOINT .
PRETRAIN alleviates this problem because the model is trained with only Dg during ﬁne - tuning .
We therefore suppose that PRETRAIN is crucial for utilizing extensive pseudo data .
( b ) Amount of Pseudo Data We investigate how increasing the amount of pseudo data affects the PRETRAIN setting .
We pretrain the model with different amounts of pseudo data { 1.4 M , 7 M , 14 M , 30 M , 70 M } .
The results in Figure 1 show that BACKTRANS ( NOISY ) has superior sample efﬁciency to DIRECTNOISE .
The best model ( pretrained with 70 M BACKTRANS ( NOISY ) ) achieves
PRETRAIN BACKTRANS ( NOISY ) 1.4 M 49.6 24.3 41.1 PRETRAIN DIRECTNOISE 1.4 M 48.4 21.2 38.5 BACKTRANS ( NOISY ) 1.4 M 43.8 30.8 40.4 JOINT 1.4 M 48.3 25.5 41.0 DIRECTNOISE JOINT
PRETRAIN BACKTRANS ( NOISY ) 14 M 50.6 30.1 44.5 PRETRAIN DIRECTNOISE 14 M 49.8 25.8 42.0 BACKTRANS ( NOISY ) 14 M 43.0 32.3 40.3 JOINT 14 M 48.7 23.5 40.1 DIRECTNOISE JOINT
Table 4 : Performance of the model with different optimization settings on BEA - valid .
The seed corpus T is Wikipedia .
Figure 1 : Performance on BEA - valid for different amounts of pseudo data ( |Dp| ) .
The seed corpus T is Wikipedia .
F0.5 = 45.9 .
4.5 Comparison with Current Top Models
The present experimental results show that the following conﬁgurations are effective for improving the model performance : ( i ) the combination of JOINT and Gigaword ( Section 4.3 ) , ( ii ) the amount of pseudo data Dp not being too large in JOINT ( Section 4.4(a ) ) , and ( iii ) PRETRAIN with BACKTRANS ( NOISY ) using large pseudo data Dp ( Section 4.4(b ) ) .
We summarize these ﬁndings and attempt to combine PRETRAIN and JOINT .
Specifically , we pretrain the model using 70 M pseudo data of BACKTRANS ( NOISY ) .
We then ﬁne - tune the model by combining BEA - train and relatively small DIRECTNOISE pseudo data generated from Gigaword ( we set |Dp| = 250 K ) .
However , the performance does not improve on BEA - valid .
Therefore , the best approach available is simply to pretrain the model with large ( 70 M ) BACKTRANS ( NOISY ) pseudo data and then ﬁne - tune using BEAtrain , which hereinafter we refer to as PRETLARGE .
We use Gigaword for the seed corpus T because it has the best performance in Table 3 .
We evaluate the performance of PRETLARGE on test sets and compare the scores with the current top models .
Table 5 shows a remarkable result , that is ,
1239010203040506070AmountofPseudoData|Dp|(M)40424446F0.5scoreBaselineBacktrans(noisy)DirectNoise  CoNLL-2014 ( M 2 scorer )
CoNLL-2014 ( ERRANT )
JFLEG
BEA - test ( ERRANT )
Model
Ensemble
Prec .
Rec .
F0.5
Prec .
Rec .
F0.5
GLEU
Prec .
Rec .
F0.5
Chollampatt and Ng ( 2018 ) Junczys - Dowmunt et al ( 2018 ) Grundkiewicz and Junczys - Dowmunt ( 2018 ) Lichtarge et al ( 2019 )
Chollampatt and Ng ( 2018 ) Junczys - Dowmunt et al ( 2018 )
Lichtarge et al ( 2019 ) Zhao et al ( 2019 )
Grundkiewicz et al ( 2019 )
PRETLARGE
PRETLARGE+SSE+R2L PRETLARGE+SSE+R2L+SED
60.9 66.8 65.5
65.5 61.9 66.7 71.6 67.9
72.4 73.3
23.7 34.5 37.1
33.1 40.2 43.9 38.7 44.1
46.1 44.2
46.4 53.0 56.3 56.8
54.8 55.8 60.4 61.2 64.2
61.3
65.0 64.7
61.2
67.3 68.1
42.0
44.0 42.1
56.0
60.9 60.6
51.3 57.9 61.5 61.6
57.5 59.9 63.3 61.0 61.2
59.7
61.4 61.2
72.3
65.5
72.1 74.7
60.1
59.4
61.8 56.7
69.5
64.2
69.8 70.2
( cid:88 ) ( cid:88 ) ( cid:88 ) ( cid:88 ) ( cid:88 )
( cid:88 ) ( cid:88 )
Table 5 : Comparison of our best model and current top models : a bold value indicates the best result within the column .
our PRETLARGE achieves F0.5 = 61.3 on CoNLL2014 .
This result outperforms not only all previous single - model results but also all ensemble results except for that by Grundkiewicz et al ( 2019 ) .
To further improve the performance , we incorporate the following techniques that are widely used in shared tasks such as BEA-2019 and WMT13 : Synthetic Spelling Error ( SSE ) Lichtarge et al ( 2019 ) proposed the method of probabilistically injecting character - level noise into the source sentence of pseudo data Dp .
Speciﬁcally , one of the following operations is applied randomly at a rate of 0.003 per character : deletion , insertion , replacement , or transposition of adjacent characters .
Right - to - left Re - ranking ( R2L ) Following Sennrich et al ( 2016a , 2017 ) ; Grundkiewicz et al ( 2019 ) , we train four right - to - left models .
The ensemble of four left - to - right models generate n - best candidates and their corresponding scores ( i.e. , conditional probabilities ) .
We then pass each candidate to the ensemble of the four right - to - left models and compute the score .
Finally , we re - rank the n - best candidates based on the sum of the two scores .
Sentence - level Error Detection ( SED ) SED classiﬁes whether a given sentence contains a grammatical error .
Asano et al ( 2019 ) proposed incorporating SED into the evaluation pipeline and reported improved precision .
Here , the GEC model is applied only if SED detects a grammatical error in the given source sentence .
The motivation is that SED could potentially reduce the number of false - positive errors of the GEC model .
We use the re - implementation of the BERT - based SED model ( Asano et al , 2019 ) .
Table 5 presents the results of applying SSE ,
13http://www.statmt.org/wmt19/
is noteworthy that PRETR2L , and SED .
It LARGE+SSE+R2L achieves state - of - the - art performance on both CoNLL-2014 ( F0.5 = 65.0 ) and BEA - test ( F0.5 = 69.8 ) , which are better than those of the best system of the BEA-2019 shared task ( Grundkiewicz et al , 2019 ) .
In addition , PRETLARGE+SSE+R2L+SED can further improve the performance on BEA - test ( F0.5 = 70.2 ) .
However , unfortunately , incorporating SED decreased the performance on CoNLL-2014 and JFLEG .
This fact implies that SED is sensitive to the domain of the test set since the SED model is ﬁne - tuned with the ofﬁcial validation split of BEA dataset .
We leave this sensitivity issue as our future work .
5 Conclusion
In this study , we investigated several aspects of incorporating pseudo data for GEC .
Through extensive experiments , we found the following to be effective : ( i ) utilizing Gigaword as the seed corpus , and ( ii ) pretraining the model with BACKTRANS ( NOISY ) data .
Based on these ﬁndings , we proposed suitable settings for GEC .
We demonstrated the effectiveness of our proposal by achieving stateof - the - art performance on the CoNLL-2014 test set and the BEA-2019 test set .
