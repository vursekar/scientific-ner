TOD - BERT : Pre - trained Natural Language Understanding for Task - Oriented Dialogue
Chien - Sheng Wu , Steven Hoi , Richard Socher , and Caiming Xiong Salesforce Research
[ wu.jason , shoi , rsocher , cxiong]@salesforce.com
Abstract
The underlying difference of linguistic patterns between general text and task - oriented dialogue makes existing pre - trained language models less useful in practice .
In this work , we unify nine human - human and multi - turn task - oriented dialogue datasets for language modeling .
To better model dialogue behavior during pre - training , we incorporate user and system tokens into the masked language modeling .
We propose a contrastive objective function to simulate the response selection task .
Our pre - trained task - oriented dialogue BERT ( TOD - BERT ) outperforms strong baselines like BERT on four downstream taskoriented dialogue applications , including intention recognition , dialogue state tracking , dialogue act prediction , and response selection .
We also show that TOD - BERT has a stronger few - shot ability that can mitigate the data scarcity problem for task - oriented dialogue .
1
Introduction
Pre - trained models with self - attention encoder architectures ( Devlin et al , 2018 ; Liu et al , 2019 ) have been commonly used in many NLP applications .
Such models are self - supervised based on a massive scale of general text corpora , such as English Wikipedia or books ( Zhu et al , 2015 ) .
By further ﬁne - tuning these representations , breakthroughs have been continuously reported for various downstream tasks , especially natural language understanding .
However , previous work ( Rashkin et al , 2018 ; Wolf et al , 2019 ) shows that there are some deﬁciencies in the performance to apply ﬁne - tuning on conversational corpora directly .
One possible reason could be the intrinsic difference of linguistic patterns between human conversations and writing text , resulting in a large gap of data distributions ( Bao et al , 2019 ) .
Therefore , pre - training dialogue
language models using chit - chat corpora from social media , such as Twitter or Reddit , has been recently investigated , especially for dialogue response generation ( Zhang et al , 2019 ) and retrieval ( Henderson et al , 2019b ) .
Although these opendomain dialogues are diverse and easy - to - get , they are usually short , noisy , and without speciﬁc chatting goals .
On the other hand , a task - oriented dialogue has explicit goals ( e.g. restaurant reservation or ticket booking ) and many conversational interactions .
But each dataset is usually small and scattered because obtaining and labeling such data is time - consuming .
Moreover , a task - oriented dialogue has explicit user and system behaviors where a user has his / her goal , and a system has its belief and database information , which makes the language understanding component and dialogue policy learning more important than those chit - chat scenarios .
This paper aims to prove this hypothesis : selfsupervised language model pre - training using taskoriented corpora can learn better representations than existing pre - trained models for task - oriented downstream tasks .
We emphasize that what we care about the most is not whether our pre - trained model can achieve state - of - the - art results on each downstream task since most of the current best models are built on top of pre - trained models , and ours can easily replace them .
We avoid adding too many additional components on top of the pre - training architecture when ﬁne - tuning in our experiments .
We collect and combine nine human - human and multi - turn task - oriented dialogue corpora to train a task - oriented dialogue BERT ( TOD - BERT ) .
In total , there are around 100k dialogues with 1.4 M utterances across over 60 different domains .
Like BERT ( Devlin et al , 2018 ) , TOD - BERT is formulated as a masked language model and uses a deep bidirectional Transformer ( Vaswani
et al , 2017 )
Proceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing , pages917–929,November16–20,2020.c(cid:13)2020AssociationforComputationalLinguistics917  encoder as its model architecture .
Unlike BERT , TOD - BERT incorporates two special tokens for user and system to model the corresponding dialogue behavior .
A contrastive objective function of response selection task is combined during pretraining stage to capture response similarity .
We select BERT because it is the most widely used model in NLP research recently , and our uniﬁed datasets can be easily applied to pre - train any existing language models .
We test TOD - BERT on task - oriented dialogue systems on four core downstream tasks , including intention recognition , dialogue state tracking , dialogue act prediction , and response selection .
What we observe is : TOD - BERT outperforms BERT and other strong baselines such as GPT-2 ( Radford et al , 2019 ) and DialoGPT ( Zhang et al , 2019 ) on all the selected downstream tasks , which further conﬁrms its effectiveness for improving dialogue language understanding .
We ﬁnd that response contrastive learning is beneﬁcial , but it is currently overlooked not well - investigated in dialogue pretraining research .
More importantly , TOD - BERT has a stronger few - shot ability than BERT on each task , suggesting that it can reduce the need for expensive human - annotated labels .
TOD - BERT can be easily leveraged and adapted to a new taskoriented dialogue dataset .
Our source code and data processing are released to facilitate future research on pre - training and ﬁne - tuning of task - oriented dialogue 1 .
2 Related Work
General Pre - trained Language Models , which are trained on massive general text such as Wikipedia and BookCorpus , can be roughly divided into two categories : uni - directional or bidirectional attention mechanisms .
GPT ( Radford et al , 2018 ) and GPT-2 ( Radford et al , 2019 ) are representatives of uni - directional language models using a Transformer decoder , where the objective is to maximize left - to - right generation likelihood .
These models are commonly applied in natural language generation tasks .
On the other hand , BERT ( Devlin et al , 2018 ) , RoBERTa ( Liu et al , 2019 ) , and their variances are pre - trained using a Transformer encoder with bi - directional token prediction .
These models are usually evaluated on classiﬁcation tasks such as GLUE benchmark ( Wang et al , 2018 ) or span - based question answering tasks ( Ra1github.com/jasonwu0731/ToD-BERT
jpurkar et al , 2016 ) .
Some language models can support both unidirectional and bi - directional attention , such as UniLM ( Dong et al , 2019 ) .
Conditional language model pre - training is also proposed .
For example , CTRL ( Keskar et al , 2019 ) is a conditional Transformer model , trained to condition on control codes that govern style , content , and task - speciﬁc behavior .
Recently , multi - task language model pretraining with uniﬁed sequence - to - sequence generation is proposed .
Text - to - text Transformer ( T5 ) ( Raffel et al , 2019 ) uniﬁes multiple text modeling tasks and achieves the promising results in various NLP benchmarks .
Dialogue Pre - trained Language Models are mostly trained on open - domain conversational data from Reddit or Twitter for dialogue response generation .
Transfertransfo ( Wolf et al , 2019 ) achieves good performance on ConvAI-2 dialogue competition using GPT-2 .
DialoGPT ( Zhang et al , 2019 ) is an extension of GPT-2 that is pre - trained on Reddit data for open - domain response generation .
ConveRT
( Henderson et al , 2019a ) pre - trained a dual transformer encoder for response selection task on large - scale Reddit ( input , response ) pairs .
PLATO ( Bao et al , 2019 ) uses both Twitter and Reddit data to pre - trained a dialogue generation model with discrete latent variables .
All of them are designed to cope with the response generation task for opendomain chatbots .
Pretraining for task - oriented dialogues , on the other hand , has few related works .
Budzianowski and Vuli´c ( 2019 ) ﬁrst apply the GPT-2 model to train on response generation task , which takes system belief , database result , and last dialogue turn as input to predict next system responses .
It only uses one dataset to train its model because few public datasets have database information available .
Henderson et al ( 2019b ) pre - trained a response selection model for task - oriented dialogues .
They ﬁrst pre - train on Reddit corpora and then ﬁne - tune on target dialogue domains , but their training and ﬁne - tuning code is not released .
Peng et al ( 2020 ) focus on the natural language generation ( NLG ) task , which assumes dialogue acts and slot - tagging results are given to generate a natural language response .
Pre - training on a set of annotated NLG corpora can improve conditional generation quality using a GPT-2 model .
918  Name MetaLWOZ ( Lee et al , 2019 ) Schema ( Rastogi et al , 2019 )
Taskmaster ( Byrne et al , 2019 ) MWOZ ( Budzianowski et al , 2018 ) MSR - E2E ( Li et al , 2018 ) SMD ( Eric and Manning , 2017 ) Frames ( Asri et al , 2017 ) WOZ ( Mrkˇsi´c et al , 2016 )
CamRest676 ( Wen et al , 2016 )
# Dialogue 37,884 22,825 13,215 10,420 10,087 3,031 1,369 1,200 676
# Utterance Avg .
Turn # Domain 11.4 20.3 22.9 6.9 7.4 5.3 14.6 4.2 4.1
432,036 463,284 303,066 71,410 74,686 15,928 19,986 5,012 2,744
47 17 6 7 3 3 3 1 1
Table 1 : Data statistics for task - oriented dialogue datasets .
3 Method
This section discusses each dataset used in our taskoriented pre - training and how we process the data .
Then we introduce the selected pre - training base model and its objective functions .
3.1 Datasets
We collect nine different task - oriented datasets which are English , human - human and multi - turn .
In total , there are 100,707 dialogues , which contain 1,388,152 utterances over 60 domains .
Dataset statistics is shown in Table 1 .
• MetaLWOZ ( Lee et al , 2019 ): Meta - Learning Wizard - of - Oz is a dataset designed to help develop models capable of predicting user responses in unseen domains .
This large dataset was created by crowdsourcing 37,884 goaloriented dialogs , covering 227 tasks in 47 domains .
The MetaLWOZ dataset is used as the fast adaptation task for DSTC8 ( Kim et al , 2019 ) dialogue competition .
• Schema ( Rastogi et al , 2019 ): Schema - guided dialogue has 22,825 dialogues and provides a challenging testbed for several tasks , in particular , dialogue state tracking .
Each schema is a set of tracking slots , and each domain could have multiple possible schemas .
This allows a single dialogue system to support many services and facilitates the simple integration of new services without requiring much training data .
The Schema dataset is used as the dialogue state tracking task for DSTC8 ( Kim et al , 2019 ) dialogue competition .
• Taskmaster ( Byrne et al , 2019 ):
This dataset includes 13,215 dialogues comprising six domains , including 5,507 spoken and 7,708 written dialogs created with two distinct procedures .
One is a two - person Wizard of Oz approach that one person acts like a robot , and the other is a self - dialogue approach in which crowdsourced workers wrote the entire dialog themselves .
It has 22.9 average conversational turns in a single dialogue , which is the longest among all taskoriented datasets listed .
• MWOZ ( Budzianowski et al , 2018 ): MultiDomain Wizard - of - Oz dataset contains 10,420 dialogues over seven domains , and it has multiple domains in a single dialogue .
It has a detailed description of the data collection procedure , user goal , system act , and dialogue state labels .
Different from most of the existing corpora , it also provides full database information .
• MSR - E2E ( Li et al , 2018 ): Microsoft end - toend dialogue challenge has 10,087 dialogues in three domains , movie - ticket booking , restaurant reservation , and taxi booking .
It also includes an experiment platform with built - in simulators in each domain .
• SMD ( Eric and Manning , 2017 ): Stanford multidomain dialogue is an in - car personal assistant dataset , comprising 3,301 dialogues and three domains : calendar scheduling , weather information retrieval , and point - of - interest navigation .
It is designed to smoothly interface with knowledge bases , where a knowledge snippet is attached with each dialogue as a piece of simpliﬁed database information .
• Frames ( Asri et al , 2017 ): This dataset comprises 1,369 human - human dialogues with an average of 14.6 turns per dialogue , where users
919  are given some constraints to book a trip and assistants who search a database to ﬁnd appropriate trips .
Unlike other datasets , it has labels to keep track of different semantic frames , which is the decision - making behavior of users throughout each dialogue .
• WOZ ( Mrkˇsi´c et al , 2016 ) and CamRest676
( Wen et al , 2016 ):
These two corpora use the same data collection procedure and same ontology from DSTC2 ( Henderson et al , 2014 ) .
They are one of the ﬁrst task - oriented dialogue datasets that use Wizard of Oz style with text input instead of speech input , which improves the model ’s capacity for the semantic understanding instead of its robustness to automatic speech recognition errors .
3.2 TOD - BERT
We train our TOD - BERT based on BERT architecture using two loss functions : masked language modeling ( MLM ) loss and response contrastive loss ( RCL ) .
Note that the datasets we used can be used to pre - train any existing language model architecture , and here we select BERT because it is the most widely used model in NLP research .
We use the BERT - base uncased model , which is a transformer self - attention encoder ( Vaswani et al , 2017 ) with 12 layers and 12 attention heads with its hidden size dB = 768 .
To capture speaker information and the underlying interaction behavior in dialogue , we add two special tokens , [ USR ] and [ SYS ] , to the bytepair embeddings ( Mrkˇsi´c et al , 2016 ) .
We preﬁx the special token to each user utterance and system response , and concatenate all the utterances in the same dialogue into one ﬂat sequence , as shown in Figure 1 .
For example , for a dialogue D = { S1 , U1 , . . .
, Sn , Un } , where n is the number of dialogue turns and each Si or Ui contains a sequence of words , the input of the pre - training model is processed as “ [ SYS ] S1 [ USR ] U1 . . . ”
with standard positional embeddings and segmentation embeddings .
replacement are performed once in the beginning and saved for the training duration .
Here we conduct token masking dynamically during batch training .
TOD - BERT is initialized from BERT , a good starting parameter set , then is further pre - trained on those task - oriented corpora .
The MLM loss function is
Lmlm = − ( cid:80)M
m=1 log P ( xm ) ,
( 1 )
where M is the total number of masked tokens and P ( xm ) is the predicted probability of the token xm over the vocabulary size .
Response contrastive loss can also be used for dialogue language modeling since it does not require any additional human annotation .
Pretraining with RCL can bring us several advantages : 1 ) we can learn a better representation for the [ CLS ] token , as it is essential for all the downstream tasks , and 2 ) we encourage the model to capture underlying dialogue sequential order , structure information , and response similarity .
Unlike the original next sentence prediction ( NSP ) objective in BERT pre - training , which concatenates two segments A and B to predict whether they are consecutive text with binary classiﬁcation , we apply a dual - encoder approach ( Henderson et al , 2019a ) and simulate multiple negative samples .
We ﬁrst draw a batch of dialogues { D1 , . . .
, Db } and split each dialogue at a randomly selected turn t.
For example , D1 will be separated into two segments , one is the context 1 , U 1 { S1 t } and the other is the response { S1 t+1 } .
We use TOD - BERT to encode all the contexts and their corresponding responses separately .
Afterwards , we have a context matrix C ∈ Rb×dB and a response matrix R ∈ Rb×dB by taking the output [ CLS ] representations from the b dialogues .
We treat other responses in the same batch as randomly selected negative samples .
The RCL objective function is
1 , . . .
, S1
t , U 1
Lrcl = −
b ( cid:80 ) i=1 M = Softmax(CRT ) ∈ Rb×b .
log Mi , i ,
( 2 )
Masked language modeling is a common pretraining strategy for BERT - like architectures , in which a random sample of tokens in the input sequence is selected and replaced with the special token [ MASK ] .
The MLM loss function is the crossentropy loss on predicting the masked tokens .
In the original implementation , random masking and
Increasing batch size to a certain amount can obtain better performance on downstream tasks , especially for the response selection .
The Softmax function normalizes the vector per row .
In our setting , increasing batch size also means changing the positive and negative ratio in the contrastive learning .
Batch size is a hyper - parameter that may be
920  models predict one single intent class over I possible intents .
Pint = Softmax(W1(F ( U ) ) )
∈ RI ,
( 3 )
where F is a pre - trained language model and we use its [ CLS ] embeddings as the output representation .
W1 ∈ RI×dB is a trainable linear mapping .
The model is trained with cross - entropy loss between the predicted distributions Pint and the true intent labels .
Dialogue state tracking can be treated as a multi - class classiﬁcation problem using a predeﬁned ontology .
Unlike intent , we use dialogue history X ( a sequence of utterances ) as input and a model predicts slot values for each ( domain , slot ) pair at each dialogue turn .
Each corresponding value vj i , the i - th value for the j - th ( domain , slot ) pair , is passed into a pre - trained model and ﬁxed its representation during training .
i = Sim(Gj(F ( X ) ) , F ( vj Sj
i ) )
∈ R1 ,
( 4 )
where Sim is the cosine similarity function , and Sj ∈ R|vj | is the probability distribution of the j - th ( domain , slot ) pair over its possible values .
Gj is the slot projection layer of the j slot , and the number of layers |G| is equal to the number of ( domain , slot ) pairs .
The model is trained with cross - entropy loss summed over all the pairs .
Dialogue act prediction is a multi - label classiﬁcation problem because a system response may contain multiple dialogue acts , e.g. , request and inform at the same time .
Model take dialogue history as input and predict a binary result for each possible dialogue act :
A = Sigmoid(W2(F ( X ) ) )
∈ RN ,
( 5 )
where W2 ∈ RdB×N is a trainable linear mapping , N is the number of possible dialogue acts , and each value in A is between [ 0 , 1 ] after a Sigmoid layer .
The model is trained with binary cross - entropy loss and the i - th dialogue act is considered as a triggered dialogue act if Ai > 0.5 .
Response selection is a ranking problem , aiming to retrieve the most relative system response from a candidate pool .
We use a dual - encoder strategy ( Henderson et al , 2019b ) and compute similarity scores between source X and target Y ,
ri = Sim(F ( X ) , F ( Yi ) )
∈ R1 ,
( 6 )
Figure 1 : Dialogue pre - training based on Transformer encoder with user and system special tokens .
Two objective functions are used : masked language modeling and response contrastive learning .
limited by hardware .
We also try different negative sampling strategies during pre - training such as local sampling ( Saeidi et al , 2017 ) , but do not observe signiﬁcant change compared to random sampling .
function is
Overall pre - training loss the weighted - sum of Lmlm and Lrcl , and in our experiments , we simply sum them up .
We gradually reduce the learning rate without a warm - up period .
We train TOD - BERT with AdamW ( Loshchilov and Hutter , 2017 ) optimizer with a dropout ratio of 0.1 on all layers and attention weights .
GELU activation functions ( Hendrycks and Gimpel , 2016 ) is used .
Models are early - stopped using perplexity scores of a held - out development set , with mini - batches containing 32 sequences of maximum length 512 tokens .
Experiments are conducted on two NVIDIA Tesla V100 GPUs .
4 Downstream Tasks
We care the most in this paper whether TOD - BERT , a pre - trained language model using aggregated taskoriented corpora , can show any advantage over BERT .
Therefore , we avoid adding too many additional components on top of its architecture when ﬁne - tuning on each downstream task .
Also , we use the same architecture with a similar number of parameters for a fair comparison .
All the model parameters are updated with a gradient clipping to 1.0 using the same hyper - parameters during ﬁnetuning .
We select four crucial task - oriented downstream tasks to evaluate : intent recognition , dialogue state tracking , dialogue act prediction , and response selection .
All of them are core components in modularized task - oriented systems ( Wen et al , 2016 ) .
Intent recognition task is a multi - class classiﬁcation problem , where we input a sentence U and
921  where Yi is the i - th response candidate and ri is its cosine similarity score .
Source X can be truncated , and we limit the context lengths to the most recent 256 tokens in our experiments .
We randomly sample several system responses from the corpus as negative samples .
Although it may not be a true negative sample , it is common to train a ranker and evaluate its results ( Henderson et al , 2019a ) .
dialogues , especially for dialogue state tracking .
It has 8420/1000/1000 dialogues for train , validation , and test sets , respectively .
Across seven different domains , in total , it has 30 ( domain , slot ) pairs that need to be tracked in the test set .
We use its revised version MWOZ 2.1 , which has the same dialogue transcripts but with cleaner state label annotation .
5 Evaluation Datasets
We pick up several datasets , OOS , DSTC2 , GSIM , and MWOZ , for downstream evaluation .
The ﬁrst three corpora are not included in the pre - trained task - oriented datasets .
For MWOZ , to be fair , we do not include its test set dialogues during the pretraining stage .
Details of each evaluation dataset are discussed in the following :
• OOS ( Larson et al , 2019 ): The out - of - scope intent dataset is one of the largest annotated intent datasets , including 15,100/3,100/5,500 samples for the train , validation , and test sets , respectively .
It covers 151 intent classes over ten domains , including 150 in - scope intent and one outof - scope intent .
The out - of - scope intent means that a user utterance that does not fall into any of the predeﬁned intents .
Each of the intents has 100 training samples .
• DSTC2 ( Henderson et al , 2014 ): DSTC2 is a human - machine task - oriented dataset that may include a certain system response noise .
It has 1,612/506/1117 dialogues for train , validation , and test sets , respectively .
We follow Paul et al ( 2019 ) to map the original dialogue act labels to universal dialogue acts , which results in 9 different system dialogue acts .
• GSIM ( Shah et al , 2018a ): GSIM is a humanrewrote machine - machine task - oriented corpus , including 1500/469/1039 dialogues for the train , validation , and test sets , respectively .
We combine its two domains , movie and restaurant domains , into one single corpus .
It is collected by Machines Talking To Machines ( M2 M ) (
Shah et al , 2018b ) approach , a functionality - driven process combining a dialogue self - play step and a crowdsourcing step .
We map its dialogue act labels to universal dialogue acts ( Paul et al , 2019 ) , resulting in 6 different system dialogue acts .
6 Results
For each downstream task , we ﬁrst conduct the experiments using the whole dataset , and then we simulate the few - shot setting to show the strength of our TOD - BERT .
We run at least three times with different random seeds for each few - shot experiment to reduce data sampling variance , and we report its mean and standard deviation for these limited data scenarios .
We investigate two versions of TOD - BERT ; one is TOD - BERT - mlm that only uses MLM loss during pre - training , and the other is TOD - BERT - jnt , which is jointly trained with the MLM and RCL objectives .
We compare TOD - BERT with BERT and other baselines , including two other strong pre - training models GPT2 ( Radford et al , 2019 ) and DialoGPT ( Zhang et al , 2019 ) .
For a GPT - based model , we use mean pooling of its hidden states as its output representation , which we found it is better than using only the last token .
6.1 Linear Probe
Before ﬁne - tuning each pre - trained models , we ﬁrst investigate their feature extraction ability by probing their output representations .
Probing methods are proposed to determine what information is carried intrinsically by the learned embeddings ( Tenney et al , 2019 ) .
We probe the output representation using one single - layer perceptron on top of a “ ﬁxed ” pre - trained language model and only ﬁnetune that layer for a downstream task with the same hyper - parameters .
Table 3 shows the probing results of domain classiﬁcation on MWOZ , intent identiﬁcation on OOS , and dialogue act prediction on MWOZ .
TOD - BERT - jnt achieves the highest performance in this setting , suggesting its representation contains the most useful information .
6.2
Intent Recognition
• MWOZ ( Budzianowski et al , 2018 ):
MWOZ is the most common benchmark for task - oriented
TOD - BERT outperforms BERT and other strong baselines in one of the largest intent recognition
922  Model
Acc ( all )
Acc ( in )
Acc ( out )
Recall ( out )
1 - Shot
10 - Shot
TOD - BERT - jnt
BERT 29.3 % ± 3.4 % 35.7 % ± 4.1 % 81.3 % ± 0.4 % 0.4 % ± 0.3 % TOD - BERT - mlm 38.9 % ± 6.3 % 47.4 % ± 7.6 % 81.6 % ± 0.2 % 0.5 % ± 0.2 % 42.5 % ± 0.1 % 52.0 % ± 0.1 % 81.7 % ± 0.1 % 0.1 % ± 0.1 % BERT 75.5 % ± 1.1 % 88.6 % ± 1.1 % 84.7 % ± 0.3 % 16.5 % ± 1.7 % TOD - BERT - mlm 76.6 % ± 0.8 % 90.5 % ± 1.2 % 84.3 % ± 0.2 % 14.0 % ± 1.3 % 77.3 % ± 0.5 % 91.0 % ± 0.5 % 84.5 % ± 0.4 % 15.3 % ± 2.1 %
Full ( 100 - Shot )
TOD - BERT - jnt FastText * SVM * CNN * GPT2 DialoGPT BERT TOD - BERT - mlm TOD - BERT - jnt
83.0 % 83.9 % 84.9 % 85.9 % 86.6 %
89.0 % 91.0 % 91.2 % 94.1 % 95.5 % 95.8 % 96.1 % 96.2 %
87.7 % 87.6 % 88.1 % 89.5 % 89.9 %
9.7 % 14.5 % 18.9 % 32.0 % 32.1 % 35.6 % 46.3 % 43.6 %
Table 2 : Intent recognition results on the OOS dataset , one of the largest intent corpus .
Models with * are reported from Larson et al ( 2019 ) .
Intent Domain ( acc ) ( acc ) 63.5 % 74.7 % 63.0 % 65.7 % 60.5 % 71.1 % TOD - BERT - mlm 63.9 % 70.7 % 68.7 % 77.8 %
GPT2 DialoGPT BERT
TOD - BERT - jnt
Dialogue Act ( F1 - micro ) 85.7 % 84.2 % 85.3 % 83.5 % 86.2 %
Table 3 : Probing results of different pre - trained language models using a single - layer perceptron .
datasets , as shown in Table 2 .
We evaluate accuracy on all the data , the in - domain intents only , and the out - of - scope intent only .
Note that there are two ways to predict out - of - scope intent , one is to treat it as an additional class , and the other is to set a threshold for prediction conﬁdence .
Here we report the results of the ﬁrst setting .
TOD - BERTjnt achieves the highest in - scope and out - of - scope accuracy .
Besides , we conduct 1 - shot and 10 - shot experiments by randomly sampling one and ten utterances from each intent class in the training set .
TOD - BERT - jnt has 13.2 % all - intent accuracy improvement and 16.3 % in - domain accuracy improvement compared to BERT in the 1 - shot setting .
6.3 Dialogue State Tracking
Two evaluation metrics are commonly used in dialogue state tracking task : joint goal accuracy and slot accuracy .
The joint goal accuracy compares the predicted dialogue states to the ground truth at each dialogue turn .
The ground truth includes slot values for all the possible ( domain , slot ) pairs .
The output is considered as a correct prediction if and only if all the predicted values exactly match its ground truth values .
On the other hand , the slot
accuracy individually compares each ( domain , slot , value ) triplet to its ground truth label .
In Table 5 , we compare BERT to TOD - BERTjnt on the MWOZ 2.1 dataset and ﬁnd the latter has 2.4 % joint goal accuracy improvement .
Since the original ontology provided by Budzianowski et al ( 2018 ) is not complete ( some labeled values are not included in the ontology ) , we create a new ontology of all the possible annotated values .
We also list several well - known dialogue state trackers as reference , including DSTReader ( Gao et al , 2019 ) , HyST ( Goel et al , 2019 ) , TRADE ( Wu et al , 2019 ) , and ZSDST ( Rastogi et al , 2019 ) .
We also report the few - shot experiments using 1 % , 5 % , 10 % , and 25 % data .
Note that 1 % of data has around 84 dialogues .
TOD - BERT outperforms BERT in all the setting , which further show the strength of task - oriented dialogue pre - training .
6.4 Dialogue Act Prediction
We conduct experiments on three different datasets and report micro - F1 and macro - F1 scores for the dialogue act prediction task , a multi - label classiﬁcation problem .
For the MWOZ dataset , we remove the domain information from the original system dialogue act labels .
For example , the “ taxi - inform ” will be simpliﬁed to “ inform ” .
This process reduces the number of possible dialogue acts from 31 to 13 .
For DSTC2 and GSIM corpora , we follow Paul et al ( 2019 ) to apply universal dialogue act mapping that maps the original dialogue act labels to a general dialogue act format , resulting in 9 and 6 unique system dialogue acts in DSTC2 and GSIM , respectively .
We run two other baselines , MLP and RNN , to further show the strengths of BERT - based
923  MWOZ ( 13 )
DSTC2 ( 9 )
GSIM ( 6 )
micro
-
F1
macro - F1
micro
-
F1
macro - F1
micro
-
F1
macro - F1
1 % Data
10 % Data
TOD - BERT - jnt
BERT 84.0 % ± 0.6 % 66.7 % ± 1.7 % 77.1 % ± 2.1 % 25.8 % ± 0.8 % 67.3 % ± 1.4 % 26.9 % ± 1.0 % TOD - BERT - mlm 87.5 % ± 0.6 % 73.3 % ± 1.5 % 79.6 % ± 1.0 % 26.4 % ± 0.5 % 82.7 % ± 0.7 % 35.7 % ± 0.3 % 86.9 % ± 0.2 % 72.4 % ± 0.8 % 82.9 % ± 0.4 % 28.0 % ± 0.1 % 78.4 % ± 3.2 % 32.9 % ± 2.1 % BERT 89.7 % ± 0.2 % 78.4 % ± 0.3 % 88.2 % ± 0.7 % 34.8 % ± 1.3 % 98.4 % ± 0.3 % 45.1 % ± 0.2 % TOD - BERT - mlm 90.1 % ± 0.2 % 78.9 % ± 0.1 % 91.8 % ± 1.7 % 39.4 % ± 1.7 % 99.2 % ± 0.1 % 45.6 % ± 0.1 % 90.2 % ± 0.2 % 79.6 % ± 0.7 % 90.6 % ± 3.2 % 38.8 % ± 2.2 % 99.3 % ± 0.1 % 45.7 % ± 0.0 %
Full Data
TOD - BERT - jnt MLP RNN GPT2 DialoGPT BERT TOD - BERT - mlm TOD - BERT - jnt
61.6 % 90.4 % 90.8 % 91.2 % 91.4 % 91.7 % 91.7 %
45.5 % 77.3 % 79.8 % 79.7 % 79.7 % 79.9 % 80.6 %
77.6 % 90.8 % 92.5 % 93.8 % 92.3 % 90.9 % 93.8 %
18.1 % 29.4 % 39.4 % 42.1 % 40.1 % 39.9 % 41.3 %
89.5 % 98.4 % 99.1 % 99.2 % 98.7 % 99.4 % 99.5 %
26.1 % 45.2 % 45.6 % 45.6 % 45.2 % 45.8 % 45.8 %
Table 4 : Dialogue act prediction results on three different datasets .
The numbers reported are the micro and macro F1 scores , and each dataset has different numbers of dialogue acts .
1 % Data
5 % Data
10 % Data
25 % Data
Full Data
Model
Joint Acc
Slot Acc
BERT
TOD - BERT - jnt
TOD - BERT - jnt
6.4 % ± 1.4 % 84.4 % ± 1.0 % TOD - BERT - mlm 9.9 % ± 0.6 % 86.6 % ± 0.5 % 8.0 % ± 1.0 % 85.3 % ± 0.4 % BERT 19.6 % ± 0.1 % 92.0 % ± 0.5 % TOD - BERT - mlm 28.1 % ± 1.6 % 93.9 % ± 0.1 % 28.6 % ± 1.4 % 93.8 % ± 0.3 % BERT 32.9 % ± 0.6 % 94.7 % ± 0.1 % TOD - BERT - mlm 39.5 % ± 0.7 % 95.6 % ± 0.1 % 37.0 % ± 0.1 % 95.2 % ± 0.1 % BERT 40.8 % ± 1.0 % 95.8 % ± 0.1 % TOD - BERT - mlm 44.0 % ± 0.4 % 96.4 % ± 0.1 % 44.3 % ± 0.3 % 96.3 % ± 0.2 %
TOD - BERT - jnt
TOD - BERT - jnt DSTReader * HyST * ZSDST * TRADE * GPT2 DialoGPT BERT TOD - BERT - mlm TOD - BERT - jnt
36.4 % 38.1 % 43.4 % 45.6 % 46.2 % 45.2 % 45.6 % 47.7 % 48.0 %
96.6 % 96.5 % 96.6 % 96.8 % 96.9 %
Table 5 : Dialogue state tracking results on MWOZ 2.1 .
We report joint goal accuracy and slot accuracy for the full data setting and the simulated few - shot settings .
( a ) BERT
( b ) BERT
( c ) TOD - BERT - mlm
( d ) TOD - BERT - mlm
models .
The MLP model simply takes bag - of - word embeddings to make dialogue act prediction , and the RNN model is a bi - directional GRU network .
In Table 4 , one can observe that in full data scenario , TOD - BERT consistently works better than BERT and other baselines , no matter which datasets or which evaluation metrics .
In the fewshot experiments , TOD - BERT - mlm outperforms BERT by 3.5 % micro - F1 and 6.6 % macro - F1 on MWOZ corpus in the 1 % data scenario .
We also found that 10 % of training data can achieve good performance that is close to full data training .
( e ) TOD - BERT - jnt
( f ) TOD - BERT - jnt
Figure 2 : The tSNE visualization of BERT , TODBERT - mlm and TOD - BERT - jnt representations of system responses in the MWOZ test set .
Different colors in the left - hand column mean different domains , and in the right - hand column represent different dialogue acts .
6.5 Response Selection
To evaluate response selection in task - oriented dialogues , we follow the k - to-100 accuracy , which is becoming a research community standard ( Yang et al , 2018 ; Henderson et al , 2019a ) .
The k - of-100
924  1 % Data
10 % Data
Full Data
MWOZ
DSTC2
GSIM
1 - to-100
3 - to-100 7.8 % ± 2.0 % 20.5 % ± 4.4 % 3.7 % ± 0.6 %
1 - to-100
3 - to-100 9.6 % ± 1.3 %
BERT
TOD - BERT - jnt
4.0 % ± 0.4 % 10.3 % ± 1.1 % TOD - BERT - mlm 13.0 % ± 1.1 % 34.6 % ± 0.4 % 12.5 % ± 6.7 % 24.9 % ± 10.7 % 7.2 % ± 4.0 % 15.4 % ± 8.0 % 37.5 % ± 0.6 % 55.9 % ± 0.4 % 12.5 % ± 0.9 % 26.8 % ± 0.8 % 9.8 % ± 0.1 % 24.4 % ± 1.2 % TOD - BERT - mlm 22.3 % ± 3.2 % 48.7 % ± 4.0 % 19.0 % ± 16.3 % 33.8 % ± 20.4 % 11.2 % ± 2.5 % 26.0 % ± 2.7 % 49.7 % ± 0.3 % 66.6 % ± 0.1 % 23.0 % ± 1.0 % 42.6 % ± 1.0 %
BERT 20.9 % ± 2.6 % 45.4 % ± 3.8 % 8.9 % ± 2.3 %
21.4 % ± 3.1 %
1 - to-100
3 - to-100
TOD - BERT - jnt GPT2 DialoGPT BERT TOD - BERT - mlm TOD - BERT - jnt
47.5 % 35.7 % 47.5 % 48.1 % 65.8 %
75.4 % 64.1 % 75.5 % 74.3 % 87.0 %
53.7 % 39.8 % 46.6 % 50.0 % 56.8 %
69.2 % 57.1 % 62.1 % 65.1 % 70.6 %
39.1 % 16.5 % 13.4 % 36.5 % 41.0 %
60.5 % 39.5 % 32.9 % 60.1 % 65.4 %
Table 6 : Response selection evaluation results on three corpora for 1 % , 10 % and full data setting .
We report 1 - to-100 and 3 - to-100 accuracy , which is similar to recall1 and recall@3 given 100 candidates .
metric is computed using a random batch of 100 examples so that responses from other examples in the same batch can be used as random negative candidates .
This allows us to be compute the metric across many examples in batches efﬁciently .
While it is not guaranteed that the random negatives will indeed be “ true ” negatives , the 1 - of-100 metric still provides a useful evaluation signal .
During inference , we run ﬁve different random seeds to sample batches and report the average results .
In Table 6 , we conduct response selection experiments on three datasets , MWOZ , DSTC2 , and GSIM .
TOD - BERT - jnt achieves 65.8 % 1 - to-100 accuracy and 87.0 % 3 - to-100 accuracy on MWOZ , which surpasses BERT by 18.3 % and 11.5 % , respectively .
The similar results are also consistently observed in DSTC2 and GSIM datasets , and the advantage of the TOD - BERT - jnt is more evident in the few - shot scenario .
We do not report TODBERT - jnt for MWOZ few - shot setting because it is not fair to compare them with others as the full MWOZ training set is used for response contrastive learning during pre - training stage .
The response selection results are sensitive to the training batch size since the larger the batch size the harder the prediction .
In our experiments , we set batch size equals to 25 for all the models .
7 Visualization
In Figure 2 , we visualize the embeddings of BERT , TOD - BERT - mlm , and TOD - BERT - jnt given the same input from the MWOZ test set .
Each sample point is a system response representation , which is passed through a pre - trained model and reduced its high - dimension features to a two - dimension point using the t - distributed stochastic neighbor embedding ( tSNE ) for dimension reduction .
Since we know the true domain and dialogue act labels for
each utterance , we use different colors to represent different domains and dialogue acts .
As one can observe , TOD - BERT - jnt has more clear group boundaries than TOD - BERT - mlm , and two of them are better than BERT .
To analyze the results quantitatively , we run Kmeans , a common unsupervised clustering algorithms , on top of the output embeddings of BERT and TOD - BERT .
We set K for K - means equal to 10 and 20 .
After the clustering , we can assign each utterance in the MWOZ test set to a predicted class .
We then compute the normalized mutual information ( NMI ) between the clustering result and the actual domain label for each utterance .
Here is what we observe : TOD - BERT consistently achieves higher NMI scores than BERT .
For K=10 , TOD - BERT has a 0.143 NMI score , and BERT only has 0.094 .
For K=20 , TOD - BERT achieves a 0.213 NMI score , while BERT has 0.109 .
8 Conclusion
We propose task - oriented dialogue BERT ( TODBERT ) trained on nine human - human and multiturn task - oriented datasets across over 60 domains .
TOD - BERT outperforms BERT on four dialogue downstream tasks , including intention classiﬁcation , dialogue state tracking , dialogue act prediction , and response selection .
It also has a clear advantage in the few - shot experiments when only limited labeled data is available .
TOD - BERT is easy - to - deploy and will be open - sourced , allowing the NLP research community to apply or ﬁne - tune any task - oriented conversational problem .
