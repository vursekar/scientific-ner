NLQuAD : A Non - Factoid Long Question Answering Data Set
Amir Soleimani Informatics Institute University of Amsterdam Amsterdam , The Netherlands a.soleimani@uva.nl
Christof Monz Informatics Institute University of Amsterdam Amsterdam , The Netherlands c.monz@uva.nl
Marcel Worring Informatics Institute University of Amsterdam Amsterdam , The Netherlands m.worring@uva.nl
Abstract
language understanding .
We introduce NLQuAD , the ﬁrst data set with baseline methods for non - factoid long question answering , a task requiring documentIn contrast level to existing span detection question answering data sets , NLQuAD has non - factoid questions that are not answerable by a short span of text and demanding multiple - sentence descriptive answers and opinions .
We show the limitation of the F1 score for evaluation of long answers and introduce Intersection over Union ( IoU ) , which measures position - sensitive overlap between the predicted and the target answer spans .
To establish baseline performances , we compare BERT , RoBERTa , and Longformer models .
Experimental results and human evaluations show that Longformer outperforms the other architectures , but results are still far behind a human upper bound , leaving substantial room for improvements .
NLQuAD ’s samples exceed the input limitation of most pretrained Transformer - based models , encouraging future research on long sequence language models.1
1
Introduction
Over the last few years , there have been remarkable improvements in the area of Machine Reading Comprehension ( MRC ) and open - domain Question Answering ( QA ) due to the availability of large scale data sets such as SQuAD ( Rajpurkar et al , 2016 ) and pre - trained language models such as BERT ( Devlin et al , 2018 ) .
Although non - factoid questions represent a large number of real - life questions , current QA data sets barely cover this area .
The reason is that context passages in existing QA data sets are mostly very short and questions mostly factoid , i.e. , can be answered by simple facts or entities such as a person name and location ( Jurafsky and Martin , 2019 ) .
Little attention has been
1Dataset and Models : github.com/asoleimanib/NLQuAD
Question : How are people coping in the lockdown ?
Headline : China coronavirus :
Death toll rises as more cities restrict travel
Document : China has widened its travel restrictions in Hubei province - the centre of the coronavirus outbreak - as the death toll climbed to 26 .
The restrictions will affect at least 20 million people across 10 cities , including the capital , Wuhan , where the virus emerged .
On Thursday , a coronavirus patient died in northern Hebei province - making it the ﬁrst death outside Hubei .
[ ... ] We now know this is not a virus that will burn out on its own and disappear .
[ ... ] And we still do n’t know when people are contagious .
Is it before symptoms appear , or only after severe symptoms emerge ?
One is signiﬁcantly harder to stop spreading than the other .
[ ... ] One doctor , who requested anonymity , describes the conditions at a hospital in Wuhan .
[ ... ] “ I was planning to stay in my apartment because I ’m scared to go to the gym , and I ’m scared to go to out in public , and not many people are willing to go out . ”
( 141 words ) .
Vietnam and Singapore were on Thursday added to the nations recording conﬁrmed cases , joining Thailand , the US , Taiwan and South Korea .
[ ... ] Taiwan has banned people arriving from Wuhan and the US state department warned American travellers to exercise increased caution in China .
( document length : 921 words )
Figure 1 : A question - answer pair in NLQuAD .
QA models must predict the answer span within the context document .
The correct answer span is bolded .
We extract questions and answers , respectively , from the subheadings and the sub - section bodies from real - word English news articles .
Two other questions based on the same article : Can the Coronavirus be stopped ?
What ’s the global situation ?
paid to non - factoid and open - ended questions that require complex answers such as descriptions or opinions ( Hashemi et al , 2020 ) .
Answers to nonfactoid questions extend to multiple sentences or paragraphs having few words overlapping with the question ( Cohen and Croft , 2016 ) .
Non - factoid QA facilitates document assistance systems , where for example , journalists can seek assistance to highlight relevant opinions and interpretations .
It can further motivate more research on long sequence
Proceedingsofthe16thConferenceoftheEuropeanChapteroftheAssociationforComputationalLinguistics , pages1245–1255April19 - 23,2021. © 2021AssociationforComputationalLinguistics1245  language models .
Therefore , a high - quality data set in this area is clearly desired .
To support research towards non - factoid and long QA tasks and to address the existing shortcomings as identiﬁed above , we have built NLQuAD , a non - factoid long question answering data set .
NLQuAD contains 31k non - factoid questions and long answers collected from 13k BBC news articles .
We extract questions and answers from the articles ’ sub - headings and the following body paragraphs of the sub - headings ( see Figure 1 ) .
Questions in NLQuAD are not answerable by a short span of text within the documents .
This is in contrast to existing long - context but factoid QA data sets such as NewsQA ( Trischler et al , 2017 ) , TriviaQA ( Joshi et al , 2017 ) , NarrativeQA ( Koˇcisk´y et al , 2018 ) , DuoRC ( Saha et al , 2018 ) , HotpotQA ( Yang et al , 2018 ) , and Natural Questions ( Kwiatkowski et al , 2019 ) .
Although these data sets contain long documents , questions are answerable by short entities or a span of entities .
In particular , Natural Questions covers two types of short and long answers .
However , due to its factoid questions , most long answers are still sections containing exactly the short answers and so are trivial ( e.g. , “ Where is the world ’s largest ice sheet ... ? ” , Short : “ Antarctica ” ; Long : “ The Antarctic ice sheet is the largest single mass of ice on Earth ... ” ) .
Furthermore , although a small portion ( 13 % ) of Natural Questions samples have only long answers , they are still spans of simple facts .
For example , “ Who is the author of the book Arabian Nights ? ” has no short answer simply because there are multiple authors : “ The work was collected over many centuries by various authors , translators ... ” .
In contrast , we address non - factoid questions requiring complex answers like opinions and explanations .
NLQuAD ’s answers are open and not predeﬁned .
Figure 3 and Table 3 present our question types .
NLQuAD ’s questions are also not self - contained .
For example , “ How are people coping in the lockdown ? ”
or “ What ’s the global situation ? ” can not be answered without the context from the document ( see Figure 1 ) .
Section 3.2 discusses our question types in detail .
In most existing QA data sets such as SQuAD , crowd - workers generate questions based on provided short passages and extract answers from the passages ( Rajpurkar et al , 2016 ) .
This method of question generation can make QA samples trivial because models can simply detect the most related
span to the question by guessing based on shallow pattern matching ( Koˇcisk´y et al , 2018 ) .
In contrast , all annotations in NLQuAD are done automatically and directly based on the news articles themselves .
NLQuAD , unlike MS MARCO ( Bajaj et al , 2016 ) and ELI5 ( Fan et al , 2019 ) , does not use information retrieval ( IR ) methods to collect supporting documents .
Retrieved documents in these data sets are not guaranteed to contain all facts required to answer the question or they occasionally just contain information related to the question but no answers .
NLQuAD requires document - level language understanding .
With an average document length and answer length of 877 and 175 words , respectively , it exceeds the maximum input length of the state of the art QA models such as BERT ( Devlin et al , 2018 ) and RoBERTa ( Liu et al , 2019 ) due to their memory and computational requirements .
Thus , training and evaluating the ( document , question , answer ) tuples is impossible using such models in an end - to - end manner .
It is worth noting that it is also harder to perform pre - selection methods before the ﬁnal span detection because our answers are long .
Meanwhile , most of our questions are not self - contained .
For example , to answer the question “ How are people coping in the lockdown ? ”
( Figure 1 ) , the system needs to read the document to interpret the concept of “ lockdown ” and then locate the information regarding the people ’s behaviour .
We also show the shortcomings of the F1 score and ROUGE - N scores in evaluating long sequences .
There is a higher chance of overlap between the word N - grams in two long sequences causing F1 and ROUGE - N to over - estimate the performance .
Therefore , we propose to use Intersection over Union ( IoU ) measuring position - sensitive overlap between two spans .
In summary , our contributions are as follows : ( 1 ) We introduce a new data set for non - factoid long QA that to the best of our knowledge is the ﬁrst data set requiring long answer span detection given non - self - contained and non - factoid questions ; ( 2 ) We show the limitations of the F1 score in evaluating long answers and propose a new evaluation metric ; ( 3 ) To establish baseline results , we experiment with three state - of - the - art models : BERT , RoBERTa , and Longformer , and compare them with human performance .
To handle the input length limitations of BERT and RoBERTa , we pro1246  Avg # Words
data sets SQuAD NewsQA TriviaQA NarrativeQA DouRC - Self DouRC - Pharaphrase HotpotQA Natural Questions DuReader DQA MS MARCO ELI5 NLQuAD
Que .
Doc .
Ans .
QA Type 3 10 4 8 2 14 5 10 3 9 3 9 2 18 192 9 67 5 54 N / A 14 6 131 42 175 7
Factoid Span Detection Factoid Span Detection Factoid Span Detection Factoid Span Detection Factoid Span Detection Factoid Span Detection Factoid Span Detection Factoid Span Detection Factoid & Non - Factoid Span Detection Factoid & Non - Factoid Span Detection Answer Generation Answer Generation Non - Factoid Span Detection
117 616 2895 656 591 1240 917 7360 396 N / A 56 858 877
Samples 150k 100k 95k 47k 186k 186k 113k 307k 200k 17k 183k 272k 31k
Table 1 : Comparison of NLQuAD with SQuAD , MS MARCO , and long - context QA data sets .
pose to train these models in a sliding - window approach ; ( 4 ) We ﬁnally show that the state - of - the - art models have limited performance in the non - factoid long QA task .
2 Existing data sets
Existing large - scale QA data sets can be categorized based on their context passage length in two groups : short - context QA , i.e. , data sets with paragraph - level context , and long - context QA , i.e. , data sets with multiple - paragraph or documentlevel context .
Long - context QA can potentially In include questions demanding long answers .
this section , we only review QA datasets .
However , it is worth noting that very recently , ( Tay et al , 2020a ) introduced a uniﬁed benchmark using different tasks for evaluating model quality under long - context scenarios .
2.1 Short - Context Question Answering
SQuAD ( Rajpurkar et al , 2016 ) is a factoid span detection data set with short answers .
Crowdworkers generated the questions given a set of articles .
DROP ( Dua et al , 2019 ) makes the problem more challenging by adversarially - created questions requiring discrete reasoning over the text .
SQuAD and DROP use Wikipedia pages as context passages whereas SearchQA ( Dunn et al , 2017 ) uses IR approaches to collect context passages .
Answer generation based on a set of passages is another approach to address this task .
MS MARCO ( Bajaj et al , 2016 ) consists of real - world search queries and retrieved documents corresponding to the queries .
There are also different types of QA data sets such as Antique ( Hashemi et al , 2020 ) , which is a data set for answer retrieval for non - factoid questions .
There is also a range of multiple - choice QA tasks such as RACE ( Lai et al , 2017 ) , ARC ( Clark et al , 2018 ) , SWAQ ( Zellers et al , 2018 ) , and COSMOS QA ( Huang et al , 2019 ) that are clustered together with the short - context QA data sets .
2.2 Long - Context Question Answering
Factoid QA has been applied to longer documents , however , the nature of factoid questions limits answers to short texts .
NewsQA ( Trischler et al , 2017 ) , TriviaQA ( Joshi et al , 2017 ) , NarrativeQA ( Koˇcisk´y et al , 2018 ) , and DuoRC ( Saha et al , 2018 ) fall into this category and their documents are extracted from news articles , stories , and movie plots , respectively .
On the other hand , DQA ( ter Hoeve et al , 2020 ) is a document - centred QA data set aimed at document assistance systems .
Along with Yes / No questions , it also includes non - factoid questions with relatively long answers .
However , the questions are generated by crowd - workers based on a small set of documents .
DuReader ( He et al , 2018 ) consists of real - word Chinese queries and corresponding retrieved documents .
It contains both factoid and non - factoid ( 40 % ) questions and consequently has longer average answer length than pure factoid datasets .
The multi - hop QA task , requiring multi - hop reasoning over multiple paragraphs , can also be considered as long - context QA if models process paragraphs together .
HotpotQA ( Yang et al , 2018 ) is a multi - hop data set , but the answer length of its factoid questions is as limited as that of short - context QA data sets .
Natural Questions ( Kwiatkowski et al , 2019 ) is a factoid QA task with much longer documents and two types of answer lengths .
It consists of factoid questions , retrieved Wikipedia pages , and short
1247  Number of QA pairs Number of Documents Number of Unique Questions Avg . Document Length ( Word ) Avg .
Answer Length ( Word ) Avg .
Question Length ( Word ) Avg . Document Length ( Sentence ) Avg .
Answer Length ( Sentence ) Avg .
Question Length ( Sentence ) Avg . Question per Document
31k 13k 24k 876.8 174.6 7.0 38.7 7.5 1.0 2.4
Table 2 : NLQuAD : data set statistics .
Figure 2 : Distribution of the number of words in document , question and answer .
answers ( yes / no , entities ) as well as long answers ( bounding boxes with the information to infer the answer ) .
However , due to the nature of factoid questions , the majority of long answers are sections containing exactly the short answer or simple facts .
ELI5 ( Fan et al , 2019 ) consists of real - world questions with answers provided by the Reddit community .
The task is to generate answers given a set of documents retrieved from the Web .
However , the documents are not guaranteed to completely address the questions .
Furthermore , evaluation metrics for sequence generation tasks such as the ROUGE score ( Lin and Och , 2004 ) are far from perfect to assess the quality of generated answers .
Table 1 compares existing long - context question answering data sets along with SQuAD and MS MARCO .
We report the average length for data sets with different types of answers .
3 Data Set Design
NLQuAD consists of news articles as context documents , interrogative sub - headings in the articles as questions , and body paragraphs corresponding to the sub - headings as contiguous answers to the questions .
We automatically extract target answers because annotating for non - factoid long QA is rather challenging and costly .
To ensure the quality of answers in addition to the initial investigations , we perform human evaluations ( Section 5.3 ) .
We choose the BBC news website as the resource of our documents and the question - answer pairs , mainly because its articles contain a considerable amount of high - quality question - like sub - headings which are suitable for the QA task .
NLQuAD ’s characteristics make it an appealing and challenging data set for the non - factoid long QA task : Its context documents are long , and its questions are non - factoid in a way that can not be answered by single or multiple entities .
The questions are addressed by more than seven sentences on average .
Meanwhile , it covers a wide range of topics , making it an open - domain QA data set .
The BBC news articles typically follow a speciﬁc template .
They begin with an introductory section consisting of news summaries ( Narayan et al , 2018 ) and one or more sections accompanied by sub - headings .
Each section contains multiple short to medium - length paragraphs .
We remove the template and section break - lines to prevent revealing possible answer boundaries .
3.1 Data Curation We exploit Wayback Machine,2 a digital archive of the Web , and Wayback Machine Scraper3 to scrape the article archives .
Links in the scraped pages are used to collect additional pages from the original website .
We scraped the English BBC news website from 2016 to 2020 as a limited number of questions can be found in articles before 2016 .
Only textual information is kept and we strip away multimedia objects and hyperlinks outside of the body of the articles .
Duplicate documents are removed and questions with bullet list answer types are discarded .
We detect interrogative sub - headings by checking if they end with a question mark .
3.2 Data Set Statistics
NLQuAD contains 31k non - factoid questions based on 13k supporting documents from news articles .
Table 2 shows the data set statistics .
We randomly partition the data set into training ( 80 % ) , development ( 10 % ) , and evaluation ( 10 % ) sets .
While NLQuAD has long documents and longanswer QA pairs , the histograms in Figure 2 indicate the wide range of samples .
Figure 3 presents a visualisation of the distribution of question types
2archive.org/web 3github.com/sangaline/wayback-machine-scraper
124825050075010001250150017502000Document Length10203040Frequency ( % ) 50100150200250300350400450500Answer Length10203040Frequency ( % ) 2468101214Questions Length10203040Frequency ( % )  ( Devlin et al , 2018 ) , RoBERTa ( Liu et al , 2019 ) , and Longformer ( Beltagy et al , 2020 ) .
Longformer is a scalable model for processing long documents and has been used for long sequences such as document classiﬁcation ( Beltagy et al , 2020 ) and document re - ranking ( Sekuli´c et al , 2020 ) .
We refer readers to Tay et al ( 2020b ) for a detailed survey on efﬁcient transformers .
We train these Transformerbased ( Vaswani et al , 2017 ) models to predict the span of the answer in a context document given a question and document .
4.1 BERT and RoBERTa
The BERT QA model concatenates question and document pairs into a single sequence and predicts the answer span by a dot product between the ﬁnal hidden vectors , a start vector and an end vector ( Devlin et al , 2018 ) .
Due to the memory and computational requirements , BERT can encode sequences with a maximum length of 512 tokens that is less than the average sample length in NLQuAD .
Therefore , we adopt a sliding window approach .
We split the samples into segments using a sliding window of 512 tokens and a stride of 128 tokens .
Each segment is augmented with its corresponding question .
The segments can include no answer , a portion of the answer , or the entire answer .
We train BERT on the segments independently .
Finally , the predicted spans corresponding to a single sample are aggregated to predict the ﬁnal span that is the span between the earliest start position and the latest end position .
The output is considered empty when all segments have empty spans .
RoBERTa has the same model architecture and input length limitation as BERT but with a robustly optimized pre - training scheme allowing it to generalize better to downstream tasks such as QA ( Liu et al , 2019 ) .
We apply the same sliding window approach for RoBERTa .
4.2 Longformer
In order to process the question and entire documents at the same time , we use the Longformer model .
It employs an attention mechanism scaling linearly with the sequence length which enables Longformer to process up to 4,096 tokens .
It uses multiple attention heads with different dilation conﬁgurations to attend to the entire sequence and includes global attention to question tokens in the sequence .
Question and document pairs are packed together into a single sequence without having to
Figure 3 : Distribution of trigram preﬁxes of questions in NLQuAD .
Empty portions indicate sufﬁxes with small percentages .
NLQuAD covers a wide range of non - factoid question types .
What is the background ... is the latest ... is the reaction ... is happening in ... is in the ... are the allegations ...
did the court ... happened in the ... has the reaction ...
do we know ...
How did the attack ... did we get ...
did it come ...
does the US / UK ...
does it work ... has the government ... have the authorities ...
do you know ... is the shutdown ... many people are ...
Why is the US ... is this happening ... is there a ... are there protests ... are the ﬁres ... did the US ... was the vote ... does this matter ...
has the US ... do not we ...
Table 3 : Top 4 - grams preﬁxes of questions in NLQuAD .
Even ’ What ’ questions are non - factoid and need longer answers ( descriptions or opinions )
in terms of their ﬁrst three tokens .
Table 3 also lists high frequency examples of “ what ” , “ how ” and “ why ” questions .
NLQuAD has a large percentage of “ how ” and “ why ” question types where also the “ what ” examples are non - factoid and consequently require longer explanations as answers .
We manually investigated 100 randomly sampled question - answer pairs from the NLQuAD training set and ﬁnd that 87 % of the questions are not self - contained and require additional contextual information to be understood or disambiguated .
Most of the answers consist of explanations , descriptions , or opinions , and only 2 % of the questions can be answered by a short span of text .
4 Baseline Models
To investigate the difﬁculty level of NLQuAD for state - of - the - art QA systems and to establish baseline results , we evaluate the performance of BERT
1249thethethethewethethetheisaredidhappenedhasdodoesaboutwashappensdidisaredidisarewhathowwhywhoiswherewillare  Figure 4 : Comparing F1 , ROUGE - N and IoU. Left / Middle : All scores behave similarly in the higher values , but F1 and ROUGE - N over - estimate the performance in the lower IoU values due to a higher chance of overlap between the bag of words , n - grams , or longer LCSs in the prediction and target spans .
The dashed line shows y
=
x. Right : F1 and ROUGE - N over - estimate more in samples with longer answers .
Results are plotted for the Longformer on the development set .
use sliding windows and the answer span is calculated by a dot product ( Beltagy et al , 2020 ) .
5 Experiments
5.1 Evaluation Metrics
Exact Match ( EM ) and the macro - averaged F1 score are the two main evaluation metrics in the span detection QA task ( Rajpurkar et al , 2016 ) .
Exact Match determines if the prediction exactly matches the target which can be a too strict criterion for long answers .
The F1 score measures the overlap between the words in the prediction and the target .
It treats sequences as a bag of words .
Unfortunately , in long answers , it is highly likely that a random , long span shares a considerable number of tokens with the target span .
The ROUGE - N scores ( Lin and Och , 2004 ) , which are primarily used for sequence generation evaluation , have the same drawback in long sequences .
ROUGE - N measures the N - gram overlap between the prediction and target .
High chances of overlap of unigrams and bigrams in long sequences cause ROUGE-1 and ROUGE-2 to over - estimate performance .
The same holds for ROUGE - L with the Longest Common Sub - sequence ( LCS ) because of a high chance of longer LCSs between two long sequences .
To better take sequence similarities into account , we propose to evaluate models with the Intersection over Union ( IoU ) score , also known as Jaccard Index .
IoU is deﬁned as follows :
Question : How did we get here ?
Headline : Eta disarms : French police ﬁnd 3.5 tonnes of weapons
Target Answer : Slowly , and with many false starts .
Eta used parts of south - western France as a base , even though most of its operations were against Spanish targets in Spain .
The group has , however , killed some French policemen , but mostly during police raids on members of the group .
Eta´s ﬁrst ceaseﬁre was in 1998 , but collapsed the following year .
A similar declaration in 2006 only lasted a matter of months , ending when Eta bombed an airport car park , killing two people .
Four years later , in 2010 , Eta announced it would not carry out further attacks and in January 2011 , it declared a permanent and “ internationally veriﬁable ” ceaseﬁre but refused to disarm .
In recent years , police in France and Spain have arrested hundreds of Eta ﬁgures and seized many of its weapons .
Eta´s political wing , Herri Batasuna , was banned by the Spanish government , which argued that the two groups were inextricably linked .
Prediction : The group was set up more than 50 years ago in the era of Spanish dictator General Franco , who repressed the Basques politically and culturally .
Eta ’s goal was to create an independent Basque state out of territory in south - west France and northern Spain .
Its ﬁrst known killing was in 1968 , when a secret police chief was shot dead in the Basque city of San Sebastian .
France and Spain refuse to negotiate with Eta , which is on the EU blacklist of terrorist organisations .
Figure 5 : A prediction span that is semantically different from the target span but has a F1=30 % ( Prec.=43 % , Rec.=23 % ) and IoU=0 .
Red shows the overlapping words in the prediction span with the target .
Articles ( a , an , the ) and punctuations are discarded before overlapping calculation .
( ROUGE-1=32 % , ROUGE-2=4 % , ROUGE - L=24 % )
IoU =
|p ∩ t| |p ∪ t|
where p and t and are the predicted and target contiguous intervals over the context document , containing the positions of the tokens .
Intersec1250020406080100Intersection over Union ( IoU ) ( % ) 020406080100F1 , ROUGE - L Score ( % ) F1ROUGE - LF1ROUGE - L020406080100Intersection over Union ( IoU ) ( % ) 020406080100ROUGE - N Score ( % ) ROUGE-1ROUGE-2ROUGE-1ROUGE-2100200300400500Answer Length0.0500.0750.1000.1250.1500.1750.2000.225Mean(Score - IoU )
( % ) ROUGE - LROUGE-1F1ROUGE-2  Table 4 : Ranking results on the development set .
BM25L performs similar to selecting the last 512 tokens in the context document as the answer .
BM25L - oracle knows the target answer span size .
Method BM25L BM25L - oracle Random Span
First Span Last Span
Method BERT - base e=2,s=128 BERT - base e=1,w , s=128 BERT - base e=2,w , s=128 BERT - base e=3,w , s=128 BERT - base e=2,w , s=256 BERT - base e=2,w , s=512 RoBERTa - base e=2,s=128 RoBERTa - base e=1,w , s=128 RoBERTa - base e=2,w , s=128 RoBERTa - base e=3,w , s=128 RoBERTa - base e=2,w , s=256 RoBERTa - base e=2,w , s=512 BERT - large e=2,w , s=128 RoBERTa - large e=2,w , s=128
EM 0.03 12.03 0.00 0.00 0.03
EM 23.27 23.33 24.53 22.77 24.09 17.87 26.18 25.32 27.21 26.65 27.33 17.17 28.54 30.92
Prec .
29.66 50.44 28.43 25.40 29.38
Prec .
60.28 59.79 61.78 60.24 61.64 58.06 62.59 61.76 62.71 61.83 62.21 62.16 63.83 66.74
Rec . 83.37 51.18 78.40 72.30 83.90
Rec . 84.10 81.50 83.46 83.73 79.08 66.35 82.87 84.36 85.34 84.78 82.33 64.71 84.68 87.47
F1 41.86 50.30 39.91 36.02 41.77
F1 64.34 63.12 64.90 63.89 63.38 55.98 65.25 65.22 66.17 65.55 66.08 57.11 66.95 69.85
IoU 23.28 29.16 20.67 15.70 23.63
IoU 54.04 53.11 54.81 53.49 53.41 46.01 55.47 55.28 56.33 55.79 56.23 47.17 57.24 60.56
Table 5 : BERT and RoBERTa results on the development set .
e=#epoch , w = warm - up over the ﬁrst 1,000 steps , s = stride .
tion ( p ∩ t = { x|x ∈ p and x ∈ t } ) measures the overlapping interval and union ( ∪ ) is deﬁned as p ∪ t = { x|x ∈ p or x ∈ t } .
Figure 4 ( left / middle ) compares the F1 and ROUGE - N scores and IoU for the Longformer model on the development set .
The F1 and ROUGE - N scores are always higher than IoU , but the metrics perform similarly in their higher values .
Somewhat surprisingly , the F1 score can be up to 40 % while there is no overlap between the two spans and IoU=0 .
We manually inspected the spans with F1>0 and IoU=0 and saw no significant semantic similarity between the predicted answer span and the target span .
The same pattern repeats for the ROUGE - N scores .
ROUGE-1 similar to F1 can reach 40 % while IoU=0 , but ROUGE-2 and ROUGE - L are less prone to such over - estimation due to lower chance of overlap of bigrams than unigrams and shorter LCSs in two random non - overlapping sequences .
Figure 4 ( right ) indicates that the F1 and ROUGE - N scores are higher than IoU for longer answers reiterating the fact that these scores over - estimate more for longer sequences .
Figure 5 shows two spans in a document with high F1 and ROUGE - N percentages , but different meanings .
5.2 Results and Discussion
We use the BM25L ranking function ( Trotman et al , 2014 ) to investigate how a basic IR approach can detect answer spans using TF - IDF features .
We adopt a sliding window approach with a window size of 512 and a stride of one sentence .
We compare BM25L with random window ( span ) selection and the ﬁrst and last window selection in the documents .
Table 4 presents the results of the ranking functions .
In the BM25L - oracle , we set the window size to the target answer span size .
BM25L - oracle outperforms the other methods but the results are far from perfect .
There is no signiﬁcant difference between BM25L and other methods .
The results restate the fact that there is little word overlap between non - factoid questions and their answers .
We analyze the performance of BERT and RoBERTa with different hyper - parameters on the development set in Table 5 .
Smaller strides , i.e. , higher overlap between the segments , and warm - up contribute to better performances .
RoBERTa constantly outperforms BERT , which is to be expected as RoBERTa is optimized robustly during the pretraining .
We use the HuggingFace ’s Transformers ( Wolf et al , 2019 ) code4 and train the base and large models on 2 and 4 GPUs , respectively .
We
4github.com/huggingface/transformers
1251  Method BERT - base BERT - large RoBERTa - base RoBERTa - large Longformer
# Param .
110 M 340 M 125 M 355 M 149 M
EM 25.03 30.29 29.07 33.40 50.30
Prec .
60.60 64.87 64.02 67.79 83.92
Rec . 82.48 84.62 84.79 87.56 85.17
F1 63.96 67.91 67.19 71.10 81.38
IoU 53.75 58.39 57.65 62.39 73.57
Table 6 : NLQuAD evaluation set results .
Longformer surpasses the other models in all the metrics except recall .
Method RoBERTa - large Human - AVG Longformer Human - UB
IoU
EM Prec .
Rec . F1 36.00 61.78 87.00 66.41 57.09 35.50 86.67 68.66 72.52 62.94 56.00 78.55 83.69 78.27 70.64 74.00 97.79 94.88 95.49 92.63
Table 7 : Comparing human performance with Longformer and RoBERTa - large on a subset of evaluation set .
UB = upper bound , AVG = average .
have to use a batch size of 12 and 8 , respectively , for the base and large models because of the long input sequence size and memory limitations .
We use the ofﬁcial AllenAI Longformer code5 to train Longformer on NLQuAD .
We use the same batch size of 12 ( batch size of 1 and gradient accumulation over 12 batches ) and learning rate warmup for the ﬁrst 1,000 steps .
Due to memory requirements , we limit the experiments to only the Longformer base model ( the large model can not ﬁt on our GPUs even with a batch size of 1 ) .
We ran the experiments on 2 NVIDIA P40 ( 24 GB GPU memory ) for about one day for 5 epochs .
Similarly , we choose the best epoch based on the performance on the development set .
Table 6 summarizes the scores obtained by the baseline systems on the NLQuAD evaluation set .
While Longformer signiﬁcantly outperforms BERT and RoBERTa , its performance , particularly in terms of IoU and EM , is far from perfect .
This demonstrates that NLQuAD and non - factoid QA is still an open problem for state - of - the - art models .
5.3 Human Evaluation
To ensure that the samples are of high quality , in addition to the initial investigation and pre - processing steps , we asked four volunteers to investigate 50 random samples from the evaluation set .
They rated the goodness of answers on a 3 - point scale : ( 1 : Irrelevant answer ; 2 : Good answer after adding or removing some sentences ; 3 : Perfect answer ) .
The average score is 2.56 indicating the high quality of NLQuAD ’s QA samples .
In order to benchmark human performance , we asked the four volunteers to answer 50 questions , a randomly sampled subset of evaluation set .
They were given unlimited time to detect the answers , but on average , it took them about 270 seconds to answer a question .
Table 7 compares human performance with Longformer and RoBERTa - large on the same subset .
Similar to HotpotQA ( Yang et al , 2018 ) , we estimate the human upper bound by tak5github.com/allenai/longformer
Pairwise comparison between the target Figure 6 : spans , Longformer , and RoBERTa ’s predicted spans .
X > Y means X is more preferable .
ing the best human answer in terms of our primary evaluation metric ( IoU ) for each sample .
While NLQuAD is a challenging task both for humans and the state of the art QA models , the human upper bound performance signiﬁcantly outperforms the models .
We suspect that the mediocre average of human performance , considering the high score of the target answers , might be because volunteers are not familiar with the articles ’ writing style or they might have become exhausted by reading long articles .
Furthermore , we asked another volunteer to compare the target answers with the predicted answers in a pairwise comparison for 100 samples .
Figure 6 shows that the target answers are preferred in 37 % and 64 % of cases over the Longformer and RoBERTa predictions , respectively .
The human evaluation is in line with the results shown in Table 6 and Table 7 .
1252Target >
Longformer   37%Target
= Longformer   61%Target < Longformer   2%Target >
RoBERTa   64%Target = RoBERTa   34%Target < RoBERTa   2%Longformer > RoBERTa   54%Longformer = RoBERTa   30%Longformer < RoBERTa   16%01020304050607080  Figure 7 : Effect of document and answer length on the performances .
Left : IoU drops in all models for longer documents .
Middle : RoBERTa and BERT outperform Longformer in longer answers .
Right : Longformer has a bias to predict shorter answers while RoBERTa and BERT predict longer answers .
The dashed line means y = x.
5.4 Error Analysis
Acknowledgments
Figure 7 compares the performance of BERT , RoBERTa , and Longformer for instances with different document and answer lengths .
As expected , both longer documents and longer answers are harder for the models .
Surprisingly , BERT and RoBERTa outperform Longformer for longer answers .
The same pattern occurs for F1 and EM ( not shown in the ﬁgure ) .
Figure 7 ( right ) shows that RoBERTa and BERT behave completely differently compared to Longformer for longer answer lengths .
The former models have a bias to predict longer spans while Longformer under - estimates the length of the answer span .
This different behaviour might be due to the sliding window approach and the prediction aggregation in the RoBERTa and BERT models and the attention dilation strategy in Longformer .
6 Conclusion
We introduce NLQuAD , a non - factoid long question answering data set from BBC news articles .
NLQuAD ’s question types and the long lengths of its context documents as well as answers , make it a challenging real - world task .
We propose to use Intersection over Union ( IoU ) as an evaluation metric for long question answering .
To establish a baseline performance , we experimented with the BERT , RoBERTa , and Longformer question answering models .
Longformer outperforms the other methods with an IoU of 73.57 % , but the results show that the performance of state - of - the - art question answering systems is far from perfect .
We hope NLQuAD will inspire more research in the area of document - level language understanding and question answering .
This research was partly supported by VIVAT .
We thank the BBC for giving permission to publish our extracted data for non - commercial , research purposes .
We also thank our volunteers for providing human assessments .
