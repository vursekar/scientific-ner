On the Computational Power of Transformers and its Implications in Sequence Modeling
Satwik Bhattamishra Arkil Patel Navin Goyal Microsoft Research India { t-satbh,t-arkpat,navingo}@microsoft.com
Abstract
Transformers are being used extensively across several sequence modeling tasks .
Signiﬁcant research effort has been devoted to experimentally probe the inner workings of Transformers .
However , our conceptual and theoretical understanding of their power and inherent limitations is still nascent .
In particular , the roles of various components in Transformers such as positional encodings , attention heads , residual connections , and feedforward networks , are not clear .
In this paper , we take a step towards answering these questions .
We analyze the computational power as captured by Turing - completeness .
We ﬁrst provide an alternate and simpler proof to show that vanilla Transformers are Turing - complete and then we prove that Transformers with only positional masking and without any positional encoding are also Turing - complete .
We further analyze the necessity of each component for the Turing - completeness of the network ; interestingly , we ﬁnd that a particular type of residual connection is necessary .
We demonstrate the practical implications of our results via experiments on machine translation and synthetic tasks .
1
Introduction
Transformer ( Vaswani et al , 2017 ) is a recent selfattention based sequence - to - sequence architecture which has led to state of the art results across various NLP tasks including machine translation ( Ott et al , 2018 ) , language modeling ( Radford et al , 2018 ) and question answering ( Devlin et al , 2019 ) .
Although a number of variants of Transformers have been proposed , the original architecture still underlies these variants .
While the training and generalization of machine learning models such as Transformers are the central goals in their analysis , an essential prerequisite to this end is characterization of the computational
Figure 1 : ( a ) Self - Attention Network with positional encoding , ( b ) Self - Attention Network with positional masking without any positional encoding
power of the model : training a model for a certain task can not succeed if the model is computationally incapable of carrying out the task .
While the computational capabilities of recurrent networks ( RNNs ) have been studied for decades ( Kolen and Kremer , 2001 ; Siegelmann , 2012 ) , for Transformers we are still in the early stages .
The celebrated work of Siegelmann and Sontag ( 1992 ) showed , assuming arbitrary precision , that RNNs are Turing - complete , meaning that they are capable of carrying out any algorithmic task formalized by Turing machines .
Recently , P´erez et al ( 2019 ) have shown that vanilla Transformers with hard - attention can also simulate Turing machines given arbitrary precision .
However , in contrast to RNNs , Transformers consist of several components and it is unclear which components are necessary for its Turing - completeness and thereby crucial to its computational expressiveness .
The role of various components of the Transformer in its efﬁcacy is an important question for further improvements .
Since the Transformer does not process the input sequentially , it requires some form of positional information .
Various positional encoding schemes have been proposed to capture order information ( Shaw et al , 2018 ; Dai et al , 2019 ; Huang et al , 2018 ) .
At the same time , on
Proceedingsofthe24thConferenceonComputationalNaturalLanguageLearning , pages455–475Online , November19 - 20,2020.c(cid:13)2020AssociationforComputationalLinguisticshttps://doi.org/10.18653 / v1 / P17455POS(1)POS(2)POS(3)(a)(b )  machine translation , Yang et al ( 2019 ) showed that the performance of Transformers with only positional masking ( Shen et al , 2018 ) is comparable to that with positional encodings .
In case of positional masking ( Fig . 1 ) , as opposed to explicit encodings , the model is only allowed to attend over preceding inputs and no additional positional encoding vector is combined with the input vector .
Tsai et al ( 2019 ) raised the question of whether explicit encoding is necessary if positional masking is used .
Additionally , since P´erez et al ( 2019 ) ’s
Turingcompleteness proof relied heavily on residual connections , they asked whether these connections are essential for Turing - completeness .
In this paper , we take a step towards answering such questions .
Below , we list the main contributions of the paper ,
• We provide an alternate and arguably simpler proof to show that Transformers are Turingcomplete by directly relating them to RNNs .
• More importantly , we prove that Transformers with positional masking and without positional encoding are also Turing - complete .
• We analyze the necessity of various components such as self - attention blocks , residual connections and feedforward networks for Turing - completeness .
Figure 2 provides an overview .
• We explore implications of our results on machine translation and synthetic tasks.1
2 Related Work
Computational Power of neural networks has been studied since the foundational paper McCulloch and Pitts ( 1943 ) ; in particular , among sequence - to - sequence models , this aspect of RNNs has long been studied ( Kolen and Kremer , 2001 ) .
The seminal work by Siegelmann and Sontag ( 1992 ) showed that RNNs can simulate a Turing machine by using unbounded precision .
Chen et al ( 2018 ) showed that RNNs with ReLU activations are also Turing - complete .
Many recent works have explored the computational power of RNNs in practical settings .
Several works ( Merrill et al , 2020 ) , ( Weiss et al , 2018 ) recently studied the ability of RNNs to recognize counter - like languages .
The capability of RNNs to recognize strings of balanced
1We have made our
available https://github.com/satwik77/Transformer-ComputationAnalysis .
source
code
at
parantheses has also been studied ( Sennhauser and Berwick , 2018 ;
Skachkova et al , 2018 ) .
However , such analysis on Transformers has been scarce .
Theoretical work on Transformers was initiated by P´erez et al ( 2019 ) who formalized the notion of Transformers and showed that it can simulate a Turing machine given arbitrary precision .
Concurrent to our work , there have been several efforts to understand self - attention based models ( Levine et
al , 2020 ; Kim et al , 2020 ) .
Hron et al ( 2020 ) show that Transformers behave as Gaussian processes when the number of heads tend to inﬁnity .
Hahn ( 2020 ) showed some limitations of Transformer encoders in modeling regular and context - free languages .
It has been recently shown that Transformers are universal approximators of sequence - tosequence functions given arbitrary precision ( Yun et al , 2020 ) .
However , these are not applicable2 to the complete Transformer architecture .
With a goal similar to ours , Tsai et al ( 2019 ) attempted to study the attention mechanism via a kernel formulation .
However , a systematic study of various components of Transformers has not been done .
3 Deﬁnitions and Preliminaries
All the numbers used in our computations will be from the set of rational numbers denoted Q.
For : = a sequence X = ( x1 , . . .
, xn ) , we set Xj ( x1 , . .
. , xj ) for 1 ≤ j ≤ n. We will work with an alphabet Σ of size m , with special symbols # and $ signifying the beginning and end of the input sequence , respectively .
The symbols are mapped to vectors via a given ‘ base ’ embedding fb : Σ → Qdb , where db is the dimension of the embedding .
E.g. , this embedding could be the one used for processing the symbols by the RNN .
We set fb ( # ) = 0db and fb($ ) = 0db .
Positional encoding is a function pos : N → Qdb .
Together , these provide embedding for a symbol s at position i given by f ( fb(s ) , pos(i ) ) , often taken ∈ Qm to be simply fb(s ) + pos(i ) .
Vector denotes one - hot encoding of a symbol s ∈ Σ.
s ( cid:74 )
( cid:75 )
3.1 RNNs
We follow Siegelmann and Sontag ( 1992 ) in our deﬁnition of RNNs .
To feed the sequences
2Hahn ( 2020 ) and Yun et al ( 2020 ) study encoder - only seqto - seq models with ﬁxed length outputs in which the computation halts as soon as the last symbol of the input is processed .
Our work is about the full Transformer ( encoder and decoder ) which is a seq - to - seq model with variable length sequence output in which the decoder starts operating sequentially after the encoder .
456  s1s2 . . .
sn ∈ Σ∗ to the RNN , these are converted to the vectors x1 , x2 , . . .
, xn
where xi = fb(si ) .
The RNN is given by the recurrence
ht = g(Whht−1 +
Wxxt + b ) , where t ≥ 1 , function g ( · ) is a multilayer feedforward network ( FFN ) with activation σ , bias vector b ∈ Qdh ,
matrices Wh ∈ Qdh×dh and Wx ∈ Qdh×db , and
ht ∈ Qdh is the hidden state with given initial hidden state h0 ; dh is the hidden state dimension .
After the last symbol sn has been fed , we continue to feed the RNN with the terminal symbol fb($ ) until it halts .
This allows the RNN to carry out computation after having read the input .
A class of seq - to - seq neural networks is Turingcomplete if the class of languages recognized by the networks is exactly the class of languages recognized by Turing machines .
Theorem 3.1 .
( Siegelmann and Sontag , 1992 )
Any seq - to - seq function Σ∗ → Σ∗ computable by a Turing machine can also be computed by an RNN .
For details please see section B.1 in appendix .
3.2 Transformer Architecture
Vanilla Transformer .
We describe the original Transformer architecture with positional encoding ( Vaswani et al , 2017 ) as formalized by P´erez et al ( 2019 ) , with some modiﬁcations .
All vectors in this subsection are from Qd .
The transformer , denoted Trans , is a seq - to - seq architecture .
Its input consists of ( i ) a sequence X = ( x1 , . . .
, xn ) of vectors , ( ii ) a seed vector y0 .
The output is a sequence
Y = ( y1 , . . .
, yr ) of vectors .
The sequence X is obtained from the sequence ( s1 , . . .
, sn ) ∈ Σn of symbols by using the embedding mentioned earlier : xi = f ( fb(si ) , pos(i ) ) .
The transformer consists of composition of transformer encoder and transformer decoder .
For the feedforward networks in the transformer layers we use the activation as in Siegelmann and Sontag ( 1992 ) , namely the saturated linear activation function σ(x ) which takes value 0 for x < 0 , value x for 0
<
x < 1 and value 1 for x > 1 .
This activation can be easily replaced by the standard ReLU activation via σ(x )
= ReLU(x )
− ReLU(x − 1 ) .
Self - attention .
The self - attention mechanism takes as input ( i ) a query vector q , ( ii ) a sequence of key vectors K = ( k1 , . . .
, kn ) , and ( iii ) a sequence of value vectors V = ( v1 , . . .
, vn ) .
The q - attention over K and V , denoted Att(q , K , V ) , is a vector a = α1v1+α2v2 + · · · + αnvn , where ( i )
( α1 , . .
. , αn ) = ρ(f att(q , k1 ) , . . .
, f att(q , kn ) ) .
( ii ) The normalization function ρ :
Qn → Qn ≥0 is hardmax : for x = ( x1 , . . .
, xn ) ∈ Qn , if the maximum value occurs r times among x1 , . . .
, xn , then hardmax(x)i : = 1 / r if xi is a maximum value and hardmax(x)i : = 0 otherwise .
In practice , the softmax is often used but its output values are in general not rational .
( iii ) For vanilla transformers , the scoring function f att used is a combination of multiplicative attention ( Vaswani et al , 2017 ) and a non - linear func(cid:12)(cid:104)q , ki(cid:105)(cid:12 ) tion : f att(q , ki )
= − ( cid:12 ) ( cid:12 ) .
This was also used by P´erez et al ( 2019 ) .
Transformer encoder .
A single - layer encoder is a function Enc(X ; θ ) , with input X = ( x1 , . . .
, xn ) a sequence of vectors in Qd , and parameters θ .
The output is another sequence Z = ( z1 , . . .
, zn ) of vectors in Qd .
The parameters θ specify functions Q ( · ) , K ( · ) , V ( · ) , and O ( · ) , all of type Qd → Qd .
The functions Q ( · ) , K ( · ) , and V ( · ) are linear transformations and O ( · ) an FFN .
For 1 ≤ i ≤ n , the output of the self - attention block is produced by
ai = Att(Q(xi ) , K(X ) , V ( X ) )
+ xi
( 1 )
This operation is also referred to as the encoderencoder attention block .
The output Z is computed by zi = O(ai ) + ai for 1 ≤ i ≤ n.
The addition operations + xi and + ai are the residual connections .
The complete L - layer transformer encoder TEnc(L)(X ; θ ) = ( Ke , V e ) has the same input X = ( x1 , . . .
, xn ) as the single - layer encoder .
In contrast , its output Ke = ( ke 1 , . . . , ke n ) and V e = ( ve 1 , . . .
ve n )
contains two sequences .
TEnc(L ) is obtained by composition of L singlelayer encoders : let X ( 0 ) : = X , and for 0 ≤ ( cid:96 ) ≤ L − 1 , let X ( ( cid:96)+1 ) = Enc(X ( ( cid:96 ) ) ; θ(cid:96 ) ) and ﬁnally , Ke = K(L)(X ( L ) ) , V e = V ( L)(X ( L ) ) .
Transformer decoder .
The input to a singlelayer decoder is ( i ) ( Ke , V e ) output by the encoder , and ( ii ) sequence
Y = ( y1 , . . .
, yk ) of vectors for k ≥ 1 .
The output is another sequence Z = ( z1 , . . . , zk ) .
Similar to the single - layer encoder , a singleis parameterized by functions
layer decoder Q ( · ) , K ( · ) , V ( · ) and O ( · ) and is deﬁned by
pt = Att(Q(yt ) , K(Yt ) , V ( Yt ) )
+ yt , ( 2 ) at = Att(pt , Ke , V e ) + pt , ( 3 ) zt = O(at ) + at ,
where 1 ≤ t ≤ k.
The operation in ( 2 ) will be
457  referred to as the decoder - decoder attention block and the operation in ( 3 ) as the decoder - encoder attention block .
In ( 2 ) , positional masking is applied to prevent the network from attending over symbols which are ahead of them .
An
L - layer
Transformer
z = F ( yL
decoder TDecL((Ke , V e ) , Y ; θ ) = z is obtained by repeated application of L single - layer decoders each with its own parameters , and a transformation function F :
Qd → Qd applied to the last vector in the sequence of vectors output by the ﬁnal decoder .
Formally , for 0 ≤ ( cid:96 ) ≤ L−1 and Y 0 : = Y we have Y ( cid:96)+1 = Dec((Ke , V e ) , Y ( cid:96 ) ; θ(cid:96 ) ) , k ) .
Note that while the output of a single - layer decoder is a sequence of vectors , the output of an L - layer Transformer decoder is a single vector .
The complete Transformer .
The output Trans(X , y0 )
= Y is computed by the recurrence ˜yt+1 = TDec(TEnc(X ) , ( y0 , y1 , . . .
, yt ) ) , for 0 ≤ t ≤ r − 1 .
We get yt+1 by adding positional encoding : yt+1 = ˜yt+1 + pos(t + 1 ) .
Directional Transformer .
We denote the Transformer with only positional masking and no positional encodings as Directional Transformer and use them interchangeably .
In this case , we use standard multiplicative attention as the scoring function in our construction , i.e , f att(q , ki ) = ( cid:104)q , ki(cid:105 ) .
The general architecture is the same as for the vanilla case ; the differences due to positional masking are the following .
t ) .
t , V e
There are no positional encodings .
So the input vectors xi only involve fb(si ) .
Similarly , yt = ˜yt .
In ( 1 ) , Att ( · ) is replaced : = by Att(Q(xi ) , K(Xi ) , V ( Xi ) ) where Xi ( x1 , . . .
, xi ) for 1 ≤ i ≤ n.
Similarly , in ( 3 ) , Att ( · ) is replaced by Att(pt , Ke Remark 1 .
Our deﬁnitions deviate slightly from practice , hard - attention being the main one since hardmax keeps the values rational whereas softmax takes the values to irrational space .
Previous studies have shown that soft - attention behaves like hard - attention in practice and Hahn ( 2020 ) discusses its practical relevance .
Remark 2 .
Transformer Networks with positional encodings are not necessarily equivalent in terms of their computational expressiveness ( Yun et al , 2020 ) to those with only positional masking when considering the encoder only model ( as used in BERT and GPT-2 ) .
Our results in Section 4.1 show their equivalence in terms of expressiveness for the complete seq - to - seq architecture .
4 Primary Results
4.1 Turing - Completeness Results
In light of Theorem 3.1 , to prove that Transformers are Turing - complete , it sufﬁces to show that they can simulate RNNs .
We say that a Transformer simulates an RNN ( as deﬁned in Sec . 3.1 ) if on every input s ∈ Σ∗ , at each step t , the vector yt contains the hidden state ht as a subvector , i.e. yt =
[ ht , · ] , and halts at the same step as the RNN .
Theorem 4.1 .
The class of Transformers with positional encodings is Turing - complete .
Proof Sketch .
The input s0 , . . .
, sn ∈ Σ∗ is provided to the transformer as the sequence of vectors x0 , . . .
, xn , where xi
=
[ 0dh , fb(si ) , 0dh , i , 1 ] , which has as sub - vector the given base embedding fb(si ) and the positional encoding i , along with extra coordinates set to constant values and will be used later .
The basic observation behind our construction of the simulating Transformer is that the transformer decoder can naturally implement the recurrence operations of the type used by RNNs .
To this end , the FFN Odec ( · ) of the decoder , which plays the same role as the FFN component of the RNN , needs sequential access to the input in the same way as RNN .
But the Transformer receives the whole input at the same time .
We utilize positional encoding along with the attention mechanism to isolate xt
at time t and feed it to Odec ( · ) , thereby simulating the RNN .
As stated earlier , we append the input s1 , . . .
, sn of the RNN with $ ’s until it halts .
Since the Transformer takes its input all at once , appending by $ ’s is not possible ( in particular , we do not know how long the computation would take ) .
Instead , we append the input with a single $ .
After encountering a $ once , the Transformer will feed ( encoding of ) $ to Odec ( · ) in subsequent steps until termination .
Here we conﬁne our discussion to the case t ≤ n ; the t > n case is slightly different but simpler .
The construction is straightforward : it has only one head , one encoder layer and one decoder layer ; moreover , the attention mechanisms in the encoder and the decoder - decoder attention block of the decoder are trivial as described below .
The encoder attention layer does trivial computation in that it merely computes the identity function : zi = xi , which can be easily achieved , e.g. by using the residual connection and setting the value vectors to 0 .
The ﬁ458  t < n and at i = n for t ≥ n.
This use of scoring function is similar to P´erez et al ( 2019 ) .
At this point , Odec ( · ) has at its disposal the hidden state ht ( coming from yt via pt and the residual connection ) and the input symbol xt
( coming via the attention mechanism and the residual connection ) .
Hence O ( · ) can act just like the FFN ( Lemma C.4 ) underlying the RNN to compute ht+1 and thus yt+1 , proving the induction hypothesis .
The complete construction can be found in Sec .
C.2 in the appendix .
Theorem 4.2 .
The class of Transformers with positional masking and no explicit positional encodings is Turing - complete .
Proof Sketch .
As before , by Theorem 3.1 it sufﬁces to show that Transformers can simulate RNNs .
The input s0 , . . .
, sn is provided to the transformer as the sequence of vectors x0 , . . .
, xn , where xi
=
[ 0dh , 0dh , fb(si ) , , 0 , 0 m , 0 m , 0 m ] .
The si ( cid:75 ) ( cid:74 ) general goal for the directional case is similar to the vanilla case , namely we would like the FFN Odec ( · ) of the decoder to directly simulate the computation in the underlying RNN .
In the vanilla case , positional encoding and the attention mechanism helped us feed input xt
at the t - th iteration of the decoder to Odec ( · )
.
However , we no longer have explicit positional information in the input xt such as a coordinate with value t.
The key insight is that we do not need the positional information explicitly to recover xt
at step t : in our construction , the attention mechanism with masking will recover xt in an indirect manner even though it ’s not able to “ zero in ” on the t - th position .
Let us ﬁrst explain this without details of the construction .
We maintain in vector ωt ∈ Qm , with a coordinate each for symbols in Σ , the fraction of times the symbol has occurred up to step t.
Now , at a step t ≤ n , for the difference ωt − ωt−1 ( which is part of the query vector ) , it can be shown easily that only the coordinate corresponding to st is positive .
Thus after applying the linearized sigmoid σ(ωt − ωt−1 ) , we can isolate the coordinate corresponding to st . Now using this query vector , the ( hard ) attention mechanism will be able to retrieve the value vectors for all indices j such that sj = st and output their average .
Crucially , the value vector for an index j is essentially xj which depends only on sj .
Thus , all these vectors are equal to xt , and so is their average .
This recovers xt , which
Figure 2 : Transformer network with various components highlighted .
The components marked red are essential for the Turing - completeness whereas for the pairs of blocks and residual connections marked green , either one of the component is enough .
The dashed residual connection is not necessary for Turingcompleteness of the network .
nal K(1 ) ( · ) and V ( 1 ) ( · ) functions bring ( Ke , V e ) into useful forms by appropriate linear transformations :
ki = [ 0db , 0db , 0db , −1 , i ] and vi = [ 0db , fb(si ) , 0db , 0 , 0 ] .
Thus , the key vectors only encode the positional information and the value vectors only encode the input symbols .
The output sequence of the decoder is y1 , y2 , . . ..
Our construction will ensure , by induction on t , that yt contains the hidden states ht of the RNN as a sub - vector along with positional information :
yt =
[ ht , 0db , 0db , t + 1 , 1 ] .
This is easy to arrange for t = 0 , and assuming it for t we prove it for t+1 .
As for the encoder , the decoder - decoder attention block acts as the identity : pt = yt .
Now , using the last but one coordinate in yt representing the time t + 1 , the attention mechanism Att(pt , Ke , V e ) can retrieve the embedding of the t - th input symbol xt .
This is possible because in the key vector ki mentioned above , almost all coordinates other than the one representing the position i are set to 0 , allowing the mechanism to only focus on the positional information and not be distracted by the other contents of pt = yt : the scoring function has value f att(pt , ki ) = −|(cid:104)pt , ki(cid:105)| = −|i − ( t + 1)| .
For a given t , it is maximized at i = t + 1 for
459Encoder - Encoder Attention HeadFeed Forward NetworkInputEmbeddingDecoder - DecoderAttention HeadDecoder - EncoderAttention HeadFeed Forward NetworkOutputEmbeddingPositional EncodingPositional Encoding  can now be fed to Odec ( · ) , simulating the RNN .
We now outline the construction and relate it to the above discussion .
As before , for simplicity we restrict to the case t ≤ n. We use only one head , one layer encoder and two layer decoder .
The encoder , as in the vanilla case , does very little other than pass information along .
The vectors in ( Ke , V e ) are obtained by the trivial attention mechanism followed by simple linear transformations : ke , 0 , 0 m , 0 m , 0 m ] ( cid:75 ) and ve
si ( cid:74 ) Our construction ensures that at step t we have 2 t , 0 m , 0 m , ωt−1 ] .
As
yt =
[ ht−1 , 0dh , 0db , 0 m , 1 before , the proof is by induction on t.
i =
[ 0dh , 0dh , fb(si ) , 0 m , 0 , 0 m ,
i =
[ 0dh , 0dh , 0db ,
, 0 m ] .
( cid:75 )
si ( cid:74 )
In the ﬁrst
layer of decoder ,
the decoderp(1 )
= decoder attention block is trivial : t yt .
In the decoder - encoder attention block , we give equal attention to all the t + 1 values , which along with Oenc ( · ) , leads to z(1 ) t = 1 2t+1 , 0 m , 0 m , ωt ] , where [ ht−1 , 0dh , 0db , δt , essentially δt = σ(ωt − ωt−1 ) , except with a change for the last coordinate due to special status of the last symbol $ in the processing of RNN .
t
, ke
j ( cid:105 ) = ( cid:104)δt ,
t = z(1 )
In the second layer , the decoder - decoder attention block is again trivial with p(2 ) .
We remark that in this construction , the scoring function is the standard multiplicative attention 3 .
Now ( cid:104)p(2 ) ( cid:105 ) = δt , j , which is positive if t ( cid:75 ) and only if sj = st , as mentioned earlier .
Thus attention weights in Att(p(2 ) t ) satisfy hardmax((cid:104)p(2 ) 1(cid:105 ) , . . .
, ( cid:104)p(2 ) ( I(s0 = , ke st ) , I(s1 = st ) , . . .
, I(st = st ) ) , where λt is a normalization constant and I ( · ) is the indicator .
See Lemma D.3 for more details .
t , V e , Ke t ( cid:105 ) )
= 1
λt
sj ( cid:74 )
, ke
t
t
t
t
via p(2 )
At this point , Odec ( · ) has at its disposal the hidden state ht ( coming from z(1 ) and the residual connection ) and the input symbol xt
( coming via the attention mechanism and the residual connection ) .
Hence Odec ( · ) can act just like the FFN underlying the RNN to compute ht+1 and thus yt+1 , proving the induction hypothesis .
t
The complete construction can be found in
Sec .
D in the Appendix .
In practice , Yang et al ( 2019 ) found that for NMT , Transformers with only positional masking achieve comparable performance compared to the ones with positional encodings .
Similar evidence
3Note that it is closer to practice than the scoring function
−|(cid:104)q , k(cid:105)| used in P´erez et al ( 2019 ) and Theorem 4.1
was found by Tsai et al ( 2019 ) .
Our proof for directional transformers entails that there is no loss of order information if positional information is only provided in the form of masking .
However , we do not recommend using masking as a replacement for explicit encodings .
The computational equivalence of encoding and masking given by our results implies that any differences in their performance must come from differences in learning dynamics .
4.2 Analysis of Components
The results for various components follow from our construction in Theorem 4.1 .
Note that in both the encoder and decoder attention blocks , we need to compute the identity function .
We can nullify the role of the attention heads by setting the value vectors to zero and making use of only the residual connections to implement the identity function .
Thus , even if we remove those attention heads , the model is still Turing - complete .
On the other hand , we can remove the residual connections around the attention blocks and make use of the attention heads to implement the identity function by using positional encodings .
Hence , either the attention head or the residual connection is sufﬁcient to achieve Turing - completeness .
A similar argument can be made for the FFN in the encoder layer : either the residual connection or the FFN is sufﬁcient for Turing - completeness .
For the decoder - encoder attention head , since it is the only way for the decoder to obtain information about the input , it is necessary for the completeness .
The FFN is the only component that can perform computations based on the input and the computations performed earlier via recurrence and hence , the model is not Turing - complete without it .
Figure 2 summarizes the role of different components with respect to the computational expressiveness of the network .
Proposition 4.3 .
The class of Transformers without residual connection around the decoderencoder attention block is not Turing - complete .
Proof Sketch .
We conﬁne our discussion to singlelayer decoder ; the case of multilayer decoder is similar .
Without the residual connection , the decoder - encoder attention block produces at = Att(pt , Ke , V e )
= ( cid:80)n i=1
αive i for some αi ’s such that ( cid:80)n i αi = 1 .
Note that , without residual connection at can take on at most 2n − 1 values .
This is because by the deﬁnition of hard attention the vector ( α1 , . . .
, αn ) is characterized by the set of zero coordinates and there are at most 2n − 1
460  such sets ( all coordinates can not be zero ) .
This restriction on the number of values on at holds regardless of the value of pt .
If the task requires the network to produce values of at that come from a set with size at least 2n , then the network will not be able to perform the task .
Here ’s an example task : given a number ∆ ∈ ( 0 , 1 ) , the network must produce numbers 0 , ∆ , 2∆ , . .
. , k∆ , where k is the maximum integer
such that k∆ ≤ 1 .
If the network receives a single input ∆ , then it is easy to see that the vector at will be a constant ( ve 1 ) at any step and hence the output of the network will also be constant at all steps .
Thus , the model can not perform such a task .
If the input is combined with n − 1 auxiliary symbols ( such as # and $ ) , then in the network , each at takes on at most 2n − 1 values .
Hence , the model will be incapable of performing the task if ∆ <
1/2n .
Such a limitation does not exist with a residual connection since the vector at = ( cid:80)n
i + pt can take arbitrary number of values depending on its prior computations in pt .
For further details , see Sec . C.1 in the Appendix .
i=1 αive
Discussion .
It is perhaps surprising that residual connection , originally proposed to assist in the learning ability of very deep networks , plays a vital role in the computational expressiveness of the network .
Without it , the model is limited in its capability to make decisions based on predictions in the previous steps .
We explore practical implications of this result in section 5 .
5 Experiments
In this section , we explore the practical implications of our results .
Our experiments are geared towards answering the following questions : Q1 .
Are there any practical implications of the limitation of Transformers without decoder - encoder residual connections ?
What tasks can they do or not do compared to vanilla Transformers ?
Q2 .
Is there any additional beneﬁt of using positional masking as opposed to absolute positional encoding ( Vaswani et al , 2017 ) ?
Although we showed that Transformers without decoder - encoder residual connection are not Turing complete , it does not imply that they are incapable of performing all the tasks .
Our results suggest that they are limited in their capability to make inferences based on their previous computations , which is required for tasks such as counting and language modeling .
However , it can be shown that the model
is capable of performing tasks which rely only on information provided at a given step such as copying and mapping .
For such tasks , given positional information at a particular step , the model can look up the corresponding input and map it via the FFN .
We evaluate these hypotheses via our experiments .
Model
Copy Task
Counting
Vanilla Transformers - Dec - Enc Residual - Dec - Dec Residual
100.0 99.7 99.7
100.0 0.0 99.8
Table 1 : BLEU scores ( ↑ ) for copy and counting task .
Please see Section 5 for details
For our experiments on synthetic data , we consider two tasks , namely the copy task and the counting task .
For the copy task , the goal of a model is to reproduce the input sequence .
We sample sentences of lengths between 5 - 12 words from Penn Treebank and create a train - test split of 40k-1k with all sentences belonging to the same range of length .
In the counting task , we create a very simple dataset where the model is given one number between 0 and 100 as input and its goal is to predict the next ﬁve numbers .
Since only a single input is provided to the encoder , it is necessary for the decoder to be able to make inferences based on its previous predictions to perform this task .
The beneﬁt of conducting these experiments on synthetic data is that they isolate the phenomena we wish to evaluate .
For both these tasks , we compare vanilla Transformer with the one without decoder - encoder residual connection .
As a baseline we also consider the model without decoder - decoder residual connection , since according to our results , that connection does not inﬂuence the computational power of the model .
We implement a single layer encoderdecoder network with only a single attention head in each block .
We then assess the inﬂuence of the limitation on Machine Translation which requires a model to do a combination of both mapping and inferring from computations in previous timesteps .
We evaluate the models on IWSLT’14 German - English dataset and IWSLT’15 English - Vietnamese dataset .
We again compare vanilla Transformer with the ones without decoder - encoder and decoder - decoder residual connection .
While tuning the models , we vary the number of layers from 1 to 4 , the learning rate , warmup steps and the number of heads .
Speciﬁcations of the models , experimental setup , datasets and sample outputs can be found in Sec .
E
461  Model
De - En
En - Vi
Vanilla Transformers - Dec - Enc Residual - Dec - Dec Residual
32.9 24.1 30.6
28.8 21.8 27.2
Table 2 : BLEU scores ( ↑ ) for translation task .
Please see Section 5 for details .
in the Appendix .
Results on the effect of residual connections on synthetic tasks can be found in Table 1 .
As per our hypothesis , all the variants are able to perfectly perform the copy task .
For the counting task , the one without decoder - encoder residual connection is incapable of performing it .
However , the other two including the one without decoder - decoder residual connection are able to accomplish the task by learning to make decisions based on their prior predictions .
Table 3 provides some illustrative sample outputs of the models .
For the MT task , results can be found in Table 2 .
While the drop from removing decoder - encoder residual connection is signiﬁcant , it is still able to perform reasonably well since the task can be largely fulﬁlled by mapping different words from one sentence to another .
For positional masking , our proof technique suggests that due to lack of positional encodings , the model must come up with its own mechanism to make order related decisions .
Our hypothesis is that , if it is able to develop such a mechanism , it should be able to generalize to higher lengths and not overﬁt on the data it is provided .
To evaluate this claim , we simply extend the copy task upto higher lengths .
The training set remains the same as before , containing sentences of length 5 - 12 words .
We create 5 different validation sets each containing 1k sentences each .
The ﬁrst set contains sentences within the same length as seen in training ( 5 - 12 words ) , the second set contains sentences of length 13 - 15 words while the third , fourth and ﬁfth sets contain sentences of lengths 15 - 20 , 21 - 25 and 26 - 30 words respectively .
We consider two models , one which is provided absolute positional encodings and one where only positional masking is applied .
Figure 3 shows the performance of these models across various lengths .
The model with positional masking clearly generalizes up to higher lengths although its performance too degrades at extreme lengths .
We found that the model with absolute positional encodings during training overﬁts on the fact that the 13th token is always the terminal symbol .
Hence , when evaluFigure 3 : Performance of the two models on the copy task across varying lengths of test inputs .
DiSAN refers to Transformer with only positional masking .
SAN refers to vanilla Transformers .
ated on higher lengths it never produces a sentence of length greater than 12 .
Other encoding schemes such as relative positional encodings ( Shaw et al , 2018 ; Dai et al , 2019 ) can generalize better , since they are inherently designed to address this particular issue .
However , our goal is not to propose masking as a replacement of positional encodings , rather it is to determine whether the mechanism that the model develops during training is helpful in generalizing to higher lengths .
Note that , positional masking was not devised by keeping generalization or any other beneﬁt in mind .
Our claim is only that , the use of masking does not limit the model ’s expressiveness and it may beneﬁt in other ways , but during practice one should explore each of the mechanisms and even a combination of both .
Yang et al ( 2019 ) showed that a combination of both masking and encodings is better able to learn order information as compared to explicit encodings .
SOURCE REFERENCE
VANILLA TRANSFORMER - DEC - ENC RESIDUAL - DEC - DEC RESIDUAL
– 42 – 43 44 45 46 47
– 43 44 45 46 47 – 27 27 27 27 27 – 43 44 45 46 47
Table 3 : Sample outputs by the models on the counting task .
Without the residual connection around DecoderEncoder block , the model is incapable of predicting more than one distinct output .
6 Discussion and Final Remarks
We showed that the class of languages recognized by Transformers and RNNs are exactly the same .
This implies that the difference in performance of both the networks across different tasks can be attributed only to their learning abilities .
In contrast to RNNs , Transformers are composed of multiple components which are not essential for their com462  putational expressiveness .
However , in practice they may play a crucial role .
Recently , Voita et al ( 2019 ) showed that the decoder - decoder attention heads in the lower layers of the decoder do play a signiﬁcant role in the NMT task and suggest that they may be helping in language modeling .
This indicates that components which are not essential for the computational power may play a vital role in improving the learning and generalization ability .
Take - Home Messages .
We showed that the order information can be provided either in the form of explicit encodings or masking without affecting computational power of Transformers .
The decoder - encoder attention block plays a necessary role in conditioning the computation on the input sequence while the residual connection around it is necessary to keep track of previous computations .
The feedforward network in the decoder is the only component capable of performing computations based on the input and prior computations .
Our experimental results show that removing components essential for computational power inhibit the model ’s ability to perform certain tasks .
At the same time , the components which do not play a role in the computational power may be vital to the learning ability of the network .
Although our proofs rely on arbitrary precision , which is common practice while studying the computational power of neural networks in theory ( Siegelmann and Sontag , 1992 ; P´erez et al , 2019 ; Hahn , 2020 ; Yun et al , 2020 ) , implementations in practice work over ﬁxed precision settings .
However , our construction provides a starting point to analyze Transformers under ﬁnite precision .
Since RNNs can recognize all regular languages in ﬁnite precision ( Korsky and Berwick , 2019 ) , it follows from our construction that Transformer can also recognize a large class of regular languages in ﬁnite precision .
At the same time , it does not imply that it can recognize all regular languages given the limitation due to the precision required to encode positional information .
We leave the study of Transformers in ﬁnite precision for future work .
