Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP , pages 300‚Äì313 Online , November 20 , 2020 .
c  2020 Association for Computational Linguistics300Evaluating Attribution Methods using White - Box LSTMs Yiding Hao Yale University New Haven , CT ,
USA yiding.hao@yale.edu
Abstract Interpretability methods for neural networks are difÔ¨Åcult to evaluate because we do not understand the black - box models typically used to test them .
This paper proposes a framework in which interpretability methods are evaluated using manually constructed networks , which we call white - box networks , whose behavior is understood a priori .
We evaluate Ô¨Åve methods for producing attribution heatmaps by applying them to white - box LSTM classiÔ¨Åers for tasks based on formal languages .
Although our white - box classiÔ¨Åers solve their tasks perfectly and transparently , we Ô¨Ånd that all Ô¨Åve attribution methods fail to produce the expected model explanations .
1 Introduction Attribution methods are a family of interpretability techniques for individual neural network predictions that attempt to measure the importance of input features for determining the model ‚Äôs output .
Given an input , an attribution method produces a vector of attribution orrelevance scores , which is typically visualized as a heatmap that highlights portions of the input that contribute to model behavior .
In the context of NLP , attribution scores are usually computed at the token level , so that each score represents the importance of a token within an input sequence .
These heatmaps can be used to identify keywords upon which networks base their decisions ( Li et al . , 2016 ; Sundararajan et al . , 2017 ; Arras et al . , 2017a , b;Murdoch et al . , 2018 , inter alia ) .
One of the main challenges facing the evaluation of attribution methods is that it is difÔ¨Åcult to assess the quality of a heatmap when the network in question is not understood in the Ô¨Årst place .
If a word is deemed relevant by an attribution method , we do not know whether the model actually considers that word relevant , or whether the attribu - tion method has erroneously estimated its importance .
Indeed , previous studies have argued that attribution methods are sensitive to features unrelated to model behavior in some cases ( e.g. , Kindermans et al . , 2019 ) , and altogether insensitive to model behavior in others ( Adebayo et al . , 2018 ) .
To tease the evaluation of attribution methods apart from the interpretation of models , this paper proposes an evaluation framework for attribution methods in NLP that uses only models that are fully understood a priori .
Instead of testing attribution methods on black - box models obtained through training , we construct white - box models for testing by directly setting network parameters by hand .
Our focus is on white - box LSTMs that implement intuitive strategies for solving simple classiÔ¨Åcation tasks based on formal languages with deterministic solutions .
We apply our framework to Ô¨Åve attribution methods : occlusion ( Zeiler and Fergus , 2014 ) , saliency ( Simonyan et
al . , 2014 ; Li et al . , 2016 ) , gradient input , ( G I , Shrikumar et al . , 2017 ) , integrated gradients ( IG , Sundararajan et al . , 2017 ) , and layer - wise relevance propagation ( LRP , Bach et al . , 2015 ) .
In doing so , we make the following contributions .
We construct four white - box LSTMs that can be used to test attribution methods .
We provide a complete description of our model weights in Appendix A .1Beyond
the Ô¨Åve methods considered here , our white - box networks can be used to test any attribution method compatible with LSTMs .
Empirically , we show that all Ô¨Åve attribution methods produce erroneous heatmaps for our white - box networks , despite the models ‚Äô transparent behavior .
As a preview of our re1We also provide code for our models at https:// github.com/yidinghao/whitebox-lstm .
301Task : Determine whether the input contains one of the following subsequences : ab , bc , cd , ordc .
Output : True , since the input aacb contains two ( noncontiguous ) instances of ab .
Occlusion Saliency G I IG LRP aacb aacb aacbaacbaacb aacb aacb aacbaacbaacb Table 1 : Sample heatmaps for two white - box networks : a ‚Äú counter - based ‚Äù network ( top ) and an ‚Äú FSA - based ‚Äù network ( bottom ) .
The features relevant to the output are the two as and the b. sults , Table 1 shows sample heatmaps computed for two models designed to identify the non - contiguous subsequence abin the input aacb .
Even though both models ‚Äô outputs are determined by the presence of the two as and the b , all four methods either incorrectly highlight the cor fail to highlight at least one of the as in at least one case .
We identify two general ways in which four of the Ô¨Åve methods do not behave as intended .
Firstly , while saliency , G I and IG are theoretically invariant to differences in model implementation ( Sundararajan et al . , 2017 ) , in practice we Ô¨Ånd that these methods can still produce qualitatively different heatmaps for nearly identical models .
Secondly , we Ô¨Ånd that LRP is susceptible to numerical issues , which cause heatmaps to be zeroed out when values are rounded to zero .
2 Related Work Several approaches have been taken in the literature for understanding how to evaluate attribution methods .
On a theoretical level , axiomatic approaches propose formal desiderata that attribution methods should satisfy , such as implementation invariance ( Sundararajan et al . , 2017 ) , input translation invariance ( Kindermans et al . , 2019 ) , continuity with respect to inputs ( Montavon et al . , 2018 ; Ghorbani et al . , 2019 ) , or the existence of relationships between attribution scores and logit or softmax scores ( Sundararajan et al . , 2017 ; Ancona et al . , 2018 ; Montavon , 2019 ) .
The degree to which attribution methods fulÔ¨Åll these criteria can be determined either mathematically or empirically .
Other approaches , which are more experimental in nature , attempt to directly assess the relationship between attribution scores and model behav - ior .
A common test , due to Bach et al . ( 2015 ) and Samek et al . ( 2017 ) and applied to sequence modeling by Arras et al .
( 2017a ) , involves ablating or perturbing parts of the input , from those with the highest attribution scores to those with the lowest , and counting the number of features that need to be ablated in order to change the model ‚Äôs prediction .
Another test , proposed by Adebayo et al . ( 2018 ) , tracks how heatmaps change as layers of a network are incrementally randomized .
A third kind of approach evaluates the extent to which heatmaps identify salient input features .
For example , Zhang et al .
( 2018 ) propose the pointing game task , in which the highest - relevance pixel for an image classiÔ¨Åer input must belong to the object described by the target output class .
Within this framework , Kim et al . ( 2018 ) , Poerner et al . ( 2018 ) , Arras et al . ( 2019 ) , and Yang and Kim ( 2019 ) construct datasets in which input features exhibit experimentally controlled notions of importance , yielding ‚Äú ground truth ‚Äù attributions against which heatmaps can be evaluated .
Our paper incorporates elements of the groundtruth approaches , since it is straightforward to determine which input features are important for our formal language tasks .
We enhance these approaches by using white - box models that are guaranteed to be sensitive to those features .
3 Formal Language Tasks Formal languages are often used to evaluate the expressive power of RNNs .
Here , we focus on formal languages that have been recently used to probe LSTMs ‚Äô ability to capture three kinds of dependencies : counting , long - distance , and hierarchical dependencies .
We deÔ¨Åne a classiÔ¨Åcation task based on each of these formal languages .
3.1 Counting Dependencies Counter languages ( Fischer , 1966 ; Fischer et
al . , 1968 ) are languages recognized by automata equipped with counters .
Weiss et
al .
( 2018 ) demonstrate using an acceptance task for the languages anbnandanbncnthat LSTMs naturally learn to use cell state units as counters .
Merrill ‚Äôs ( 2019 ) asymptotic analysis shows that LSTM acceptors accept only counter languages when their weights are fully saturated .
Thus , counter languages may be viewed as a characterization of the expressive power of LSTMs .
We deÔ¨Åne the counting task based on a simple
302example of a counting language .
Task 1 ( Counting Task ) .Given
a string in x2 fa;bg , determine whether or not xhas strictly more as than bs .
Example 2 .
The counting task classiÔ¨Åes aaab as True , abasFalse , andbbbba asFalse .
A counter automaton can solve the counting task by incrementing its counter whenever an a is encountered and decrementing it whenever a b is encountered .
It outputs True if and only if its counter is at least 1 .
We expect attribution scores for all input symbols to have roughly the same magnitude , but that scores assigned to awill have the opposite sign to those assigned to b. 3.2 Long - Distance Dependencies Strictly piecewise ( SP , Heinz , 2007 ) languages were used by Avcu et
al . ( 2017 ) and Mahalunkar and Kelleher ( 2018 , 2019a , b ) to test the propensity of LSTMs to learn long - distance dependencies , compared to Elman ‚Äôs ( 1990 ) simple recurrent networks .
SP languages are regular languages whose membership is deÔ¨Åned by the presence or absence of certain subsequences , which may or may not be contiguous .
For example , adis a subsequence of abcde , since both letters of adoccur inabcde , in the same order .
Based on these ideas , we deÔ¨Åne the SP task as follows .
Task 3 ( SP Task ) .Given
x2 fa;b;c;dg , determine whether or not xcontains at least one of the following subsequences : ab , bc , cd , dc .
Example 4 .
In the SP task , aab is classiÔ¨Åed as True , since it contains the subsequence ab .
Similarly , acb is classiÔ¨Åed as True , since it contains abnon - contiguously .
The string aaa is classiÔ¨Åed asFalse .
The choice of SP languages as a test for longdistance dependencies is motivated by the fact that symbols in a non - contiguous subsequence may occur arbitrarily far from one another .
The SP task yields a variant of the pointing game task in the sense that the input string may or may not contain an ‚Äú object ‚Äù ( one of the four subsequences ) that the network must identify .
Therefore , we expect an input symbol to receive a nonzero attribution score if and only if it comprises a subsequence .
3.3 Hierarchical Dependencies The Dyck language is the language Dgenerated by the following context - free grammar , where " isthe empty string .
S!SSj(S)j[S]j " Dcontains all balanced strings of parentheses and square brackets .
Since Dis often viewed as a canonical example of a context - free language ( Chomsky and Sch ¬®utzenberger , 1959 ) , several recent studies , including Sennhauser and Berwick ( 2018 ) , Bernardy ( 2018 ) , Skachkova et al .
( 2018 ) , andYu et al . ( 2019 ) , have used Dto evaluate whether LSTMs can learn hierarchical dependencies implemented by pushdown automata .
Here , we consider the bracket prediction task proposed bySennhauser and Berwick ( 2018 ) .
Task 5 ( Bracket Prediction Task ) .Given
a preÔ¨Åx p of some string in D , identify the next valid closing bracket for p. Example 6 .
The string [ ( [ ] requires a prediction of ) , since the ( is the last unclosed bracket .
Similarly , ( ( ) [ requires a prediction of ] .
Strings with no unclosed brackets , such as [ ( ) ] , require a prediction of None .
In heatmaps for the bracket prediction task , we expect the last unclosed bracket to receive the highest - magnitude relevance score .
4 White - Box Networks We use two approaches to construct white - box networks for our tasks .
In the counter - based approach , the cell state contains a set of counters , which are incremented or decremented throughout the computation .
The network ‚Äôs Ô¨Ånal output is based on the values of the counters .
In the automaton - based approach , we use the LSTM to simulate an automaton , with the cell state containing a representation of the automaton ‚Äôs state .
We use a counter - based network to solve the counter task and an automaton - based network to solve the bracket prediction task .
We use both kinds of networks to solve the SP task .
All networks perfectly solve the tasks they were designed for .
This section describes our white - box networks at a high level ; a detailed description is given in Appendix A .
In the rest of this paper , we identify the alphabet symbols a , b , c , and dwith the one - hot vectors for indices 1,2,3 , and 4 , respectively .
The vectors f(t),i(t ) , ando(t)represent the forget , input , and output gates , respectively .
g(t)is the value added to the cell state at each time step , and represents
303the sigmoid function .
We assume that the hidden stateh(t)and cell state c(t)are updated as follows .
c(t)=f(t)‚äôc(t 1)+i(t)‚äôg(t ) h(t)=o(t)‚äôtanh ( c(t ) ) 4.1 Counter - Based Networks In the counter - based approach , each position of the cell state contains the value of a counter .
To adjust the counter in position jby some value v2( 1;1 ) , we set g(t )
j = v , and we saturate the gates by setting them to (m)1 , where m‚â´0 is a large constant .
For example , our network for the counting task uses a single hidden unit , with the gates always saturated and with g(t)given by g(t)= tanh ( u [ 1 1 ] x(t ) ) , where u > 0is a hyperparameter that scales the counter by a factor of v= tanh ( u).2When x(t)= a , we have g(t)=v , so the counter is incremented byv .
When x(t)=b , we compute g(t)= v , so the counter is decremented by v. For the SP task , we use seven counters .
The Ô¨Årst four counters record how many occurrences of each symbol have been observed at time step t.
The next three counters record the number of bs , cs , and ds that form one of the four distinguished subsequences with an earlier symbol .
For example , after seeing the input aaabbc , the counterbased network for the SP task satisÔ¨Åes c(6)=v [ 3 2 1 0 2 1 0]‚ä§.
The Ô¨Årst four counters represent the fact that the input has 3 as , 2bs , 1c , and no ds .
Counter # 5 is2vbecause the two bs form a subsequence with theas , and counter # 6 is vbecause the cforms a subsequence with the bs .
The logit scores of our counter - based networks are computed by a linear decoder using the tanh of the counter values .
For the counting task , the score of the True class is h(t ) , while the score of theFalse class is Ô¨Åxed to tanh ( v)=2 .
This means that the network outputs True if and only if the Ô¨Ånal counter value is at least v. For the SP task , the score of the True class is h(t ) 5+h(t ) 6+h(t ) 7 , while the score of the False class is again tanh ( v)=2 . 2We use u= 0:5for the counting task , u= 0:7for the SP task , and m= 50 for both tasks.4.2 Automata - Based Networks We consider two types of automata - based networks : one that implements a Ô¨Ånite - state automaton ( FSA ) for the SP task , and one that implements a pushdown automaton ( PDA ) for the bracket prediction task .
Our FSA construction is similar to Korsky and Berwick ‚Äôs ( 2019 )
FSA construction for simple recurrent networks .
Consider a deterministic FSA Awith states Qand alphabet .
To simulate Ausing an LSTM , we use jQj  jjhidden units , with the following interpretation .
Suppose thatAtransitions to state qafter reading input x(1);x(2 ) ; : : : ; x(t ) .
The hidden state h(t)is a onehot representation of the pair‚ü® q;x(t)‚ü© , which encodes both the current state of Aand the most recent input symbol .
Since the FSA undergoes a state transition with each input symbol , the forget gate always clears c(t ) , so that information written to the cell state does not persist beyond a single time step .
The output layer simply detects whether or not the FSA is in an accepting state .
Details are provided in Appendix A.3 .
Next , we describe how to implement a PDA for the bracket prediction task .
We use a stack containing all unclosed brackets observed in the input string , and make predictions based on the top item of the stack .
We represent a bounded stack of size kusing 2k+ 1hidden units .
The Ô¨Årst k 1positions contain all stack items except the top item , with(represented by the value 1,[represented by 1 , and empty positions represented by 0 .
The kth position contains the top item of the stack .
The nextkpositions contain the height of the stack in unary notation , and the last position contains a bit indicating whether or not the stack is empty .
For example , after reading the input ( [ ( ( ) with a stack of size 4 , the stack contents ( [ ( are represented by c(5)= [ 1 1 0 1 1 1 1 0 0]‚ä§. The 1 in position 4 indicates that the top item of the stack is ( , and the 1, 1 , and 0 in positions 1‚Äì3 indicate that the remainder of the stack is ( [ .
The three 1s in positions 5‚Äì8 indicate that the stack height is 3 , and the 0 in position 9 indicates that the stack is not empty .
When x(t)is(or
[ , it is copied to c(t ) k , and c(t ) k is copied to the highest empty position in c(t ) :
k 1 , pushing the opening bracket to the stack .
The empty stack bit is then set to 0 , marking the stack
304Name Formula Saliency R(c ) t;i(X ) = @^yc @x(t ) i   x(t )
i = Xt;i GI R(c ) t;i(X ) = Xt;i@^yc @x(t ) i   x(t )
i = Xt;i IG R(c ) t;i(X ) = Xt;i‚à´1 0@^yc @x(t ) i   x(t ) i=  X t;id   Table 2 : DeÔ¨Ånitions of the gradient - based methods .
as non - empty .
When the current input symbol is a closing bracket , the highest item of positions 1 through k 1is deleted and copied to position k , popping the top item from the stack .
Because the PDA network is quite complex , we focus here on describing how the top stack item in position kis determined , and leave other details for Appendix A.4 .
Let  ( t)be1ifx(t)=(, 1if x(t)= [ , and 0otherwise .
At each time step , g(t ) k= tanh ( mu(t ) ) ,
where m‚â´0and u(t)= 2k  ( t)+k 1‚àë j=12j 1h(t 1 ) j. ( 1 ) Observe that mu(t)‚â´0when  ( t)= 1 , and mu(t)‚â™0when  ( t)= 1 .
Thus , g(t ) kcontains the stack encoding of the current input symbol if it is an opening bracket .
If the current input symbol is a closing bracket , then  ( t)= 0 , so the sign of u(t)is determined by the highest item of h(t 1 ) : k 1 . 5 Attribution Methods LetXbe a matrix of input vectors , such that the input at time tis the row vector Xt;:= ( x(t))‚ä§. Given X , an LSTM classiÔ¨Åer produces a vector ^yof logit scores .
Based on X,^y , and possibly abaseline input X , an attribution method assigns an attribution score R(c ) t;i(X)to input feature Xt;i for each output class c.
These feature - level scores are then aggregated to produce token - level scores : R(c ) t(X )
= ‚àë iR(c ) t;i(X ) .
Broadly speaking , our Ô¨Åve attribution methods are grouped into three types : one perturbation - based , three gradient - based , and one decompositionbased .
The following subsections describe how each method computes R(c ) t;i(X).5.1 Perturbation- and Gradient - Based Methods Perturbation - based methods are premised on the idea that if Xt;iis an important input feature , then changing the value of Xt;iwould cause ^y to change .
The one perturbation method we consider is occlusion .
In this method , R(c ) t;i(X)is the change in ^ycobserved when Xt;:is replaced by 0 .
Gradient - based methods rely on the same intuition as perturbation - based methods , but use automatic differentiation to simulate inÔ¨Ånitesimal perturbations .
The deÔ¨Ånitions of our three gradientbased methods are given in Table 2 .
The most basic of these is saliency , which simply measures relevance by the derivative of the logit score with respect to each input feature .
G I attempts to improve upon saliency by using the Ô¨Årst - order terms in a Taylor - series approximation of the model instead of the gradients on their own .
IG is designed to address the issue of small gradients found in saturated units by integrating G I along the line connecting Xto a baseline input X , here taken to be the zero matrix .
5.2 Decomposition - Based Methods Decomposition - based methods are methods that satisfy the relation ^yc = R(c ) bias+‚àë t;iR(c ) t;i(X ) , ( 2 ) where R(c ) biasis a relevance score assigned to the bias units of the network .
The interpretation of equation ( 2 ) is that the logit score ^ycis ‚Äú distributed ‚Äù among the input features and the bias units , so that the relevance scores form a ‚Äú decomposition ‚Äù of ^yc .
The one decomposition - based method we consider is LRP , which computes scores using a backpropagation algorithm that distributes scores layer by layer .
The scores of the output layer are initialized to r(c;output )
i = { ^yi ; i = c 0;otherwise .
For each layer lwith activation z(l ) , activation function f(l ) , and output a(l)=f(l ) ( z(l ) ) , the relevance r(c;l)ofa(l)is determined by the following propagation rule : r(c;l ) i=‚àë l‚Ä≤‚àë jr(c;l‚Ä≤ ) jW(l‚Ä≤ l ) j;i a(l ) i z(l‚Ä≤ ) j+ sign ( z(l‚Ä≤ ) j ) " ,
305where l‚Ä≤ranges over all layers to which lhas a forward connection via W(l‚Ä≤ l)and " >
0is a stabilizing constant.3For the LSTM gate interactions , we follow Arras et al .
( 2017b ) in treating multiplicative connections of the form a(l1)‚äôa(l2 ) as activation functions of the form a(l1)‚äôf(l2)( ) , where a(l1)isf(t),i(t ) , oro(t ) .
The Ô¨Ånal attribution scores are given by the values propagated to the input layer : R(c ) t;i(X ) = r(c;inputt ) i .
6 Qualitative Evaluation To evaluate attribution methods under our framework , we begin with a qualitative description of the heatmaps that are computed for our whitebox networks , based on the illustrative sample of heatmaps appearing in Table 3 .
6.1 Counting Task Occlusion , G I , and IG are well - behaved for the counting task .
As expected , these methods assign aa positive value and ba negative value when the output class for attribution is c = True .
When the number of as is different from the number of bs , occlusion assigns a lower - magnitude score to the symbol with fewer instances .
When c = False , all relevance scores are 0 .
This is because ^yFalseis Ô¨Åxed to a constant value supplied by a bias term , so input features can not affect its value .
Saliency and LRP both fail to produce nonzero scores , at least in some cases .
Saliency scores satisfy R(True ) t;1(X )
=  R(True ) t;2(X ) , resulting in token - level scores of 0for all inputs .
Heatmaps # 3 and # 4 show that LRP assigns scores of 0to preÔ¨Åxes containing equal numbers of as and bs .
We will see in Subsection 7.1 that this phenomenon appears to be related to the fact that the LSTM gates are saturated .
6.2 SP Task We obtain radically different heatmaps for the two SP task networks , despite the fact that they produce the same classiÔ¨Åcations for all inputs .
For the counter - based network , all methods except for saliency assign positive scores for c= True to symbols constituting one of the four subsequences , and scores of zero elsewhere .
The saliency heatmaps do not adhere to this pattern , and instead generally assign higher scores 3We use " = 0:001.to tokens occurring near the end of the input .
Heatmaps # 7‚Äì10 show that LRP fails to assign positive scores to the Ô¨Årst symbol of each subsequence , while the other methods generally do not.4 The LRP behavior reÔ¨Çects the fact that the initial adoes not increment the subsequence counters , which determine the Ô¨Ånal logit score .
In contrast , the behavior of occlusion , G I , and IG is explained by the fact that removing either the aor thebdestroys the subsequence .
Note that the as in heatmap # 9 receive scores of 0from occlusion and G I , since removing only one of the two as does not destroy the subsequence .
For the FSA - based network , saliency , G I , and LRP assign only the last symbol a nonzero score when the relevance output class cmatches the network ‚Äôs predicted class .
IG appears to produce erratic heatmaps , exhibiting no immediately obvious pattern .
Although occlusion appears to be erratic at Ô¨Årst glance , its behavior can be explained by the fact that changing x(t)to0causes h(t)to be0 , which the LSTM interprets as the initial state of the FSA ; thus , R(c ) t(X)Ã∏= 0precisely when Xt+1:;:is classiÔ¨Åed differently from X. In all cases , the heatmaps for the FSA - based network diverge signiÔ¨Åcantly from the expected heatmaps .
6.3 Bracket Prediction Task
The heatmaps for the PDA - based network also differ strikingly from those of the other networks , in that the gradient - based methods never assign nonzero scores .
This is because equation ( 1 ) causes g(t)to be highly saturated , resulting in zero gradients .
In the case of LRP , the matching bracket is highlighted when cÃ∏=None .
When the matching bracket is not the last symbol of the input , the other unclosed brackets are also highlighted , with progressively smaller magnitudes , and with brackets of the opposite type from creceiving negative scores .
This pattern reÔ¨Çects the mechanism of ( 1 ) , in which progressively larger powers of 2are used to determine the content copied to c(t ) k.
When the relevance output class is c = None , LRP assigns opening brackets a negative score , revealing the fact that those input symbols set the bit c(t ) 2k+1to indicate that the stack is not empty .
Although occlusion sometimes highlights the matching bracket , it does not appear to be consistent in doing so .
For example , it fails to highlight the matching bracket 4Although
it is difÔ¨Åcult to see , IG assigns a small positive score to the bs in heatmaps # 7 and # 8 .
306Network # c Target Occlusion Saliency G I IG LRP Counting1
True True aaabb aaabb aaabb aaabb aaabb 2
True False bbbaa bbbaa bbbaa bbbaa bbbaa 3 True False aaabbbaaabbbaaabbbaaabbbaaabbb 4 True False aabbb aabbb aabbb aabbb aabbb 5 False True aaabb aaabb aaabb aaabb aaabb 6 False False aabbb aabbb aabbb aabbb aabbb SP ( Counter)7
True True
acb acb
acb acb
acb 8 True True acbb acbb acbb acbb acbb 9 True True aacb aacb aacb aacb aacb 10 True True abcab abcab abcab abcab abcab 11 True False aacc aacc aacc aacc aacc
12 False True
acb acb
acb acb
acb 13 False False aacc aacc aacc aacc aacc SP ( FSA)14
True True
acb acb
acb acb acb 15 True True acbb acbb acbb acbb acbb 16 True True aacb aacb aacb aacb aacb 17 True True abcab abcab abcab abcab abcab 18
True False aacc aacc aacc aacc aacc 19 False True
acb acb
acb acb
acb 20 False False aacc aacc aacc aacc aacc Bracket ( PDA)21 ] ]
( [ [ ( [ ( [ [ ( [ ( [ [ ( [ ( [ [ ( [ ( [ [ ( [ 22 ) )
( [ [ ( [ ] ( [ [ ( [ ] ( [ [ ( [ ] ( [ [ ( [ ] ( [ [ ( [ ] 23 None None ( [ [ ] ] )
( [ [ ] ] ) ( [ [ ] ] ) ( [ [ ] ] ) ( [ [ ] ] ) 24 ] ]
[ ( [ ] [ ( )
[ ( [ ] [ ( )
[ ( [ ] [ ( )
[ ( [ ] [ ( )
[ ( [ ] [ ( ) 25 ) ]
[ ( [ ] [ ( )
[ ( [ ] [ ( )
[ ( [ ] [ ( )
[ ( [ ] [ ( )
[ ( [ ] [ ( ) Table 3 : Selected heatmaps based on R(c ) t(X).Red represents positive values and blue represents negative values .
Heatmaps with all values within the range of 110 5are shown as all 0s .
u v ^yTrue Saliency G I IG 0.6 0.537 0.151 accb accbaccb 0.7 0.604 0.533 accb accbaccb 0.8 0.664 0.581 accb accbaccb 1 0.762 0.642 accb accbaccb 4 0.999 0.761 accb accbaccb 8 1.000 0.762 accb accbaccb 16 1.000 0.762 accb accbaccb 64 1.000 0.762 accb
accbaccb Table 4 : Gradient - based heatmaps of R(True ) t ( accb ) for the counter - based SP network , with 0:6u64 .
in heatmap # 21 , and highlights one other bracket in heatmaps # 23‚Äì24 .
7 Detailed Evaluations We now turn to focused investigations of particular phenomena that attribution methods exhibit when applied to white - box networks .
Subsection 7.1 begins by discussing the effect of network saturation on the gradient - based methods and LRP .
In Subsection 7.2 we apply Bach et al . ‚Äôs ( 2015 ) ablation test to our attribution methods for the SP task .
7.1 Saturation As mentioned in the previous section , network saturation causes gradients to be approximately 0when using sigmoid or tanh activation functions .
To test how attribution methods are affectedm  ( m ) c(t)Accuracy
% Blank 4 0.982  8.7410 390.1 0.2 5 0.993  3.4810 396.1 2.2 6 0.998  1.3210 399.8 6.5 7 0.999  4.9110 4100.0 22.0 8 1.000  1.8110 4100.0 42.1 9 1.000  6.6810 5100.0 69.9 10 1.000  2.4610 5100.0 92.3 11 1.000  9.0510 6100.0 98.7 12 1.000  3.3310 6100.0 99.8 Table 5 : The results of the LRP saturation test , including the value of m , the average value of c(t)when the counter reaches 0 , the network ‚Äôs testing accuracy , and the percentage of examples with blank heatmaps for preÔ¨Åxes with equal numbers of as and bs .
by saturation , Table 4 shows heatmaps for the inputaccb generated by gradient - based methods for different instantiations of the counter - based SP network with varying degrees of saturation .
Recall from Section 4 that counter values for this network are expressed in multiples of the scaling factorv .
We control the saturation of the network via the parameter u= tanh 1(v ) .
For all three gradient - based methods , scores for adecrease and scores for bincrease as uincreases .
Additionally , saliency scores for the Ô¨Årst cdecrease when uincreases .
When u= 8,vis almost completely saturated , causing G I to produce all - zero heatmaps .
307On the other hand , IG is still able to produce nonzero heatmaps even at u= 64 .
Thus , IG is much more resistant to the effects of saturation than G I.
According to Sundararajan et al . ( 2017 ) , gradient - based methods satisfy the axiom of implementation invariance : they produce the same heatmaps for any two networks that compute the same function .
This formal property is seemingly at odds with the diverse array of heatmaps appearing in Table 4 , which are produced for networks that all yield identical classiÔ¨Åers .
In particular , the networks with u= 8,16 , and 64yield qualitatively different heatmaps , despite the fact that the three networks are distinguished only by differences in vof less than 0:001 .
Because the three functions are technically not equal , implementation invariance is not violated in theory ; but the fact that IG produces different heatmaps for three nearly identical networks shows that the intuition described by implementation invariance is not borne out in practice .
Besides the gradient - based methods , LRP is also susceptible to problems arising from saturation .
Recall from heatmaps # 3 and # 4 of Table 3 that for the counting task network , LRP assigns scores of 0to preÔ¨Åxes with equal numbers of as andbs .
We hypothesize that this phenomenon is related to the fact c(t)= 0after reading such preÔ¨Åxes , since the counter has been incremented and decremented in equal amounts .
Accordingly , we test whether this phenomenon can be mitigated by desaturating the gates so that c(t)does not exactly reach 0 .
Recall that the white - box LSTM gates approximate 1(m)using a constant m‚â´0 .
We construct networks with varying values of mand compute LRP scores on a randomly generated testing set of 1000 strings , each of which contains at least one preÔ¨Åx with equal numbers of as and bs .
InTable 5 we report the percentage of examples for which such preÔ¨Åxes receive LRP scores of 0 , along with the network ‚Äôs accuracy on this testing set and the average value of c(t)when the counter reaches 0 .
Indeed , the percentage of preÔ¨Åxes receiving scores of 0increases as the approximation c(t)0becomes more exact .
7.2 Ablation Test So far , we have primarily compared attribution methods via visual inspection of individual examples .
To compare the Ô¨Åve methods quantitatively , Method SP ( Counter ) SP ( FSA ) Occlusion 61.8 12.2 52.611.7 Saliency 97.81.1 96.02.5 GI 65.714.4 96.02.5 IG 47.57.6 94.92.9 LRP 64.312.7 96.02.5 Random 96.12.5 Optimal 42.73.8 Table 6 : Mean and standard deviation results of the ablation test , normalized by string length and expressed as a percentage .
‚Äú Optimal ‚Äù is the best possible score .
we apply the ablation test of Bach et al .
( 2015 ) to our two white - box networks for the SP task.5 Given an input string classiÔ¨Åed as True , we iteratively remove the symbol with the highest relevance score , recomputing heatmaps at each iteration , until the string no longer contains any of the four subsequences .
We apply the ablation test to 100 randomly generated input strings , and report the average percentage of each string that is ablated in Table 6 .
A peculiar property of the SP task is that removing a symbol preserves the validity of input strings .
This means that , unlike in NLP settings , our ablation test does not suffer from the issue that ablation produces invalid inputs .
Saliency , G I , and LRP perform close to the random baseline on the FSA network ; this is unsurprising , since these methods only assign nonzero scores to the last input symbol .
While Table 3 shows some variation in the IG heatmaps , IG also performs close to the random baseline .
Only occlusion performs considerably better , since it is able to identify symbols whose ablation would destroy subsequences .
On the counter - based SP network , IG performs remarkably close to the optimal benchmark , which represents the best possible performance on this task .
Occlusion , G I , and LRP achieve a similar level of performance to one another , while saliency performs worse than the random baseline .
8 Conclusion Of all the heatmaps considered in this paper , only those computed by G I and IG for the counting task fully matched our expectations .
In other cases , all attribution methods fail to identify at least some of the input features that should be considered relevant , or assign relevance to input features that do 5We do not consider the counting task because its heatmaps are already easy to understand , and we do not consider the PDA network because the gradient - based methods fail to produce nonzero heatmaps for that network .
308not affect the model ‚Äôs behavior .
Among the Ô¨Åve methods , saliency achieves the worst performance : it never assigns nonzero scores for the counting and bracket prediction tasks , and it does not identify the relevant symbols for either of the two SP networks .
Saliency also achieves the worst performance on the ablation test for both the counterbased and the FSA - based SP networks .
Among the four white - box networks , the two automatabased networks proved to be much more challenging for the attribution methods than the counterbased networks .
While the LRP heatmaps for the PDA network correctly identify the matching bracket when available , no other method produces reasonable heatmaps for the PDA network , and all Ô¨Åve methods fail to interpret the FSA network .
Taken together , our results suggest that attribution heatmaps should be viewed with skepticism .
This paper has identiÔ¨Åed cases in which heatmaps fail to highlight relevant features , as well as cases in which heatmaps incorrectly highlight irrelevant features .
Although most of the methods perform better for the counter - based networks than the automaton - based networks , in practical settings we do not know what kinds of computations are implemented by a trained network , making it impossible to determine whether the network under analysis is compatible with the attribution method being used .
In future work , we encourage the use of our four white - box models as qualitative benchmarks for evaluating interpretability methods .
For example , the style of evaluation we have developed can be replicated for attribution methods not covered in this paper , including DeepLIFT ( Shrikumar et al . , 2017 ) and contextual decomposition ( Murdoch et al . ,2018 ) .
We believe that insights gleaned from white - box analysis can help researchers choose between different attribution methods and identify areas of improvement in current techniques .
Acknowledgments I would like to thank Dana Angluin and Robert Frank for their advice and mentorship on this project .
I would also like to thank Yoav Goldberg , John Lafferty , Tal Linzen , R. Thomas McCoy , Aaron Mueller , Karl Mulligan , Shauli Ravfogel , Jason Shaw , and the reviewers for their helpful feedback and discussion .
References Julius Adebayo , Justin Gilmer , Michael Muelly , Ian Goodfellow , Moritz Hardt , and Been Kim . 2018 .
Sanity Checks for Saliency Maps .
In Advances in Neural Information Processing Systems 31 , volume 31 , pages 9505‚Äì9515 , Montreal , Canada .
Curran Associates , Inc.
Marco Ancona , Enea Ceolini , Cengiz ¬®Oztireli , and Markus Gross . 2018 .
Towards better understanding of gradient - based attribution methods for Deep Neural Networks .
InICLR 2018
Conference Track , Vancouver , Canada .
OpenReview .
Leila Arras , Franziska Horn , Gr ¬¥ egoire Montavon , Klaus - Robert M ¬®uller , and Wojciech Samek .
2017a .
‚Äú What is relevant in a text document ? ‚Äù : An interpretable machine learning approach .PLOS
ONE , 12(8):e0181142 .
Leila Arras , Gr ¬¥ egoire Montavon , Klaus - Robert M ¬®uller , and Wojciech Samek .
2017b .
Explaining Recurrent Neural Network Predictions in Sentiment Analysis .
InProceedings of the 8th Workshop on Computational Approaches to Subjectivity , Sentiment and Social Media Analysis , pages 159‚Äì168 , Copenhagen , Denmark .
Association for Computational Linguistics .
Leila Arras , Ahmed Osman , Klaus - Robert M ¬®uller , and Wojciech Samek .
2019 .
Evaluating Recurrent Neural Network Explanations .
In Proceedings of the 2019 ACL Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 113 ‚Äì 126 , Florence , Italy .
Association for Computational Linguistics .
Enes Avcu , Chihiro Shibata , and Jeffrey Heinz .
2017 .
Subregular Complexity and Deep Learning .
In Proceedings of the Conference on Logic and Machine Learning in Natural Language ( LaML 2017 ) , Gothenburg , 12‚Äì13 June 2017 , volume 1 of CLASP Papers in Computational Linguistics , pages 20‚Äì33 , Gothenburg , Sweden .
Centre for Linguistic Theory and Studies in Probability ( CLASP ) , University of Gothenburg .
Sebastian Bach , Alexander Binder , Gr ¬¥ egoire Montavon , Frederick Klauschen , Klaus - Robert M ¬®uller , and Wojciech Samek .
2015 .
On Pixel - Wise Explanations for Non - Linear ClassiÔ¨Åer Decisions by Layer - Wise Relevance Propagation .PLOS
ONE , 10(7):e0130140 .
Jean - Philippe Bernardy .
2018 .
Can Recurrent Neural Networks Learn Nested Recursion ?
Linguistic Issues in Language Technology , 16(1):1‚Äì20 .
N. Chomsky and M. P. Sch ¬®utzenberger .
1959 .
The Algebraic Theory of Context - Free Languages .
In P. Braffort and D. Hirschberg , editors , Studies in Logic and the Foundations of Mathematics , volume 26 of Computer Programming and Formal Systems , pages 118‚Äì161 .
North - Holland Publishing Company , Amsterdam , Netherlands .
309Jeffrey L. Elman .
1990 .
Finding Structure in Time .
Cognitive Science , 14(2):179‚Äì211 .
Patrick C. Fischer .
1966 .
Turing Machines with Restricted Memory Access .Information and Control , 9(4):364‚Äì379 .
Patrick C. Fischer , Albert R. Meyer , and Arnold L. Rosenberg .
1968 .
Counter Machines and Counter Languages .Mathematical systems theory , 2(3):265 ‚Äì 283 .
Amirata Ghorbani , Abubakar Abid , and James Zou .
2019 .
Interpretation of Neural Networks Is Fragile .
Proceedings of the AAAI Conference on ArtiÔ¨Åcial Intelligence , 33(01):3681‚Äì3688 .
Jeffrey Nicholas Heinz .
2007 .
Inductive Learning of Phonotactic Patterns .
PhD Dissertation , University of California , Los Angeles , Los Angeles , CA , USA .
Been Kim , Martin Wattenberg , Justin Gilmer , Carrie Cai , James Wexler , Fernanda Viegas , and Rory Sayres .
2018 .
Interpretability Beyond Feature Attribution : Quantitative Testing with Concept Activation Vectors ( TCA V ) .
In International Conference on Machine Learning , 10‚Äì15 July 2018 , Stockholmsm ¬®assan , Stockholm Sweden , volume 80 of Proceedings of Machine Learning Research , pages 2668‚Äì2677 , Stockholm , Sweden .
PMLR .
Pieter - Jan Kindermans , Sara Hooker , Julius Adebayo , Maximilian Alber , Kristof T. Sch ¬®utt , Sven D¬®ahne , Dumitru Erhan , and Been Kim .
2019 .
The ( Un)reliability of Saliency Methods .
In Wojciech Samek , Gr ¬¥ egoire Montavon , Andrea Vedaldi , Lars Kai Hansen , and Klaus - Robert M ¬®uller , editors , Explainable AI : Interpreting , Explaining and Visualizing Deep Learning , number 11700 in Lecture Notes in Computer Science , pages 267‚Äì280 .
Springer International Publishing , Cham , Switzerland .
Samuel A. Korsky and Robert C. Berwick .
2019 .
On the Computational Power of RNNs .Computing Research Repository , arXiv:1906.06349 .
Jiwei Li , Xinlei Chen , Eduard Hovy , and Dan Jurafsky . 2016 .
Visualizing and Understanding Neural Models in NLP .
In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 681‚Äì691 , San Diego , CA , USA .
Association for Computational Linguistics .
Abhijit Mahalunkar and John Kelleher .
2019a .
MultiElement Long Distance Dependencies : Using SP k Languages to Explore the Characteristics of LongDistance Dependencies .
InProceedings of the Workshop on Deep Learning and Formal Languages : Building Bridges , pages 34‚Äì43 , Florence , Italy .
Association for Computational Linguistics .
Abhijit Mahalunkar and John D. Kelleher .
2018 .
Using Regular Languages to Explore the Representational Capacity of Recurrent Neural Architectures .InArtiÔ¨Åcial
Neural Networks and Machine Learning ‚Äì ICANN 2018 , volume 11141 of Lecture Notes in Computer Science , pages 189‚Äì198 , Rhodes , Greece .
Springer International Publishing .
Abhijit Mahalunkar and John D. Kelleher .
2019b .
Understanding Recurrent Neural Architectures by Analyzing and Synthesizing Long Distance Dependencies in Benchmark Sequential Datasets .Computing Research Repository , arXiv:1810.02966v3 .
William Merrill .
2019 .
Sequential Neural Networks as Automata .
InProceedings of the Workshop on Deep Learning and Formal Languages : Building Bridges , pages 1‚Äì13 , Florence , Italy .
Association for Computational Linguistics . Gr¬¥egoire Montavon .
2019 .
Gradient - Based Vs .
Propagation - Based Explanations : An Axiomatic Comparison .
In Wojciech Samek , Gr ¬¥ egoire Montavon , Andrea Vedaldi , Lars Kai Hansen , and KlausRobert M ¬®uller , editors , Explainable AI : Interpreting , Explaining and Visualizing Deep Learning , number 11700 in Lecture Notes in Computer Science , pages 253‚Äì265 .
Springer International Publishing , Cham , Switzerland . Gr¬¥egoire Montavon , Wojciech Samek , and KlausRobert M ¬®uller .
2018 .
Methods for interpreting and understanding deep neural networks .Digital
Signal Processing , 73:1‚Äì15 .
W. James Murdoch , Peter J. Liu , and Bin Yu .
2018 .
Beyond Word Importance : Contextual Decomposition to Extract Interactions from LSTMs .
In ICLR 2018 Conference Track , Vancouver , Canada . OpenReview .
Nina Poerner , Hinrich Sch ¬®utze , and Benjamin Roth .
2018 .
Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement .
InProceedings of the 56th Annual Meeting of the Association for Computational Linguistics , volume 1 : Long Papers , pages 340‚Äì350 , Melbourne , Australia .
Association for Computational Linguistics .
Wojciech Samek , Alexander Binder , Gr ¬¥ egoire Montavon , Sebastian Lapuschkin , and Klaus - Robert M¬®uller . 2017 .
Evaluating the Visualization of What a Deep Neural Network Has Learned .IEEE Transactions on Neural Networks and Learning Systems , 28(11):2660‚Äì2673 .
Luzi Sennhauser and Robert Berwick .
2018 .
Evaluating the Ability of LSTMs to Learn Context - Free Grammars .
In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 115‚Äì124 , Brussels , Belgium .
Association for Computational Linguistics .
Avanti Shrikumar , Peyton Greenside , Anna Shcherbina , and Anshul Kundaje . 2017 .
Not Just a Black Box : Learning Important Features Through Propagating
310Activation Differences .Computing Research Repository , arXiv:1605.01713 .
Karen Simonyan , Andrea Vedaldi , and Andrew Zisserman .
2014 .
Deep Inside Convolutional Networks : Visualising Image ClassiÔ¨Åcation Models and Saliency Maps .
In ICLR 2014 Workshop Proceedings , Banff , Canada . arXiv .
Natalia Skachkova , Thomas Trost , and Dietrich Klakow .
2018 .
Closing Brackets with Recurrent Neural Networks .
In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 232‚Äì239 , Brussels , Belgium .
Association for Computational Linguistics .
Mukund Sundararajan , Ankur Taly , and Qiqi Yan . 2017 .
Axiomatic Attribution for Deep Networks .
In Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 3319‚Äì3328 ,
Sydney , Australia . PMLR .
Gail Weiss , Yoav Goldberg , and Eran Yahav .
2018 .
On the Practical Computational Power of Finite Precision RNNs for Language Recognition .
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , volume 2 : Short Papers , pages 740‚Äì745 , Melbourne , Australia .
Association for Computational Linguistics .
Mengjiao Yang and Been Kim . 2019 .
Benchmarking Attribution Methods with Relative Feature Importance .Computing Research Repository , arXiv:1907.09701 .
Xiang Yu , Ngoc Thang Vu , and Jonas Kuhn . 2019 .
Learning the Dyck Language with Attention - based Seq2Seq Models .
In Proceedings of the 2019 ACL Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 138‚Äì146 , Florence , Italy .
Association for Computational Linguistics .
Matthew D. Zeiler and Rob Fergus .
2014 .
Visualizing and Understanding Convolutional Networks .
In Computer Vision ‚Äì ECCV 2014 , volume 8689 of Lecture Notes in Computer Science , pages 818‚Äì833 , Zurich , Switzerland .
Springer International Publishing .
Jianming Zhang , Sarah Adel Bargal , Zhe Lin , Jonathan Brandt , Xiaohui Shen , and Stan Sclaroff .
2018 .
Top - Down Neural Attention by Excitation Backprop .
International Journal of Computer Vision , 126(10):1084‚Äì1102 .
A Detailed Descriptions of White - Box Networks This appendix provides detailed descriptions of our four white - box networks .
A.1 Counting Task Network As described in Subsection 4.1 , the network for the counting task simply sets g(t)tov= tanh ( u ) when x(t)=aand vwhen x(t)=b .
All gates are Ô¨Åxed to 1 .
The output layer uses h(t)= tanh ( c(t ) ) as the score for the True class and v=2 as the score for the False class .
g(t)= tanh ( u [ 1 1 ] x(t ) ) f(t)=(m ) i(t)=(m ) o(t)=(m ) ^y(t)=[1 0 ] h(t)+[0 v=2 ] A.2 SP Task Network ( Counter - Based )
The seven counters for the SP task are implemented as follows .
First , we compute g(t)under the assumption that one of the Ô¨Årst four counters is always incremented , and one of the last three counters is always incremented as long as x(t)Ã∏=a .
g(t)= tanh0 BB@u2 664I4 0 1 0 0 0 0 1 0 0 0 0 13 775x(t)1 CCA
Then , we use the input gate to condition the last three counters on the value of the Ô¨Årst four counters .
For example , if h(t 1 ) 1 = 0 , then no as have been encountered in the input string before time t.
In that case , the input gate for counter # 5 , which represents subsequences ending with b , is set to i(t ) 5=( m)0 .
This is because a bencountered at time twould not form part of a subsequence if no as have been encountered so far , so counter # 5 should not be incremented .
i(t)=0
BB@2m2 6640 0 1 0 0 0 0 1 0 1 0 0 1 003 775h(t 1 ) + m [ 1 1 1 1  1 1 1]‚ä§ )
All other gates are Ô¨Åxed to 1 .
The output layer sets the score of the True class to h(t ) 5+h(t ) 6+h(t ) 7and the score of the False class to v=2 .
f(t)=(m1 ) o(t)=(m1 ) ^y(t)=[01 1 1 00 0 0 ] h(t)+[0 v=2 ]
311A.3 FSA Network Here we describe a general construction of an LSTM simulating an FSA with states Q , accepting states QFQ , alphabet  , and transition function:Q!Q. Recall that h(t)contains a one - hot representation of pairs in Qencoding the current state of the FSA and the most recent input symbol .
The initial state h(0)=0represents the starting conÔ¨Åguration of the FSA .
At a high level , the state transition system works as follows .
First , g(t)Ô¨Årst marks all the positions corresponding to the current input x(t).6 g(t ) ‚ü®q;x‚ü©= { v ; x = x(t ) 0;otherwise
The input gate then Ô¨Ålters out any positions that do not represent valid transitions from the previous state q‚Ä≤ , which is recovered from h(t 1 ) .
i(t ) ‚ü®q;x‚ü©= { 1 ; (q‚Ä≤ ; x ) = q 0;otherwise
Now , we describe how this behavior is implemented in our LSTM .
The cell state update is straightforwardly implemented as follows :
g(t)= tanh ( uW(c;x)x(t ) ) , where W(c;x ) ‚ü®q;x‚ü©;j= { 1 ; j is the index for x 0;otherwise .
Observe that the matrix W(c;x)essentially contains a copy of I4for each state , such that each copy is distributed across the different cell state units designated for that state .
The input gate is more complex .
First , the bias term handles the case where the current case is the starting state q0 .
This is necessary because the initial conÔ¨Åguration of the network is represented by h(0)=0 .
b(i ) ‚ü®q;x‚ü©= { m ;  ( q0 ; x ) = q  m ; otherwise The bias vector sets i(t ) ‚ü®q;x‚ü©to be 1if the FSA transitions from q0toqafter reading x , and 0otherwise .
We replicate this behavior for other values 6We use v= tanh(1 ) 0:762.ofh(t 1)by using the weight matrix W(i;h ) , taking the bias vector into account : i(t)= ( W(i;h)h(t 1)+b(i ) ) , where W(i ) ‚ü®q;x‚ü©;‚ü®q‚Ä≤;x‚Ä≤‚ü©= { m b(i ) ‚ü®q;x‚ü© ;  ( q‚Ä≤ ; x )
= q  m b(i ) ‚ü®q;x‚ü©;otherwise .
The forget gate is Ô¨Åxed to  1 , since the state needs to be updated at every time step .
The output gate is Ô¨Åxed to 1 . f(t)=( m1 ) o(t)=(m1 )
The output layer simply selects hidden units that represent accepting and rejecting states : ^y(t)=W h(t ) , where Wc;‚ü®q;x‚ü©=8 > < > :1 ; c = True andq2QF 1 ; c = False andq = 2QF 0;otherwise .
A.4 PDA Network
Finally , we describe how the PDA network for the bracket prediction task is implemented .
Of the four networks , this one is the most intricate .
Recall from Subsection 4.2 that we implement a bounded stack of size kusing 2k+ 1hidden units , with the following interpretation : c(t ) : k 1contains the stack , except for the top item c(t ) kcontains the top item of the stack c(t ) k+1:2 kcontains the height of the stack in unary notation c2k+1is a bit , which is set to be positive if the stack is empty and nonpositive otherwise .
We represent the brackets ( , [ , ) , and ] in onehot encoding with the indices 1,2,3 , and 4 , respectively .
The opening brackets ( and[are represented on the stack by 1and 1 , respectively .
T
312We begin by describing g(t ) .
Due to the complexity of the network , we describe the weights and biases individually , which are combined as follows .
g(t)= tanh ( m ( z(g;t ) ) ) , where z(g;t)=W(c;x)x(t)+W(c;h)h(t 1)+b(c )
First , the bias vector sets c(t ) 2k+1to be 1 , indicating that the stack is empty .
This ensures that the initial hidden state h(t)=0is treated as an empty stack .
b(c)=[0
2 ] W(c;x)serves three functions when x(t)is an open bracket , and does nothing when x(t)is a closing bracket .
First , it pushes x(t)to the top of the stack , represented by c(t ) k.
The values 2kare determined by equation ( 1 ) in Subsection 4.2 .
Second , it sets g(t ) k+1:2 kto1 in order to increment the unary counter for the height of the stack .
Later , we will see that the input gate Ô¨Ålters out all positions except for the top of the stack .
Finally , W(c;x)sets the empty stack indicator to  1 , indicating that the stack is not empty .
W(c;x)=2 6640 0 0 0 2k 2k0 0 1 1 0 0  2 2 0 03 775 W(c;h)performs two functions .
First , it completes equation ( 1 ) for c(t ) k , setting it to be the secondhighest stack item from the previous time step .
Second , it copies the top of the stack to the Ô¨Årst k 1positions , with the input gate Ô¨Åltering out all but the highest position .
W(c;h)=2 6640 1 0 0 2 4  2k 100 0 0 0 0 0 0 0 103 775 Finally , the  1s serve to decrease the empty stack indicator by an amount proportional to the stack height at time t 1 .
Observe that if x(t)is a closing bracket and h(t 1)represents a stack with only one item , then W(c;x ) 2k+1;:x(t)+W(c;h ) 2k+1;:h(t 1)+b(c ) 2k+1 =  1 + 2 = 1 , so the empty stack indicator is set to 1 , indicating that the stack is empty .
Otherwise , W(c;x ) 2k+1;:x(t)+W(c;h ) 2k+1;:h(t 1)  2 , so the empty stack indicator is nonpositive .
Now , we describe the input gate , given by the following .
i(t)= ( m ( z(i;t ) ) )
z(i;t)=W(i;x)x(t)+W(i;h)h(t 1)+b(i ) W(i;x)sets the input gate for the Ô¨Årst k 1positions to 0when x(t)is a closing bracket .
In that case , an item needs to be popped from the stack , so nothing can be copied to these hidden units .
When x(t)is an opening bracket , W(i;x)setsi(t ) k= 1 , so that the bracket can be copied to the top of the stack .
W(i;x)= 22
40 0  1 1 1 1 0 0 03 5 W(i;h)uses a matrix Tn2Rnn , deÔ¨Åned below .
Tn=2 6666641 1 0 : : : 0 0 0 1  1 : : : 0 0 .................. 0 0 0 : : : 1 1 0 0 0 : : : 0 13 777775 Suppose vrepresents the number sin unary notation : vjis1ifjsand0otherwise .
Tnhas the special property that Tnvis a one - hot vector for s. Based on this , W(i;h)is deÔ¨Åned as follows .
W(i;h)= 22 6640(Tk):k 1 ; : 0 ( Tk):k 1 ; : 003 775 W(i;h ) : k 1;k+1:2 kcontains Tk , with the last row truncated .
This portion of the matrix converts h(t 1 ) k+1:2 k , which contains a unary encoding of the stack height , to a one - hot vector marking the position of the top of the stack .
This ensures that , when pushing to the stack , the top stack item from time t 1is only copied to the appropriate position of h(t ) : k 1 .
The other copy of Tk , again with the last row omitted , occurs in W(i;h )
k+2:2 k;k+1:2 k.
This copy ofTkensures that when the unary counter for the
313stack height is incremented , only the appropriate position is updated .
Finally , the bias vector ensures that the top stack item and the empty stack indicator are always updated .
b(i)=2 664 1 1  1 13 775
The forget gate is responsible for deleting portions of memory when stack items are popped .
f(t)= ( m ( z(f;t ) ) )
z(f;t)=W(f;x)x(t)+W(f;h)h(t 1)+b(f )
W(f;x)Ô¨Årst ensures that no stack items are deleted when an item is pushed to the stack .
W(f;x)=
22 6641 1 0 0 0 0 0 0 1 1 0 0 0 0 0 03 775 Next , W(f;h)marks the second highest stack position and the top of the unary counter for deletion , in case an item needs to be popped .
W(f;h)= 22 6640 (Tk)2 : ; : 0  Tk 003 775
Finally , the bias term ensures that the top stack item and empty stack indicator are always cleared .
b(i)=2 6641  1 1  13 775 To complete the construction , we Ô¨Åx the output gate to 1 , and have the output layer read the top stack position : o(t)=(m1 ) ^y(t)=W h(t ) , where Wc;j=8 > > > > < > > > > :1 ; c =) andj = k  1 ; c =] andj = k 1 ; c
= None andj= 2k+ 1 0 ; otherwise .
