WikiGraphs : A Wikipedia Text - Knowledge Graph Paired Dataset
Luyu Wang * and Yujia Li * and Ozlem Aslan and Oriol Vinyals * Equal contribution DeepMind , London , UK { luyuwang,yujiali,ozlema,vinyals}@google.com
Abstract
We present a new dataset of Wikipedia articles each paired with a knowledge graph , to facilitate the research in conditional text generation , graph generation and graph representation learning .
Existing graph - text paired datasets typically contain small graphs and short text ( 1 or few sentences ) , thus limiting the capabilities of the models that can be learned on the data .
Our new dataset WikiGraphs is collected by pairing each Wikipedia article from the established WikiText-103 benchmark ( Merity et al , 2016 ) with a subgraph from the Freebase knowledge graph ( Bollacker et al , 2008 ) .
This makes it easy to benchmark against other state - of - the - art text generative models that are capable of generating long paragraphs of coherent text .
Both the graphs and the text data are of signiﬁcantly larger scale compared to prior graph - text paired datasets .
We present baseline graph neural network and transformer model results on our dataset for 3 tasks : graph → text generation , graph → text retrieval and text → graph retrieval .
We show that better conditioning on the graph provides gains in generation and retrieval quality but there is still large room for improvement .
1
1
Introduction
Parallel datasets that pair data from different sources and modalities have enabled large amounts of research on cross modality learning .
Paired image - caption datasets enable models to describe visual scenes in natural language ( Lin et al , 2014 ; Vinyals et al , 2016 ) , paired streams of speech and transcription data makes it possible to train speech recognition systems ( Garofolo et al , 1993 ;
Panayotov et al , 2015 ) or text - to - speech synthesis models ( Oord et al , 2016 ) , and parallel corpus of text in different languages enable learned machine translation models ( Barrault et al , 2020 ) .
1The data and the code to reproduce our baseline results are available at https://github.com/deepmind/ deepmind - research / tree / master / wikigraphs
Figure 1 : Illustration of a pair of Wikipedia article and the corresponding knowledge graph in our dataset .
We present a new dataset of Wikipedia text articles each paired with a relevant knowledge graph ( KG ) , which enables building models that can generate long text conditioned on a graph structured overview of relevant topics , and also models that extract or generate graphs from a text description .
There has been many prior efforts trying to build datasets for learning graph → text generation models ( Jin et al , 2020 ; Gardent et al , 2017 ; Lebret et al , 2016 ) .
However , existing graph - text paired datasets are mostly small scale , where the graphs tend to have 10 - 20 or even less nodes , and the text typically only contains one or a few sentences .
This represents a signiﬁcant contrast with the state - ofthe - art text generation models ( Dai et al , 2019 ; Brown et al , 2020 ) , which can already generate very ﬂuent and long text that spans thousands of tokens over multiple paragraphs .
We attempt to bridge this gap , with the goal of advancing the state - of - the - art graph → text generation models , graph representation learning models and also text - conditioned graph generative models .
Each text document in our dataset is a full - length Wikipedia article , and we pair each of them with a KG that are signiﬁcantly bigger than prior datasets of similar nature and includes much richer information .
Hand labelling text articles with KGs is expensive and not scalable ( Lebret et al , 2016 ) ,
ProceedingsoftheFifteenthWorkshoponGraph - BasedMethodsforNaturalLanguageProcessing(TextGraphs-15),pages67–82June11,2021. © 2021AssociationforComputationalLinguistics67“Where the Streets Have No Name ” is a song by Irish rock band U2 .
It is the opening track from their 1987 album The Joshua Tree and was released as the album ’s third single in August 1987 .
The song ’s hook is a repeating guitar arpeggio using a delay effect , played during the song ’s introduction and again at the end .  
Lead vocalist Bono wrote ... Freebasens / m.06t2j0ns / m.01vswwxmusic.composer.compositionsns / m.01vswx5music.composer.compositionsns / m.074ftns / music.composition.formSongkey / wikipedia.enBonokey / wikipedia.enPaul
Hewsontype.object.nameWhere the streets have no namekey / wikipedia.enpeople.person.date of birth1960 - 05 - 10The Edgekey / wikipedia.en1.77mpeople.person.date of birth1961-08-08people.person.height metersWikiText-103ns / music.composition.languageDavid Howell Evans , better known by his stage name The Edge , is a British - born Irish musician , songwriter ... ns / common.topic.descriptionns / m.02h40lc  therefore we utilize an existing and established knowledge base , Freebase ( Bollacker et al , 2008 ) , and designed an automated process to extract a relevant subgraph from it for each Wikipedia article .
To make the text generation results on our dataset directly comparable to the state - of - the - art , we chose the set of Wikipedia articles from the established language modeling benchmark WikiText103 ( Merity et al , 2016 ) , which contains a subset of high - quality Wikipedia articles .
This gives us a dataset of 23,522 graph - text pairs in total , covering 82.3 % of Wikitext-103 articles .
On average each graph has 38.7 nodes and 48.3 edges , and each text article contains 3,533.8 tokens .
In addition to structural information , our graphs also contain rich text information with an average of 895.1 tokens in each graph .
Furthermore , the automatic process we used to create this dataset can be extended to pair any Wikipedia document with Freebase , and can be scaled up to create over 3 M graph - text pairs .
Out of many exciting new tasks that this dataset enables , we present 3 possibilities : graph → text generation , graph → text retrieval , and text → graph retrieval .
We benchmarked a few baseline models on these tasks .
The models we considered were based on the recent Transformer - XL ( Dai et al , 2019 ) model , and we adapted it to condition the text generation on the KG in different ways .
Our results show that better conditioning on the graph indeed improves the relevance of the generated text and the retrieval quality .
However , there is still signiﬁcant room for improvement on these tasks , which makes this an exciting dataset for research .
Our data and code for baseline models will be made publicly available .
2 Related work
Graph - text paired data There has been a lot of prior work on creating graph - text paired datasets .
Example applications include generating text summaries conditioned on Abstract Meaning Representation graphs ( Liu et al , 2018 ) , generating the abstract of a scientiﬁc article given a KG and title ( Koncel - Kedziorski et al , 2019 ) and generating text from RDF triples ( Gardent et al , 2017 ; Jin et al , 2020 ) .
In the following we will mostly review related work on KG - text paired datasets .
Annotating KG or text to create paired datasets is expensive , as a good quality annotation requires annotators that understand the content and structure of the text and the corresponding KG ( Jin et al ,
Dataset # examples # triples # tokens # vocab 15.26 WebNLG 13,036 1,484 1.3 M 21.46 476,341 GenWiki 3,533.8 238,071 23,522 Ours
2.54 1.95 48.3
Table 1 : Our dataset contains signiﬁcantly larger graphs ( average # triples per graph ) and longer text ( average # tokens per text ) than previous KG - text datasets .
2020 )
.
Therefore previous KG - text paired datasets that rely on human annotation have limited scale .
Among these , Gardent et al ( 2017 ) crowdsourced human annotators to verbalize RDF triplets taken from DBpedia ( Auer et al , 2007 ) to a few sentences ( WebNLG ) and this caused errors in annotation that were ﬁxed with a few updates through years .
Parikh et al ( 2020 ) paired Wikipedia Table with one sentence text that is created by annotators that revise Wikipedia text .
Another line of research focuses on eliminating the need of human annotations by automatically matching KG - text pairs or generating KGs from text using existing tools .
Lebret et al ( 2016 ) automatically matched Wikipedia infobox of biographies with their ﬁrst sentence .
Koncel - Kedziorski et al ( 2019 ) utilized an earlier information extraction system that extracts entities , co - reference and relations from given text to build KG ’s .
The GenWiki dataset ( Jin et al , 2020 ) is automatically constructed by querying KGs in DBpedia with the title of articles in Wikipedia followed by ﬁltering and entity annotation .
We construct our WikiGraphs dataset by extracting a subgraph from Freebase ( Bollacker et al , 2008 ) for each Wikipedia article following a scalable automatic process .
Compared to previous work , our WikiGraphs dataset contains signiﬁcantly larger graphs and longer text ( Table 1 ) .
Models for graph - text paired data Recent state of art language models are based on the Transformer architecture ( Vaswani et al , 2017 ) that uses the self attention mechanism .
The TransformerXL ( Dai et al , 2019 ) model further introduces a segment level recurrence with a novel positional encoding resulting in impressive performance in long sequences by capturing dependencies beyond a ﬁxed length window .
Graph neural networks ( GNNs ) ( Battaglia et al , 2018 ; Gilmer et al , 2017 ) learn representations for graph structured data through a message passing process .
This class of models naturally exploit
68  Test 43
Num . pairs
Valid 48
Train 23,431
All 23,522 % of WikiText-103 82.3 % 80.0 % 71.7 % 82.3 % 38.7 48.3 2.5 895.1 21.1 M 31,090 3,533.8 83.1 M 238,071
40.6 49.5 2.4 1,010.1 43,435 3,531.7 3,644.2 4,564.7 82.8 M 174,923 196,280 Nodes per graph Edges per graph Avg .
Node degree Tokens per graph Total graph tokens Graph vocab size Tokens per article Total text tokens Text vocab size
38.7 35.4 48.3 42.8 2.5 2.4 807.7 895.1 21.0 M 38,771
Table 2 : Basic statistics about our WikiGraphs dataset .
the graph structures , making them a good ﬁt for graph data .
GNNs have been used in many applications on KG ’s ( Kipf and Welling , 2016 ; Wang et al , 2019 ; Xu et
al , 2019 ) .
Fundamentally , transformers can also be understood as a special type of GNNs with a fully - connected graph structure .
The most recent prior work on graph - to - text generation follows an encoder - decoder architecture ( Koncel - Kedziorski et al , 2019 ; Jin et al , 2020 ) , where the graph part is encoded with a GNN model , e.g. Graph Attention Network ( GAT ) ( Veliˇckovi´c et al , 2018 ) .
The text part is typically modeled using an attention based decoder with a copy mechanism ( e.g. BiLSTMs as in ( Jin et al , 2020 ) ) to process input from both the KG and text .
The models we benchmarked for graph - to - text generation were based on the Transformer - XL architecture and conditioned on the graph through a GNN , making full use of the graph structure and capable of generating very long text comparable to the state - of - the - art .
3 Dataset
In this section we ﬁrst present some properties of our dataset , and then describe the process that we used to create it .
3.1 Properties of the data
3.1.1 Scale of the data
Basic statistics about our WikiGraphs dataset are listed in Table 2 .
An illustration of a graph - text pair is shown in Figure 1 .
A few actual examples from our dataset are included in the Appendix ( Figure 7 , 8) .
All of the articles come from the WikiText-103 dataset ( Merity et al , 2016 ) , which contains highquality articles that ﬁt the Good or Featured criteria speciﬁed by the Wikipedia editors when the data was collected .
Merity et al ( 2016 ) have already cleaned up and tokenized the articles , therefore
they appear as plain text without any markup tags .
As will be described in Section 3.2 , we try to pair each article with a subgraph from Freebase , centered at the entity node that has a Wikipedia link to the title of the article .
We are not able to match every article to an entity in Freebase , but through this process we retained a signiﬁcant portion of 82.3 % of the WikiText-103 articles .
We kept the original train / valid / test split .
As we will see in Section 4.2 , training models on this set gives us results that are very close to training on the full WikiText-103 dataset when evaluated on our test set .
Therefore the text part of WikiGraphs appears to be sufﬁcient to reproduce and benchmark against the state - of - the - art text generative models .
Figure 2 shows the distribution of graph sizes and article lengths across our dataset .
All the distributions are skewed with a long tail .
Notably , average graph size in our dataset is 38.7 nodes and 48.3 edges , considerably larger than the graphs in previous datasets ( Jin et al , 2020 ; Gardent et al , 2017 ) .
Also the length of the text articles averages to 3,533.8 tokens and can go up to 26,994 tokens , which is orders of magnitudes longer than the text data in previous graph - text paired datasets that typically only contains a single or few sentences ( Jin et al , 2020 ; Gardent et al , 2017 ; Lebret et al , 2016 ) .
3.1.2 Nodes and edges
The graphs in our dataset contains two types of nodes : entities and string literals .
Each entity is labeled by a unique Freebase entity ID , e.g. ns / m.0f9q9z , and each string literal contains some natural language text , that could be for example a name , date , or description of an entity .
Each edge in the graphs also has an associated edge label , e.g. ns / common.topic.description , indicating which type of edge it is .
There are a total of 522 different edge types in our dataset .
Figure 3 shows the frequency of all the different edge types in our dataset .
Every graph always has one entity node ( we call it “ center node ” ) that has a link to the paired Wikipedia article , through a special edge key / wikipedia.en , and the whole graph is a 1 - hop neighborhood of entities around the center node within the bigger Freebase KG , plus the string literals associated with all the entities included .
Note that it is possible to have edges between the 1 - hop neighbors of the center node , therefore the graphs typically are not star structured .
Section 3.2
69  Figure 2 : Distribution of graph and article sizes across our WikiGraphs dataset .
indicating that the entities are supplemented with the rich information in the string literals .
The distribution of information is not uniform across the nodes in a graph .
Figure 5 shows that most entity nodes in our graph has a small degree , while few nodes have much larger degrees .
Also most string literal nodes contain short text , while fewer nodes contain longer text .
The skewed distribution of nodes and edges in our dataset reﬂect the nature of KG ’s like Freebase , and presents new challenges to graph representation learning models .
3.2 The dataset construction process
We follow three principles when designing the dataset construction process :
1 .
The text part of the data should be directly comparable in complexity to the capability of state - of - the - art text generative models .
2 . The graph part of the data should be constructed in an automatic and scalable way .
3 . The graph part of the data should be relevant
for the paired text data .
Note that our process is general , and can be applied to any set of Wikipedia articles .
We have tried to pair a full dump of English Wikipedia with Freebase and managed to get over 3 million graphtext pairs .
Here we restrict the process to the set of articles from the WikiText-103 dataset .
We try to map each Wikipedia article to a relevant subgraph of the existing large scale KG Freebase ( Bollacker et al , 2008 ) .
We used the last public dump of Freebase2 , which contains 1.9B triples and a total of 250 GB of data .
We ﬁltered the data by keeping only the entities with at least 4 string attributes ( otherwise the entities are less interpretable ) , and keeping only the top 1024 most frequent relation types and restricting the relations to
2https://developers.google.com/
freebase
Figure 3 : Edge type distribution roughly follows an inverse exponential law .
Figure 4 : Distribution of the per - graph number of entity nodes and string literal nodes in our dataset .
provides more details about how these graphs are constructed and any additional ﬁltering we did .
One special characteristic about our graph data is that the natural language text contained in the string literal nodes can sometimes be quite long ( see e.g. Figure 7,8 ) , and therefore provide much richer information not included in the graph structure itself .
On average , each graph contains 895.1 tokens across all the string literal nodes in one graph ( Table 2 , Figure 2 , “ Tokens per graph ” ) .
Figure 4 shows the distribution of per - graph number of entity nodes and string literal nodes in our dataset .
We can see that our graphs tend to have more string literal nodes than entity nodes ,
70050100150200250Nodes per graph010002000300040005000600070008000CountMin 3 , Mean 38.7 , Max 2550100200300400500Edges per graph0200040006000800010000CountMin 2 , Mean 48.3 , Max 50402000400060008000Tokens per graph02000400060008000CountMin 7 , Mean 895.1 , Max 90920500010000150002000025000Tokens per article010002000300040005000600070008000CountMin 69 , Mean 3533.8 , Max 269940100200300400500Rank of edge type100101102103104105Edge type frequencyMin 1 , Mean 2,177.0 , Max 163,722050100150200Number of nodes in a graph02500500075001000012500150001750020000CounttypeEntityString literal  ( a ) Center node degree dist .
( b ) Non - center node degree dist .
( c ) String literal node length dist .
Figure 5 : Node degree distribution for entity nodes and token count distribution for string literal nodes .
only those among the retained entities and between the entities and string attributes .
We also simpliﬁed the entity and relation names by stripping off the irrelevant “ http://rdf.freebase.com/ ” and further removed duplicates .
This gives us a signiﬁcantly cleaner and smaller backbone graph for Freebase , with about 20 M nodes .
Finding the relevant subgraph for an article in such a cleaned up but still large KG remains nontrivial .
Our process for this contains 3 stages : mapping , expansion , and ﬁltering .
Mapping In the ﬁrst stage of the process , we map each article into an entity in our processed Freebase KG .
This is made possible through triples from Freebase like the following :
not star structured .
This gives us a relevant but compact graph for each article .
We have also investigated the possibility of a 2 - hop neighborhood from the center node , and found that 2 - hop neighborhoods are signiﬁcantly larger than 1 - hop and through some “ hub ” nodes like “ Male ” or “ Female ” a 2 - hop neighborhood from an entity can easily include many other irrelevant entities .
Based on such observations we decided to use the 1 - hop neighborhood to keep the relevance of the subgraph high .
Filtering The last stage of the process involves more ﬁltering and cleaning up of the data .
We noticed that in Freebase it is common for one entity to have multiple relations of the same type pointing to different string attributes , like the following :
ns / g.11b6jbqpt4
key / wikipedia.en
" Madunnella "
ns / m.07c72 key / wikipedia.en " The SImpsons "
where ns / g.11b6jbqpt4 refers to an entity in the KG , key / wikipedia.en is the type of the edge , which indicates that this entity is linked to a Wikipedia article and “ Madunnella ” is the title of that article .
We normalize the title string ( and in general any string literals ) from Freebase by replacing “ _ ” with white space and handle unicode characters properly .
We extract the titles from the Wikipedia article through string matching , where titles are enclosed in a “ = [ title ] = " pattern .
In this step we managed to map 24,345 out of 28,475 ( 85.5 % ) article titles from WikiText-103 to an entity in our KG .
Expansion We treat each of the mapped entities as the center node of a subgraph , and expand 1 hop out in the entire ﬁltered Freebase graph to include all the neighboring entities that are the most relevant to the center entity .
We then expand further from this 1 - hop graph out to include all the relations that connect the selected entities to string attributes as well as between these entities themselves .
Note that because of these edges between the 1 - hop neighbor entities the graphs are typically
ns / m.07c72 key / wikipedia.en " The Simpson "
ns / m.07c72 key / wikipedia.en " The simsons "
ns / m.07c72 key / wikipedia.en " Thr Simpsons "
ns / m.07c72 key / wikipedia.en " The Simpson ’s "
It is clear that there is a lot of redundancy in this data .
We reduced all such edges ( from the same entity with the same edge type to string attributes ) to a single edge by picking the most “ canonical ” one .
This was done by ﬁtting a unigram model to the characters in the collection of strings and using that model to pick the most likely string .
We also ﬁltered the graphs based on size and created three versions of the data with maximum graph size capped at 256 , 512 , and 1024 nodes , respectively .
All the statistics and results in the rest of the paper are based on graphs with a maximum size of 256 , but all versions of the data are made available online .
4 Experiments
We perform a set of experiments to showcase how the text and graph information can be combined in a language model .
Speciﬁcally , we consider three
71020406080100120140Node degree0200040006000800010000CountMin 2 , Mean 12.6 , Max 140010203040506070Node degree020000400006000080000CountMin 2 , Mean 7.6 , Max 720200400600800Number of tokens per string literal0100000200000300000400000500000600000CountMin 1 , Mean 28.6 , Max 872  tasks : text generation conditioned on the graph , graph retrieval given the text , and text retrieval given the graph .
4.1 Graph - conditioned Transformer - XL
In order to incorporate graph information into an advanced language model , we adapt the recent Transformer - XL model ( Dai et al , 2019 ) to also attend to the graph features .
At a high - level our model embeds the graph into a set of embedding vectors , and then exposes these embeddings to the Transformer - XL model as extra “ token ” embeddings to condition on .
The size of this set depends on the graph model we choose .
Given the features for T text tokens Ht ∈ RT ×d and features for T ( cid:48 ) graph “ tokens ” Hg ∈ RT ( cid:48)×d(cid:48 ) , we illustrate the graph - conditioned attention procedure with a single head as follows :
Qt , Kt , Vt = HtWt Kg , Vg = HgWg At , Ag = QtK(cid:62 )
q , HtWt k , HgWg t , QtK(cid:62 ) g
v
k , HtWt v
A , V =
[ At ◦ Ag ] , [ Vt ◦
Vg ]
O = Masked - Softmax(A)V
where [ a ◦ b ] stands for concatenation on the sequence dimension and thus A ∈ RT ×(T + T ( cid:48 ) ) and V ∈ R(T + T ( cid:48))×dh , where dh is the head dimension .
In other words , comparing to the original Transformer - XL , our model also computes the attention scores between the text queries Qt and both the text keys Kt and the graph keys Kg .
As a result , the attention outputs contain information from both the graph and the text context .
Note that this formulation is compatible with an additional memory ( Dai et al , 2019 ) with minimal changes , as it simply adds in an extra set of “ tokens ” for the model to attend to .
We do n’t use position encodings for the graph “ tokens ” as there is no sequential ordering for them .
In this work we consider three different approaches for encoding the graph structure :
• Nodes only ( Nodes ): we construct separate BoW representations for each node and project each to an embedding and ignore the edges .
In this case T ( cid:48 ) is equal to the number of nodes in the graph .
• Graph neural network ( GNN ): we embed BoW representations for both nodes and edges and then use a graph neural network ( Battaglia et al , 2018 ) on top of those embeddings to compute a new set of node embeddings .
T ( cid:48 ) is equal to the number of nodes .
The T ( cid:48 ) graph embeddings from this process are shared across all the time steps for text tokens .
This model can be further improved , e.g. by using word embeddings and text summarization techniques , but we leave these for future work .
Implementation details
4.1.1 We reimplement the Transformer - XL model in Jax ( Bradbury et al , 2018 ) .
In our experiments , we employ the base model in ( Dai et al , 2019 ) , except that we increase the tail shrinkage factor used for the adaptive softmax and input representations from 1 to 4 , which saves 63 % of the parameters without compromising the performance .
On the full Wikitext-103 dataset , our implementation has a test perplexity of 24.2 ( published result for this base model was 24.0 ) .
We train our models using the standard likelihood objective for language models with a total batch size of 64 on 8 V100 GPUs .
Adam optimizer is used with an initial learning rate of 2.5 × 10−4 , which decays up to 200k steps following a cosine curve .
During training , we use text segments of 150 steps and a memory of equal size .
When evaluating the model , we use a sequence length of 64 and memory size 640 .
Unless further noted , in our experiments we use an embedding size of 256 for BoW - conditioned models .
For other models , we project each node or edge represented by BoW to an embedding space of size 128 .
The default GNN we use has a single linear message passing layer of 256 hidden units .
• Bag - of - words ( BoW ): we construct a single bag - of - words representation of all the tokens from both the nodes and edges in the graph .
Entity IDs and numeric values in the graph are replaced with special tokens < entity > and < number > .
The BoW vector is further projected using a linear layer to a latent space .
In this case T ( cid:48 ) = 1 .
4.2 Graph → text generation
Our ﬁrst task is text generation conditioned on the graph .
We evaluate model performance by ( 1 ) computing model perplexity on held - out text and ( 2 ) drawing samples from the model and comparing that to the ground truth text article .
We use BLEU score ( Papineni et al , 2002 ) to measure the similarity of our generated samples to the ground truth .
72  rBLEU(w / title )
Cond .
rBLEU
Test Ppl .
Valid 25.85 10.97
Test Valid None 27.98 9.98 BoW 26.65 29.53 24.41 32.41 Nodes 27.40 30.51 25.31 32.60 GNN 26.93 31.39 26.22 32.65
Test 24.07 27.39 27.43 28.35
Table 3 : The perplexity and the generated text reverseBLEU score of different types of graph - conditioned models .
We show the reverse - BLEU score with or without prompting the original title at the start of the text generation .
Unlike previous use cases for BLEU score where there are many references for one generated sample , here we have only one ground truth reference but we can generate multiple samples .
We therefore simply swapped the reference with the samples when computing the score , which we term as the reverse - BLEU ( rBLEU ) .
We have also tried other ways of computing the BLEU score and ﬁnd that they do n’t change how models compare against each other .
Unless explicitly stated , we let the model sample with a memory size of 640 , and condition on the graphs in the test set to generate text for up to 512 tokens per sample for a total of 20 samples per graph .
The rBLEU score is computed based on these samples and corresponding ground - truth texts are truncated to the same length .
We sample the texts from the distribution with a temperature of 0.8 .
For each case , we report the average rBLEU score of 3 sampling runs .
We ﬁnd the variances are insigniﬁcant which do not affect the comparison results .
In Appendix A.3 we also report results for generating longer samples for up to 4096 tokens .
4.2.1 Main result
In Table 3 , we show the perplexity and the rBLEU score of the unconditional , BoW , nodes - only , and GNN conditioned models .
As a reference , a standard Transformer - XL model trained on the full Wikitext-103 training set reaches 25.08 perplexity on our test set , which contains 71.7 % of the original test articles .
We can see that the unconditional , i.e. text only , model trained on our dataset gets a very similar performance as trained on the full set .
This is strong evidence that our dataset can be a good benchmark for state - of - the - art text generative models .
We also see that conditioned on the graphs , model perplexity did n’t improve , but the relevance
# MP layers Test Ppl .
Test rBLEU 26.65 27.40 27.20 26.85
25.31 26.22 26.16 25.91
0 1 3 5
Table 4 : The test perplexity and the generated text reverse - BLEU score ( without title prompt ) of GNNbased models with different numbers of message passing layers .
Figure 6 : Performance vs size of graph to condition on .
The model is trained with a smaller version of the data by subsampling the number of nodes .
of the samples measured by the BLEU scores did improve signiﬁcantly .
This indicates that the graph conditioned models can indeed steer the language model towards more relevant topics , but this so far can not yet improve likelihood metrics .
To make the evaluation more fair to the text - only model , we also tried to prompt the generation with the title of the article , such that the text - only model also has some context .
In this setting the graph models are still better , showing the importance of modeling the structure .
Lastly , among all the 3 graph model variants , we observe that using a set of embeddings from the nodes model is better than using a single embedding from the BoW model , and fully utilizing the graph structure through the GNN model is consistently better than ignoring the edges as in the nodes model .
However the differences among the methods are relatively small .
For visualizations of a few graphs in our dataset and the corresponding samples generated based on them please refer to Appendix A.
4.2.2 Ablation studies
We show a few ablations on the graph model and sampling parameters , to provide some insights into the models .
730.20.30.40.50.60.70.80.91.0Proportion of graph nodes to keep2223242526Test rBLEUBoWNodesGNN  Cond .
Recall@1 Recall@5 mAP
0.10 None 25.98 BoW 26.62 Nodes 27.79 GNN
0.02 16.28 16.28 18.60
0.12 30.23 34.88 34.88
Cond .
Recall@1 Recall@5 mAP
0.02 None 97.67 BoW 96.51 Nodes 100.00 GNN
0.07 100.00 100.00 100.00
0.02 95.35 93.02 100.00
Table 5 : Text retrieval given the graph .
Table 6 : Graph Retrieval given the text .
Table 4 shows the effect of varying the number of message passing layers in the GNN .
We can observe that there is a big difference between using message passing ( ≥ 1 layers ) or not ( 0 layers ) in terms of rBLEU score , but increasing the number of message passing layers does not change the results signiﬁcantly .
We believe however , that these results can be improved by employing bigger and more powerful graph representation learning models , and potentially use initial node and edge representations better than bag - of - words .
In Figure 6 we show the effect of the graph size on model performance .
In this experiment we subsample the nodes in each graph to control for the amount of context the model has access to .
It is clear from the results that when we heavily subsample and keep only a small portion of the graphs , the GNN model performs similarly as the simpler BoW model , but GNNs beneﬁt more as we keep more of the graph structure .
4.3 Graph → text retrieval
In this task , we evaluate the possibility of retrieving relevant text for a given query graph .
We pair all articles with all graphs in the test set , resulting in 43×43=1849 pairs .
Then the trained graphconditioned language models are used to produce the per - token likelihood of each pair , and we use these likelihood scores to rank the text articles for each graph .
We expect the learned models can rank the correct pairs higher than wrong ones .
To measure the results we use standard ranking metrics including recall@K , which computes the fraction of times the correct pair is included in the top K predictions , as well as mean average precision ( mAP ) .
In Table 5 , it is observed that graph - conditioned models can indeed retrieve more relevant texts from the graph than the unconditional model , among which the GNN - based model performs the best , and the unconditional model performs close to a random guess .
4.4 Text → graph retrieval
In this last task , we evaluate the performance of graph retrieval given a text query .
We use exactly the same setting and scores as Section 4.3 , but instead rank the graphs for each text article using the likelihood scores .
The results are shown in Table 6 .
Note that this task is quite easy with our data and setup , potentially because the graphs are much more distinguishable than the text articles .
All the graph - conditioned models perform almost perfectly , with the GNN model again outperforming the others .
5 Conclusion
In this paper , we present WikiGraphs , a new graphtext paired dataset with signiﬁcantly larger graphs and longer text compared to previous datasets of similar nature .
We show that the text part of this data is a good benchmark for state - of - the - art text generation models , and the paired dataset can help us benchmark models that are capable of generating long and coherent text conditioned on a graph structure .
In the ﬁrst set of experiments on this dataset we showcase 3 different tasks using our dataset , and demonstrate the beneﬁt of better models that make more use of the graph structure .
There is still signiﬁcant room for improvement for these tasks on our dataset , and we hope the release of the data and baseline code can help spur more interest in developing models that can generate long text conditioned on graphs , and generate graphs given text , which is another exciting direction our dataset enables but we did not explore , and eventually bridging the graph and text modalities .
