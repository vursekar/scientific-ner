The AFRL WMT19 Systems : Old Favorites and New Tricks
Jeremy Gwinnup , Grant Erdmann , Timothy Anderson Air Force Research Laboratory { jeremy.gwinnup.1 , grant.erdmann , timothy.anderson.20}@us.af.mil
Abstract
This paper describes the Air Force Research Laboratory ( AFRL ) machine translation systems and the improvements that were developed during the WMT19 evaluation campaign .
This year , we refine our approach to training popular neural machine translation toolkits , experiment with a new domain adaptation technique and again measure improvements in performance on the Russian – English language pair .
1
Introduction
As part of the 2019 Conference on Machine Translation ( Bojar et al , 2019 ) news - translation shared the AFRL Human Language Technology task , team participated in the Russian – English portion of the competition .
We build on our strategies from last year ( Gwinnup et al , 2018 ) , adding additional language ID based data processing and optimizing subword segmentation strategies .
For Russian – English we again submitted an entry comprising our best systems trained with Marian ( JunczysDowmunt et al , 2018 ) , Sockeye ( Hieber et al , 2017 ) with Elastic Weight Consolidation ( EWC ) ( Thompson et al , 2019 ) , OpenNMT ( Klein et al , 2018 ) , and Moses ( Koehn et al , 2007 ) combined using the Jane system combination method ( Freitag et al , 2014 ) .
2 Data and Preprocessing
2.1 Data Preparation
the pre - built language ID model developed by the authors of fastText ( Joulin et al , 2016a , b ) , we developed a utility that examined the source and target sentence pairs and discarded that pair if either side fell below 0.81 probability of the desired language .
We applied this filtering to all provided parallel corpora , removing 33.7 % of lines .
This process was particularly effective when used to filter the Paracrawl corpus where 57.1 % of lines were removed .
Pre and post - filtering line counts for various corpora are shown in Table 1 .
Corpus
Total
Retained
CommonCrawl newscommentary Yandex ParaCrawl UN2016
723,256 290,866 1,000,000 12,061,155 11,365,709
655,069 264,089 901,307 5,173,675 9,871,406
Total Lines
25,440,968
16,865,546
Table 1 : Training corpus total and retained lines after fastText filtering
testset
wmt18preproc wmt19filt
newstest2014 newstest2015 newstest2016 newstest2017 newstest2018
33.0 28.6 28.4 30.8 26.9
34.1 29.6 29.4 31.8 27.9
We used and preprocess data as outlined in Gwinnup et al ( 2018 ) .
For all systems trained , we applied either byte - pair encoding ( BPE ) ( Sennrich et al , 2016 ) or SentencePiece ( Kudo and Richardson , 2018 ) subword strategies to address the vocabulary - size problem .
For this year , we also employed a language ID filtering step for the BPE - based systems .
Using
Table 2 : Test set comparison for non - filtered WMT18 training corpus and filtered WMT19 training corpus measured by SacreBLEU .
A comparison with the organizer - provided parallel training data used in our WMT18 system
1We chose this value arbitrarily ; future work will explore
varying this threshold .
ProceedingsoftheFourthConferenceonMachineTranslation(WMT),Volume2 : SharedTaskPapers(Day1)pages203–208Florence , Italy , August1 - 2,2019.c(cid:13)2019AssociationforComputationalLinguistics203  ( which is largely the same as the provided parallel data for WMT19 in the Russian – English language pair ) on baseline Marian transformer systems with identical training conditions show that aggressive language ID based filtering yields an approximate +1 BLEU point improvement as measured by SacreBLEU ( Post , 2018 ) .
These results are shown in Table 2 .
2.2 Exploration of Byte - Pair Encoding
Merge Sizes
One of the problems faced when addressing the closed - vocabulary problem is the granularity of the subword units either produced by SentencePiece or BPE .
To that end , we examined varying the number of BPE merge operations in order to determine an optimal setting to maximize performance for the Russian – English language pair .
For the OpenNMT - based systems , a vocabulary size of 32k entries was employed during training of a SentencePiece segmentation model2 .
This vocabulary size was determined empirically from the training data .
Alternatively , for the BPE - based systems , we systematically examined varying sizes of BPE merge operations and vocabulary sizes in 10k increments from 30k to 80k .
Results in Table 3 show that 40k BPE merge operations perform best across all test sets decoded for this language pair .
All subsequent Marian experiments in this work utilize this 40k BPE training corpus .
3 MT Systems
This year , we focused system - building efforts on the Marian , Sockeye , OpenNMT , and Moses toolkits , having explored a variety of parameters , data , and conditions .
While most of our experimentation builds off of previous years ’ efforts , we did examine domain adaptation via continued training , including Elastic Weight Consolidation ( EWC ) ( Thompson et al , 2019 ) .
3.1 Marian
As with last year ’s efforts , we train multiple Marian ( Junczys - Dowmunt et al , 2018 ) models with both University of Edinburgh ’s “ bi - deep ” ( Miceli Barone et al , 2017 ; Sennrich et al , 2017 ) and Google ’s transformer ( Vaswani et
al , 2017 )
architectures .
Network hyperparameters are the same as detailed in Gwinnup et al ( 2018 ) .
We again use newstest2014 as the validation set during training .
Utilizing the best - performing BPE parameters from Section 2.2 , we first trained a baseline system in each of the two network architectures , noting the Transformer system ’s better performance of +0.82 BLEU on average across decoded test sets .
An additional six distinct transformer models were then independently3 trained for use in ensemble decoding .
We then ensemble decoded test sets with all eight models .
Marian typically assigns each model used in ensemble decoding a feature weight of 1.0 ; thus each model contributes equally to the decoding process .
Borrowing from our Moses training approach , we utilize a multi - iteration decode and optimize feature weights using the “ Expected Corpus BLEU ” ( ECB ) metric with the Drem optimizer ( Erdmann and Gwinnup , 2015 ) .
We experimented using newstest2014 and newstest2017 as tuning sets – 2017 did not help performance , but using 2014 did improve performance by up to +0.9 BLEU4 over the non - tuned ensemble .
Scores for all the above - mentioned systems are shown in Table 4 .
The best - performing ensemble ( ensemble tune14 ) was used in system combination .
3.2 Sockeye
For our Sockeye ( Hieber et al , 2017 ) systems , we experimented with continued training ( Luong and Manning , 2015 ; Sennrich et al , 2015 ) – a means to specialize a model in a new domain after a period of training on a general domain .
One downside of utilizing continued training is the model adapts “ too - well ” to the new domain at the expense of performance in the original domain ( Freitag and Al - Onaizan , 2016 ) .
One method to mitigate this performance drop is to prevent certain parameters of the network from changing with Elastic Weight Consolidation ( EWC ) ( Kirkpatrick et al , 2017 ) .
Thompson et al ( 2019 ) conveniently provides an implementation of this technique in Sockeye .
That work illustrated a use case where the original domain is news articles , while the new domain is text of patent applications – a marked dif3Identical training data and starting parameters except for
2SentencePiece was used in part to provide diversity between our OpenNMT and other systems trained with BPE data .
random seed .
dation during training .
4This may be due to the choice of newstest2014 for vali204  System newstest2014
newstest2015
newstest2016
newstest2017
newstest2018
bpe30k bpe40k bpe50k bpe60k bpe70k bpe80k
33.7 34.1 33.9 33.4 33.0 32.6
28.9 29.6 29.2 29.1 28.8 28.7
28.7 29.4 29.1 28.7 28.8 28.2
31.4 31.8 31.6 31.3 31.2 31.1
27.6 27.9 27.8 27.6 26.9 26.9
Table 3 : Cased , detokenized BLEU for various test sets and BPE merge - value treatments .
Best scores for each test set are denoted with bold text .
System
newstest2014
newstest2015
newstest2016
newstest2017
newstest2018
single bi - deep single transformer untuned ensemble ensemble tune17 ensemble tune14
32.7 34.1 36.2 35.3 37.1
29.0 29.6 31.6 31.1 31.3
28.7 29.4 30.5 30.2 31.2
31.3 31.8 34.2 34.2 34.5
27.0 27.9 29.7 29.7 30.5
Table 4 : Test set comparison for baseline bi - deep , transformer , untuned and tuned ensembles for various test sets measured in cased , detokenized BLEU .
Best scores for each test set are denoted with bold text .
ference in style and content .
Here , we created a news subdomain corpus from the newstest2014 through newstest2017 test sets .
The intuition is that more current events will be discussed in these test sets than the remainder of the provided training corpora , allowing better adaptation of new events in the newest test sets ( newstest2018 and newstest2019 . )
We first trained a baseline transformer system using the best - performing BPE parameters from Section 2.2 , 512 - dimension word embeddings , 6 layer encoder and decoder , 8 attention heads , label smoothing and transformer attention dropout of 0.1 .
We then continue - train a model on the adaptation set described above .
We also followed the Sockeye EWC training procedure , producing a model more resilient to overfitting due to continued training .
Results for these systems are shown in Table 5 .
We see that the baseline Sockeye transformer model performs similarly to the baseline singlemodel Marian transformer system shown in Table 4 .
The continued - training system ( con’t train ) system predictably overfit on newstest2014 as expected , since that test set is a part of the adaptation set .
Likewise , performance on the out - ofdomain newstest2018 also dropped as a result of overfitting .
The best - performing EWC system5
actually improved performance on 2018 with lesspronounced overfitting on 2014 .
System
newstest2014
newstest2018
baseline con’t train best EWC
33.4 89.3 48.5
27.6 24.3 29.5
Table 5 : Sockeye system scores for newstest2014 ( in - domain ) and newstest2018 ( out - of - domain ) test sets for various training conditions measured in SacreBLEU .
For system combination outlined later in Section 4 , we decoded test sets with an ensemble of the four highest - scoring model checkpoints from the best EWC training run .
3.3 OpenNMT - T
Our first Open - NMT system was trained using the Transformer architecture with the default “ TransformerBig ” settings as described in Vaswani et
al ( 2017 ): 6 layers of 1024 units , 16 attention heads .
Dropout rates of 0.3 for layers and 0.1 for attention heads and relu ’s .
Training data for this system utilized the training corpus from our WMT17 Russian – English system ( Gwinnup et al , 2017 ) consisting of provided parallel and backtranslated
5EWC applied with weight - decay of 0.001 and learningrate of 0.00001
205  data .
This data was then processed with a joint 32k word vocabulary SentencePiece model .
3.4 OpenNMT - G
For our second OpenNMT system , we first trained language - specific , 32k word vocabularies using SentencePiece .
WMT news test data from all years except 2014 and 2017 were used to train SentencePiece .
These data , with the addition of the language ID filtered ParaCrawl corpus outlined in Section 2.1 , were used for training the system .
WMT news test data from 2014 was used for validation .
OpenNMT - tf was used to create the system , using the stock “ Transformer ” model .
3.5 Moses
As in previous years , we trained a phrase - based Moses ( Koehn et al , 2007 ) system with the same data as the Marian system outlined in Section 3.1 in order to provide diversity for system combination .
This system employed a hierarchical reordering model ( Galley and Manning , 2008 ) and 5 - gram operation sequence model ( Durrani et al , 2011 ) .
The 5 - gram English language model was trained with KenLM on all permissable monolingual English news - crawl data .
The BPE model used was applied to both the parallel training data and the language modeling corpus .
System weights were tuned with the Drem ( Erdmann and Gwinnup , 2015 ) optimizer using the “ Expected Corpus BLEU ” ( ECB ) metric .
System
BLEU BEER
1 .
Marian 30.47 2 . Sockeye EWC 29.43 26.22 3 . OpenNMT - T 30.05 4 . OpenNMT - G 27.33 5 .
Moses
0.5995 0.5968 0.5737 0.6017 0.5836
Syscomb-5
32.12
0.6072
Table 6 : System combination and input system scores measured in cased , detokenized BLEU and BEER on the newstest2018 test set .
System
BLEU BEER
afrl - syscomb19 afrl - ewc
37.2 34.3
0.627 0.613
Table 7 : Final submission system scores measured in cased BLEU and BEER on the newstest2019 test set .
6 Conclusion
We presented a series of improvements to our including improved Russian – English systems , preprocessing and domain adaptation .
Clever remixing of older techniques from the phrasebased MT era enabled improvements in ensembled neural decoding .
Lastly , we performed system combination to leverage benefits from these new techniques and favorite approaches from previous years .
4 System Combination
