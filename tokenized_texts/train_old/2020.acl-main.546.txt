Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 6141‚Äì6151 July 5 - 10 , 2020 .
c  2020 Association for Computational Linguistics6141NeuInfer : Knowledge Inference on N - ary Facts Saiping Guan , Xiaolong Jin , Jiafeng Guo , Yuanzhuo Wang , and Xueqi Cheng School of Computer Science and Technology , University of Chinese Academy of Sciences ; CAS Key Laboratory of Network Data Science and Technology , Institute of Computing Technology , Chinese Academy of Sciences fguansaiping , jinxiaolong , guojiafeng , wangyuanzhuo , cxq g@ict.ac.cn Abstract Knowledge inference on knowledge graph has attracted extensive attention , which aims to Ô¨Ånd out connotative valid facts in knowledge graph and is very helpful for improving the performance of many downstream applications .
However , researchers have mainly poured attention to knowledge inference on binary facts .
The studies on n - ary facts are relatively scarcer , although they are also ubiquitous in the real world .
Therefore , this paper addresses knowledge inference on n - ary facts .
We represent each n - ary fact as a primary triple coupled with a set of its auxiliary descriptive attribute - value pair(s ) .
We further propose a neural network model , NeuInfer , for knowledge inference on n - ary facts .
Besides handling the common task to infer an unknown element in a whole fact , NeuInfer can cope with a new type of task , Ô¨Çexible knowledge inference .
It aims to infer an unknown element in a partial fact consisting of the primary triple coupled with any number of its auxiliary description(s ) .
Experimental results demonstrate the remarkable superiority of NeuInfer .
1 Introduction With the introduction of connotative valid facts , knowledge inference on knowledge graph improves the performance of many downstream applications , such as vertical search and question answering ( Dong et al . , 2015 ; Lukovnikov et al . , 2017 ) .
Existing studies ( Nickel et al . , 2016 ; Wang et al . , 2017 ) mainly focus on knowledge inference on binary facts with two entities connected with a certain binary relation , represented as triples , ( head entity , relation , tail entity ) .
They attempt to infer the unknown head / tail entity or the unknown relation of a given binary fact .
However , n - ary facts involving more than two entities are also ubiquitous .
For example , in Freebase , more than 1=3entities participate in n - ary facts ( Wen et al . , 2016 ) .
Thefact thatJohnBardeen receivedNobelPrizein Physics in1956 together with WalterHouser Brattain andWilliamShockley1is a typical 5ary fact .
So far , only a few studies ( Wen et al . , 2016 ; Zhang et al . , 2018 ; Guan et al . , 2019 ) have tried to address knowledge inference on n - ary facts .
In existing studies for knowledge inference on nary facts , each n - ary fact is represented as a group of peer attributes and attribute values .
In practice , for each n - ary fact , there is usually a primary triple ( the main focus of the n - ary fact ) , and other attributes along with the corresponding attribute values are its auxiliary descriptions .
Take the above 5 - ary fact for example , the primary triple is ( JohnBardeen;award -received;Nobel Prize in Physics ) , and other attribute - value pairs includingpoint -in - time : 1956 , together -with : Walter Houser Brattain andtogether -with : WilliamShockley are its auxiliary descriptions .
Actually , in YAGO ( Suchanek et al . , 2007 ) and Wikidata ( Vrande Àáci¬¥c and Kr ¬®otzsch , 2014 ) , a primary triple is identiÔ¨Åed for each n - ary fact .
The above 5 - ary fact is a relatively complete example .
In the real - world scenario , many n - ary facts appear as only partial ones , each consisting of a primary triple and a subset of its auxiliary description(s ) , due to incomplete knowledge acquisition .
For example , ( JohnBardeen;award received;NobelPrizeinPhysics ) withpoint in - time : 1956 and it withftogether -with : Walter Houser Brattain ; together -with : WilliamShockley gare two typical partial facts corresponding to the above 5 - ary fact .
For differentiation , we call those relatively complete facts as whole ones .
We noticed that existing studies on n - ary facts infer an unknown element in a welldeÔ¨Åned whole fact and have not paid attention to knowledge inference on partial facts .
Later on , we 1https://www.wikidata.org/wiki/Q949
6142refer the former as simple knowledge inference , while the latter as Ô¨Çexible knowledge inference .
With these considerations in mind , in this paper , by discriminating the information in the same n - ary fact , we propose a neural network model , called NeuInfer , to conduct both simple and Ô¨Çexible knowledge inference on n - ary facts .
Our speciÔ¨Åc contributions are summarized as : ‚Ä¢We treat the information in the same n - ary fact discriminatingly and represent each n - ary fact as a primary triple coupled with a set of its auxiliary descriptive attribute - value pair(s ) .
‚Ä¢We propose a neural network model , NeuInfer , for knowledge inference on n - ary facts .
NeuInfer can particularly handle the new type of task , Ô¨Çexible knowledge inference , which infers an unknown element in a partial fact consisting of a primary triple and any number of its auxiliary description(s ) .
‚Ä¢Experimental results validate the signiÔ¨Åcant effectiveness and superiority of NeuInfer .
2 Related Works 2.1 Knowledge Inference on Binary Facts They can be divided into tensor / matrix based methods , translation based methods , and neural network based ones .
The quintessential one of tensor / matrix based methods is RESCAL ( Nickel et
al . , 2011 ) .
It relates a knowledge graph to a three - way tensor of head entities , relations , and tail entities .
The learned embeddings of entities and relations via minimizing the reconstruction error of the tensor are used to reconstruct the tensor .
And binary facts corresponding to entries of large values are treated as valid .
Similarly , ComplEx ( Trouillon et al . , 2016 ) relates each relation to a matrix of head and tail entities , which is decomposed and learned like RESCAL .
To improve the embeddings and thus the performance of inference , researchers further introduce the constraints of entities and relations ( Ding et al . , 2018 ; Jain et al . , 2018 ) .
Translation based methods date back to TransE ( Bordes et al . , 2013 ) .
It views each valid binary fact as the translation from the head entity to the tail entity via their relation .
Thus , the score function indicating the validity of the fact is deÔ¨Åned based on the similarity between the translation result and the tail entity .
Then , a Ô¨Çurry of methodsspring up ( Wang et al . , 2014 ; Lin et al . , 2015b ; Ji et al . , 2015 ; Guo et al . , 2015 ; Lin et al . , 2015a ; Xiao et al . , 2016 ; Jia et al . , 2016 ; Tay et al . , 2017 ; Ebisu and Ichise , 2018 ; Chen et al . , 2019 ) .
They modify the above translation assumption or introduce additional information and constraints .
Among them , TransH ( Wang et al . , 2014 ) translates on relationspeciÔ¨Åc hyperplanes .
Entities are projected into the hyperplanes of relations before translating .
Neural network based methods model the validity of binary facts or the inference processes .
For example , ConvKB ( Nguyen et al . , 2018 ) treats each binary fact as a three - column matrix .
This matrix is fed into a convolution layer , followed by a concatenation layer and a fully - connected layer to generate a validity score .
Nathani et al .
( 2019 ) further proposes a generalized graph attention model as the encoder to capture neighborhood features and applies ConvKB as the decoder .
ConvE ( Dettmers et al . , 2018 ) models entity inference process via 2D convolution over the reshaped then concatenated embedding of the known entity and relation .
ConvR ( Jiang et al . , 2019 ) further adaptively constructs convolution Ô¨Ålters from relation embedding and applies these Ô¨Ålters across entity embedding to generate convolutional features .
SENN ( Guan et al . , 2018 ) models the inference processes of head entities , tail entities , and relations via fullyconnected neural networks , and integrates them into a uniÔ¨Åed framework .
2.2 Knowledge Inference on N - ary Facts As aforesaid , only a few studies handle this type of knowledge inference .
The m - TransH method ( Wen et al . , 2016 ) deÔ¨Ånes n - ary relations as the mappings from the attribute sequences to the attribute values .
Each n - ary fact is an instance of the corresponding n - ary relation .
Then , m - TransH generalizes TransH ( Wang et al . , 2014 ) on binary facts to nary facts via attaching each n - ary relation with a hyperplane .
RAE ( Zhang et al . , 2018 ) further introduces the likelihood that two attribute values co - participate in a common n - ary fact , and adds the corresponding relatedness loss multiplied by a weight factor to the embedding loss of m - TransH. SpeciÔ¨Åcally , RAE applies a fully - connected neural network to model the above likelihood .
Differently , NaLP ( Guan et al . , 2019 ) represents each n - ary fact as a set of attribute - value pairs directly .
Then , convolution is adopted to get the embeddings of the attribute - value pairs , and a fully - connected neural
6143network is applied to evaluate their relatedness and Ô¨Ånally to obtain the validity score of the input n - ary fact .
In these methods , the information in the same n - ary fact is equal - status .
Actually , in each n - ary fact , a primary triple can usually be identiÔ¨Åed with other information as its auxiliary description(s ) , as exempliÔ¨Åed in Section 1 .
Moreover , these methods are deliberately designed only for the inference on whole facts .
They have not tackled any distinct inference task .
In practice , the newly proposed Ô¨Çexible knowledge inference is also prevalent .
3 Problem Statement 3.1 The Representation of N - ary Facts Different from the studies that deÔ¨Åne n - ary relations Ô¨Årst
and then represent n - ary facts ( Wen et al . , 2016 ; Zhang et
al . , 2018 ) , we represent each n - ary fact as a primary triple ( head entity , relation , tail entity ) coupled with a set of its auxiliary description(s ) directly .
Formally , given an n - ary fact Fct with the primary triple ( h;r;t ) , mattributes and attribute values , its representation is :   ( h;r ; t ) ; f j  a1 : v1 ; j  a2 : v2 ; j   : : : ; j  am : vmg ; where eachai : vi(i= 1;2;:::;m ) is an attributevalue pair , also called an auxiliary description to the primary triple .
An element of Fct refers to h / r / t / ai / vi;AFct = fa1;a2;:::;a mgisFct ‚Äôs attribute set and aimay be the same to aj(i;j= 1;2;:::;m;i6 = j);VFct = fv1;v2;:::;v mgis Fct ‚Äôs attribute value set .
For example , the representation of the 5 - ary fact , mentioned in Section 1 , is :   ( John Bardeen ; award -received ; Nobel Prize in Physics ) ; f j  point -in - time : 1956 ; j  together -with : Walter Houser Brattain ; j  together -with : William Shockley g :
Note that , in the real world , there is a type of complicated cases , say , where more than two entities participate in the same n - ary fact with the same primary attribute .
We follow Wikidata ( Vrande Àáci¬¥c and Kr ¬®otzsch , 2014 ) to view the cases from different aspects of different entities .
Take the case thatJohnBardeen , WalterHouserBrattain , andWilliamShockley receivedNobelPrizein Physics in1956 for example , besides the above 5 - ary fact from the view of John Bardeen , we get other two 5 - ary facts from the views of Walter HouserBrattain2andWilliamShockley3 , respectively :   ( Walter Houser Brattain ; award -received ; Nobel Prize in Physics ) ; f j  point -in - time : 1956 ; j  together -with : John Bardeen ; j  together -with : William Shockley g :   ( William Shockley ; award -received ; Nobel Prize in Physics ) ; f j  point -in - time : 1956 ; j  together -with : Walter Houser Brattain ; j  together -with : John Bardeeng : 3.2 Task Statement In this paper , we handle both the common simple knowledge inference and the newly proposed Ô¨Çexible knowledge inference .
Before giving their deÔ¨Ånitions under our representation form of n - ary facts , let us deÔ¨Åne whole fact and partial fact Ô¨Årst .
DeÔ¨Ånition 1 ( Whole fact and partial fact ) .
For the factFct , assume its set of auxiliary description(s ) asSd = fai : viji= 1;2;:::;mg .
Then a partial fact ofFctis : Fct0=  ( h;r;t ) ; S0 d , whereS0 d Sd , i.e. ,S0 dis a subset of Sd .
And we call Fctthe whole fact to differentiate it from Fct0 .
Notably , whole fact and partial fact are relative concepts , and a whole fact is a relatively complete fact compared to its partial fact .
In this paper , partial facts are introduced to imitate a typical openworld setting where different facts of the same type may have different numbers of attribute - value pair(s ) .
DeÔ¨Ånition 2 ( Simple knowledge inference ) .
It aims to infer an unknown element in a whole fact .
DeÔ¨Ånition 3 ( Flexible knowledge inference ) .
It aims to infer an unknown element in a partial fact .
4 The NeuInfer Method 4.1 The Framework of NeuInfer To conduct knowledge inference on n - ary facts , NeuInfer Ô¨Årst models the validity of the n - ary facts and then casts inference as a classiÔ¨Åcation task .
2https://www.wikidata.org/wiki/Q184577 3https://www.wikidata.org/wiki/Q163415
6144 hrt - FCNs ‚Ä¶ ‚Ä¶‚Ä¶ ‚Ä¶‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ FCN1Validity score ‚Ä¶ ‚Ä¶ hrtav - FCNs ùëñ:1‚ÜíùëöFCN2‚®ÅFinalscoreminCompatibilityscore ‚Ä¶ ùëñ:1‚Üíùëö ‚Ä¶ ‚Ä¶ ùêöùê¢ùêØùê¢ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ ‚Ä¶ Concate ‚Ä¶ ùê´ ‚Ä¶ ùê≠ùê° ‚Ä¶ ConcateùêΩùëú‚Ñéùëõ 	 ùêµùëéùëüùëëùëíùëíùëõùëéùë§ùëéùëüùëë‚àíùëüùëíùëêùëíùëñùë£ùëíùëëùëÅùëúùëèùëíùëô 	 ùëÉùëüùëñùëßùëí 	 ùëñùëõ 	 ùëÉ‚Ñéùë¶ùë†ùëñùëêùë† ùëùùëúùëñùëõùë°‚àíùëñùëõ‚àíùë°ùëñùëöùëí1956ùë°ùëúùëîùëíùë°‚Ñéùëíùëü‚àíùë§ùëñùë°‚Ñéùëäùëéùëôùë°ùëíùëü 	 ùêªùëúùë¢ùë†ùëíùëü 	 ùêµùëüùëéùë°ùë°ùëéùëñùëõùë°ùëúùëîùëíùë°‚Ñéùëíùëü‚àíùë§ùëñùë°‚Ñéùëäùëñùëôùëôùëñùëéùëö 	 ùëÜ‚Ñéùëúùëêùëòùëôùëíùë¶Figure 1 : The framework of the proposed NeuInfer method .
4.1.1
The Motivation of NeuInfer How to estimate whether an n - ary fact is valid or not ?
Let us look into two typical examples of invalid n - ary facts :   ( John Bardeen ; award -received ; Turing Award ) ; f j  point -in - time : 1956 ; j  together -with :
Walter Houser Brattain ; j  together -with : William Shockley g :   ( John Bardeen ; award -received ; Nobel Prize in Physics ) ; f j  point -in - time : 1956 ; j  together -with : Walter Houser Brattain ; j  place -of - marriage : New Y ork Cityg :
In the above Ô¨Årst n - ary fact , the primary triple is invalid .
In the second one , some auxiliary description is incompatible with the primary triple .
Therefore , we believe that a valid n - ary fact has two prerequisites .
On the one hand , its primary triple should be valid .
If the primary triple is invalid , attaching any number of attribute - value pairs to it does not make the resulting n - ary fact valid ; on the other hand , since each auxiliary description presents a qualiÔ¨Åer to the primary triple , it should be compatible with the primary triple .
Even if the primary triple is basically valid , any incompatible attribute - value pair makes the n - ary fact invalid .
Therefore , NeuInfer is designed to characterize these two aspects and thus consists of two components corresponding to the validity evaluation of the primary triple and the compatibility evaluation of the n - ary fact , respectively.4.1.2 The Framework of NeuInfer The framework of NeuInfer is illustrated in Figure 1 , with the 5 - ary fact presented in Section 1 as an example .
For an n - ary fact Fct , we look up the embeddings of its relation rand the attributes in AFct from the embedding matrix MR2RjRjkof relations and attributes , where Ris the set of all the relations and attributes , and kis the dimension of the latent vector space .
The embeddings of h , t , and the attribute values in VFctare looked up from the embedding matrix ME2RjEjkof entities and attribute values , where Eis the set of all the entities and attribute values .
In what follows , the embeddings are denoted with the same letters but in boldface by convention .
As presented in Figure 1 , these embeddings are fed into the validity evaluation component ( the upper part of Figure 1 ) and the compatibility evaluation component ( the bottom part of Figure 1 ) to compute the validity score of ( h;r;t ) and the compatibility score of Fct , respectively .
These two scores are used to generate the Ô¨Ånal score of Fctby weighted sumand further compute the loss .
Note that , following RAE ( Zhang et al . , 2018 ) and NaLP ( Guan et al . , 2019 ) , we only apply fully - connected neural networks in NeuInfer .
4.2 Validity Evaluation This component estimates the validity of ( h;r;t ) , including the acquisition of its interaction vector and the assessment of its validity , corresponding to ‚Äú hrt - FCNs ‚Äù and ‚Äú FCN 1 ‚Äù in Figure 1 , respectively .
Detailedly , the embeddings of h , r , andtare
6145concatenated and fed into a fully - connected neural network .
After layer - by - layer learning , the last layer outputs the interaction vector ohrtof(h;r;t ): ohrt = f(f(f(f([h;r;t]W1;1+b1;1) W1;2+b1;2))W1;n1+b1;n1);(1 ) wheref()is
the ReLU function ; n1is the number of the neural network layers ; fW1;1;W1;2 ; : : : ; W1;n1gandfb1;1;b1;2;:::;b1;n1gare their weight matrices and bias vectors , respectively .
Withohrtas the input , the validity score valhrt of(h;r;t ) is computed via a fully - connected layer and then the sigmoid operation : valhrt=(ohrtWval+bval ) ; ( 2 ) where Wvalandbvalare the weight matrix and bias variable , respectively ; (x ) = 1 1+e xis the sigmoid function , which constrains valhrt2(0;1 ) .
For simplicity , the number of hidden nodes in each fully - connected layer of ‚Äú hrt - FCNs ‚Äù and ‚Äú FCN 1 ‚Äù gradually reduces with the same difference between layers .
4.3 Compatibility Evaluation This component estimates the compatibility of Fct .
It contains three sub - processes , i.e. , the capture of the interaction vector between ( h;r;t ) and each auxiliary description ai : vi(i= 1;2;:::;m ) , the acquisition of the overall interaction vector , and the assessment of the compatibility of Fct , corresponding to ‚Äú hrtav - FCNs ‚Äù , ‚Äú min ‚Äù and ‚Äú FCN 2 ‚Äù in Figure 1 , respectively .
Similar to ‚Äú hrt - FCNs ‚Äù , we obtain the interaction vector ohrta iviof(h;r;t ) andai : vi : ohrta ivi = f(f(f(f([h;r;t;ai;vi]W2;1+b2;1) W2;2+b2;2))W2;n2+b2;n2 ) ; ( 3 ) wheren2is the number of the neural network layers;fW2;1;W2;2;:::;W2;n2gandfb2;1 ; b2;2;:::;b2;n2gare their weight matrices and bias vectors , respectively .
The number of hidden nodes in each fully - connected layer also gradually reduces with the same difference between layers .
And the dimension of the resulting ohrta iviisd .
All the auxiliary descriptions share the same parameters in this sub - process .
The overall interaction vector ohrtav ofFctis generated based on ohrta ivi .
Before introducing this sub - process , let us see the principle behind Ô¨Årst .
Straightforwardly , if Fctis valid , ( h;r;t ) should be compatible with any of its auxiliary description .
Then , the values of their interaction vector , measuring the compatibility in many different views , are all encouraged to be large .
Therefore , for each dimension , the minimum over it of all the interaction vectors is not allowed to be too small .
Thus , the overall interaction vector ohrtav of ( h;r;t ) and its auxiliary description(s ) is : ohrtav = minm i=1(ohrta ivi ) ; ( 4 ) where min()is the element - wise minimizing function .
Then , similar to ‚Äú FCN 1 ‚Äù , we obtain the compatibility scorecompFctofFct : compFct=(ohrtavWcomp + bcomp);(5 ) where Wcomp of dimension d1andbcomp are the weight matrix and bias variable , respectively .
4.4 Final Score and Loss Function The Ô¨Ånal score sFctofFctis the weighted sum  of the above validity score and compatibility score : sFct = valhrtcompFct = wvalhrt+ ( 1 w)compFct;(6 ) wherew2(0;1)is the weight factor .
If the arity of Fctis 2 , the Ô¨Ånal score is equal to the validity score of the primary triple ( h;r;t ) .
Then , Equation ( 6 ) is reduced to : sFct = valhrt : ( 7 ) Currently , we obtain the Ô¨Ånal score sFctofFct .
In addition , Fcthas its target score lFct .
By comparingsFctwithlFct , we get the binary crossentropy loss : LFct= lFctlogsFct (1 lFct ) log(1 sFct);(8 ) wherelFct= 1 , ifFct2 T , otherwiseFct2T  , lFct= 0 .
Here , Tis the training set and T is the set of negative samples constructed by corrupting the n - ary facts in T. SpeciÔ¨Åcally , for each n - ary fact inT , we randomly replace one of its elements with a random element in E / Rto generate one negative sample not contained in T.
We then optimize NeuInfer via backpropagation , and Adam ( Kingma and Ba , 2015 ) with learning rateis used as the optimizer .
61465 Experiments 5.1 Datasets and Metrics We conduct experiments on two n - ary datasets .
The Ô¨Årst one is JF17 K ( Wen et al . , 2016 ; Zhang et
al . , 2018 ) , derived from Freebase ( Bollacker et al . , 2008 ) .
In JF17 K , an n - ary relation of a certain type is deÔ¨Åned by a Ô¨Åxed number of ordered attributes .
Then , any n - ary fact of this relation is denoted as an ordered sequence of attribute values corresponding to the attributes .
For example , for all n - ary facts of the n - ary relation olympics : olympic medalhonor , they all have four attribute values ( e.g. , 2008 Summer Olympics , United States , Natalie Coughlin , andSwimming at the 2008 SummerOlympics ‚Äì Women0s4100metre freestyle relay ) , corresponding to the four ordered attributes of this n - ary relation .
The second one is WikiPeople ( Guan et al . , 2019 ) , derived from Wikidata ( Vrande Àáci¬¥c and Kr ¬®otzsch , 2014 ) .
Its n - ary facts are more diverse than JF17 K ‚Äôs .
For example , for all n - ary facts that narrateaward -received , some have the attribute together -with , while some others do not .
Thus , WikiPeople is more difÔ¨Åcult .
To run NeuInfer on JF17 K and WikiPeople , we transform the representation of their n - ary facts .
For JF17 K , we need to convert each attribute value sequence of a speciÔ¨Åc n - ary relation to a primary triple coupled with a set of its auxiliary description(s ) .
The core of this process is to determine the primary triple , formed by merging the two primary attributes of the n - ary relation and the corresponding attribute values .
The two primary attributes are selected based on RAE ( Zhang et al . , 2018 ) .
For each attribute of the n - ary relation , we count the number of its distinct attribute values from all the n - ary facts of this relation .
The two attributes that correspond to the largest and second - largest numbers are chosen as the two primary attributes .
For WikiPeople , since there is a primary triple for each n - ary fact in Wikidata , with its help , we simply reorganize a set of attribute - value pairs in WikiPeople to a primary triple coupled with a set of its auxiliary description(s ) .
The statistics of the datasets after conversion or reorganization are outlined in Table 1 , where # Train , # Valid , and # Test are the sizes of the training set , validation set , and test set , respectively .
As for metrics , we adopt the standard Mean Re - DatasetjRjjEj#Train # V alid # Test JF17 K 501 28,645 76,379 - 24,568 WikiPeople 193 47,765 305,725 38,223 38,281 Table 1 : The statistics of the datasets .
ciprocal Rank ( MRR ) and Hits@ N.
For each n - ary test fact , one of its elements is removed and replaced by all the elements in E / R. These corrupted n - ary facts are fed into NeuInfer to obtain the Ô¨Ånal scores .
Based on these scores , the n - ary facts are sorted in descending order , and the rank of the n - ary test fact is stored .
Note that , except the nary test fact , other corrupted n - ary facts existing in the training / validation / test set , are discarded before sorting .
This process is repeated for all other elements of the n - ary test fact .
Then , MRR is the average of these reciprocal ranks , and Hits@ Nis the proportion of the ranks less than or equal to N. Knowledge inference includes entity inference and relation inference .
As presented in Table 1 , the number of relations and attributes in each dataset is far less than that of entities and attribute values ( on JF17K , jRj= 501 , whilejEj= 28;645 ; on WikiPeople , jRj= 193 , whilejEj= 47;765 ) .
That is , inferring a relation / attribute is much simpler than inferring an entity / attribute value .
Therefore , we adopt MRR and Hits@ f1 , 3 , 10gon entity inference , while pouring attention to more Ô¨Ånegrained metrics , i.e. , MRR and Hits@1 on relation inference .
5.2 Experimental Settings The hyper - parameters of NeuInfer are tuned via grid search in the following ranges : The embedding dimension k2f50;100 g , the batch size  2f128;256 g , the learning rate 2f5e 6 ; 1e 5;5e 5;1e 4;5e 4;1e 3 g , the numbers n1 andn2of the neural network layers of ‚Äú hrt - FCNs ‚Äù and ‚Äú hrtav - FCNs ‚Äù in f1;2 g , the dimension dof the interaction vector ohrta iviinf50;100;200;400 ; 500;800;1000;1200 g , the weight factor wof the scores inf0:1;0:2 ; : : : ; 0:9 g. The adopted optimal settings are : k= 100 ,  = 128 , = 5e 5 , n1= 2,n2= 1,d= 1200 , andw= 0:1for JF17K;k= 100 ,  = 128 , = 1e 4,n1= 1 , n2= 1,d= 1000 , andw= 0:3for WikiPeople .
5.3 Simple Knowledge Inference Simple knowledge inference includes simple entity inference and simple relation inference .
For an nary fact , they infer one of the entities / the relation in
6147MethodJF17 K WikiPeople MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10 RAE 0.310 0.219 0.334 0.504 0.172 0.102 0.182 0.320
NaLP 0.366 0.290 0.391 0.516 0.338 0.272 0.364 0.466 NeuInfer
0.517 0.436 0.553 0.675 0.350 0.282 0.381 0.467 Table 2 : Experimental results of simple entity inference .
the primary triple or the attribute value / attribute in an auxiliary description , given its other information .
5.3.1 Baselines Knowledge inference methods on n - ary facts are scarce .
The representative methods are mTransH ( Wen et al . , 2016 ) and its modiÔ¨Åed version RAE ( Zhang et al . , 2018 ) , and the state - of - the - art one is NaLP ( Guan et
al . , 2019 ) .
As m - TransH is worse than RAE , following NaLP , we do not adopt it as a baseline .
5.3.2 Simple Entity Inference
The experimental results of simple entity inference are reported in Table 2 .
From the results , it can be observed that NeuInfer performs much better than the best baseline NaLP , which veriÔ¨Åes the superiority of NeuInfer .
SpeciÔ¨Åcally , on JF17 K , the performance gap between NeuInfer and NaLP is signiÔ¨Åcant .
In essence , 0.151 on MRR , 14.6 % on Hits@1 , 16.2 % on Hits@3 , and 15.9 % on Hits@10 .
On WikiPeople , NeuInfer also outperforms NaLP .
It testiÔ¨Åes the strength of NeuInfer treating the information in the same n - ary fact discriminatingly .
By differentiating the primary triple from other auxiliary description(s ) , NeuInfer considers the validity of the primary triple and the compatibility between the primary triple and its auxiliary description(s ) to model each n - ary fact more appropriately and reasonably .
Thus , it is not surprising that NeuInfer beats the baselines .
And on simpler JF17 K ( see Section 5.1 ) , NeuInfer gains more signiÔ¨Åcant performance improvement than on WikiPeople .
5.3.3 Simple Relation Inference Since RAE is deliberately developed only for simple entity inference , we compare NeuInfer only with NaLP on simple relation inference .
Table 3 demonstrates the experimental results of simple relation inference .
From the table , we can observe that NeuInfer outperforms NaLP consistently .
Detailedly , on JF17 K , the performance improvement of NeuInfer on MRR and Hits@1 is 0.036 and 7.0 % , respectively ; on WikiPeople , they are 0.030and 9.1 % , respectively .
It is ascribed to the reasonable modeling of n - ary facts , which not only improves the performance of simple entity inference but also is beneÔ¨Åcial to pick the exact right relations / attributes out .
MethodJF17 K WikiPeople MRR Hits@1 MRR Hits@1 NaLP 0.825 0.762 0.735 0.595 NeuInfer 0.861 0.832 0.765 0.686 Table 3 : Experimental results of simple relation inference .
5.4 Ablation Study We perform an ablation study to look deep into the framework of NeuInfer .
If we remove the compatibility evaluation component , NeuInfer is reduced to a method for binary but not n - ary facts .
Since we handle knowledge inference on n - ary facts , it is inappropriate to remove this component .
Thus , as an ablation , we only deactivate the validity evaluation component , denoted as NeuInfer . The experimental comparison between NeuInfer and NeuInfer  is illustrated in Figure 2 .
It can be observed from the Ô¨Ågure that NeuInfer outperforms NeuInfer  signiÔ¨Åcantly .
It suggests that the validity evaluation component plays a pivotal role in our method .
Thus , each component of our method is necessary .
5.5 Flexible Knowledge Inference The newly proposed Ô¨Çexible knowledge inference focuses on n - ary facts of arities greater than 2 .
It includes Ô¨Çexible entity inference and Ô¨Çexible relation inference .
For an n - ary fact , they infer one of the entities / the relation in the primary triple given any number of its auxiliary description(s ) or infer the attribute value / attribute in an auxiliary description given the primary triple and any number of other auxiliary description(s ) .
In existing knowledge inference methods on n - ary facts , each n - ary fact is represented as a group of peer attributes and attribute values .
These methods have not poured attention to the above Ô¨Çexible knowledge inference .
Thus , we conduct this new type of task only on
6148 MRR Hits@1 Hits@3 Hits@10 Ablation study of simple entity inference on JF17K0.4000.5000.6000.700Scores0.517 0.4360.5530.675 0.433 0.3790.4650.529
MRR Hits@1 Hits@3 Hits@10 Ablation study of simple entity inference on WikiPeople0.0000.2000.4000.350 0.2820.3810.467 0.0500.0330.0550.085NeuInfer NeuInfer MRR Hits@1 Hits@3 Hits@10 Ablation study of simple relation inference on JF17K0.7000.8000.9000.861 0.8320.8860.904 0.710 0.7020.713 0.717 MRR Hits@1 Hits@3 Hits@10 Ablation study of simple relation inference on WikiPeople0.2500.5000.7501.000 0.765 0.6860.8280.897 0.2110.1830.209 0.229Figure 2 : The experimental comparison between NeuInfer and NeuInfer . DatasetFlexible entity inference Flexible relation inference MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 JF17 K
0.398 0.348
0.422 0.494 0.616 0.599 WikiPeople 0.200 0.161 0.208 0.276 0.477 0.416 Table 4 : Experimental results of Ô¨Çexible knowledge inference .
NeuInfer .
Before elaborating on the experimental results , let us look into the new test set used in this section Ô¨Årst .
5.5.1
The New Test Set We generate the new test set as follows : ‚Ä¢Collect the n - ary facts of arities greater than 2 from the test set .
‚Ä¢For each collected n - ary fact , compute all the subsets of the auxiliary description(s ) .
The primary triple and each subset form a new n - ary fact , which is added to the candidate set .
‚Ä¢Remove the n - ary facts that also exist in the training / validation set from the candidate set and then remove the duplicate n - ary facts .
The remaining n - ary facts form the new test set .
The size of the resulting new test set on JF17 K is 34,784 , and that on WikiPeople is 13,833 .
5.5.2 Flexible Entity and Relation Inference
The experimental results of Ô¨Çexible entity and relation inference on these new test sets are presented in Table 4 .
It can be observed that NeuInfer well tackles Ô¨Çexible entity and relation inference on partial facts , and achieves excellent performance .
We also attribute this to the reasonable modeling of n - ary facts .
For each n - ary fact , NeuInfer distinguishes the primary triple from other auxiliarydescription(s ) and models them properly .
Thus , NeuInfer well handles various types of entity and relation inference concerning the primary triple coupled with any number of its auxiliary description(s ) .
5.6 Performance under Different Scenarios To further analyze the effectiveness of the proposed NeuInfer method , we look into the breakdown of its performance on different arities , as well as on primary triples and auxiliary descriptions .
Without loss of generality , here we report only the experimental results on simple entity inference .
The test sets are grouped into binary and n - ary ( n>2 ) categories according to the arities of the facts .
Table 5 presents the experimental results of simple entity inference on these two categories of JF17 K and WikiPeople .
From the tables , we can observe that NeuInfer consistently outperforms the baselines on both categories on simpler JF17K.
On more difÔ¨Åcult WikiPeople , NeuInfer is comparable to the best baseline NaLP on the binary category and gains much better performance on the n - ary category in terms of the Ô¨Åne - grained MRR and Hits@1 .
In general , NeuInfer performs much better on JF17 K than on WikiPeople .
We attribute this to the simplicity of JF17K. Where does the above performance improvement come from ?
Is it from inferring the head / tail
6149Dataset MethodMRR Hits@1 Hits@3 Hits@10 Binary N - ary Binary N - ary Binary N - ary Binary N - ary JF17KRAE 0.115 0.397 0.050 0.294 0.108 0.434 0.247 0.618 NaLP 0.118 0.477 0.058 0.394 0.121 0.512 0.246 0.637 NeuInfer 0.267 0.628 0.173 0.554 0.300 0.666 0.462 0.770 WikiPeopleRAE 0.169 0.187 0.096 0.126 0.178 0.198 0.323 0.306 NaLP 0.351 0.283 0.291 0.187 0.374 0.322 0.465 0.471 NeuInfer 0.350 0.349 0.278 0.303 0.385 0.364 0.473 0.439 Table 5 : Experimental results of simple entity inference on binary and n - ary categories of JF17 K and WikiPeople .
Dataset MethodMRR Hits@1 Hits@3 Hits@10 Binary N - ary Overall Binary N - ary
Overall Binary N - ary Overall Binary N - ary
Overall JF17KNaLP 0.118 0.456 0.313
0.058 0.369 0.237 0.121 0.491 0.334 0.246 0.625 0.464 NeuInfer 0.267 0.551 0.431 0.173 0.467 0.342 0.300 0.588 0.466 0.462 0.720 0.611 WikiPeopleNaLP 0.351 0.237 0.337 0.291 0.161 0.276 0.374 0.262 0.361 0.465 0.384 0.455 NeuInfer 0.350 0.280
0.342 0.278 0.225 0.272 0.385 0.299 0.382 0.473 0.375 0.463 Table 6 : Detailed experimental results on inferring head / tail entities .
MethodJF17 K WikiPeople MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10 NaLP 0.510 0.432 0.545 0.655 0.345 0.223 0.402 0.589 NeuInfer 0.746 0.687 0.787 0.848 0.443 0.408 0.453 0.516 Table 7 : Experimental results on inferring attribute values .
entities in primary triples or the attribute values in auxiliary descriptions ?
To go deep into it , we study the performance of NeuInfer on inferring the head / tail entities and the attribute values and compare it with the best baseline NaLP .
The detailed experimental results are demonstrated in Tables 6 and 7 .
It can be observed that NeuInfer brings more performance gain on inferring attribute values .
It indicates that combining the validity of the primary triple and the compatibility between the primary triple and its auxiliary description(s ) to model each n - ary fact is more effective than only considering the relatedness of attribute - value pairs in NaLP , especially for inferring attribute values .
6 Conclusions In this paper , we distinguished the information in the same n - ary fact and represented each n - ary fact as a primary triple coupled with a set of its auxiliary description(s ) .
We then proposed a neural network model , NeuInfer , for knowledge inference on n - ary facts .
NeuInfer combines the validity evaluation of the primary triple and the compatibility evaluation of the n - ary fact to obtain the validity score of the n - ary fact .
In this way , NeuInfer has the ability of well handling simple knowledge inference , which copes with the inference on wholefacts .
Furthermore , NeuInfer is capable of dealing with the newly proposed Ô¨Çexible knowledge inference , which tackles the inference on partial facts consisting of a primary triple coupled with any number of its auxiliary descriptive attributevalue pair(s ) .
Experimental results manifest the merits and superiority of NeuInfer .
Particularly , on simple entity inference , NeuInfer outperforms the state - of - the - art method signiÔ¨Åcantly in terms of all the metrics .
NeuInfer improves the performance of Hits@3 even by 16.2 % on JF17K.
In this paper , we use only n - ary facts in the datasets to conduct knowledge inference .
For future works , to further improve the method , we will explore the introduction of additional information , such as rules and external texts .
Acknowledgments The work is supported by the National Key Research and Development Program of China under grant 2016YFB1000902 , the National Natural Science Foundation of China under grants U1911401 , 61772501 , U1836206 , 91646120 , and 61722211 , the GFKJ Innovation Program , Beijing Academy of ArtiÔ¨Åcial Intelligence ( BAAI ) under grant BAAI2019ZD0306 , and the Lenovo - CAS Joint Lab Youth Scientist Project .
6150References Kurt Bollacker , Colin Evans , Praveen Paritosh , Tim Sturge , and Jamie Taylor . 2008 .
Freebase :
A collaboratively created graph database for structuring human knowledge .
In Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data , pages 1247‚Äì1250 .
Antoine Bordes , Nicolas Usunier , Alberto GarciaDuran , Jason Weston , and Oksana Yakhnenko .
2013 .
Translating embeddings for modeling multirelational data .
In Proceedings of the 26th International Conference on Neural Information Processing Systems , pages 2787‚Äì2795 .
Mingyang Chen , Wen Zhang , Wei Zhang , Qiang Chen , and Huajun Chen . 2019 .
Meta relational learning for few - shot link prediction in knowledge graphs .
In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 4208‚Äì4217 , Hong Kong , China .
Tim Dettmers , Pasquale Minervini , Pontus Stenetorp , and Sebastian Riedel .
2018 .
Convolutional 2D knowledge graph embeddings .
In Proceedings of the 32nd AAAI Conference on ArtiÔ¨Åcial Intelligence , pages 1811‚Äì1818 .
Boyang Ding , Quan Wang , Bin Wang , and Li Guo .
2018 .
Improving knowledge graph embedding using simple constraints .
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , pages 110‚Äì121 .
Li Dong , Furu Wei , Ming Zhou , and Ke Xu . 2015 .
Question answering over Freebase with multicolumn convolutional neural networks .
In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics , pages 260‚Äì269 .
Takuma Ebisu and Ryutaro Ichise .
2018 .
TorusE :
Knowledge graph embedding on a Lie group .
In Proceedings of the 32nd AAAI Conference on ArtiÔ¨Åcial Intelligence , pages 1819‚Äì1826 .
Saiping Guan , Xiaolong Jin , Yuanzhuo Wang , and Xueqi Cheng .
2018 .
Shared embedding based neural networks for knowledge graph completion .
In Proceedings of the 27th ACM International Conference on Information and Knowledge Management , pages 247‚Äì256 .
Saiping Guan , Xiaolong Jin , Yuanzhuo Wang , and Xueqi Cheng .
2019 .
Link prediction on n - ary relational data .
In Proceedings of the 28th International Conference on World Wide Web , pages 583‚Äì593 .
Shu Guo , Quan Wang , Bin Wang , Lihong Wang , and Li Guo . 2015 .
Semantically smooth knowledge graph embedding .
In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics , pages 84‚Äì94.Prachi
Jain , Pankaj Kumar , Soumen Chakrabarti , et al . 2018 .
Type - sensitive knowledge base inference without explicit type supervision .
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics , pages 75‚Äì80 .
Guoliang Ji , Shizhu He , Liheng Xu , Kang Liu , and Jun Zhao . 2015 .
Knowledge graph embedding via dynamic mapping matrix .
In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics , pages 687‚Äì696 .
Yantao Jia , Yuanzhuo Wang , Hailun Lin , Xiaolong Jin , and Xueqi Cheng . 2016 .
Locally adaptive translation for knowledge graph embedding .
In Proceedings of the 30th AAAI Conference on ArtiÔ¨Åcial Intelligence , pages 992‚Äì998 .
Xiaotian Jiang , Quan Wang , and Bin Wang .
2019 .
Adaptive convolution for multi - relational learning .
InProceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 978‚Äì987 , Minneapolis , Minnesota .
Diederik Kingma and Jimmy Ba . 2015 .
Adam : A method for stochastic optimization .
In Proceedings of the 3rd International Conference for Learning Representations .
Yankai Lin , Zhiyuan Liu , Huanbo Luan , Maosong Sun , Siwei Rao , and Song Liu . 2015a .
Modeling relation paths for representation learning of knowledge bases .
InProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 705‚Äì714 .
Yankai Lin , Zhiyuan Liu , Maosong Sun , Yang Liu , and Xuan Zhu . 2015b .
Learning entity and relation embeddings for knowledge graph completion .
In Proceedings of the 29th AAAI Conference on ArtiÔ¨Åcial Intelligence , pages 2181‚Äì2187 .
Denis Lukovnikov , Asja Fischer , Jens Lehmann , and S¬®oren Auer .
2017 .
Neural network - based question answering over knowledge graphs on word and character level .
In Proceedings of the 26th International Conference on World Wide Web , pages 1211‚Äì1220 .
Deepak Nathani , Jatin Chauhan , Charu Sharma , and Manohar Kaul .
2019 .
Learning attention - based embeddings for relation prediction in knowledge graphs .
In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 4710‚Äì4723 , Florence , Italy .
Dai Quoc Nguyen , Tu Dinh Nguyen , Dat Quoc Nguyen , and Dinh Phung .
2018 .
A novel embedding model for knowledge base completion based on convolutional neural network .
In Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 327‚Äì333 .
6151Maximilian Nickel , Kevin Murphy , V olker Tresp , and Evgeniy Gabrilovich .
2016 .
A review of relational machine learning for knowledge graphs .
Proceedings of the IEEE , 104(1):11‚Äì33 .
Maximilian Nickel , V olker Tresp , and Hans - Peter Kriegel . 2011 .
A three - way model for collective learning on multi - relational data .
In Proceedings of the 28th International Conference on Machine Learning , pages 809‚Äì816 .
Fabian M Suchanek , Gjergji Kasneci , and Gerhard Weikum .
2007 .
Yago :
A core of semantic knowledge .
In Proceedings of the 16th International Conference on World Wide Web , pages 697‚Äì706 .
Yi Tay , Anh Tuan Luu , and Siu Cheung Hui . 2017 .
Non - parametric estimation of multiple embeddings for link prediction on dynamic knowledge graphs .
InProceedings of the 31st AAAI Conference on ArtiÔ¨Åcial Intelligence , pages 1243‚Äì1249 .
Th¬¥eo Trouillon , Johannes Welbl , Sebastian Riedel , Eric Gaussier , and Guillaume Bouchard . 2016 .
Complex embeddings for simple link prediction .
In Proceedings of the 33rd International Conference on Machine Learning , pages 2071‚Äì2080 .
Denny Vrande Àáci¬¥c and Markus Kr ¬®otzsch .
2014 .
Wikidata : A free collaborative knowledgebase .
Communications of the ACM , 57(10):78‚Äì85.Quan Wang , Zhendong Mao , Bin Wang , and Li Guo . 2017 .
Knowledge graph embedding : A survey of approaches and applications .
IEEE Transactions on Knowledge and Data Engineering , 29(12):2724 ‚Äì 2743 .
Zhen Wang , Jianwen Zhang , Jianlin Feng , and Zheng Chen .
2014 .
Knowledge graph embedding by translating on hyperplanes .
In Proceedings of the 28th AAAI Conference on ArtiÔ¨Åcial Intelligence , pages 1112‚Äì1119 .
Jianfeng Wen , Jianxin Li , Yongyi Mao , Shini Chen , and Richong Zhang .
2016 .
On the representation and embedding of knowledge bases beyond binary relations .
In Proceedings of the 25th International Joint Conference on ArtiÔ¨Åcial Intelligence , pages 1300‚Äì1307 .
Han Xiao , Minlie Huang , and Xiaoyan Zhu . 2016 .
TransG :
A generative model for knowledge graph embedding .
In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics , pages 2316‚Äì2325 .
Richong Zhang , Junpeng Li , Jiajie Mei , and Yongyi Mao . 2018 .
Scalable instance reconstruction in knowledge bases via relatedness afÔ¨Åliated embedding .
In Proceedings of the 27th International Conference on World Wide Web , pages 1185‚Äì1194 .
