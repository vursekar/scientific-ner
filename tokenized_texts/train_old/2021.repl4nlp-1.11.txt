Proceedings of the 6th Workshop on Representation Learning for NLP ( RepL4NLP-2021 ) , pages 90–99 Bangkok , Thailand ( Online ) , August 6 , 2021 .
© 2021 Association for Computational Linguistics90Revisiting Pretraining with Adapters Seungwon Kim , Alex Shum , Nathan Susanj , Jonathan Hilgart Georgia Institute of Technology fskim3222 , ashum7 , nsusanj3 , jhilgart3 g@gatech.edu
Abstract Pretrained language models have served as the backbone for many state - of - the - art NLP results .
These models are large and expensive to train .
Recent work suggests that continued pretraining on task - speciﬁc data is worth the effort as pretraining leads to improved performance on downstream tasks .
We explore alternatives to full - scale task - speciﬁc pretraining of language models through the use of adapter modules , a parameter - efﬁcient approach to transfer learning .
We ﬁnd that adapter - based pretraining is able to achieve comparable results to task - speciﬁc pretraining while using a fraction of the overall trainable parameters .
We further explore direct use of adapters without pretraining and ﬁnd that the direct ﬁnetuning performs mostly on par with pretrained adapter models , contradicting previously proposed beneﬁts of continual pretraining in full pretraining ﬁne - tuning strategies .
Lastly , we perform an ablation study on task - adaptive pretraining to investigate how different hyperparameter settings can change the effectiveness of the pretraining .
1 Introduction Pretrained Language Models ( PLM ) are predominant in tackling current Natural Language Processing ( NLP ) tasks .
Most PLMs based on the Transformer architecture ( Vaswani et al . , 2017 ) are ﬁrst trained on massive text corpora with the selfsupervised objective to learn word representations ( Devlin et al . , 2019 ; Liu et al . , 2019 ) , and then are ﬁne - tuned for a speciﬁc target task .
The pretraining and ﬁne - tuning of PLMs achieves state - ofthe - art ( SOTA ) performance in many NLP tasks .
Inspired by the beneﬁts of pretraining , there have been studies demonstrate the effects of continued pretraining on the domain of a target task or the target task dataset ( Mitra et al . , 2020 ; Han and Eisenstein , 2019 ; Gururangan et al . , 2020 ) .
Gururangan et
al . , 2020 adapt PLMs on the target taskby further pretraining RoBERTa ( Liu et al . , 2019 ) on the target text corpus before it is ﬁne - tuned for the corresponding task and showed that this task adaptation consistently improves the performance for text classiﬁcation tasks .
However , this full process of pretraining and then ﬁne - tuning can be parameter inefﬁcient for recent PLMs that have millions or billions of parameters ( Devlin et al . , 2019 ; Radford et al . , 2018 ) .
This parameter inefﬁciency becomes even worse when one continues pre - training all the parameters of PLMs on the task - speciﬁc corpus .
Furthermore , recent PLMs need more than 100s of MB to store all the weights ( Liu et al . , 2019 ; Radford et al . , 2018 ) , making it difﬁcult to download and share the pre - trained models on the ﬂy .
Recently , adapters have been proposed as an alternative approach to decrease the substantial number of parameters of PLMs in the ﬁne - tuning stage ( Houlsby et al . , 2019 ) .
Finetuning with adapters mostly matches the performance of those with the full ﬁne - tuning strategy on many NLP tasks including GLUE benchmark ( Wang et al . , 2018 ) and reduces the size of the model from 100s of MB to the order of MB ( Pfeiffer et al . , 2020b ) .
As such , a natural question arises from the successes of the adapter approach : can the adapter alone adapt PLMs to the target task when it is used in the second phase of the pretraining stage and thus lead to the improvement of the performance on the corresponding task ?
In this paper , we explore task - adaptive pretraining , termed TAPT ( Gururangan et al . , 2020 ) , with adapters to address this question and overcome the limitations of the conventional full pretraining and ﬁne - tuning .
We only train the adapter modules in the second phase of pretraining as well as the ﬁne - tuning stage to achieve both parameter efﬁciency and the beneﬁts of continual pretraining and compare those with the adapter - based model without pretraining .
Surprisingly , we ﬁnd that directly
91ﬁne - tuning adapters performs mostly on par with the pre - trained adapter model and outperforms the full TAPT , contradicting the previously proposed beneﬁts of continual pretraining in the full pretraining ﬁne - tuning scheme .
As directly ﬁne - tuning adapters skips the second phase of pretraining and the training steps of adapters are faster than those of the full model , it substantially reduces the training time .
We further investigate different hyperparameter settings that affect the effectiveness of pretraining . 2 Pretraining and Adapters Pre - trained language model We use RoBERTa ( Liu et al . , 2019 ) , a Transformer - based language model that is pre - trained on a massive text corpus , following Gururangan et al . , 2020 .
RoBERTa is an extension of BERT ( Devlin et
al . , 2019 ) with optimized hyperparameters and a modiﬁcation of the pretraining objective , which excludes next sentence prediction and only uses the randomly masked tokens in the input sentence .
To evaluate the performance of RoBERTa on a certain task , a classiﬁcation layer is appended on top of the language model after the pretraining and all the parameters in RoBERTa are trained in a supervised way using the label of the dataset .
In this paper , training word representations using RoBERTa on a masked language modeling task will be referred to as pretraining .
Further , taking this pretrained model and adding a classiﬁcation layer with additional updates to the language model parameters will be referred to as ﬁne - tuning .
Task - adaptive pretraining ( TAPT )
Although RoBERTa achieves strong performance by simply ﬁne - tuning the PLMs on a target task , there can be a distributional mismatch between the pretraining and target corpora .
To address this issue , pretraining on the target task or the domain of the target task can be usefully employed to adapt the language models to the target task and it further improves the performance of the PLMs .
Such methods can be referred to as Domain - Adaptive Pretraining ( DAPT ) or Task Adaptive - Pretraining ( TAPT ) ( Gururangan et al . , 2020 ) .
In this paper , we limit the scope of our works to TAPT as domain text corpus is not always available for each task , whereas TAPT can be easily applied by directly using the dataset of the target task while its performance often matches with DAPT ( Gururangan et al . , 2020 ) .
In TAPT , the second phase of pretraining is perFigure 1 : The adapter achitecture in the Transformer layer ( Pfeiffer et al . , 2020a ) formed with RoBERTa using the unlabeled text corpus of the target task , and then it is ﬁne - tuned on the target task .
Adapter Adapter modules have been employed as a feature extractor in computer vision ( Rebufﬁ et al . , 2017 ) and have been recently adopted in the NLP literature as an alternative approach to fully ﬁne - tuning PLMs .
Adapters are sets of new weights that are typically embedded in each transformer layer of PLMs and consist of feed - forward layers with normalizations , residual connections , and projection layers .
The architectures of adapters vary with respect to the different conﬁguration settings .
We use the conﬁguration proposed by Pfeiffer et al . , 2020a in Figure 1 , which turned out to be effective on diverse NLP tasks , and add the adapter layer to each transformer layer .
Pfeiffer et al . , 2020c use two types of adapter : language - speciﬁc adapters and taskspeciﬁc adapters for cross - lingual transfer .
These two types of adapter modules have similar architecture as in Figure 1 .
However , the language adapters involve invertible adapters after the embedding layer to capture token - level language representation when those are trained via masked language modeling in the pretraining stage , whereas the task adapters are simply embedded in each transformer layer and trained in the ﬁne - tuning stage to learn the task representation .
Following Pfeiffer et al . , 2020c , we employ language adapter modules with invertible adapter layers to perform pretraining adapters on the unlabeled target dataset .
However , we perform ﬁne - tuning pre - trained parameters of the language adapter modules for evaluation to align with
92Domain Task Label type Number of inst ( Train / Dev / Test )
Classes
Biomedical CHEMPROT Relationship classiﬁcation 4169 / 2427 / 3469 13 Biomedical RCT Abstract sentence roles 18040 / 30212 / 30135 5 Computer Science ACL - ARC Citation intent 1688 / 114 / 139 6 Computer Science SCIERC Relation classiﬁcation 3219 / 455 / 974 7 News HYPERPARTISAN Partisanship 515 / 65 / 65 2 News AGNEWS Topic 115000
/ 5000 / 7600 4 Reviews HELPFULNESS Review helpfulness 115251 / 5000 / 25000 2 Reviews IMDB Review sentiment 20000 / 5000 / 25000 2 Table 1 : Datasets used for experimentation .
Datasets include both high - resource ( RCT ( Dernoncourt and Lee , 2017 ) , AGNEWS ( Zhang et al . , 2015 ) , HELPFULNESS ( McAuley et al . , 2015 ) , IMDB ( Maas et al . , 2011 ) ) and low - resource ( CHEMPROT ( Kringelum et al . , 2016 ) , ACL - ARC ( Jurgens et al . , 2018 ) , SCIERC ( Luan et al . , 2018 ) , HYPERPARTISAN ( Kiesel et al . , 2019 ) settings .
TAPT , whereas Pfeiffer et al . , 2020c employ both the language and the task adapters by stacking task adapters on top of the language adapters .
3 Experiments We now propose an adapter - based approach that is a parameter efﬁcient variant of Task - Adaptive Pretraining ( TAPT ) and measure the margin of the performance between the pre - trained adapter model and the adapter model without pretraining .
For pretraining adapters , we added the adapter module in each transformer layer of RoBERTa using adaptertransformer ( Pfeiffer et al . , 2020b)1and continued pretraining all the weights in adapter layers on target text corpus while keeping the original parameters in RoBERTa ﬁxed .
After ﬁnishing the second phase of pretraining , we performed ﬁne - tuning of RoBERTa by training the weights in the adapters and the ﬁnal classiﬁcation layers while keeping all of the parameters in RoBERTa frozen .
3.1 Dataset Following Gururangan
et al . , 20202 , we consider 8 classiﬁcation tasks from 4 different domains .
The speciﬁcation of each task is shown in Table 1 .
We covered news and review texts that are similar to the pretraining corpus of RoBERTa as well as scientiﬁc domains in which text corpora can have largely different distributions from those of RoBERTa .
Furthermore , the pretraining corpora of the target tasks include both large and small cases to determine whether the adapter - based approach can be applicable in both low and high - resource settings .
1https://github.com/Adapter-Hub/ adapter - transformers 2Downloadble link for task dataset : https://github . com / allenai / dont - stop - pretraining3.2 Implementation Details Our implementation is based on HuggingFace since we found AllenNLP ( Gardner et al . , 2018 ) used in Gururangan et al . , 2020 is incompatible with adapter - transformer ( Pfeiffer et al . , 2020b ) .
We follow the hyperparameters setting in Gururangan et al . , 2020 , and each model in the pretraining and ﬁne - tuning stage is trained on a single GPU ( NVIDIA RTX 3090 ) .
Details of hyperparameters are described in
Appendix A. Note that for the pretraining step , we use a batch size of 8 and accumulate the gradient for every 32 steps to be consistent with the hyperparameter setting in Gururangan et al . , 2020 .
We perform pretraining with the self - supervised objectives , which are randomly masked tokens , with a probability of 15 % for each epoch and we do not apply validation to pretraining and save the model at the end of the training from a single seed .
For TAPT , we train the entire parameters of the RoBERTa via masked language modeling ( MLM ) on the target dataset , whereas for the adapter - based model , we embed the language adapters in each transformer layer and add invertible adapters after the embedding layers to perform MLM while freezing the original parameters of RoBERTa , following Pfeiffer et al . , 2020c .
Fine - tuning step is straightforward .
We perform ﬁne - tuning parameters that are pretrained via MLM for both TAPT and the adapter model .
Validation is performed after each epoch and the best checkpoint is loaded at the end of the training to evaluate the performance on the test set .
3.3 Experimental setup Experiments cover four different models .
First , we reproduce the performance of RoBERTa and TAPT in Gururangan et al . , 2020 as presented in Appendix C. Then we proceed to the adapter - based approach .
93Dataset Baseline RoBERTa TAPT Adapter w/o PT Adapter w/ PT CHEMPROT 81.9 1:0 82.6 0:4 82.69 0:4 82.71 0:4 RCT 87.2 0:1 87.7 0:1 87.35 0:04 87.4 0:1
ACL - ARC 63.0 5:8 67.4 1:8 69.47 2:4 69.25 2:5 SCIERC 77.3 1:9 79.3 1:5 81.5 0:9 82.37 1:0 HYPERPARTISAN 86.6 0:9 90.4 5:2 93.01 4:7 84.97 6:4 AGNEWS 93.9 0:2 94.5 0:1 94.00 0:1 93.94 0:1 HELPFULNESS 65.1 3:4 68.5 1:9 70.96 0:6 70.83 0:8 IMDB 95.0 0:2 95.5 0:1 95.51 0:1
95.57 0:1
Average F1 81.3 83.24 84.31 83.38 Trainable params per task ( PT / FT ) -/124.64
M 163.35M/124.64 M -/1.78 M 2.18M/2.08 M Ratio to total params ( PT / FT ) -/100 % 100 %
/100 %
-/1.42 % 1.32%/1.65 % Relative training speed ( PT / FT ) -/1.0 1.0/1.0 -/1.29 1.14/1.24 Relative inference speed ( PT / FT ) -/1.0 1.0/1.0 -/0.98 0.88/0.98 Table 2 : Average F1score with standard deviation on test set .
Each score is averaged over 5 random seeds .
Evaluation metric is macro- F1scores on test set for each task except for CHMEPROT and RCT which use microF1 .
We report the results of baseline RoBERTa and TAPT from Gururangan et al . , 2020 .
Following R ¨uckl´e et al . , 2020 , we measure the average relative speed for the training and the inference time across all tasks except for the the inference speed in ﬁne - tuning stage , which excludes low - resource tasks .
PT and FT indicate pretraining and ﬁne - tuning respectively .
To investigate the beneﬁts of task - adaptive pretraining with adapters , we compare the performance of the pre - trained adapter model with the model without pretraining , i.e. , directly ﬁne - tuning adapters in RoBERTa on the target task .
For the adapter - based approach , we compare the adapter - based model with the second phase of pretraining and the model without the pretraining .
Since the weights of the adapters are randomly initialized , we empirically found that a larger learning rate worked well compared to the full ﬁne - tuning experiments .
We sweep the learning rates in f2e-5 , 1e-4 , 3e-4 , 6e-4 gand the number of epochs in f10 , 20gon the validation set and report the test score that performs the best on the validation set .
3.4 Results The results are summarized in Table 2 .
Surprisingly , for the average F1score , the adapter - based model without task - adaptive pretraining performs best , followed by the other adapter with the pretraining model , TAPT , and the baseline RoBERTa .
Except for Hyperpartisan news , the adapter model without pretraining performs mostly on par with the counterpart adapter model that involves pretraining on target text corpus , suggesting that the beneﬁts of additional task - adaptive pretraining diminish when we use the adapter - based approach .
Furthermore , directly ﬁne - tuned adapter model only trains 1.42 % of the entire parameters which leads to the 30 % faster - training step than the full model and skips the pretraining stage that typically expensive to train than the ﬁne - tuning , substantially reducing Figure 2 : F1score as a function of learning rate on test set with log scale on x - axis .
F1score is averaged over 5 random seeds for low - resource tasks ( CHEMPROT , ACL - ARC , SCIERC , HYPER ) due to the high variance .
For high - resource tasks ( RCT , AGNEWS , HELPFULNESS , IMDB ) , we report the F1score from a single random seed for each task .
For RoBERTa and TAPT , we follow the hyper - parameter settings in Gururangan et al . , 2020 except for the learning rate .
the training time while the relative speed for the inference only decreases by 2 % to the full model .
3.5 Analysis We analyze how the adapter alone can surpass or perform on par with both the full model and adapter model with task - adaptive pretraining .
Since we sweep the learning rates and the number of epochs in the range that includes larger ﬁgures compared to those in the full model when ﬁne - tuning adapters and kept the other hyper - parameters the same as in Gururangan et al . , 2020 , we hypothesize that
94Dataset Baseline RoBERTa TAPT CHEMPROT 82.8 0:9 82.62 0:5 RCT 86.89 0:1 87.4 0:2 ACL - ARC 69.24 2:6 70.08 2:3 SCIERC 80.59 0:9 81.28 1:2 HYPER 94.53 2:0 86.17 1:3 AGNEWS 93.9 0:2 94.05 0:1 HELPFUL 69.63 0:6 71.28 0:8 IMDB 94.93 0:1 95.33 0:1 Average F1 84.06 83.52 Table 3 : Best performance of baseline RoBERTa and TAPT ( Gururangan et al . , 2020 ) on our implementation .
Each score is averaged over 5 random seeds .
Best conﬁguration settings for each task is described in Appendix Table 8 .
the larger learning rate zeroes out the beneﬁts of pretraining .
Figure 2 . shows the average F1score across all tasks as a function of learning rate .
The adapter model without a second phase of pretraining consistently outperforms or performs on par with the adapter model with pretraining from 1e-4 to 6e-4 , demonstrating that the additional pretraining turns out to be ineffective .
In contrast , TAPT outperforms baseline RoBERTa from 2e-5 , where both TAPT and baseline RoBERTa perform best .
The results show that different learning rates used in the ﬁne - tuning stage can affect the effectiveness of pretraining and demonstrate that directly ﬁne - tuning a fraction of parameters can provide comparable performance to the full - model as well as the adapter model with pretraining while substantially reducing the training time .
Inspired by the results of the adapter models , we perform the same experiments for the full model ( baseline RoBERTa and TAPT ) on our implementation by sweeping the learning rates and the number of epochs .
We hypothesize that proper hyperparameter settings such as a larger learning rate or increasing the number of training steps in the ﬁne - tuning stage can improve the performance of baseline RoBERTa , making pretraining on the unlabeled target task less effective .
We sweep the learning rates in f1e-5 , 2e-5 , 3e-5 gand the number of epochs in f10 , 20 gon the validation set and report the test score that performs the best on the validation set .
Table 3 shows the best performance of the full models for each task among different hyper - parameter settings .
The average F1score of baseline RoBERTa greatly increases and surprisingly , it surpasses the performance of TAPT in some tasks .
The results ensure that although pretraining PLMs on the target task results in betterperformance , one can achieve comparable performance by simply using a larger learning rate or increasing training steps in the ﬁne - tuning stage while skipping the pretraining step that is computationally demanding compared to the ﬁne - tuning .
4 Conclusion Our work demonstrates that adapters provide a competitive alternative to large - scale task - adaptive pretraining for NLP classiﬁcation tasks .
We show that it is possible to achieve similar performance to TAPT with pretraining training just 1.32 % of the parameters through pretraining with adapters .
However , the most computationally efﬁcient option is to skip pretraining and only perform ﬁne - tuning with adapters .
We found that skipping pretraining altogether and just ﬁne - tuning with adapters outperforms or performs mostly on par with TAPT and the adapter model with pretraining across our tasks while substantially reducing the training time .
Acknowledgments We would like to thank Zsolt Kira , Mandeep Baines , Shruti Bhosale , and Siddharth Goyal for helpful feedback and suggestions .
We also would like to thank anonymous reviewers for their insightful comments on the earlier version of the paper .
References Franck Dernoncourt and Ji Young Lee . 2017 .
PubMed 200k RCT : a dataset for sequential sentence classiﬁcation in medical abstracts .
In Proceedings of the Eighth International Joint Conference on Natural Language Processing ( Volume 2 : Short Papers ) , pages 308–313 , Taipei , Taiwan .
Asian Federation of Natural Language Processing .
Jacob Devlin , Ming - Wei Chang , Kenton Lee , and Kristina Toutanova . 2019 .
BERT : Pre - training of deep bidirectional transformers for language understanding .
In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 1 ( Long and Short Papers ) , pages 4171–4186 , Minneapolis , Minnesota .
Association for Computational Linguistics .
Matt Gardner , Joel Grus , Mark Neumann , Oyvind Tafjord , Pradeep Dasigi , Nelson F. Liu , Matthew Peters , Michael Schmitz , and Luke Zettlemoyer .
2018 .
AllenNLP : A deep semantic natural language processing platform .
In Proceedings of Workshop for NLP Open Source Software ( NLP - OSS ) , pages 1 – 6 , Melbourne , Australia .
Association for Computational Linguistics .
95Suchin Gururangan , Ana Marasovi ´ c , Swabha Swayamdipta , Kyle Lo , Iz Beltagy , Doug Downey , and Noah A. Smith .
2020 .
Do n’t stop pretraining : Adapt language models to domains and tasks .
In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics , pages 8342–8360 , Online .
Association for Computational Linguistics .
Xiaochuang Han and Jacob Eisenstein .
2019 .
Unsupervised domain adaptation of contextualized embeddings for sequence labeling .
In EMNLP .
Neil Houlsby , Andrei Giurgiu , Stanislaw Jastrzebski , Bruna Morrone , Quentin De Laroussilhe , Andrea Gesmundo , Mona Attariyan , and Sylvain Gelly .
2019 .
Parameter - efﬁcient transfer learning for NLP .
InProceedings of the 36th International Conference on Machine Learning , volume 97 of Proceedings of Machine Learning Research , pages 2790–2799 .
PMLR .
David Jurgens , Srijan Kumar , Raine Hoover , Dan McFarland , and Dan Jurafsky .
2018 .
Measuring the evolution of a scientiﬁc ﬁeld through citation frames .
Transactions of the Association for Computational Linguistics , 6:391–406 .
Johannes Kiesel , Maria Mestre , Rishabh Shukla , Emmanuel Vincent , Payam Adineh , David Corney , Benno Stein , and Martin Potthast .
2019 .
SemEval2019 task 4 : Hyperpartisan news detection .
In Proceedings of the 13th International Workshop on Semantic Evaluation , pages 829–839 , Minneapolis , Minnesota , USA .
Association for Computational Linguistics .
Jens Kringelum , Sonny Kim Kjaerulff , Søren Brunak , Ole Lund , Tudor I Oprea , and Olivier Taboureau .
2016 .
Chemprot-3.0 : a global chemical biology diseases mapping .
Database , 2016 .
Yinhan Liu , Myle Ott , Naman Goyal , Jingfei Du , Mandar Joshi , Danqi Chen , Omer Levy , Mike Lewis , Luke Zettlemoyer , and Veselin Stoyanov .
2019 .
Roberta : A robustly optimized bert pretraining approach .
arXiv preprint arXiv:1907.11692 .
Yi Luan , Luheng He , Mari Ostendorf , and Hannaneh Hajishirzi .
2018 .
Multi - task identiﬁcation of entities , relations , and coreference for scientiﬁc knowledge graph construction .
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing , pages 3219–3232 , Brussels , Belgium . Association for Computational Linguistics .
Andrew L. Maas , Raymond E. Daly , Peter T. Pham , Dan Huang , Andrew Y .
Ng , and Christopher Potts . 2011 .
Learning word vectors for sentiment analysis .
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics : Human Language Technologies , pages 142–150 , Portland , Oregon , USA . Association for Computational Linguistics .
Julian McAuley , Christopher Targett , Qinfeng Shi , and Anton Van Den Hengel . 2015 .
Image - based recommendations on styles and substitutes .
In Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval , pages 43–52 .
Arindam Mitra , Pratyay Banerjee , Kuntal Kumar Pal , Swaroop Ranjan Mishra , and Chitta Baral .
2020 .
Exploring ways to incorporate additional knowledge to improve natural language commonsense question answering . arXiv:1909.08855v3 .
Jonas Pfeiffer , Aishwarya Kamath , Andreas R ¨uckl´e , Kyunghyun Cho , and Iryna Gurevych .
2020a .
Adapterfusion : Non - destructive task composition for transfer learning .
arXiv preprint arXiv:2005.00247 .
Jonas Pfeiffer , Andreas R ¨uckl´e , Clifton Poth , Aishwarya Kamath , Ivan Vuli ´ c , Sebastian Ruder , Kyunghyun Cho , and Iryna Gurevych .
2020b .
AdapterHub :
A framework for adapting transformers .
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 46–54 , Online .
Association for Computational Linguistics .
Jonas Pfeiffer , Ivan Vuli ´ c , Iryna Gurevych , and Sebastian Ruder .
2020c .
MAD - X : An Adapter - Based Framework for Multi - Task Cross - Lingual Transfer .
InProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 7654–7673 , Online .
Association for Computational Linguistics .
Alec Radford , Karthik Narasimhan , Tim Salimans , and Ilya Sutskever . 2018 .
Improving language understanding by generative pre - training .
Sylvestre - Alvise Rebufﬁ , Hakan Bilen , and Andrea Vedaldi .
2017 .
Learning multiple visual domains with residual adapters .
In Advances in Neural Information Processing Systems , volume 30 , pages 506 – 516 .
Curran Associates , Inc.
Andreas R ¨uckl´e , Gregor Geigle , Max Glockner , Tilman Beck , Jonas Pfeiffer , Nils Reimers , and Iryna Gurevych . 2020 .
Adapterdrop :
On the efﬁciency of adapters in transformers .
arXiv preprint arXiv:2010.11918 .
Ashish Vaswani , Noam Shazeer , Niki Parmar , Jakob Uszkoreit , Llion Jones , Aidan N Gomez , Ł ukasz Kaiser , and Illia Polosukhin . 2017 .
Attention is all you need .
In Advances in Neural Information Processing Systems , volume 30 , pages 5998–6008 .
Curran Associates , Inc.
Alex Wang , Amanpreet Singh , Julian Michael , Felix Hill , Omer Levy , and Samuel Bowman .
2018 .
GLUE :
A multi - task benchmark and analysis platform for natural language understanding .
In Proceedings of the 2018 EMNLP Workshop BlackboxNLP : Analyzing and Interpreting Neural Networks for NLP , pages 353–355 , Brussels , Belgium .
Association for Computational Linguistics .
96Xiang Zhang , Junbo Zhao , and Yann LeCun . 2015 .
Character - level convolutional networks for text classiﬁcation .
In Advances in Neural Information Processing Systems , volume 28 , pages 649–657 .
Curran Associates , Inc.
A Hyperparameter Details Details of hyperparameter setting including the learning rates for the best performing results are provided in Table 4 , 5 , and 6 . B Validation Results
We present validation performance in Table 7 and Figure 3 and 8 .
C Replication results We provide replication results of Gururangan et al . , 2020 in Table 9 .
97Hyper - parameter Value Optimizer Adam Adam epsilon 1e-8 , 0.999 Learning rate 1e-4 Batch size 8 Gradient accumulation step 32 Epochs 40 or 100 Adapter reduction factor 12 Maximum sequence length 512 Table 4 : Details of hyperparameters used in pretraining experiments .
We used 40 number of epochs for HELPFULNESS and 100 for the other tasks .
Hyper - parameter Value Optimizer Adam Adam epsilon 1e-8 , 0.999 Batch size 16 Gradient accumulation step 1 Epochs 10 or 20 Patience 3 or 5 Adapter reduction factor 12 Dropout 0.1 Feedforward layer 1
Feedforward nonlinearity tanh Classiﬁcation layer 1
Learning rate see Table 6 Learning rate decay linear
Warmup proportion 0.06 Maximum sequence length 512 Table 5 : Details of hyperparameters used in ﬁne - tuning experiments .
For baseline RoBERTa and TAPT , we used 10 number of epochs with patience of 3 and the learning rate of 2e-5 .
For adapter experiments , see Table 6 .
Dataset Adapter w/o PT ( LR , Epochs , Patience )
Adapter w/ PT ( LR , Epochs , Patience ) CHEMPROT 3e-4 , 20 , 5 6e-4 , 20 , 5 RCT 1e-4 , 10 , 3 1e-4 , 10 , 3 ACL - ARC 6e-4 , 10 , 3 6e-4 , 20 , 5 SCIERC 3e-4 , 20 , 5 6e-4 , 20 , 5 HYPER 3e-4 , 20 , 5 1e-4 , 20 , 5 AGNEWS 1e-4 , 10 , 3 1e-4 , 10 , 3 HELPFUL 3e-4 , 20 , 5 1e-4 , 20 , 5 IMDB 1e-4 , 10 , 3 1e-4 , 10 , 3 Table 6 : Learning rate , the nubmer of epochs and patience for best - performing models .
For adapter experiments , we sweep the learning rates in f1e-4 , 3e-4 , 6e-4 g , the number of epochs in f10 , 20 g , and patience factor in f3 , 5 g on validation set .
98Dataset Adapter w/o pretraining Adapter w/ pretraining CHEMPROT 83.77 0:5 84.02 0:7 RCT 88.16 0:1 88.13 0:1 ACL - ARC 72.41 2:2 77.31 2:9 SCIERC 86.86 0:5 87.87 0:3 HYPER 86.33 1:4 86.00 3:5 AGNEWS 94.28 0:1 94.57 0:1 HELPFUL 70.83 1:2 70.8 0:7 IMDB 95.52 0:1 95.6 0:1
Average F1 84.77 85.54 Table 7 : Validation performance of adapter experiments .
Each score is averaged over 5 random seeds .
Evaluation metric is macro- F1scores for each task except for CHMEPROT and RCT which use micro- F1 .
Figure 3 : F1score as a function of learning rate on development setwith log scale on x - axis .
F1score is averaged over 5 random seeds for low - resource tasks ( CHEMPROT , ACL - ARC , SCIERC , HYPER ) due to the high variance .
For high - resource tasks ( RCT , AGNEWS , HELPFULNESS , IMDB ) , we report the F1score from a single random seed for each task .
Here we sweep the learning rates in f1e-4 , 3e-4 , 6e-4 g , the number of epochs in f10 , 20 g , and the patience factor in f3 , 5 g. Dataset Baseline RoBERTa TAPT Hyper - parameters ( LR , Epochs , Patience ) CHEMPROT 82.8 0:9 82.62 0:5 3e-5 , 20 , 5 RCT 86.89 0:1 87.4 0:2 2e-5 , 10 , 3 ACL - ARC 69.24 2:6 70.08 2:3 3e-5 , 20 , 5 SCIERC 80.59 0:9 81.28 1:2 2e-5 , 20 , 5 HYPER 94.53 2:0 86.17 1:3 3e-5 , 10 , 3 AGNEWS 93.9 0:2 94.05 0:1 2e-5 , 10 , 3 HELPFUL 69.63 0:6 71.28 0:8 2e-5 , 20 , 5 IMDB 94.93 0:1 95.33 0:1 2e-5 , 20 , 5 Average F1 84.06 83.52 Table 8 : Validation performance of Baseline RoBERTa and TAPT experiments that corresponds to Table 3 .
Each score is averaged over 5 random seeds .
99Original Results Original Results Our Results Our Results Dataset Baseline RoBERTa TAPT Baseline RoBERTa TAPT CHEMPROT 81.9 1:0 82.6 0:4 81.64 0:8 82.58 0:5 RCT 87.2 0:1 87.7 0:1 86.89 0:1 87.4 0:2
ACL - ARC 63.0 5:8 67.4 1:8 64.12 5:5 66.11 4:6 SCIERC 77.3 1:9 79.3 1:5 78.89 2:7 79.94 0:7 HYPER 86.6 0:9 90.4 5:2 85.03 6:0 91.56 2:5 AGNEWS 93.9 0:2 94.5 0:1 93.72 0:2 94.05 0:1 HELPFULNESS 65.1 3:4 68.5 1:9 69.2 1:4 71.24 0:7 IMDB 95.0 0:2 95.5 0:1 95.15 0:1 95.33 0:1 Average F1 81.3 83.24 81.83 83.53 Table 9 : Reproducing Baseline RoBERTa and TAPT Results , average F1Scores with standard deviation .
F1score is averaged over 5 random seeds .
We use the same hyper - parameters in Gururangan et al . , 2020 .
