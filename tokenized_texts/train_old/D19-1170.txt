Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing , pages 1596–1606 , Hong Kong , China , November 3–7 , 2019 .
c  2019 Association for Computational Linguistics1596A
Multi - Type Multi - Span Network forReading Comprehension that Requires Discrete ReasoningMinghao Hu , Yuxing Peng , Zhen Huang , Dongsheng LiNational University of Defense Technology , Changsha , China{huminghao09,pengyuxing , huangzhen , dsli}@nudt.edu.cnAbstractRapid progress has been made in the ﬁeld ofreading comprehension and question answer - ing , where several systems have achieved hu - man parity in some simpliﬁed settings .
How - ever , the performance of these models de - grades signiﬁcantly when they are appliedto more realistic scenarios , where answersare involved with various types , multiple textstrings are correct answers , or discrete rea - soning abilities are required .
In this paper , we introduce the Multi - Type Multi - Span Net - work ( MTMSN ) , a neural reading comprehen - sion model that combines a multi - type answerpredictor designed to support various answertypes ( e.g. , span , count , negation , and arith - metic expression ) with a multi - span extractionmethod for dynamically producing one or mul - tiple text spans .
In addition , an arithmeticexpression reranking mechanism is proposedto rank expression candidates for further con-ﬁrming the prediction .
Experiments show thatour model achieves 79.9 F1 on the DROP hid - den test set , creating new state - of - the - art re - sults .
Source code1is released to facilitate fu - ture work.1 IntroductionThis paper considers the reading comprehensiontask in which somediscrete - reasoningabilities areneeded to correctly answer questions .
Speciﬁ-cally , we focus on a new reading comprehensiondataset called DROP ( Dua et al . ,2019 ) , whichrequires Discrete Reasoning Over the content ofParagraphs to obtain the ﬁnal answer .
Unlike pre - vious benchmarks such as CNN / DM ( Hermannet al . ,2015 ) and SQuAD ( Rajpurkar et al . ,2016)that have been well solved ( Chen et al . ,2016;Devlin et al . ,2019 ) , DROP is substantially morechallenging in three ways .
First , the answers to1https://github.com/huminghao16/MTMSNthe questions involve a wide range of types suchas numbers , dates , or text strings .
Therefore , var - ious kinds of prediction strategies are required tosuccessfully ﬁnd the answers .
Second , rather thanrestricting the answer to be a span of text , DROPloosens the constraint so that answers may be a setof multiple text strings .
Third , for questions thatrequire discrete reasoning , a system must have amore comprehensive understanding of the contextand be able to perform numerical operations suchas addition , counting , or sorting .
Existing approaches , when applied to this morerealistic scenario , have three problems .
First , to produce various answer types , Dua et al.(2019 ) extend previous one - type answer predic - tion ( Seo et al . ,2017 ) to multi - type predictionthat supports span extraction , counting , and ad - dition / subtraction .
However , they have not fullyconsidered all potential types .
Take the question“What percent are not non - families ? ” and the pas - sage snippet “ 39.9 % were non - families ” as an ex - ample , anegationoperation is required to infer theanswer .
Second , previous reading comprehensionmodels ( Wang et al . ,2017;Yu et al . ,2018;Huet al . ,2018 ) are designed to produce one singlespan as the answer .
But for some questions such as“Which ancestral groups are smaller than 11%?”,there may exist several spans as correct answers(e.g . , “ Italian ” , “ English ” , and “ Polish ” ) , whichcan not be well handled by these works .
Third , to support numerical reasoning , prior work ( Duaet al . ,2019 ) learns to predict signed numbers forobtaining an arithmetic expression that can be ex - ecuted by a symbolic system .
Nevertheless , theprediction of each signed number is isolated , andthe expression ’s context information has not beenconsidered .
As a result , obviously - wrong expres - sions , such as all predicted signs are either minusor zero , are likely produced .
To address the above issues , we introduce
1597the Multi - Type Multi - Span Network ( MTMSN),a neural reading comprehension model for pre - dicting various types of answers as well asdynamically extracting one or multiple spans .
MTMSN utilizes a series of pre - trained Trans - former blocks ( Devlin et al . ,2019 ) to obtain adeep bidirectional context representation .
On topof it , a multi - type answer predictor is proposedto not only support previous prediction strategiessuch as span , count number , and arithmetic ex - pression , but also add a new type of logical nega - tion .
This results in a wider range of coverageof answer types , which turns out to be crucial toperformance .
Besides , rather than always produc - ing one single span , we present a multi - span ex - traction method to produce multiple answers .
Themodel ﬁrst predicts the number of answers , andthen extracts non - overlapped spans to the speciﬁcamount .
In this way , the model can learn to dy - namically extract one or multiple spans , thus beingbeneﬁcial for multi - answer cases .
In addition , wepropose an arithmetic expression reranking mech - anism to rank expression candidates that are de - coded by beam search , so that their context infor - mation can be considered during reranking to fur - ther conﬁrm the prediction .
Our MTMSN model outperforms all existingapproaches on the DROP hidden test set by achiev - ing 79.9 F1 score , a 32.9 % absolute gain over priorbest work at the time of submission .
To make a faircomparison , we also construct a baseline that usesthe same BERT - based encoder .
Again , MTMSNsurpasses it by obtaining a 13.2 F1 increase on thedevelopment set .
We also provide an in - depth ab - lation study to show the effectiveness of our pro - posed methods , analyze performance breakdownby different answer types , and give some qualita - tive examples as well as error analysis.2 Task DescriptionIn the reading comprehension task that requiresdiscrete reasoning , a passage and a question aregiven .
The goal is to predict an answer to the ques - tion by reading and understanding the passage .
Unlike previous dataset such as SQuAD ( Ra - jpurkar et al . ,2016 ) where the answer is limitedto be a single span of text , DROP loosens the con - straint so that the answer involves various typessuch as number , date , or span of text ( Figure1).Moreover , the answer can be multiple text stringsinstead of single continuous span ( A2 ) .
To suc - Passage : As of the census of 2000 , there were 218,590people , 79,667 households , ... 22.5 % were of Germanpeople , 13.1 % Irish people , 9.8 % Italian people , ... Q1 : Which group from the census is larger : German orIrish?A1 : GermanQ2 : Which ancestral groups are at least 10%?A2 : German , IrishQ3 : How many more people are there than households?A3 : 138,923Q4 : How many percent were not German?A4 : 77.5Figure 1 : Question - answer pairs along with a passagefrom the DROP dataset.cessfully ﬁnd the answer , some discrete reasoningabilities , such as sorting ( A1 ) , subtraction ( A3),and negation ( A4 ) , are required.3 Our ApproachFigure2gives an overview of our model that aimsto combine neural reading comprehension withnumerical reasoning .
Our model uses BERT ( De - vlin et al . ,2019 ) as encoder : we map word em - beddings into contextualized representations usingpre - trained Transformer blocks ( Vaswani et al . ,2017)(§3.1 ) .
Based on the representations , weemploy a multi - type answer predictor that is ableto produce four answer types : ( 1 ) span from thetext ; ( 2 ) arithmetic expression ; ( 3 ) count number;(4 ) negation on numbers ( § 3.2 ) .
FollowingDuaet al.(2019 ) , we ﬁrst predict the answer type ofa given passage - question pair , and then adopt in - dividual prediction strategies .
To support multi - span extraction ( § 3.3 ) , the model explicitly pre - dicts the number of answer spans .
It then outputsnon - overlapped spans until the speciﬁc amount isreached .
Moreover , we do not directly use thearithmetic expression that possesses the maximumprobability , but instead re - rank several expressioncandidates that are decoded by beam search to fur - ther conﬁrm the prediction ( § 3.4 ) .
Finally , themodel is trained under weakly - supervised signalsto maximize the marginal likelihood over all pos - sible annotations ( § 3.5).3.1 BERT - Based EncoderTo obtain a universal representation for both thequestion and the passage , we utilize BERT ( De - vlin et al . ,2019 ) , a pre - trained deep bidirectionalTransformer model that achieves state - of - the - artperformance across various tasks , as the encoder .
Speciﬁcally , we ﬁrst tokenize the question and
1598 Embeddings ( WordPiece , position , and segment)Transformer BlockSpanAnswer TypeAdd / SubCountNegation SpanSpan ... SpanSpant=2 ... Beam searchExp1Exp2Exp3NMSx Lu1 ... sign1psign2psign3psignpNu2u3uNarith1parith2parith3p CLSTok1ToknSEPTok1SEPTokm ...... startpendpspanpMulti - Span ExtractionArithmetic Expression RerankingMulti - Type Answer PredictorFigure 2 : An illustration of MTMSN architecture .
The multi - type answer predictor supports four kinds of answertypes including span , addition / subtraction , count , and negation .
A multi - span extraction method is proposed todynamically produce one or several spans .
The arithmetic expression reranking mechanism aims to rank expressioncandidates that are decoded by beam search for further validating the prediction.the passage using the WordPiece vocabulary ( Wuet al . ,2016 ) , and then generate the input sequenceby concatenating a[CLS]token , the tokenizedquestion , a[SEP]token , the tokenized passage , and a ﬁnal[SEP]token .
For each token in thesequence , its input representation is the element - wise addition of WordPiece embeddings , posi - tional embeddings , and segment embeddings ( De - vlin et al . ,2019 ) .
As a result , a list of input embed - dingsH02RT ⇥ Dcan be obtained , whereDisthe hidden size andTis the sequence length .
A se - ries ofLpre - trained Transformer blocks are thenused to project the input embeddings into contex - tualized representationsHias : Hi= TransformerBlock(Hi 1),8i2[1,L]Here , we omit a detailed introduction of the blockarchitecture and refer readers toVaswani et al.(2017 ) for more details.3.2 Multi - Type Answer PredictorRather than restricting the answer to always be aspan of text , the discrete - reasoning reading com - prehension task involves different answer types(e.g . , number , date , span of text ) .
FollowingDuaet al.(2019 ) , we design a multi - type answer pre - dictor to selectively produce different kinds of an - swers such as span , count number , and arithmeticexpression .
To further increase answer coverage , we propose adding a new answer type to sup - port logical negation .
Moreover , unlike prior workthat separately predicts passage spans and ques - tion spans , our approach directly extracts spansfrom the input sequence .
Answer type predictionInspired by the Aug - mented QANet model ( Dua et al . ,2019 ) , we usethe contextualized token representations from thelast four blocks ( HL 3 , ... , HL ) as the inputs toour answer predictor , which are denoted asM0,M1,M2,M3 , respectively .
To predict the answertype , we ﬁrst split the representationM2into aquestion representationQ2and a passage repre - sentationP2according to the index of intermedi - ate[SEP]token .
Then the model computes twovectorshQ2andhP2that summarize the questionand passage information respectively: ↵ Q= softmax(WQQ2),hQ2= ↵ QQ2wherehP2is computed in a similar way overP2.Next , we calculate a probability distribution torepresent the choices of different answer types as : ptype= softmax(FFN([hQ2;hP2;hCLS]))Here , hCLSis the ﬁrst vector in the ﬁnal con - textualized representationM3 , andFFNdenotesa feed - forward network consisting of two linearprojections with a GeLU activation ( Hendrycksand Gimpel,2016 ) followed by a layer normaliza - tion ( Lei Ba et al . ,2016 ) in between . SpanTo extract the answer either from the pas - sage or from the question , we combine the gatingmechanism ofWang et al.(2017 ) with the standarddecoding strategy ofSeo et al.(2017 ) to predictthe starting and ending positions across the en - tire sequence .
Speciﬁcally , we ﬁrst compute threevectors , namelygQ0,gQ1,gQ2 , that summarize
1599the question information among different levels ofquestion representations: Q= softmax(FFN(Q2),gQ2= QQ2wheregQ0andgQ1are computed overQ0andQ1respectively , in a similar way as described above .
Then we compute the probabilities of the start - ing and ending indices of the answer span from theinput sequence as:¯Mstart=[M2;M0;gQ2 ⌦ M2;gQ0 ⌦ M0],¯Mend=[M2;M1;gQ2 ⌦ M2;gQ1 ⌦ M1],pstart= softmax(WS¯Mstart),pend= softmax(WE¯Mend)where ⌦ denotes the outer product between thevectorgand each token representation inM.Arithmetic expressionIn order to model theprocess of performing addition or subtractionamong multiple numbers mentioned in the pas - sage , we assign a three - way categorical variable(plus , minus , or zero ) for each number to indicateits sign , similar toDua et al.(2019 ) .
As a result , an arithmetic expression that has a number as theﬁnal answer can be obtained and easily evaluated .
Speciﬁcally , for each number mentioned in thepassage , we gather its corresponding representa - tion from the concatenation ofM2andM3 , even - tually yieldingU=(u1 , . . .
, uN)2RN ⇥ 2 ⇤ DwhereNnumbers exist .
Then the probabilities ofthei - th number being assigned a plus , minus orzero is computed as : psigni= softmax(FFN([ui;hQ2;hP2;hCLS]))CountWe consider the ability of counting en - tities and model it as a multi - class classiﬁcationproblem .
To achieve this , the model ﬁrst producesa vectorhUthat summarizes the important infor - mation among all mentioned numbers , and thencomputes a counting probability distribution as: ↵ U= softmax(WUU),hU= ↵ UU , pcount= softmax(FFN([hU;hQ2;hP2;hCLS]))NegationOne obvious but important linguisticphenomenon that prior work fails to capture isnegation .
We ﬁnd there are many cases in DROPthat require to perform logical negation on num - bers .
The question ( Q4 ) in Figure1gives a qual - itative example of this phenomenon .
To modelthis phenomenon , we assign a two - way categori - cal variable for each number to indicate whethera negation operation should be performed .
Thenwe compute the probabilities of logical negationon thei - th number as : pnegationi= softmax(FFN([ui;hQ2;hP2;hCLS]))3.3 Multi - Span ExtractionAlthough existing reading comprehension tasksfocus exclusively on ﬁnding one span of text asthe ﬁnal answer , DROP loosens the restriction sothat the answer to the question may be severaltext spans .
Therefore , speciﬁc adaption should bemade to extend previous single - span extraction tomulti - span scenario .
To do this , we propose directly predicting thenumber of spans and model it as a classiﬁcationproblem .
This is achieved by computing a proba - bility distribution on span amount aspspan= softmax(FFN([hQ2;hP2;hCLS]))To extract non - overlapped spans to the speciﬁcamount , we adopt the non - maximum suppression(NMS ) algorithm ( Rosenfeld and Thurston,1971)that is widely used in computer vision for pruningredundant bounding boxes , as shown in Algorithm1 .
Concretely , the model ﬁrst proposes a set oftop - KspansSaccording to the descending orderof the span score , which is computed aspstartkpendlfor the span(k , l ) .
It also predicts the amount ofextracted spanstfrompspan , and initializes a newset˜S. Next , we add the spansithat possesses themaximum span score to the set˜S , and remove itfromS. We also delete any remaining spansjthatoverlaps withsi , where the degree of overlap ismeasured using the text - level F1 function .
Thisprocess is repeated for remaining spans inS , untilSis empty or the size of˜Sreachest.3.4 Arithmetic Expression RerankingAs discussed in§3.2 , we model the phenomenonof discrete reasoning on numbers by learning topredict a plus , minus , or zero for each number inthe passage .
In this way , an arithmetic expres - sion composed of signed numbers can be obtained , where the ﬁnal answer can be deduced by per - forming simple arithmetic computation .
However , since the sign of each number is only determinedby the number representation and some coarse - grained global representations , the context infor - mation of the expression itself has not been con - sidered .
As a result , the model may predict some
1600Algorithm 1Multi - span extractionInput : pstart;pend;pspan1 : Generate the setSby extracting top - Kspans2 : SortSin descending order of span scores3 : t= arg maxpspan+14 : Initialize˜S={}5 : whileS6={}and|˜S|<tdo6 : forsiinSdo7 : Add spansito˜S8 : Remove spansifromS9 : forsjinSdo10 : iff1(si , sj)>0then11 : Remove spansjfromS12 : return˜Sobviously wrong expressions ( e.g. , the signs thathave maximum probabilities are either minus orzero , resulting in a large negative value ) .
There - fore , in order to further validate the prediction , itis necessary to rank several highly conﬁdent ex - pression candidates using the representation sum - marized from the expression ’s context .
Speciﬁcally , we use beam search to producetop - ranked arithmetic expressions , which are sentback to the network for reranking .
Since each ex - pression consists of several signed numbers , weconstruct an expression representation by takingboth the numbers and the signs into account .
Foreach number in the expression , we gather its cor - responding vector from the representationU. Asfor the signs , we initialize an embedding matrixE2R3 ⇥ 2 ⇤ D , and ﬁnd the sign embeddings foreach signed number .
In this way , given thei - th ex - pression that containsMsigned numbers at most , we can obtain number vectorsVi2RM ⇥ 2 ⇤ Daswell as sign embeddingsCi2RM ⇥ 2 ⇤ D. Then theexpression representation along with the rerankingprobability can be calculated as: ↵ Vi= softmax(WV(Vi+Ci)),hVi= ↵ Vi(Vi+Ci),parithi= softmax(FFN([hVi;hQ2;hP2;hCLS]))3.5 Training and InferenceSince DROP does not indicate the answer
typebut only provides the answer string , we thereforeadopt the weakly supervised annotation scheme , as suggested inBerant et al.(2013);Dua et al.(2019 ) .
We ﬁnd all possible annotations that pointto the gold answer , including matching spans , arithmetic expressions , correct count numbers , negation operations , and the number of spans .
Weuse simple rules to search over all mentioned num - bers to ﬁnd potential negations .
That is , if 100minus a number is equal to the answer , then anegation occurs on this number .
Besides , we onlysearch the addition / subtraction of three numbers atmost due to the exponential search space .
To train our model , we propose using a two - step training method composed of an inferencestep and a training step .
In the ﬁrst step , we usethe model to predict the probabilities of sign as - signments for numbers .
If there exists any an - notation of arithmetic expressions , we run beamsearch to produce expression candidates and la - bel them as either correct or wrong , which arelater used for supervising the reranking compo - nent .
In the second step , we adopt the marginallikelihood objective function ( Clark and Gardner,2018 ) , which sums over the probabilities of allpossible annotations including the above labeledexpressions .
Notice that there are two objectivefunctions for the multi - span component : one is adistantly - supervised loss that maximizes the prob - abilities of all matching spans , and the other is aclassiﬁcation loss that maximizes the probabilityon span amount .
At test time , the model ﬁrst chooses the answertype and then performs speciﬁc prediction strate - gies .
For the span type , we use Algorithm1fordecoding .
If the type is addition / subtraction , arith - metic expression candidates will be proposed andfurther reranked .
The expression with the maxi - mum product of cumulative sign probability andreranking probability is chosen .
As for the count - ing type , we choose the number that has the max - imum counting probability .
Finally , if the type isnegation , we ﬁnd the number that possesses thelargest negation probability , and then output theanswer as 100 minus this number.4 Experiments4.1
Implementation DetailsDatasetWe consider the reading comprehensionbenchmark that requires Discrete Reasoning OverParagraphs ( DROP ) ( Dua et al . ,2019 ) to trainand evaluate our model .
DROP contains crowd - sourced , adversarially - created , 96.6 K question - answer pairs , with 77.4 K for training , 9.5 K forvalidation , and another 9.6 K hidden examples fortesting .
Passages are extracted from Wikipediaarticles and the answer to each question involvesvarious types such as number , date , or text string .
Some answers may even be a set of multiple spansof text in the passage .
To ﬁnd the answers , a com-
1601ModelDev TestEM F1 EM F1Heuristic Baseline ( Dua et al . ,2019)4.28 8.07 4.18 8.59Semantic Role Labeling ( Carreras and M`arquez,2004)11.03 13.67 10.87 13.35BiDAF ( Seo et al . ,2017)26.06 28.85 24.75 27.49QANet+ELMo ( Yu et al . ,2018)27.71 30.33 27.08 29.67BERTBASE(Devlin et al . ,2019)30.10 33.36 29.45 32.70NAQANet ( Dua et al . ,2019)46.20 49.24 44.07 47.01NABERTBASE55.82 58.75 - -NABERTLARGE64.61 67.35 - -MTMSNBASE68.17 72.81 - -MTMSNLARGE76.68 80.54 75.85 79.88Human Performance ( Dua et al . ,2019)- - 92.38 95.98Table 1 : The performance of MTMSN and other competing approaches on DROP dev and test set .
ModelBASE LARGEEM F1 EM F1MTMSN68.2 72.8 76.7 80.5w /
o Add / Sub46.7
51.3 53.8 58.0w / o Count62.5 66.4 71.8 75.6w / o Negation59.4
63.6 67.2 70.9w / o
Multi - Span67.5
70.7 75.6 78.4w / o Reranking66.9 71.2 74.9 78.7Table 2 : Ablation tests of base and large models on theDROP dev set.prehensive understanding of the context as well
asthe ability of numerical reasoning are required .
Model settingsWe build our model upon twopublicly available uncased versions of BERT : BERTBASEand BERTLARGE2 , and refer readerstoDevlin et al.(2019 ) for details on model sizes .
We use Adam optimizer with a learning rate of 3e-5 and warmup over the ﬁrst 5 % steps to train .
Themaximum number of epochs is set to 10 for basemodels and 5 for large models , while the batch sizeis 12 or 24 respectively .
A dropout probability of0.1 is used unless stated otherwise .
The number ofcounting class is set to 10 , and the maximum num - ber of spans is 8 .
The beam size is 3 by default , while the maximum amount of signed numbersMis set to 4 .
All texts are tokenized using Word-2BERTBASEis the original version while BERTLARGEis the model augmented with n - gram masking andsynthetic self - training : https://github.com / google - research / bert . ModelEM
F1MTMSN76.7 80.5w / o Q / P Vectors75.1
79.2w / o CLS Vector74.0 78.4Q / P Vectors Using Last Hidden76.5 80.2w / o Gated Span Prediction75.8 79.7Combine Add / Sub with Negation75.5
79.4Table 3 : Ablation tests of different architecture choicesusing MTMSNLARGE.Piece vocabulary ( Wu et al . ,2016 ) , and truncatedto sequences no longer than 512 tokens .
BaselinesFollowing the implementation ofAugmented QANet ( NAQANet ) ( Dua et al . ,2019 ) , we introduce a similar baseline called Aug - mented BERT ( NABERT ) .
The main differenceis that we replace the encoder of QANet ( Yuet al . ,2018 ) with the pre - trained Transformerblocks ( Devlin et al . ,2019 ) .
Moreover , it also sup - ports the prediction of various answer types suchas span , arithmetic expression , and count number.4.2 Main ResultsTwo metrics , namely Exact Match ( EM ) and F1score , are utilized to evaluate models .
We use theofﬁcial script to compute these scores .
Since thetest set is hidden , we only submit the best singlemodel to obtain test results .
Table1shows the performance of our modeland other competitive approaches on the develop-
1602Type(%)NABERT MTMSNEM F1 EM F1Date1.6 55.7 60.8 55.7 69.0Number61.9 63.8 64.0 80.9 81.1Single Span31.7 75.9 80.6 77.5 82.8Multi Span4.8 0 22.7 25.1 62.8Table 4 : Performance breakdown of NABERTLARGEand MTMSNLARGEby gold answer types.ment and test sets .
MTMSN outperforms all ex - isting approaches by a large margin , and createsnew state - of - the - art results by achieving an EMscore of 75.85 and a F1 score of 79.88 on the testset .
Since our best model utilizes BERTLARGEas encoder , we therefore compare MTMSNLARGEwith the NABERTLARGEbaseline .
As we can see , our model obtains 12.07/13.19 absolute gain ofEM / F1 over the baseline , demonstrating the effec - tiveness of our approach .
However , as the humanachieves 95.98 F1 on the test set , our results sug - gest that there is still room for improvement.4.3 Ablation StudyComponent ablationTo analyze the effect ofthe proposed components , we conduct ablationstudies on the development set .
As illustrated inTable2 , the use of addition and subtraction is ex - tremely crucial : the EM / F1 performance of boththe base and large models drop drastically by morethan 20 points if it is removed .
Predicting countnumbers is also an important component that con - tributes nearly 5 % gain on both metrics .
More - over , enhancing the model with the negation typesigniﬁcantly increases the F1 by roughly 9 percenton both models .
In brief , the above results showthat multi - type answer prediction is vitally impor - tant for handling different forms of answers , es - pecially in cases where discrete reasoning abilitiesare required .
We also report the performance after remov - ing the multi - span extraction method .
The resultsreveal that it has a more negative impact on theF1 score .
We interpret this phenomenon as fol - lows : producing multiple spans that are partiallymatched with ground - truth answers is much easierthan generating an exactly - matched set of multipleanswers .
Hence for multi - span scenarios , the gainof our method on F1 is relatively easier to obtainthan the one on EM .
Finally , to ablate arithmeticexpression reranking , we simply use the arithmeticexpression that has the maximum cumulative signTypeNABERT MTMSN(% ) EM F1 ( % ) EM
F1Span43.0 67.9 74.2 42.7 72.2 81.0Add / Sub43.6 62.0 62.1 32.4 78.1 78.2Count13.4 62.4 62.4 13.4 70.4 70.4Negation0 0 0 11.5 96.3 96.3Table 5 : Performance breakdown of NABERTLARGEand
MTMSNLARGEby predicted answer types.probability instead .
We ﬁnd that our rerankingmechanism gives 1.8 % gain on both metrics forthe large model .
This conﬁrms that validating ex - pression candidates with their context informationis beneﬁcial for ﬁltering out highly - conﬁdent butwrong predictions .
Architecture ablationWe further conduct a de - tailed ablation in Table3to evaluate our architec - ture designs .
First , we investigate the effects ofsome “ global vectors ” used in our model .
Speciﬁ-cally , we ﬁnd that removing the question and pas - sage vectors from all involved computation leadsto 1.3 % drop on F1 .
Ablating the representationof[CLS]token leads to even worse results .
Wealso try to use the last hidden representation ( de - noted asM3 ) to calculate question and passagevectors , but ﬁnd that does not work .
Next , we re - move the gating mechanism used during span pre - diction , and observe a nearly 0.8 % decline on bothmetrics .
Finally , we share parameters between thearithmetic expression component and the negationcomponent , and ﬁnd the performance drops by1.1 % on F1.4.4 Analysis and DiscussionPerformance breakdownWe now provide aquantitative analysis by showing performancebreakdown on the development set .
Table4showsthat our gains mainly come from the most frequentnumber type , which requires various types of sym - bolic , discrete reasoning operations .
Moreover , signiﬁcant improvements are also obtained in themulti - span category , where the F1 score increasesby more than 40 points .
This result further provesthe validity of our multi - span extraction method .
We also give the performance statistics thatare categorized according to the predicted answertypes in Table5 .
As shown in the Table , the mainimprovements are due to the addition / subtractionand negation types .
We conjecture that there aretwo reasons for these improvements .
First , our
1603 Figure 3 : EM / F1 scores of MTMSNLARGEwith differ - ent maximum numbers of spans .
Figure 4 : EM / F1 scores of MTMSNLARGEwith differ - ent beam sizes and amounts of signed numbers ( M).proposed expression reranking mechanism helpsvalidate candidate expressions .
Second , a new in - ductive bias that enables the model to perform log - ical negation has been introduced .
The impressiveperformance on the negation type conﬁrms ourjudgement , and suggests that the model is able toﬁnd most of negation operations .
In addition , wealso observe promising gains brought by the spanand count types .
We think the gains are mainlydue to the multi - span extraction method as well asarchitecture designs .
Effect of maximum number of spansTo inves - tigate the effect of maximum number of spans onmulti - span extraction , we conduct an experimenton the dev set and show the curves in Figure3.We vary the value from 2 to 12 , increased by 2,and also include the extreme value 1 .
Accordingto the Figure , the best results are obtained at 8.A higher value could potentially increase the an - swer recall but damage the precision by makingmore predictions , and a smaller value may forcethe model to produce limited number of answers , resulting in high precision but low recall .
There - fore , a value of 8 turns out to be a good trade - offbetween recall and precision .
Moreover , when thevalue decreases to 1 , the multi - span extraction de - grades to previous single - span scenario , and theperformance drops signiﬁcantly .
ConﬁgurationSkipped Kept Ratio ( % ) F1Span33752 43657 56.4 38.9+|6384 71025 91.7 59.2+|+4282 73127 94.4 63.6+|++~1595 75814
97.9 72.8Table 6 : Annotation statistics under different combi - nations of answer types in the DROP train set .
“ Kept”and “ Skipped ” mean the number of examples with orwithout annotation , respectively.|refers to Add / Sub,denotes Count , and ~ indicates Negation .
F1 scoresare benchmarked using MTMSNBASEon the dev set .
Effect of beam size andMWe further investi - gate the effect of beam size and maximum amountof signed numbers in Figure4 .
As we can see , a beam size of 3 leads to the best performance , likely because a larger beam size might confusethe model as too many candidates are ranked , onthe other hand , a small size could be not sufﬁ-cient to cover the correct expression .
In addition , we ﬁnd that the performance constantly decreasesas the maximum thresholdMincreases , suggest - ing that most of expressions only contain two orthree signed numbers , and setting a larger thresh - old could bring in additional distractions .
Annotation statisticsWe list the annotationstatistics on the DROP train set in Table6 .
Aswe can see , only annotating matching spans resultsin a labeled ratio of 56.4 % , indicating that DROPincludes various answer types beyond text spans .
By further considering the arithmetic expression , the ratio increase sharply to 91.7 % , suggestingmore than 35 % answers need to be inferred withnumeral reasoning .
Continuing adding countingleads to a percentage of 94.4 % , and a ﬁnal 97.9%coverage is achieved by additionally taking nega - tion into account .
More importantly , the F1 scoreconstantly increases as more answer types are con - sidered .
This result is consistent with our observa - tions in ablation study .
Error analysisFinally , to better understand theremaining challenges , we randomly sample 100incorrectly predicted examples based on EM andcategorize them into 7 classes .
38 % of errors areincorrect arithmetic computations , 18 % requiresorting over multiple entities , 13 % are due to mis - takes on multi - span extraction , 10 % are single - span extraction problems , 8 % involve miscount - ing , another 8 % are wrong predictions on spannumber , the rest ( 5 % ) are due to various reasons
1604such as incorrect preprocessing , negation error , and so on .
See Appendix for some examples ofthe above error cases.5
Related WorkReading comprehension benchmarksPromis - ing advancements have been made for readingcomprehension due to the creation of many largedatasets .
While early research used cloze - styletests ( Hermann et al . ,2015;Hill et al . ,2016 ) , mostof recent works ( Rajpurkar et al . ,2016;Joshi et al . ,2017 ) are designed to extract answers from thepassage .
Despite their success , these datasets onlyrequire shallow pattern matching and simple logi - cal reasoning , thus being well solved ( Chen et al . ,2016;Devlin et al . ,2019 ) .
Recently , Dua et al.(2019 ) released a new benchmark named DROPthat demands discrete reasoning as well as deeperparagraph understanding to ﬁnd the answers .
Sax - ton et al.(2019 ) introduced a dataset consistingof different types of mathematics problems to fo - cuses on mathematical computation .
We choose towork on DROP to test both the numerical reason - ing and linguistic comprehension abilities .
Neural reading modelsPrevious neural read - ing models , such as BiDAF ( Seo et al . ,2017),R - Net ( Wang et al . ,2017 ) , QANet ( Yu et al . ,2018 ) , Reinforced Mreader ( Hu et al . ,2018 ) , areusually designed to extract a continuous spanof text as the answer .
Dua et al.(2019 ) en - hanced prior single - type prediction to support var - ious answer types such as span , count number , and addition / subtraction .
Different from theseapproaches , our model additionally supports anew negation type to increase answer coverage , and learns to dynamically extract one or multiplespans .
Morevoer , answer reranking has been wellstudied in several prior works ( Cui et
al . ,2016;Wang et
al . ,2018a , b , c;Hu et al . ,2019 ) .
We fol - low this line of work , but propose ranking arith - metic expressions instead of candidate answers .
End - to - end symbolic reasoningCombiningneural methods with symbolic reasoning was con - sidered byGraves et al.(2014);Sukhbaatar et al.(2015 ) , where neural networks augmented withexternal memory are trained to execute simple pro - grams .
Later works on program induction ( Reedand De Freitas,2016;Neelakantan et
al . ,2016;Liang et al . ,2017 ) extended this idea by usingseveral built - in logic operations along with a key - value memory to learn different types of compo - sitional programs such as addition or sorting .
Incontrast to these works , MTMSN does not modelvarious types of reasoning with a universal mem - ory mechanism but instead deals each type withindividual predicting strategies .
Visual question answeringIn computer vi - sion community , the most similar work to ourapproach is Neural Module Networks ( Andreaset al . ,2016b ) , where a dependency parser is usedto lay out a neural network composed of severalpre - deﬁned modules .
Later , Andreas et al.(2016a)proposed dynamically choosing an optimal lay - out structure from a list of layout candidates thatare produced by off - the - shelf parsers .
Hu et al.(2017 ) introduced an end - to - end module networkthat learns to predict instance - speciﬁc networklayouts without the aid of a parser .
Compared tothese approaches , MTMSN has a static networklayout that can not be changed during training andevaluation , where pre - deﬁned “ modules ” are usedto handle different types of answers.6 ConclusionWe introduce MTMSN , a multi - type multi - spannetwork for reading comprehension that requiresdiscrete reasoning over the content of paragraphs .
We enhance a multi - type answer predictor to sup - port logical negation , propose a multi - span extrac - tion method for producing multiple answers , anddesign an arithmetic expression reranking mecha - nism to further conﬁrm the prediction .
Our modelachieves 79.9 F1 on the DROP hidden test set , cre - ating new state - of - the - art results .
As future work , we would like to consider handling additionaltypes such as sorting or multiplication / division .
We also plan to explore more advanced methodsfor performing complex numerical reasoning .
AcknowledgmentsWe would like to thank the anonymous reviewersfor their thoughtful comments and insightful feed - back .
This work was supported by the NationalKey Research and Development Program of China(2016YFB100101).ReferencesJacob Andreas , Marcus Rohrbach , Trevor Darrell , andDan Klein .
2016a .
Learning to compose neural net-
1605works for question answering .
InProceedings ofNAACL.Jacob Andreas , Marcus Rohrbach , Trevor Darrell , andDan Klein .
2016b .
Neural module networks .
InProceedings of CVPR.Jonathan Berant , Andrew Chou , Roy Frostig , and PercyLiang .
2013 .
Semantic parsing on freebase fromquestion - answer pairs .
InProceedings of EMNLP.Xavier Carreras and Llu´ıs M`arquez .
2004 .
Introduc - tion to the conll-2004 shared task : Semantic role la - beling .
InProceedings of CONLL.Danqi Chen , Jason Bolton , and Christopher D Man - ning .
2016 .
A thorough examination of thecnn / daily mail reading comprehension task.arXivpreprint arXiv:1606.02858.Christopher
Clark and Matt Gardner .
2018 .
Simpleand effective multi - paragraph reading comprehen - sion .
InProceedings of ACL.Yiming Cui , Zhipeng Chen , Si Wei , Shijin Wang , Ting Liu , and Guoping Hu . 2016 .
Attention - over - attention neural networks for reading comprehen - sion.arXiv preprint arXiv:1607.04423.Jacob Devlin , Ming - Wei Chang , Kenton Lee , andKristina Toutanova .
2019 .
Bert : Pre - training of deepbidirectional transformers for language understand - ing .
InProceedings of NAACL.Dheeru Dua , Yizhong Wang , Pradeep Dasigi , GabrielStanovsky , Sameer Singh , and Matt Gardner .
2019.Drop : A reading comprehension benchmark requir - ing discrete reasoning over paragraphs .
InProceed - ings of NAACL.Alex Graves , Greg Wayne , and Ivo Danihelka.2014 .
Neural turing machines.arXiv preprintarXiv:1410.5401.Dan Hendrycks and Kevin Gimpel .
2016 .
Bridgingnonlinearities and stochastic regularizers with gaus - sian error linear units .
CoRR , abs/1606.08415.Karl Moritz Hermann , Tomas Kocisky , EdwardGrefenstette , Lasse Espeholt , Will Kay , Mustafa Su - leyman , and Phil Blunsom .
2015 .
Teaching ma - chines to read and comprehend .
InProceedings ofNIPS.Felix Hill , Antoine Bordes , Sumit Chopra , and JasonWeston .
2016 .
The goldilocks principle : Readingchildrens books with explicit memory representa - tions .
InProceedings of ICLR.Minghao Hu , Yuxing Peng , Zhen Huang , and Dong - sheng Li .
2019 .
Retrieve , read , rerank : Towardsend - to - end multi - document reading comprehension .
InProceedings of ACL.Minghao Hu , Yuxing Peng , Zhen Huang , Xipeng Qiu , Furu Wei , and Ming Zhou .
2018 .
Reinforcedmnemonic reader for machine reading comprehen - sion .
InProceedings of IJCAI.Ronghang Hu , Jacob Andreas , Marcus Rohrbach , Trevor Darrell , and Kate Saenko .
2017 .
Learningto reason : End - to - end module networks for visualquestion answering .
InProceedings of ICCV.Mandar Joshi , Eunsol Choi , Daniel S Weld , and LukeZettlemoyer .
2017 .
Triviaqa :
A large scale distantlysupervised challenge dataset for reading comprehen - sion .
InProceedings of ACL.Jimmy Lei Ba , Jamie Ryan Kiros , and Geoffrey E Hin - ton . 2016 .
Layer normalization.arXiv preprintarXiv:1607.06450.Chen Liang , Jonathan Berant , Quoc Le , Kenneth DForbus , and Ni Lao .
2017 .
Neural symbolic ma - chines : Learning semantic parsers on freebase withweak supervision .
InProceedings of ACL.Arvind Neelakantan , Quoc V Le , and Ilya Sutskever.2016 .
Neural programmer : Inducing latent pro - grams with gradient descent .
InProceedings ofICLR.Pranav Rajpurkar , Jian Zhang , Konstantin Lopyrev , andPercy Liang .
2016 .
Squad : 100,000 + questions formachine comprehension of text .
InProceedings ofEMNLP.Scott Reed and Nando De Freitas .
2016 .
Neuralprogrammer - interpreters .
InProceedings of ICLR.Azriel Rosenfeld and Mark Thurston .
1971 .
Edge andcurve detection for visual scene analysis .
IEEETransactions on computers , ( 5):562–569.David Saxton , Edward Grefenstette , Felix Hill , andPushmeet Kohli .
2019 .
Analysing mathematicalreasoning abilities of neural models .
InProceedingsof ICLR.Minjoon Seo , Aniruddha Kembhavi , Ali Farhadi , andHannaneh Hajishirzi .
2017 .
Bidirectional attentionﬂow for machine comprehension .
InProceedings ofICLR.Sainbayar Sukhbaatar , Jason Weston , Rob Fergus , et al.2015 .
End - to - end memory networks .
InProceed - ings of NIPS.Ashish Vaswani , Noam Shazeer , Niki Parmar , JakobUszkoreit , Llion Jones , Aidan N Gomez , ŁukaszKaiser , and Illia Polosukhin .
2017 .
Attention is allyou need .
InProceedings of NIPS.Shuohang Wang , Mo Yu , Jing Jiang , Wei Zhang , Xiaoxiao Guo , Shiyu Chang , Zhiguo Wang , TimKlinger , Gerald Tesauro , and Murray Campbell.2018a .
Evidence aggregation for answer re - rankingin open - domain question answering .
InProceedingsof ICLR.Wenhui Wang , Nan Yang , Furu Wei , Baobao Chang , and Ming Zhou .
2017 .
Gated self - matching net - works for reading comprehension and question an - swering .
InProceedings of ACL .
1606Yizhong Wang , Kai Liu , Jing Liu , Wei He , YajuanLyu , Hua Wu , Sujian Li , and Haifeng Wang .
2018b .
Multi - passage machine reading comprehension withcross - passage answer veriﬁcation .
InProceedingsof ACL.Zhen Wang , Jiachen Liu , Xinyan Xiao , Yajuan Lyu , and Tian Wu .
2018c .
Joint training of candidateextraction and answer selection for reading compre - hension .
InProceedings of ACL.Yonghui Wu , Mike Schuster , Zhifeng Chen , Quoc VLe , Mohammad Norouzi , Wolfgang Macherey , Maxim Krikun , Yuan Cao , Qin Gao , KlausMacherey , et al . 2016 .
Google ’s neural ma - chine translation system : Bridging the gap betweenhuman and machine translation.arXiv preprintarXiv:1609.08144.Adams Wei Yu , David Dohan , Quoc Le , Thang Luong , Rui Zhao , and Kai Chen .
2018 .
Fast and accuratereading comprehension by combining self - attentionand convolution .
InProceedings of ICLR .
