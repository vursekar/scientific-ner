{"text": "The 1 : BLEU scores of all systems . Bold figures mean GBMT ctx is significantly better than GBMT at p \u2264 0.01 . * means a system is significantly better than PBMT at p \u2264 0.01 . + means a system is significantly better than TBMT at p \u2264 0.01 . Following Li et al ( 2016 ) , Chinese and German sentences are parsed into projective dependency trees which are then converted to graphs by adding bigram edges . Word alignment is performed by GIZA++ ( Och and Ney , 2003 ) with the heuristic function grow - diag - final - and . We use SRILM ( Stolcke , 2002 ) to train a 5 - gram language model on the Xinhua portion of the English Gigaword corpus 5th edition with modified Kneser - Ney discounting ( Chen and Goodman , 1996 ) . Batch MIRA ( Cherry and Foster , 2012 ) is used to tune feature weights . We report BLEU ( Papineni et al , 2002 ) scores averaged on three runs of MIRA ( Clark et al , 2011 ) . We compare our system GBMT ctx with several other systems . A system PBMT is built using the phrase - based model in Moses ( Koehn et al , 2007 ) . GBMT is the graph - based translation system described in Li et al ( 2016 ) . To examine the influence of bigram links , GBMT is also used to translate dependency trees where treelets Xiong et al , 2007 ) are the basic translation units . Accordingly , we name the system TBMT . All systems are implemented in Moses .", "entities": [[3, 4, "MetricName", "BLEU"], [81, 83, "TaskName", "Word alignment"], [165, 166, "MetricName", "BLEU"]]}
{"text": "Table 1 shows BLEU scores of all systems . We found that GBMT ctx is better than PBMT across all test sets . Specifically , the improvements are +2.0/+0.7 BLEU on average on ZH - EN and DE - EN , respectively . This improvement is reasonable as our system allows discontinuous phrases which can reduce data sparsity and handle longdistance relations ( Galley and Manning , 2010 ) . In addition , the system TBMT does not show consistent improvements over PBMT while both GBMT and GBMT ctx achieve better BLEU scores than TBMT on both ZH - EN ( +1.8 BLEU , in terms of The number of rules in GBMT ctx according to their type GBMT ctx ) and DE - EN ( +0.6 BLEU , in terms of GBMT ctx ) . This suggests that continuous phrases connected by bigram links are essential to system performance since they help to improve phrase coverage ( Hanneman and Lavie , 2009 ) . We also found that GBMT ctx is significantly better than GBMT on both ZH - EN ( +1.0 BLEU ) and , which indicates that explicitly modeling a segmentation using context is helpful . The main reason for the improvement is that context helps to select proper subgraphs and target phrases . Figure 3 shows example translations . We found that in Figure 3a , after translating a parenthesis , GBMT ctx correctly selects a subgraph Gang Ao Tai and generates a target phrase hong kong , macao and taiwan . In Figure 3b , both GBMT and GBMT ctx choose to translate the subgraph WoMen Ye ZhiLi . However , given the context of the subgraph , GBMT ctx selects a correct target phrase we are also committed to for it .", "entities": [[3, 4, "MetricName", "BLEU"], [29, 30, "MetricName", "BLEU"], [91, 92, "MetricName", "BLEU"], [102, 103, "MetricName", "BLEU"], [127, 128, "MetricName", "BLEU"], [183, 184, "MetricName", "BLEU"]]}
{"text": "Recall that , compared with GBMT , GBMT ctx contains three types of rules : basic rules , segmenting rules , and selecting rules . While basic rules exist in both systems , segmenting and selecting rules make GBMT ctx context - aware . Table 2 shows the number of rules in GBMT ctx according to their types . We found that on both language pairs 35 % - 36 % of rules are basic rules . While the proportion of segmenting rules is \u223c53 % , selecting rules only account for 11 % - 12 % . This is because segmenting rules contain richer contextual information than selecting rules . Table 3 shows BLEU scores of GBMT ctx when different types of rules are used . Note that when only basic rules are allowed , our system degrades to the conventional GBMT system . The results in Table 3 suggest that both segmenting and selecting rules consistently improve GBMT on both language pairs . However , segmenting rules are more useful than selecting rules . This is reasonable since ( hong kong macao taiwan ) hong kong spring festival retail business rise 10 % ( Gang Ao Tai ) XiangGang XinChun LingShou ShengYi ShangSheng YiCheng Ref : GBMT : GBMTctx : ( hong kong , macao and taiwan ) hong kong 's retail sales up 10 % during spring festival ( the spring festival ) hong kong retail business in hong kong , macao and taiwan rose by 10 % ( hong kong , macao and taiwan ) hong kong spring retail business will increase by 10 % ( a ) subgraph selection we also dedicate protect and improve living emvironment .", "entities": [[0, 1, "MetricName", "Recall"], [114, 115, "MetricName", "BLEU"]]}
{"text": "A more structured approach to building joint models over sentences can instead be observed in Fact Verification Systems , e.g. , the methods developed in the FEVER challenge ( Thorne et al , 2018a ) . Such systems take a claim , e.g. , Joe Walsh was inducted in 2001 , as input ( see Tab . 1 ) , and verify if it is valid , using related sentences called evidences ( typically retrieved by a search engine ) . For example , Ev 1 , As a member of the Eagles , Walsh was inducted into the Rock and Roll Hall of Fame in 1998 , and into the Vocal Group Hall of Fame in 2001 , and Ev 3 , Walsh was awarded with the Vocal Group Hall of Fame in 2001 , support the veracity of the claim . In contrast , Ev 2 is neutral as it describes who Joe Walsh is but does not contribute to establish the induction . We conjecture that supporting evidence for answer correctness in AS2 task can be modeled with a similar rationale . In this paper , we design joint models for AS2 based on the assumption that , given q and a target answer candidate t , the other answer candidates , ( c 1 , .. c k ) can provide positive , negative , or neutral support to decide the correctness of t. Our first approach exploits Fact Checking research : we adapted a state - of - the - art FEVER system , KGAT ( Liu et al , 2020 ) , for AS2 . We defined a claim as a pair constituted of the question and one target answer , while considering all the other answers as evidences . We re - trained and rebuilt all its embeddings for the AS2 task . Our second method , Answer Support - based Reranker ( ASR ) , is completely new , it is based on the representation of the pair , ( q , t ) , generated by state - of - the - art AS2 models , concatenated with the representation of all the pairs ( t , c i ) . The latter summarizes the contribution of each c i to t using a maxpooling operation . c i can be unrelated to ( q , t ) since the candidates are automatically retrieved , thus it may introduce just noise . To mitigate this problem , we use an Answer Support Classifier ( ASC ) to learn the relatedness between t and c i by classifying their embedding , which we obtain by applying a transformer network to their concatenated text . ASC tunes the ( t , c i ) embedding parameters according to the evidence that c i provides to t. Our Answer Support - based Reranker ( ASR ) significantly improves the state of the art , and is also simpler than our approach based on KGAT . Our third method is an extension of ASR . It should be noted that , although ASR exploits the information from the k candidates , it still produces a score for a target t without knowing the scores produced for the other target answers . Thus , we jointly model the representation obtained for each target in a multi - ASR ( MASR ) architecture , which can then carry out a complete global reasoning over all target answers . We experimented with our models over three datasets , WikiQA , TREC - QA and WQA , where the latter is an internal dataset built on anonymized customer questions . The results show that : ASR improves the best current model for AS2 , i.e. , TANDA by \u223c3 % , corresponding to an error reduction of 10 % in Accuracy , on both Wik - iQA and TREC - QA . We also obtain a relative improvement of \u223c3 % over TANDA on WQA , confirming that ASR is a general solution to design accurate QA systems . Most interestingly , MASR improves ASR by additional 2 % , confirming the benefit of joint modeling . Finally , it is interesting to mention that MASR improvement is also due to the use of FEVER data for pre - fine - tuning ASC , suggesting that the fact verification inference and the answer support inference are similar .", "entities": [[15, 17, "TaskName", "Fact Verification"], [26, 27, "DatasetName", "FEVER"], [242, 244, "TaskName", "Fact Checking"], [256, 257, "DatasetName", "FEVER"], [590, 591, "DatasetName", "WikiQA"], [592, 593, "DatasetName", "TREC"], [641, 642, "MetricName", "Accuracy"], [649, 650, "DatasetName", "TREC"], [715, 716, "DatasetName", "FEVER"], [728, 730, "TaskName", "fact verification"]]}
{"text": "The task of reranking answer - sentence candidates provided by a retrieval engine can be modeled with a classifier scoring the candidates . Let q be an element of the question set , Q , and A = { c 1 , . . . , c n } be a set of candidates for q , a reranker can be defined as R : Q \u00d7 \u03a0 ( A ) \u03a0 ( A ) , where \u03a0 ( A ) is the set of all permutations of A. Previous work targeting ranking problems in the text domain has classified reranking functions into three buckets : pointwise , pairwise , and listwise methods . Pointwise reranking : This approach learns p ( q , c i ) , which is the probability of c i correctly answering q , using a standard binary classification setting . The final rank is simply obtained sorting c i , based on p ( q , c i ) . Previous work estimates p ( q , c i ) with neural models ( Severyn and Moschitti , 2015 ) , also using attention mechanisms , e.g. , Compare - Aggregate ( Yoon et al , 2019 ) , inter - weighted alignment networks ( Shen et al , 2017 ) , and pre - trained Transformer models , which are the state of the art . Garg et al ( 2020 ) proposed TANDA , which is the current most accurate model on WikiQA and TREC - QA . Pairwise reranking : The method considers binary classifiers of the form \u03c7 ( q , c i , c j ) for determining the partial rank between c i and c j , then the scoring function p ( q , c i ) is obtained by summing up all the contributions with respect to the target candidate t = c i , e.g. , p ( q , c i ) = j \u03c7 ( q , c i , c j ) . There has been a large body of work preceding Transformer models , e.g. , ( Laskar et al , 2020 ; Tayyar Madabushi et al , 2018 ; Rao et al , 2016 ) . However , these methods are largely outperformed by the pointwise TANDA model . Listwise reranking : This approach , e.g. , ( Bian et al , 2017 ; Cao et al , 2007 ; Ai et al , 2018 ) , aims at learning p ( q , \u03c0 ) , \u03c0 \u03a0 ( A ) , using the information on the entire set of candidates . The loss function for training such networks is constituted by the contribution of all elements of its ranked items . The closest work to our research is by Bonadiman and Moschitti ( 2020 ) , who designed several joint models . These improved early neural networks based on CNN and LSTM for AS2 , but failed to improve the state of the art using pre - trained Transformer models .", "entities": [[222, 223, "MethodName", "Transformer"], [250, 251, "DatasetName", "WikiQA"], [252, 253, "DatasetName", "TREC"], [350, 351, "MethodName", "Transformer"], [444, 445, "MetricName", "loss"], [493, 494, "MethodName", "LSTM"], [510, 511, "MethodName", "Transformer"]]}
{"text": "One simple and effective method to build an answer selector is to use a pre - trained Transformer model , adding a simple classification layer to it , and fine - tuning the model on the AS2 task . Specifically , q = Tok q 1 , ... , Tok , inserted at the beginning , as separator , and at the end , respectively . This input is encoded as three embeddings based on tokens , segments and their positions , which are fed as input to several layers ( up to 24 ) . Each of them contains sublayers for multi - head attention , normalization and feed forward processing . The result of this transformation is an embedding , E , representing ( q , c ) , which models the dependencies between words and segments of the two sentences . For the downstream task , E is fed ( after applying a non - linearity function ) to a fully connected layer having weights : W and B. The output layer can be used to implement the task function . For example , a softmax can be used to model the probability of the question / candidate pair classification , as : p ( q , c ) = sof tmax ( W \u00d7 tanh ( E ( q , c ) ) + B ) . We can train this model with log cross - entropy loss : L = \u2212 l { 0 , 1 } y l \u00d7 log ( \u0177 l ) on pairs of texts , where y l is the correct and incorrect answer label , \u0177 1 = p ( q , c ) , and\u0177 0 = 1 \u2212 p ( q , c ) . Training the Transformer from scratch requires a large amount of labeled data , but it can be pre - trained using a masked language model , and the next sentence prediction tasks , for which labels can be automatically generated . Several methods for pretraining Transformer - based language models have been proposed , e.g. , BERT ( Devlin et al , 2018 ) , RoBERTa ( Liu et al , 2019 ) , XLNet ( Yang et al , 2019 ) , AlBERT ( Lan et al , 2020 ) .", "entities": [[17, 18, "MethodName", "Transformer"], [102, 106, "MethodName", "multi - head attention"], [188, 189, "MethodName", "softmax"], [213, 214, "DatasetName", "sof"], [241, 242, "MetricName", "loss"], [248, 249, "DatasetName", "0"], [287, 288, "DatasetName", "0"], [300, 301, "MethodName", "Transformer"], [343, 344, "MethodName", "Transformer"], [354, 355, "MethodName", "BERT"], [363, 364, "MethodName", "RoBERTa"], [372, 373, "MethodName", "XLNet"]]}
{"text": "The first baseline is also a Transformer - based architecture : we concatenate the question with the top k + 1 answer can - didates , i.e. , ( q [ SEP ] c 1 [ SEP ] c 2 . . . [ SEP ] c k+1 ) , and provide this input to the same Transformer model used for pointwise reranking . We use the final hidden vector E corresponding to the first input token [ CLS ] generated by the Transformer , and a classification layer with weights W R ( k+1 ) \u00d7 | E | , and train the model using a standard cross - entropy classification loss : y \u00d7 log ( sof tmax ( EW T ) ) , where y is a one - hot vector representing labels for the k + 1 candidates , i.e. , | y | = k + 1 . We use a transformer model fine - tuned with the TANDA - RoBERTa - base or large models , i.e. , RoBERTa models fine - tuned on ASNQ ( Garg et al , 2020 ) . The scores for the candidate answers are calculated as p ( c 1 ) , .. , p ( c k+1 ) = sof tmax ( EW T ) . Then , we rerank c i according their probability . Joint Model Pairwise Our second baseline is similar to the first . We concatenate the question with each c i to constitute the ( q , c i ) pairs , which are input to the Transformer , and we use the first input token [ CLS ] as the representation of each ( q , c i ) pair . Then , we concatenate the embedding of the pair containing the target candidate , ( q , t ) with the embedding of all the other candidates ' [ CLS ] . ( q , t ) is always in the first position . We train the model using a standard classification loss . At classification time , we select one target candidate at a time , and set it in the first position , followed by all the others . We classify all k + 1 candidates and use their score for reranking them . It should be noted that to qualify for a pairwise approach , Joint Model Pairwise should use a ranking loss . However , we always use standard cross - entropy loss as it is more efficient and the different is performance is negligible . Joint Model with KGAT Liu et al ( 2020 ) presented an interesting model , Kernel Graph Attention Network ( KGAT ) , for fact verification : given a claimed fact f , and a set of evidences Ev = { ev 1 , ev 2 , . . . , ev m } , their model carries out joint reasoning over Ev , e.g. , aggregating information to estimate the probability of f to be true or false , p ( y | f , Ev ) , where y { true , false } . The approach is based on a fully connected graph , G , whose nodes are the n i = ( f , ev i ) pairs , and p ( y | f , Ev ) = p ( y | f , ev i , Ev ) p ( ev i | f , Ev ) , where p ( y | f , ev i , Ev ) = p ( y | n i , G ) is the label probability in each node i conditioned on the whole graph , and p ( ev i | f , Ev ) = p ( n i | G ) is the probability of selecting the most informative evidence . KGAT uses an edge kernel to perform a hierarchi - cal attention mechanism , which propagates information between nodes and aggregate evidences . We built a KGAT model for AS2 as follows : we replace ( i ) ev i with the set of candidate answers c i , and ( ii ) the claim f with the question and a target answer pair , ( q , t ) . KGAT constructs the evidence graph G by using each claim - evidence pair as a node , which , in our case , is ( ( q , t ) , c i ) , and connects all node pairs with edges , making it a fully - connected evidence graph . This way , sentence and token attention operate over the triplets , ( q , t , c i ) , establishing semantic links , which can help to support or undermine the correctness of t. The original KGAT aggregates all the pieces of information we built , based on their relevance , to determine the probability of t. As we use AS2 data , the probability will be about the correctness of t. More in detail , we initialize the node representation using the contextual embeddings obtained with two TANDA - RoBERTa - base models 1 : the first produces the embedding of ( q , t ) , while the second outputs the embedding of ( q , c i ) . Then , we apply a max - pooling operation on these two to get the final node representation . The rest of the architecture is identical to the original KGAT . Finally , at test time , we select one c i at a time , as the target t , and compute its probability , which ranks c i .", "entities": [[6, 7, "MethodName", "Transformer"], [57, 58, "MethodName", "Transformer"], [83, 84, "MethodName", "Transformer"], [112, 113, "MetricName", "loss"], [118, 119, "DatasetName", "sof"], [165, 166, "MethodName", "RoBERTa"], [174, 175, "MethodName", "RoBERTa"], [180, 181, "DatasetName", "ASNQ"], [212, 213, "DatasetName", "sof"], [265, 266, "MethodName", "Transformer"], [342, 343, "MetricName", "loss"], [405, 406, "MetricName", "loss"], [416, 417, "MetricName", "loss"], [446, 449, "MethodName", "Graph Attention Network"], [454, 456, "TaskName", "fact verification"], [864, 865, "MethodName", "RoBERTa"]]}
{"text": "We developed ASR architecture described in Figure 2 . To reduce the noise that may be introduced by irrelevant c i , we use the Answer Support Classifier ( ASC ) , which classifies each ( t , c i ) in one of the following four classes : 0 : t and c i are both correct , 1 : t is correct while c i is not , 2 : vice versa , and 3 : both incorrect . This multi - classifier , described in Figure 1b , is built on top a RoBERTa Transformer , which produced a PairWise Representation ( PWR ) . ASC is trained end - to - end with the rest of the network in a multi - task learning fashion , using its specific cross - entropy loss , computed with the labels above . 3 . The ASR ( see Figure 1c ) uses the joint representation of ( q , t ) with ( t , c i ) , i = 1 , .. , k , where t and c i are the top - candidates reranked by PR . The k representations are summarized by applying a max - pooling operation , which will aggregate all the supporting or not supporting properties of the candidates with respect to the target answer . The concatenation of the PR embedding with the max - pooling embedding is given as input to the final classification layer , which scores t with respect to q , also using the information from the other candidates . For training and testing , we select a t from the k + 1 candidates of q at a time , and compute its score . This way , we can rerank all the k + 1 candidates with their scores . Implementation details : ASR is a PR that also exploits the relation between t and A \\ { t } . We use RoBERTa to generate the [ CLS ] R d embedding of ( q , t ) = E t . We denote with\u00ca j the [ CLS ] output by another RoBERTa Transformer applied to answer pairs , i.e. , ( t , c j ) . Then , we concatenate E t to the max - pooling tensor from\u00ca 1 , .. , \u00ca k : V = [ E t : Maxpool ( [ \u00ca 1 , .. , \u00ca k ] ) ] , ( 1 ) where V R 2d is the final representation of the target answer t. Then , we use a standard feedforward network to implement a binary classification layer : p ( y i | q , t , C k ) = sof tmax ( V W T + B ) , where W R 2\u00d72d and B are parameters to transform the representation of the target answer t from dimension 2d to dimension 2 , which represents correct or incorrect labels . ASC labels There can be different interpretations when attempting to define labels for answer pairs . An alternative to the definition illustrated above is to use the following FEVER compatible encoding : 0 : t is correct , while c i can be any value , as also an incorrect c i may provide important context ( corresponding to FEVER Support label ) ; 1 : t is incorrect , c i correct , since c i can provide evidence that t is not similar to a correct answer ( corresponding to FEVER Refutal label ) ; and 2 : both are incorrect , in this case , nothing can be told ( corresponding to FEVER Neutral label ) .", "entities": [[49, 50, "DatasetName", "0"], [96, 97, "MethodName", "RoBERTa"], [97, 98, "MethodName", "Transformer"], [124, 128, "TaskName", "multi - task learning"], [136, 137, "MetricName", "loss"], [330, 331, "MethodName", "RoBERTa"], [361, 362, "MethodName", "RoBERTa"], [362, 363, "MethodName", "Transformer"], [439, 441, "MethodName", "feedforward network"], [461, 462, "DatasetName", "sof"], [530, 531, "DatasetName", "FEVER"], [534, 535, "DatasetName", "0"], [561, 562, "DatasetName", "FEVER"], [594, 595, "DatasetName", "FEVER"], [617, 618, "DatasetName", "FEVER"]]}
{"text": "The goal of MASR is to measure the relation between k + 1 target answers , t 0 , .. , t k . The representation of each target answer is the embedding V R 2d from Equation 1 in ASR . Then , we concatenate the hidden vectors of k + 1 target answers to form a matrix V ( q , k+1 ) R ( k+1 ) \u00d72d . We use this matrix and a classification layer weights W R 2d , and compute a standard multi - class classification loss : L M ASR = y * log ( sof tmax ( V ( q , k+1 ) W T ) , ( 2 ) where y is a one - hot - vector , and | y | = | k + 1 | .", "entities": [[17, 18, "DatasetName", "0"], [88, 92, "TaskName", "multi - class classification"], [92, 93, "MetricName", "loss"], [102, 103, "DatasetName", "sof"]]}
{"text": "Metrics The performance of QA systems is typically measured with Accuracy in providing correct answers , i.e. , the percentage of correct responses . This is also referred to Precision - at - 1 ( P@1 ) in the context of reranking , while standard Precision and Recall are not essential in our case as we assume the system does not abstain from providing answers . We also use Mean Average Precision ( MAP ) and Mean Reciprocal Recall ( MRR ) evaluated on the test set , using the entire set of candidates for each Table 4 : Results on WikiQA , TREC - QA and WQA , using RoBERTa base Transformer . \u2020 is used to indicate that the difference in P@1 between ASR and the other marked systems is statistically significant at 95 % . JOINT - MULTICLASSIFER JOINT - PAIR KGAT ASR 1 2 3 4 5 \u22123 \u22122 \u22121 0 1 2 3 4 5 k Improvement ( % ) Figure 2 : Impact of k on the WQA dev . set question ( this varies according to the dataset ) , to have a direct comparison with the state of the art . Models We use the pre - trained RoBERTa - Base ( 12 layer ) and RoBERTa - Large - MNLI ( 24 layer ) models , which were released as checkpoints for use in downstream tasks 4 . Reranker training We adopt Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 2e - 5 for the transfer step on the ASNQ dataset ( Garg et al , 2020 ) , and a learning rate of 1e - 6 for the adapt step on the target dataset . We apply early stopping on the development set of the target corpus for both fine - tuning steps based on the highest MAP score . We set the max number of epochs equal to 3 and 9 for the adapt and transfer steps , respectively . We set the maximum sequence length for RoBERTa to 128 tokens . KGAT and ASR training Again , we use the Adam optimizer with a learning rate of 2e - 6 for training the ASR model on the target dataset . We utilize 1 Tesla V100 GPU with 32 GB memory and a train batch size of eight . We set the maximum sequence length for RoBERTa Base / Large to 130 tokens and the number of training epochs to 20 . The other training configurations are the same of the original KGAT model from ( Liu et al , 2020 ) . We use two transformer models for ASR : a RoBERTa 4 https://github.com/pytorch/fairseq Base / Large for PR , and one for ASC . We set the maximum sequence length for RoBERTa to 128 tokens and the number of epochs to 20 .", "entities": [[10, 11, "MetricName", "Accuracy"], [29, 30, "MetricName", "Precision"], [35, 36, "MetricName", "P@1"], [45, 46, "MetricName", "Precision"], [47, 48, "MetricName", "Recall"], [70, 72, "MetricName", "Average Precision"], [73, 74, "DatasetName", "MAP"], [78, 79, "MetricName", "Recall"], [80, 81, "MetricName", "MRR"], [101, 102, "DatasetName", "WikiQA"], [103, 104, "DatasetName", "TREC"], [110, 111, "MethodName", "RoBERTa"], [112, 113, "MethodName", "Transformer"], [123, 124, "MetricName", "P@1"], [154, 155, "DatasetName", "0"], [206, 207, "MethodName", "RoBERTa"], [214, 215, "MethodName", "RoBERTa"], [218, 219, "DatasetName", "MNLI"], [241, 242, "MethodName", "Adam"], [242, 243, "HyperparameterName", "optimizer"], [252, 254, "HyperparameterName", "learning rate"], [264, 265, "DatasetName", "ASNQ"], [276, 278, "HyperparameterName", "learning rate"], [293, 295, "MethodName", "early stopping"], [313, 314, "DatasetName", "MAP"], [320, 323, "HyperparameterName", "number of epochs"], [344, 345, "MethodName", "RoBERTa"], [358, 359, "MethodName", "Adam"], [359, 360, "HyperparameterName", "optimizer"], [362, 364, "HyperparameterName", "learning rate"], [391, 393, "HyperparameterName", "batch size"], [403, 404, "MethodName", "RoBERTa"], [449, 450, "MethodName", "RoBERTa"], [470, 471, "MethodName", "RoBERTa"], [476, 479, "HyperparameterName", "number of epochs"]]}
{"text": "Table 4 reports the P@1 , MAP and MRR of the rerankers , and different answer supporting models on WikiQA , TREC - QA and WQA datasets . As WQA is an internal dataset , we only report the improvement over PR in the tables . All models use RoBERTa - Base pre - trained checkpoint and start from the same set of k candidates reranked by PR ( state - of - the - art model ) . The table shows that : PR replicates the MAP and MRR of the stateof - the - art reranker by Garg et al ( 2020 ) on WikiQA . Joint Model Multi - classifier performs lower than PR for all measures and all datasets . This is in line with the findings of Bonadiman and Moschitti ( 2020 ) , who also did not obtain improvement when jointly used all the candidates altogether in a representation . Joint Model Pairwise differs from ASR as it concatenates the embeddings of the ( q , c i ) , instead of using max - pooling , and does not use any Answer Support Classifier ( ASC ) . Still , it exploits the idea of aggregating the information of all pairs ( q , c i ) with respect to a target answer t , which proves to be effective , as the model improves on PR over all measures and datasets . Our KGAT version for AS2 also improves PR over all datasets and almost all measures , confirming that the idea of using candidates as support of the target answer is generally valid . However , it is not superior to Joint Model Pairwise . ASR achieves the highest performance among all models ( but MASR - FP on WQA ) , all datasets , and all measures . For example , it outperforms PR by almost 3 absolute percent points in P@1 on WikiQA , and by almost 6 points on TREC from 91.18 % to 97.06 % , which corresponds to an error reduction of 60 % . We perform randomization test ( Yeh , 2000 ) to verify if the models significantly differ in terms of prediction outcome . We use 100 , 000 trials for each calculation . The results confirm the statistically significant difference between ASR and all the baselines , with p < 0.05 for WikiQA , and between ASR and all models ( i.e. , including also KGAT ) on WQA .", "entities": [[4, 5, "MetricName", "P@1"], [6, 7, "DatasetName", "MAP"], [8, 9, "MetricName", "MRR"], [19, 20, "DatasetName", "WikiQA"], [21, 22, "DatasetName", "TREC"], [49, 50, "MethodName", "RoBERTa"], [87, 88, "DatasetName", "MAP"], [89, 90, "MetricName", "MRR"], [106, 107, "DatasetName", "WikiQA"], [321, 322, "MetricName", "P@1"], [323, 324, "DatasetName", "WikiQA"], [331, 332, "DatasetName", "TREC"], [400, 401, "DatasetName", "WikiQA"]]}
{"text": "As the state of the art for AS2 is obtained using RoBERTa Large , we trained KGAT and ASR using this pre - trained language model . Table 5 also reports the comparison with PR , which is the official state of the art . Again , our PR replicates the results of Garg et al ( 2020 ) , obtaining slightly lower performance on WikiQA but higher on TREC - QA . KGAT performs lower than PR on both datasets . ASR establishes the new state of the art on WikiQA with an MAP of 92.80 vs. 92.00 . The P@1 also significantly improves by 2 % , i.e. , achieving 89.71 , which is impressively high . Also , on TREC - QA , ASR outperforms all models , being on par with PR regarding P@1 . The latter is 97.06 , which corresponds to mistaking the answers of only two questions . We manually checked these and found out that these were two annotation errors : ASR achieves perfect accuracy while PR only mistakes one answer . Of course , this just provides evidence that PR based on RoBERTa - Large solves the task of selecting the best answers ( i.e. , measuring P@1 on this dataset is not meaningful anymore ) . Sec . 4.1 ) . ACC is the overall accuracy while F1 refers to the category 0 . We note that ASC in MASR - FP achieves the highest accuracy with respect to the average over all datasets . This happens since we pre - fine - tuned it with the FEVER data .", "entities": [[11, 12, "MethodName", "RoBERTa"], [65, 66, "DatasetName", "WikiQA"], [69, 70, "DatasetName", "TREC"], [91, 92, "DatasetName", "WikiQA"], [94, 95, "DatasetName", "MAP"], [101, 102, "MetricName", "P@1"], [122, 123, "DatasetName", "TREC"], [137, 138, "MetricName", "P@1"], [172, 173, "MetricName", "accuracy"], [191, 192, "MethodName", "RoBERTa"], [206, 207, "MetricName", "P@1"], [221, 222, "MetricName", "ACC"], [224, 226, "MetricName", "overall accuracy"], [227, 228, "MetricName", "F1"], [232, 233, "DatasetName", "0"], [245, 246, "MetricName", "accuracy"], [267, 268, "DatasetName", "FEVER"]]}
{"text": "We analyzed examples for which ASR is correct and PR is not . Tab . 7 shows that , given q and k = 3 candidates , PR chooses c 1 , a suitable but wrong answer . This probably happens since the answer best matches the syntactic / semantic pattern of the question , which asks for a type of color , indeed , the answer offers such type , primary colors . PR does not rely on any background information that can support the set of colors in the answer . In contrast , ASR selects c 2 as it can rely on the support of other answers . Its ASC provides an average score for the category 0 ( both members are correct ) of c 2 , i.e. , 1 k i = 2 ASC ( c 2 , c i ) = 0.653 , while for c 1 the average score is significant lower , i.e. , 0.522 . This provides higher support for c 2 , which is used by ASR to rerank the output of PR . Tab . 8 shows an interesting case where all the sentences contain the required information , i.e. , February . However , PR and ASR both choose answer c 0 , which is correct but not natural , as it provides the requested information indirectly . Also , it contains a lot of ancillary information . In contrast , MASR is able to rerank the best answer , c 1 , in the top position .", "entities": [[22, 24, "HyperparameterName", "k ="], [120, 121, "DatasetName", "0"], [213, 214, "DatasetName", "0"]]}
{"text": "Why - question answering ( why - QA ) tasks retrieve from a text archive answers to such why - questions as \" Why does honey last such a long time ? \" Previous why - QA methods retrieve from a text archive answer passages , each of which consists of several sentences , like A in Table 1 ( Girju , 2003 ; Higashinaka and Isozaki , 2008 ; Oh et al , , 2013Sharp et al , 2016 ; Verberne et al , 2011 ) , and then determine whether the passages answer the question . A proper answer passage must contain ( 1 ) a paraphrase of the why - question ( e.g. , the underlined texts in Table 1 ) and ( 2 ) the reasons or the causes ( e.g. , the bold texts in Table 1 ) of Q Why does honey last a long time ? A While excavating Egypt 's pyramids , archaeologists have found pots of honey in an ancient tomb : thousands of years old and still preserved . Honey can last a long time due to three special properties . Its average pH is 3.9 , which is quite acidic . Such high level of acidity is certainly hostile and hinders the growth of many microbes . Though honey contains around 17 - 18 % water , its water activity is too low to support the growth of microbes . Moreover honey contains hydrogen peroxide , which is thought to help prevent the growth of microbes in honey . Despite these properties , honey can be contaminated under certain circumstances . C Because its acidity , low water activity , and hydrogen peroxide together hinder the growth of microbes . Table 1 : Answer passage A to why - question Q and its compact answer C the events described in the why - question , both of which are often written in multiple non - adjacent sentences . This multi - sentenceness implies that the answer passages often contain redundant parts that are not directly related to a why - question or its reason / cause and whose presence complicates the why - QA task . Highly accurate why - QA methods should be able to find the exact reason sought by a why - question in an answer passage without being distracted by redundancy . In this paper , we train a neural network ( NN ) to generate , from an answer passage , a vector representation of the non - redundant reason asked by a why - question , and exploit the generated vector representation as evidence for judging whether the passage answers the why - question . This idea was inspired by Ishida et al ( 2018 ) , who used a seq2seq model to automatically generate such compact answers as C in Table 1 from the answer passages retrieved by a why - QA method . Compact answers are sentences or phrases that express the reasons for a given why - question without redundancy . If we can use such automatically generated compact - answers to support a why - QA method in finding the exact reason of a whyquestion in these passages , why - QA accuracy may be improved . We actually tried this idea in a preliminary study in which we generated a compact answer from a given question - passage pair by using the compact - answer generation method of Iida et al ( 2019 ) and used the generated compactanswer along with the given question - passage pair to find proper answer passages . However , we were disappointed by the small performance improvement , as shown in our experimental results . We chose an alternative approach . Instead of generating a compact answer of an answer passage as word sequences , we devised a model to generate a compact - answer representation , which is a vector representation for a compact answer , from an answer passage . Inspired by the generative adversarial network ( GAN ) approach ( Goodfellow et al , 2014 ) , we developed an adversarial network called the Adversarial networks for Generating compact - answer Representation ( AGR ) . Like the original GAN , an AGR is composed of a generator and a discriminator : the generator network is trained for generating ( from answer passages ) fake representations to make it hard for the discriminator network to distinguish these fake representations from the true representations derived from manually created compact - answers . We combined the generator network in the AGR with an extension of the state - of - the - art why - QA method . Our evaluation against a Japanese open - domain why - QA dataset , which was created using general web texts as a source of answer passages , revealed that the generator network significantly improved the accuracy of the top - ranked answer passages and that the combination significantly outperformed several strong baselines , including a combination of a generator network and a BERT model ( Devlin et al , 2019 ) . This combination also outperformed a vanilla BERT model , suggesting that the generator network in our AGR may be effective even if it is combined with many types of NN architectures . Another interesting point is that the performance improved even when we replaced , as the inputs to AGR , the word embedding vectors that represent an answer passage , with a random vector . This observation warrants further exploration in our future work . Finally , we applied our AGR to a distantly su - ( Chen et al , 2017 ) , which is an extension of a machinereading task , to check whether it is applicable to other datasets . We combined our generator network with a state - of - the - art DS - QA method , OpenQA ( Lin et al , 2018 ) , and used a generated compact - answer representation from a given passage as evidence to 1 ) select relevant passages from the retrieved ones and 2 ) find an answer from the selected passages . Although the task was not our initial target ( why - QA ) and the answers in the DS - QA task were considerably shorter than those in the why - QA , experiments using three publicly available datasets ( Quasar - T ( Dhingra et al , 2017 ) , SearchQA ( Dunn et al , 2017 ) , and Triv - iaQA ( Joshi et al , 2017 ) ) revealed that the generator network improved the performance in most cases . This suggests that AGR may be applicable to many QA - like tasks . 2 Why - QA Model Figure 1 illustrates the architecture of our why - QA model and the AGR . Our why - QA model computes the probability that a given answer passage describes a proper answer to a given why - question using the representations of a question , an answer passage , and a compact answer . The probability ( the why - QA model 's final output ) is computed from these representations by our answer selection module , which is a logistic regression layer with dropout and softmax output . The representations of why - questions and answer passages are generated by Convolutional Neural Networks ( CNNs ) ( Collobert et al , 2011 ; LeCun et al , 1998 ) that ( 1 ) are augmented by two types of attention mechanisms , similarityattention and causality - attention , and ( 2 ) are given two types of word embeddings , general word embeddings computed by word2vec ( Mikolov et al , 2013 ) using Wikipedia and causal word embeddings ( Sharp et al , 2016 ) . Note that in computing a question 's representation , the answer passage is given to the question encoder to guide the computation . Likewise the passage encoder is given the question and the representation of the compact answer . We represent these information flows with dotted arrows in Fig . 1 ( a ) . The representations of compact answers are created by a generator network called a fakerepresentation generator ( F in Fig . 1 ( a ) ) , which is pre - trained in an adversarial learning manner ( Fig . 1 ( b ) ) . During the training of the whole why - QA model , the generator 's parameters are fixed and no further fine - tuning is conducted . In the next section , we describe our main contribution : the AGR and the fake - representation generator . The entire why - QA model can be seen as an extension of the state - of - the - art why - QA method . Its details are described in Section A of the supplementary materials .", "entities": [[2, 4, "TaskName", "question answering"], [467, 468, "MethodName", "seq2seq"], [543, 544, "MetricName", "accuracy"], [576, 578, "TaskName", "answer generation"], [670, 671, "DatasetName", "Inspired"], [673, 676, "MethodName", "generative adversarial network"], [677, 678, "MethodName", "GAN"], [710, 711, "MethodName", "GAN"], [822, 823, "MetricName", "accuracy"], [849, 850, "MethodName", "BERT"], [865, 866, "MethodName", "BERT"], [1076, 1079, "DatasetName", "Quasar - T"], [1087, 1088, "DatasetName", "SearchQA"], [1212, 1214, "TaskName", "answer selection"], [1219, 1221, "MethodName", "logistic regression"], [1225, 1226, "MethodName", "softmax"], [1287, 1289, "TaskName", "word embeddings"], [1291, 1293, "TaskName", "word embeddings"], [1307, 1309, "TaskName", "word embeddings"]]}
{"text": "In our implementation , both F and R are networks with identical structure called Encoder . They are defined as follows , where p , c , and q are respectively an answer passage , a manually created compact - answer , and a why - question : F ( p | q ) = Encoder ( p ; \u03b8 F , q ) R ( c | q ) = Encoder ( c ; \u03b8 R , q ) Here \u03b8 F and \u03b8 R represent the parameters of networks F and R. The details of Encoder are described below . Discriminator D ( r ) takes as input r , either the output of F ( p | q ) or that of R ( c | q ) , and computes the probability that given representation r comes from a real compact - answer using a feedforward network with two hidden layers ( 100 nodes in the first layer and 50 in the second layer ) and a logistic regression layer on top of the hidden layers . We used sigmoid outputs by the logistic regression layer as the output probability .", "entities": [[59, 60, "HyperparameterName", "\u03b8"], [75, 76, "HyperparameterName", "\u03b8"], [81, 82, "HyperparameterName", "\u03b8"], [84, 85, "HyperparameterName", "\u03b8"], [149, 151, "MethodName", "feedforward network"], [171, 173, "MethodName", "logistic regression"], [187, 189, "MethodName", "logistic regression"]]}
{"text": "Figure 2 illustrates the architecture shared by our fake - representation generator F and real - representation generator R , namely , Encoder ( t ; \u03b8 , q ) , where \u03b8 is a set of parameters , q is a why - question , and t is either an answer passage or a manually created compact - answer . Encoder ( t ; \u03b8 , q ) first represents question q and passage / compact - answer t with pre - trained word embeddings , which are supplemented with attention mechanisms . The resulting attention - weighted word embeddings are given to convolutional neural networks ( CNNs ) that generate a single feature vector , which is an output / value of Encoder ( t ; \u03b8 , q ) . In the following , we give an overview of the word embeddings , the attention mechanisms , and the CNNs used in Encoder ( t ; \u03b8 , q ) . All of these techniques were proposed by previous works . Further details are given in Section B of the supplementary materials .", "entities": [[26, 27, "HyperparameterName", "\u03b8"], [32, 33, "HyperparameterName", "\u03b8"], [65, 66, "HyperparameterName", "\u03b8"], [84, 86, "TaskName", "word embeddings"], [99, 101, "TaskName", "word embeddings"], [128, 129, "HyperparameterName", "\u03b8"], [143, 145, "TaskName", "word embeddings"], [159, 160, "HyperparameterName", "\u03b8"]]}
{"text": "The pre - trained word embeddings used in Encoder ( t ; \u03b8 , q ) were obtained by concatenating two types of d - dimensional word embeddings ( d = 300 in this work ) : general word embeddings and causal word embeddings . General word embeddings are widely used embedding vectors ( 300 dimensions ) that were pretrained for about 1.65 million words by applying word2vec ( Mikolov et al , 2013 ) to about 35 million sentences from Japanese Wikipedia ( January 2015 version ) . Causal word embeddings ( Sharp et al , 2016 ) were proposed for representing the causal associations between words . Sharp et al ( 2016 ) cre - ated a set of cause - effect word pairs by paring each content word in a cause part with each content word in an effect part of the same causality expression , such as \" Volcanoes erupt because magma pushes through vents and fissures . \" In this work , we extracted 100 million causality expressions from 4 - billion Japanese web pages using the causality recognizer of Oh et al ( 2013 ) . Then , following Sharp et al ( 2016 ) , we trained 300dimensional causal word embeddings for about 1.85 million words by applying the generalized skip - gram embedding model of Levy and Goldberg ( 2014 ) to the causality expressions .", "entities": [[4, 6, "TaskName", "word embeddings"], [12, 13, "HyperparameterName", "\u03b8"], [26, 28, "TaskName", "word embeddings"], [38, 40, "TaskName", "word embeddings"], [42, 44, "TaskName", "word embeddings"], [45, 46, "DatasetName", "General"], [46, 48, "TaskName", "word embeddings"], [90, 92, "TaskName", "word embeddings"], [206, 208, "TaskName", "word embeddings"]]}
{"text": "att is given to CNNs to generate final representation r t of a given passage / compact - answer t. The CNNs resembles those in Kim ( 2014 ) . Convolutions are performed over the word embeddings using both multiple filters and multiple filter windows ( e.g. , sliding over 1 , 2 , or 3 word windows at a time and 100 filters for each window ) . An average pooling operation is applied to the convolution results to generate representation r t , which is the output / value of Encoder ( t ; \u03b8 , q ) ; r t = Encoder ( t ; \u03b8 , q ) . In our experiments , we set the dimension of representation r t to 300 . 4 Why - QA Experiments", "entities": [[35, 37, "TaskName", "word embeddings"], [70, 72, "MethodName", "average pooling"], [77, 78, "MethodName", "convolution"], [96, 97, "HyperparameterName", "\u03b8"], [108, 109, "HyperparameterName", "\u03b8"]]}
{"text": "In our proposed methods and their variants , all the weights in the CNNs were initialized using He 's method ( He et al , 2015 ) , and the other weights in our why - QA model were initialized randomly with a uniform distribution in the range of ( - 0.01 , 0.01 ) . For the CNN - based components , we set the window size of the filters to \" 1 , 2 , 3 \" with 100 filters each 2 . We used dropout ( Srivastava et al , 2014 ) with probability 0.5 on the final logistic regression layer . All of these hyper - parameters were chosen with our development data . We optimized the learned parameters with the Adam stochastic gradient descent ( Kingma and Ba , 2015 ) . The learning rate was set to 0.001 , and the batch size for each iteration was set to 20 .", "entities": [[101, 103, "MethodName", "logistic regression"], [125, 126, "MethodName", "Adam"], [126, 129, "MethodName", "stochastic gradient descent"], [138, 140, "HyperparameterName", "learning rate"], [147, 149, "HyperparameterName", "batch size"]]}
{"text": "Same as BERT+FOP except that it used FRV instead of FOP for producing compact - answer representation . To pre - train the BERT - based models , we used a combination of sentences extracted from Japanese Wikipedia articles ( August 2018 version ) and causality expressions automatically recognized from a causality recognizer ( Oh et al , 2013 ) . This data mix consists of 75 % of sentences extracted from Wikipedia ( 14 , 675 , 535 sentences taken out of 784 , 869 articles randomly sampled ) and 25 % of cause and effect phrases taken from causality expressions ( 4 , 891 , 846 phrases from 2 , 445 , 923 causal relations ) . This ratio was determined through preliminary experiments using the development data . For the pre - training parameters , we followed the settings of BERT BASE in Devlin et al ( 2019 ) 3 except for the batch size of 50 . We ran 3 epochs with the learning rate of 1e - 5 for finetuning the BERT - based models 4 . + + + + + + E [ CLS ] E ' E \u2032 ) E Q E Q E P E 0 E 1 E N+M+1 E [ A BERT - based model , BERT , takes a questionpassage pair as input and computes the input representation using token , segment , position , and attention feature embeddings ( Fig . 3 ) . For the input representation computation , the original BERT only used the token , segment , and position embeddings , while BERT additionally used the attention feature embeddings 5 to exploit the same similarity - attention and causalityattention features used in our proposed method . We used the attention feature embeddings during the fine - tuning and testing , but not during the pretraining of the BERT - based model . The attention feature embeddings for answer passages ( i.e. , E sim w 1 , , E sim w M , and E caus w 1 , , E caus w M ) were computed from the same attention feature vectors , a s and a c , as those in our proposed methods ; those for the other parts ( i.e. , questions , [ CLS ] , and [ SEP ] ) were computed from a zero vector ( indicating no attention feature ) . The transformer encoder processed the input representation to gen - 3 12 - layers , 768 hidden states , 12 heads and training for 1 - million steps with the warmup rate of 1 % using Adam optimizer with the learning rate of 1e - 4 . 4 We tested all the combinations of epochs { 1 , 2 , 3 , 4 , 5 } and learning rates of { 1e - 5 , 2e - 5 , 3e - 5 } and chose the one that maximized the performance on the development data in W hySet . 5 We also evaluated a BERT - based model that did not use the attention feature embeddings , but its P@1 ( 41.4 ) was much lower than that of BERT ( 51.2 ) . P@1 MAP Oh et al ( 2013 ) 41.8 41.0 Sharp et al ( 2016 )", "entities": [[23, 24, "MethodName", "BERT"], [143, 144, "MethodName", "BERT"], [144, 145, "MethodName", "BASE"], [156, 158, "HyperparameterName", "batch size"], [167, 169, "HyperparameterName", "learning rate"], [176, 177, "MethodName", "BERT"], [204, 205, "DatasetName", "0"], [212, 213, "MethodName", "BERT"], [217, 218, "MethodName", "BERT"], [255, 256, "MethodName", "BERT"], [268, 269, "MethodName", "BERT"], [313, 314, "MethodName", "BERT"], [441, 442, "MethodName", "Adam"], [442, 443, "HyperparameterName", "optimizer"], [445, 447, "HyperparameterName", "learning rate"], [509, 510, "MethodName", "BERT"], [524, 525, "MetricName", "P@1"], [534, 535, "MethodName", "BERT"], [539, 540, "MetricName", "P@1"], [540, 541, "DatasetName", "MAP"]]}
{"text": "Table 3 shows the performances of all the methods in the Precision of the top answer ( P@1 ) and the Mean Average Precision ( MAP ) ( Oh et al , 2013 ) . Note that the Oracle method indicates the performance of a fictional method that ranks the answer passages perfectly , i.e. , it locates all the m correct answers to a question in the top - m ranks , based on the gold - standard labels . This performance is the upper bound of those of all the implementable methods . Our proposed method , Ours ( OP ) , outperformed all the other methods . Our starting point , i.e. , BASE , was already superior to the methods in the previous works . Compared with BASE and BASE+AddTr , neither of which used compactanswer representations or fake - representation generator F , Ours ( OP ) gave 3.4 % and 2.8 % improvement in P@1 , respectively . It also outperformed BASE+CAns and BASE+CEnc , which generated compact - answer representations in a way different from the proposed method , and BASE+Enc , which trained the fake - representation generator without adversarial learning . These performance differences were statistically significant ( p < 0.01 by the McNemar 's test ) . Ours ( OP ) also outperformed all the BERTbased models but an interesting point is that fakerepresentation generator F boosted the performance of the BERT - based models ( statistically significant with p < 0.01 by the McNemar 's test ) . These results suggest that AGR is effective in both our why - QA model and our BERT - based model .", "entities": [[11, 12, "MetricName", "Precision"], [17, 18, "MetricName", "P@1"], [22, 24, "MetricName", "Average Precision"], [25, 26, "DatasetName", "MAP"], [116, 117, "MethodName", "BASE"], [131, 132, "MethodName", "BASE"], [160, 161, "MetricName", "P@1"], [241, 242, "MethodName", "BERT"], [275, 276, "MethodName", "BERT"]]}
{"text": "Another interesting point is that Ours ( RV ) , in which fake - representation generator F RV was trained using random vectors , achieved almost the same performance as that of Ours ( OP ) . This result was puzzling , so we first checked whether F RV 's output was not just random noise ( which could prevent the why - QA model from overfitting ) by replacing in Ours ( RV ) the output of F RV by random vectors . Although we sampled the random vectors from different distribution types with various ranges , we obtained at best similar performance to that of BASE : 51.6 in P@1 . This result confirms that it is not trivial to mimic F RV using random vectors at least . We investigated the F RV 's output to check whether it actually focused on the compact answer in a given passage . We computed the following three representation sets from a gold set of 3 , 608 triples of why - questions , answer passages and manually created compact - answers that do not overlap with CmpAns : { r org i } : F RV 's output with the pairs of a whyquestion and an answer passage in the gold set as its input ; { r in i } : F RV 's output for the same input as { r org i } , where we replaced the word embeddings of all the content words in the answer passages that also appeared in the associated gold compact - answers with random vectors ; { r out i } : F RV 's output for the same input as { r org i } , where we replaced the word embeddings of all the content words in the answer passages that did not appear in the associated gold compact - answers with random vectors 6 . If F RV perfectly focuses on the gold standard compact - answers , for each question - passage pair , 6 For both r in i and r out i , we never replaced the word embeddings for the words that also appeared in the question . r out i should be the same as r org i and r in i should significantly differ from r org i . Next we computed the average Euclidian distance among { r org i } , { r in i } and { r out i } . The average distance ( 2.67 ) between { r org i } and { r out i } was much smaller than the average distance ( 13.3 ) between { r org i } and { r in i } . Note that we replaced the word embeddings for much more words with random vectors in the computation of { r out i } than those in the computation of { r in i } ( 38.1 words vs. 5.6 words ) . This implies that the distance between { r org i } and { r out i } might be much larger than that between { r org i } and { r in i } if F RV focused equally on every answer passage word . However , the actual results suggest that this is not the case . Although we can not draw decisive conclusions due to the complex nature of neural networks , we believe from the results that F RV does actually focus more on words that are a part of a compact answer than on other words . We also computed { r org i } , { r in i } , and { r out i } with fakerepresentation generator F OP in the same way and observed the same tendency .", "entities": [[107, 108, "MethodName", "BASE"], [111, 112, "MetricName", "P@1"], [242, 244, "TaskName", "word embeddings"], [292, 294, "TaskName", "word embeddings"], [354, 356, "TaskName", "word embeddings"], [461, 463, "TaskName", "word embeddings"]]}
{"text": "We tested our framework on another task , the distantly supervised open - domain question answering ( DS - QA ) task ( Chen et al , 2017 ) , to check its generalizability . Table 4 shows the statistics for the datasets used in this experiment . The first three , Quasar - T , SearchQA , and TriviaQA provided by Lin et al ( 2018 ) , were used for training and evaluating DS - QA methods . The training data of SQuAD v1.1 ( Rajpurkar et al , 2016 ) was used for training our AGR . The SQuAD dataset consisted of the triples of a question , an answer , and a paragraph that includes the answer . We assume that the answers are our compact answers , although the answers in the dataset are consecutive short word sequences ( 2.8 words on average ) , whose majority are noun phrases , unlike the compact answers for our why - QA experiment , i.e. , sentences or phrases ( 8.3 words on average ) . We trained our AGR with all the triples of a question , an answer , and a paragraph in the training data of SQuAD - v1.1 under the same settings for the AGR 's hyperparameters as in our why - QA experiment except that we use neither causal word embeddings nor causality - attention . In this experiment , we used the AGR training schemes for Ours ( OP ) and Ours ( RV ) . We used the 300 - dimensional GloVe word embeddings learned from 840 billion tokens in the web crawl data ( Pennington et al , 2014 ) , as general word embeddings . Then we combined the resulting fake - representation generator F in the AGR with the state - of - the - art DS - QA method , OpenQA ( Lin et al , 2018 ) 7 . We also used the hyperparameters presented in Lin et al ( 2018 ) . OpenQA is composed of two components : a paragraph selector to choose relevant paragraphs ( or answer passages in our terms ) from a set of paragraphs and a paragraph reader to extract answers from the selected paragraphs . For identifying answer a to given question q from set of paragraphs P = { p i } , the paragraph selector and the paragraph reader respectively compute probabilities P r ( p i | q , P ) and P r ( a | q , p i ) , and final output P r ( a | q , P ) is obtained by combining the probabilities . We introduced c i , which is a compact - answer representation generated by fakerepresentation generator F with question q and paragraph p i as its input , to the computation of the probabilities as follows : P r ( a | q , P , C ) = i P r ( a | q , p i , c i ) P r ( p i | q , P , c i ) In the original OpenQA , the paragraph selector and the reader use bidirectional stacked RNNs for encoding paragraphs , where word embeddings p i of a paragraph is used as the input . In our implementation , we computed attention - weighted embeddingp i of a paragraph by using compactanswer representation c i . Given word embedding p j i for the j - th word in paragraph p i , its attentionweighted embeddingp j i was computed by using a bilinear function ( Sutskever et al , 2009 ) : p j i = softmax j ( p T i Mc i ) p j i , where M R d\u00d7d is a trainable matrix , softmax j ( x ) denotes the j - th element of the softmaxed vector of x , and d = 300 . We gave [ p j i ; p j i ] , a concatenation of p j i andp j i , as the word embedding of the j - th word in paragraph p i to the bidirectional stacked RNNs . Table 5 shows the performances of the four DS - QA methods : R 3 ( Wang et al , 2018 ) , OpenQA ( Lin et al , 2018 ) , Ours ( OP ) , and Ours ( RV ) evaluated against the Quasar - T , SearchQA and TriviaQA datasets . All the methods were evaluated with EM and F1 scores , following Lin et al ( 2018 ) . EM measures the percentage of predictions that exactly match one of the ground - truth answers and F1 is a metric that loosely measures the average overlap between the prediction and ground - truth answer . Note that both Ours ( OP ) and Ours ( RV ) outperformed both previous methods , R 3 and OpenQA , except for the F1 score for the TriviaQA dataset . Some of the improvements over the previous state - ofthe - art method , OpenQA , were statistically significant . These findings suggest that our framework can be effective for tasks other than the original why - QA and the other datasets .", "entities": [[11, 16, "TaskName", "open - domain question answering"], [52, 55, "DatasetName", "Quasar - T"], [56, 57, "DatasetName", "SearchQA"], [59, 60, "DatasetName", "TriviaQA"], [84, 85, "DatasetName", "SQuAD"], [101, 102, "DatasetName", "SQuAD"], [202, 203, "DatasetName", "SQuAD"], [227, 229, "TaskName", "word embeddings"], [261, 262, "MethodName", "GloVe"], [262, 264, "TaskName", "word embeddings"], [284, 286, "TaskName", "word embeddings"], [543, 545, "TaskName", "word embeddings"], [618, 619, "MethodName", "softmax"], [640, 641, "MethodName", "softmax"], [750, 753, "DatasetName", "Quasar - T"], [754, 755, "DatasetName", "SearchQA"], [756, 757, "DatasetName", "TriviaQA"], [765, 766, "MetricName", "EM"], [767, 768, "MetricName", "F1"], [778, 779, "MetricName", "EM"], [795, 796, "MetricName", "F1"], [839, 841, "MetricName", "F1 score"], [843, 844, "DatasetName", "TriviaQA"]]}
{"text": "Along with the flourishing development of neural networks , the sequence - to - sequence framework has been widely used for conversation response generation ( Shang et al , 2015 ; Sordoni et al , 2015 ) where the mapping from a query x to a reply y is learned with the negative log likelihood . However , these models suffer from the \" safe \" response problem . To address this problem , various methods have been proposed . Li et al ( 2016a ) propose a diversity - promoting objective function to encourage diverse responses during decoding . Zhou et al ( , 2018a introduce a responding mechanism between the encoder and decoder to generate various responses . incorporate topic information to generate informative responses . However , these models suffer from the deterministic structure when generating multiple diverse responses . Besides , during the training of these models , response utterances are only used in the loss function and ignored when forward computing , which can confuse the model for pursuing multiple objectives simultaneously . A few works explore to change the deterministic structure of sequence - to - sequence models by introducing stochastic latent variables . VAE is one of the most popular methods ( Bowman et al , 2016 ; Serban et al , 2017 ; Cao and Clark , 2017 ) , where the discourse - level diversity is modeled by a Gaussian distribution . However , it is observed that in the CVAE with a fixed Gaussian prior , the learned conditional posteriors tend to collapse to a single mode , resulting in a relatively simple scope ( Wang et al , 2017 ) . To tackle this , WAE ( Gu et al , 2018 ) which adopts a Gaussian mixture prior network with Wasserstein distance and VAD ( Du et al , 2018 ) which sequentially introduces a series of latent variables to condition each word in the response sequence are proposed . Although these models overcome the deterministic structure of sequence - to - sequence model , they still ignore the correlation of multiple valid responses and each case is trained separately . To consider the multiple responses jointly , the maximum likelihood strategy is explored . Zhang et al ( 2018a ) propose the maximum generated likelihood criteria which model a query with its multiple responses as a bag of instances and proposes to optimize the model towards the most likely answer rather than all possible responses . Similarly , Rajendran et al ( 2018 ) propose to reward the dialogue system if any valid answer is produced in the reinforcement learning phase . Though considering multiple responses jointly , the maximum likelihood strategy fails to utilize all the references during training with some cases ig - Figure 2 : The overall architecture of our proposed dialogue system where the two generation steps and testing process are illustrated . Given an input query x , the model aims to approximate the multiple responses in a bag { y } simultaneously with the continuous common and distinctive features , i.e. , the latent variables c and z obtained from the two generation phases respectively . nored . In our approach , we consider multiple responses jointly and model each specific response separately by a two - step generation architecture .", "entities": [[22, 24, "TaskName", "response generation"], [159, 160, "MetricName", "loss"], [200, 201, "MethodName", "VAE"], [249, 250, "MethodName", "CVAE"]]}
{"text": "In the first generation step , we aim to map from the input query x to the common feature c of the response bag { y } . Inspired by multi - instance learning ( Zhou , 2004 ) , we start from the simple intuition that it is much easier for the model to fit multiple instances from their mid - point than a random start - point , as illustrated in Figure 1 . To obtain this , we model the common feature of the response bag as the mid - point of embeddings of multiple responses . In practice , we first encode the input x with a bidirectional gated recurrent units ( GRU ) to obtain an input representation h x . Then , the common feature c is computed by a mapping network which is implemented by a feed - forward neural network whose trainable parameter is denoted as \u03b8 . The feature c is then fed into the response decoder to obtain the intermediate response y c which is considered to approximate all valid responses . Mathematically , the objective function is defined as : L avg = 1 | { y } | y { y } log p \u03c8 ( y | c ) ( 1 ) where | { y } | is the cardinality of the response bag { y } and p \u03c8 represents the response decoder . Besides , to measure how well the intermediate response y c approximates the mid - point response , we set up an individual discriminator and derive the mapping function to produce better results . As to the discriminator , we first project each utterance to an embedding space with fixed dimensionality via convolutional neural networks ( CNNs ) with different kernels as the process shown in Figure 3 . Then , the cosine similarity of the query and response embeddings is computed , denoted as D \u03b8 ( x , y ) , where \u03b8 represents trainable parameter in the discriminator . For the response bag { y } , the average response embedding is used to compute the matching score . The objective of intermediate response y c is then to minimize the difference between D \u03b8 ( x , y c ) and D \u03b8 ( x , { y } ) : L disc = E x , { y } , y c [ D \u03b8 ( x , y c ) \u2212 D \u03b8 ( x , { y } ) ] ( 2 ) where y c denotes the utterance produced by the decoder conditioned on the variable c. To overcome the discrete and non - differentiable problem , which breaks down gradient propagation from the discriminator , we adopt a \" soft \" continuous approximation ( Hu et al , 2017 ) : y ct \u223c softmax ( o t /\u03c4 ) ( 3 ) where o t is the logit vector as the inputs to the softmax function at time - step t and the temperature \u03c4 is set to \u03c4 0 as training proceeds for increasingly peaked distributions . The whole loss for the step - one generation is then L f irst = L avg + L disc ( 4 ) which is optimized by a minimax game with adversarial training ( Goodfellow et al , 2014 ) .", "entities": [[28, 29, "DatasetName", "Inspired"], [116, 117, "MethodName", "GRU"], [154, 155, "HyperparameterName", "\u03b8"], [325, 326, "HyperparameterName", "\u03b8"], [333, 334, "HyperparameterName", "\u03b8"], [376, 377, "HyperparameterName", "\u03b8"], [385, 386, "HyperparameterName", "\u03b8"], [408, 409, "HyperparameterName", "\u03b8"], [417, 418, "HyperparameterName", "\u03b8"], [482, 483, "MethodName", "softmax"], [503, 504, "MethodName", "softmax"], [518, 519, "DatasetName", "0"], [529, 530, "MetricName", "loss"]]}
{"text": "The second generation phase aims to model each specific response in a response bag respectively . In practice , we adopt the CVAE architecture , while two prominent modifications remain . Firstly , rather than modeling each response with the latent variable z from scratch , our model approximates each response based on the bag representation c with only the distinctive feature of each specific response remaining to be captured . Secondly , the prior common feature c can provide extra information for the sampling network which is supposed to decrease the latent searching space . Specifically , similar to the CVAE architecture , the overall objective for our model in the second generation phase is as below : L cvae = E q \u03c6 ( z | x , y , c ) p \u03b8 ( c | x ) [ log p \u03c8 ( y | c , z ) ] \u2212 D [ q \u03c6 ( z | x , y , c ) | | p \u03d5 ( z | x , c ) ] ( 5 ) where q \u03c6 represents the recognition network and p \u03d5 is the prior network with \u03c6 and \u03d5 as the trainable parameters ; D ( | | ) is the regularization term which measures the distance between the two distributions . In practice , the recognition networks are implemented with a feed - forward network that \u00b5 log \u03c3 2 = W q \uf8ee \uf8f0 h x h y c \uf8f9 \uf8fb + b q ( 6 ) where h x and h y are the utterance representations of query and response got by GRU respectively , and the latent variable z \u223c N ( \u00b5 , \u03c3 2 I ) . For the prior networks , we consider two kinds of implements . One is the vanilla CVAE model where the prior p \u03d5 ( z | x , c ) is modeled by a another feed - forward network conditioned on the representations h x and c as follows , \u00b5 log \u03c3 2 = W p h x c + b p ( 7 ) and the distance D ( | | ) here is measured by the KL divergence . For the other , we adopt the WAE model ( Gu et al , 2018 ) in which the prior p \u03d5 ( z | x , c ) is modeled by a mixture of Gaussian distributions GMM ( \u03c0 k , \u00b5 k , \u03c3 k 2 I ) K k=1 , where K is the number of Gaussian distributions and \u03c0 k is the mixture coefficient of the k - th component of the GMM module as computed : \u03c0 k = exp ( e k ) K i=1 exp ( e i ) ( 8 ) and \uf8ee \uf8f0 e k \u00b5 k log \u03c3 2 k \uf8f9 \uf8fb = W p , k h x c + b p , k ( 9 ) To sample an instance , Gumble - Softmax reparametrization trick ( Kusner and Hern\u00e1ndez - Lobato , 2016 ) is utilized to normalize the coefficients . The distance here is measured by the Wasserstein distance which is implemented with an adversarial discriminator . Recap that in the second generation phase the latent variable z is considered to only capture the distinctive feature of each specific response . Hence to distinguish the latent variable z for each separate response , we further introduce a multireference bag - of - word loss ( MBOW ) which requires the network to predict the current response y against the response bag : L mbow = E q \u03c6 ( z | x , y , c ) [ log p ( y bow | x , z ) + \u03bb log ( 1 \u2212 p ( { \u0233 } bow | x , z ) ) ] ( 10 ) where the probability is computed by a feedforward network f as the vanilla bag - of - word loss does ; { \u0233 } is the complementary response bag of y and its probability is computed as the average probability of responses in the bag ; and \u03bb is a scaling factor accounting for the difference in magnitude . As it shows , the MBOW loss penalizes the recognition networks if other complementary responses can be predicted from the distinctive variable z. Besides , since the probability of the complementary term may approach zero which makes it difficult to optimize , we actually adopt its lower bound in practice : log ( 1 \u2212 p ( y bow | x , z ) ) = log ( 1 \u2212 | y | t=1 e fy t | V | j e f j ) \u2265 log ( | y | t=1 ( 1 \u2212 e fy t | V | j e f j ) ) ( 11 ) where | V | is vocabulary size . Totally , the whole loss for the step - two generation is then : L second = L cvae + L mbow ( 12 ) which can be optimized in an end - to - end way .", "entities": [[22, 23, "MethodName", "CVAE"], [101, 102, "MethodName", "CVAE"], [120, 121, "MethodName", "cvae"], [135, 136, "HyperparameterName", "\u03b8"], [276, 277, "MethodName", "GRU"], [310, 311, "MethodName", "CVAE"], [458, 460, "HyperparameterName", "k ="], [511, 512, "MethodName", "Softmax"], [593, 594, "MetricName", "loss"], [667, 669, "MethodName", "feedforward network"], [678, 679, "MetricName", "loss"], [725, 726, "MetricName", "loss"], [841, 842, "MetricName", "loss"], [855, 856, "MethodName", "cvae"]]}
{"text": "Our whole model can be trained in an end - to - end fashion . To train the model , we first pre - train the word embedding using Glove ( ( Pennington et al , 2014 ) ) 1 . Then modules of the model are jointly trained by optimizing the losses L f irst and L second of the two generation phases respectively . To overcome the vanishing latent variable problem ( Wang et al , 2017 ) of CVAE , we adopt the KL annealing strategy ( Bowman et al , 2016 ) , where the weight of the KL term is gradually increased during training . The other technique employed is the MBOW loss which is able to sharpen the distribution of latent variable z for each specific response and alleviate the vanishing problem at the same time . During testing , diverse responses can be obtained by the two generation phases described above , where the distinctive latent variable z corresponding to each specific response is sampled from the prior probability network . This process is illustrated in Figure 2 . Capable of capturing the common feature of the response bag , the variable c is obtained from the mapping network and no intermediate utterance is required , which facilitates reducing the complexity of decoding .", "entities": [[81, 82, "MethodName", "CVAE"], [117, 118, "MetricName", "loss"]]}
{"text": "We compare our model with representative dialogue generation approaches as listed below : S2S : the vanilla sequence - to - sequence model with attention mechanism where standard beam search is applied in testing to generate multiple different responses . Method Multi - BLEU EMBEDDING Intra - Dist Inter - Dist BLEU - 1 BLEU - 2 G A E Dist - 1 Dist - 2 S2S+DB : the vanilla sequence - to - sequence model with the modified diversity - promoting beam search method ( Li et al , 2016b ) where a fixed diversity rate 0.5 is used . MMS : the modified multiple responding mechanisms enhanced dialogue model proposed by Zhou et al ( 2018a ) which introduces responding mechanism embeddings for diverse response generation . CVAE : the vanilla CVAE model with and without BOW ( bag - of - word ) loss ( CVAE+BOW and CVAE ) . WAE : the conditional Wasserstein autoencoder model for dialogue generation ( Gu et al , 2018 ) which models the distribution of data by training a GAN within the latent variable space . Ours : we explore our model Ours and conduct various ablation studies : the model with only the second stage generation ( Ours - First ) , the model without the discriminator ( Ours - Disc ) and multireference BOW loss ( Ours - MBOW ) , and the model with GMM prior networks ( Ours+GMP ) .", "entities": [[6, 8, "TaskName", "dialogue generation"], [43, 44, "MetricName", "BLEU"], [51, 52, "MetricName", "BLEU"], [54, 55, "MetricName", "BLEU"], [126, 128, "TaskName", "response generation"], [129, 130, "MethodName", "CVAE"], [133, 134, "MethodName", "CVAE"], [146, 147, "MetricName", "loss"], [150, 151, "MethodName", "CVAE"], [158, 159, "MethodName", "autoencoder"], [161, 163, "TaskName", "dialogue generation"], [179, 180, "MethodName", "GAN"], [226, 227, "MetricName", "loss"]]}
{"text": "To comprehensively evaluate the quality of generated response utterances , we adopt both automatic and human evaluation metrics : BLEU : In dialogue generation , BLEU is widely used in previous studies ( Yao et al , 2017 ; Shang et al , 2018 ) . Since multiple valid responses exist in this paper , we adopt multi - reference BLEU where the evaluated utterance is compared to provided multiple references simultaneously . Distinctness : To distinguish safe and commonplace responses , the distinctness score ( Li et al , 2016a ) is designed to measure word - level diversity by counting the ratio of distinctive [ 1 , 2 ] - grams . In our experiments , we adopt both Intra - Dist : the distinctness scores of multiple responses for a given query and Inter - Dist : the distinctness scores of generated responses of the whole testing set . Embedding Similarity : Embedding - based metrics compute the cosine similarity between the sentence embedding of a ground - truth response and that of the generated one . There are various ways to obtain the sentence - level embedding from the constituent word embeddings . In our experiments , we apply three most commonly used strategies : Greedy matches each word of the reference with the most similar word in the evaluated sentence ; Average uses the average of word embed - Input \u706b\u5c71\u55b7\u53d1\u77ac\u95f4\u7684\u4e00\u4e9b\u58ee\u89c2\u666f\u8c61 \u3002 \u518d\u8fc7\u5341\u5206\u949f\u5c31\u8fdb\u5165win8\u65f6\u4ee3\uff0c\u6211\u662f\u7cfb\u7edf\u5347\u7ea7\u63a7 \u3002 Query These are some magnificent sights at the moment of the volcanic eruption . There remain ten minutes before we entering the era of win8 . I am a geek of system updating . What application is this . \u5982\u6b64\u8fd9\u822c\u8fd9\u822c\u6dfc\u5c0f \u3002 \u6211\u89c9\u5f97\u8fd9\u6837\u7684\u754c\u9762\u66f4\u50cfwindows8 \u3002 It is so so imperceptible . I think interface like this looks more like windows8 . Table 3 : Case study for the generated responses from the testing set of Weibo , where the Chinese utterances are translated into English for the sake of readability . For each input query , we show four responses generated by each method and an additional intermediate utterance ( marked with underline ) for our model . dings ; and Extreme takes the most extreme value among all words for each dimension of word embeddings in a sentence . Since multiple references exist , for each utterance to be evaluated , we compute its score with the most similar reference . Human Evaluation with Case Analysis : As automatic evaluation metrics lose sight of the overall quality of a response ( Tao et al , 2018 ) , we also adopt human evaluation on 100 random samples to assess the generation quality with three independent aspects considered : relevance ( whether the reply is relevant to the query ) , diversity ( whether the reply narrates with diverse words ) and readability ( whether the utterance is grammatically formed ) . Each property is assessed with a score from 1 ( worst ) to 5 ( best ) by three annotators . The evaluation is conducted in a blind process with the utterance belonging unknown to the reviewers .", "entities": [[19, 20, "MetricName", "BLEU"], [22, 24, "TaskName", "dialogue generation"], [25, 26, "MetricName", "BLEU"], [60, 61, "MetricName", "BLEU"], [165, 167, "TaskName", "sentence embedding"], [194, 196, "TaskName", "word embeddings"], [312, 313, "DatasetName", "Weibo"], [371, 373, "TaskName", "word embeddings"]]}
{"text": "All models are trained with the following hyperparameters : both encoder and decoder are set to one layer with GRU cells , where the hidden state size of GRU is 256 ; the utterance length is limited to 50 ; the vocabulary size is 50 , 000 and the word embedding dimension is 256 ; the word embeddings are shared by the encoder and decoder ; all trainable parameters are initialized from a uniform distribution [ - 0.08 , 0.08 ] ; we employ the Adam ( Kingma and Ba , 2014 ) for optimization with a mini - batch size 128 and initialized learning rate 0.001 ; the gradient clipping strategy is utilized to avoid gradient explosion , where the gradient clipping value is set to be 5 . For the latent variable , we adopt dimensional size 256 and the component number of the mixture Gaussian for prior networks in WAE is set to 5 . As to the discriminator , we set the initialized learning rate as 0.0002 and use 128 different kernels for each kernel size in { 2 , 3 , 4 } . The size of the response bag is limited to 10 where the instances inside are randomly sampled for each mini - batch . All the models are implemented with Pytorch 0.4.1 4 .", "entities": [[19, 20, "MethodName", "GRU"], [28, 29, "MethodName", "GRU"], [49, 52, "HyperparameterName", "word embedding dimension"], [56, 58, "TaskName", "word embeddings"], [85, 86, "MethodName", "Adam"], [97, 101, "HyperparameterName", "mini - batch size"], [104, 106, "HyperparameterName", "learning rate"], [109, 111, "MethodName", "gradient clipping"], [121, 123, "MethodName", "gradient clipping"], [167, 169, "HyperparameterName", "learning rate"], [178, 180, "HyperparameterName", "kernel size"]]}
{"text": "Table 1 shows our main experimental results , with baselines shown in the top and our models at the bottom . The results show that our model ( Ours ) outperforms competitive baselines on various evaluation metrics . The Seq2seq based models ( S2S , S2S - DB and MMS ) tend to generate fluent utterances and can share some overlapped words with the references , as the high BLEU - 2 scores show . However , the distinctness scores illustrate that these models fail to generate multiple diverse responses in spite of the diversitypromoting objective and responding mechanisms used . We attribute this to that these models fail to consider multiple references for the same query , which may confuse the models and lead to a commonplace utterance . As to the CVAE and WAE models , with the latent variable to control the discourse - level diversity , diverse responses can be obtained . Compared against these previous methods , our model can achieve the best or second best performances on different automatic evaluation metrics where the improvements are most consistent on BLEU - 1 and embedding - based metrics , which demonstrates the overall effectiveness of our proposed architecture . In order to better study the quality of generated responses , we also report the human evaluation results in Table 2 . As results show , although there remains a huge gap between existing methods and human performance ( the Gold ) , our model gains promising promotions over previous methods on generating appropriate responses with diverse expressions . With both obvious superiority ( readability for S2S and diversity for CVAE ) and inferiority ( diversity for S2S and relevance for CVAE ) , the baselines show limited overall performances , in contrast to which our method can output more diverse utterances while maintaining the relevance to the input query and achieve a high overall score .", "entities": [[39, 40, "MethodName", "Seq2seq"], [69, 70, "MetricName", "BLEU"], [133, 134, "MethodName", "CVAE"], [184, 185, "MetricName", "BLEU"], [273, 274, "MethodName", "CVAE"], [284, 285, "MethodName", "CVAE"]]}
{"text": "To better understand the effectiveness of each component in our model , we further conduct the ablation studies with results shown at the bottom of Table 1 . Above all , to validate the effectiveness of the common feature , we remove the first generation stage and get the Ours - First model . As the results of BLEU and embedding - based metrics show , the system can benefit from the common feature for better relevance to the query . Moreover , pairwise comparisons Ours - Disc vs. Ours and Ours - MBOW vs. Ours validate the effects of the discriminator and modified multireference bag - of - word loss ( MBOW ) . As results show , the discriminator facilitates extracting the common feature and yields more relevant responses to the input query afterward . The MBOW loss , similar to the effects of BOW loss in the CVAE , can lead to a more unique latent variable for each response and improve the final distinctness scores of generated utterances . In the experiments , we also observed the KL vanishing problem when training our model and we overcame it with the KL weight annealing strategy and the MBOW loss described above .", "entities": [[58, 59, "MetricName", "BLEU"], [110, 111, "MetricName", "loss"], [139, 140, "MetricName", "loss"], [147, 148, "MetricName", "loss"], [150, 151, "MethodName", "CVAE"], [201, 202, "MetricName", "loss"]]}
{"text": "Decoders are algorithms to search for the highest scoring hypothesis . The list of predictors determines how ( partial ) hypotheses are scored by implementing the methods initialize ( ) , get state ( ) , set state ( ) , predict next ( ) , and consume ( ) . The Decoder class implements versions of these methods which apply to all predictors in the list . initialize ( ) is always called prior to decoding a new sentence . Many popular search strategies can be described via the remaining methods get state ( ) , set state ( ) , predict next ( ) , and consume ( ) . Algs . 1 and 2 show how to define greedy and beam decoding in this way . 45 Tab . 3 contains a list of currently implemented decoders . The UML diagram in Fig . 1 illustrates the relation between decoders and predictors . Algorithm 1 Greedy ( src sen ) 1 : initialize ( src sen ) 2 : h < s > 3 : repeat 4 : P predict next ( ) 5 : ( t , c ) arg max ( t , c ) P c 6 : h h t 7 : consume ( t ) 8 : until t = < /s > 9 : return h NMT batch decoding The flexibility of the predictor framework comes with degradation in decoding time . SGNMT provides two ways of speeding up pure NMT decoding , especially on the GPU . The vanilla decoding strategy exposes the beam search implementation in Blocks ( van Merri\u00ebnboer et al , 2015 ) which processes all active hypotheses in the beam in parallel . We also implemented a beam decoder version which decodes multiple sentences at once ( batch decoding ) rather than in a sequential order . Batch decoding is potentially more efficient since larger batches can make better use of GPU parallelism . The key concepts of our batch decoder implementation are : We use a scheduler running on a separate CPU thread to construct large batches of computation ( GPU jobs ) from multiple sentences and feeding them to the jobs queue . The GPU is operated by a single thread which communicates with the CPU scheduler thread via queues containing jobs . This thread is only responsible for retrieving jobs in the jobs queue , computing them , and putting them in the jobs results queue , minimizing the down - time of GPU computation . Yet another CPU thread is responsible for processing the results computed on the GPU H next H next \u222a ( t , c ) P ( h t , c + c , s ) 9 : end for 10 : H 11 : for all ( h , c , s ) n - best ( H next ) do end for 16 : until Best hypothesis in H ends with < /s > 17 : return Best hypothesis in H in the job results queue , e.g. by getting the n - best words from the posteriors . Processed jobs are sent back to the CPU scheduler where they are reassembled into new jobs . This decoder is able to translate the WMT English - French test sets news - test2012 to news - test2014 on a Titan X GPU with 911.6 words per second with the word - based NMT model described in Stahlberg et al ( 2016 ) . 6 This decoding speed seems to be slightly faster than sequential decoding with high - performance NMT decoders like Marian - NMT ( Junczys - Dowmunt et al , 2016 ) with reported decoding speeds of 865 words per second . 7 However , batch decoding with Marian - NMT is much faster reaching over 4 , 500 words per second . 8 We think that these differences are mainly due to the limited multithreading support and performance in Python especially when using external libraries as opposed to the highly optimized C++ code in Marian - NMT . We did not push for even faster decoding as speed is not a major design goal of SGNMT . Note that batch decoding bypasses the predictor framework and can only be used for pure NMT decoding . Ensembling with models at multiple tokenization levels SGNMT allows masking predictors with alternative sets of modelling units . The conversion between the tokenization schemes of different predictors is defined with FSTs . This makes it possible to decode by combining scores from both a subword - unit ( BPE ) based NMT ( Sennrich et al , 2016 ) and a word - based NMT model with character - based NMT , masking the BPE - based and word - based NMT predictors with FSTs which transduce character sequences to BPE or word sequences . Masking is transparent to the decoding strategy as predictors are replaced by a special wrapper ( fsttok ) that uses the masking FST to translate predict next ( ) and consume ( ) calls to ( a series of ) predictor calls with alternative tokens . The syncbeam variation of beam search compares competing hypotheses only after consuming a special word boundary symbol rather than after each token . This allows combining scores at the word level even when using models with multiple levels of tokenization . Joint decoding with different tokenization schemes has the potential of combining the benefits of the different schemes : character - and BPE - based models are able to address rare words , but word - based NMT can model long - range dependencies more efficiently . System - level combination We showed in Sec . 2.1 how to formulate NMT ensembling as a set of NMT predictors . Ensembling averages the individual model scores in each decoding step . Alternatively , system - level combination decodes the entire sentence with each model separately , and selects the best scoring complete hypothesis over all models . In our experiments , system - level combination is not as effective as en - 1080 ) , ( b ) a different training and test set , ( c ) a slightly different network architecture , and ( d ) words rather than subword units . 8 https://marian - nmt.github.io/ features/ sembling but still leads to moderate gains for pure NMT . However , a trivial implementation which selects the best translation in a postprocessing step after separate decoding runs is slow . The sepbeam decoding strategy reduces the runtime of system - level combination to the single system level . The strategy applies only one predictor rather than a linear combination of all predictors to expand a hypothesis . The single predictor is linked by the parent hypothesis . The initial stack in sepbeam contains hypotheses for each predictor ( i.e. system ) rather than only one as in normal beam search . We report a moderate gain of 0.5 BLEU over a single system on the Japanese - English ASPEC test set ( Nakazawa et al , 2016 ) by combining three BPE - based NMT models from using the sepbeam decoder . Iterative beam search Normal beam search is difficult to use in a time - constrained setting since the runtime depends on the target sentence length which is a priori not known , and it is therefore hard to choose the right beam size beforehand . The bucket search algorithm sidesteps the problem of setting the beam size by repeatedly performing small beam search passes until a fixed computational budget is exhausted . Bucket search produces an initial hypothesis very quickly , and keeps the partial hypotheses for each length in buckets . Subsequent beam search passes refine the initial hypothesis by iteratively updating these buckets . Our initial experiments suggest that bucket search often performs on a similar level as standard beam search with the benefit of being able to support hard time constraints . Unlike beam search , bucket search lends itself to risk - free ( i.e. admissible ) pruning since all partial hypotheses worse than the current best complete hypothesis can be discarded .", "entities": [[562, 563, "DatasetName", "Titan"], [769, 770, "MethodName", "BPE"], [795, 796, "MethodName", "BPE"], [811, 812, "MethodName", "BPE"], [924, 925, "MethodName", "BPE"], [1170, 1171, "MetricName", "BLEU"], [1180, 1181, "DatasetName", "ASPEC"], [1193, 1194, "MethodName", "BPE"]]}
{"text": "Instead of applying the aforementioned word substitutions directly to the original test dataset , our framework perturbs the test dataset within a small neighborhood to construct similar natural sentences . This is to identify vulnerable examples with respect to the model . Note that examples in the neighborhood are not required to have the same meaning as the original example , since we only study the prediction difference caused by applying synonym substitution p ( 2.1 ) . Constraints on the neighborhood . We limit the neighborhood sentences within a small 0 norm ball ( regarding the test instance ) to ensure syntactic similarity , and empirically ensure the naturalness through a language model . The neighborhood of an input sentence x 0 X is : Neighbor k ( x 0 ) \u2286 Ball k ( x 0 ) \u2229 X natural , ( 1 ) where Ball k ( x 0 ) = { x | x \u2212 x 0 0 \u2264 k , x X } is the 0 norm ball around x 0 ( i.e. , at most k different tokens ) , and X natural denotes natural sentences that satisfy a certain language model score which will be discussed next . Construction with masked language model . We construct neighborhood sentences from x 0 by substituting at most k tokens . As shown in Algorithm 1 , the construction employs a recursive approach and replaces one token at a time . For each recursion , the algorithm first masks each token of the input sentence ( may be the original x 0 or thex from last recursion ) separately and predicts likely replacements with a masked language model ( e.g. , DistilBERT , Sanh et al 2019 ) . To ensure the naturalness , we keep the top 20 tokens for each mask with the largest logit ( subject to a threshold , Line 9 ) . Then , the algorithm constructs neighborhood sentences by replacing the mask with found tokens . We use the notationx in the following sections to denote the constructed sentences within the neighborhood . lmin max { L ( \u03ba ) , L ( 0 ) \u2212 \u03b4 } ; ( i ) denotes the ith element . We empirically set \u03ba 20 and \u03b4 3 . L 10 Tnew { t | l > lmin , ( t , l ) T \u00d7 L } ; 11 Xnew { x0 | x ( i ) 0 t , t Tnew } ; Construct new sentences by replacing the ith token . 12 Xneighbor Xneighbor \u222a Xnew ; 13 return Xneighbor ;", "entities": [[91, 92, "DatasetName", "0"], [122, 123, "DatasetName", "0"], [130, 131, "DatasetName", "0"], [137, 138, "DatasetName", "0"], [151, 152, "DatasetName", "0"], [160, 161, "DatasetName", "0"], [161, 162, "DatasetName", "0"], [170, 171, "DatasetName", "0"], [175, 176, "DatasetName", "0"], [217, 218, "DatasetName", "0"], [265, 266, "DatasetName", "0"], [285, 286, "MethodName", "DistilBERT"], [363, 364, "DatasetName", "0"], [366, 367, "HyperparameterName", "\u03b4"], [383, 384, "HyperparameterName", "\u03b4"], [415, 416, "DatasetName", "0"]]}
{"text": "Adversarial attacks search for small and invariant perturbations on the model input that can alter the prediction . To simplify the discussion , in the following , we take a binary classifier f ( x ) : X { 0 , 1 } as an example to describe our framework . Let x 0 be the sentence from the test set with label y 0 , then the smallest perturbation \u03b4 * under 0 norm distance is : 2 \u03b4 * : = argmin \u03b4 \u03b4 0 s.t . f ( x 0 \u03b4 ) = y 0 . Here \u03b4 = p 1 p l denotes a series of substitutions . In contrast , our second - order attacks fix \u03b4 = p and search for the vulnerable x 0 .", "entities": [[39, 40, "DatasetName", "0"], [53, 54, "DatasetName", "0"], [64, 65, "DatasetName", "0"], [70, 71, "HyperparameterName", "\u03b4"], [73, 74, "DatasetName", "0"], [79, 80, "HyperparameterName", "\u03b4"], [84, 85, "HyperparameterName", "\u03b4"], [85, 86, "HyperparameterName", "\u03b4"], [86, 87, "DatasetName", "0"], [92, 93, "DatasetName", "0"], [93, 94, "HyperparameterName", "\u03b4"], [97, 98, "DatasetName", "0"], [100, 101, "HyperparameterName", "\u03b4"], [121, 122, "HyperparameterName", "\u03b4"], [130, 131, "DatasetName", "0"]]}
{"text": "A naive approach for solving Eq . ( 3 ) is to enumerate through Neighbor k ( x 0 ) . The enumeration finds the smallest perturbation , but is only applicable for small k ( e.g. , k \u2264 2 ) given the exponential complexity . Beam - search attack ( SO - Beam ) . The efficiency can be improved by utilizing the probability output , where we solve Eq . ( 3 ) by minimizing the crossentropy loss with regard to x Neighbor k ( x 0 ) : L ( x ; p ) : = \u2212 log ( 1 \u2212 f min ) \u2212 log ( f max ) , ( 4 ) where f min and f max are the smaller and the larger output probability between f soft ( x ) and f soft ( x 3 We assume a binary classification task , but our framework is general and can be extended to multi - class classification . p ) , respectively . Minimizing Eq . ( 4 ) effectively leads to f min 0 and f max 1 , and we use a beam search to find the best x. At each iteration , we construct sentences through Neighbor 1 ( x ) and only keep the top 20 sentences with the smallest L ( x ; p ) . We run at most k iterations , and stop earlier if we find a vulnerable example . We provide the detailed implementation in Algorithm 2 and a flowchart in Fig . 3 . 4 for i 1 , . . . , k do 5 Xnew x X beam Neighbor 1 ( x ) ; 6x0 argmax x Xnew | F ( x ; p ) | ; 7 if F ( x0 ; p ) = 0 then returnx0 ; 8 Xnew SortIncreasing ( Xnew , L ) ; 9 Xbeam { X ( 0 ) new , . . . , X ( \u03b2\u22121 ) new } ; Keep the best beam . We set \u03b2 20 . 10 return None ;", "entities": [[18, 19, "DatasetName", "0"], [80, 81, "MetricName", "loss"], [89, 90, "DatasetName", "0"], [161, 165, "TaskName", "multi - class classification"], [182, 183, "DatasetName", "0"], [306, 307, "DatasetName", "0"], [324, 325, "DatasetName", "0"], [346, 347, "HyperparameterName", "\u03b2"]]}
{"text": "Base models . For BoW , CNN , and LSTM , all models use pre - trained GloVe embeddings ( Pennington et al , 2014 ) , and have one hidden layer of the corresponding type with 100 hidden size . Similar to the baseline performance reported in GLUE , our trained models have an evaluation accuracy of 81.4 % , 82.5 % , and 81.7 % , respectively . For attention - based models , we train a 3 - layer Transformer ( the largest size in ) and fine - tune a pre - trained bertbase - uncased from HuggingFace ( Wolf et al , 2020 ( Morris et al , 2020 ) . Attack success rate ( second - order ) . We also quantify second - order robustness through attack success rate , which measures the ratio of test examples that a vulnerable example can be found . To evaluate the impact of neighborhood size , we experiment with two configurations : ( 1 ) For the small neighborhood ( k = 2 ) , we use SO - Enum that finds the most similar vulnerable example . ( 2 ) For the large neighborhood ( k = 6 ) , SO - Enum is not applicable and we use SO - Beam to find vulnerable examples . We consider the most challenging setup and use patch words p from the same set of counter - fitted synonyms as robust models ( they are provably robust to these synonyms on the test set ) . We also provide a random baseline to validate the effectiveness of minimizing Eq . ( 4 ) ( Appendix A.1 ) .", "entities": [[9, 10, "MethodName", "LSTM"], [17, 19, "MethodName", "GloVe embeddings"], [48, 49, "DatasetName", "GLUE"], [56, 57, "MetricName", "accuracy"], [82, 83, "MethodName", "Transformer"], [174, 176, "HyperparameterName", "k ="], [200, 202, "HyperparameterName", "k ="]]}
{"text": "We experiment with the validation split ( 872 examples ) on a single RTX 3090 . The average running time per example ( in seconds ) on base LSTM is 31.9 for Genetic , 1.1 for BAE , 7.0 for SO - Enum ( k = 2 ) , and 1.9 for SO - Beam ( k = 6 ) . We provide additional running time results in Appendix A.3 . Table 1 provides an example of the attack result where all attacks are successful ( additional examples in Appendix A.5 ) . As shown , our secondorder attacks find a vulnerable example by replacing grease musicals , and the vulnerable example has different predictions for bad and unhealthy . Note that , Genetic and BAE have different objectives from second - order attacks and focus on finding the adversarial example . Next we discuss the results from two perspectives . Second - order robustness . We observe that existing robustly trained models are not second - order robust . As shown in Furthermore , applying existing attacks on the vulnerable examples constructed by our method will lead to much smaller perturbations . As a reference , on the robustly trained CNN , Genetic attack constructs adversarial examples by perturbing 2.7 words on average ( starting from the input examples ) . However , if Genetic starts from our vulnerable examples , it would only need to perturb a single word ( i.e. , the patch words p ) to alter the prediction . These results demonstrate the weakness of the models ( even robustly trained ) for those inputs beyond the test set .", "entities": [[28, 29, "MethodName", "LSTM"], [44, 46, "HyperparameterName", "k ="], [56, 58, "HyperparameterName", "k ="]]}
{"text": "In contrast to second - order robustness , where we consider the model vulnerable as long as there exists one vulnerable example , counterfactual bias focuses on the expected prediction , which is the average prediction among all examples within the neighborhood . We consider a model biased if the expected predictions for protected groups are different ( assuming the model is not intended to discriminate between these groups ) . For instance , a sentiment classifier is biased if the expected prediction for inputs containing woman is more positive ( or negative ) than inputs containing man . Such bias is harmful as they may make unfair decisions based on protected attributes , for example in situations such as hiring and college admission . Counterfactual token bias . We study a narrow case of counterfactual bias , where counterfactual examples are constructed by substituting protected tokens in the input . A naive approach of measuring this bias is to construct counterfactual examples directly from the test set , however such evaluation may not be robust since test examples are only a small subset of natural sentences . Formally , let p be a pair of protected tokens such as ( he , she ) or ( Asian , American ) , X test \u2286 X p be a test set ( as in 2.1 ) , we define counterfactual token bias by : B p , k : = E x Neighbor k ( Xtest ) F soft ( x ; p ) . ( 5 ) We calculate Eq . ( 5 ) through an enumeration across all natural sentences within the neighborhood . 7 Here Neighbor k ( X test ) = x Xtest Neighbor k ( x ) denotes the union of neighborhood examples ( of distance k ) around the test set , and F soft ( x ; p ) : X \u00d7 V 2 [ \u22121 , 1 ] denotes the difference between probability outputs f soft ( similar to Eq . ( 2 ) ) : F soft ( x ; p ) : = f soft ( x p ) \u2212 f soft ( x ) . ( 6 ( k = 3 ) in X filter . The model is unbiased on p if B p , k \u2248 0 , whereas a positive or negative B p , k indicates that the model shows preference or against to p ( 2 ) , respectively . Fig . 4 illustrates the distribution of ( x , x p ) for both an unbiased model and a biased model . The aforementioned neighborhood construction does not introduce additional bias . For instance , let x 0 be a sentence containing he , even though it is possible for Neighbor 1 ( x 0 ) to contain many stereotyping sentences ( e.g. , contains tokens such as doctor and driving ) that affect the distribution of f soft ( x ) , but it does not bias Eq . ( 6 ) as we only care about the prediction difference of replacing he she . The construction has no information about the model objective , thus it would be difficult to bias f soft ( x ) and f soft ( x p ) differently .", "entities": [[369, 371, "HyperparameterName", "k ="], [389, 390, "DatasetName", "0"], [454, 455, "DatasetName", "0"], [471, 472, "DatasetName", "0"]]}
{"text": "We evaluate counterfactual token bias on the SST - 2 dataset with both the base and debiased models . We focus on binary gender bias and set p to pairs of gendered pronouns from Zhao et al ( 2018a ) . Base Model . We train a single layer LSTM with pre - trained GloVe embeddings and 75 hidden size ( from TextAttack , Morris et al 2020 ) . The model has 82.9 % accuracy similar to the baseline performance reported in GLUE . Debiased Model . Data - augmentation with gender swapping has been shown effective in mitigating gender bias ( Zhao et al , 2018a . We augment the training split by swapping all male entities with the corresponding female entities and vice - versa . We use the same setup as the base LSTM and attain 82.45 % accuracy . Here \" original \" is equivalent to k = 0 , \" perturbed \" is equivalent to k = 3 , p is in the form of ( male , female ) . Metrics . We evaluate model bias through the proposed B p , k for k = 0 , . . . , 3 . Here the bias for k = 0 is effectively measured on the original test set , and the bias for k \u2265 1 is measured on our constructed neighborhood . We randomly sample a subset of constructed examples when k = 3 due to the exponential complexity . Filtered test set . To investigate whether our method is able to reveal model bias that was hidden in the test set , we construct a filtered test set on which the bias can not be observed directly . Let X test be the original validation split , we construct X filter by the equation below and empirically set = 0.005 . We provide statistics in Table 5 . X filter : = { x | | F soft ( x ; p ) | < , x X test } .", "entities": [[7, 8, "DatasetName", "SST"], [49, 50, "MethodName", "LSTM"], [54, 56, "MethodName", "GloVe embeddings"], [75, 76, "MetricName", "accuracy"], [83, 84, "DatasetName", "GLUE"], [137, 138, "MethodName", "LSTM"], [142, 143, "MetricName", "accuracy"], [151, 153, "HyperparameterName", "k ="], [153, 154, "DatasetName", "0"], [161, 163, "HyperparameterName", "k ="], [191, 193, "HyperparameterName", "k ="], [193, 194, "DatasetName", "0"], [205, 207, "HyperparameterName", "k ="], [207, 208, "DatasetName", "0"], [240, 242, "HyperparameterName", "k ="]]}
{"text": "Our method is able to reveal the hidden model bias on X filter , which is not visible with naive measurements . In Fig . 5 , the naive approach ( k = 0 ) observes very small biases on most tokens ( as constructed ) . In contrast , when evaluated by our double perturbation framework ( k = 3 ) , we are able to observe noticeable bias , where most p has a positive bias on the base model . This observed bias is in line with the measurements on the original X test ( Appendix A.4 ) , indicating that we reveal the correct model bias . Furthermore , we observe mitigated biases in the debiased model , which demonstrates the effectiveness of data augmentation . To demonstrate how our method reveals hidden bias , we conduct a case study with p = ( actor , actress ) and show the relationship between the bias B p , k and the neighborhood distance k. We present the histograms for F soft ( x ; p ) in Fig . 6 and plot the corresponding B p , k vs. k in the right - most panel . Surprisingly , for the base model , the bias is Figure 6 : Left and Middle : Histograms for F soft ( x ; p ) ( x - axis ) with p = ( actor , actress ) . Right : The plot for the average F soft ( x ; p ) ( i.e. , counterfactual token bias ) vs. neighborhood distance k. Results show that the counterfactual bias on p can be revealed when increasing k. negative when k = 0 , but becomes positive when k = 3 . This is because the naive approach only has two test examples ( Table 5 ) thus the measurement is not robust . In contrast , our method is able to construct 141 , 780 similar natural sentences when k = 3 and shifts the distribution to the right ( positive ) . As shown in the right - most panel , the bias is small when k = 1 , and becomes more significant as k increases ( larger neighborhood ) . As discussed in 4.1 , the neighborhood construction does not introduce additional bias , and these results demonstrate the effectiveness of our method in revealing hidden model bias .", "entities": [[31, 33, "HyperparameterName", "k ="], [33, 34, "DatasetName", "0"], [58, 60, "HyperparameterName", "k ="], [127, 129, "TaskName", "data augmentation"], [282, 284, "HyperparameterName", "k ="], [284, 285, "DatasetName", "0"], [290, 292, "HyperparameterName", "k ="], [332, 334, "HyperparameterName", "k ="], [360, 362, "HyperparameterName", "k ="]]}
{"text": "We experiment with the validation split on a single RTX 3090 , and measure the average running time per example . As shown in A.4 Additional Results on Protected Tokens Fig . 7 presents the experimental results with additional protected tokens such as nationality , religion , and sexual orientation ( from Ribeiro et al ( 2020 ) ) . We use the same base LSTM as described in 4.2 . One interesting observation is when p = ( gay , straight ) where the bias is negative , indicating that the sentiment classifier tends to give more negative prediction when substituting gay straight in the input . This phenomenon is opposite to the behavior of toxicity classifiers ( Dixon et al , 2018 ) In Fig . 8 , we measure the bias on X test and observe positive bias on most tokens for both k = 0 and k = 3 , which indicates that the model \" tends \" to make more positive predictions for examples containing certain female pronouns than male pro - nouns . Notice that even though gender swap mitigates the bias to some extent , it is still difficult to fully eliminate the bias . This is probably caused by tuples like ( him , his , her ) which can not be swapped perfectly , and requires additional processing such as part - of - speech resolving ( Zhao et al , 2018a ) . To help evaluate the naturalness of our constructed examples used in 4 , we provide sample sentences in Table 9 and Table 10 . Bold words are the corresponding patch words p , taken from the predefined list of gendered pronouns . Distance k = 1 97 % Negative ( 97 % Negative ) it 's hampered by a lifetime - channel kind of plot and lone lead actor ( actress ) who is out of their depth . 56 % Negative ( 55 % Positive ) it 's hampered by a lifetime - channel kind of plot and a lead actor ( actress ) who is out of creative depth . 89 % Negative ( 84 % Negative ) it 's hampered by a lifetime - channel kind of plot and a lead actor ( actress ) who talks out of their depth . 98 % Negative ( 98 % Negative ) it 's hampered by a lifetime - channel kind of plot and a lead actor ( actress ) who is out of production depth . 96 % Negative ( 96 % Negative ) it 's hampered by a lifetime - channel kind of plot and a lead actor ( actress ) that is out of their depth .", "entities": [[65, 66, "MethodName", "LSTM"], [146, 148, "HyperparameterName", "k ="], [148, 149, "DatasetName", "0"], [150, 152, "HyperparameterName", "k ="], [229, 232, "DatasetName", "part - of"], [286, 288, "HyperparameterName", "k ="]]}
{"text": "Distance k = 2 88 % Negative ( 87 % Negative ) it 's hampered by a lifetime - channel cast of stars and a lead actor ( actress ) who is out of their depth . 96 % Negative ( 95 % Negative ) it 's hampered by a simple set of plot and a lead actor ( actress ) who is out of their depth . 54 % Negative ( 54 % Negative ) it 's framed about a lifetime - channel kind of plot and a lead actor ( actress ) who is out of their depth . 90 % Negative ( 88 % Negative ) it 's hampered by a lifetime - channel mix between plot and a lead actor ( actress ) who is out of their depth . 78 % Negative ( 68 % Negative ) it 's hampered by a lifetime - channel kind of plot and a lead actor ( actress ) who storms out of their mind . Distance k = 3 52 % Positive ( 64 % Positive ) it 's characterized by a lifetime - channel combination comedy plot and a lead actor ( actress ) who is out of their depth . 93 % Negative ( 93 % Negative ) it 's hampered by a lifetime - channel kind of star and a lead actor ( actress ) who falls out of their depth . 58 % Negative ( 57 % Negative ) it 's hampered by a tough kind of singer and a lead actor ( actress ) who is out of their teens . 70 % Negative ( 52 % Negative ) it 's hampered with a lifetime - channel kind of plot and a lead actor ( actress ) who operates regardless of their depth . 58 % Negative ( 53 % Positive ) it 's hampered with a lifetime - channel cast of plot and a lead actor ( actress ) who is out of creative depth .", "entities": [[1, 3, "HyperparameterName", "k ="], [168, 170, "HyperparameterName", "k ="]]}
{"text": "Named entity recognition ( NER ) is one of the most fundamental and important tasks in natural language processing ( NLP ) . While the literature ( Peters et al , 2018 ; Akbik et al , 2018 ; Devlin et al , 2019 ) largely focuses on training deep language models to improve the contextualized word representations , previous studies show that the structured information such as interactions between non - adjacent words can also be important for NER ( Finkel et al , 2005 ; Jie et al , 2017 ; Aguilar and Solorio , 2019 ) . However , sequence models such as bidirectional LSTM ( Hochreiter and Schmidhuber , 1997 ) are not able to fully capture the long - range dependencies ( Bengio , 2009 ) . For instance , Figure 1 ( top ) shows one type of structured information in NER . The words \" Precision Castparts Corp. \" can be easily inferred as ORGANIZATION by its context ( i.e. , Corp. ) . However , the second entity \" PCP \" could be misclassified as a PRODUCT entity if a model relies more on the context \" begin trading with \" but ignores the hidden information that \" PCP \" is the symbol of \" Precision Castparts Corp. \" . Previous research works ( Li et al , 2017 ; Jie and Lu , 2019 ; Wang et al , 2019 ) have been using the parse trees ( Chomsky , 1956 ( Chomsky , , 1969Sandra and Taft , 2014 ) to incorporate such structured information . Figure 1 ( Dependency Path ) shows that the first entity can be connected to the second entity following the dependency tree with 5 hops . Incorporating the dependency information can be done with graph neural networks ( GNNs ) such as graph convolutional networks ( GCNs ) ( Kipf and Welling , 2017 ) . However , simply stacking the LSTM and GCN architectures for NER can only provide us with modest improvements ; sometimes , it decreases performance ( Jie and Lu , 2019 ) . Based on the depen - dency path in Figure 1 , it requires a 5 - layer GCN to capture the connections between these two entities . However , deep GCN architectures often face training difficulties , which cause a performance drop ( Hamilton et al , 2017b ; Kipf and Welling , 2017 ) . Directly stacking GCN and LSTM has difficulties in modeling the interaction between dependency trees and contextual information . To address the above limitations , we propose the Synergized - LSTM ( Syn - LSTM ) , a new recurrent neural network architecture that considers an additional graph - encoded representation to update the memory and hidden states , as shown in Figure 2 . More specifically , the graph - encoded representation for each word can be obtained with GCNs . Our proposed Syn - LSTM allows the cell to receive the structured information from the graph - encoded representation . With the newly designed gating mechanism , our model is able to make independent assessments on the amounts of information to be retrieved from the word representation and the graph - encoded representation respectively . Such a mechanism allows for better integration of both contextual and structured information . Our contributions can be summarized as : We propose a simple and robust Syn - LSTM model to better incorporate the structured information conveyed by dependency trees . The output of the Syn - LSTM cell is jointly determined by both contextual and structured information . We adopt the classic conditional random fields ( CRF ) ( Lafferty et al , 2001 ) on top of the Syn - LSTM for NER . We conduct extensive experiments on several standard datasets across four languages . The proposed model significantly outperforms previous approaches on these datasets . We show that the proposed model can capture long - distance interactions between entities . Our further analysis statistically demonstrates the proposed gating mechanism is able to aggregate the structured information selectively . 2 Synergized - LSTM", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 5, "TaskName", "NER"], [79, 80, "TaskName", "NER"], [106, 108, "MethodName", "bidirectional LSTM"], [147, 148, "TaskName", "NER"], [152, 153, "MetricName", "Precision"], [213, 214, "MetricName", "Precision"], [327, 328, "MethodName", "LSTM"], [329, 330, "MethodName", "GCN"], [332, 333, "TaskName", "NER"], [371, 372, "MethodName", "GCN"], [384, 385, "MethodName", "GCN"], [412, 413, "MethodName", "GCN"], [414, 415, "MethodName", "LSTM"], [439, 440, "MethodName", "LSTM"], [443, 444, "MethodName", "LSTM"], [495, 496, "MethodName", "LSTM"], [575, 576, "MethodName", "LSTM"], [594, 595, "MethodName", "LSTM"], [614, 615, "MethodName", "CRF"], [629, 630, "MethodName", "LSTM"], [631, 632, "TaskName", "NER"], [692, 693, "MethodName", "LSTM"]]}
{"text": "We propose the Synergized - LSTM ( Syn - LSTM ) to better integrate the contextual and structured information to address the above limitations . The inputs of the Syn - LSTM cell include previous cell state c t\u22121 , previous hidden state h t\u22121 , current cell input x t , and an additional graph - encoded representation g t . The outputs of the Syn - LSTM cell include current cell state c t and current hidden state h t . Within the cell , there are four gates : input gate i t , forget gate f t , output gate o t , and an additional new gate m t to control the flow of information . Note that the forget gate f t and output gate o t are not just looking at h t\u22121 and x t ; they are also affected by the graph - encoded representation g t . The cell state c t and hidden state h t are computed as follows : f t = \u03c3 ( W ( f ) x t + U ( f ) h t\u22121 + Q ( f ) g t + b ( f ) ) ( 1 ) o t = \u03c3 ( W ( o ) x t + U ( o ) h t\u22121 + Q ( o ) g t + b ( o ) ) ( 2 ) i t = \u03c3 ( W ( i ) x t + U ( i ) h t\u22121 + b ( i ) ) ( 3 ) m t = \u03c3 ( W ( m ) g t + U ( m ) h t\u22121 + b ( m ) ) ( 4 ) c t = tanh ( W ( u ) x t + U ( u ) h t\u22121 + b ( u ) ) ( 5 ) s t = tanh ( W ( n ) g t + U ( n ) h t\u22121 + b ( n ) ) ( 6 ) c t = f t c t\u22121 + i t c t + m t s t ( 7 ) h t = o t tanh ( c t ) ( 8 ) where \u03c3 is the sigmoid function , W ( ) , U ( ) , Q ( ) and b ( ) are learnable parameters . The additional new gate m t is used to control the information from the graph - encoded representation directly . Such a design allows the original input gates i t and our new gate m t to make independent assessments on the amounts of information to be retrieved from the word representation x t and the graph - encoded representation g t respectively . On the other hand , we also have a different candidate states t to represent the cell state that corresponds to the graph - encoded representation separately . With the proposed Syn - LSTM , the structured information captured by the dependency trees can be passed to each cell , and the additional gate m t is able to control how much structured information can be incorporated . The additional gate enables the model to feed the contextual and structured information into the LSTM cell separately . Such a mechanism allows our model to aggregate the information from linear sequence and dependency trees selectively . Similar to the previous work ( Levy et al , 2018 ) , it is also possible to show that the cell state c t implicitly computes the element - wise weighted sum of the previous states by expanding Equation 7 : xt - 1 xt xt+1 xt+2 g L t - 1 g L t g L t+1 g L t+2 Syn - LSTM Syn - LSTM Syn - LSTM Syn - LSTM y t\u22121 y t y t+1 y t+2 g 0 t - 1 g 0 t g 0 t+1 g 0 t+2 Graph Convolutional Network c t = f t c t\u22121 + i t c t + m t s t ( 9 ) = t j=0 ( i j t k = j+1 f k ) c j + t j=0 ( m j t k = j+1 f k ) s j ( 10 ) = t j=0 a t j c j + t j=0 q t j s j ( 11 ) Note that the two terms , a t j and q t j , are the product of gates . The value of the two terms are in the range from 0 to 1 . Since thec t ands t represent contextual and structured features , the corresponding weights control the flow of information .", "entities": [[5, 6, "MethodName", "LSTM"], [9, 10, "MethodName", "LSTM"], [31, 32, "MethodName", "LSTM"], [68, 69, "MethodName", "LSTM"], [504, 505, "MethodName", "LSTM"], [554, 555, "MethodName", "LSTM"], [640, 641, "MethodName", "LSTM"], [643, 644, "MethodName", "LSTM"], [646, 647, "MethodName", "LSTM"], [649, 650, "MethodName", "LSTM"], [659, 660, "DatasetName", "0"], [664, 665, "DatasetName", "0"], [667, 668, "DatasetName", "0"], [670, 671, "DatasetName", "0"], [672, 675, "MethodName", "Graph Convolutional Network"], [702, 704, "HyperparameterName", "k ="], [717, 719, "HyperparameterName", "k ="], [778, 779, "DatasetName", "0"]]}
{"text": "The goal of named entity recognition is to predict the label sequence y = { y 1 , y 2 , ... , y n } given the input sequence w = { w 1 , w 2 , ... , w n } , where w t represents the t - th word and n is the number of words . Our model is mainly constructed with three layers : input representation layer , bi - directional Syn - LSTM layer , and CRF layer . The architecture of our Syn - LSTM - CRF is shown in Figure 3 . Input Representation Layer Similar to the work by Lample et al ( 2016 ) , our input representation also includes the character embeddings , which are the hidden states of character - based BiLSTM . Jie and Lu ( 2019 ) highlight that the dependency relation helps to enhance the input representation . Furthermore , previous methods ) use embeddings of part - ofspeech ( POS ) tags as additional input representation . The input representation x t of our model is the concatenation of the word embedding v t , the character representation e t , the dependency relation embedding r t , and the POS embedding p t : x t = [ v t ; e t ; r t ; p t ] ( 12 ) where both r t and p t embeddings are randomly initialized and are fine - tuned during training . For experiments with the contextualized representations ( e.g. , BERT ( Devlin et al , 2019 ) ) , we further concatenate the contextual word representation to x t . For our task , we employ the graph convolutional network ( Kipf and Welling , 2017 ; Zhang et al , 2018b ) to get the graph - encoded representation g t . Given a graph , an adjacency matrix A of size n \u00d7 n is able to represent the graph structure , where n is the number of nodes ; A i , j = 1 indicates that node i and node j are connected . We transform dependency tree into its corresponding adjacency matrix 3 A , and A i , j = 1 denotes that node i and node j have dependency relation . Note that the purpose of graph - encoded representation g t is to incorporate the dependency information from neighbor nodes . The input and output representations of the l - th layer GCN at t - th position are denoted as g l\u22121 t and g l t respectively . Similar to the work by Zhang et al ( 2018b ) , we use d t = n j=1 A t , j , which is the total number of neighbors of node t , to normalize the representation before going through the nonlinear function . The GCN operation is defined as : g l t = ReLU ( n j=1 A t , j W l g l\u22121 t /d t + b l ) ( 13 ) where W l is a linear transformation and b l is a bias . The initial g 0 t is the concatenation of word embedding v t , character embedding e t , and dependency relation embedding r t : \u2212 h t from backward Syn - LSTM to form the contextual representation of t - th token : g 0 t = [ v t ; e t ; r t ] . Bi - directional h t = [ \u2212 h t ; \u2212 h t ] . CRF Layer The CRF ( Lafferty et al , 2001 ) is widely used in NER tasks as it is capable of capturing the structured correlations between adjacent output labels . Given the sentence w and dependency tree \u03c4 , the probability of the label sequence y is defined as : P ( y | w , \u03c4 ) = exp ( score ( w , \u03c4 , y ) ) y exp ( score ( w , \u03c4 , y ) ) ( 14 ) The score function is defined as : score ( w , \u03c4 , y ) = n t=0 T yt , y t+1 + n t=1 E yt ( 15 ) where T yt , y t+1 denotes the transition score from label y t to y t+1 , E yt denotes the score of label y t at the t - th position and the scores are computed using the hidden state h t . We learn the model parameters by minimizing the negative log - likelihood and employ the Viterbi algorithm to obtain the best label sequence during evaluation .", "entities": [[3, 6, "TaskName", "named entity recognition"], [80, 81, "MethodName", "LSTM"], [84, 85, "MethodName", "CRF"], [93, 94, "MethodName", "LSTM"], [95, 96, "MethodName", "CRF"], [135, 136, "MethodName", "BiLSTM"], [260, 261, "MethodName", "BERT"], [288, 291, "MethodName", "graph convolutional network"], [421, 422, "MethodName", "GCN"], [486, 487, "MethodName", "GCN"], [496, 497, "MethodName", "ReLU"], [535, 536, "DatasetName", "0"], [565, 566, "MethodName", "LSTM"], [578, 579, "DatasetName", "0"], [608, 609, "MethodName", "CRF"], [611, 612, "MethodName", "CRF"], [623, 624, "TaskName", "NER"], [779, 782, "MetricName", "log - likelihood"]]}
{"text": "Datasets The proposed model is evaluated on four benchmark datasets : SemEval 2010 Task 1 ( Recasens et al , 2010 ) Catalan and Spanish datasets , and OntoNotes 5.0 ( Weischedel et al , 2013 ) English and Chinese datasets . We choose these four datasets as they have explicit dependency annotations which allow us to evaluate the effectiveness of our approach when dependency trees of different qualities are used . For SemEval 2010 Task 1 datasets , there are 4 entity types : PER , LOC and ORG and MISC . For OntoNotes 5.0 datasets , there are 18 entity types in total . Following the work by Jie and Lu ( 2019 ) , we transform the parse trees into the Stanford dependency trees ( De Marneffe and Manning , 2008 ) by using Stanford CoreNLP . Detailed statistics of each dataset can be found in Table 1 . Intuitively , longer sentences would require the model to capture more long - distance interactions in the sentences . We present the number of entities in terms of different sentence lengths to show that these datasets have a modest amount of entities in long sentences . Experimental Setup For Catalan , Spanish , and Chinese , we use the FastText ( Grave et al , 2018 ) 300 dimensional embeddings to initialize the word embeddings . For OntoNotes 5.0 English , we adopt the publicly available GloVE ( Pennington et al , 2014 ) 100 dimensional embeddings to initialize the word embeddings . For experiments with the contextualized representation , we adopt the pre - trained language model BERT ( Devlin et al , 2019 ) for the four datasets . Specifically , we use bert - as - service ( Xiao , 2018 ) to generate the contextualized word representation without fine - tuning . Following Luo et al ( 2020 ) , we use the cased version of BERT large model for the experiments on the OntoNotes 5.0 English data . We use the cased version of BERT base model for the experiments on the other three datasets . For the character embedding , we randomly initialize the character embeddings and set the dimension as 30 , and set the hidden size of character - level BiLSTM as 50 . The hidden size of GCN and Syn - LSTM is set as 200 , the number of GCN layer is 2 . We adopt stochastic gradient descent ( SGD ) to optimize our model with batch size 100 , L2 regularization 10 \u22128 , initial learning rate lr 0.2 and the learning rate is decayed 4 with respect to the number of epoch . We select the best model based on the performance on the dev set 5 and apply it to the test set . We use the bootstrapping t - test to compare the results . Baselines We compare our model with several baselines with or without dependency tree information . The first one is BERT - CRF , where we apply a CRF layer on top of BERT ( Devlin et al , 2019 ) . Secondly , we compare with the BERT implementation by HuggingFace ( Wolf et al , 2019 ) . For models with dependency trees , we take the models BiLSTM - GCN - CRF and dependency - 4 We set the decay as 0.1 and the learning rate for each epoch equals to lr/ ( 1 + decay * ( epoch \u2212 1 ) ) . 5 The experimental results on the dev set and other experimental details can be found in the Appendix . ( Peters et al , 2018 ) , but we also implement it with BERT . Besides , we compare our model with previous works that have results on these datasets .", "entities": [[28, 30, "DatasetName", "OntoNotes 5.0"], [94, 96, "DatasetName", "OntoNotes 5.0"], [211, 212, "MethodName", "FastText"], [225, 227, "TaskName", "word embeddings"], [229, 231, "DatasetName", "OntoNotes 5.0"], [252, 254, "TaskName", "word embeddings"], [270, 271, "MethodName", "BERT"], [322, 323, "MethodName", "BERT"], [330, 332, "DatasetName", "OntoNotes 5.0"], [341, 342, "MethodName", "BERT"], [380, 381, "MethodName", "BiLSTM"], [388, 389, "MethodName", "GCN"], [392, 393, "MethodName", "LSTM"], [401, 402, "MethodName", "GCN"], [408, 411, "MethodName", "stochastic gradient descent"], [412, 413, "MethodName", "SGD"], [419, 421, "HyperparameterName", "batch size"], [423, 425, "HyperparameterName", "L2 regularization"], [429, 431, "HyperparameterName", "learning rate"], [435, 437, "HyperparameterName", "learning rate"], [501, 502, "MethodName", "BERT"], [503, 504, "MethodName", "CRF"], [509, 510, "MethodName", "CRF"], [514, 515, "MethodName", "BERT"], [529, 530, "MethodName", "BERT"], [551, 552, "MethodName", "BiLSTM"], [553, 554, "MethodName", "GCN"], [555, 556, "MethodName", "CRF"], [568, 570, "HyperparameterName", "learning rate"], [621, 622, "MethodName", "BERT"]]}
{"text": "In language model BERT ( Devlin et al , 2019 ) for the four datasets . Specifically , we use bert - as - service ( Xiao , 2018 ) to generate the contextualized word representation without fine - tuning . Following Luo et al ( 2020 ) , we select the 18 th layer of the cased version of BERT large model for the experiments on the OntoNotes 5.0 English data . We use the the 9 th layer of cased version of BERT base model for the experiments on the rest three datasets . For the character embedding , we randomly initialize the character embeddings and set the dimension as 30 , and set the hidden size of character - level BiLSTM as 50 . The hidden size of GCN and Syn - LSTM is set as 200 . Note that we only use one layer of bi - directional Syn - LSTM for our experiments . Dropout is set to 0.5 for input embeddings and hidden states . We adopt stochastic gradient descent ( SGD ) to optimize our model with batch size 100 , L2 regularization 10 \u22128 , learning rate 0.2 and the learning rate is decayed with respect to the number of epoch 9 .", "entities": [[3, 4, "MethodName", "BERT"], [60, 61, "MethodName", "BERT"], [68, 70, "DatasetName", "OntoNotes 5.0"], [84, 85, "MethodName", "BERT"], [123, 124, "MethodName", "BiLSTM"], [131, 132, "MethodName", "GCN"], [135, 136, "MethodName", "LSTM"], [154, 155, "MethodName", "LSTM"], [159, 160, "MethodName", "Dropout"], [173, 176, "MethodName", "stochastic gradient descent"], [177, 178, "MethodName", "SGD"], [184, 186, "HyperparameterName", "batch size"], [188, 190, "HyperparameterName", "L2 regularization"], [193, 195, "HyperparameterName", "learning rate"], [198, 200, "HyperparameterName", "learning rate"]]}
{"text": "Table 8 presents the performance of dependency parser . 9 We set the decay as 0.1 and the learning rate for each epoch equals to learning_rate/ ( 1 + decay * ( epoch \u2212 1 ) ) .", "entities": [[18, 20, "HyperparameterName", "learning rate"]]}
{"text": "We test our model on RTX 2080 Ti GPU and Nvidia Tesla V100 GPU , with CUDA version 10.1 , PyTorch version 1.40 . The average run time for Syn - LSTM is 52 sec / epoch , 55 sec / epoch , 290 sec / epoch , 350 sec / epoch for Catalan , Spanish , Chinese and English datasets respectively . The total number of parameters is 11M. Table 10 shows the performance of our model on the dev sets of OntoNotes 5.0 English and Chinese , SemEval 2010 Task 1 Catalan and Spanish . For hyper - parameter , we use the FastText ( Grave et al , 2018 ) 300 dimensional embeddings to initialize the word embeddings for Catalan , Spanish , and Chinese . For OntoNotes 5.0 English , we adopt the publicly available GloVE ( Pennington et al , 2014 ) 100 dimensional embeddings to initialize the word embeddings . For experiments with the contextualized representation , we adopt the pre - trained", "entities": [[31, 32, "MethodName", "LSTM"], [65, 68, "HyperparameterName", "number of parameters"], [83, 85, "DatasetName", "OntoNotes 5.0"], [105, 106, "MethodName", "FastText"], [119, 121, "TaskName", "word embeddings"], [130, 132, "DatasetName", "OntoNotes 5.0"], [153, 155, "TaskName", "word embeddings"]]}
{"text": "We first train 100 and 300 dimensions for both GloVe embeddings and skip - thought embeddings using the same mechanism as in ( Pennington et al , 2014 ; Kiros et al , 2015 ) . In some posts the length of sentences is very large , so we bound the length at 50 words . We do not treat the problem separately from the negative take as the GRU will anyway put more importance on the information that comes last . We split the labelled data in a 8 : 1 : 1 ratio for training , validation and testing in a 10 - fold cross validation for both GRU and CNN training . A distinct network is trained for each concept , i. e. one for thinking errors , one for emotions and one for situations . The hidden size of the FNN is 150 . To tackle the data bias problem , we utilise oversampling . Different ratios ( 1:1 , 1:3 , 1:5 , 1:7 ) of positive and negative samples are explored . We used filter windows of 2 , 3 , and 4 with 50 feature maps for the CNN model . For the GRU model , the hidden size is set at 150 , so that both models have comparable number of parameters . Mini - batches of size 24 are used and gradients are clipped with maximum norm 5 . We initialise the learning rate as 0.001 with a decay rate of 0.986 every 10 steps . The non - recurrent weights with a truncated normal distribution ( 0 , 0.01 ) , and the recurrent weights with orthogonal initialisation ( Saxe et al , 2013 ) . To overcome over - fitting , we employ dropout with rate 0.8 and l2 - normalisation . Both models were trained with Adam algorithm and implemented in Tensorflow ( Girija , 2016 ) .", "entities": [[9, 11, "MethodName", "GloVe embeddings"], [69, 70, "MethodName", "GRU"], [110, 111, "MethodName", "GRU"], [200, 201, "MethodName", "GRU"], [217, 220, "HyperparameterName", "number of parameters"], [241, 243, "HyperparameterName", "learning rate"], [247, 249, "HyperparameterName", "decay rate"], [266, 267, "DatasetName", "0"], [308, 309, "MethodName", "Adam"]]}
{"text": "Table 5 gives the average F1 scores and the average F1 scores weighted with the frequency of CBT labels for all models under the oversampling ratio 1:1 . It shows that GloVe word vectors with CNN achieves the best performance both in 100 and 300 dimensions . Table 6 shows the F1 - measure of the compared models that detect thinking errors , emotions and situations under the 1 : 1 oversampling ratio . We only include the results of the best performing models , SVMs , CNNs and GRUs , due to limited space . The results show that both models outperform SVM - BOW in larger embedding dimensions . Although SVM - BOW is comparable to 100 dimensional GRU - Skip - thought in terms on average F1 , in all other cases CNN - GloVe and GRU - Skipthought overshadow SVM - BOW . We also find that CNN - GloVe on average works better than GRU - Skip - thought , which is expected as the space of words is smaller in comparison to the space of sentences so the word vectors can be more accurately trained . While the CNN operating on 100 dimensional word vectors is comparable to the CNN operating on 300 dimensional word vectors , the GRU - Skip - thought tends to be worse on 100 dimensional skip - thoughts , suggesting that sentence vectors generally need to be of a higher dimension to represent the meaning more accurately than word vectors . Table 7 shows a more detailed analysis of the 300 dimensional CNN - GloVe performance , where both precision and recall are presented , indicating that oversampling mechanism can help overcome the data bias problem . To illustrate the capabilities of this model , we give samples of two posts and their predicted and true labels in Figure 6 , which shows that our model discerns the classes reasonably well even in some difficult cases . While oversampling is essential for both models , GRU - Skip - thought is less sensitive to lower oversampling ratios , suggesting that skip - thoughts can already capture sentiment on the sentence level . Therefore , including only a limited ratio of positive samples is sufficient to train the classifier . Instead , models using word vectors need more positive data to learn sentence sentiment features .", "entities": [[4, 6, "MetricName", "average F1"], [9, 11, "MetricName", "average F1"], [17, 18, "DatasetName", "CBT"], [31, 32, "MethodName", "GloVe"], [51, 52, "MetricName", "F1"], [103, 104, "MethodName", "SVM"], [112, 113, "MethodName", "SVM"], [120, 121, "MethodName", "GRU"], [128, 130, "MetricName", "average F1"], [137, 138, "MethodName", "GloVe"], [139, 140, "MethodName", "GRU"], [143, 144, "MethodName", "SVM"], [153, 154, "MethodName", "GloVe"], [159, 160, "MethodName", "GRU"], [214, 215, "MethodName", "GRU"], [265, 266, "MethodName", "GloVe"], [336, 337, "MethodName", "GRU"]]}
{"text": "We now present our metric for unintended demographic bias , RNSB . For gold standard labeled positive / negative sentiment words , ( x i , y i ) , in training set , S , where x i is a word vector from a possibly biased word embedding model , we find the minimizer , f * ( x i ) = \u03c3 ( w T x i ) , for the logistic loss , l , and learned weights , w. min w R d n i=0 l ( y i , w T x i ) + \u03bb w 2 , \u03bb > 0 Then for a set , K = { k 1 , ... , k t } , of t demographic identity word vectors from a particular protected group ( i.e. national origin , religion , etc . ) , we define a set , P , containing the predicted negative sentiment probability via minimizer , f * , normalized to be one probability mass . P = f * ( k 1 ) t i=1 f * ( k i ) , ... , f * ( k t ) t i=1 f * ( k i ) Thus , our metric , RN SB ( P ) , is defined as the KL divergence of P from U , where U is the uniform distribution for t elements . RN SB ( P ) = D KL ( P U ) We choose our set of neutral identity terms based on the most populous demographics for each protected group . However , due to the simplicity of this method , one can easily adapt it to include identity terms that suit the application in need of analysis . Since neutral identity terms are inherently not associated with sentiment , it is unfair to have identity term with differing levels of negative sentiment . This type of discrimination can show up in many downstream sentiment analysis applications . Thus , we want no differences between negative sentiment predictions of various identity terms . Mathematically , this can be represented as a uniform distribution of negative sentiment probability for identity terms from a protected group . Our RNSB metric captures the distance , via KL divergence , between the current distribution of negative sentiment and the fair uniform distribution . So the more fair a word embedding model with respect to sentiment bias , the lower the RNSB metric .", "entities": [[74, 75, "MetricName", "loss"], [106, 107, "DatasetName", "0"], [112, 114, "HyperparameterName", "K ="], [331, 333, "TaskName", "sentiment analysis"]]}
{"text": "I like going outside . I love Disneyland ! I go there every week . ' Will I sound like me ? ' Figure 1 : Illustration of the consistency issue in dialogue . While a literal dialogue agent ( S 0 ) fails to deliver a consistent persona , our self - conscious agent ( S 1 ) does so , by modeling an imaginary listener . Icons are designed by Nhor Phai and Vincent Le Moign . are highly insensitive to contradictory words , and thus fail to deliver consistent persona to the interlocutor ( Figure 1 ) . Also , extra modules other than the generative model is often required for improving consistency . Recent works on consistency in persona - based dialogue actively adopt the NLIbased approach ( Welleck et al , 2019 ; Song et al , 2019 ; Li et al , 2020 ; Song et al , 2020 ) , which have the following prerequisites . First , they require labeled pairs of persona sentences and dialogue utterances with three categories : entailment , neutral , and contradiction . Next , methods with NLI models for rating the agent 's consistency also need to train them separately with those labels . In this work , we step back from this NLI - based supervised approach and ponder : how do humans maintain consistency ? We humans never learn how to be consistent . Instead , we have an innate drive for consistency to hold our beliefs and behavior in harmony ( Festinger , 1962 ) . If so , how do we know we are consistent or not ? We do not ask others . We ask ourselves by predicting how we are perceived by others . Public self - consciousness is this awareness of the self as a social object that can be observed and evaluated by others ( Fenigstein et al , 1975 ) . We particularly emphasize that public self - consciousness is not equivalent to the philosophical self - consciousness ( or self - awareness ) 1 . Simply put , public self - consciousness is the concern about how oneself will be perceived by others , as opposed to the philosophical state of being conscious of self - existence . According to Doherty and Schlenker ( 1991 ) , people with high public self - consciousness tend to act more consistent with known information about themselves . They care deeply about how others will evaluate them and have a strong tendency to avoid negative evaluations ( Fenigstein et al , 1975 ) . Since inconsistency is condemned by others , one who has high public self - consciousness will try more to maintain consistency . In order to predict how we are perceived , we rely on abstract models of others ( Gopnik and Wellman , 1992 ) and simulate others ' reactions based on imagination ( Hassabis et al , 2013 ) . Inspired by this , our intuition is that self - consciousness through an imaginary listener will let dialogue agents better maintain consistency . Modeling a listener has been one of the main topics in computational pragmatics . Our work extends this long line of work in cognitive science by making use of the Bayesian Rational Speech Acts framework ( Frank and Goodman , 2012 ) , which has been originally applied to improving informativeness of referring expressions . Since personas ought to express who we are , we adopt this framework for dialogue agents by regarding personas as targets that should be conveyed to the interlocutor . As the agent tries to generate tokens that help the imaginary listener identify the agent 's persona , it can lastly generate more consistent utterances . In summary , we take inspiration from social cognition and pragmatics to endow generative agents with self - consciousness , which makes them imagine the listener 's reaction and incorporate it to the generation process for improving consistency . Our major contributions can be outlined as follows : ( 1 ) We propose an orthogonally applicable approach for any persona - based generative agents to improve consistency without the use of additional consistency labels and training . Moreover , it is even generalizable to improve context - consistency beyond persona in dialogue . ( 2 ) We extend the Rational Speech Acts framework ( Frank and Goodman , 2012 ) with two new technical features : ( i ) a learning method for distractor selection ( e.g. other samples different from the given target ( Andreas and Klein , 2016 ) ) , which has been usually done manually or randomly , and ( ii ) a different update for the listener 's world prior that better preserves information of previous states . ( 3 ) Our approach improves consistency of three recent generative agents ( See et al , 2019 ; Wolf et al , 2019b ; over Dialogue NLI ( Welleck et al , 2019 ) and PersonaChat ( Zhang et al , 2018 ) . Along with large reduction in contradiction , the utterance accuracy significantly increases too .", "entities": [[38, 39, "DatasetName", "agent"], [41, 42, "DatasetName", "0"], [54, 55, "DatasetName", "agent"], [195, 196, "DatasetName", "agent"], [496, 497, "DatasetName", "Inspired"], [605, 606, "DatasetName", "agent"], [617, 618, "DatasetName", "agent"], [856, 857, "MetricName", "accuracy"]]}
{"text": "Persona & Consistency in Dialogue . Li et al ( 2016 ) learn personas in embeddings . Zhang et al ( 2018 ) release the PersonaChat dataset , a chitchat dialogue set involving two interlocutors each playing their given persona . Madotto et al ( 2019 ) use meta - learning to adapt to new personas with few dialogue samples . use reinforcement learning to enhance mutual persona perception . Recent works use extra modules or NLI labels to improve consistency . Shum et al ( 2019 ) fill generated templates , and rank with a language model . use self - supervised feature extractors for generation . Welleck et al ( 2019 ) annotate NLI labels to the PersonaChat dataset . They train an NLI model and run pairwise comparison between candidates and persona to compute contradiction scores . The NLI approach is applied for coherence evaluation ( Dziri et al , 2019 ) , rewards to reinforcement learning agents ( Song et al , 2019 ) , finding inconsistent words ( Song et al , 2020 ) , and unlikelihood training ( Li et al , 2020 ) . They require NLI labels on the target dialogue dataset ; otherwise , sharp decrease in performance is observed , due to mismatch of data distribution ( Welleck et al , 2019 ) . Such dataset - specific NLI annotations and training NLI models can be costly and time - consuming . Compared to previous methods , the novelty of our approach is to improve consistency without NLI labels and extra modules . Pragmatics . Our approach belongs to the general family of Bayesian Rational Speech Acts ( Andreas and Klein , 2016 ) , image captioning ( Mao et al , 2016 ; Vedantam et al , 2017 ; Cohn - Gordon et al , 2018 ) , instruction following ( Fried et al , 2017 ) , navigating ( Fried et al , 2018 ) , translation ( Cohn - Gordon and Goodman , 2019 ) , summarization ( Shen et al , 2019 ) and referring expression generation ( Zarrie\u00df and Schlangen , 2019 ) . However , its application to the dialogue domain remains understudied . In this work , we explore how the RSA framework can be adopted in dialogue agents to alleviate the inconsistency problem . Also , we further extend the framework by making the distractor selection as a learnable process . ( Zhang et al , 2018 ) . They collect entailing and contradictory utterances to the given persona , and release an evaluation set comprised of dialogues each with 31 utterance candidates : 10 entailing , 10 neutral , and 10 contradictory utterances with 1 ground - truth ( GT ) utterance . On this evaluation set , we run three recent models ( See et al , 2019 ; Wolf et al , 2019b et al , 2020 ) that achieve the best performance on PersonaChat . We report four ranking metrics following Welleck et al ( 2019 ) : Hits@1 , Entail@1 , Neutral@1 and Contradict@1 . Each metric is the proportion of GT , entailing , neutral and contradictory utterances in the top - 1 candidates returned by the model , respectively . The models rank the candidates by perplexity scores . Figure 2 shows that all three models select contradictory candidates much more often than the GT utterances ( see further results in Table 3 ) . Though models are conditioned on a given persona , they are highly insensitive to contradictions .", "entities": [[48, 51, "TaskName", "meta - learning"], [146, 148, "TaskName", "coherence evaluation"], [285, 287, "TaskName", "image captioning"], [339, 340, "TaskName", "summarization"], [348, 350, "TaskName", "referring expression"], [510, 511, "MetricName", "Hits@1"], [551, 552, "MetricName", "perplexity"]]}
{"text": "To investigate why insensitivity to contradiction prevails in the state - of - the - art models , we further analyze the contradictory utterances returned by the models ( Contradict@1 - Utt ) , comparing with the GT utterances and the top - ranked entailing candidates ( Top Entail - Utt ) . Table 1 reports language metrics between the selected candidates and the given persona sentences using SPICE ( Anderson et al , 2016 ) and ROUGE ( Lin , 2004 ) . SPICE metric measures semantic similarity and ROUGE metric measures n - gram overlaps between two sentences . Contradict@1 - Utt shows lower SPICE scores and higher ROUGE scores than other utterances , implying that it may be different in semantics but similar in syntax to the given persona . To take a closer look , we extract the contradicting words from Contradict@1 - Utt and their counterparts from GT utterances to compare their average perplexity scores . In the Dialogue NLI dataset , every utterance is labeled with a triple ( entity 1 , relation , entity 2 ) , such as \" I just like to listen to rock music \" with ( i , like music , rock ) . By construction , Contradict@1 - Utt must contain words that are contradictory to the GT utterance and the given persona . The perplexity scores of contradictory words ( 106.7 ) were considerably lower than those of the counterparts in GT utterances ( 280.1 ) . Table 2 shows an example of such dialogue instance with perplexity per word . If properly conditioned with the given persona , models should show lower perplexity for the words in the persona . However , their perplexity scores are significantly higher than those of contradictory words . It reveals that models behave more as a plain language model rather than as a persona - conditioned model . Thus , guarantee of consistency for each word generation step is required for persona - based dialogue agents to resolve such issue .", "entities": [[68, 69, "DatasetName", "SPICE"], [84, 85, "DatasetName", "SPICE"], [87, 89, "TaskName", "semantic similarity"], [106, 107, "DatasetName", "SPICE"], [158, 159, "MetricName", "perplexity"], [228, 229, "MetricName", "perplexity"], [261, 262, "MetricName", "perplexity"], [277, 278, "MetricName", "perplexity"], [288, 289, "MetricName", "perplexity"]]}
{"text": "We seek to build a dialogue agent who is selfconscious about its consistency without the need for training on NLI labels or rating consistency with NLI models . Given that modeling the interactions between listener and speaker is a main topic in pragmatics , we take advantage of the RSA framework ( Frank and Goodman , 2012 ) . It treats language use as a recursive process where probabilistic speaker and listener reason about each other 's intentions in a Bayesian fashion . To apply the framework to sequence generation for dialogues , we extend the incremental approach proposed for image captioning ( Cohn - Gordon et al , 2018 ) . To generate an utterance , the agent computes the distribution of every next token u t at timestep t in Bayesian fashion as follows . Base Speaker S 0 . We first assume persona i is given to the base speaker , along with the dialogue \u221d # \" \u210e , $ \" , \" % \u00d7 # \" \" , \u210e , & \" ) \" ' ! ( ) Imaginary Listener : # \" ( | $ \" , \u210e , \" ) Base Speaker : # \" \" , \u210e , & \" ) Figure 3 : The proposed self - conscious agent S 1 consists of base speaker S 0 and imaginary listener L 0 . It recursively generates the next token u t at every time t. history h and partial utterance u < t , as shown in Figure 3 . The base speaker S t 0 returns a distribution over the next token at timestep t : S t 0 ( u t | i , h , u < t ) . Any conditional dialogue agent can be used as a base speaker . See the details in Section 5.2 . Imaginary Listener L 0 . While the base speaker generates each token one at a time , the imaginary listener reasons about the speaker 's persona . The imaginary listener L t 0 is the posterior distribution of the speaker 's persona in terms of the base speaker and the world prior p t ( i ) over personas as follows , L t 0 ( i | h , u \u2264t , p t ) \u221d S t 0 ( u t | i , h , u < t ) \u03b2 \u00d7 p t ( i ) i I S t 0 ( u t | i , h , u < t ) \u03b2 \u00d7 p t ( i ) . ( 1 ) where \u03b2 on S t 0 is the listener rationality coefficient that controls the amount of information from the current timestep compared to the cumulative prior p t ( i ) . L 0 returns a probability distribution over the personas in world I , which is a finite set ( | I | = 3 ) comprising the given persona i and distractor personas . The distractors are different personas from other dialogue instances in the dataset . We decide world I per dialogue instance through learning , which will be elaborated in Section 4.2 . Self - Conscious Speaker S 1 . With S t 0 and L t 0 , the self - conscious speaker S t 1 is defined as S t 1 ( u t | i , h , u < t ) \u221d L t 0 ( i | h , u \u2264t , p t ) \u03b1 \u00d7 S t 0 ( u t | i , h , u < t ) , ( 2 ) where \u03b1 is the speaker rationality coefficient that determines how much the likelihood is considered . By taking the listener 's distribution into account , the speaker is now self - conscious about what persona it sounds like . Especially , the agent seeks to be perceived as the given persona i rather than some other persona i . The likelihood of each token being identified as the persona i acts as a bonus added to the base speaker 's token scores . Hence , tokens that are consistent to the given persona are preferred to others . The token with the highest probability is added to the partial utterance , becoming the next input u < t+1 for the speaker . Updating the world prior with L 0 . Starting from a uniform distribution as the initial prior p 0 ( i ) , we update the world prior p t+1 ( i ) according to S 1 's output u t at every time step : p t+1 ( i ) = L t 0 ( i | h , u \u2264t , p t ) . ( 3 ) Hence , p t ( i ) represents the cumulative state of the partial utterance up to t. Cohn - Gordon et al ( 2018 ) report the prior update with L 1 \u221d S t 0 ( u t | i , h , u < t ) \u00d7 L t 0 ( i | h , u \u2264t , p t ) makes little practical effect compared to a uniform prior . We find that updating the prior with Eq . ( 3 ) instead is effective . See the results in Section 5.6 .", "entities": [[6, 7, "DatasetName", "agent"], [100, 102, "TaskName", "image captioning"], [118, 119, "DatasetName", "agent"], [140, 141, "DatasetName", "0"], [217, 218, "DatasetName", "agent"], [225, 226, "DatasetName", "0"], [230, 231, "DatasetName", "0"], [264, 265, "DatasetName", "0"], [278, 279, "DatasetName", "0"], [295, 296, "DatasetName", "agent"], [314, 315, "DatasetName", "0"], [343, 344, "DatasetName", "0"], [375, 376, "DatasetName", "0"], [390, 391, "DatasetName", "0"], [403, 404, "HyperparameterName", "\u03b2"], [414, 415, "DatasetName", "0"], [427, 428, "HyperparameterName", "\u03b2"], [439, 440, "HyperparameterName", "\u03b2"], [443, 444, "DatasetName", "0"], [471, 472, "DatasetName", "0"], [545, 546, "DatasetName", "0"], [549, 550, "DatasetName", "0"], [580, 581, "DatasetName", "0"], [592, 593, "HyperparameterName", "\u03b1"], [596, 597, "DatasetName", "0"], [614, 615, "HyperparameterName", "\u03b1"], [655, 656, "DatasetName", "agent"], [741, 742, "DatasetName", "0"], [753, 754, "DatasetName", "0"], [789, 790, "DatasetName", "0"], [841, 842, "DatasetName", "0"], [857, 858, "DatasetName", "0"]]}
{"text": "Distractors ( Andreas and Klein , 2016 ) are samples ( e.g. other personas in the dataset ) which are different from the given target . In previous works of RSA , the distractors to be included in world I are selected manually or randomly from the dataset . However , we find that performance variance is large according to the selected distractors . We thus propose to learn distractor selection , especially based on the life - long memory network ( Kaiser et al , 2017 ) . The life - long memory network is capable of implicitly clustering similar dialogue contexts into a few slots with associated persona . Therefore , it can efficiently memorize and retrieve distractor personas for each context . In Appendix , we experiment that our approach outperforms other models including BERT - based algorithms . To better select useful distractor personas , supervised learning is desirable . However , there is no explicit label indicating which distractors are helpful for each dialogue . We select the persona that have the best Hits@1 as the distractor label per training dialogue . The Hits@1 is the score for favoring the ground - truth next utterance ( consistent and context - relevant ) over other candidate utterances which are just being consistent ( i.e. entailing ) or contradictory to the given persona . In other words , the score represents consistency and also appropriateness at the same time . Thus , such distractors can help the self - conscious agent to generate responses which are context - relevant and allow the imaginary listener to identify the speaker 's persona . Each training datapoint comprises a given persona , a distractor persona and dialogue context . Memory Structure . The memory consists of three types of information : M = ( K , v , a ) . K R m\u00d7d is a key matrix , where m is the number of memory slots and d is the dimension of the key vectors , which are the embedding of datapoints . The value vector v R m stores the index of a persona . a R m is an age vector , which is used for memory update . We set m = 16 , 000 and d = 768 . Memory Addressing . We construct the query vector q for each datapoint with the BERT - Uncased - Base ( Devlin et al , 2019 ) model . We use the output embedding of BERT 's [ CLS ] token , and normalize it to a unit length to build q R d . Using the cosine similarity between q and each memory key , we can find the k nearest neighbors : ( n 1 , n 2 , ... , n k ) = N N k ( q , K ) . ( 4 ) Memory Loss . Suppose that the query datapoint has a distractor label l. Among ( n 1 , ... , n k ) , we denote the positive neighbor n p as the one with v [ n p ] = l and the negative neighbor n b with v [ n b ] = l. If there are multiple positive neighbors , we pick the one with the smallest memory index . If no positive neighbor is found , we select a random key whose value is l. For the negative neighbor , we select one randomly from ( n 1 , ... , n k ) . We set k = 2048 . Then , the loss is computed as L = max ( q K [ n b ] \u2212 q K [ n p ] + \u03b1 , 0 ) , ( 5 ) where \u03b1 is a positive margin , which we set as 0.2 . This loss maximizes the cosine similarity between the query q and the positive key K [ n p ] , while minimizing the similarity to the negative key K [ n b ] . We finetune the query network BERT with this loss . Memory Update . After computing the loss , memory M is updated differently for two cases . ( 1 ) If the top - 1 neighbor 's value ( i.e. persona ) is correct ( v [ n 1 ] = l ) , the key vector is updated as : K [ n 1 ] q + K [ n 1 ] q + K [ n 1 ] . ( 6 ) ( 2 ) Otherwise ( v [ n 1 ] = l ) , we make a slot for the query ; we find the oldest memory slot n according to the age vector a and write K [ n ] q , v [ n ] l , a [ n ] 0 . ( 7 ) Training & Inference . In our Distractor Memory network , training corresponds to updating the memory and the parameters of the query network . At inference , given a test example , we obtain the query by encoding the dialogue context and the persona using BERT . We find n nearest keys from the memory , and use their values ( i.e. persona indices ) as the distractor personas . We set n = 2 .", "entities": [[79, 81, "MethodName", "memory network"], [93, 95, "MethodName", "memory network"], [137, 138, "MethodName", "BERT"], [178, 179, "MetricName", "Hits@1"], [188, 189, "MetricName", "Hits@1"], [253, 254, "DatasetName", "agent"], [398, 399, "MethodName", "BERT"], [418, 419, "MethodName", "BERT"], [593, 595, "HyperparameterName", "k ="], [595, 596, "DatasetName", "2048"], [600, 601, "MetricName", "loss"], [622, 623, "HyperparameterName", "\u03b1"], [624, 625, "DatasetName", "0"], [631, 632, "HyperparameterName", "\u03b1"], [644, 645, "MetricName", "loss"], [682, 683, "MethodName", "BERT"], [685, 686, "MetricName", "loss"], [693, 694, "MetricName", "loss"], [814, 815, "DatasetName", "0"], [826, 828, "MethodName", "Memory network"], [864, 865, "MethodName", "BERT"]]}
{"text": "Base Speakers . We experiment on three pretrained models including ControlSeq2Seq ( See et al , 2019 ) , TransferTransfo ( Wolf et al , 2019b ) , and Blender as base speakers ( S 0 ) for our self - conscious agents ( S 1 ) . The ControlSeq2Seq is a Seq2Seq model with attention trained on Twitter dataset ( Miller et al , 2017 ) and finetuned on PersonaChat . TranferTransfo based on GPT ( Radford et al , 2018 ) is the winner of the ConvAI2 competition in automatic evaluation . Blender , a recently released generative dialogue model , is the state - of - the - art open - domain chatbot . Our approach improves these base speakers by ( Welleck et al , 2019 ) . + DM is the Distractor Memory . High scores in Hits@1 , Entail@1 and low scores in Contradict@1 imply better consistency . granting them the sense of self - consciousness . We defer implementation details to Appendix . Evaluation Metrics . For Dialogue NLI , we report three ranking metrics introduced in the original paper : Hits@1 , Entail@1 , and Contradict@1 . Each metric is the proportion of GT , entailing , and contradictory utterances in the top - 1 candidates returned by the model , respectively . High scores in Entail@1 and low scores in Contradict@1 indicate better consistency with the persona . For PersonaChat , we report Hits@1 , standard F1 score , perplexity and C score , following the Con - vAI2 protocol . Hits@1 is the accuracy of choosing the ground - truth next - utterance among 20 candidates as the models rank the candidates by perplexity . The C score is a metric for dialogue consistency , introduced in Madotto et al ( 2019 ) . It computes pairwise comparison between utterance u and persona sentence p j with a pretrained NLI model . The NLI model returns 1 , 0 , - 1 for entailment , neutrality , and contradiction , respectively . We sum the NLI scores across persona sentences per dialogue instance : C ( u ) = j NLI ( u , p j ) .", "entities": [[29, 30, "MethodName", "Blender"], [35, 36, "DatasetName", "0"], [52, 53, "MethodName", "Seq2Seq"], [75, 76, "MethodName", "GPT"], [88, 89, "DatasetName", "ConvAI2"], [94, 95, "MethodName", "Blender"], [115, 116, "TaskName", "chatbot"], [142, 143, "MetricName", "Hits@1"], [188, 189, "MetricName", "Hits@1"], [242, 243, "MetricName", "Hits@1"], [245, 247, "MetricName", "F1 score"], [248, 249, "MetricName", "perplexity"], [260, 261, "MetricName", "Hits@1"], [263, 264, "MetricName", "accuracy"], [283, 284, "MetricName", "perplexity"], [328, 329, "DatasetName", "0"]]}
{"text": "Results on Dialogue NLI . Table 3 compares the performance of dialogue agents on the Dialogue NLI evaluation set . Our self - conscious agent S 1 significantly reduces Contradict@1 scores and increases the Entail@1 along with the Hits@1 accuracy of the literal agents S 0 . We remind that each entailing candidate shares the same annotated triple as the GT utterance . In other words , they have similar semantics to the GT utterance and follow the ( Zhang et al , 2018 ) . C is the consistency score evaluated by a pretrained NLI model ( Madotto et al , 2019 ) . For TransferTransfo , we use the generative version to calculate Hits@1 . given persona . Thus , Entail@1 is a lenient version of Hits@1 ( Welleck et al , 2019 ) . The Distractor Memory ( DM ) is better than random distractor selection for S 1 across all metrics . It concludes that learned distractors are more effective than random distractors for pragmatic agents . Results on PersonaChat . Table 4 compares the performance of different dialogue agents on the PersonaChat dataset . Our model S 1 outperforms all other generative dialogue agents in terms of consistency related metrics , i.e. Hits@1 and C score . Since the posterior update of our self - conscious agent revises the distribution learned by the base speaker , the increase in perplexity is natural due to the effect of regularization . Nevertheless , our approach improves the F1 score for TransferTransfo and Blender . Thus , being consistent to the given persona can also help improve the generation performance of dialogue agents . Comparison with agents that use NLI model . We also test agents with pretrained NLI models attached ( Welleck et al , 2019 ) , denoted by + NLI in Table 5 . The NLI model computes contradiction scores of each candidate utterances , and penalize its rank accordingly . Compared to base agents with no self - consciousness , our agents improve consistency in all three metrics even further when using additional NLI models . Another notable result is that our agents without NLI ( S 1 + DM in Table 3 ) for ControlSeq2Seq and TransferTransfo even outperform the base agents with NLI ( S 0 + NLI ) on Hits@1 . That is , our self - conscious agents achieve better GT accuracy even without the help of an NLI model trained on consistency labels .", "entities": [[24, 25, "DatasetName", "agent"], [38, 39, "MetricName", "Hits@1"], [39, 40, "MetricName", "accuracy"], [45, 46, "DatasetName", "0"], [115, 116, "MetricName", "Hits@1"], [128, 129, "MetricName", "Hits@1"], [207, 208, "MetricName", "Hits@1"], [221, 222, "DatasetName", "agent"], [234, 235, "MetricName", "perplexity"], [250, 252, "MetricName", "F1 score"], [255, 256, "MethodName", "Blender"], [383, 384, "DatasetName", "0"], [388, 389, "MetricName", "Hits@1"], [401, 402, "MetricName", "accuracy"]]}
{"text": "To further analyze our self - conscious agent , we conduct experiments by controlling three features of our agent : world prior updates p t ( i ) , listener rationality \u03b2 and speaker rationality \u03b1 . World Prior Update . In the self - conscious agent , the world prior acts as a cumulative state over personas . We remind that we propose to update the world prior with L t 0 instead of L t 1 in Eq . ( 3 ) . As reported in Cohn - Gordon et al ( 2018 ) , our experiments on the Dialogue NLI dataset confirm the prior update with L t 1 makes little difference in performance compared with using a uniform distribution . However , our approach with L t 0 makes significant difference , as shown in Figure 5 . The reason is that the pragmatic listener L t 1 \u221d S t 0 ( u t | i , h , u < t ) \u00d7 L t 0 ( i | h , u \u2264t , p t ) reflects the current S t 0 twice ( i.e. in L t 0 and in itself ) per time step . Hence , the update with L t 1 becomes more of an instantaneous prior rather than a cumulative one . On the other hand , L t 0 moderately combines the information from both S t 0 and p t ( i ) , preserving better cumulative information . Listener Rationality \u03b2 . We add \u03b2 in L t 0 to control the amount of information incorporated to the world prior p t ( i ) . Figure 5 depicts that when \u03b2 is large , the Hits@1 scores ( i.e. the GT accuracy ) drop . With a big \u03b2 , the information S t 0 at current time step overrides the cumulative prior p t ( i ) . That is , the utterance state evolves shortsightedly , ignoring the context information from the previous steps . Therefore , setting of \u03b2 \u2264 1 is advantageous for the self - conscious agent to incrementally decode . Speaker Rationality \u03b1 . Figure 6 shows an example of how generated responses vary according to the intensity of speaker rationality \u03b1 . As \u03b1 increases , the self - conscious agent reflects the listener 's distribution ( i.e. the likelihood ) more into the posterior . When \u03b1 is too large , the posterior distribution is overwhelmed by the likelihood of the persona . Then , the language model degenerates to favor uttering fragments of the given persona while even ignoring the syntax . Hence , \u03b1 can control the degree of copying the given condition text . An appropriate \u03b1 value allows the given persona condition to blend smoothly in the utterance .", "entities": [[7, 8, "DatasetName", "agent"], [18, 19, "DatasetName", "agent"], [31, 32, "HyperparameterName", "\u03b2"], [35, 36, "HyperparameterName", "\u03b1"], [46, 47, "DatasetName", "agent"], [72, 73, "DatasetName", "0"], [131, 132, "DatasetName", "0"], [155, 156, "DatasetName", "0"], [171, 172, "DatasetName", "0"], [188, 189, "DatasetName", "0"], [195, 196, "DatasetName", "0"], [231, 232, "DatasetName", "0"], [240, 241, "DatasetName", "0"], [255, 256, "HyperparameterName", "\u03b2"], [259, 260, "HyperparameterName", "\u03b2"], [263, 264, "DatasetName", "0"], [286, 287, "HyperparameterName", "\u03b2"], [291, 292, "MetricName", "Hits@1"], [297, 298, "MetricName", "accuracy"], [304, 305, "HyperparameterName", "\u03b2"], [310, 311, "DatasetName", "0"], [347, 348, "HyperparameterName", "\u03b2"], [357, 358, "DatasetName", "agent"], [364, 365, "HyperparameterName", "\u03b1"], [383, 384, "HyperparameterName", "\u03b1"], [386, 387, "HyperparameterName", "\u03b1"], [393, 394, "DatasetName", "agent"], [410, 411, "HyperparameterName", "\u03b1"], [449, 450, "HyperparameterName", "\u03b1"], [463, 464, "HyperparameterName", "\u03b1"]]}
{"text": "This work investigated how modeling public selfconsciousness can help dialogue agents improve persona - consistency . We showed existing dialogue agents are highly insensitive to contradiction , and introduced an orthogonally applicable method using the RSA framework ( Frank and Goodman , 2012 ) to alleviate the issue . We also designed a 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3 . learning method for distractor selection , named Distractor Memory and proposed a better update for the listener 's world prior . Furthermore , we demonstrated how our approach can be generalized to improve dialogue context - consistency . Our self - conscious agents improved the base agents on the Dialogue NLI ( Welleck et al , 2019 ) and PersonaChat ( Zhang et al , 2018 ) dataset , without consistency labels and NLI models . An important future direction will be generating the distractors and learning the rationality coefficients . A Results on Variants of Distractor Selection ( Section 4.2 ) ( Welleck et al , 2019 ) . We compare our proposed Distractor Memory ( DM ) with three heuristic methods , and two variants of the pretrained BERT model ( Devlin et al , 2019 ) . As a straightforward baseline , we randomly select k personas from training set and directly use it as distractors . Second , we test the k - nearest search by speaker 's persona , denoted by Nearest ; for a given persona descriptions , we find its closest training persona embedding using cosine similarity on average pooled BERT features . The third baseline denoted by Farthest is to find the k - farthest persona among the training personas . We also compare with two variants of the BERT model . The first variant is BERT - Classifier , which takes dialogue context as input and returns the index of persona from training set as output . The second variant is bi - encoder ranking model of Miller et al ( 2017 ) , denoted by BERT - Ranker . It encodes dialogue context and candidate persona with separate BERT encoders measuring its ranking with cosine similarity . For both methods , we use top - k ranked personas as distractors and set k = 4 for all the methods . We use Adam optimizer ( Kingma and Ba , 2015 ) with learning rate 2e - 5 and finetune BERT - Uncased - Base up to 3 epochs . Table 8 compares the performance of different distractor selecting methods on the Dialogue NLI evaluation set ( Welleck et al , 2019 ) . We set \u03b1 = 8 , \u03b2 = 0.5 , and | I | = 5 . The DM model outperforms all the baselines across all metrics . The Farthest shows better performance than the Nearest . It can be understood that dissimilar distractors are more effective in the Rational Speech Acts framework ( Frank and Goodman , 2012 ) . The BERT - Ranker performs the best among baselines , but not as good as ours , which validates that memorization capability is effective for selecting useful distractors .", "entities": [[192, 193, "MethodName", "BERT"], [259, 260, "MethodName", "BERT"], [289, 290, "MethodName", "BERT"], [296, 297, "MethodName", "BERT"], [337, 338, "MethodName", "BERT"], [350, 351, "MethodName", "BERT"], [374, 376, "HyperparameterName", "k ="], [384, 385, "MethodName", "Adam"], [385, 386, "HyperparameterName", "optimizer"], [394, 396, "HyperparameterName", "learning rate"], [401, 402, "MethodName", "BERT"], [437, 438, "HyperparameterName", "\u03b1"], [441, 442, "HyperparameterName", "\u03b2"], [497, 498, "MethodName", "BERT"]]}
{"text": "Base Codes and Datasets . We use the ParlAI framework 2 ( Miller et al , 2017 ) and Hugging - Face 's Transformers 3 ( Wolf et al , 2019a ) to implement our models and baselines . We use Dialogue NLI ( Welleck et al , 2019 ) and PersonaChat ( Zhang et al , 2018 ) datasets from the ParlAI framework as is . We use the default preprocessing in ParlAI . Training . Our self - consciousness approach improves consistency for any pretrained dialogueagents without additional consistency labels and pretrained NLI models . Since it post - processes the output probability of pretrained dialogue - agents in a Bayesian fashion , no additional model parameters are added to the dialogue agents . Thus , it does not require any training . In the case of using the Distractor Memory ( DM ) , first we initialize BERT - Uncased - Base with pretrained weights and finetune it up to 3 epochs with Adam optimizer with learning rate 2e - 5 . Then we find the best distractor persona for each model and use those labels to train our DM . We train our DM on one NVIDIA TITAN Xp GPU up to 7 epochs . Hyperparameters . For Dialogue NLI evaluation , we set the speaker rationality \u03b1 = 8.0 , the listener rationality \u03b2 = 1.0 , and the cardinality of the world I to 3 . In PersonaChat evaluation , we set \u03b1 = 2.0 , \u03b2 = 0.3 for ControlSeq2Seq ( See et al , 2019 ) , \u03b1 = 2 , \u03b2 = 0.9 for TransferTransfo ( Wolf et al , 2019b ) , and \u03b1 = 2.0 , \u03b2 = 0.5 for Blender 90 M . We also set | I | = 3 . We experiment \u03b1 = { 1.0 , 2.0 , 4.0 , 8.0 , 16.0 } , \u03b2 = { 0.3 , 0.5 , 0.9 , 1.0 , 2.0 , 4.0 } , and | I | = { 2 , 3 , 5 } . We choose the hyper - parameter configuration showing the best performance in Hits@1 for Dialogue NLI and F1 score for PersonaChat . The posterior distribution of our self - conscious agents are computed deterministically . For our Distractor Memory , we set the memory key matrix as K R m\u00d7d , where m = 16000 and d = 768 . We set the number of nearest neighbor k = 2048 . Inference . We use greedy decoding for all methods . The average runtime for our self - conscious approach is dependent on the base dialogue agents and the cardinality of world I which can be run in parallel like beam search . Evaluation . We follow the evaluation of the Par - lAI framework . Following Madotto et al ( 2019 ) , 2 https://parl.ai/ 3 https://huggingface.co/transformers/ we use the finetuned BERT - based NLI model 4 to compute the C score .", "entities": [[150, 151, "MethodName", "BERT"], [166, 167, "MethodName", "Adam"], [167, 168, "HyperparameterName", "optimizer"], [169, 171, "HyperparameterName", "learning rate"], [201, 202, "DatasetName", "TITAN"], [221, 222, "HyperparameterName", "\u03b1"], [228, 229, "HyperparameterName", "\u03b2"], [248, 249, "HyperparameterName", "\u03b1"], [252, 253, "HyperparameterName", "\u03b2"], [265, 266, "HyperparameterName", "\u03b1"], [269, 270, "HyperparameterName", "\u03b2"], [283, 284, "HyperparameterName", "\u03b1"], [287, 288, "HyperparameterName", "\u03b2"], [291, 292, "MethodName", "Blender"], [306, 307, "HyperparameterName", "\u03b1"], [320, 321, "HyperparameterName", "\u03b2"], [361, 362, "MetricName", "Hits@1"], [366, 368, "MetricName", "F1 score"], [416, 418, "HyperparameterName", "k ="], [418, 419, "DatasetName", "2048"], [491, 492, "MethodName", "BERT"]]}
{"text": "To investigate the influence of context features on analogical reasoning , we consider not only word features , but also ngram features inspired by statistical language models , and character ( Hanzi ) features based on the close relationship between Chinese words and their composing characters 5 . Specifically , we use word bigrams for ngram features , character unigrams and bigrams for character features . Ngrams and Chinese characters are effective features in training word representations ( Zhao et al , 2017 ; Chen et al , 2015 ; Bojanowski et al , 2016 ) . However , Table 4 shows that there is only a slight increase on CA_translated dataset with ngram features , and the accuracies in most cases decrease after integrating character features . In contrast , on CA8 dataset , the introduction of ngram and character features brings significant and consistent improvements on almost all the categories . Furthermore , character features are especially advantageous for reasoning of morphological relations . SGNS model integrating with character features even doubles the accuracy in morphological questions . Besides , the representations achieve surprisingly high accuracies in some categories of CA_translated , which means that there is little room for further improvement . However it is much harder for representation methods to achieve high accuracies on CA8 . The best configuration only achieves 68.0 % .", "entities": [[175, 176, "MetricName", "accuracy"]]}
{"text": "We used the scoring metric described in Thorne et al ( 2018 ) to evaluate the submissions . The FEVER shared task requires submission of evidence to justify the labeling of a claim . The training , development and test data splits contain multiple sets of evidence for each claim , each set being a minimal set of sentences that fully support or refute it . The primary scoring metric for the task is the label accuracy conditioned on providing at least one complete set of evidence , referred to as the FEVER score . Sentences labeled ( correctly ) as NOTENOUGHINFO do not require evidence . Correctly labeled claims with no or only partial evidence received no points for the FEVER score . Where multiple sets of evidence was annotated in the data , only one set was required for the claim to be considered correct for the FEVER score . Since the development and evaluation data splits are balanced , random baseline label accuracy ignoring the requirement for evidence is 33.33 % . This performance level can also be achieved for the FEVER score by predicting NOTENOUGHINFO for every claim . However , as the FEVER score requires evidence for SUP - PORTED and REFUTED claims , a random baseline is expected to score lower on this metric . We provide an open - source release of the scoring software . 2 Beyond the FEVER score , it computes precision , recall , F 1 , and label accuracy to provide diagnostic information . The recall point is awarded , as is the case for the FEVER score , only by providing a complete set of evidence for the claim .", "entities": [[19, 20, "DatasetName", "FEVER"], [76, 77, "MetricName", "accuracy"], [92, 93, "DatasetName", "FEVER"], [121, 122, "DatasetName", "FEVER"], [149, 150, "DatasetName", "FEVER"], [165, 166, "MetricName", "accuracy"], [184, 185, "DatasetName", "FEVER"], [197, 198, "DatasetName", "FEVER"], [236, 237, "DatasetName", "FEVER"], [250, 251, "MetricName", "accuracy"], [268, 269, "DatasetName", "FEVER"]]}
{"text": "A large number of teams report a multi - step approach to document selection . The majority of submissions report extracting some combination of Named Entities , Noun Phrases and Capitalized Expressions from the claim . These were used either as inputs to a search API ( i.e. Wikipedia Search or Google Search ) , search server ( e.g. Lucene 5 or Solr 6 ) or as keywords for matching against Wikipedia page titles or article bodies . BUPT - NLPer report using S - MART for entity linking ( Yang and Chang , 2015 ) and the highest scor - ing team , UNC - NLP , report using page viewership statistics to rank the candidate pages . GESIS Cologne report directly selecting sentences using the Solr search , bypassing the need to perform document retrieval as a separate step . The team which scored highest on evidence precision and evidence F1 was Papelo ( precision = 92.18 % and F 1 = 64.85 % ) who report using a combination of TF - IDF for document retrieval and string matching using named entities and capitalized expressions . The teams which scored highest on evidence recall were Athene UKP TU Darmstadt ( recall = 85.19 % ) and UCL Machine Reading Group ( recall = 82.84 % ) 7 8 Athene report extracting nounphrases from the claim and using these to query the Wikipedia search API . A similar approach was used by Columbia NLP who query the Wikipedia search API using named entities extracted from the claim as a query string , all the text before the first lowercase verb phrase as a query string and also combine this result with Wikipedia pages identified with Google search using the entire claim . UCL Machine Reading Group report a document retrieval approach that identifies Wikipedia article titles within the claim and ranks the results using features such as capitalization , sentence position and token match .", "entities": [[51, 52, "DatasetName", "Google"], [87, 89, "TaskName", "entity linking"], [152, 153, "MetricName", "F1"], [199, 200, "DatasetName", "UKP"], [287, 288, "DatasetName", "Google"]]}
{"text": "Document retrieval We applied the constituency parser from AllenNLP to extract noun phrases in the claim and made use of Wikipedia API to search corresponding pages for each noun phrase . So as to remove noisy pages from the results , we have stemmed the words of their titles and the claim , and then discarded pages whose stemmed words of the title are not completely included in the set of stemmed words in the claim . Sentence selection The hinge loss with negative sampling is applied to train the enhanced LSTM . For a given positive claim - evidence pair , negative samples are generated by randomly sampling sentences from the retrieved documents . RTE We combine the 5 sentences from sentence selection and the claim to form 5 pairs and then apply enhanced LSTM for each pair . We combine the resulting representations using average and max pooling and feed the resulting vector through an MLP for classification .", "entities": [[81, 82, "MetricName", "loss"], [91, 92, "MethodName", "LSTM"], [115, 116, "DatasetName", "RTE"], [135, 136, "MethodName", "LSTM"], [148, 150, "MethodName", "max pooling"], [157, 158, "DatasetName", "MLP"]]}
{"text": "We develop a system for the FEVER fact extraction and verification challenge that uses a high precision entailment classifier based on transformer networks pretrained with language modeling ( Radford and Salimans , 2018 ) , to classify a broad set of potential evidence . The precision of the entailment classifier allows us to enhance recall by considering every statement from several articles to decide upon each claim . We include not only the articles best matching the claim text by TFIDF score , but read additional articles whose titles match named entities and capitalized expressions occurring in the claim text . The entailment module evaluates potential evidence one statement at a time , together with the title of the page the evidence came from ( providing a hint about possible pronoun antecedents ) . In preliminary evaluation , the system achieved 57.36 % FEVER score , 61.08 % label accuracy , and 64.85 % evidence F1 on the FEVER shared task test set .", "entities": [[6, 7, "DatasetName", "FEVER"], [143, 144, "DatasetName", "FEVER"], [149, 150, "MetricName", "accuracy"], [155, 156, "MetricName", "F1"], [158, 159, "DatasetName", "FEVER"]]}
{"text": "In this paper , we describe the system we designed for the FEVER 2018 Shared Task . The aim of this task was to conceive a system that can not only automatically assess the veracity of a claim but also retrieve evidence supporting this assessment from Wikipedia . In our approach , the Wikipedia documents whose Term Frequency - Inverse Document Frequency ( TFIDF ) vectors are most similar to the vector of the claim and those documents whose names are similar to the named entities ( NEs ) mentioned in the claim are identified as the documents which might contain evidence . The sentences in these documents are then supplied to a decomposable attention - based textual entailment recognition module . This module calculates the probability of each sentence supporting the claim , contradicting the claim or not providing any relevant information . Various features computed using these probabilities are finally used by a Random Forest classifier to determine the overall truthfulness of the claim . The sentences which support this classification are returned as evidence . Our approach achieved a 42.77 % evidence F1 - score , a 51.36 % label accuracy and a 38.33 % FEVER score .", "entities": [[12, 13, "DatasetName", "FEVER"], [185, 188, "MetricName", "F1 - score"], [193, 194, "MetricName", "accuracy"], [198, 199, "DatasetName", "FEVER"]]}
{"text": "We describe the UMBC - FEVER system that we used in the 2018 FEVER shared task . The system employed a frame - based information retrieval approach to select Wikipedia sentences providing evidence and used a two - layer multilayer perceptron ( MLP ) for classification . Our submission achieved a score of 0.3695 on the Evidence F1 metric for retrieving relevant evidential sentences ( 10 th out of 24 ) and a score of 0.2376 on the FEVER metric ( just below the baseline system ) .", "entities": [[5, 6, "DatasetName", "FEVER"], [13, 14, "DatasetName", "FEVER"], [24, 26, "TaskName", "information retrieval"], [42, 43, "DatasetName", "MLP"], [57, 58, "MetricName", "F1"], [78, 79, "DatasetName", "FEVER"]]}
{"text": "Social chatbots , also known as chit - chat chatbots , evolve rapidly with large pretrained language models . Despite the huge progress , privacy concerns have arisen recently : training data of large language models can be extracted via model inversion attacks . On the other hand , the datasets used for training chatbots contain many private conversations between two individuals . In this work , we further investigate the privacy leakage of the hidden states of chatbots trained by language modeling which has not been well studied yet . We show that speakers ' personas can be inferred through a simple neural network with high accuracy . To this end , we propose effective defense objectives to protect persona leakage from hidden states . We conduct extensive experiments to demonstrate that our proposed defense objectives can greatly reduce the attack accuracy from 37.6 % to 0.5 % . Meanwhile , the proposed objectives preserve language models ' powerful generation ability .", "entities": [[15, 18, "TaskName", "pretrained language models"], [107, 108, "MetricName", "accuracy"], [142, 143, "MetricName", "accuracy"]]}
{"text": "Social chatbots have been widely used to benefit many applications from answering factual questions to showing emotional companionship . With recent progress in large pretrained language models ( Radford et al , 2019 ; Yang et al , 2019 ) , some attempts ( Wolf et al , 2019 ; Ham et al , 2020 ; Shen et al , 2021 ; Sevegnani et al , 2021 ; Gu et al , 2021b ) are made to build chatbots based on large generative language models ( LMs ) . To train such LM - based chatbots , private conversations are collected . Unfortunately , large language models tend to memorize training data and some private data can be recovered from models ( Pan et al , 2020 ; Carlini et al , 2021 ) . Besides such memorization problems , \" overlearning \" on simple training objectives can reveal sensitive attributes indirectly related to the learning task ( Song and Shmatikov , 2020 ) . LM - based social chatbots essentially inherit the privacy issues of general LMs and the overlearning problem . For example , as Figure 1 shows , when using a fine - tuned GPT - 2 as the encoder and decoder of an LM - based social chatbot , if the learned representation of each utterance can be obtained by an adversary , then the adversary can build a classifier to predict the persona information based on the representation . As shown by the example , for five out of 14 utterances , the attacker can successfully predict the persona , which can be harmful if the users ( speakers of the utterances ) do not prefer to reveal the persona information . Thus , in practice , when deploying such kinds of chatbots in real applications , we should first make sure that no private information can be leaked by the models . To systematically study the privacy issues in LMbased social chatbots , there are several challenges . First , there is no existing data that can be used to quantify how much private information is revealed by an LM . Second , there has been no existing work showing how to attack utterance - level representations to obtain sensitive information . Third , there has been no existing LM - based chatbot that can defend against persona inference attacks , and no study shows how to protect both known and unknown persona attributes . In this paper , to address the above challenges , we use the fine - tuned GPT - 2 as our chatbot . We first collect a dataset by aligning personas with corresponding utterances in PersonaChat dataset ( Zhang et al , 2018 ) . Then we show that \" overlearning \" can happen for LM - based chatbots to reveal personas of speakers . We build a single external multi - layer perception ( MLP ) attacker model to perform black - box persona inference attacks on the utterance - level embeddings . With no access to parameters of the chatbot , the attacker model can infer speakers ' personas with 37.59 % accuracy over 4 , 332 personas . The high accuracy of the attacker model implies that the utterance - level embeddings have potential vulnerabilities to reveal \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fc \uf0fc \uf0fc \uf0fc \uf0fc \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb \uf0fb Figure 1 : Black - box persona inference attacks ( over 4 , 332 personas ) on a dialog . Every representation of the utterance , which is based on the last hidden state of GPT - 2 , is attacked without defense ( column of \" Attacks on LM \" ) and with defense ( column of \" Attacks on the defensed LM \" ) . If the model can predict the persona of the speaker based on the observed representation , then we regard it as a successful attack ; otherwise , unsuccessful . In practice , when deploying a model , a robust model which will reveal nothing of the encoded utterances is expected . speakers ' private persona attributes . Thus , it is necessary to improve training algorithms to address such overlearning issues . Finally , we apply defense learning strategies on the GPT - 2 to prevent such black - box attacks . We combine proposed KL divergence loss ( KL loss ) with mutual information loss ( MI loss ) ( Song et al , 2019 ) as additional defense objectives to train the GPT - 2 and decrease the attacker 's persona inference accuracy to 0.53 % . Our contributions can be summarized as follows : 1 1 ) : To the best of our knowledge , we are the first to disclose and analyze the persona inference attack for LM - based chatbots and treat it as a privacy risk . 2 ) : We propose an effective defensive training algorithm to prevent dialog representations from leaking personas of the corresponding speakers by uniform distribution approximation and mutual information minimization . 3 ) : We conduct extensive experiments to quantify both privacy and utility of proposed defense mechanisms . Besides solving the persona leakage issue , the proposed training algorithm has nearly no negative influence on utility .", "entities": [[24, 27, "TaskName", "pretrained language models"], [197, 198, "MethodName", "GPT"], [211, 212, "TaskName", "chatbot"], [388, 389, "TaskName", "chatbot"], [427, 428, "MethodName", "GPT"], [432, 433, "TaskName", "chatbot"], [486, 487, "DatasetName", "MLP"], [512, 513, "TaskName", "chatbot"], [525, 526, "MetricName", "accuracy"], [534, 535, "MetricName", "accuracy"], [614, 615, "MethodName", "GPT"], [727, 728, "MethodName", "GPT"], [743, 744, "MetricName", "loss"], [746, 747, "MetricName", "loss"], [751, 752, "MetricName", "loss"], [754, 755, "MetricName", "loss"], [770, 771, "MethodName", "GPT"], [780, 781, "MetricName", "accuracy"], [814, 816, "TaskName", "inference attack"]]}
{"text": "We assume that there is a GPT - 2 based chatbot f pretrained on private conversations D. Only language modeling is used to train the chatbot : L f ( u ; \u03b8 f ) = \u2212 | u | i=1 log ( Pr ( wi | c , w0 , w1 , ... , wi\u22121 ) ) , ( 1 ) where f refers to the LM - based chatbot with given utterance u = { w 0 , w 1 , ... , w | u | \u22121 } and previous context c. An adversary owns one external annotated dialog dataset D a = { ( U 1 , s 1 ) , ( U 2 , s 2 ) , ... , ( U n , s n ) } with n conversations where U i indicates a list of utterances { u i1 , u i2 , ... , u in i } of i - th conversation and s i corresponds to a list of sensitive personas { s i1 , s i2 , ... , s in i } for corresponding utterance . Each persona s kj is an integer that can be mapped to its persona according to a predefined dictionary and 0 \u2264 s kj \u2264 C \u2212 1 where C is the total number of predefined persona attributes . The goal of the adversary is to infer speakers ' personas s from utterances ' embeddings f ( u ) where u and s refer to any utterance and its persona label .", "entities": [[6, 7, "MethodName", "GPT"], [10, 11, "TaskName", "chatbot"], [25, 26, "TaskName", "chatbot"], [32, 33, "HyperparameterName", "\u03b8"], [70, 71, "TaskName", "chatbot"], [78, 79, "DatasetName", "0"], [209, 210, "DatasetName", "0"]]}
{"text": "The persona inference attack can be viewed as a supervised classification task . For the black - box attack setup , the adversary can only query the target dialog model f with access to embeddings of adversary 's inputs and can not access or modify model parameters \u03b8 f . As shown in the left part of Figure 2 , the adversary tries to build its attacker model A with its external data D a and dialog model f . The persona predictor 's output A ( f ( u ) ) is the estimated probability distribution over C persona attributes . Its loss function L A exploits cross - entropy between the predicted distribution and ground truth distribution that can be formulated as : LA ( u kj , s kj ; \u03b8A ) = CE ( A ( f ( u kj ) ) , s kj ) , ( 2 ) where CE refers to cross - entropy loss between persona label s kj and A ( f ( u kj ) ) . A well - performed persona predictor A can cause great privacy threats . For machine learning as a service ( MLaaS ) , A can be applied to perform a man - in - the - middle attack on the application programming interfaces . Moreover , even if the raw data are protected and the transmission channel is secure , a curious service provider can train its attacker A to collect personas of service users .", "entities": [[2, 4, "TaskName", "inference attack"], [47, 48, "HyperparameterName", "\u03b8"], [103, 104, "MetricName", "loss"], [161, 162, "MetricName", "loss"]]}
{"text": "The LM training objective in Equation 1 only considers the utility of chatbots . In later experiment sections , we show that LM brings severe overlearning issues . Ideally , to achieve an optimal privacypreserving chatbot against persona inference attacks , the probability distribution of the attacker model A should be close to the uniform distribution . That is , the adversary can not improve its inference accuracy from posterior estimation A ( f ( u ) ) and the accuracy is no better than making random guesses on the persona attributes . Moreover , the constraints on privacy should have minor degradation on the utility to maintain the strong generation ability of chatbots . Following the intuition that the adversary can not obtain better results than a random guess , in Section 4.1 , we propose KL loss that aims to flatten the persona predictor 's estimated distribution . Based on minimizing the mutual information between hidden states f ( u ) of chatbots and private persona attributes s , we propose MI loss in Section 4.2 . Lastly , we show the overall training objective in Section 4.3 .", "entities": [[35, 36, "TaskName", "chatbot"], [67, 68, "MetricName", "accuracy"], [80, 81, "MetricName", "accuracy"], [138, 139, "MetricName", "loss"], [174, 175, "MetricName", "loss"]]}
{"text": "KL loss aims to minimize the Kullback - Leibler divergence between A ( f ( u ) ) and the uniform distribution . It flattens the distribution of A ( f ( u ) ) so that the adversary can not gain any useful knowledge after training attacker model A. The KL divergence between the uniform distribution and A ( f ( u ) ) can be formulated as : DKL ( UNI | | A ( f ( u ) ) ) = \u2212 1 C C\u22121 k=0 log ( CPr ( k | f ( u ) , \u03b8A ) ) , ( 3 ) where UNI indicates the uniform distribution and k indicates the k - th persona label of C labels . For optimization , we can leave out constant terms and the logarithm ( Mireshghallah et al , 2021 ) \u20dd and the attacking stage is marked by 2 \u20dd. Both language modeling and defender objectives are jointly trained for the defense to optimize the GPT - 2 model . After GPT - 2 's training stage 1 \u20dd is finished , parameters of GPT - 2 are all frozen and then the attacking stage 2 \u20dd starts . The defender shares the same architecture as the attacker and uses L kl with L mi as defense objectives . the following loss function : LD ( u ; \u03b8A ) = \u2212 1 C C\u22121 k=0 Pr ( k | f ( u ) , \u03b8A ) . However , from the perspective of defenders , they have no access to attacker model A and its parameters . Instead , they can build their own persona predictor as a fake attacker . More specifically , they may mimic the adversary to annotate a dataset D \u2032 a and a persona predictor A p . Then the KL loss becomes : L kl ( u ; \u03b8A p , \u03b8 f ) = \u2212 1 C C\u22121 k=0 Pr ( k | f ( u ) , \u03b8A p ) , ( 5 ) where parameters of the chatbot \u03b8 f and the fake attacker \u03b8 Ap are updated via KL loss . The intuition is to train the chatbot together with a fake attacker to prevent model overlearning by flattening the attacker model 's distribution .", "entities": [[1, 2, "MetricName", "loss"], [170, 171, "MethodName", "GPT"], [176, 177, "MethodName", "GPT"], [189, 190, "MethodName", "GPT"], [226, 227, "MetricName", "loss"], [312, 313, "MetricName", "loss"], [323, 324, "HyperparameterName", "\u03b8"], [352, 353, "TaskName", "chatbot"], [353, 354, "HyperparameterName", "\u03b8"], [359, 360, "HyperparameterName", "\u03b8"], [365, 366, "MetricName", "loss"], [373, 374, "TaskName", "chatbot"]]}
{"text": "The privacy constraint requires that hidden representations should not reveal the persona attributes . In other words , given any utterance u and persona s behind the utterance u , we want to minimize the mutual information between f ( u ) and s : min \u03b8 f I ( f ( u ) ; s ) . ( 6 ) Following the derivation in Song et al ( 2019 ) and , the upper bound can be formulated as : I ( f ( u ) ; s ) \u2264 E q ( f ( u ) ) DKL ( q ( s | f ( u ) ) | | p ( s ) ) , ( 7 ) where p ( s ) can be any distribution for s , q ( x ) refers to probability distribution of model f parameterized by \u03b8 f and f ( u ) is assumed to be sampled from the conditional distribution q ( f ( u ) | x , s ) . However , q ( s | f ( u ) ) is hard to estimate . Instead , we use p \u03a8 ( s | f ( u ) ) to approximate q ( s | f ( u ) ) via minimizing their KL divergence and then we can obtain the following lower bound ( Song et al , 2019 ) : E q ( f ( u ) ) DKL ( q ( s | f ( u ) ) | | p ( s ) ) \u2265 E q ( f ( u ) ) [ log p\u03a8 ( s | f ( u ) ) \u2212 log p ( s ) ] . ( 8 ) Therefore , our objective in Equation 6 can be formulated as an adversarial training objective : min \u03b8 f max \u03a8 E q ( f ( u ) ) [ log p\u03a8 ( s | f ( u ) ) \u2212 log p ( s ) ] . ( 9 ) log p ( s ) is independent of f ( u ) , and we may leave this term out in Equation 9 : min \u03b8 f max \u03a8 E q ( f ( u ) ) [ log p\u03a8 ( s | f ( u ) ) ] . ( 10 ) Then , Equation 10 illustrates an adversarial game between an adversary p \u03a8 who manages to infer s from f ( u ) and a defender who modifies \u03b8 f to protect s from persona inference attack . Adversarial training is widely used to protect sensitive features in natural language processing ( Elazar and Goldberg , 2018 ; Coavoux et al , 2018 ; Li et al , 2018 ) . Using the persona predictor model A p with softmax activation to learn p \u03a8 , we obtain the final objective for the defender : min \u03b8 Ap max \u03b8 f CE ( Ap ( f ( u ) ) , s ) . ( 11 ) We can rewrite Equation 11 into two losses : L mi1 ( u kj , s kj ; \u03b8 Ap ) = CE ( A p ( f ( u kj ) ) , s kj ) and L mi2 ( u kj , s kj ; \u03b8 f ) = \u2212CE ( A p ( f ( u kj ) ) , s kj ) for the fake adversary and the chatbot respectively . Then our MI loss can be formulated as : Lmi = \u03bb0Lmi1 + Lmi2 , ( 12 ) where \u03bb 0 controls the ratio between two the fake attacker A p and the defensed chatbot f .", "entities": [[46, 47, "HyperparameterName", "\u03b8"], [146, 147, "HyperparameterName", "\u03b8"], [311, 312, "HyperparameterName", "\u03b8"], [370, 371, "HyperparameterName", "\u03b8"], [426, 427, "HyperparameterName", "\u03b8"], [433, 435, "TaskName", "inference attack"], [477, 478, "MethodName", "softmax"], [494, 495, "HyperparameterName", "\u03b8"], [497, 498, "HyperparameterName", "\u03b8"], [533, 534, "HyperparameterName", "\u03b8"], [562, 563, "HyperparameterName", "\u03b8"], [587, 588, "TaskName", "chatbot"], [593, 594, "MetricName", "loss"], [610, 611, "DatasetName", "0"], [624, 625, "TaskName", "chatbot"]]}
{"text": "The right part of Figure 2 illustrates how the chatbot is trained to address the black - box attack . The loss function for the defender combines KL loss , MI loss and LM loss . Notice that the fake adversary objective in MI loss violates KL loss which tries to make the distribution of A p flatten . Our proposed loss assigns more weights to the KL loss : L = L f + \u03bb1L kl + \u03bb2Lmi , ( 13 ) where \u03bb 1 and \u03bb 2 are hyper - parameters and \u03bb 1 \u2265 10\u03bb 2 to flatten the distribution of A p . Though the chatbot trained with overall loss L still can not interfere training process of A during black - box attacks , L aims to mitigate persona overlearning issues of f to address such persona inference attacks .", "entities": [[9, 10, "TaskName", "chatbot"], [21, 22, "MetricName", "loss"], [28, 29, "MetricName", "loss"], [31, 32, "MetricName", "loss"], [34, 35, "MetricName", "loss"], [44, 45, "MetricName", "loss"], [47, 48, "MetricName", "loss"], [61, 62, "MetricName", "loss"], [68, 69, "MetricName", "loss"], [109, 110, "TaskName", "chatbot"], [113, 114, "MetricName", "loss"]]}
{"text": "Dataset . To train the GPT - 2 as our chatbot , we use the DialoGPT pretrained on Reddit comment chains . Then we use PersonaChat dataset ( Zhang et al , 2018 ) to fine - tune the GPT - 2 . To obtain annotated dataset D a for the adversary , we align personas to corresponding utterances through positive ( utterance , persona ) pairs provided in Dialogue NLI ( Welleck et al , 2019 ) dataset . For those utterances with no annotations , we assign label \u22121 to them . We reshuffle the dataset to balance the label distribution among train / val / test datasets with the ratio of 8 : 1 : 1 . We first let the attacker and defender share the same training data . In later sections , we will separate the annotated data for the adversary and defender with no overlap . A summary statistics of D a is shown in Table 1 . Attacker model . In our experiment , we use a 2 - layer neural network with cross - entropy loss as the attacker model . The attacker model exploits the final layer embedding of the last token \" < | endof - text | > \" from the GPT - 2 as model input . We also try other attacker model architectures ( transformer block based attackers ) and input embeddings ( average of all embeddings in the final layer of GPT - 2 ) , but the attacking performance is worse than the 2 - layer model mentioned above . Evaluation Metrics . The evaluation metrics are based on privacy and utility . For privacy , we use persona inference accuracy and weighted F1score to evaluate the attacker 's performance . We also use Bayesian Privacy ( BP ) ( Gu et al , 2021a ) to quantify the attacker 's privacy loss for the estimated persona distribution . Top - k accuracy is reported in the Appendix . For utility , we apply BERTScore , Distinct ( Li et al , 2016 ) , BLEU ( Papineni et al , 2002 ) and perplexity ( PPL ) as evaluation metrics . BERTScore and BLEU measure similarity between generated outputs and ground truth while Distinct ( Dist ) focuses on diversity . Perplexity shows the uncertainty when the LM model fits the data .", "entities": [[5, 6, "MethodName", "GPT"], [10, 11, "TaskName", "chatbot"], [18, 19, "DatasetName", "Reddit"], [39, 40, "MethodName", "GPT"], [183, 184, "MetricName", "loss"], [212, 213, "MethodName", "GPT"], [245, 246, "MethodName", "GPT"], [285, 286, "MetricName", "accuracy"], [317, 318, "MetricName", "loss"], [327, 328, "MetricName", "accuracy"], [350, 351, "MetricName", "BLEU"], [359, 360, "MetricName", "perplexity"], [369, 370, "MetricName", "BLEU"], [387, 388, "MetricName", "Perplexity"]]}
{"text": "Attacks without Defense . We list the attacking performance of A in multiple scenarios shown in Acc refers to test persona inference accuracy . F1 uses weighted average F1 - score . Max - Ratio indicates the ratio that the most frequent prediction shares among all predictions . The worse the attack model performs , the better privacy protection can be achieved . distribution , then it can randomly guess over 4 , 332 labels ( Random Pred ) . Otherwise the adversary can perform Best Guess that only guesses the most frequent persona in the dataset . LM indicates the attacker performance that only language modeling objective is applied to train the chatbot without any defense mechanism . From the table , the test persona inference accuracy on the LM achieves 37.59 % while guessing on the label with most occurrences merely has 0.72 % accuracy . That is , the black - box persona inference attack has 52\u00d7 the accuracy of guessing . The huge performance gap between the attacker model and the baseline guess method indicates that simple language modeling objective has serious overlearning issues that unintentionally capture private personas of speakers . Attacks on the Defensed LM . To avoid the persona overlearning issue , we use additional defense objectives illustrated in Section 4 . LM+KL+MI utilizes language modeling , KL loss and MI loss in Equation 13 to train the GPT - 2 . As demonstrated in Table 2 , the attacker performance on LM+KL+MI significantly reduces the attacking accuracy from 37.59 % to 0.53 % and F1 - score drops from 0.37 to nearly 0 . This defense mechanism can even outperform Best Guess in terms of privacy protection . That is , even if the adversary annotates its own dataset to train an attacker model , the attacking performance is still worse than simply guessing the most frequent label . As a result , the black - box persona prediction attack becomes useless after applying the defenses for the chatbot . The adversary can not obtain any speaker 's persona from the embedding f ( u ) by training A. To learn why the proposed defenses work so well , we further examine the ratio of the most frequent predicted label ( Max - Ratio ) among all pre - dictions . The accuracy of Best Guess reveals that the most frequent label in the test set has a ratio of 0.72 % . After applying KL loss and MI loss , the attacker model tends to make predictions on a single label . For LM+KL+MI , the Max - Ratio even occupies 81.87 % predictions . This implies that the proposed defense strategies may have the potential to fool the attacker model to make wrong predictions on a single slot . We will further investigate this implication in later sections . Overall , the above experiment demonstrates that our proposed defense learning strategies can effectively mitigate the persona overlearning issue and avoid black - box persona inference attacks .", "entities": [[16, 17, "MetricName", "Acc"], [22, 23, "MetricName", "accuracy"], [24, 25, "MetricName", "F1"], [28, 31, "MetricName", "F1 - score"], [113, 114, "TaskName", "chatbot"], [127, 128, "MetricName", "accuracy"], [146, 147, "MetricName", "accuracy"], [156, 158, "TaskName", "inference attack"], [161, 162, "MetricName", "accuracy"], [225, 226, "MetricName", "loss"], [228, 229, "MetricName", "loss"], [235, 236, "MethodName", "GPT"], [254, 255, "MetricName", "accuracy"], [262, 265, "MetricName", "F1 - score"], [270, 271, "DatasetName", "0"], [336, 337, "TaskName", "chatbot"], [390, 391, "MetricName", "accuracy"], [414, 415, "MetricName", "loss"], [417, 418, "MetricName", "loss"]]}
{"text": "To show the effectiveness of proposed KL loss and MI loss and how they affect the performance of black - box persona inference attacks , we consider the inclusion and exclusion of proposed defense objectives . The result is shown in Table 2 . LM+KL indicates the GPT - 2 is trained with language modeling and KL loss . LM+MI applies language modeling and MI loss . From the table , it can be seen that LM+KL , LM+MI and LM+KL+MI are all able to reduce the test accuracy of the attacks . The KL loss is weaker from the perspective of defense , but it tends to flatten the estimated persona distribution with much smaller Max - Ratio . The LM+MI shares similar test accuracy and F1 - score with LM+KL+MI , but nearly all predictions are made on a single persona label with a ratio of 99.84 % . This suggests that MI loss causes the attacker model to predict all labels on a single persona attribute . After KL loss is applied on LM+KL+MI , the Max - Ratio drops to 81.87 % . As discussed earlier , high Max - Ratio may also cause privacy leakage . Suppose the adversary knows the persona with Max - Ratio , then it can improve its guess by not predicting this persona , which is a threat for fewer persona labels ( for example , binary classification ) . These results verify that KL loss introduces flatter estimation and MI loss is more effective against persona overlearning , which conforms to our intuition of their objectives in Section 4 .", "entities": [[7, 8, "MetricName", "loss"], [10, 11, "MetricName", "loss"], [47, 48, "MethodName", "GPT"], [57, 58, "MetricName", "loss"], [65, 66, "MetricName", "loss"], [88, 89, "MetricName", "accuracy"], [95, 96, "MetricName", "loss"], [125, 126, "MetricName", "accuracy"], [127, 130, "MetricName", "F1 - score"], [155, 156, "MetricName", "loss"], [172, 173, "MetricName", "loss"], [245, 246, "MetricName", "loss"], [251, 252, "MetricName", "loss"]]}
{"text": "Besides privacy , utility is another key objective to train a chatbot . Several automatic metrics are considered to evaluate the generation performance . For generation , we use Table 4 : Evaluation on the privacy for 8 clusters . Unseen shows the results only for the first 3 persona labels that defender has never seen . Overall refers to the results on all 8 labels . Acc and Max - Ratio are measured in % . BP u corresponds to Bayesian Privacy loss on the uniform distribution . Still , the worse the attack model performs , the better privacy protection can be achieved . the second speaker ( Human B in Figure 1 ) with all previous turns as context . Then we compared the generated model outputs with ground truth replies . We use Dist - 1 and Dist - 2 to count ratios of distinct unigrams and bigrams . BLEU - 1 , BLEU - 2 and BLEU - 4 are applied to evaluate generation similarity with ground truth . Due to the one - to - many nature of chit - chats , the BLEU is not adequate to compare generated responses with ground truth . Hence , we adapt Precision , Recall and Precision of BERTScore to measure the similarity in the embedding space . The evaluation result is shown in Table 3 , where same models from Table 2 are evaluated . The result indicates that adding KL loss will increase the perplexity greatly from 14.8 to 28.9 . After combining KL loss with MI loss , its perplexity decreases to 19.674 . A plausible explanation is that KL loss confuses the persona predictor and indirectly increases the uncertainty of the GPT - 2 . All GPT - 2 models have relatively low BLEU scores due to the one - to - many mapping between contexts and responses . For Distinct and BERTScore , there are only minor differences between LM and defensed LMs . Though the uncertainty increases after applying KL loss and MI loss , it does no harm to the quality of generation . In summary , there is almost no negative influence on the utility after applying the proposed defense strategies .", "entities": [[11, 12, "TaskName", "chatbot"], [67, 68, "MetricName", "Acc"], [83, 84, "MetricName", "loss"], [153, 154, "MetricName", "BLEU"], [157, 158, "MetricName", "BLEU"], [161, 162, "MetricName", "BLEU"], [189, 190, "MetricName", "BLEU"], [205, 206, "MetricName", "Precision"], [207, 208, "MetricName", "Recall"], [209, 210, "MetricName", "Precision"], [245, 246, "MetricName", "loss"], [249, 250, "MetricName", "perplexity"], [259, 260, "MetricName", "loss"], [262, 263, "MetricName", "loss"], [265, 266, "MetricName", "perplexity"], [276, 277, "MetricName", "loss"], [288, 289, "MethodName", "GPT"], [293, 294, "MethodName", "GPT"], [300, 301, "MetricName", "BLEU"], [339, 340, "MetricName", "loss"], [342, 343, "MetricName", "loss"]]}
{"text": "Attacks on Imbalanced Data Distribution . Previous black - box attacks usually assume that the annotated dataset D a must share similar data distri - bution with the defender 's training data . To examine the performance of defense strategies on unseen personas , we assign the adversary 's dataset D a with labels that the defender can not acquire . We split data with 500 persona labels that are uniquely held by the adversary . The defender owns 8 , 031 conversations with persona labels ranging from 500 to 4 , 331 while the adversary holds 2 , 376 dialogues with persona labels ranging from 0 to 4 , 331 . For testing , 500 conversations with persona labels ranging from 0 to 4 , 331 are used . Under imbalanced data distribution , the attack on the defensed LM has Acc 0.47 % , F1 1.90e - 3 and Max - Ratio 94.06 % . The persona inference accuracy is still very low and the attacker model tends to predict more on a single persona label than the balanced data distribution setup . This result shows that the proposed overall loss can also prevent black - box persona inference attacks on unseen personas . It also verifies previous suggestions that combining LM loss with MI loss may fool the attacker model to make wrong predictions . Attacks on Fewer Persona Labels . The above experiments are based on 4 , 332 persona labels . In fact , many personas share similar meanings and can be further clustered . Besides , to better evaluate privacy loss for the estimated distribution , a smaller label space is preferred . Therefore , it is necessary to consider defense performance on a smaller label space . We use Sentence - BERT ( Reimers and Gurevych , 2020 ) to embed all persona sentences and perform k - means clustering on the embeddings to obtain 8 clusters . We manually checked these clusters and classified them as cars , food , animals For both conversations , the \" context \" is fixed and used as the first four utterances . Then the bot and the user start interactive conversations with the \" context \" . Since there is no gold standard , the results are annotated by the authors . ( pets ) , family information , hobbies , jobs , personal information and music tastes respectively . To evaluate how the clustering performs , we randomly sample 100 utterances with clustered labels and invite two volunteers to inspect those samples . Both of them agree on 90 % of the clustered annotations . After manual inspection of the remaining 10 % annotations , the clustering error rate is 8 % . Following previous imbalanced data split , we assign data in the first 3 clusters only to the adversary to make the data distribution imbalanced . Here , the defender owns 6 , 654 conversations with persona labels ranging from 3 to 7 while the adversary holds 3 , 753 dialogues with persona labels ranging from 0 to 7 . For testing , 500 conversations with persona labels ranging from 0 to 7 are used . The attacking performance for both unseen labels and all labels is displayed in Table 4 . BP u measures the KL divergence D KL ( F 0 | | A ( f ( u ) ) ) where F 0 refers to uniform distribution . For imbalanced data distribution with a small label space , our proposed defenses can still achieve much lower attack accuracy than LM on both Unseen and Overall . However , for Overall , LM+KL+MI has higher accuracy with a lower F1 - score compared with two baselines . This indicates that proposed defenses fail to protect privacy as we desired in the baselines . For BP u , LM+KL+MI are around 10 times smaller than LM . It means that after applying defense objectives , the attacker 's estimated distribution is much closer to the uniform distribution . Thus the effectiveness of the KL loss is verified . In addition , Max - Ratio with 8 clusters on Unseen is smaller than 4 , 332 labels even though the distribution of 8 clusters is obviously tighter . Still , the Max - Ratio of 58.15 % accounts for a much larger fraction than other predictions . In summary , the above results imply that for the smaller label space , our proposed defense objectives are still effective even on unseen persona labels .", "entities": [[106, 107, "DatasetName", "0"], [122, 123, "DatasetName", "0"], [142, 143, "MetricName", "Acc"], [146, 147, "MetricName", "F1"], [160, 161, "MetricName", "accuracy"], [192, 193, "MetricName", "loss"], [214, 215, "MetricName", "loss"], [217, 218, "MetricName", "loss"], [266, 267, "MetricName", "loss"], [298, 299, "MethodName", "BERT"], [313, 317, "MethodName", "k - means clustering"], [514, 515, "DatasetName", "0"], [528, 529, "DatasetName", "0"], [560, 561, "DatasetName", "0"], [573, 574, "DatasetName", "0"], [598, 599, "MetricName", "accuracy"], [615, 616, "MetricName", "accuracy"], [619, 622, "MetricName", "F1 - score"], [683, 684, "MetricName", "loss"]]}
{"text": "Previous experiments mainly consider accuracy as the evaluation metric . In this section , we use top - k accuracy for the black - box persona inference attacks to measure privacy protection . As shown in Table 5 , our defense is much more robust than LM when k \u2264 50 . When k is larger than 500 , the defense degrades rapidly as k increases . This result implies that the ground truth personas mostly lie in the top 2 , 000 predictions even if the defense is applied . For a smaller k , our proposed defense learning strategies are still effective .", "entities": [[4, 5, "MetricName", "accuracy"], [19, 20, "MetricName", "accuracy"]]}
{"text": "For each conversation , the utterances are concatenated by the special token \" < | endoftext | > \" to train the GPT - 2 . To decode outputs from GPT - 2 , we apply the Nucleus Sampling ( Holtzman et al , 2020 ) method . We set top - p = 0.9 with a temperature coefficient 0.9 to sample words from the GPT - 2 . For optimization , we set 2 AdamW optimizers ( Loshchilov and Hutter , 2019 ) for the chatbot and the persona predictor respectively . The learning rate is 3e - 5 with linear warm - up and decay . For hyper - parameters , we set \u03bb 0 = 1 , \u03bb 1 = 10 and \u03bb 2 = 1 .", "entities": [[22, 23, "MethodName", "GPT"], [30, 31, "MethodName", "GPT"], [65, 66, "MethodName", "GPT"], [75, 76, "MethodName", "AdamW"], [86, 87, "TaskName", "chatbot"], [94, 96, "HyperparameterName", "learning rate"], [116, 117, "DatasetName", "0"]]}
{"text": "To make predictions on personas , the arg max function is used for the estimated distribution of persona predictors . However , the internal distribution conveys crucial information about how the persona predictors estimate f ( u ) . We follow the setup of imbalanced data split of 8 clusters in Section 5.5 to examine persona predictors of attacker A and fake attacker A p . Figure 4 shows the data distribution of the test set and average distribution after softmax activation over the 8 labels for attacker A and defender A p . For attacker A , we consider the attack on LM and LM+KL+MI . The defender A p tends to have a large difference with Data and tries to flatten its distribution among its own training set ( the last 5 labels ) . This behavior conforms to the KL loss 's objective that aims to flatten the distribution and deviate from the ground truth distribution . For attacker A , distributions of both LM and LM+KL+MI seem close to the ground truth distribution . This indicates that the attacker model A can still learn statistical in -", "entities": [[80, 81, "MethodName", "softmax"], [143, 144, "MetricName", "loss"]]}
{"text": "Since NMT systems have achieved the highest translation quality in recent evaluation contests , the Marian - NMT package ( Junczys - Dowmunt et al , 2018 ) is adopted for experimentation here . Marian - NMT development was funded by the European Commission to consolidate NMT research and incorporates the most recent advances in NMT . Its code is optimized to reduce the CPU / GPU time required to complete the simulations of NMT systems . For creating NMT systems , three of the models provided by Marian - NMT were chosen , termed as the \" transformer \" , \" amun \" and \" s2s \" models . The \" transformer \" model has been based on the work of Vaswani et al ( 2017 ) and uses a simple structure incorporating attention mechanisms and dispensing with recurrence to implement a fast NMT system . The other two models are more conventional , using a recurrent neural network to implement the translation . The \" amun \" model follows the approach of Bahdanau et al ( 2016 ) , employing a recurrent neural network but allowing the model to automatically search for wider ranges of the source language ( SL ) to connect with the target language side ( TL ) words . Finally , \" s2s \" implements a recurrent neural network - based encoder - decoder model with attention mechanism , using the architecture proposed in ( Sennrich et al , 2017 ) . Hereafter , the three models are identified via the names used within Marian - NMT , which are also used in evaluations ( cf . Bojar et al , 2018 ) . The main configuration parameters used for each model are depicted in Table 1 , to enable replication of experiments . For each model , different optimization options from Marian - NMT during the validation phase are used to create three NMT variants of each model , namely optimizing with ( i ) BLEU , ( ii ) entropy and ( iii ) word - wise normalized crossentropy ( denoted as \" ce - mean \" and representing the default optimization for Marian - NMT ) . Regarding the main NMT parameters , all recurrent networks comprise 1 , 024 units in the hidden layer , an encoder depth of 6 layers and an embedding size of 512 . All cells used both in the encoder and decoder side are gated recurrent units ( GRU ) . The transformer dimension is set to 2 , 048 . To reduce the lexicon size , a total of 85 , 000 merge operations are allowed using the BPE ( Byte Pair Encoding ) method proposed in ( Sennrich et al , 2016 ) , this being the default setting for marian - nmt applications . Initially , the three Marian - NMT models are trained to provide the base NMT systems . Typically , for a single - GPU system ( equipped with an NVIDIA Titan XP GTX1080 GPU card driven by an Intel i - 9700 K CPU ) , 24 hours are required for training the transformer , 130 hours for amun and 308 hours for s2s . This is equivalent to a ratio of 1:5:12 to train the respective systems .", "entities": [[332, 333, "MetricName", "BLEU"], [413, 414, "MethodName", "GRU"], [444, 445, "MethodName", "BPE"], [446, 449, "MethodName", "Byte Pair Encoding"], [502, 503, "DatasetName", "Titan"]]}
{"text": "The experiments aim to improve the translation accuracy of an NMT system , taking into account limited training data and constrained computing resources . In order to investigate translation into a lesser - used and highly inflectional language , we Common to all 3 models layer - normalization yes exponential smoothing yes beam - size 6 normalize 0.6 early - stopping 5 Transformer - specific transformer - dropout 0.1 transformer - dropout - attention 0.1 transformer - dropout - ffn 0.1 Amun and s2s - specific dropout - rnn 0.2 dropout - src 0.1 dropout - trg 0.1 have chosen the English - to - Greek language pair . When selecting the training corpora , it has been decided to refrain from using expensive language resources such as specialized or hand - built parallel corpora . Instead , only standard publicly available parallel corpora have been adopted , namely the Europarl and DGT - Acquis corpora 2 , as listed in Table 2 . The largest part of the Europarl corpus and the entire DGT - Acquis corpus are used to train the NMT system . Three small portions of the Europarl corpus have been reserved for test and validation purposes . More specifically , two independent sets of approx . 3 , 000 Europarl sentences each are excluded , to ensure that the NMT evaluation is unbiased . In the present experiment , one of these sets is used for in - training validation . The other set is reserved to allow additional cross - evaluation of experiments in the future , without invalidating the previously trained models . Finally , a sample 2 The Europarl corpus ( ver.7 ) was retrieved from https://www.statmt.org/europarl . The DGT - Acquis corpus was retrieved from https://ec.europa.eu/jrc/en/languagetechnologies/dgt - translation - memory of 1 , 000 sentences from Europarl ( Testset2 ) is retained to provide an unseen in - domain test set . Another independent test set was drawn from the PRESEMT project resources , comprising 200 sentences which have not been used to either train an MT model or create any resources used herewith ( denoted as Testset1 ) . A preliminary analysis of the NMT outputs has shown that translations are commendably fluent , though errors are evident . A sample of amun translations is shown in Figure 1 . In sentence # 1 , the term \" \u0391\u03bc\u03b5\u03c1\u03b9\u03ba\u03b1\u03bd\u03bf\u03af \" ( Transl . \" Americans \" ) is erroneously used as a translation of the terms \" American \" , \" European \" , and \" Japanese monopolies \" . Similarly , in sentence # 2 , the phrase \" \u03b7 \u03ba\u03b1\u03c4\u03b1\u03c0\u03bf\u03bb\u03ad\u03bc\u03b7\u03c3\u03b7 \u03c4\u03b7\u03c2 \u03c6\u03c4\u03ce\u03c7\u03b5\u03b9\u03b1\u03c2 \" ( meaning \" the reduction of poverty \" ) is used to translate semantically diverse phrases , including \" genetically modified organisms \" , and \" the negative social effects of unbridled , unregulated globalization \" . Repetition is a widely reported weakness of NMT systems , most frequently attributed to insufficient training data . An additional problem concerns the translation of rare words ( i.e. words with low frequency in the corpus ) , due to the limited vocabulary that NMT systems can directly handle . This is especially severe when translating towards languages with complex morphology , which increases the effective vocabulary size . For example the word \" ostensibly \" is translated into Greek as \" ostenfigher \" ( ungrammatical ) . Similarly the word \" room \" is translated as \" \u03b4\u03c9\u03bc\u03b1\u03c4\u03b5\u03af\u03bf \" instead of the correct \" \u03b4\u03c9\u03bc\u03ac\u03c4\u03b9\u03bf \" ( meaning room ) , whilst the word \" indistinct \" is translated as \" \u03ac\u03c7\u03c9\u03c1\u03bf\u03c2 \" which is not a valid Greek word . \u0391nother issue is that entire phrases present in the source text may be omitted in the translation . For instance the sentence \" Businesses have undertaken the education \" is translated by a transformer NMT as \" H \u03b5\u03ba\u03c0\u03b1\u03af\u03b4\u03b5\u03c5\u03c3\u03b7 \u03ad\u03c7\u03b5\u03b9 \u03b1\u03bd\u03b1\u03bb\u03ac\u03b2\u03b5\u03b9 , [ meaning \" education has undertaken \" ] . Hence , the subject \" business \" has been deleted .", "entities": [[7, 8, "MetricName", "accuracy"], [62, 63, "MethodName", "Transformer"]]}
{"text": "To improve translation accuracy , the main errors need to be identified in an automated manner . The idea is that a poor alignment between source text and translation indicates substantial loss of meaning during translation . On the contrary a high alignment score is indicative of a high likelihood that Figure 1 : Example translations generated by amun , with repetitions of texts highlighted in grey the NMT output is an accurate translation . To this end a module will be added to implement alignment verification ( AVM ) , by determining the match between the input sentence and its translation . The establishment of representative alignment scores allows in turn the combination of multiple NMT models , using AVM to evaluate the accuracy of each candidate translation and thus select the best translation on a sentence - by - sentence basis . For this research , MT software and tools released via open - source code have been surveyed and the Phrase Aligner Module ( PAM , cf . Troullinos , 2013 ) has been selected . The architecture of the proposal hybrid NMT is depicted in Figure 2 .", "entities": [[3, 4, "MetricName", "accuracy"], [31, 32, "MetricName", "loss"], [124, 125, "MetricName", "accuracy"]]}
{"text": "PAM was developed as part of the PRESEMT hybrid MT methodology ( Tambouratzis et al , 2017 ( Tambouratzis et al , 2017 ) . PRESEMT was designed to create MT systems requiring only very limited amounts of specialized , expensive linguistic resources . Frequently , the most expensive resource is the parallel corpus of SL - TL sentences . PRESEMT uses parallel corpora of only a few hundred sentences , augmented by very extensive but comparatively inexpensive monolingual corpora . Within the PRESEMT methodology , the small parallel corpus serves to establish the transformation from the SL structure to the TL one , using the Phrase Aligner module . This module , handling sentence pairs from this parallel corpus , identifies the correspondence of words and phrases from SL to TL , to determine the translation accuracy .", "entities": [[137, 138, "MetricName", "accuracy"]]}
{"text": "In the current application , PAM determines the suitability of each candidate translation , based on its match with the source sentence . Thus , the assumption made is that the input sentence and the candidate translation represent the corresponding SL and TL entries of a parallel corpus and PAM determines their level of parallelism . As the requirement is to grade various translations , the PAM operation is reversed , to identify the quality of match between the input sentence and the generated translations . When PAM was used in PRESEMT , sentence pairs from the parallel corpus with a very low percentage of successful alignments were discarded without measuring their degree of parallelism , as poor exemplars of the structural transformations from SL to TL . Here , PAM is modified so that for all pairs of input sentence and NMT - translation the word alignments and assignments of words to phrases are calculated . This allows the re - roled PAM to grade any source / translation pair , no matter how poor the match of the two sentences is . Two metrics have been established to calculate divergence between the SL sentence and its NMTderived translations . The first metric ( Uscore ) calculates the number of unaligned words of the source sentence , after PAM is applied . The aim is to have as few unaligned words as possible , so the lower Uscore is , the better the translation is . U score = # unaligned words ( 1 ) The second metric ( Wscore ) is a weighted combination of several indicators of alignment between source sentence and candidate translation . This summarizes in one measurement the type of alignments and the stage at which they were achieved . Hence , for a sentence with K words , Wscore is defined as : W score = K i=1 ( w i * align stage i ) ( 2 ) In equation ( 2 ) , align - stage i denotes the stage ( cf . section 4.3 for the different stages ) at which the i - th SL word is aligned successfully to a TL word , and w i denotes the relevant weight for this stage . In the case of the weighted metric Wscore , the higher the score , the more accurate the corresponding translation is . The actual weight values must reward the establishment of alignments at an earlier rather than a later stage . Thus , w i should be larger than w j , for i smaller than j. For the purposes of the present article , w i is set to integer values of 5 , 2 and 1 for the first , second and third stage respectively ( other sets of weight values that follow this reasoning produce similar results to those reported here ) . The code of PAM has been modified to integrate Wscore and Uscore calculation , though the actual alignment ( Schmid , 1994 ) is used , with a reported tagging accuracy exceeding 96 % .", "entities": [[512, 513, "MetricName", "accuracy"]]}
{"text": "To determine the quality of the NMT - based translations ( amun - , s2s - and transformer - based models ) , two widely used MT evaluation metrics are utilised , namely BLEU ( Papineni et al , 2002 ) and NIST ( Doddington , 2002 ) . To calculate both these metrics , the mt - eval package ( version 13a ) is used . For PAM , the PRESEMT bilingual lexicon from Greek to English is used , which contains approx . 8 , 000 lemmas and 40 , 000 Greek - English token pairs . This lexicon is from the same domain as testset1 and is thus out - of - domain for testset2 , providing a more limited coverage for this testset . Two different types of experiments are possible , depending on whether the ensemble comprises multiple NMT architectures , or only one type of architecture . The first experiment reported here involves NMT ensembles that all share the same architecture , but are optimized with different criteria . The second type of experiment studies ensembles which consist of systems with different architectures , to investigate if their combination results in a better translation quality .", "entities": [[33, 34, "MetricName", "BLEU"]]}
{"text": "The results obtained for testset1 of the Englishto - Greek translation pair are depicted in Table 3 , when running the single transformer , amun and s2s models respectively , as well as their ensembles . The corresponding results for testset2 are depicted in Table 4 . In Tables 3 and 4 , the first 3 rows correspond to single NMT models generated when Marian - NMT is trained to optimise ( i ) BLEU , ( ii ) entropy and ( iii ) the word - wise normalised cross - entropy ( this is denoted as \" ce - mean \" ) . The final three rows of Tables 3 and 4 report the accuracy of translations obtained by NMT ensembles . Marian - NMT implements a standard ensemble method , which allows the user to combine different models provided they use the same lexicon . The user may specify weighting factors to boost selection of the models deemed to be better . For this article , this ensemble combines the three aforementioned NMT models ( i ) , ( ii ) and ( iii ) , with equal weights for all NMTs . The last two rows report the accuracy of ensembles using PAM with ( i ) Uscore and ( ii ) Wscore , respectively . A key difference of the Marian - NMT ensemble is that it is able to recombine partial results of the translation process from each NMT model and thus may generate a new translation that is different from all the translations of single - NMT systems . On the contrary , Uscore and Wscore grade the translations generated by single - NMT models in the ensemble , and then select the highest - scoring translation to be the translation produced by the PAM - based ensemble . To evaluate the quality of translations produced by the PAM - based ensembles , two baselines are selected . The first baseline is the \" ce - mean \" option of the Marian - NMT translation system . The second , and stronger , baseline is the Marian - NMT ensemble ( referred to as \" Marian - ensemble \" hereafter ) . Entries that exceed the first baseline are depicted in bold . Entries with scores that exceed the stronger Marian - ensemble baseline are annotated with an asterisk . Based on Table 3 , for testset1 the best BLEU scores are achieved by the Marian - NMT ensemble in comparison to single - NMT models . The PAM - Wscore ensemble gives a higher accuracy than the Marian - NMT ensemble , whilst the accuracy of PAM - Uscore is lower than PAM - Wscore . On the whole , it is Marian - ensemble and PAM - Wscore that generate the best NIST and BLEU scores . A broadly similar situation is found when using testset2 ( Table 4 ) . Here , the improvement conferred by the ensemble methods over the three base models is much more marked . For instance , for BLEU , the score is only 19.0 to 20.0 for single NMT models , but rises to more than 28.0 for the ensembles , which equates to more than eight BLEU percentage points of improvement .", "entities": [[74, 75, "MetricName", "BLEU"], [115, 116, "MetricName", "accuracy"], [201, 202, "MetricName", "accuracy"], [405, 406, "MetricName", "BLEU"], [431, 432, "MetricName", "accuracy"], [441, 442, "MetricName", "accuracy"], [472, 473, "MetricName", "BLEU"], [512, 513, "MetricName", "BLEU"], [542, 543, "MetricName", "BLEU"]]}
{"text": "One question is whether the improvements conferred by the ensembles are statistically significant . To that end , the BLEU and NIST scores of all the independent sentences are assembled , forming two populations of scores ( one for BLEU and one for NIST ) for each experimental run . Then the Wilcoxon and sign tests are used to determine if these populations have significant differences . For testset1 , the scores of the single NMT systems and the NMT - ensembles are relatively close , differing by less than 2 BLEU points . Applying the sign and Wilcoxon tests , Marian - ensemble produces statistically better NIST scores ( at a 0.05 level ) than the default Marian - NMT output for amun and s2s models , but not for the transformer model . For the transformer and s2s models , the scores generated by PAM - Wscore are significantly better that those of single - model Marian - NMT , according to both the Wilcoxon and sign tests ( at a 0.05 level ) . Similarly , PAM - Uscore gives statistically superior results to Marian - NMT ( ce - mean optimization ) for the s2s model ( at a significance level of 0.05 ) . Comparing the ensembles to each other , Wscore consistently produces higher scores than Uscore . This superiority is statistically significant at a 0.05 level according to both Wilcoxon and sign tests , for the transformer and the amun models . PAM - Wscore achieves consistently higher translation scores than Marian - ensemble for both BLEU and NIST . According to the Wilcoxon test , these differences are statistically significant , at a 0.05 level , only for the s2s ( BLEU score ) and the transformer model ( both BLEU and NIST scores ) . Turning to testset2 , the results are more clearly separated . All three ensembles ( i.e. PAM - Wscore , PAM - Uscore and Marian - ensemble ) have statistically superior scores to Marian ( optimised with ce - mean ) for both BLEU and NIST , at a significance level of 0.01 . This extends to all three NMT models ( amun , transformer and s2s ) , and indicates that both Marian - ensemble and the two PAM - based ensembles give substantially higher scores than single Marian - NMT models . On the other hand , when comparing PAM - Wscore to PAM - Uscore for testset2 , no statistically significant difference ( at a 0.05 level of signifi - cance ) between the two systems is discerned by either the Wilcoxon or sign test . Similarly , no statistically significant differences at a 0.05 level are found between the PAM - based ensembles and the Marian - ensemble and only small differences at a 0.10 level . Thus , even though PAM - based ensembles achieve scores higher than Marian - ensemble , differences are not significant .", "entities": [[19, 20, "MetricName", "BLEU"], [39, 40, "MetricName", "BLEU"], [91, 92, "MetricName", "BLEU"], [263, 264, "MetricName", "BLEU"], [289, 291, "MetricName", "BLEU score"], [298, 299, "MetricName", "BLEU"], [347, 348, "MetricName", "BLEU"]]}
{"text": "To quantize the improvements achieved by the proposed PAM - Wscore approach , in this section the computational requirements posed by each NMT system are also considered . To this end , the most accurate NMT system is defined for each dataset and metric combination . Two baselines are chosen , namely the most accurate NMT model and the most accurate Marian - ensemble . We focus on the transformer model , which is the least expensive model to train . For each ensemble using transformers , the aim is to determine how close to the Marian - ensemble baseline this is . Results are shown in Table 5 , where the accuracy of each transformer NMT is expressed as a fraction of the Marian - ensemble score . The best single transformer model achieves for testset1 88.7 % of the baseline BLEU score and 93.1 % of the NIST score . Using the Wscore ensembling method , this rises to 90.7 % for BLEU and 95.2 % for NIST , showing a gain of 2 % . Turning to dataset2 , the single transformer scores just 70.5 % in comparison to the baseline BLEU score and 73.4 % of the NIST score ( therefore it is 27 % to 30 % lower ) . The Wscore ensemble improves relative scores , reaching 92.7 % and 94.5 % of the baseline scores for BLEU and NIST respectively . This equates to an increase of ca . 22 % in both scores , making the final result directly comparable to s2s , though GPU training requirements are reduced by a factor of 12 .", "entities": [[112, 113, "MetricName", "accuracy"], [142, 144, "MetricName", "BLEU score"], [164, 165, "MetricName", "BLEU"], [194, 196, "MetricName", "BLEU score"], [233, 234, "MetricName", "BLEU"]]}
{"text": "A second type of evaluation moves away from metrics to focus on analysing the translation errors by different models , with subjective methods . For instance , when transformer NMT models are tasked to translate testset1 , the BLEU - optimised NMT generates 26 ungrammatical words , the entropyoptimised NMT generates 24 ungrammatical words and the cross - entropy optimised model produces 23 ungrammatical words . The Wscore - ensemble reduces the ungrammatical words to 21 , improving translation . The ungrammatical words were determined in all cases by visual inspection of the body of translations complemented by spell - checking tools to aid detection . Further inspection of translation quality has involved comparing the Marian - ensemble and Wscore - ensemble outputs . The length ( in words ) of translations per test sentence is found to differ substantially between the two ensembles , with the difference being more than 1/10 for 9 % of sentences , more than 1/4 for 2.5 % of sentences and more than 1/2 for 1 % of sentences ( close to identical results are obtained for testset1 and testset2 ) . As such deviations are unexpectedly large , an analysis was performed , with typical examples being shown in Figure 3 . As can be seen , PAM assists the Wscore - ensemble in retaining all phrases of the sentence . On the contrary , Marian - ensemble fails to ensure this , and frequently discards portions of the input sentence . In one case ( sentence # 774 ) Marianensemble results in a null - length translation , and in another ( sentence # 648 ) the final translation covers less than 10 % of the input text , radically distorting meaning . Both PAM - ensembles are unaffected by such phenomena .", "entities": [[38, 39, "MetricName", "BLEU"]]}
{"text": "This article has studied the creation of translation systems towards highly inflectional languages , when the amount of in - domain training data is limited . Emphasis has been placed on improving the translation accuracy of NMT models that can be trained more rapidly and cost - effectively ( in terms of CPU processing power ) and rendering this performance comparable to that of more complex models . The Marian - NMT package has been chosen as the starting point to create NMT models for the English to Greek language pair . Using only publicly available text corpora , the NMT models produce commendably fluent translations . Identified errors in the NMT translations are typical of a lack of training data . A hybrid methodology has been proposed that samples an ensemble of NMT models to select the final translation , chosen by a module calculating the alignment level between the input sentence and each translation . This module was developed for resource - poor MT systems . The proposed hybrid approach has resulted in higher BLEU and NIST scores , compared to those of single NMT models . Improvements are in many cases statistically significant even over the ensemble system provided within the Marian - NMT package , indicating the promising nature of the hybrid approach . Also , the translation process is found to be more robust , giving more consistent translations in comparison to the Marian - NMT ensemble system , which occasionally omits large portions of the input text from the translation . One of the advantages of the proposed method is that it is general - purpose and does not rely on the use of ensembles of Neural MT systems with a specific architecture . Instead , it can be used to combine the results of different types of Neural MT systems , or MT systems that belong to different paradigms , or even to combine human translations . In addition the proposed method can be used to clean up a corpus of parallel sentences or several such corpora , by removing sentence pairs for which the source and target - language texts do not have a high degree of parallelism . Similarly , the proposed method may be used to filter a corpus consisting of original text and its MT - derived translation , to produce a parallel corpus for training of other MT systems , fulfilling a role similar to that proposed by ( Rikters and Fishel , 2017 ) . One point for future research is how effective a filtering system based on PAM would be , in comparison to already proposed systems . Future work involves some relatively simple activities that can be imminently implemented , such as releasing the modified version of PAM for experimentation by interested parties . Another short term activity involves using the proposed method with sacreBLEU instead of the BLEU and NIST metrics provided by mt - eval . Future experiments will investigate the effectiveness of this hybrid approach for other language pairs . One area of interest would be to determine the effectiveness of the PAM - based method when very limited dictionaries are available as well as the limitations when the accuracy of the parser used is relatively low . All these represent issues for the future . It is also planned to study the approach using systematic optimisation of the PAM parameters , to identify in more detail configurations that produce more accurate translations . Another possibility is to use PAM to detect sub - sentential parts of the translated sentences with particularly poor alignments between input and translation and seek better translations of only these specific parts . Another direction is to investigate more extensively cases where the translation is not sufficiently close to the input sentence . Then , comparisons to other low - scored translations are more difficult and result in a reduced level of confidence of the chosen translation . Such a line of study will evaluate more thoroughly the robustness of the proposed method .", "entities": [[34, 35, "MetricName", "accuracy"], [176, 177, "MetricName", "BLEU"], [479, 480, "MetricName", "sacreBLEU"], [483, 484, "MetricName", "BLEU"], [537, 538, "MetricName", "accuracy"]]}
{"text": "To ensure translation quality , we hired two professional translators with at least seven years of experience who specialize in academic papers / books as well as business contracts . The two translators each post - edited half of the dataset and cross - checked each other 's translation afterward . This was further examined by one of the authors , who is fluent in both English and Korean . We also note that the professional translators did not have to edit much during post - editing , suggesting that the machine - translated sentences were often good enough to begin with . We found that the BLEU scores between the machine - translated and post - edited sentences were 63.30 for KorNLI and 73.26 for KorSTS , and for approximately half the time ( 47 % for KorNLI and 53 % for KorSTS ) , the translators did not have to change the machinetranslated sentence at all . Finally , we note that translators did not see the English gold labels during post - editing , in order to expedite the post - editing process . See Section 5 for a discussion on the effect of translation on data quality .", "entities": [[107, 108, "MetricName", "BLEU"], [122, 123, "DatasetName", "KorNLI"], [126, 127, "DatasetName", "KorSTS"], [138, 139, "DatasetName", "KorNLI"], [143, 144, "DatasetName", "KorSTS"]]}
{"text": "As illustrated with BERT ( Devlin et al , 2019 ) and many of its variants , the de facto standard approach for NLU tasks is to pre - train a large language model and fine - tune it on each task . In the cross - encoding Examples Score A : \u1112 \u1161 \u11ab \u1102 \u1161 \u11b7\u110c \u1161\u1100 \u1161 \u110b \u1173 \u11b7\u1109 \u1175 \u11a8\u110b \u1173 \u11af \u1106 \u1165 \u11a8\u1100 \u1169 \u110b \u1175 \u11bb\u1103 \u1161 . 4.2 \" A man is eating food . \" B : \u1112 \u1161 \u11ab \u1102 \u1161 \u11b7\u110c \u1161\u1100 \u1161 \u1106 \u116f \u11ab\u1100 \u1161\u1105 \u1173 \u11af \u1106 \u1165 \u11a8\u1100 \u1169 \u110b \u1175 \u11bb\u1103 \u1161 . \" A man is eating something . \" A : \u1112 \u1161 \u11ab \u110b \u1167\u1109 \u1165 \u11bc\u110b \u1175 \u1100 \u1169\u1100 \u1175\u1105 \u1173 \u11af \u110b \u116d\u1105 \u1175\u1112 \u1161\u1100 \u1169 \u110b \u1175 \u11bb\u1103 \u1161 . 0.0 \" A woman is cooking meat . \" B : \u1112 \u1161 \u11ab \u1102 \u1161 \u11b7\u110c \u1161\u1100 \u1161 \u1106 \u1161 \u11af\u1112 \u1161\u1100 \u1169 \u110b \u1175 \u11bb\u1103 \u1161 . \" A man is speaking . \" approach , the pre - trained language model takes each sentence pair as a single input for fine - tuning . These cross - encoding models typically achieve the state - of - the - art performance over bi - encoding models , which encode each input sentence separately . For both KorNLI and KorSTS , we consider two pre - trained language models . We first pre - train a Korean RoBERTa ( Liu et al , 2019 ) , both base and large versions , on a collection of internally collected Korean corpora ( 65 GB ) . We construct a byte pair encoding ( BPE ) ( Gage , 1994 ; Sennrich et al , 2016 ) dictionary of 32 K tokens using Sen - tencePiece ( Kudo and Richardson , 2018 ) . We train our models using fairseq with 32 V100 GPUs for the base model ( 25 days ) and 64 for the large model ( 20 days ) . We also use XLM - R ( Conneau and Lample , 2019 ) , a publicly available cross - lingual language model that was pre - trained on 2.5 TB of Common Crawl corpora in 100 languages including Korean ( 54 GB ) . Note that the base and large architectures of XLM - R are identical to those of RoBERTa , except that the vocabulary size is significantly larger ( 250 K ) , making the embedding and output layers that much larger . In Table 5 , we report the test set scores for crossencoding models fine - tuned on KorNLI ( accuracy ) and KorSTS ( Spearman correlation ) . For KorNLI , we additionally include results for XLM - R models fine - tuned on the original MNLI training set ( also known as cross - lingual transfer in XNLI ) . To ensure comparability across settings , we only train on the MNLI portion when fine - tuning on KorNLI . Overall , the Korean RoBERTa models outperform the XLM - R models , regardless of whether they are fine - tuned on Korean or English training sets . For each model , the larger variant outperforms the base one , consistent with previous findings . The large version of Korean RoBERTa performs the best for both KorNLI ( 83.67 % ) and KorSTS ( 85.27 % ) among all models tested . Among the XLM - R models for KorNLI , those fine - tuned on the Korean training set consistently outperform the cross - lingual transfer variants .", "entities": [[3, 4, "MethodName", "BERT"], [49, 50, "MetricName", "Score"], [231, 232, "DatasetName", "KorNLI"], [233, 234, "DatasetName", "KorSTS"], [251, 252, "MethodName", "RoBERTa"], [282, 285, "MethodName", "byte pair encoding"], [286, 287, "MethodName", "BPE"], [348, 349, "MethodName", "XLM"], [376, 378, "DatasetName", "Common Crawl"], [397, 398, "MethodName", "XLM"], [405, 406, "MethodName", "RoBERTa"], [447, 448, "DatasetName", "KorNLI"], [449, 450, "MetricName", "accuracy"], [452, 453, "DatasetName", "KorSTS"], [454, 456, "MetricName", "Spearman correlation"], [459, 460, "DatasetName", "KorNLI"], [466, 467, "MethodName", "XLM"], [476, 477, "DatasetName", "MNLI"], [483, 487, "TaskName", "cross - lingual transfer"], [488, 489, "DatasetName", "XNLI"], [502, 503, "DatasetName", "MNLI"], [509, 510, "DatasetName", "KorNLI"], [515, 516, "MethodName", "RoBERTa"], [519, 520, "MethodName", "XLM"], [561, 562, "MethodName", "RoBERTa"], [567, 568, "DatasetName", "KorNLI"], [573, 574, "DatasetName", "KorSTS"], [585, 586, "MethodName", "XLM"], [590, 591, "DatasetName", "KorNLI"], [604, 608, "TaskName", "cross - lingual transfer"]]}
{"text": "We also report the KorSTS scores of bi - encoding models . The bi - encoding approach bears practical importance in applications such as semantic search , where computing pairwise similarity among a large set of sentences is computationally expensive with cross - encoding . Here , we first provide two baselines that do not use pre - trained language models : Korean fastText and the multilingual universal sentence encoder ( M - USE ) . Korean fastText ( Bojanowski et al , 2017 ) is a pre - trained word embedding model 6 trained on Korean text from Common Crawl . To produce sentence embeddings , we take the average of fastText word embeddings for each sentence . M - USE 7 ( Yang et al , 2019 ) , is a CNN - based sentence encoder model trained for NLI , questionanswering , and translation ranking across 16 languages including Korean . For both Korean fastText and M - USE , we compute the cosine similarity between two input sentence embeddings to make an unsupervised STS prediction . Pre - trained language models can also be used as bi - encoding models following the approach of Sen - tenceBERT ( Reimers and Gurevych , 2019 ) , which involves fine - tuning a BERT - like model with a Siamese network structure on NLI and/or STS . We use the SentenceBERT approach for both Korean RoBERTa ( \" Korean SRoBERTa \" ) and XLM - R ( \" SXLM - R \" ) . We adopt the MEAN pooling strategy , i.e. , computing the sentence vector as the mean of all contextualized word vectors . In Table 6 , we present the KorSTS test set scores ( 100 \u00d7 Spearman correlation ) for the biencoding models . We categorize each result based on whether the model was additionally trained on KorNLI and/or KorSTS . Note that models that are not fine - tuned at all or only fine - tuned to KorNLI can be considered as unsupervised w.r.t . KorSTS . Also note that M - USE is trained on a machinetranslated version of SNLI , which is a subset of KorNLI , as part of its pre - training step . 6 https://dl.fbaipublicfiles.com/ fasttext / vectors - crawl / cc.ko.300.bin.gz 7 https://tfhub.dev/google/ universal - sentence - encoder - multilingual/ 3 First , given each model , we find that supplementary training on KorNLI consistently improves the KorSTS scores for both unsupervised and supervised settings , as was the case with English models ( Conneau et al , 2017 ; Reimers and Gurevych , 2019 ) . This shows that the KorNLI dataset can be an effective intermediate training source for biencoding approaches . When comparing the baseline models in each setting , we find that both M - USE and the SentenceBERT - based models trained on KorNLI achieve competitive unsupervised Ko - rSTS scores . Both models significantly outperform the average of fastText embeddings model and the Korean SRoBERTa and SXLM - R models without fine - tuning . Among our baselines , large SXLM - R trained on KorNLI followed by KorSTS achieves the best score ( 81.84 ) .", "entities": [[4, 5, "DatasetName", "KorSTS"], [63, 64, "MethodName", "fastText"], [66, 70, "MethodName", "multilingual universal sentence encoder"], [73, 74, "MethodName", "USE"], [77, 78, "MethodName", "fastText"], [99, 101, "DatasetName", "Common Crawl"], [104, 106, "TaskName", "sentence embeddings"], [112, 113, "MethodName", "fastText"], [113, 115, "TaskName", "word embeddings"], [121, 122, "MethodName", "USE"], [157, 158, "MethodName", "fastText"], [161, 162, "MethodName", "USE"], [171, 173, "TaskName", "sentence embeddings"], [177, 178, "TaskName", "STS"], [215, 216, "MethodName", "BERT"], [221, 223, "MethodName", "Siamese network"], [227, 228, "TaskName", "STS"], [237, 238, "MethodName", "RoBERTa"], [245, 246, "MethodName", "XLM"], [285, 286, "DatasetName", "KorSTS"], [292, 294, "MetricName", "Spearman correlation"], [313, 314, "DatasetName", "KorNLI"], [315, 316, "DatasetName", "KorSTS"], [334, 335, "DatasetName", "KorNLI"], [342, 343, "DatasetName", "KorSTS"], [349, 350, "MethodName", "USE"], [357, 358, "DatasetName", "SNLI"], [364, 365, "DatasetName", "KorNLI"], [377, 378, "MethodName", "fasttext"], [406, 407, "DatasetName", "KorNLI"], [410, 411, "DatasetName", "KorSTS"], [444, 445, "DatasetName", "KorNLI"], [472, 473, "MethodName", "USE"], [481, 482, "DatasetName", "KorNLI"], [497, 498, "MethodName", "fastText"], [524, 525, "DatasetName", "KorNLI"], [527, 528, "DatasetName", "KorSTS"]]}
{"text": "As noted in ( Conneau et al , 2018 ) , translation quality does not necessarily guarantee that the semantic relationships between sentences are preserved . We also translated each sentence independently and took the gold labels from the original English pair , so the resulting label might no longer be \" gold , \" due to both incorrect translations and ( in rarer cases ) linguistic differences that make it difficult to translate specific concepts . Fortunately , it was also pointed out in ( Conneau et al , 2018 ) that annotators could recover the NLI labels at a similar accuracy in translated pairs ( 83 % in French ) as in original pairs ( 85 % in English ) . In addition , our baseline experiments in Section 4.1 show that supplementary training on KorNLI improves KorSTS performance ( +1 % for RoBERTa and +4 - 11 % for XLM - R ) , suggesting that the labels of KorNLI are still meaningful . Another quantitative evidence is that the performance of XLM - R fine - tuned on KorNLI ( 80.3 % with cross - lingual transfer ) is within a comparable range of the model 's performance on other XNLI languages ( 80.1 % on average ) . Nevertheless , we could also find some ( not many ) examples the gold label becomes incorrect after translating input sentences to Korean . For example , there were cases in which the two input sentences for KorSTS were so similar ( with 4 + similarity scores ) that upon translation , the two inputs simply became identical . In another case , the English word sir appeared in the premise of an NLI example and was translated to \u1109 \u1165 \u11ab\u1109 \u1162 \u11bc\u1102 \u1175 \u11b7 , which is a correct word translation but is a gender - neutral noun , because there is no gender - specific counterpart to the word in Korean . As a result , when the hypothesis referencing the entity as the man got translated into \u1102 \u1161 \u11b7\u110c \u1161 ( gender - specific ) , the English gold label ( entailment ) was no longer correct in the translated example . More systematically analyzing these errors is an interesting future work , although the amount of human efforts involved in this analysis would match that of labeling a new dataset .", "entities": [[102, 103, "MetricName", "accuracy"], [137, 138, "DatasetName", "KorNLI"], [139, 140, "DatasetName", "KorSTS"], [145, 146, "MethodName", "RoBERTa"], [152, 153, "MethodName", "XLM"], [162, 163, "DatasetName", "KorNLI"], [175, 176, "MethodName", "XLM"], [182, 183, "DatasetName", "KorNLI"], [187, 191, "TaskName", "cross - lingual transfer"], [204, 205, "DatasetName", "XNLI"], [250, 251, "DatasetName", "KorSTS"], [304, 306, "TaskName", "word translation"]]}
{"text": "To fine - tune Korean RoBERTa and XLM - R models using the bi - encoding approach ( 4.2 ) , we train Korean Sentence RoBERTa ( \" Korean SRoBERTa \" ) and Sentence XLM - R ( \" SXLM - R \" ) , following the fine - tuning procedure of SentenceBERT ( Reimers and Gurevych , 2019 ) . Unless described otherwise , we follow the experimental settings , including all hyperparameters , of SentenceBERT 10 . For each model size , we manually search among learning rates { 2e - 5 , 1e - 5 } for training on KorNLI , { 1e - 5 , 2e - 6 } for training on KorSTS , and { 1e - 5 , 2e - 6 } for training on KorSTS after KorNLI . After training until convergence , we choose the learning rate that lead to the highest KorSTS score on the development set . These hyperparameters are shown in Table 10 . We report the development set scores in Table 11 . Korean SRoBERTa ( large ) achieves the best development set performance on both supervised settings , but SXLM - R ( large ) achieves the best performance for the KorNLI KorSTS setting on test set .", "entities": [[5, 6, "MethodName", "RoBERTa"], [7, 8, "MethodName", "XLM"], [25, 26, "MethodName", "RoBERTa"], [34, 35, "MethodName", "XLM"], [102, 103, "DatasetName", "KorNLI"], [116, 117, "DatasetName", "KorSTS"], [131, 132, "DatasetName", "KorSTS"], [133, 134, "DatasetName", "KorNLI"], [143, 145, "HyperparameterName", "learning rate"], [150, 151, "DatasetName", "KorSTS"], [204, 205, "DatasetName", "KorNLI"], [205, 206, "DatasetName", "KorSTS"]]}
{"text": "We propose a novel transition - based algorithm that straightforwardly parses sentences from left to right by building n attachments , with n being the length of the input sentence . Similarly to the recent stack - pointer parser by Ma et al ( 2018 ) , we use the pointer network framework that , given a word , can directly point to a position from the sentence . However , our left - to - right approach is simpler than the original top - down stack - pointer parser ( not requiring a stack ) and reduces transition sequence length in half , from 2n \u2212 1 actions to n. This results in a quadratic non - projective parser that runs twice as fast as the original while achieving the best accuracy to date on the English PTB dataset ( 96.04 % UAS , 94.43 % LAS ) among fully - supervised singlemodel dependency parsers , and improves over the former top - down transition system in the majority of languages tested .", "entities": [[50, 52, "MethodName", "pointer network"], [132, 133, "MetricName", "accuracy"], [138, 139, "DatasetName", "PTB"]]}
{"text": "Dependency parsing , the task of automatically obtaining the grammatical structure of a sentence expressed as a dependency tree , has been widely studied by natural language processing ( NLP ) researchers in the last decades . Most of the models providing competitive accuracies fall into two broad families of approaches : graph - based ( Mc - Donald et al , 2005a , b ) and transition - based ( Yamada and Matsumoto , 2003 ; Nivre , 2003 ) dependency parsers . Given an input sentence , a graph - based parser scores trees by decomposing them into factors , and performs a search for the highest - scoring tree . In the past two years , this kind of dependency parsers have been ahead in terms of accuracy thanks to the graph - based neural architecture developed by Dozat and Manning ( 2016 ) , which not only achieved state - of - the - art accuracies on the Stanford Dependencies conversion of the English Penn Treebank ( hereinafter , PTB - SD ) , but also obtained the best results in the majority of languages in the CoNLL 2017 Shared Task ( Dozat et al , 2017 ) . This tendency recently changed , since a transition - based parser developed by Ma et al ( 2018 ) managed to outperform the best graphbased model in the majority of datasets tested . Transition - based parsers incrementally build a dependency graph for an input sentence by applying a sequence of transitions . This results in more efficient parsers with linear time complexity for parsing projective sentences , or quadratic for handling non - projective structures , when implemented with greedy or beam search . However , their main weakness is the lack of access to global context information when transitions are greedily chosen . This favours error propagation , mainly affecting long dependencies that require a larger number of transitions to be built ( McDonald and Nivre , 2011 ) . Many attempts have been made to alleviate the impact of error propagation in transition - based dependency parsing , but the latest and most successful approach was developed by Ma et al ( 2018 ) . In particular , they make use of pointer networks ( Vinyals et al , 2015 ) to implement a new neural network architecture called stack - pointer network . The proposed framework provides a global view of the input sentence by capturing information from the whole sentence and all the arcs previously built , crucial for reducing the effect of error propagation ; and , thanks to an attention mechanism ( Bahdanau et al , 2014 ; Luong et al , 2015 ) , is able to return a position in that sentence that corresponds to a word related to the word currently on top of the stack . They take advantage of this and propose a novel transition system that follows a top - down depth - first strategy to perform the syntactic analysis . Concretely , it considers the word pointed by the neural network as the child of the word on top of the stack , and builds the corresponding dependency relation between them . This results in a transition - based algorithm that can process unrestricted non - projective sentences in O ( n 2 ) time complexity and requires 2n - 1 actions to successfully parse a sentence with n words . We also take advantage of pointer network capabilities and use the neural network architecture introduced by Ma et al ( 2018 ) to design a nonprojective left - to - right transition - based algorithm , where the position value pointed by the network has the opposite meaning : it denotes the index that corresponds to the head node of the current focus word . This results in a straightforward transition system that can parse a sentence in just n actions , without the need of any additional data structure and by just attaching each word from the sentence to another word ( including the root node ) . Apart from increasing the parsing speed twofold ( while keeping the same quadratic time complexity ) , it achieves the best accuracy to date among fully - supervised single - model dependency parsers on the PTB - SD , and obtains competitive accuracies on twelve different languages in comparison to the original top - down version . 2 Preliminaries Ma et al ( 2018 ) propose a novel neural network architecture whose main backbone is a pointer network ( Vinyals et al , 2015 ) . This kind of neural networks are able to learn the conditional probability of a sequence of discrete numbers that correspond to positions in an input sequence ( in this case , indexes of words in a sentence ) and , by means of attention ( Bahdanau et al , 2014 ; Luong et al , 2015 ) , implement a pointer that selects a position from the input at decoding time . Their approach initially reads the whole sentence , composed of the n words w 1 , . . . , w n , and encodes each w i one by one into an encoder hidden state e i . As encoder , they employ a combination of CNNs and bi - directional LSTMs ( Chiu and Nichols , 2016 ; Ma and Hovy , 2016 ) . For each word , CNNs are used to obtain its character - level representation that is concatenated to the word and PoS embeddings to finally be fed into BiLSTMs that encode word context information . As decoder they present a top - down transition system , where parsing configurations use the classic data structures ( Nivre , 2008 ) : a buffer ( that contains unattached words ) and a stack ( that holds partially processed words ) . The available parser actions are two transitions that we call Shift - Attach - p and Reduce . Given a configuration with word w i on top of the stack , as the pointer network just returns a position p from a given sentence , they proceed as follows to determine which transition should be applied : If p = i , then the pointed word w p is considered as a child of w i ; so the parser chooses a Shift - Attach - p transition to move w p from the buffer to the stack and build an arc w i w p . On the other hand , if p = i , then w i is considered to have found all its children , and a Reduce transition is applied to pop the stack . The parsing process starts with a dummy root $ on the stack and , by applying 2n - 1 transitions , a dependency tree is built for the input in a top - down depth - first fashion , where multiple children of a same word are forced during training to be created in an inside - out manner . More in detail , for each parsing configuration c t , the decoder ( implemented as a uni - directional LSTM ) receives the encoder hidden state e i of the word w i on top of the stack to generate a decoder hidden state d t . After that , d t , together with the sequence s i of encoder hidden states from words still in the buffer plus e i , are used to compute the attention vector a t as follows : v t i = score ( d t , s i ) ( 1 ) a t = sof tmax ( v t ) ( 2 ) As attention scoring function ( score ( ) ) , they adopt the biaffine attention mechanism described in ( Luong et al , 2015 ; Dozat and Manning , 2016 ) . Finally , the attention vector a t will be used to return the highest - scoring position p and choose the next transition . The parsing process ends when only the root remains on the stack . As extra high - order features , Ma et al ( 2018 ) add grandparent and sibling information , whose encoder hidden states are added to that of the word on top of the stack to generate the corresponding decoder hidden state d t . They prove that these additions improve final accuracy , especially when children are attached in an inside - out fashion . According to the authors , the original stackpointer network is trained to maximize the likelihood of choosing the correct word for each possible top - down path from the root to a leaf . More in detail , a dependency tree can be represented as a sequence of top - down paths p 1 , . . . , p k , where each path p i corresponds to a sequence of words $ , w i , 1 , w i , 2 , . . . , w i , l i from the root to a leaf . Thus , the conditional probability P \u03b8 ( y | x ) of the dependency tree y for an input sentence x can be factorized according to this top - down structure as : P \u03b8 ( y | x ) = k i=1 P \u03b8 ( p i | p < i , x ) = k i=1 l i j=1 P \u03b8 ( w i , j | w i , < j , p < i , x ) where \u03b8 represents model parameters , p < i stands for previous paths already explored , w i , j denotes the jth word in path p i and w i , < j represents all the previous words on p i . For more thorough details of the stack - pointer network architecture and the top - down transition system , please read the original work by Ma et al ( 2018 ) .", "entities": [[0, 2, "TaskName", "Dependency parsing"], [130, 131, "MetricName", "accuracy"], [168, 170, "DatasetName", "Penn Treebank"], [173, 174, "DatasetName", "PTB"], [348, 353, "TaskName", "transition - based dependency parsing"], [397, 399, "MethodName", "pointer network"], [583, 585, "MethodName", "pointer network"], [708, 709, "MetricName", "accuracy"], [722, 723, "DatasetName", "PTB"], [763, 765, "MethodName", "pointer network"], [1024, 1026, "MethodName", "pointer network"], [1211, 1212, "MethodName", "LSTM"], [1295, 1296, "DatasetName", "sof"], [1425, 1426, "MetricName", "accuracy"], [1546, 1547, "HyperparameterName", "\u03b8"], [1575, 1576, "HyperparameterName", "\u03b8"], [1585, 1586, "HyperparameterName", "\u03b8"], [1603, 1604, "HyperparameterName", "\u03b8"], [1623, 1624, "HyperparameterName", "\u03b8"], [1673, 1675, "MethodName", "pointer network"]]}
{"text": "We take advantage of the neural network architecture designed by Ma et al ( 2018 ) and introduce a simpler left - to - right transition system that requires neither a stack nor a buffer to process the input sentence and where , instead of selecting a child of the word on top of the stack , the network points to the parent of the current focus word . In particular , in our proposed approach , the parsing configuration just corresponds to a focus word pointer i , that is used to point to the word currently being processed . The decoding process starts with i pointing at the first word of the sentence and , at each parsing configuration , only one action is available : the parameterized Attach - p transition , that links the focus word w i to the head word w p in position p of the sentence ( producing the dependency arc w p w i ) and moves i one position to the right . Note that , in our algorithm , p can equal 0 , attaching , in that case , w i to the dummy root node . The parsing process ends when the last word from the sentence is attached . This can be easily represented as a loop that traverses the input sentence from left to right , linking each word to another from the same sentence or to the dummy root . Therefore , we just need n steps to process the n words of a given sentence and build a dependency tree . While our novel transition system intrinsically holds the single - head constraint ( since , after attaching the word w i , i points to the next word w i+1 in the sentence ) , it can produce an output with cycles . 1 Therefore , in order to build a wellformed dependency tree during decoding , attachments that generate cycles in the already - built dependency graph must be forbidden . Please note that the need of a cycle - checking extension does not increase the overall quadratic runtime complexity of the original implementation by Ma et al ( 2018 ) since , as in other transition - based parsers such as ( Covington , 2001 ; G\u00f3mez - Rodr\u00edguez and Nivre , 2010 ) , cycles can be incrementally identified in amortized constant time by keeping track of connected components using path compression and union by rank . Therefore , the left - to - right algorithm requires n steps to produce a parse . In addition , at each step , the attention vector a t needs to be computed and cycles must be checked , both in O ( n ) + O ( n ) = O ( n ) runtime . This results in a O ( n 2 ) time complexity for decoding . 2 On the other hand , while in the top - down decoding only available words in the buffer ( plus the word on top of the stack ) can be pointed to by the network and they are reduced as arcs are created ( basically to keep the single - head constraint ) ; our proposed approach is less rigid : all words from the sentence ( including the root node and excluding w i ) can be pointed to , as long as they satisfy the acyclicity constraint . This is necessary because two different words might be attached to the same head node and the latter can be located in the sentence either before or after w i . Therefore , the sequence s i , required by the attention score function ( Eq . ( 1 ) ) , is composed of the encoder hidden states of all words from the input , excluding e i , and prepending a special vector representation denoting the root node . We also add extra features to represent the current focus word . Instead of using grandparent and sibling information ( more beneficial for a topdown approach ) , we just add the encoder hidden states of the previous and next words in the sentence to generate d t , which seems to be more suitable for a left - to - right decoding . In dependency parsing , a tree for an input sentence of length n can be represented as a set of n directed and binary links l 1 , . . . , l n . Each link l i is characterized by the word w i in position i in the sentence and its head word w h , resulting in a pair ( w i , w h ) . Therefore , to train this novel variant , we factorize the conditional probability P \u03b8 ( y | x ) to a set of head - dependent pairs as follows : P \u03b8 ( y | x ) = n i=1 P \u03b8 ( l i | l < i , x ) = n i=1 P \u03b8 ( w h | w i , l < i , x ) Therefore , the left - to - right parser is trained by maximizing the likelihood of choosing the correct head word w h for the word w i in position i , given the previous predicted links l < i . Finally , following a widely - used approach ( also implemented in ( Ma et al , 2018 ) ) , dependency labels are predicted by a multiclass classifier , which is trained in parallel with the parser by optimizing the sum of their objectives .", "entities": [[183, 184, "DatasetName", "0"], [726, 728, "TaskName", "dependency parsing"], [810, 811, "HyperparameterName", "\u03b8"], [828, 829, "HyperparameterName", "\u03b8"], [838, 839, "HyperparameterName", "\u03b8"], [853, 854, "HyperparameterName", "\u03b8"]]}
{"text": "UAS LAS Chen and Manning ( 2014 ) 91.8 89.6 Dyer et al ( 2015 ) 93.1 90.9 93.99 92.05 93.56 91.42 Kiperwasser and Goldberg ( 2016 ) 93.9 91.9 94.23 92.36 94.3 92.2 Fern\u00e1ndez - G and G\u00f3mez - R ( 2018 ) Systems marked with * , including the improved variant described in ( Ma et al , 2018 ) of the graph - based parser by ( Dozat and Manning , 2016 ) , are implemented under the same framework as our approach and use the same training settings . Like ( Ma et al , 2018 ) , we report the average accuracy over 5 repetitions . Finally , we use the same hyper - parameter values , pre - trained word embeddings and beam size ( 10 for PTB - SD and 5 for UD ) as Ma et al ( 2018 ) .", "entities": [[105, 107, "MetricName", "average accuracy"], [125, 127, "TaskName", "word embeddings"], [133, 134, "DatasetName", "PTB"], [139, 140, "DatasetName", "UD"]]}
{"text": "There is previous work that proposes to implement dependency parsing by independently selecting the head of each word in a sentence , using neural networks . In particular , Zhang et al ( 2017 ) make use of a BiLSTM - based neural architecture to compute the probability of attaching each word to one of the other input words , in a similar way as pointer networks do . During decoding , a postprocessing step is needed to produce well - formed trees by means of a maximum spanning tree algorithm . Our approach does not need this postprocessing , as cycles are forbidden during parsing instead , and achieves a higher accuracy thanks to the pointer network architecture and the use of information about previous dependencies . Before Ma et al ( 2018 ) presented their topdown parser , Chorowski et al ( 2017 ) had already employed pointer networks ( Vinyals et al , 2015 ) for dependency parsing . Concretely , they developed a pointer - network - based neural architecture with multitask learning able to perform preprocessing , tagging and dependency parsing exclusively by reading tokens from an input sen - tence , without needing POS tags or pre - trained word embeddings . Like our approach , they also use the capabilities provided by pointer networks to undertake the parsing task as a simple process of attaching each word as dependent of another . They also try to improve the network performance with POS tag prediction as auxiliary task and with different approaches to perform label prediction . They do not exclude cycles , neither by forbidding them at parsing time or by removing them by post - processing , as they report that their system produces parses with a negligible amount of cycles , even with greedy decoding ( matching our observation for our own system , in our case with beam - search decoding ) . Finally , the system developed by Chorowski et al ( 2017 ) is constrained to projective dependencies , while our approach can handle unrestricted non - projective structures .", "entities": [[8, 10, "TaskName", "dependency parsing"], [39, 40, "MethodName", "BiLSTM"], [112, 113, "MetricName", "accuracy"], [116, 118, "MethodName", "pointer network"], [159, 161, "TaskName", "dependency parsing"], [184, 186, "TaskName", "dependency parsing"], [205, 207, "TaskName", "word embeddings"]]}
{"text": "We present a novel left - to - right dependency parser based on pointer networks . We follow the same neural network architecture as the stack - pointerbased approach developed by Ma et al ( 2018 ) , but just using a focus word index instead of a buffer and a stack . Apart from doubling their system 's speed , our approach proves to be a competitive alternative on a variety of languages and achieves the best accuracy to date on the PTB - SD . The good performance of our algorithm can be explained by the shortening of the transition sequence length . In fact , it has been proved by several studies ( Fern\u00e1ndez - Gonz\u00e1lez and G\u00f3mez - Rodr\u00edguez , 2012 ; Fern\u00e1ndez - Gonz\u00e1lez and G\u00f3mez - Rodr\u00edguez , 2018 ) that by reducing the number of applied transitions , the impact of error propagation is alleviated , yielding more accurate parsers . Our system 's source code is freely available at https://github.com/danifg/ Left2Right - Pointer - Parser .", "entities": [[78, 79, "MetricName", "accuracy"], [83, 84, "DatasetName", "PTB"]]}
{"text": "Recent events such as the last two U.S. presidential elections have been greatly affected by fake news , defined as \" fabricated information that disseminates deceptive content , or grossly distort actual news reports , shared on social media platforms \" ( Allcott and Gentzkow , 2017 ) . In fact , the World Economic Forum 2013 report designates massive digital misinformation as a major technological and geopolitical risk ( Bovet and Makse , 2019 ) . As daily social media usage increases ( Statista Research Department , 2021 ) , manual fact - checking can not keep up with this deluge of information . Automatic fact - checking models are therefore a necessity , and most of them function using a system of claims and evidence ( Hassan et al , 2017 ) . Given a specific claim , the models use external knowledge as evidence . Typically , a web search query is treated as the claim , and a subset of the top search results is treated as the evidence . There is an implicit assumption that the fact - checking models are reasoning in some way , using the evidence to confirm or refute the claim . Recent research ( Hansen et al , 2021 ) found this conclusion may be premature ; current models can show improved performance when considering evidence alone , essentially fact - checking an unasked question . While this might seem reasonable given that the evidence is conditioned on the claims by the search engine , this can be exploited as illustrated in Figure 1 , which shows that evidence returned using a ridiculous claim can still appear reasonable if we view the evidence alone without the claim . Furthermore , textual entailment requires both a text and a hypothesis ; if we have a result without a hypothesis , we are performing a different , unknown task . This finding indicates a problem with current automatic fake news detection , signaling that the models rely on features in the evidence typical to fake news , rather than using entailment . Since most automated fact - checking research is primarily concerned with the accuracy of the results , rather than addressing how the results are achieved , we propose a novel investigation into these models and their evidence . We use a variety of pre - processing steps , including neural and non - neural ones , to attempt to reduce the affectations common in evidence : Stemming , stopword removal , negation , and POS - filtering ( Babanejad et al , 2020 ) . Style transfer neural models using the Styleformer model to perform informal - to - formal and formal - to - informal paraphrasing methods ( Li et al , 2018 ; Schmidt , 2020 ) . We also develop our own BERT - based model as an extension of the EmoCred system ( Giachanou Figure 1 : An example of why evidence alone does not suffice in identifying fake news , despite the evidence being conditioned on the claim as a search - engine query . Although the returned evidence appearing reputable , it is clear that it has little relevance to deciding the veracity of the claim that \" all Canadians have eaten at least one bear . \" et al , 2019 ) , adding an \" emotional attention \" layer to weight the most relevant emotional signals in a given evidence snippet . We make our code publicly available . 1 With each of these methods , we focus on scores where the models perform better using both the claims and the evidence combined , S C&E , rather than with the evidence alone , S E . Going forward , we will refer to the difference between these dataset combinations as the delta of the pre - processing step , where delta = S C&E \u2212 S E . A positive delta score indicates that the claim was useful and helped yield an increase in performance . Since we are removing indicators that the current models rely on , some of the models perform worse at the task than they did previously . However , a surprising result is that many improved , and the need to consider the claim and the evidence together is a sign of using reasoning rather than manipulable indicators . Under current fact - checking models , adversarial data can subvert these detectors . Paraphrasing can be performed by inserting fictitious statements into otherwise truthful evidence with little effect on the model 's output . For example , an article titled \" Is the GOP losing Walmart ? \" , could have \" Walmart \" substituted with \" Apple , \" and the predictions are nearly identical despite the news now being fictitious ( Zhou et al , 2019 ) . 1", "entities": [[326, 329, "TaskName", "fake news detection"], [362, 363, "MetricName", "accuracy"], [435, 437, "TaskName", "Style transfer"], [475, 476, "MethodName", "BERT"]]}
{"text": "The EmoCred systems of EmoLexi and EmoInt use a lexicon to determine emotional word counts and intensities , respectively ( Giachanou et al , 2019 ) . We use the NRC Affect Intensity Lexicon , a \" highcoverage lexicons that captures word - affect intensities \" for eight basic emotions , which were created using a technique called best - worst scaling ( Mohammad , 2017 ) . These eight emotions can be used to create an emotion vector for a sentence , where each index corresponds to a score : [ anger , anticipation , disgust , fear , joy , sadness , surprise , trust ] . As an example , a sentence that contains the word \" suffering \" conveys sadness with an NRC Affect Intensity Lexicon intensity of 0.844 , whereas the word \" affection \" indicates joy with an intensity of 0.647 . We create the vector of length eight , and for each word associated with an emotion , the emotion 's indexed value is either : ( 1 ) incremented by one for EmoLexi ; or , ( 2 ) incremented by its intensity for EmoInt . Thus , the sentence \" He had an affection for suffering \" would have an EmoLexi emotion vector of [ 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 ] and an EmoInt emotion vector of [ 0 , 0 , 0 , 0 , 0.647 , 0.844 , 0 , 0 ] We build on this EmoCred framework , adding an attention system for emotion that gives a weight to each emotion vector , just as the attention layer for each snippet gives a weight to each snippet . The end result is that two independent attention layers attend to the ten snippets and ten emotional representations independently , and we call the resulting system Emotional Attention ( see Figure 3 ) .", "entities": [[77, 78, "DatasetName", "emotion"], [163, 164, "DatasetName", "emotion"], [166, 167, "DatasetName", "emotion"], [210, 211, "DatasetName", "emotion"], [214, 215, "DatasetName", "0"], [216, 217, "DatasetName", "0"], [218, 219, "DatasetName", "0"], [220, 221, "DatasetName", "0"], [226, 227, "DatasetName", "0"], [228, 229, "DatasetName", "0"], [233, 234, "DatasetName", "emotion"], [237, 238, "DatasetName", "0"], [239, 240, "DatasetName", "0"], [241, 242, "DatasetName", "0"], [243, 244, "DatasetName", "0"], [249, 250, "DatasetName", "0"], [251, 252, "DatasetName", "0"], [265, 266, "DatasetName", "emotion"], [272, 273, "DatasetName", "emotion"], [297, 299, "HyperparameterName", "attention layers"]]}
{"text": "Surprisingly , the four top - performing models with the Snopes dataset include two non - neural models and two neural models . All four achieve greater F1 Macro scores than the baseline BERT model without pre - processing ( see Figure 2 ) . POS and STOP yield the biggest delta between S C&E vs. S E , followed by EmoInt and Informal Style Transfer . However , EmoInt yields the highest F1 Macro , followed by POS , Informal , and STOP . In PolitiFact , none of the pre - processing steps achieve a delta greater than zero for S C&E versus S E . The combination of POS+STOP steps come closest to parity , followed by EmoInt , then POS and STOP . For the best F1 Macro scores overall , EmoAttention 's two forms ( i.e. , EmoInt and EmoLexi ) were the two best , followed by STOP Macro scores and deltas are in red . With the exception of EmoLexi tying for the lowest delta , the best pre - processing steps outperform the baseline BERT model from Hansen et al ( 2021 ) . and POS . All of these pre - processing steps achieve higher F1 Macro scores than the baseline BERT model . Further , they yield better deltas for S C&E versus S E , implying that the model now requires the claims to reason .", "entities": [[10, 11, "DatasetName", "Snopes"], [27, 29, "MetricName", "F1 Macro"], [33, 34, "MethodName", "BERT"], [64, 66, "TaskName", "Style Transfer"], [73, 75, "MetricName", "F1 Macro"], [86, 87, "DatasetName", "PolitiFact"], [130, 132, "MetricName", "F1 Macro"], [182, 183, "MethodName", "BERT"], [204, 206, "MetricName", "F1 Macro"], [210, 211, "MethodName", "BERT"]]}
{"text": "Many pre - processing steps increase both the model 's F1 scores and its need for claims and evidence , validating our hypothesis that signals in style and tone have become a crutch for factchecking models . Rather than doing entailment , they are leveraging other signals - perhaps similar to sentiment analysis - and relying on a \" gut feeling \" . EmoAttention generates our best predictions and deltas , confirming our suspicion that the models rely on emotionally charged style as a predictive feature . This is further narrowed to emotional intensity : the EmoInt intensity score - based model performs much better than its count - based counterpart EmoLexi . Thus , evidence containing emotions associated with fake news will be considered more when scoring the claim . One surprising result is the effectiveness of the simple POS and STOP pre - processing steps . POS only included nouns , verbs , and adjectives ( i.e. , a superset of STOP ) . This could explain why it has the best delta between S C&E vs. S E . Future research could investigate if stopwords , which are often discarded , actually contain signals such as anaphora : a repetitive rhetoric style which can affect NLP analyses ( Liddy , 1990 ) . As an example , Donald Trump makes heavy use of anaphora in his 2017 inauguration speech : \" Together , we will make America strong again . We will make America wealthy again . We will make America proud again . We will make America safe again . And , yes , together , we will make america great again . \" ( Trump Inauguration Address , 2017 ) By removing stopwords \" we \" , \" will \" and \" again \" , the model relies less on the text 's rhetoric style and more on the entailment we are seeking . We propose further study on the effects of STOP and POS , as well as experimenting with different emotional vectors and EmoAttention to make factchecking models more robust . Automatic Fake News detection remains a challenging problem , and unfortunately , current fact - checking models can be subverted by adversarial techniques that exploit emotionally charged writing .", "entities": [[10, 11, "MetricName", "F1"], [51, 53, "TaskName", "sentiment analysis"], [349, 352, "TaskName", "Fake News detection"]]}
{"text": "We trained the models on transcriptions of childdirected speech , i.e. samples of naturalistic productions in the linguistic environment of a child . We extracted the child - directed speech data from the CHILDES database ( MacWhinney , 2000 ) , for all the varieties of English , for ages ranging from 0 to 60 months . We used the childesr library to extract the child - directed utterances ( Sanchez et al , 2019 ) 2 . Word tokens were coded at the lemma level . The resulting dataset contains a total number of 3 , 135 , 822 sentences , 34 , 961 word types , and 12 , 975 , 520 word tokens . To evaluate the models , we used data collected with the MacArthur - Bates Communicative Development Inventory forms ( CDI ) . These are forms , given to parents of young children , that contain checklists of common early acquired words . Parents complete the forms according to whether their child understands or produces each of those words . These forms are collected at different ages , and thus can be used to estimate the Age of Acquisition ( AoA ) of words . We used all the variants of English ' Words & Sentences ' CDIs from the Wordbank database ( Frank et al , 2017 ) , with the exception of those involving twins ( as significant differences have been observed in the language development of twins and singletons , Tomasello et al , 1986 ) . We estimated the AoA of a word by considering that a word is acquired at the age at which at least 50 % of the children in the sample produced a given word . 2 http://childes - db.stanford.edu/about . html 4 Method 1 : Neighbourhood Density Our first evaluation method is inspired by prior work on human word learning , presented in Hills et al ( 2010 ) . In their work , the authors modeled the emerging network of semantic associations that children build during language acquisition . Their model consists of a simple word co - occurrence matrix , where all the counts greater than zero are flattened into a count of one , resulting in a binary matrix . The authors view the resulting matrix as a network of associations , where words are connected only if they have co - occurred . The number of connections of each word is then used as an index , which the authors call Contextual Diversity ( CD ) . This index has been repeatedly shown to predict language acquisition phenomena , such as the age of acquisition of words in different syntactic categories ( Hills et al , 2010 ; Stella et al , 2017 ) and individual differences between typically developing children and late talkers ( Beckage et al , 2011 ) . We propose a variant evaluation method that takes token co - occurrences into account . Because of the binarization of the co - occurrence matrix , the CD index is an indicator of type co - occurrences , and is therefore agnostic to co - occurrence frequency . The models we work with , on the contrary , are sensitive to co - occurrence frequencies , providing a more fine - grained characterization of the semantic space . Our method works as follows . First , we derived the semantic networks based on the cosine distance between representations . This required us to set a minimum cosine similarity threshold \u03b8 to determine if two words are connected , which we treat as a hyperparameter ( with values [ .6 , .7 , .8 , .9 ] ) . Second , given this network , we counted the number of neighbours of each word as the number of other words connected to it . We refer to this index as neighbourhood density ( ND ) . Third , we computed the Pearson 's r correlation between this index and the AoA norms . Figure 1 shows the distribution of the computed metric . Note that these correlations can not be expected to be of the same order as those found when evaluating against adult ratings , since age of acquisition is predicted by a variety of factors , of which distributional information is only one , and it is subject to greater individual differences than adult semantic knowledge . Therefore , moderate but significant correlations are generally consid - ered meaningful . As a reference , the CD index , has a correlation of r = 0.32 in our dataset 3 . As can be seen , the SGNS model is more likely to provide a semantic space that correlates with AoA , and some configurations yield an effect size comparable ( even larger ) than the CD metric . This indicates that the SGNS model builds word representations in a way that reflects the relative difficulty of each word , and thus offers a good starting point for understanding how children use distributional context for vocabulary acquisition . The fact that the correlation is positive prompts the prediction that , when co - occurrence frequency is incorporated in the model , words inhabiting less dense neighbourhoods are acquired earlier . This finding suggests that semantic neighbours may act as competitors in the process of word learning . Among the hyperparameters of these models , one that is particularly relevant to language acquisition is the window size , as this reveals the amount of context that children most likely attend to in the analyzed ages . To investigate this , we took the best model of our previous analyses ( SGNS with window size 1 , negative sampling 15 , frequency threshold 10 ) , and varied only the window size . Results are in figure 2 . As can be seen , smaller window sizes have better correlation with the data , indicating that the exploited context at this age is very local . Such a result makes intuitive sense in the context of children 's immature verbal memory spans , which only improve as they acquire more language .", "entities": [[52, 53, "DatasetName", "0"], [84, 85, "DatasetName", "lemma"], [342, 344, "TaskName", "language acquisition"], [434, 436, "TaskName", "language acquisition"], [499, 500, "TaskName", "binarization"], [590, 591, "HyperparameterName", "\u03b8"], [911, 913, "TaskName", "language acquisition"]]}
{"text": "We implemented a neural machine translation system that uses automatic sequence tagging to improve the quality of translation . Instead of operating on unannotated sentence pairs , our system uses pre - trained tagging systems to add linguistic features to source and target sentences . Our proposed neural architecture learns a combined embedding of tokens and tags in the encoder , and simultaneous token and tag prediction in the decoder . Compared to a baseline with unannotated training , this architecture increased the BLEU score of German to English film subtitle translation outputs by 1.61 points using named entity tags ; however , the BLEU score decreased by 0.38 points using part - of - speech tags . This demonstrates that certain token - level tag outputs from off - theshelf tagging systems can improve the output of neural translation systems using our combined embedding and simultaneous decoding extensions .", "entities": [[4, 6, "TaskName", "machine translation"], [83, 85, "MetricName", "BLEU score"], [104, 106, "MetricName", "BLEU score"], [111, 114, "DatasetName", "part - of"]]}
{"text": "Neural machine translation ( NMT ) uses neural networks to translate unannotated text between a source and target language , but without additional linguistic information certain ambiguous inputs may be translated incorrectly . Consider the following examples : 1 ) Titanic struggles between good and evil . \uc120\uacfc \uc545 \uc0ac\uc774\uc758 \uc5c4\uccad\ub09c \ud22c\uc7c1 . big fight between good and evil \ud0c0\uc774\ud0c0\ub2c9\uc740 \uc120\uacfc \uc545 \uc0ac\uc774\uc5d0\uc11c \ud22c\uc7c1 \uc911\uc774\ub2e4 . The Titanic is fighting between good and evil 2 ) Titanic struggles to stay afloat . \ud0c0\uc774\ud0c0\ub2c9\uc740 \uce68\ubab0\ud558\uc9c0 \uc54a\ub3c4\ub85d \uace0\uad70\ubd84\ud22c \uc911\uc774\ub2e4 . The Titanic is struggling not to sink \uce68\ubab0\ud558\uc9c0 \uc54a\uae30 \uc704\ud55c \uc5c4\uccad\ub09c \ud22c\uc7c1 . big fight not to sink In ( 1 ) , \" Titanic \" is best translated as a common adjective ; in ( 2 ) , it most likely refers to a named entity , the famous ship . In addition to the bare token sequences , part - of - speech or named entity annotation of each token , provided manually or automatically , could provide additional information to improve the quality of translation . Natural language processing ( NLP ) tools have benefited from the same explosion in deep learning and neural network developments that has spurred NMT . NLP tools include part - of - speech ( POS ) taggers , identifying the syntactic function of each input token , and named entity recognition systems . Named entity recognition ( NER ) identifies which tokens refer to named entities , including proper nouns such as people , place names , organizations , or dates . Recently , automatic named entity recognition ( NER ) systems have seen much development and refinement with the same deep learning tools used for NMT ( Li et al , 2020 ) . Automatic neural NER systems have achieved accuracy exceeding 92 % F 1 scores in many languages and domains ( Wang et al , 2019 ; Akbik et al , 2018 ) . NER tags produced by these systems are useful in many other natural language processing contexts , such as coreference resolution , entity linking , or entity extraction ( Ferreira Cruz et al , 2020 ) . POS taggers have also achieved very high accuracy exceeding 98 % on public treebank datasets ( Akbik et al , 2018 ) . We aim to use tags from publicly available pre - trained tagging systems as additional features to improve NMT training and output . Tag assisted NMT requires modifications to the neural architecture to accommodate a tag at each token position . The encoder must learn an embedding that combines information from each token and its tag , then compute a hidden state from these embeddings . The decoder must learn to predict tokens and their tags simultaneously from the decoder state . Adding tag information to the predic - tion and corresponding training loss encourages the model to incorporate this information into its latent representations to improve outputs . Compared to an untagged baseline system on word - tokenized data , our tagged translation system improved the BLEU score by 1.61 points on German to English parallel film subtitles data tagged with publicly available pre - trained named entity recognition systems , while part - of - speech tagging decreased the score by 0.38 BLEU points . Subword tokenization reduced these effects to +0.22 points and - 0.22 points respectively . Nonetheless , this demonstrates the feasibility of using certain pre - trained tagging outputs to improve translation quality .", "entities": [[1, 3, "TaskName", "machine translation"], [148, 151, "DatasetName", "part - of"], [205, 208, "DatasetName", "part - of"], [225, 228, "TaskName", "named entity recognition"], [230, 233, "TaskName", "Named entity recognition"], [234, 235, "TaskName", "NER"], [262, 265, "TaskName", "named entity recognition"], [266, 267, "TaskName", "NER"], [294, 295, "TaskName", "NER"], [298, 299, "MetricName", "accuracy"], [324, 325, "TaskName", "NER"], [342, 344, "TaskName", "coreference resolution"], [345, 347, "TaskName", "entity linking"], [367, 368, "MetricName", "accuracy"], [476, 477, "MetricName", "loss"], [510, 512, "MetricName", "BLEU score"], [530, 533, "TaskName", "named entity recognition"], [536, 542, "TaskName", "part - of - speech tagging"], [547, 548, "MetricName", "BLEU"]]}
{"text": "The decoder state d i at each step is conditioned on the target prefix and the encoded source sentence ( 3 ) . d i = Decoder ( prefix , src ) ( 3 ) This shared decoder state is used to predict both the next token and the next tag , with token and tag feature projections T and \u03c4 ( 4 and 5 ) . P ( token k | prefix ; src ) = softmax k ( T d i ) ( 4 ) P ( tag k | prefix ; src ) = softmax k ( \u03c4 d i ) ( 5 ) We model these probabilities independently ( 6 ) for the same data sparsity and model size reasons as the embeddings , and we can compute each pair probability and loss accordingly ( 7 ) . P ( token , tag | prefix ; src ) = P ( token | pre . ; src ) P ( tag | pre . ; src ) ( 6 ) L = \u2212 log P ( token | prefix ; src ) \u2212 log P ( tag | prefix ; src ) ( 7 ) This combined loss encourages the shared decoder state d i to model the correct tag identity so that it can be used by the token prediction layer to improve translation . 4 Data Preparation", "entities": [[77, 78, "MethodName", "softmax"], [97, 98, "MethodName", "softmax"], [136, 137, "MetricName", "loss"], [201, 202, "MetricName", "loss"]]}
{"text": "We used a Transformer encoder and decoder ( Vaswani et al , 2017 ) for the base seq2seq system , each with 6 layers and 8 attention heads , and layer and embedding dimensions 512 . Training was done for 40 epochs at half precision with the optimizer known as Adam ( Kingma and Ba , 2015 ) with \u03b2 = ( 0.9 , 0.98 ) and an inverse square root learning schedule with maximum learning rate 5 \u00d7 10 \u22124 after 500 updates and decay 1 \u00d7 10 \u22124 . Parameter updates occurred after every 8 , 192 token - tag pairs at most ( rounding off to complete sentences ) , with 30 % dropout and label smoothing of 0.1 on the training loss . At inference time , a beam of 5 candidates was maintained , and the models were evaluated with their BLEU score on the token sequence only ( tagging accuracy was not evaluated due to the difficulty of establishing alignment ) .", "entities": [[3, 4, "MethodName", "Transformer"], [17, 18, "MethodName", "seq2seq"], [47, 48, "HyperparameterName", "optimizer"], [50, 51, "MethodName", "Adam"], [59, 60, "HyperparameterName", "\u03b2"], [75, 77, "HyperparameterName", "learning rate"], [118, 120, "MethodName", "label smoothing"], [125, 126, "MetricName", "loss"], [146, 148, "MetricName", "BLEU score"], [155, 156, "MetricName", "accuracy"]]}
{"text": "BLEU scores from untagged and tagged translation experiments show an improvement from the use of NER tags ( Table 1 ) . Adding NER tags , the 3 baseline 4 enhanced baseline / ablation study 5 ablation study BLEU score on sentences containing some named entities improved by a larger margin , 3.07 points , presumably due to the tags ' assistance with translating those named entities . We also note an improvement in the BLEU score on sentences containing no named entities , which increased by 1.14 points . This suggests that given O tag information the model can also treat common words with confidence that they are not named entities and should not be translated as such . These improvements averaged out to a net gain of 1.61 BLEU points on the entire test split . We also evaluated a model trained with POS tags , but found a decrease in BLEU score ( Table 2 ) . Translation scores with POS tags decreased by 0.38 BLEU points . There are two ways to understand this in comparison with NER tags . First , POS tags carry a significant amount of information about the sentence , not only helping to disambiguate between different word senses by part - of - speech , but also assisting the model with encoding the sentence 's syntactic structure . Compared to NER tags , this amount of structural information might be difficult to model with the same decoder architecture used for token prediction . Second , POS tags tend to carry the same amount of information for each tag at each position , compared to NER tags only conveying most of their information at the named entity spans which are few and far between . This also lends itself to the idea that POS tags have a higher information content that is less easily modeled by the decoder , leading to worse results than NER tagging .", "entities": [[0, 1, "MetricName", "BLEU"], [15, 16, "TaskName", "NER"], [23, 24, "TaskName", "NER"], [38, 40, "MetricName", "BLEU score"], [75, 77, "MetricName", "BLEU score"], [130, 131, "MetricName", "BLEU"], [153, 155, "MetricName", "BLEU score"], [160, 161, "TaskName", "Translation"], [168, 169, "MetricName", "BLEU"], [181, 182, "TaskName", "NER"], [208, 211, "DatasetName", "part - of"], [229, 230, "TaskName", "NER"], [273, 274, "TaskName", "NER"], [322, 323, "TaskName", "NER"]]}
{"text": "For both NER and POS tagged results , the baseline was the same Transformer architecture trained only on untagged data ( without adding tag embeddings or predicting tags from the decoder ) . Adding in only source - side tag embeddings could be considered an enhanced baseline , since this kind of ( Sennrich and Haddow , 2016 ; Hoang et al , 2016b ) . Our results show that this source - only tagging does not provide significant benefits compared to training on untagged data ( Table 1 ) , although for POS tagging this remains the best result . On the other hand , adding in target - side tags while also predicting them from the decoder , without adding in source - side tag embeddings could be considered an ablation test to isolate the effects of our main contribution : target - side tag decoding . Our results show that this target tagging provides the same benefit as the fully tagged training regime , demonstrating that it is the simultaneous tag decoding that accounts for the entire effect observed . For NER tagging this was an improvement in BLEU scores , but for POS tagging scores decreased when adding target tagging . Whereas source - side tag information is added into the embeddings without any modification to the training objective , target - side tag predictions are a part of the modified training loss , so that it is the target - side tag prediction that pushes the model to incorporate accurate knowledge of the tags into its learning representations . That NER tag modeling improved results while POS tag modeling did not is consistent with our earlier observation that POS tag modeling seems to be more difficult than NER tag modeling , and is not done effectively by the current architecture .", "entities": [[2, 3, "TaskName", "NER"], [13, 14, "MethodName", "Transformer"], [184, 185, "TaskName", "NER"], [191, 192, "MetricName", "BLEU"], [236, 237, "MetricName", "loss"], [265, 266, "TaskName", "NER"], [292, 293, "TaskName", "NER"]]}
{"text": "Experiments with subword tokenized data showed similar effects , but of a significantly reduced size . Adding NER tags improved the results , adding 0.22 points to the BLEU score , with the improvement again coming largely from the target side tagging , and again showing a larger improvement on sentences with named entities than on those without ( Table 3 ) . Adding POS tags hurt results , decreasing the score by 0.22 , and again we see that source - only tagging is best case for POS tagging ( Table 4 ) . However , the reduced magnitude of these deltas to the range of 0.1 - 0.4 BLEU points suggests these are not significant changes to the translation performance , in the subword tokenization case . It would appear that subword tokenization interferes with the benefits of tagging the data . Since tags are aligned one - to - one with the input words , subword tokenization destroys this alignment , and copying tags across a word 's constituent subwords may interfere with the model 's ability to make sense the of tag information . In particular for named entities , rare words are likely to tokenized into a larger number of subword tokens , exacerbating this effect . The set of embeddings for the subwords in a word may not be as useful to the model for translating a named entity or other rare category as the single embedding learned specifically for the full word in a word tokenization setting , and further these subword embeddings may be affected by other contexts unrelated to the larger word . Specifically for the named entity case , subword tokenization algorithms might prioritize the atomicity of certain rare words tagged as named entities in order to counteract this .", "entities": [[17, 18, "TaskName", "NER"], [28, 30, "MetricName", "BLEU score"], [110, 111, "MetricName", "BLEU"]]}
{"text": "Due to the conditional independence assumption , the cross - entropy loss ( 7 ) conveniently decomposes into separate terms for tokens and tags ( 8 ) , allowing us to measure the relative information content of each channel ( Table 5 ) . While adding tag information naturally increases the overall cross - entropy , as there are more possibilities to account for and to be predicted , restricting our attention only to the token loss shows that the token - level cross - entropy is consistently reduced from 2.000 ( base - 2 ) to 1.985 with NER tags or 1.972 for POS tags . This shows how both tag types can add disambiguating information to the token prediction process , with POS tags naturally add more of such information , since they carry syntactic information . Looking only at tag - level cross - entropy , it 's interesting to notice that the POS tagging loss is significantly higher than the NER tagging loss . While this could be simply because the lower - bound inherent entropy is higher ( POS tags naturally contain more information , being more uniformly distributed than NER tags ) , this could also be consistent with the idea that POS tag modeling is more difficult , explaining the decreased translation scores observed with POS tag prediction .", "entities": [[11, 12, "MetricName", "loss"], [76, 77, "MetricName", "loss"], [99, 100, "TaskName", "NER"], [158, 159, "MetricName", "loss"], [164, 165, "TaskName", "NER"], [166, 167, "MetricName", "loss"], [195, 196, "TaskName", "NER"]]}
{"text": "We implemented extensions to existing neural machine translation models that allow the use of offthe - shelf token - level tagging systems to improve translation accuracy . Translation inputs and training outputs were tagged with pre - trained sequence labeling systems . A standard encoder - decoder architecture was extended to include tag embeddings and tag prediction at each token position . At model input , token and tag embedding vectors were added to produce a combined embedding . At model output , the final decoder layer used separate softmax layers to predict tokens and tags . During training , a combined loss function encouraged the model to learn token and tag information jointly . This tag assisted translation system was tested against baseline token - only systems on a German to English film subtitle corpus with both word and subword tokenization . Subword tokenization reduced the size of the effect , suggesting the need for specialized subword tokenization to prioritize the integrity of important word categories . However , on word tokenized data , the 1.61 point increase in BLEU score using named entity tags demonstrates that the proposed architecture is useful for improving translation outputs with automatic named entity recognition , while the 0.38 point decrease using part - of - speech tags indicates more difficulty in utilizing that tag information . Further examination of the cross - entropy showed that adding tags reduced the token cross - entropy thereby improving token modeling . Future experiments can explore the use of other types of tag data as well as other decoding paradigms .", "entities": [[6, 8, "TaskName", "machine translation"], [25, 26, "MetricName", "accuracy"], [27, 28, "TaskName", "Translation"], [89, 90, "MethodName", "softmax"], [102, 103, "MetricName", "loss"], [180, 182, "MetricName", "BLEU score"], [199, 202, "TaskName", "named entity recognition"], [209, 212, "DatasetName", "part - of"]]}
{"text": "The next step was to train a classifier using the Labelled Corpus as training data , so that the resulting model could be deployed on the Raw Corpus . Our goal is to obtain automatic predictions for the relevance of each tweet in this corpus , according to probabilities given by our model . We created ( stratified ) test and training sets that maintain the same proportion of relevant and irrelevant tweets associated with each query word in the Labelled Corpus . We chose to include 80 % of these tweets in the training set and 20 % in the test set ( see Table 1 Using the AffectiveTweets package ( Bravo - Marquez et al , 2019 ) , our labelled tweets were transformed into feature vectors based on the word n - grams they contain . We then trained various classification models on this transformed data in Weka ( Hall et al , 2009 ) . The models we tested were 1 ) Multinomial Naive Bayes ( McCallum et al , 1998 ) with unigram attributes and 2 ) L2regularised logistic regression models with different word n - gram features , as implemented in LIB - LINEAR 2 . We selected Multinomial Naive Bayes as the best model because it produced the highest AUC , Kappa and weighted average F - Score ( see Table 2 for a summary of results ) . Overall , logistic regression with unigrams performed the worst , yielding ( slightly ) lower values for all three measures . After deploying the Multinomial Naive Bayes model on the Raw Corpus , we found that 1 , 179 , 390 tweets were classified as relevant and 448 , 652 as irrelevant ( with probability threshold = 0.5 ) . Table 3 shows examples from our corpus of each type of classification . Some tweets were falsely classified as \" irrelevant \" and some were falsely classified as \" relevant \" . A short explanation why the irrelevant tweets were coded as such is given in brackets at the end of each tweet . We removed all tweets classified as irrelevant , thereby producing the Processed Corpus . A summary of all three corpora is given in Table 4 .", "entities": [[183, 185, "MethodName", "logistic regression"], [216, 217, "MetricName", "AUC"], [224, 225, "MetricName", "Score"], [238, 240, "MethodName", "logistic regression"]]}
{"text": "Semantic Dependency Parsing ( SDP ) is defined as the task of recovering sentence - internal bilexical semantic dependency structures , which encode predicate - argument relationships for all content words . Such sentence - level semantic analysis of text is concerned with the characterization of events and is therefore important to understand the essential meaning of a natural language sentence . With the advent of many supporting resources , SDP has become a well - defined task with a substantial body of work and comparative evaluation . ( Almeida and Martins , 2015 ; Du et al , 2015a ; Zhang et al , 2016 ; Peng et al , 2017 ; Wang et al , 2018 ) . Two SDP shared tasks have been run as part of the 2014 and 2015 International Workshops on Semantic Evaluation ( SemEval ) ( Oepen et al , 2014 ( Oepen et al , , 2015 . There are two key dimensions of the data - driven dependency parsing approach : decoding and disambiguation . Existing decoding approaches to syntactic or semantic analysis into bilexical dependencies can be categorized into two dominant types : transition - based ( Zhang et al , 2016 ; Wang et al , 2018 ) and graph - based , i.e. , Maximum Subgraph ( Kuhlmann and Jonsson , 2015 ; Cao et al , 2017a ) approaches . For disambiguation , while early work on dependency parsing focused on global linear models , e.g. , structured perceptron ( Collins , 2002 ) , recent work shows that deep learning techniques , e.g. , LSTM ( Hochreiter and Schmidhuber , 1997 ) , is able to significantly advance the state - of - the - art of the parsing accuracy . From the above two perspectives , i.e. , the decoding and disambiguation frameworks , we find that what is still underexploited is neural Maximum Subgraph parsing for highly constrained graph classes , e.g. , noncrossing graphs . In this paper , we fill this gap in the literature by developing a neural Maximum Subgraph parser . Previous work showed that the 1 - endpointcrossing , pagenumber - 2 ( 1EC / P2 ) graphs are an appropriate graph class for modeling semantic dependency structures ( Cao et al , 2017a ) . In this paper , we build a parser that targets 1EC / P2 graphs . Based on an efficient first - order Maximum Subgraph decoder , we implement a data - driven parser that scores arcs based on stacked bidirectional - LSTM ( BiLSTM ) together with a multi - layer perceptron . Using the benchmark data sets from the SemEval 2015 Task 18 ( Oepen et al , 2015 ) , our parser gives very competitive results for English semantic parsing . To test the ability for crosslingual parsing , we also conduct experiments on the Chinese CCGBank ( Tse and Curran , 2010 ) and Enju HPSGBank ( Yu et al , 2010 ) data . Our parser plays equally well for Chinese , resulting in an error reduction of 23.5 % and 9.4 % over the best published result reported in Zhang et al ( 2016 ) and Du et al ( 2015b ) . Most studies on semantic parsing focused on the in - domain setting , meaning that both training and testing data are drawn from the same domain . Even a data - driven parsing system achieves a high in - domain accuracy , it usually performs rather poorly on the out - of - domain data ( Oepen et al , 2015 ) . How to build robust semantic dependency parsers that can learn across domains remains an under - addressed problem . To improve the cross - domain parsing performance , we propose a data - oriented model to explore the linguistic generality encoded in a hand - crafted , domainindependent , linguistically - precise English grammar , namely English Resource Grammar ( ERG ; Flickinger , 2000 ) . In particular , we introduce a cost - sensitive training model to learn crossdomain semantic information implicitly encoded in WikiWoods ( Flickinger et al , 2010 ) , i.e. , a corpus that collects the wikipedia 1 texts as well as their automatic syntactico - semantic annotations produced by ERG . Evaluation demonstrates the usefulness of the imperfect annotations automatically created by ERG . Our parser is available at https://github . com / draplater / msg - parser .", "entities": [[1, 3, "TaskName", "Dependency Parsing"], [166, 168, "TaskName", "dependency parsing"], [240, 242, "TaskName", "dependency parsing"], [268, 269, "MethodName", "LSTM"], [293, 294, "MetricName", "accuracy"], [428, 429, "MethodName", "LSTM"], [430, 431, "MethodName", "BiLSTM"], [441, 443, "DatasetName", "the benchmark"], [467, 469, "TaskName", "semantic parsing"], [485, 486, "DatasetName", "CCGBank"], [548, 550, "TaskName", "semantic parsing"], [585, 586, "MetricName", "accuracy"]]}
{"text": "In order to update graphs which achieve high model scores but are actually wrong , we use a margin - based approach to compute loss from the gold graph G * and the best prediction\u011c under current model . We define the loss term as : max ( 0 , \u2206 ( G * , \u011c ) \u2212 SCORE ( G * ) + SCORE ( \u011c ) ) The margin objective \u2206 measures the similarity between the gold graph G * and the prediction G. Follow Peng et al ( 2017 ) 's approach , we define \u2206 as weighted Hamming to trade off between precision and recall . Banarescu et al , 2013 ) . Different from data - driven syntactic parsing , semantic parsing for the first type of annotation can leverage a precision grammar - guided model . Such a model applies a rich set of precise linguistic rules to constrain their search for a preferable syntactic or semantic analysis . In recent years , several of these linguistically motivated parsing systems achieved high performances that are comparable or even superior to the treebank - based purely data - driven parsers . For example , using ERG ( Flickinger , 2000 ) , which provides precise linguistic analyses for a broad range of phenomena , as the the core engine , PET 2 ( Callmeier , 2000 ) and ACE 3 produce better results than all existing datadriven semantic parsers for sentences that can be parsed by ERG . The main weakness of the precision grammarguided parsers is their robustness with respect to both coverage and efficiency . Even for treebanking on the newswire data , i.e. , the Wall Street Journal data from Penn TreeBank , ERG lacks analyses for c.a . 11 % sentences ( Oepen et al , 2015 ) . For the texts from the web , e.g. , tweets , this problem is much more serious . Moreover , checking all linguistic constraints makes a grammar - guided parser too slow for many realistic NLP applications . On the contrary , light - weight , data - driven parsers usually have complementary strengthes in terms of both coverage and efficiency .", "entities": [[24, 25, "MetricName", "loss"], [42, 43, "MetricName", "loss"], [48, 49, "DatasetName", "0"], [125, 127, "TaskName", "semantic parsing"], [225, 226, "DatasetName", "PET"], [288, 290, "DatasetName", "Penn TreeBank"]]}
{"text": "In this paper , we introduce a new data - oriented strategy to consume a precision grammar . The key idea is to take a grammar as an imperfect annotator : We let a precision grammar - guided parser parse large - scale raw texts in an offline way , and then utilize the automatically generated analysis as imperfect training data . Because we only need raw texts to be parsed once , even if this process takes much time , it is still reasonable . A grammarguided parser can not parse a considerable portion of data , but this will not cause serious problems because we can take an enormous amount of sentences as annotation candidates . Just considering the wikipedia , we can collect at least dozens of millions of comparatively high - quality sentences . An essential problem of this method is that such imperfect annotations bring in annotation errors which may hurt parser training . To deal with this problem , we adopted a cost - sensitive training method to train our model on the extended training data . In each epoch , we trained on imperfect corpus first and then on gold - standard corpus . When processing an imperfect sentence , we do not take a loss into consideration if the loss of this sentence is too small . In particular , if a loss of a bilexical relation between two tokens is less than 0.05 , we would exclude the loss . As for label assigning , we exclude losses less than 0.5 . These threshold numbers are tuned on the development data .", "entities": [[212, 213, "MetricName", "loss"], [217, 218, "MetricName", "loss"], [230, 231, "MetricName", "loss"], [247, 248, "MetricName", "loss"]]}
{"text": "To evaluate neural Maximum Subgraph parsing in practice , we first conduct experiments on the three English data sets , namely DM , PAS and PSD 4 , which are from the SemEval 2015 Task18 ( Oepen et al , 2015 ) . We use the \" standard \" training , validation , and test splits to facilitate comparisons . In other words , the data splitting policy follows the shared task . In addition to English parsing , we consider Chinese SDP and use two data sets : ( 1 ) Chinese PAS data provided by SemEval 2015 , and ( 2 ) Chinese CCGBank ( Tse and Curran , 2010 ) to evaluate the cross - lingual ability of our model . All the SemEval data sets are publicly available from LDC ( Oepen et al , 2016 ) . We use DyNet 5 to implement our neural models . We use the automatic batch technique ( Neubig et al , 2017 ) in DyNet to perform mini - batch gradient descent training . The batch size is 32 . The detailed network hyper - parameters are summarized in Table 2 . We use the same pre - trained word embedding as Kiperwasser and Goldberg ( 2016 ) .", "entities": [[105, 106, "DatasetName", "CCGBank"], [177, 179, "HyperparameterName", "batch size"]]}
{"text": "Table 1 lists the parsing accuracy of our system as well as the best published results in the literature for comparison . Results from other papers are of different yet representative decoding or disambiguation frameworks . Du et al ( 2015a ) sion of the two linear model - based parsers is comparable or even superior to our neural parser , but the recall is far behind .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "Ensemble methods have been shown very helpful to boost the accuracy of neural network based parsing . We evaluate two ensemble methods , voting and score averaging . In the voting method , each model parses the sentence to graph respectively . An edge will exist on the combined graph only if more than half output graphs of these models contain this edge . The label of this edge will be the most common label . In the score averaging method , we use averaged score parts to get a maximum graph and classify labels . We choose 3/10 kind of different initial parameters to train models for ensemble . Figure 5 shows the result of the two ensemble methods . The averaging method has slightly better performance on the 3 datasets . The performance of this method on test data is shown on Table 1 .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "To test the ability for cross - lingual parsing , we conduct experiments on HPSG and CCG grounded semantic analyses respectively . The HPSG grounded analysis is provided by SemEval 2015 and the underlying framework is the same to the English PAS data . The CCG grounded analysis is from Chinese CCGBank . We use the same set - up as Zhang et al ( 2016 ) . Both data sets are transformed from Chinese TreeBank with two rich sets of heuristic rules ( Yu et al , 2010 ; Tse and Curran , 2010 ) . Table 4 and 5 Chinese POS tagging has a great impact on parsing . In this paper , we consider two POS taggers : a symbol - refined generative HMM tagger ( SR - HMM ) ( Huang et al , 2009 ) and a BiLSTM - CRF model when assisting Chinese SDG . For the neural tagging model , in addition to a BiL - STM layer for encoding words , we set a BiLSTM layer for encoding characters , which supports us to derive character - level representations for all words . In particular , vectors from the characterlevel LSTM is concatenated with the pre - trained word embedding before feeding into the other word - level BiLSTM network to capture contextual information . The final module of our CRF tagger is a linear chain CRF which scores the output sequence by factoring it in local tag bi - grams . From Table 5 , we can see that POS information is very important to Chinese SDP . This phenomenon is consist with Chinese syntactic parsing , including both constituency and dependency parsing . Mandarin Chinese is recognized as a morphology - poor language : POS tags are defined mainly according to words ' distributional rather than morphological properties . \" ZDSW \" is the system that obtained the best parsing accuracy on the Chinese CCGBank data in the literature . the power of the RNN architecture to learn nonlocal dependencies and thus benefit our semantic dependency parser a lot .", "entities": [[51, 52, "DatasetName", "CCGBank"], [74, 76, "DatasetName", "Chinese TreeBank"], [142, 143, "MethodName", "BiLSTM"], [144, 145, "MethodName", "CRF"], [172, 173, "MethodName", "BiLSTM"], [198, 199, "MethodName", "LSTM"], [216, 217, "MethodName", "BiLSTM"], [228, 229, "MethodName", "CRF"], [234, 235, "MethodName", "CRF"], [280, 282, "TaskName", "dependency parsing"], [320, 321, "MetricName", "accuracy"], [324, 325, "DatasetName", "CCGBank"]]}
{"text": "Parsing sentences to linguistically - rich semantic representations is a key goal of Natural Language Understanding . We introduce a new parser for semantic dependency analysis , which combines two promising parsing techniques , i.e. , decoding based on Maximum Subgraph algorithms and disambiguation based on BiLSTMs . To our knowledge , this is the first neural Maximum Subgraph parser . Our parser significantly improves state - ofthe - art accuracy on three out of total four data sets from SemEval 2015 for English / Chinese parsing and the CCGBank data for Chinese parsing . We also propose a new data - oriented method to leverage ERG , a linguistically - motivated , hand - crafted grammar , to improve cross - domain performance . Experiments demonstrate the effectiveness of taking ERG as an imperfect annotator . We think this method can be re - used for other types of datadriven semantic parsing models .", "entities": [[13, 16, "TaskName", "Natural Language Understanding"], [70, 71, "MetricName", "accuracy"], [89, 90, "DatasetName", "CCGBank"], [151, 153, "TaskName", "semantic parsing"]]}
{"text": "Relation extraction ( RE ) aims to extract relational facts between two entities from plain texts . For example , with the sentence \" Hayao Miyazaki is the director of the film ' The Wind Rises ' \" , we can extract a relation \" director_of \" between two entities \" Hayao Miyazaki \" and \" The Wind Rises \" . Recent progress in supervised methods to RE has achieved great successes . Supervised methods can effectively learn significant relation semantic patterns based on existing labeled data , but the data constructions are time - consuming and human - intensive . To lower the level of supervision , several semi - supervised approaches have been developed , including bootstrapping , active learning , label propagation ( Pawar et al , 2017 ) . Mintz ( 2009 ) also proposes distant supervision to generate training data automatically . It assumes that if two entities have a relation in KBs , all sentences that contain these two entities will express this relation . Still , all these approaches can only extract pre - defined relations that have already appeared either in human - annotated datasets or KBs . It is hard for them to cover the great variety of novel relational facts in the open - domain corpora . Open relation extraction ( OpenRE ) aims to extract relational facts on the open - domain corpus , where the relation types may not be predefined . There are some efforts concentrating on extracting triples with new relation types . Banko ( 2008 ) directly extracts words or phrases in sentences to represent new relation types . However , some relations can not be explicitly represented with tokens in sentences , and it is hard to align different relational tokens that exactly have the same meanings . Yao ( 2011 ) consid - ers OpenRE as a clustering task for extracting triples with new relation types . However , previous clustering - based OpenRE methods ( Yao et al , 2011 ( Yao et al , , 2012Marcheggiani and Titov , 2016 ; Elsahar et al , 2017 ) are mostly unsupervised , and can not effectively select meaningful relation patterns and discard irrelevant information . In this paper , we propose to take advantage of high - quality supervised data of pre - defined relations for OpenRE . The approach is non - trivial , however , due to the considerable gap between the pre - defined relations and novel relations of interest in open domain . To bridge the gap , we propose Relational Siamese Networks ( RSNs ) to learn transferable relational knowledge from supervised data for OpenRE . Specifically , RSNs learn relational similarity metrics from labeled data of pre - defined relations , and then transfer the metrics to measure the similarity of unlabeled sentences for open relation clustering . We describe the flowchart of our framework in Figure 1 . Moreover , we show that RSNs can also be generalized to various weakly - supervised scenarios . We propose Semi - supervised RSN to learn from both supervised data of pre - defined relations and unsupervised data with novel relations , and Distantly - supervised RSN to learn from distantly - supervised data and unsupervised data . We conduct experiments on real - world RE datasets , FewRel and FewRel - distant , by splitting relations into seen and unseen set , and evaluate our models in supervised , semi - supervised , and distantly - supervised scenarios . The results demonstrate that our models significantly outperform state - of - the - art baseline methods in all scenarios without using external linguistic tools . To summarize , the main contributions of this work are as follows : ( 1 ) We develop a novel relational knowledge transfer framework RSN for OpenRE , which can effectively transfer existing relational knowledge to novel - relation data and accurately identify novel relations . To the best of our knowledge , RSN is the first model to consider knowledge transfer in clustering - based OpenRE task . ( 2 ) We further propose Semi - supervised RSNs and Distantly - supervised RSNs that can learn from various weakly supervised scenarios . The experimental results show that all these RSN models achieve significant improvements in F - measure compared with state - of - the - art baselines .", "entities": [[0, 2, "TaskName", "Relation extraction"], [120, 122, "TaskName", "active learning"], [218, 220, "TaskName", "relation extraction"], [560, 561, "DatasetName", "FewRel"], [562, 563, "DatasetName", "FewRel"], [724, 727, "MetricName", "F - measure"]]}
{"text": "The architecture of our Relational Siamese Networks is shown in Figure 2 . CNN modules encode a pair of relational instances into vectors , and several shared layers compute their similarity . Sentence Encoder . We use a CNN module as the sentence encoder . The CNN module includes an embedding layer , a convolutional layer , a max - pooling layer , and a fully - connected ( FC ) layer . The embedding layer transforms the words in a sentence x and the positions of entities e head and e tail into pre - trained word embeddings and random - initialized position embeddings . Following ( Zeng et al , 2014 ) , we concatenate these embeddings to form a vector sequence . Next , a one - dimensional convolutional layer and a maxpooling layer transform the vector sequence into features . Finally , an FC layer with sigmoid activation maps features into a relational vector v. To summarize , we obtain a vector representation v for a relational sentence with our CNN module : v = CNN ( s ) , ( 1 ) in which we denote the joint information of a sentence x and two entities in it e head and e tail as a data sample s. And with paired input relational instances , we have : vl = CNN ( s l ) , vr = CNN ( sr ) , ( 2 ) in which two CNN modules are identical and share all the parameters . Similarity Computation . Next , to measure the similarity of two relational vectors , we calculate their absolute distance and transform it into a realnumber similarity p [ 0 , 1 ] . First , a distance layer computes the element - wise absolute distance of two vectors : vd = | vl \u2212 vr | . ( 3 ) Then , a classifier layer calculates a metric p for relation similarity . The layer is a one - dimensionaloutput FC layer with sigmoid activation : p = \u03c3 ( kvd + b ) , ( 4 ) in which \u03c3 denotes the sigmoid function , k and b denote the weights and bias . To summarize , we obtain a good similarity metric p of relational instances . Cross Entropy Loss . The output of RSN p can also be explained as the probability of two sentences mentioning two different relations . Thus , we can use binary labels q and binary cross entropy loss to train our RSN : L l = E d l \u223cD l [ q ln ( p \u03b8 ( d l ) ) + ( 1 \u2212 q ) ln ( 1 \u2212 p \u03b8 ( d l ) ) ] , ( 5 ) in which \u03b8 indicates all the parameters in the RSN .", "entities": [[97, 99, "TaskName", "word embeddings"], [150, 152, "MethodName", "sigmoid activation"], [282, 283, "DatasetName", "0"], [337, 339, "MethodName", "sigmoid activation"], [419, 420, "MetricName", "loss"], [438, 439, "HyperparameterName", "\u03b8"], [455, 456, "HyperparameterName", "\u03b8"], [468, 469, "HyperparameterName", "\u03b8"]]}
{"text": "To discover relation clusters in the open - domain corpus , it is beneficial to not only learn from labeled data , but also capture the manifold of unlabeled data in the semantic space . To this end , we need to push the decision boundaries away from high - density areas , which is known as the cluster assumption ( Chapelle and Zien , 2005 ) . We try to achieve this goal with several additional loss functions . In the following paragraphs , we denote the labeled training dataset as D l and a couple of labeled relational instances as d l . Similarly , we denote the unlabeled training dataset as D u and a couple of unlabeled instances as d u . Conditional Entropy Loss . In classification problems , a well - classified embedding space usually reserves large margins between different classified clusters , and optimizing margin can be a promising way to facilitate training . However , in clustering problems , type labels are not available during training . To optimize margin without explicit supervision , we can push the data points away from the decision boundaries . Intuitively , when the distance similarity p between two relational instances equals 0.5 , there is a high prob - ability that at least one of two instances is near the decision boundary between relation clusters . Thus , we use the conditional entropy loss ( Grandvalet and Bengio , 2005 ) , which reaches the maximum when p = 0.5 , to penalize close - boundary distribution of data points : Lu = E du\u223cDu [ p \u03b8 ( du ) ln ( p \u03b8 ( du ) ) + ( 1 \u2212 p \u03b8 ( du ) ) ln ( 1 \u2212 p \u03b8 ( du ) ) ] . ( 6 ) Virtual Adversarial Loss . Despite its theoretical promise , conditional entropy minimization suffers from shortcomings in practice . Due to neural networks ' strong fitting ability , a very complex decision hyperplane might be learned so as to keep away from all the training samples , which lacks generalizability . As a solution , we can smooth the relational representation space with locally - Lipschitz constraint . To satisfy this constraint , we introduce virtual adversarial training ( Miyato et al , 2016 ) on both branches of RSN . Virtual adversarial training can search through data point neighborhoods , and penalize most sharp changes in distance prediction . For labeled data , we have L vl = E d l \u223cD l [ DKL ( p \u03b8 ( d l ) | | p \u03b8 ( d l , t1 , t2 ) ) ] , ( 7 ) in which D KL indicates the Kullback - Leibler divergence , p \u03b8 ( d l , t 1 , t 2 ) indicates a new distance estimation with perturbations t 1 and t 2 on both input instances respectively . Specifically , t 1 and t 2 are worst - case perturbations that maximize the KL divergence between p \u03b8 ( d l ) and p \u03b8 ( d l , t 1 , t 2 ) with a limited length . Empirically , we approximate the perturbations the same as the original paper ( Miyato et al , 2016 ) . Specifically , we first add a random noise to the input , and calculate the gradient of the KL - divergence between the outputs of the original input and the noisy input . We then add the normalized gradient to the original input and get the perturbed input . And for unlabeled data , we have Lvu = E du\u223cDu [ DKL ( p \u03b8 ( du ) | | p \u03b8 ( du , t1 , t2 ) ) ] , ( 8 ) in which the perturbations t 1 and t 2 are added to word embeddings rather than the words themselves . To summarize , we use the following loss function to train Semi - supervised RSN , which learns from both labeled and unlabeled data : L all = L l + \u03bbvL vl + \u03bbu ( Lu + \u03bbvLvu ) , ( 9 ) in which \u03bb v and \u03bb u are two hyperparameters .", "entities": [[77, 78, "MetricName", "loss"], [238, 239, "MetricName", "loss"], [272, 273, "HyperparameterName", "\u03b8"], [279, 280, "HyperparameterName", "\u03b8"], [289, 290, "HyperparameterName", "\u03b8"], [299, 300, "HyperparameterName", "\u03b8"], [436, 437, "HyperparameterName", "\u03b8"], [444, 445, "HyperparameterName", "\u03b8"], [471, 472, "HyperparameterName", "\u03b8"], [519, 520, "HyperparameterName", "\u03b8"], [526, 527, "HyperparameterName", "\u03b8"], [626, 627, "HyperparameterName", "\u03b8"], [633, 634, "HyperparameterName", "\u03b8"], [659, 661, "TaskName", "word embeddings"], [674, 675, "MetricName", "loss"]]}
{"text": "To alleviate the intensive human labor for annotation , the topic of distantly - supervised learning has attracted much attention in RE . Here , we propose Distantly - supervised RSN , which can learn from both distantly - supervised data and unsupervised data for relational knowledge transfer . Specifically , we use the following loss function : L all = L l + \u03bbu ( Lu + \u03bbvLvu ) , ( 10 ) which treats auto - labeled data as labeled data but removes the virtual adversarial loss on the autolabeled data . The reason to remove the loss is simple : virtual adversarial training on auto - labeled data can amplify the noise from false labels . Indeed , we do find that the virtual adversarial loss on autolabeled data can harm our model 's performance in experiments . We do not use more denoising methods , since we think RSN has some inherent advantages of tolerating such noise . Firstly , the noise will be overwhelmed by the large proportion of negative sampling during training . Secondly , during clustering , the prediction of a new relation cluster is based on areas where the density of relational instances is high . Outliers from noise , as a result , will not influence the prediction process so much .", "entities": [[55, 56, "MetricName", "loss"], [88, 89, "MetricName", "loss"], [99, 100, "MetricName", "loss"], [128, 129, "MetricName", "loss"], [146, 147, "TaskName", "denoising"]]}
{"text": "After RSN is learned , we can use RSN to calculate the similarity matrix of testing instances . With this matrix , several clustering methods can be applied to extract new relation clusters . Hierarchical Agglomerative Clustering . The first clustering method we adopt is hierarchical agglomerative clustering ( HAC ) . HAC is a bottomup clustering algorithm . At the start , every testing instance is regarded as a cluster . For every step , it agglomerates two closest instances . There are several criteria to evaluate the distance between two clusters . Here , we adopt the complete - linkage criterion , which is more robust to extreme instances . However , there is a significant shortcoming of HAC : it needs the exact number of clusters in advance . A potential solution is to stop agglomerating according to an empirical distance threshold , but it is hard to determine such a threshold . This problem leads us to consider another clustering algorithm Louvain ( Blondel et al , 2008 ) . Louvain . Louvain is a graph - based clustering algorithm traditionally used for detecting communities . To construct the graph , we use the binary approximation of RSN 's output , with 0 indicating an edge between two nodes . The advantage of Louvain is that it does not need the number of potential clusters beforehand . It will automatically find proper sizes of clusters by optimizing community modularity . According to the experiments we conduct , Louvain performs better than HAC . After running , Louvain might produce a number of singleton clusters with few instances . It is not proper to call these clusters new relation types , so we label these instances the same as their closest labeled neighbors . Finally , we want to explain the reason why we do not use some other common clustering methods like K - Means , Mean - Shift and Ward 's ( Ward Jr , 1963 ) method of HAC : these methods calculate the centroid of several points during clustering by merely averaging them . However , the relation vectors in our model are high - dimensional , and the distance metric described by RSN is non - linear . Consequently , it is not proper to calculate the centroid by simply averaging the vectors .", "entities": [[206, 207, "DatasetName", "0"], [366, 368, "HyperparameterName", "distance metric"]]}
{"text": "Data Sampling . The input of RSN should be a pair of sampled instances . For the unlabeled set , the only possible sampling method is to select two instances randomly . For the labeled set , however , random selection would result in too many different - relation pairs , and cause severe biases for RSN . To solve this problem , we use downsampling . In our experiments , we fix the percentage of same - relation pairs in every labeled data batch as 6 % . Let us denote this percentage number as the sample ratio for convenience . Experimental results show that the sample ratio decides RSN 's tendency to predict larger or smaller clusters . In other words , it controls the granularity of the predicted relation types . This phenomenon suggests a potential application of our model in hierarchical relation extraction . However , we leave any serious discussion to future work . Hyperparameter Settings . Following ( Lin et al , 2016 ) and ( Zeng et al , 2014 ) , we fix the less influencing hyperparameters for sentence encoding as their reported optimal values . For word embeddings , we use pre - trained 50 - dimensional Glove ( Pennington et al , 2014 ) word embeddings . For position embeddings , we use randominitialized 5 - dimensional position embeddings . During training , all the embeddings are trainable . For the neural network , the number of feature maps in the convolutional layer is 230 . The filter length is 3 . The activation function after the max - pooling layer is ReLU , and the activation functions after FC layers are sigmoid . Besides , we adopt two regularization methods in the CNN module . We put a dropout layer right after the embedding layer as ( Miyato et al , 2016 ) . The dropout rate is 0.2 . We also impose L2 regularization on the convolutional layer and the FC layer , with parameters of 0.0002 and 0.001 respectively . Hyperparameters for virtual adversarial training are just the same as ( Miyato et al , 2016 ) proposed . At the same time , major hyperparameters are selected with grid search according to the model performance on a validation set . Specifically , the validation set contains 10 , 000 randomly chosen sentence pairs from the unlabeled set ( i.e. 16 novel relations ) and does not overlap with the test set . The model is evaluated according to the precision of binary classification of sentence pairs on the validation set , which is an estimation for models ' clustering ability . We do not use F1 during model validation because the clustering steps are timeconsuming . For optimization , we use Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 0.0001 , which is selected from { 0.1 , 0.01 , 0.001 , 0.0001 , 0.00001 } . The batch size is 100 selected from { 25 , 50 , 100 } . For hyperparameters in Equation 9 and Equation 10 , \u03bb v is 1.0 selected from { 0.1 , 0.5 , 1.0 , 2.0 } and \u03bb u is 0.03 selected from { 0.01 , 0.02 , 0.03 , 0.04 , 0.05 } . For baseline models , original papers do grid search for all possible hyperparameters and report the best result during testing . We follow their settings and do grid search directly on the test set .", "entities": [[145, 147, "TaskName", "relation extraction"], [195, 197, "TaskName", "word embeddings"], [214, 216, "TaskName", "word embeddings"], [263, 265, "HyperparameterName", "activation function"], [272, 273, "MethodName", "ReLU"], [324, 326, "HyperparameterName", "L2 regularization"], [449, 450, "MetricName", "F1"], [465, 466, "MethodName", "Adam"], [466, 467, "HyperparameterName", "optimizer"], [476, 478, "HyperparameterName", "learning rate"], [498, 500, "HyperparameterName", "batch size"]]}
{"text": "In this section , we demonstrate the effectiveness of our RSN models by comparing our models with state - of - the - art clustering - based OpenRE methods . We also conduct ablation experiments to detailedly investigate the contributions of different mechanisms of Semi - supervised RSN and Distantly - supervised RSN . Baselines . Conventional clustering - based OpenRE models usually cluster instances by either clustering their linguistic features ( Lin and Pantel , 2001 ; Yao et al , 2012 ; Elsahar et al , 2017 ) or imposing reconstruction constraints ( Yao et al , 2011 ; Marcheggiani and Titov , 2016 ) . To demonstrate the effectiveness of our RSN models , we compare our models with two state - of - the - art models : ( 1 ) HAC with re - weighted word embeddings ( RW - HAC ) ( Elsahar et al , 2017 ) : RW - HAC is the state - of - the - art feature clustering model for OpenRE . The model first extracts KB types and NER tags of entities as well as re - weighted word embeddings from sentences , then adopts principal component analysis ( PCA ) to reduce feature dimensionality , and finally uses HAC to cluster the concatenation of reduced feature representations . ( 2 ) Discrete - state variational autoencoder ( VAE ) ( Marcheggiani and Titov , 2016 ) : VAE is the state - of - the - art reconstruction - based model for OpenRE via unlabeled instances . It optimizes a relation classifier by reconstructing entities from pairing entities and predicted relation types . Rich features including entity words , context words , trigger words , dependency paths , and context POS tags are used to predict the relation type . RW - HAC and VAE both rely on external linguistic tools to extract rich features from plain texts . Specifically , we first align entities to Wikidata and get their KB types . Next , we preprocess the instances with part - of - speech ( POS ) tagging , named - entity recognition ( NER ) , and dependency parsing with Stanford CoreNLP . It is worth noting that these features are only used by baseline models . Our models , in contrast , only use sentences and entity pairs as inputs . Evaluation Protocol . In evaluation , we use B 3 metric ( Bagga and Baldwin , 1998 ) as the scoring function . B 3 metric is a standard measure to balance the precision and recall of clustering tasks , and is commonly used in previous OpenRE works ( Marcheggiani and Titov , 2016 ; Elsahar et al , 2017 ) . To be specific , we use F 1 measure , the harmonic mean of precision and recall . First , we report the result of supervised RSN with different clustering methods . Specifically , SN represents the original RSN structure , HAC and L indicate HAC and Louvain clustering introduced in Sec . 3.3 . The result shows that Louvain performs better than HAC , so in the following experiments we focus on using Louvain clustering . Next , for Semi - supervised and Distantlysupervised RSN , we conduct various combinations of different mechanisms to verify the contribution of each part . ( + C ) indicates that the model is powered up with conditional entropy minimization , while ( + V ) indicates that the model is pow - Experimental Result Analysis . Table 1 shows the experimental results , from which we can observe that : ( 1 ) RSN models outperform all baseline models on precision , recall , and F1 - score , among which Weakly - supervised RSN ( SN - L+CV ) achieves state - of - the - art performances . This indicates that RSN is capable of understanding new relations ' semantic meanings within sentences . ( 2 ) Supervised and distantly - supervised relational representations improve clustering performances . Compared with RW - HAC , SN - HAC achieves better clustering results because of its supervised relational representation and similarity metric . Specifically , unsupervised baselines mainly use sparse one - hot features . RW - HAC uses word embeddings , but integrates them in a rulebased way . In contrast , RSN uses distributed feature representations , and can optimize information integration process according to supervision . ( 3 ) Louvain outperforms HAC for clustering with RSN , comparing SN - HAC with SN - L. One explanation is that our model does not put additional constraints on the prior distribution of relational vectors , and therefore the relation clusters might have odd shapes in violation of HAC 's assumption . Moreover , when representations are not distinguishable enough , forcing HAC to find finegrained clusters may harm recall while contributing minimally to precision . In practice , we do observe that the number of relations SN - L extracts is constantly less than the true number 16 . ( 4 ) Both SN - L+V and SN - L+C improve the performance of supervised or distantly - supervised RSN by further utilizing unsupervised corpora . Both semi - supervised approaches bring significant improvements for F 1 scores by increasing the precision and recall , and combining both can further increase the F 1 score . ( 5 ) One interesting observation is that SN - L+V does not outperform SN - L so much on FewReldistant . This is probably because VAT on the noisy data might amplify the noise . In further experiments , we perform VAT only on unlabeled set and observe improvements on F 1 , with SN - L+V from 45.8 % to 49.2 % and SN - L+CV from 52.0 % to 52.6 % , which proves this conjecture .", "entities": [[140, 142, "TaskName", "word embeddings"], [180, 181, "TaskName", "NER"], [190, 192, "TaskName", "word embeddings"], [201, 202, "MethodName", "PCA"], [227, 229, "MethodName", "variational autoencoder"], [230, 231, "MethodName", "VAE"], [240, 241, "MethodName", "VAE"], [307, 308, "MethodName", "VAE"], [343, 346, "DatasetName", "part - of"], [358, 359, "TaskName", "NER"], [362, 364, "TaskName", "dependency parsing"], [622, 625, "MetricName", "F1 - score"], [716, 718, "TaskName", "word embeddings"]]}
{"text": "The goal of an NLU task is to predict the label y of the given input instance x , where the input x contains the sequence of tokens ( Devlin et al , 2019 ) : x = [ w 1 , w 2 , . . . , w | x | ] . Then , given a training dataset D = { ( x ( i ) , y ( i ) ) } N i=1 , the objective is to maximize the log - likelihood as follows : max \u03b8 L ( \u03b8 ) : = max \u03b8 ( x , y ) \u223cD log p ( y | x ; \u03b8 ) , p ( y | x ; \u03b8 ) = g ( H ; \u03b8 g ) , H = f ( x ; \u03b8 f ) , where f is an encoder of the PLM which outputs contextualized representation H from x , and g is a decoder which models the probability distribution p of the label y , with trainable parameters \u03b8 = ( \u03b8 f , \u03b8 g ) . If the LM is composed of L - layers of transformer blocks ( Devlin et al , 2019 ) , the function f is decomposed to multiple functions f = [ f 0 , . . . , f L ] , where each block gets the output of the previous block as the input : H l = f l ( H l\u22121 ) . 1 Knowledge - Augmented Language Model The conventional learning objective defined above might be sufficient for understanding the texts if the tasks require only the general knowledge stored in PLMs . However , it is suboptimal for tackling domain - specific tasks since the general knowledge captured by the parameters \u03b8 f may not include the knowledge required for solving the domain - specific tasks . Thus , contextualizing the texts by the domain knowledge , captured by the domain - specific entities and their relations , is more appropriate for handling such domain - specific problems . To this end , we propose a function h ( ; \u03c6 ) which augments PLMs conditioned on the domain knowledge . Formally , the objective for a NLU task with our knowledge - augmented LM is given as follows : max \u03b8 , \u03c6 L ( \u03b8 , \u03c6 ) : = max \u03b8 , \u03c6 ( x , y ) \u223cD log p ( y | x ; \u03b8 , \u03c6 ) , p ( y | x ; \u03b8 , \u03c6 ) = g ( H ; \u03b8 g ) , H l = f l ( H l\u22121 , h l ( H l\u22121 , E , M , G ; \u03c6 ) ; \u03b8 f l ) , where \u03c6 is parameters for the function h , E is the set of entities , M is the set of corresponding mentions , and G is a knowledge graph . In the following , we will describe the definition of the knowledgerelated inputs E , M , G , and the details of h ( , \u03c6 ) . Definition 1 ( Entity and Mention ) . Given a sequence of tokens x = [ w 1 , . . . , w | x | ] , let E be a set of entities in x. Then an entity e E is composed of one or multiple adjacent tokens within the input text : [ w m \u03b1 , . . . , w m \u03c9 ] x 2 . Here , m = ( m \u03b1 , m \u03c9 ) is a mention that denotes the start and end locations for the entity within the input tokens x , which term is commonly used for defining entities ( F\u00e9vry et al , 2020 ) . Consequently , for each given input x ( i ) , there are a set of entities E ( i ) = { e 1 , . . . , e K } and their corresponding mentions M ( i ) = { m 1 , . . . , m K } . For example , given an input x = [ New , York , is , a , city ] , we have two entities E = { New_York , city } and their associated mentions M = { ( 1 , 2 ) , ( 4 , 4 ) } . We further construct the entity vocabulary E train = N i=1 E ( i ) , which consists of all entities appearing in the training dataset . However , at test time , we may encounter unseen entities that are not in E train . To tackle this , we regard unknown entities as the null entity e , so that \u2200e E train \u222a { e } . Definition 2 ( Entity Memory ) . Given a set of all entities E train \u222a { e } , we represent them in the continuous vector ( feature ) space to learn meaningful entity embeddings . In order to implement this , we define the entity memory E R ( | E train | +1 ) \u00d7d that comprises of an entity e R as a key and its embedding e R d as its value . Also , to access the value in the entity memory , we define the point - wise memory access function EntEmbed which takes an entity as an input . For instance , e = EntEmbed ( New_York ) returns the embedding of the New_York entity , and e = EntEmbed ( e ) returns the zero embedding . This entity memory E is the part of the parameter \u03c6 used in function h. Definition 3 ( Knowledge Graph ) . Since the entity memory alone can not represent relational information between entities , we further define a Knowledge Graph ( KG ) G that consists of a set of factual triplets { ( h , r , t ) } , where the head and the tail entities , h and t , are the elements of E , and a relation r is an element of a set of relations R : h , t E and r R. We assume that a preconstructed KG G ( i ) is given for each input x ( i ) , and provide the details of the KGs and how to construct them in Appendix A.", "entities": [[85, 88, "MetricName", "log - likelihood"], [92, 93, "HyperparameterName", "\u03b8"], [95, 96, "HyperparameterName", "\u03b8"], [100, 101, "HyperparameterName", "\u03b8"], [114, 115, "HyperparameterName", "\u03b8"], [123, 124, "HyperparameterName", "\u03b8"], [130, 131, "HyperparameterName", "\u03b8"], [140, 141, "HyperparameterName", "\u03b8"], [179, 180, "HyperparameterName", "\u03b8"], [182, 183, "HyperparameterName", "\u03b8"], [185, 186, "HyperparameterName", "\u03b8"], [221, 222, "DatasetName", "0"], [280, 282, "TaskName", "general knowledge"], [299, 301, "TaskName", "general knowledge"], [305, 306, "HyperparameterName", "\u03b8"], [395, 396, "HyperparameterName", "\u03b8"], [400, 401, "HyperparameterName", "\u03b8"], [407, 408, "HyperparameterName", "\u03b8"], [423, 424, "HyperparameterName", "\u03b8"], [434, 435, "HyperparameterName", "\u03b8"], [443, 444, "HyperparameterName", "\u03b8"], [471, 472, "HyperparameterName", "\u03b8"], [595, 596, "HyperparameterName", "\u03b1"], [614, 615, "HyperparameterName", "\u03b1"], [862, 864, "TaskName", "entity embeddings"]]}
{"text": "The remaining problem is how to augment a PLM by conditioning it on the domain - specific knowledge , through the function h. An effective approach to do so without stacking additional layers on top of the LM is to interleave the knowledge from h with the pre - trained parameters of the language model ( Devlin et al , 2019 ) consisting of transformer layers ( Vaswani et al , 2017 ) . Before describing our interleaving method in detail , we first describe the Transformer architecture . Transformer Given | x | token representations H l\u22121 = [ h l\u22121 1 , . . . , h l\u22121 | x | ] R | x | \u00d7d from the layer l \u2212 1 where d is the embedding size , each transformer block outputs the contextualized representations for all tokens . In detail , the l - th block consists of the multi - head self - attention ( Attn ) layer and the residual feed - forward ( FF ) layer as follows : H l = LN ( H l\u22121 + Attn ( H l\u22121 ) ) F F ( \u0124 l ) = \u03c3 ( \u0124 l W 1 ) W 2 , H l = LN ( \u0124 l + F F ( \u0124 l ) ) , where LN is a layer normalization ( Ba et al , 2016 ) , \u03c3 is an activation function ( Hendrycks and Gimpel , 2016 ) , W 2 R d \u00d7d and W 1 R d\u00d7d are weight matrices , and d is an intermediate hidden size . We omit the bias term for brevity . Linear Modulation on Transformer An effective yet efficient way to fuse knowledge from different sources without modifying the original model architecture is to scale and shift the features of one source with respect to the data from another source . This scheme of feature - wise affine transformation is effective on various tasks , such as language - conditioned image reasoning or style - transfer in image generation ( Huang and Belongie , 2017 Motivated by them , we propose to linearly transform the intermediate features after the layer normalization of the transformer - based PLM , conditioned on the knowledge sources E , M , G. We term this method as the Knowledge - conditioned Feature Modulation ( KFM ) , described as follows : \u0393 , B , \u0393 , B = h l ( H l\u22121 , E , M , G ; \u03c6 ) , H l = \u0393 LN ( H l\u22121 + Attn ( H l\u22121 ) ) + B , F F ( \u0124 l ) = \u03c3 ( \u0124 l W 1 ) W 2 , H l = \u0393 LN ( \u0124 l + F F ( \u0124 l ) ) + B , ( 1 ) where H l\u22121 R | x | \u00d7d is the matrix of hidden representations from the previous layer , denotes the hadamard ( element - wise ) product , and \u0393 = [ \u03b3 1 , . . . , \u03b3 | x | ] R | x | \u00d7d , B = [ \u03b2 1 , . . . , \u03b2 | x | ] R | x | \u00d7d . \u0393 and B are learnable modulation parameters from the function h , which are conditioned by the entity representation . For instance , in Figure 3 , \u03b3 and \u03b2 for token ' New ' are conditioned on the corresponding entity New_York . However , if tokens are not part of any entity ( e.g. , ' is ' ) , \u03b3 and \u03b2 for such tokens are fixed to 1 and 0 , respectively . One notable advantage of our KFM is that multiple tokens associated to the identical entity are affected by the same modulation ( e.g. , ' New ' and ' York ' in Figure 3 ) , which allows the PLM to know which adjacent tokens are in the same entity . This is important for representing the tokens of the domain entity ( e.g. , ' cod ' and ' on ' ) , since the original PLM might regard them as separate , unrelated tokens ( See analysis in 5.5 with Figure 5 ) . However , with our KFM , the PLM can identify associated tokens and embed them to be close to each other . Then , how can we design such functional operations in h ? The easiest way is to retrieve the entity embedding of e , associated to the typical to - ken , from the entity memory E , and then use the retrieved entity embedding as the input to obtain \u03b3 and \u03b2 for every entity ( See Figure 3 ) . Formally , for each entity e E and its mention ( m \u03b1 , m \u03c9 ) M , v = EntEmbed ( e ) ( 2 ) \u03b3 j = 1 + h 1 ( v ) , \u03b2 j = h 2 ( v ) , \u03b3 j = 1 + h 3 ( v ) , \u03b2 j = h 4 ( v ) , m \u03b1 \u2264 j \u2264 m \u03c9 , where v is the retrieved entity embedding from the entity memory , h 1 , h 2 , h 3 , and h 4 are mutually independent Multi - Layer Perceptrons ( MLPs ) which return a zero vector 0 if e = e .", "entities": [[86, 87, "MethodName", "Transformer"], [89, 90, "MethodName", "Transformer"], [228, 230, "MethodName", "layer normalization"], [241, 243, "HyperparameterName", "activation function"], [284, 285, "MethodName", "Transformer"], [347, 349, "TaskName", "image generation"], [369, 371, "MethodName", "layer normalization"], [407, 408, "HyperparameterName", "\u0393"], [411, 412, "HyperparameterName", "\u0393"], [433, 434, "HyperparameterName", "\u0393"], [468, 469, "HyperparameterName", "\u0393"], [517, 518, "HyperparameterName", "\u0393"], [520, 521, "HyperparameterName", "\u03b3"], [527, 528, "HyperparameterName", "\u03b3"], [541, 542, "HyperparameterName", "\u03b2"], [548, 549, "HyperparameterName", "\u03b2"], [559, 560, "HyperparameterName", "\u0393"], [586, 587, "HyperparameterName", "\u03b3"], [588, 589, "HyperparameterName", "\u03b2"], [620, 621, "HyperparameterName", "\u03b3"], [622, 623, "HyperparameterName", "\u03b2"], [631, 632, "DatasetName", "0"], [803, 804, "HyperparameterName", "\u03b3"], [805, 806, "HyperparameterName", "\u03b2"], [827, 828, "HyperparameterName", "\u03b1"], [843, 844, "HyperparameterName", "\u03b3"], [854, 855, "HyperparameterName", "\u03b2"], [863, 864, "HyperparameterName", "\u03b3"], [874, 875, "HyperparameterName", "\u03b2"], [884, 885, "HyperparameterName", "\u03b1"], [930, 931, "DatasetName", "0"]]}
{"text": "Although the simple access to the entity memory can retrieve the necessary entity embeddings for the modulation , this approach has obvious drawbacks as it not only fails to reflect the relations with other entities , but also regards unseen entities as the same null entity e . If so , all unseen entities are inevitably modulated by the same parameters even if they have essentially different meaning . To tackle these limitations , we further consider the relational information between two entities that are linked with a particular relation . For example , the entity New_York alone will not give meaningful information . However , with two associated facts ( New_York , instance of , city ) and ( New_York , country , USA ) , it is clear that New_York is a city in the USA . Motivated by this observation , we propose Relational Retrieval which leverages a KG G to retrieve entity embeddings from the memory , according to the relations defined in the given KG ( See Figure 3 , right ) . More specifically , our goal is to effectively utilize the relations among entities in G , to improve the EntEmbed function in equation 2 . We tackle this objective by utilizing a Graph Neural Network ( GNN ) which learns feature representations of each node using a neighborhood aggregation scheme ( Hamilton et al , 2017 ) , as follows : v = UPDATE ( EntEmbed ( e ) , AGG ( { EntEmbed ( \u00ea ) : \u2200\u00ea N ( e ; G ) } ) ) , where N ( e ; G ) is a set of neighboring entities of the entity e , AGG is the function that aggregates embeddings of neighboring entities of e , and UPDATE is the function that updates the representation of e with the aggregated messages from AGG . However , simple aggregation ( e.g. , mean ) can not reflect the relative importance on neighboring nodes , thus we consider the attentive scheme ( Velickovic et al , 2018 ; Brody et al , 2021 ) for neighborhood aggregation , to allocate weights to the target entity 's neighbors by their importance . This scheme is helpful in filtering out less useful relations . Formally , we first define a scoring function \u03c8 that calculates a score for every triplet ( e i , r ij , e j ) , which is then used to weigh each node during aggregation : e i = EntEmbed ( e i ) , e j = EntEmbed ( e j ) , e * = [ e i r ij e j h e i ] , \u03c8 ( e i , r ij , e j , h e i ) = a \u03c3 ( W e * ) , where \u03c3 is a nonlinear activation , e * R 4d is concatenated vector where denotes the concatenation , a R d and W R d\u00d74d are learnable parameters , r ij R d is a embedding of the relation , and h e i R d is a context representation of the entity e i obtained from the intermediate hidden states of the LM 3 . The scores obtained from \u03c8 are normalized across all neighbors e j N ( e i ; G ) with softmax : \u03b1 ij = softmax ( \u03c8 ( e i , r ij , e j ) ) = exp ( \u03c8 ( e i , r ij , e j ) ) e j N ( e i ; G ) exp ( \u03c8 ( e i , r ij , e j ) ) . Then , we update the entity embedding with a weighted average of the neighboring nodes with \u03b1 as an attention coefficient , denoted as follows : v = UPDATE e j N ( e i ; G ) \u03b1 ij e j . ( 3 ) 1 m \u03c9 \u2212m \u03b1 +1 m \u03c9 i = m \u03b1 h l\u22121 i By replacing the EntEmbed function in equation 2 with the above GNN in equation 3 , we now represent each entity with its relational information in KG . This relational retrieval has several advantages over simple retrieval of a single entity from the entity memory . First , the relational retrieval with KG can consider richer interactions among entities , as described in Figure 3 . In addition , we can naturally represent an unseen entity - which is not seen during training but appears at test time - through neighboring aggregation , which is impossible only with the entity memory . In Figure 2 , we provide an illustrative example of the unseen entity representation , where the unseen entity restenosis is represented with a weighted sum of representations of its neighboring entities myocardial_infarction , asthma , and pethidine , which is beneficial when the set of entities for training and test datasets have small overlaps .", "entities": [[12, 14, "TaskName", "entity embeddings"], [155, 157, "TaskName", "entity embeddings"], [564, 565, "MethodName", "softmax"], [566, 567, "HyperparameterName", "\u03b1"], [569, 570, "MethodName", "softmax"], [638, 639, "HyperparameterName", "\u03b1"], [660, 661, "HyperparameterName", "\u03b1"], [672, 673, "HyperparameterName", "\u03b1"], [679, 680, "HyperparameterName", "\u03b1"]]}
{"text": "We evaluate our model on two NLU tasks : Question Answering ( QA ) and Named Entity Recognition ( NER ) . For QA , we use three domain - specific datasets : NewsQA ( News , Trischler et al , 2017 ) and two subsets ( Relation , Medication ) of EMRQA ( Clinical , Pampari et al , 2018 ) . We use the Exact - Match ( EM ) and the F1 score as evaluation metrics . For NER , we use three datasets from different domains , namely CoNLL - 2003 ( News , Sang andMeulder , 2003 ) , WNUT - 17 ( Social Media , Derczynski et al , 2017 ) and NCBI - Disease ( Biomedical , Dogan et al , 2014 ) . We use the F1 score as the evaluation metric . We report statistics and detailed descriptions of each dataset in Appendix B.2 .", "entities": [[9, 11, "TaskName", "Question Answering"], [15, 18, "TaskName", "Named Entity Recognition"], [19, 20, "TaskName", "NER"], [33, 34, "DatasetName", "NewsQA"], [52, 53, "DatasetName", "EMRQA"], [70, 71, "MetricName", "EM"], [74, 76, "MetricName", "F1 score"], [81, 82, "TaskName", "NER"], [111, 112, "DatasetName", "Derczynski"], [118, 121, "DatasetName", "NCBI - Disease"], [134, 136, "MetricName", "F1 score"]]}
{"text": "A direct baseline of our KALA is the adaptive pre - training , which is commonly used to adapt the PLM independent to the choice of a domain and task . Also , to compare ours against a more powerful baseline , we modify a recent method that alleviates forgetting of PLM during fine - tuning . Details for each baseline we use are described as follows : 1 . Vanilla Fine - Tuning ( FT ) : A baseline that directly fine - tunes the LM on downstream tasks . 2 . Fine - Tuning + more params : A baseline with one more transformer layer at the end of the means and standard deviations of performances over five different runs with Exact Match / F1 score as a metric . The numbers in bold fonts denote the best score . \u2020 indicates the method under an extremely high computational resource setting ( See Figure 1 ) . LM . We use this baseline to show that the performance gain of our model does not come from the use of additional parameters . 6 . KALA ( pointwise ) : A variant of KALA that only uses the entity memory and does not use the knowledge graphs . 7 . KALA ( relational ) : Our full model that uses KGs to perform relational retrieval from the entity memory .", "entities": [[98, 99, "MetricName", "params"], [123, 125, "MetricName", "Exact Match"], [126, 128, "MetricName", "F1 score"], [206, 208, "TaskName", "knowledge graphs"]]}
{"text": "Performance on QA and NER tasks On both extractive QA and NER tasks , our KALA outperforms all baselines , including TAPT and TAPT+RedcAdam ( Gururangan et al , 2020 ; , as shown in Table 1 and 2 . These results show that our KALA is highly effective for the language model adaptation task . KALA also largely outperforms DAPT ( Gururangan et al , 2020 ) which is trained with extra data and requires a significantly higher computational cost compare to KALA ( See Figure 1 for the plot of efficiency , discussed in Section 5.3 ) . Effect of Using more Parameters One may suspect whether the performance of our KALA comes from the increment of parameters . However , the experimental results in Table 1 and 2 show that increasing the parameters for PLM during fine - tuning ( + more params ) yields marginal performance improvements over naive fine - tuning . This result confirms that the performance improvement of KALA is not due to the increased number of parameters .", "entities": [[4, 5, "TaskName", "NER"], [11, 12, "TaskName", "NER"], [145, 146, "MetricName", "params"], [172, 175, "HyperparameterName", "number of parameters"]]}
{"text": "We first analyze the effect of feature modulation parameters ( i.e. , gamma and beta ) in transformers by ablating a subset of them in Table 3 , in which we observe that using both gamma and beta after both layer normalization on a transformer layer obtains the best performance . Architectural Variants We now examine the effectiveness of the proposed knowledge conditioning scheme in our KALA framework . To this end , we use or adapt the knowledge integration methods from previous literature , to compare their effectiveness . Specifically , we couple the following five components with KALA : Entity - as - Experts ( F\u00e9vry et al , 2020 ) , Adapter ( Houlsby et al , 2019 ) , KT - Net ( Yang et al , 2019 ) , ERNIE ( Zhang et al , 2019 ) , and ERICA ( Qin et al , 2021 ) . Note that , most of them were proposed for improving pre - training from scratch , while we adapt them for fine - tuning under our KALA framework ( The details are given in Appendix B.4 ) . As shown in Table 4 , our KFM used in KALA outperforms all variants , demonstrating the effectiveness of feature modulation in the middle of transformer layers for fine - tuning .", "entities": [[12, 13, "HyperparameterName", "gamma"], [14, 15, "HyperparameterName", "beta"], [35, 36, "HyperparameterName", "gamma"], [37, 38, "HyperparameterName", "beta"], [40, 42, "MethodName", "layer normalization"], [114, 115, "MethodName", "Adapter"]]}
{"text": "Figure 1 illustrates the performance and training FLOPs of KALA against baselines on the NewsQA dataset . We observe that the performance of TAPT decreases with the increased number of iterations , which could be due to forgetting of the knowledge from the PLM . On the other hand , DAPT , while not suffering from performance loss , requires huge computational costs as it trains on 112 times larger data for further pre - training ( See Appendix B.3 for detailed explanations on training data ) . On the other hand , our KALA outperforms DAPT without using external data , while requiring 17 times fewer computational costs , which shows that KALA is not only effective but also highly efficient . To further compare the efficiency in various aspects , we report GPU memory , training wall time , and training FLOPs for baselines and ours in Table 6 . Through this , we verify that our KALA is more efficient to train for language model adaptation settings than baselines . Note that the resource requirement of KALA could be further reduced by adjusting the size of the entity memory ( e.g. , removing less frequent entities ) . Therefore , to show the flexibility of our KALA on the typical resource constraint , we provide the experimental results on two different settings ( i.e. , tuning the number of entities in the entity memory ) - KALA with memory size of 200 and 62.8k ( full memory ) in Appendix C.6 .", "entities": [[14, 15, "DatasetName", "NewsQA"], [28, 31, "HyperparameterName", "number of iterations"], [57, 58, "MetricName", "loss"]]}
{"text": "In this work , we propose to use the Knowledge Graph ( KG ) that can define the relational information among entities that only appear in each dataset . However , unfortunately , most of the task datasets do not contain such relational facts on its context , thus we need to construct them manually to obtain the knowledge graph . In this section , we explain the way of constructing the knowledge graph that we used , consisting of facts of entities for each context in the task dataset . Relation extraction is the way how we obtain the factual knowledge from the text of the target dataset . To do so , we first need to extract entities and their corresponding mentions from the text , and then link it to the existing entities in wikidata ( Vrandecic and Kr\u00f6tzsch , 2014 ) . In order to do this , we use the existing library named as spaCy 5 , and opensourced implementation of Entity Linker 6 . To sum up , in our work , a set of entities E ( i ) and corresponding mentions M ( i ) for the given input x ( i ) are obtained through this step . Regarding a concrete example , please see format ( a ) in Figure 6 . In the example , \" Text \" indicates the entity mention within the input x , the \" start \" and \" end \" indicates its mention position denoted as ( m \u03b1 , m \u03c9 ) , and \" i d \" indicates the wikidata i d for the entity identification used in the next step . To extract the relation among entities that we obtained above , we use the scheme of Relation Extraction ( RE ) . In other words , we use the trained 5 https://spacy.io/ 6 https://github.com/egerber/spaCy - entity - linker RE model to build our own knowledge base ( KB ) instead of using the existing KG directly from the existing general - domain KB 7 . Specifically , we first fine - tune the BERT - base model ( Devlin et al , 2019 ) for 2 epochs with 600k distantly supervised data used in Qin et al ( 2021 ) , where the Wikipedia document and the Wikidata triplets are aligned . Then , we use the fine - tuned BERT model to extract the relations between entity pairs in the text . We use the model with a simple bilinear layer on top of it , which is widely used scheme in the relation extraction literature ( Yao et al , 2019 ) . For an example of the extracted fact , please see format ( b ) in Figure 6 . In the example , \" h \" denotes the wikidata i d of the head entity , \" r \" denotes the wikidata i d of the extracted relation , and \" t \" denotes the wikidata i d of the tail entity . In the relation extraction , the model returns the categorical distribution over the top 100 frequent relations . In general , the relation of top - 1 probability is used as the relation for the corresponding entity pair . However , this approach sometimes results in predicting no_relation on most entity pairs . Thus , to obtain more relations , we further use the relation of top - 2 probability in the case where no_relation has a top - 1 probability but the top - 2 probability is larger than a certain threshold ( e.g. , > 0.1 ) . In Figure 6 , we summarize our KG construction pipeline . In Table 8 , we report the hyperparameters related to our KG construction .", "entities": [[91, 93, "TaskName", "Relation extraction"], [254, 255, "HyperparameterName", "\u03b1"], [296, 298, "TaskName", "Relation Extraction"], [353, 354, "MethodName", "BERT"], [400, 401, "MethodName", "BERT"], [434, 436, "TaskName", "relation extraction"], [509, 511, "TaskName", "relation extraction"]]}
{"text": "We use the Pytorch ( Paszke et al , 2019 ) for the implementation of all models . Also , to easily implement the language model , we use the huggingface library ( Wolf et al , 2020 ) containing various transformer - based pre - trained language models ( PLMs ) and their checkpoints . Details for KALA In this paragraph , we describe the implementation details of the components , such as four linear layers in the proposed KFM , architectural specifications in the attentionbased GNN , and initialization of both the entity memory and relational embeddings , in the following . In terms of the functions h 1 , h 2 , h 3 , and h 4 in the KFM of Equation 2 , we use two linear layers with the ReLU ( Nair and Hinton , 2010 ) activation function , where the dimension is set to 768 . For relational retrieval , we implement the novel GNN model based on GATv2 ( Brody et al , 2021 ) provided by the torch - geometric package ( Fey and Lenssen , 2019 ) . Specifically , we stack two GNN layers with the RELU activation function and also use the dropout with a probability of 0.1 . For attention in our GNN , we mask the nodes of the null entity , so that the attention score becomes zero for them . Moreover , to obtain the context representation of the entity ( See Footnote 3 in the main paper ) used in the GNN attention , we use the scatter operation 8 for reduced computational cost . For Entity Memory , we experimentally found that initializing the embeddings of the entity memory with the contextualized features obtained from 8 https://github.com/rusty1s/pytorch_scatter the pre - trained language model could be helpful . Therefore , the dimension of the entity embedding is set to the same as the language model d = 768 . For relation embeddings , we randomly initialize them , where the dimension size is set to 128 . Location of KLM in the PLM Note that , the number and location of the KFM layers inside the PLM are hyperparameters . However , we empirically found that inserting one to three KFM layers at the end of the PLM ( i.e. , after the 9th - 11th layers of the BERT - base language model ) is beneficial to the performance ( See Appendix C.4 for experiments on diverse layer locations ) .", "entities": [[135, 136, "MethodName", "ReLU"], [143, 145, "HyperparameterName", "activation function"], [166, 167, "MethodName", "GATv2"], [198, 199, "MethodName", "RELU"], [199, 201, "HyperparameterName", "activation function"], [397, 398, "MethodName", "BERT"]]}
{"text": "All experiments are constrained to be done with a single 12 GB Geforce RTX 2080 Ti GPU for fairness in terms of memory and the availability on the academic budget , except for the DAPT and generative QA which use a single 48 GB Quadro 8000 GPU . KALA training needs 3 hours in wall time with a single GPU . For all experiments , we select the best checkpoint on the validation set . For the summary of training setups , please see Table 10 and 12 . Fine - tuning Setup In the following three paragraphs , we explain the setting of fine - tuning for QA , NER , and generative QA tasks . For all experiments on extractive QA tasks , we fine - tune the Pre - trained Language Model ( PLM ) for 2 epochs with the weight decay of 0.01 , learning rate of 3e - 5 , maximum sequence length of 384 , batch size of 12 , linear learning rate decay of 0.06 warmup rate , and half precision ( Micikevicius et al , 2018 ) . For all experiments on NER tasks , we finetune the PLM for 20 epochs , where the learning rate is set to 5e - 5 , maximum sequence length is set to 128 , and batch size is set to 32 . We use AdamW ( Loshchilov and Hutter , 2019 ) as an optimizer using BERT - base as the PLM . For the generative QA task in Table 7 , we finetune the T5 - small ( Raffel et al , 2020 ) for 4 epochs with the learning rate of 1e - 4 , maximum sequence length of 512 , and batch size of 64 . We also use the Adafactor ( Shazeer and Stern , 2018 ) optimizer . Instead of training with the same optimizer as in BERT for QA and NER , we instead use the independent AdamW optimizer with the learning rate of 1e - 4 and weight decay of 0.01 to train the KALA module with T5 . Adaptive Pre - training Setup In this paragraph , we describe the experimental settings of adaptive pre - training baselines , namely TAPT , TAPT ( + RecAdam ) , and DAPT . For QA tasks , we further pre - train the PLM for { 1 , 3 , 5 , 10 } epochs and then report the best performance among them . Specifically , reported TAPT result on NewsQA , Relation , and Medication are obtained by 1 epoch of further pre - training . We use the weight decay of 0.01 , learning rate of 5e - 5 , maximum sequence length of 384 , batch size of 12 , and linear learning rate decay of 0.06 warmup rate , with a half - precision . Also , the masking ratio for the pre - training objective is set to 0.15 , following the existing strategy introduced in the original BERT paper ( Devlin et al , 2019 ) . For NER tasks , we further pre - train the PLM for 3 epochs across all datasets . In particular , the learning rate is set to 5e - 5 , batch size is set to 32 , and the maximum sequence length is set to 128 . We also use AdamW ( Loshchilov and Hutter , 2019 ) as the optimizer for all experiments . In the case of T5 - small for generative QA in Table 7 , we further pre - train the PLM for 4 epochs with the learning rate of 0.001 , batch size of 64 , maximum sequence length of 384 , and Adafac - tor ( Shazeer and Stern , 2018 ) optimizer . Regarding the setting of TAPT ( + RecAdam ) on all tasks , we follow the best setting in the original paper - sigmoid as an annealing function with annealing parameters : k = 0.5 , t 0 = 250 , and the pretraining coefficient of 5000 . For training with DAPT , we need an external corpus having a large amount of data for adaptive pre - training . Thus , we first choose the datasets of two domains - News and Medical . Specifically , as the source of corpus for the News domain , we use the sampled set of 10 million News from the RealNews dataset used in Gururangan et al ( 2021 ) . As the source of corpus for the Medical domain , we use the set of approximately 100k passages from the Medical textbook provided in Jin et al ( 2020 ) . The size of pre - training data used in DAPT is much larger than TAPT . In other words , for experiments on NewsQA , TAPT only uses fine - tuning contexts containing 5.8 million words from the NewsQA training dataset , while DAPT uses more than a hundred times larger data - enormous contexts containing about 618 million words from the RealNews database . For both News and Medical domains , we further pre - train the BERT - base model for 50 epochs with the batch size of 64 , to match the similar computational cost used in Gururangan et al ( 2020 ) . Other experimental details are the same as TAPT described above .", "entities": [[110, 111, "TaskName", "NER"], [143, 145, "MethodName", "weight decay"], [148, 150, "HyperparameterName", "learning rate"], [161, 163, "HyperparameterName", "batch size"], [167, 169, "HyperparameterName", "learning rate"], [190, 191, "TaskName", "NER"], [203, 205, "HyperparameterName", "learning rate"], [221, 223, "HyperparameterName", "batch size"], [230, 231, "MethodName", "AdamW"], [240, 241, "HyperparameterName", "optimizer"], [242, 243, "MethodName", "BERT"], [261, 262, "MethodName", "T5"], [276, 278, "HyperparameterName", "learning rate"], [290, 292, "HyperparameterName", "batch size"], [299, 300, "MethodName", "Adafactor"], [307, 308, "HyperparameterName", "optimizer"], [315, 316, "HyperparameterName", "optimizer"], [318, 319, "MethodName", "BERT"], [322, 323, "TaskName", "NER"], [329, 330, "MethodName", "AdamW"], [330, 331, "HyperparameterName", "optimizer"], [333, 335, "HyperparameterName", "learning rate"], [340, 342, "MethodName", "weight decay"], [350, 351, "MethodName", "T5"], [422, 423, "DatasetName", "NewsQA"], [442, 444, "MethodName", "weight decay"], [447, 449, "HyperparameterName", "learning rate"], [460, 462, "HyperparameterName", "batch size"], [467, 469, "HyperparameterName", "learning rate"], [505, 506, "MethodName", "BERT"], [516, 517, "TaskName", "NER"], [537, 539, "HyperparameterName", "learning rate"], [546, 548, "HyperparameterName", "batch size"], [566, 567, "MethodName", "AdamW"], [576, 577, "HyperparameterName", "optimizer"], [585, 586, "MethodName", "T5"], [607, 609, "HyperparameterName", "learning rate"], [612, 614, "HyperparameterName", "batch size"], [634, 635, "HyperparameterName", "optimizer"], [668, 670, "HyperparameterName", "k ="], [673, 674, "DatasetName", "0"], [744, 745, "DatasetName", "RealNews"], [762, 764, "DatasetName", "Medical domain"], [809, 810, "DatasetName", "NewsQA"], [824, 825, "DatasetName", "NewsQA"], [848, 849, "DatasetName", "RealNews"], [864, 865, "MethodName", "BERT"], [873, 875, "HyperparameterName", "batch size"]]}
{"text": "In this subsection , we describe the details of architectural variants reported in Section 5.1 . For all variants , we use the same KGs used in KALA . Entity - as - Experts ( F\u00e9vry et al ( 2020 ) ; EaE ) utilizes the entity memory similar to our work , but they use the parametric dense retrieval more like the memory neural network ( Sukhbaatar et al , 2015 ) . Similar to F\u00e9vry et al ( 2020 ) ; Verga et al ( 2021 ) , we change the formulation of query and memory retrieval by using the mention representation of the entity from the intermediate hidden states of PLMs , which is formally defined as follows : h e = 1 m \u03c9 \u2212 m \u03b1 + 1 m \u03c9 i = m \u03b1 h l\u22121 i , ( 4 ) v = softmax ( h e E ) E , where h e represents the average of token representations of the entity mention m = ( m \u03b1 , m \u03c9 ) . We also give the supervised retrieval loss ( ELLoss in F\u00e9vry et al ( 2020 ) ) , when training the EaE model . With this retrieval , EaE also can represent the unseen entity e / E train if we know the mention boundary of the given entity on the context . We believe it is expected to work well , if the entity memory is pre - trained on the enormous text along with the pre - training of the language model from the scratch . However , it might underperform for the language model adaptation scenario , since it can fall into the problem of circular reasoning - the PLM does not properly represent the unseen entity , but it should predict which entity it is similar from the representation . Regarding the integration of the knowledge from the entity memory into the PLM , the retrieved entity representation v is simply added ( Peters et al , 2019 ) to the hidden representations H after the transformer block as follows : H l = H l + h ( v ) ( 5 ) where h is Multi - Layer Perceptrons ( MLPs ) . Adapter ( Houlsby et al , 2019 ) is introduced to fine - tune the PLM only with a few trainable parameters , instead of fine - tuning the whole parameters of the PLM . To adapt this original implementation into our KALA framework , we replace our Knowledge - conditioned Feature Modulation with it , where the Adapter is used as the knowledge integration module . We interleave the layer of Adapter after the feed - forward layer ( F F ) and before the residual connection of the transformer block . Also , instead of only providing the LM hidden states as an input , we concatenate the knowledge representation in Equation 3 to the LM hidden states . Note that we fine - tune the whole parameters following our KALA setting , unlike fine - tuning the parameters of only Adapter layers in Houlsby et al ( 2019 ) . ERNIE ( Zhang et al , 2019 ) is a notable PLM model that utilizes the external KB as an input for the language model . The key feature of ERNIE can be summarized into two folds . First , they use the multi - head self - attention scheme ( Vaswani et al , 2017 ) to contextualize the input entities . Second , ERNIE fuses the entity representation at the end of the PLM by adding it to the corresponding language representation . We assume that those two features are important points of ERNIE . Therefore , instead of using a Graph Neural Network ( GNN ) layer , we use a multi - head self - attention layer to contextualize the entity embeddings . Then , we add it to a representation of the entity from the PLM , which is the same as the design in equation 5 . KT - Net ( Yang et al , 2019 ) uses knowledge as an external input in the fine - tuning stage for extractive QA . Since they have a typical layer for integrating existing KB ( Miller , 1995 ; Carlson et al , 2010 ) with the PLM , we only adopt the self - matching layer as the architecture variant of the KFM layer used in our KALA framework . The computation of the self - matching matrix in KT - Net is costly , i.e. , it requires a large computational cost that is approximately 12 times larger than KALA . ERICA ( Qin et al , 2021 ) uses contrastive learning in LM pre - training to reflect the relational knowledge into the language model . We use the Entity Discrimination task from ERICA on the primary task of fine - tuning . We would like to note that , as reported in Section 5 of the original paper ( Qin et al , 2021 ) , the use of ERICA on fine - tuning has no effect , since the size and diversity of entities and relations in downstream training data are limited . Such limited information rather harms the performance , as it can hinder the generalization . In other words , contrastive learning can not reflect the entity and relation in the test dataset .", "entities": [[130, 131, "HyperparameterName", "\u03b1"], [138, 139, "HyperparameterName", "\u03b1"], [148, 149, "MethodName", "softmax"], [173, 174, "HyperparameterName", "\u03b1"], [185, 186, "MetricName", "loss"], [378, 379, "MethodName", "Adapter"], [436, 437, "MethodName", "Adapter"], [450, 451, "MethodName", "Adapter"], [464, 466, "MethodName", "residual connection"], [521, 522, "MethodName", "Adapter"], [655, 657, "TaskName", "entity embeddings"], [798, 800, "MethodName", "contrastive learning"], [903, 905, "MethodName", "contrastive learning"]]}
{"text": "To see how much amount of value on gamma and beta is used to shift and scale the intermediate hidden representations in transformer layers , we visualize the modulation values , namely gamma and beta , in Figure 11 . We first observe that , as shown in Figure 11 , the distribution of values of gamma and beta approximately follow the Gaussian dis - tribution , with zero mean for beta and one mean for gamma . Also , we notice that the scale of values remain nearly around the mean point , which suggests that the small amount of shifting to intermediate hidden representations on transformer layers is enough to contribute to the performance gain , as we can see in the main results of Table 1 , 2 .", "entities": [[8, 9, "HyperparameterName", "gamma"], [10, 11, "HyperparameterName", "beta"], [32, 33, "HyperparameterName", "gamma"], [34, 35, "HyperparameterName", "beta"], [56, 57, "HyperparameterName", "gamma"], [58, 59, "HyperparameterName", "beta"], [71, 72, "HyperparameterName", "beta"], [76, 77, "HyperparameterName", "gamma"]]}
{"text": "While we provide the efficiency on FLOPs in Figure 1 , we further provide the efficiency on GPU memory , wall time , and FLOPs for training each method in Table 6 . Specifically , we measure the computational cost on the NewsQA dataset with BERT - base , where we use the single Geforce RTX 2080 Ti GPU on the same machine . For our KALA , as we can flexibly manage the cost of GPU memory by reducing the number of entities in entity memory ( See Figure 8 with Appendix C.2 for more analysis on the effects of the size of entity memory ) , we provide the experimental results on two settings - KALA with memory size 0.2k and 62.8k ( full memory ) . As shown in Table 6 , we confirm that the computational cost of our KALA with the full memory is similar to the cost of the more params baseline that uses one additional transformer layer on top of BERT - base . However , by reducing the number of entities in the memory , we can achieve better efficiency than more params in terms of GPU memory and FLOPs . Also , we observe that the training cost ( i.e. , Wall Time and FLOPs ) of TAPT and DAPT is high , especially on DAPT , thus we verify that our KALA is more efficient to train for domain adaptation settings .", "entities": [[42, 43, "DatasetName", "NewsQA"], [45, 46, "MethodName", "BERT"], [156, 157, "MetricName", "params"], [167, 168, "MethodName", "BERT"], [190, 191, "MetricName", "params"], [238, 240, "TaskName", "domain adaptation"]]}
{"text": "The development of online media platforms has given users more opportunities to post and comment freely , but the negative impact of offensive language has become increasingly apparent . It is very necessary for the automatic identification system of offensive language . This paper describes our work on the task of Offensive Language Identification in Dravidian language - EACL 2021 . To complete this task , we propose a system based on the multilingual model XLM - Roberta and DPCNN . The test results on the official test data set confirm the effectiveness of our system . The weighted average F1 - score of Kannada , Malayalam , and Tamil language are 0.69 , 0.92 , and 0.76 respectively , ranked 6th , 6th , and 3rd .", "entities": [[52, 54, "TaskName", "Language Identification"], [75, 76, "MethodName", "XLM"], [100, 103, "MetricName", "F1 - score"]]}
{"text": "Due to the harm of offensive language to the network environment , the identification of offensive language has been carried out for a long time . Research so far has focused on automating the decision - making process in the form of supervised machine learning for classification tasks ( Sun et al , 2019 ) . As far back as 2012 , Chen et al ( 2012 ) proposed a lexical syntactic feature ( LSF ) framework to detect offensive content in social media , distinguished the roles of derogatory / profane and obscenity in identifying offensive content , and introduced handwritten syntax rules to identify abusive harassment . In contrast to the start - to - end training model , Howard and Ruder ( 2018 ) proposed an effective transfer learning method , Universal Language Model Tuning ( ULMFIT ) , which can be applied to any task in natural language processing , and has shown significant results on six text classification tasks . Subsequently , Abdellatif and Elgammal ( 2020 ) used the ULMFiT transfer learning method to train forward and backward models on Arabic datasets and ensemble the results to perform an offensive language detection task . Although English is currently one of the most commonly spoken languages in the world , work is ongoing to identify the offensive language in other languages that are less widely spoken . Pitenis et al ( 2020 ) tested the performance of several traditional machine learning models and deep learning models on an offensive language dataset of Greek , and the best results were achieved with the attention model of LSTM and GRU . Ozdemir and Yeniterzi ( 2020 ) ensembled CNN - LSTM , BILSTM - Attention , and BERT three models , combined with pre - trained word embedding on Twitter to complete the identification task of offensive Turkish language , and achieved a good result . A key challenge in automatically detecting hate speech on social media is to separate hate speech from other offensive languages . Davidson et al ( 2017 ) used the crowd - sourced hate speech lexicon to collect tweets containing hate speech keywords . They trained a multi - class classifier to reliably distinguish hate speech from other offensive languages , and found that racist and homophobic tweets were more likely to be classified as hate speech , but sexist tweets were generally classified as offensive . Razavi et al ( 2010 ) proposed to extract features at different conceptual levels and apply multilevel classification for offensive language detection . The system leverages a variety of statistical models and rule - based patterns , combined with an auxiliary weighted pattern library , to improve accuracy by matching text with its graded entries . Pitsilis et al ( 2018 ) proposed the ensemble of a recursive neural network ( RNN ) classifier , which combines various characteristics related to user - related information , such as the user 's sexist or racist tendencies , and was then fed to the classifier as input along with a word frequency vector derived from the text content . When there is a large amount of labeled data , increasing the size and parameters of the model will definitely improve the performance of the model . However , when the amount of training is relatively small , the large - scale model may not be able to achieve good results , so solving the problem of model training under the condition of a small amount of target data has become a research hotspot . Sun et al ( 2019 ) proposed a Hierarchical Attention Prototype Network ( HAPN ) for fewshot text classification , which designed multiple cross - concerns of a feature layer , word layer , and instance layer for the model to enhance the expressive power of semantic space . The model was validated on two standard reference text classification datasets , Fewrel and CSID . Prettenhofer and Stein ( 2010 ) built on structural correspondence learning , using untagged documents and simple word translation to induce task - specific , cross - language word correspondence . English was used as the source language and German , French , and Japanese were used as the target language to conduct the experiment in the field of cross - language sentiment classification . Using English data , Ranasinghe and Zampieri ( 2020 ) trained the model by applying cross - language contextual word embedding and transfer learning methods , and then predicted the effect of cross - language contextual embedding and transfer learning on this task in less resourceintensive languages such as Bengali , Hindi , and Spanish .", "entities": [[130, 132, "TaskName", "transfer learning"], [139, 140, "MethodName", "ULMFIT"], [161, 163, "TaskName", "text classification"], [175, 176, "MethodName", "ULMFiT"], [176, 178, "TaskName", "transfer learning"], [270, 271, "MethodName", "LSTM"], [272, 273, "MethodName", "GRU"], [283, 284, "MethodName", "LSTM"], [285, 286, "MethodName", "BILSTM"], [290, 291, "MethodName", "BERT"], [325, 327, "DatasetName", "hate speech"], [333, 335, "DatasetName", "hate speech"], [351, 353, "DatasetName", "hate speech"], [358, 360, "DatasetName", "hate speech"], [372, 374, "DatasetName", "hate speech"], [393, 395, "DatasetName", "hate speech"], [452, 453, "MetricName", "accuracy"], [614, 616, "TaskName", "text classification"], [654, 656, "TaskName", "text classification"], [658, 659, "DatasetName", "Fewrel"], [679, 681, "TaskName", "word translation"], [749, 751, "TaskName", "transfer learning"], [765, 767, "TaskName", "transfer learning"]]}
{"text": "In this experiment , the pre - training model I used was XLM - RoBERTa - base . After adding the DPCNN module , we began to set the experimental parameters . We set the learning rate as 2e - 5 , the maximum sequence length is 256 , and the gradient steps are set to 4 . The batch size is set to 32 , as shown in table 2 . In the training process , we used five - fold stratified cross - validation to make the proportion of data of each category in each subsample the same as that in the original data and finally obtained the optimal result through the voting ( Onan et al , 2016 ) system , as shown in Figure 2 .", "entities": [[12, 13, "MethodName", "XLM"], [14, 15, "MethodName", "RoBERTa"], [35, 37, "HyperparameterName", "learning rate"], [59, 61, "HyperparameterName", "batch size"]]}
{"text": "After the evaluation by the organizer , we obtained the weighted average F1 - score in the three languages , as shown in table 3 . Our team 's F1 - score is 0.69 , ranked 6th place for the Kannada language . For the Malayalam language , our team 's F1 - score is 0.92 ranked 6th place , and for the Tamil language , our team 's F1 - score is 0.76 ranked 3rd place .", "entities": [[12, 15, "MetricName", "F1 - score"], [29, 32, "MetricName", "F1 - score"], [51, 54, "MetricName", "F1 - score"], [69, 72, "MetricName", "F1 - score"]]}
{"text": "We begin by introducing the task and the datasets we use . Tabular Inference is a reasoning task that , like conventional NLI ( Dagan et al , 2013 ; Bowman et al , 2015 ; Williams et al , 2018 ) , asks whether a natural language hypothesis can be inferred from a tabular premise . Concretely , given a premise table T with m rows { r 1 , r 2 , . . . , r m } , and a hypothesis sentence H , the task maps them to ENTAIL ( E ) , CONTRADICT ( C ) or NEUTRAL ( N ) . We can denote the mapping as f ( T , H ) y ( 1 ) where , y { E , N , C } . For example , for the tabular premise in Figure 1 , the model should predict E , C , and N for the hypotheses H1 , H2 , and H3 , respectively . Trustworthy Tabular Inference is a table reasoning problem that seeks not just the NLI label , but also relevant evidence from the input table that supports the label prediction . We use T R , a subset of T , to denote the relevant rows or evidence . Then , the task is defined as follows . f ( T , H ) { T R , y } ( 2 ) In our example table , this task will also indicate the evidence rows T R of Producer and Length for hypothesis H1 , Recorded for hypothesis H2 , and Released and Recorded for hypothesis H3 . While the notion of evidence is well - defined for the ENTAIL and CONTRADICT labels , the NEU - TRAL label requires explanation . To decide on the NEUTRAL label , one must first search for relevant rows ( if any ) , i.e. , identify evidence in the premise tables . In fact , this is a causally correct sequential approach . Indeed , INFOTABS has multiple neutral hypotheses that are partly entailed by the table ; if any part of a hypothesis contradicts the table , then the inference label should be CONTRADICT . For example , in our example table , the premise table indicates that the album was recorded in 1978 , emphasizing the importance of the Recorded row for the hypothesis H2 . For NEUTRAL examples , we refer to any such pertinent rows as evidence . Dataset Details . There are several datasets for tabular NLI : TabFact , INFOTABS , and the Se - mEval'21 Task 9 ( Wang et al , 2021b ) and the FEVEROUS'21 shared task ( Aly et al , 2021 ) datasets . We use the INFOTABS data in this work . It contains finer - grained annotation ( e.g. , TabFact lacks NEUTRAL hypotheses ) and more complex reasoning than the others 3 . The dataset consists of 23 , 738 premisehypothesis pairs collected via crowdsourcing on Amazon MTurk . The tabular premises are based on 2 , 540 Wikipedia Infoboxes representing twelve diverse domains , and the hypotheses are short statements paired with NLI labels . All tables contain a title followed by two columns ( cf . Figure 1 ) ; the left columns are keys and the right ones are values ) . In addition to the train and development sets , the data includes multiple test sets , some of which are adversarial : \u03b1 1 represents a standard test set that is both topically and lexically similar to the training data ; \u03b1 2 hypotheses are designed to be lexically adversarial 4 ; and \u03b1 3 tables are drawn from topics unavailable in the training set . The dev and test set , comprising of 7200 table - hypothesis pairs , were recently extended with crowdsourced evidence rows . As one of our contributions , we describe the evidence rows annotation for the training set in the next Section 3 .", "entities": [[341, 342, "DatasetName", "INFOTABS"], [429, 430, "DatasetName", "TabFact"], [431, 432, "DatasetName", "INFOTABS"], [464, 465, "DatasetName", "INFOTABS"], [479, 480, "DatasetName", "TabFact"], [587, 588, "HyperparameterName", "\u03b1"], [606, 607, "HyperparameterName", "\u03b1"], [618, 619, "HyperparameterName", "\u03b1"]]}
{"text": "For the downstream NLI task , the function h is a two - sentence classifier whose inputs are T R ( the rows selected by g ) and the hypothesis H. We use BPR to represent T R as we did for the full table T. Since | T R | | T | , the extraction benefits larger tables ( especially in \u03b1 3 set ) which exceed the model 's token limit .", "entities": [[63, 64, "HyperparameterName", "\u03b1"]]}
{"text": "First , we briefly summarize the models used in our experiments . We investigate both unsupervised ( $ 4.1 ) and supervised ( $ 4.2 ) evidence extraction methods . We use only the extracted evidence as the premise for the tabular inference task ( $ 4.3 ) . We compare both tasks against human performance . As baselines , we use the Word Mover Distance ( WMD ) of and the original DRR ( Neeraja et al , 2021 ) with Top - 4 extracted evidence rows . For DRR ( Re - Rank + Top - S \u03c4 ) , which uses static embeddings , we set the sparsity parameter S = 2 , and the dynamic row selection parameter \u03c4 = 1.0 . Our choice of S is based on the observation that in INFOTABS most ( 92 % ) instances have only one ( 54 % ) or two ( 38 % ) relevant rows . We set \u03b4 to 0.5 for all experiments . For the Sentence Transformer , we used the paraphrase - mpnet - base v2 model ( Reimers and Gurevych , 2019 ) which is a pre - trained with the mpnet - base architecture using several existing paraphrase datasets . This choice is based on performance on the development set . Both the supervised and unsupervised SimCSE models use the same parameters as DRR ( Re - Rank + Top - K \u03c4 ) . We refer to the supervised and unsupervised variants as SimCSE - Supervised and SimCSE - Unsupervised respectively . For the NLI task , we use the BPR representation over extracted evidence T R with the RoBERTa LARGE two sentence classification model . We compare the following settings : ( a ) WMD Top - 3 from , ( b ) No extraction i.e. using the full premise table with the \" para \" representation from , ( c ) DRR Top - 4 , ( d ) DRR ( Re - Rank + Top - 2 ( \u03c4 = 1 ) ) for training , de - velopment and test sets , ( e ) training a supervised classifier with a human oracle i.e. annotated evidence extraction as discussed in $ 3 , and using the best extraction model , i.e. supervised evidence extraction with Hard Negative ( 3\u00d7 ) for the test sets , and ( f ) the human oracle across the training , development , and test sets .", "entities": [[137, 138, "DatasetName", "INFOTABS"], [162, 163, "HyperparameterName", "\u03b4"], [172, 173, "MethodName", "Transformer"], [179, 180, "MethodName", "mpnet"], [199, 200, "MethodName", "mpnet"], [225, 226, "MethodName", "SimCSE"], [253, 254, "MethodName", "SimCSE"], [257, 258, "MethodName", "SimCSE"], [279, 280, "MethodName", "RoBERTa"], [282, 284, "TaskName", "sentence classification"]]}
{"text": "Unsupervised evidence extraction . For RQ1 , Table 2 shows the performance of unsupervised methods . We see that the contextual embedding method , SimCSE - Supervised ( Hypo - Title - Swap + Re - Rank + Top - 2 ( \u03c4 = 1 ) ) , performs the best . Among the static embedding cases , DRR ( Re - Rank + Top - 2 ( \u03c4 = 1 ) ) sees substantial performance improvement over the original DRR baseline . The alignment based approach using SimAlign underperforms , especially on the \u03b1 1 and \u03b1 2 test sets . However , its performance on the \u03b1 3 data , with out of domain and longer tables , is competitive to other methods . Overall , the idea of using Top - S \u03c4 , i.e. , using the dynamic number of rows prediction and Re - Rank ( exact - match based re - ranking ) is beneficial . Previously used approaches such as DRR and WMD have low F1 - score , because of poor precision . Using Re - Rank based on exact match improves the evidence extraction recall . Furthermore , introducing sparsity with Top - S \u03c4 , i.e. considering only the Top - 2 rows ( S=2 ) and dynamic row selection ( \u03c4 = 1 ) substantially enhances evidence extraction precision . Furthermore , the zero weighting of title matches using the Hypo - Title - Swap heuristic , benefits contextualized embedding models such as SimCSE 12 . SimCSE - supervised ( Hypo - Title - Swap + Re - Rank + Top - 2 ( \u03c4 = 1 ) ) outperforms DRR ( Re - Rank + Top - 2 ( \u03c4 = 1 ) ) by 4.3 % ( \u03b1 1 ) , 2.5 % ( \u03b1 2 ) and 5.4 % ( \u03b1 3 ) absolute score . Since the table domains and the NLI reasoning involved for \u03b1 1 and \u03b1 2 are sim - ilar , so is their evidence extraction performance . However , the performance of \u03b1 3 , which contains out - of - domain and longer tables ( an average of thirteen rows , versus nine rows in \u03b1 1 and \u03b1 2 ) is relatively worse . The unsupervised approaches are still 12.69 % ( \u03b1 1 ) , 13.49 % ( \u03b1 2 ) , and 19.81 % ( \u03b1 3 ) behind the human performance , highlighting the challenges of the task . Supervised evidence extraction . For RQ2 , Table 4 shows the performance of the supervised relevant row extraction approaches that use binary classifiers trained with several sampling techniques for irrelevant rows . Overall , adding supervision is advantageous 13 . Furthermore , we observe that using the unsupervised DRR technique to extract challenging irrelevant rows , i.e. , Hard Negative , is more effective than random sampling . Indeed , using random negative examples as the irrelevant rows performs the worst . Not sampling ( 6\u00d7 ) or using only one irrelevant row , namely Hard Negative ( 1\u00d7 ) , also underperforms . We see that employing moderate sampling , i.e. , Hard Negative ( 3\u00d7 ) , performs best across all test sets . The best supervised model with Hard Negative ( 3\u00d7 ) sampling improves evidence extraction performance by 8.7 % ( \u03b1 1 ) , 10.8 % ( \u03b1 2 ) , and 4.2 % ( \u03b1 3 ) absolute score over the best unsupervised model , namely SimCSE - Supervised ( Hypo - Title - Swap + Re - Rank + Top - 2 ( \u03c4 = 1 ) ) . 14 The human oracle outperforms the best supervised model by 4.13 % ( \u03b1 1 ) and 2.65 % ( \u03b1 2 ) absolute scores - a smaller gap than the best unsupervised approach . We also observe that the supervision does not benefit the \u03b1 3 set much , where the performance gap to humans is still about 15.95 % ( only 3.80 % improvement over unsupervised approach ) . We suspect this is because of the distributional changes in \u03b1 3 set noted earlier . 13 We investigate \" How much supervision is adequate ? \" in Appendix A. 14 Although \u03b12 is adversarial owing to label flipping , rendering the NLI task more difficult , both \u03b11 and \u03b12 have instances with the same domain tables and hypotheses with similar reasoning types , making the relevant row extraction task equally challenging . This highlights directions for future improvement via domain adaptation .", "entities": [[24, 25, "MethodName", "SimCSE"], [94, 95, "HyperparameterName", "\u03b1"], [97, 98, "HyperparameterName", "\u03b1"], [108, 109, "HyperparameterName", "\u03b1"], [172, 175, "MetricName", "F1 - score"], [187, 189, "MetricName", "exact match"], [254, 255, "MethodName", "SimCSE"], [257, 258, "MethodName", "SimCSE"], [300, 301, "HyperparameterName", "\u03b1"], [307, 308, "HyperparameterName", "\u03b1"], [314, 315, "HyperparameterName", "\u03b1"], [330, 331, "HyperparameterName", "\u03b1"], [333, 334, "HyperparameterName", "\u03b1"], [352, 353, "HyperparameterName", "\u03b1"], [376, 377, "HyperparameterName", "\u03b1"], [379, 380, "HyperparameterName", "\u03b1"], [394, 395, "HyperparameterName", "\u03b1"], [401, 402, "HyperparameterName", "\u03b1"], [409, 410, "HyperparameterName", "\u03b1"], [569, 570, "HyperparameterName", "\u03b1"], [576, 577, "HyperparameterName", "\u03b1"], [584, 585, "HyperparameterName", "\u03b1"], [596, 597, "MethodName", "SimCSE"], [633, 634, "HyperparameterName", "\u03b1"], [640, 641, "HyperparameterName", "\u03b1"], [665, 666, "HyperparameterName", "\u03b1"], [701, 702, "HyperparameterName", "\u03b1"], [772, 774, "TaskName", "domain adaptation"]]}
{"text": "For RQ3 , we investigate how using only extracted evidence as a premise impacts the performance of the tabular NLI task . Table 3 shows the results . Compared to the baseline DRR , our unsupervised DRR ( Re - Rank + Top - 2 ( \u03c4 = 1 ) ) performs similarly for \u03b1 2 , worse by 1.12 % on \u03b1 1 , and outperforms by 0.95 % on \u03b1 3 . Using evidence extraction with the best supervised model , Hard Negative ( 3\u00d7 ) , trained on human - extracted ( Oracle ) rows results in 2.68 % ( \u03b1 1 ) , 3.93 % ( \u03b1 2 ) , and 4.04 % ( \u03b1 3 ) improvements against DRR . Furthermore , using human extracted ( Oracle ) rows for both training and testing sets outperforms all models - based extraction methods . The human oracle based evidence extraction leads to largest performance improvements of 3.05 % ( \u03b1 1 ) , 4.39 % ( \u03b1 2 ) , and 6.67 % ( \u03b1 3 ) over DRR . Overall , these findings indicate that extracting evidence is beneficial for reasoning in tabular inference task . Despite using human extracted ( Oracle ) rows for both training and testing , the NLI model still falls far behind human reasoning ( Human NLI ) . This gap exists because , in addition to extracting evidence , the INFOTABS hypotheses require inference with the evidence involving common - sense and knowledge , which the NLI component does not adequately perform .", "entities": [[54, 55, "HyperparameterName", "\u03b1"], [62, 63, "HyperparameterName", "\u03b1"], [71, 72, "HyperparameterName", "\u03b1"], [103, 104, "HyperparameterName", "\u03b1"], [110, 111, "HyperparameterName", "\u03b1"], [118, 119, "HyperparameterName", "\u03b1"], [163, 164, "HyperparameterName", "\u03b1"], [170, 171, "HyperparameterName", "\u03b1"], [178, 179, "HyperparameterName", "\u03b1"], [241, 242, "DatasetName", "INFOTABS"]]}
{"text": "We perform an error analysis of how well our proposed supervised extraction model ( Hard Negative ( 3x ) ) performs compared to the human annotators . The model makes two types of errors : a Type I error occurs when an evidence row is marked as irrelevant , whereas Type II error occurs when an irrelevant row is marked as evidence . A Type I error will reduce the model 's precision for the extraction model , whereas a Type II error will decrease the model 's recall . Type I errors are especially concerning for the downstream NLI task . Since mislabeled evidence rows will be absent from the extracted premise , necessary evidence will be omitted , leading to inaccurate entailment labels . On the other hand , with Type II errors , when an irrelevant row is labeled as evidence , the model has to deal with from extra noise in the premise . However , all the required evidence remains . Table 5 shows a comparison of the supervised extraction ( Hard Negative ( 3x ) ) approach with the ground truth human labels on all the three test sets for both error types . On the \u03b1 3 set , Type - I and Type - II errors are substantially higher than \u03b1 1 and \u03b1 2 . This highlights the fact that on the \u03b1 3 set , the model disagrees with with humans the most . Furthermore , the ratio of Type - II over Type - I errors is much higher for \u03b1 3 . This indicates that the super - vised extraction model marks many irrelevant rows as evidence ( Type - II error ) for \u03b1 3 set . The out - ofdomain origin of \u03b1 3 tables , as well as their larger size , might be one explanation for this poor performance . Appendix C provides several examples of both types of errors .", "entities": [[202, 203, "HyperparameterName", "\u03b1"], [218, 219, "HyperparameterName", "\u03b1"], [221, 222, "HyperparameterName", "\u03b1"], [231, 232, "HyperparameterName", "\u03b1"], [261, 262, "HyperparameterName", "\u03b1"], [286, 287, "HyperparameterName", "\u03b1"], [296, 297, "HyperparameterName", "\u03b1"]]}
{"text": "Why Sequential Prediction ? Our choice of the sequential paradigm is motivated by the observation that it enforces a causal structure . Of course , a joint or a multi - task model may make better predictions . However , these models ignore the causal relationship between evidence selection and label prediction ( Herzig et al , 2021 ; Paranjape et al , 2020 ) . Ideally , each row is independent and , its relevance to the hypothesis can be determined on its own . In a joint or a multi - task model that exploits correlations across rows and the final label , irrelevant rows and the NLI label , can erroneously influence row selection . Future Directions . Based on the observations and discussions , we identify the future directions as follows . ( 1 ) Joint Causal Model . To build a joint or a multi - task model that follows the causal reasoning structure , significant changes in model architecture are required . Such a model would first identify important rows and then use them for NLI predictions , but without risking spurious correlations . ( 2 ) How much Supervision is Needed ? As evident from our experiments , relevant row supervision improves the evidence extraction , especially on \u03b1 1 and \u03b1 2 sets compared to unsupervised extraction . But do we need full supervision for all examples ? Is there any lower limit to supervision ? We partially answered this question in the affirmative by training the evidence extraction model with limited supervision ( semi - supervised setting ) , but a deeper analysis is needed to understand the limits . See Appendix A for details . ( 3 ) Improving Zero - shot Domain Performance . As evident from 5.2 , the evidence extraction performance of outof - domain tables in \u03b1 3 needs further improvements , setting up a domain adaptation research question as future work . ( 4 ) Finally , inspired by Neeraja et al ( 2021 ) , we may be able to add explicit knowledge to improve evidence extraction .", "entities": [[215, 216, "HyperparameterName", "\u03b1"], [218, 219, "HyperparameterName", "\u03b1"], [310, 311, "HyperparameterName", "\u03b1"], [319, 321, "TaskName", "domain adaptation"]]}
{"text": "Tabular Reasoning Many recent studies investigate various NLP tasks on semi - structured tabular data , including tabular NLI and fact verification ( Chen et al , 2020b ; , various question answering and semantic parsing tasks ( Zhang and Balog , 2020 ; Pasupat and Liang , 2015 ; Krishnamurthy et al , 2017 ; Abbas et al , 2016 ; Sun et al , 2016 ; Chen et al , 2020c ; Lin et al , 2020 ; Zayats et al , 2021 ; Oguz et al , 2020 ; Chen et al , 2021 , inter alia ) , andtable - to - text generation ( e.g. , Parikh et al , 2020 ; Nan et al , 2021 ; Yoran et al , 2021 ; Chen et al , 2020a ) . Several strategies for representing Wikipedia relational tables are proposed , such as Ta - ble2vec ( Deng et al , 2019 ) , TAPAS ( Herzig et al , 2020 ) , TaBERT ( Yin et al , 2020 ) , TabStruc ( Zhang et al , 2020a ) , TABBIE ( Iida et al , 2021 ) , TabGCN ( Pramanick and Bhattacharya , 2021 ) and RCI ( Glass et al , 2021 ) . Yu et al ( 2018 ; and Neeraja et al ( 2021 ) study pre - training for improving tabular inference . Interpretability and Explainability Model interpretability can either be through explanations or by identifying the evidence for the predictions ( Feng et al , 2018 ; Serrano and Smith , 2019 ; Jain and Wallace , 2019 ; Wiegreffe and Pinter , 2019 ; DeYoung et al , 2020 ; Paranjape et al , 2020 ) . Additionally , NLI models ( e.g. Ribeiro et al , 2016Ribeiro et al , , 2018aZhao et al , 2018 ; Glockner et al , 2018 ; Naik et al , 2018 ; McCoy et al , 2019 ; Nie et al , 2019 ; Liu et al , 2019a ) must be subjected to numerous test sets with adversarial settings . These settings can focus on various aspects of reasoning , such as perturbed premises for evidence selection , zeroshot transferability ( \u03b1 3 ) , counterfactual premises ( Jain et al , 2021 ) , and contrasting hypotheses \u03b1 2 . Recently , Kumar and Talukdar ( 2020 ) introduced Natural - language Inference over Label - specific Explanations ( NILE ) , an NLI approach for generating labels and accompanying faithful explanations using auto - generated label - specific natural language explanations . Our work focuses on the extraction of label - independent evidence for correct inference , rather than on the generation of abstractive explanations for a given label . Comparison with Shared Tasks The Se - mEval'21 Task 9 ( Wang et al , 2021b ) and FEVEROUS'21 shared task ( Aly et al , 2021 ) are conceptually close to this work . The SemEval task focuses on statement verification and evidence extraction using relational tables from scientific articles . In this work , we focus on item evidence extraction for non - scientific Wikipedia Infobox entity tables , proposed a twostage sequential approach , and used the INFOTABS dataset which has complex reasoning and multiple adversarial tests for robust evaluation . The FEVEROUS'21 shared task focuses on verifying information using unstructured and structured evidence from open - domain Wikipedia . Our approach concerns evidence extraction from a single table rather than open - domain document , table or paragraph retrieval . Furthermore , we are only concerned with entity tables rather than relational tables or unstructured text , while the FEVEROUS data has relational tables , unstructured text , and fewer entity tables .", "entities": [[20, 22, "TaskName", "fact verification"], [31, 33, "TaskName", "question answering"], [34, 36, "TaskName", "semantic parsing"], [106, 108, "TaskName", "text generation"], [152, 155, "DatasetName", "Deng et al"], [159, 160, "MethodName", "TAPAS"], [168, 169, "MethodName", "TaBERT"], [186, 187, "MethodName", "TABBIE"], [374, 375, "HyperparameterName", "\u03b1"], [391, 392, "HyperparameterName", "\u03b1"], [396, 397, "DatasetName", "Kumar"], [545, 546, "DatasetName", "INFOTABS"], [618, 619, "DatasetName", "FEVEROUS"]]}
{"text": "To investigate this , we use Hard Negative ( 3x ) with RoBERTa LARGE model as our evidence extraction classifier , which is similar to the full supervision method . To simulate semi - supervision settings , we randomly sample 10 % , 20 % , 30 % , 40 % , and 50 % example instances of the train set in an incremental fashion for model training , where we repeat the random samplings three times . Figure 3 , 4 , and 5 compare the average F1 - score over three runs on the three test sets \u03b1 1 , \u03b1 2 and \u03b1 3 respectively . We discovered that adding some supervision had advantages over not having any supervision . However , we also find that 20 % supervision is adequate for reasonably good evidence extraction with only < 5 % F1 - score gap with full supervision . One key issue we observe is the lack of a visible trend due to significant variation produced by random data sub - sampling . It would be worthwhile to explore if this volatility could be reduced by strategic sampling using an unsupervised extraction model , an active learning framework , and strategic diversity maximizing sampling , which is left as future work .", "entities": [[12, 13, "MethodName", "RoBERTa"], [88, 91, "MetricName", "F1 - score"], [99, 100, "HyperparameterName", "\u03b1"], [102, 103, "HyperparameterName", "\u03b1"], [105, 106, "HyperparameterName", "\u03b1"], [144, 147, "MetricName", "F1 - score"], [198, 200, "TaskName", "active learning"]]}
{"text": "Towards automated evaluation Some aspects of the evaluation dimensions are built into the way the poems are constructed . Form : the haiku - generating subsystem guarantees that the requirements of a grammatical skeleton are met , and the 5/7/5 syllable pattern is guaranteed ( up to the accuracy of the CMU Pronouncing Dictionary ) . Surface form scales up well for rengas . Sense : the haiku generating subsystem uses an ngram model of text likelihood , which will yield a higher score for constructions that match frequently observed phrases . In our first round of experiments with rengas , sense tended to degrade quickly . Our subsequent adaptations to the renga generation algorithm prioritise greater continuitity between links . Topic : we used a vector model of the topic word ( s ) , and can measure the distance to the vector given by the sum of the words in the poem . Emotion : In our experiment with FloWr , we used a quite simple method , filtering a list for the \" most positive \" haikus . Mohammad ( 2016 ) surveys more recent work in NLP on modelling emotion , which could be exploited in future work . Beauty : Waugh ( 1980 ) points out that language is based on a \" hierarchy of signs . . . of ascending complexity , but also one of ascending freedom or creativity , \" and also remarks that a \" poem provides its own ' universe of discourse . ' \" To some extent these criteria pull in opposite directions : towards complexity , and towards coherence , respectively . Our first rengas could not be reasonably described as a ' universe of discourse ' but rather , a ' universe of random nonsense ' . This is improved in the subsequent experiment . Traditional rengas forbid repetition , and discourage overt reflection on themes like death , war , illness , impermanence , religion and sex ( Carley , 2015 ) . Thus , despite being coherent , the repetitive \" military \" theme in the final example above is not appropriate to classical constraints . A reader may identify some fortuitous resonances , e.g. , \" the flower war \" is interesting within the \" afghan \" context established in earlier links - but the system itself does not yet recognise these features . Some paths forward Wiggins and Forth ( 2015 ) use hierarchical models in a system that builds a formative evaluation as it composes or reads sentences , judging how well they match learned patterns . While this seems to have more to do with constraints around typicality , per Waugh , there is room for creativity within hierarchies . Hoey ( 2005 ) makes a convincing argument that satisfying lexical constraints while violating some familiar patterns may come across as interesting and creative . Word similarities can be found using GloVe : this would presumably produce links with more coherent meanings , compared to the edit distance - based measure we used . Ali Javaheri Javid et al ( 2016 ) use information gain to model the aesthetics of cellular automata . Can these ideas be combined to model evolving topic salience , complexity , and coherence ? If the system provided a razo ( the troubadours ' jargon for \" rationale \" ; see Agamben ( 1999 , p. 79 ) ) , we could debug that , and perhaps involve additional AI systems in the process ( Corneli et al , 2015 ) .", "entities": [[48, 49, "MetricName", "accuracy"], [193, 194, "DatasetName", "emotion"], [490, 491, "MethodName", "GloVe"]]}
{"text": "We explore whether contemporary vector - space sentence representation techniques also provide a structured representation of the different messages in \" dogwhistle \" political communication . A dogwhistle refers to a word or phrase used in manipulative communication , usually in a political context . Dogwhistles carry at least two messages : one message intended for the broader community , and another \" payload \" message intended to communicate a specific , less acceptable message to a receptive \" in - group \" . Dogwhistles depend on the \" out - group \" members not picking up on the payload message ( Albertson , 2014 ; Bhat and Klein , 2020 ) . We take several Swedish - language dogwhistles and survey data from the Swedish population about the interpretation of these dogwhistles , and we apply clustering techniques based on the transformerderived representation of the responses . We ask the question : are the responses clearly partitioned in the semantic space , and does the \" sharpness \" of this partitioning reflect the ease of dogwhistle identification by expert annotators ? While there has been work exploring dogwhistles through the lens of linguistics ( Henderson and McCready , 2019 ; Bhat and Klein , 2020 ; Saul , 2018 ) , automated approaches to exploring dogwhistles using NLP techniques are generally lacking ( Xu et al , 2021 ) . Considering the volume of social media data and the extent to which dogwhistles have been employed on these channels , it is important to create new computational techniques to detect and analyze dogwhistles that might succeed at higher data volumes . The first step in accomplishing this is to show that automatic techniques can be used to reliably extend and enhance manual analysis . Dogwhistles can be strategically used , e.g. politically to send a veiled message to one group of voters while avoiding alienating another group ( Bhat and Klein , 2020 ) . This could pose a problem in a representative democracy since the out - group portion of the voter - base are deceived into voting for a certain candidate that might not represent their political views ( Goodin and Saward , 2005 ) . Therefore , we contribute the following : We present a preliminary dataset of a word replacement task by members of the Swedish population as part of a survey of political attitudes , including a manual annotation for dogwhistle identification with inter - annotator agreement ( IAA ; Krippendorff 's \u03b1 ) scores . We use a transformer - based model to represent the responses in a semantic space and apply classification ( SVM ) and clustering techniques ( K - means ) to the vectors . We evaluate the clusterings in terms of cluster purity metrics , and we show that the lower the IAA , the lower the linear separability of the responses in the vector space . We then conclude that a Swedish BERT variant already represents important aspects of the underlying semantics of dogwhistles .", "entities": [[21, 22, "DatasetName", "dogwhistle"], [27, 28, "DatasetName", "dogwhistle"], [176, 177, "DatasetName", "dogwhistle"], [406, 407, "DatasetName", "dogwhistle"], [418, 419, "HyperparameterName", "\u03b1"], [441, 442, "MethodName", "SVM"], [494, 495, "MethodName", "BERT"]]}
{"text": "The results in Table 3 show that a high separability among clusters does indeed correspond with the IAA agreement , which indicates the annotators ease of categorizing a response as \" in - group \" or \" out - group \" . For example , the dogwhistle \" remigration \" had the lowest F1 score for both the dimensionality reduced sentence representations ( 0.72 ) and the original sentence representations ( 0.85 ) , as well as the lowest IAA overall ( 0.74/0.55 ) , as can be seen in table 2 . Similarly , \" suburban gang \" had the highest IAA ( 1/1 ) and very high F1 scores as well ( 0.98/0.97 ) . However , the evaluation of the K - means labeled clusters did not correspond well to the IAA . The evaluation metrics for \" refugee policy \" is higher than \" help on location \" ( 1/0.82 ) despite having a much lower IAA score ( 0.74/0.55 ) . An explanation for this might be that some dogwhistle clusterings are spread over a wider semantic space , while still being linearly separatable ( with an SVM ) from other clusterings . This type of data distribution will still obtain good clustering results . For example , \" enrich \" in table 4 reports the best defined clusters overall ( measured by a low DB score and high CH score ) , while only having a marginally greater F1 score ( 0.98/0.98 ) on the SVM task than \" suburban gang \" ( 0.98/0.97 ) .", "entities": [[46, 47, "DatasetName", "dogwhistle"], [53, 55, "MetricName", "F1 score"], [109, 110, "MetricName", "F1"], [174, 175, "DatasetName", "dogwhistle"], [192, 193, "MethodName", "SVM"], [244, 246, "MetricName", "F1 score"], [251, 252, "MethodName", "SVM"]]}
{"text": "In this paper , we propose to use dice loss in replacement of the standard cross - entropy objective for data - imbalanced NLP tasks . Dice loss is based on the S\u00f8rensen - Dice coefficient ( Sorensen , 1948 ) or Tversky index ( Tversky , 1977 ) , which attaches similar importance to false positives and false negatives , and is more immune to the data - imbalance issue . To further alleviate the dominating influence from easy - negative examples in training , we propose to associate training examples with dynamically adjusted weights to deemphasize easy - negative examples . Experimental results show that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training . With the proposed training objective , we observe significant performance boosts over a wide range of data imbalanced NLP tasks . Notably , we are able to achieve SOTA results on CTB5 , CTB6 and UD1.4 for the part of speech tagging task , and competitive or even better results on CoNLL03 , OntoNotes5.0 , MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identification tasks . The code can be found at https://github.com/ShannonAI/ dice_loss_for_NLP . ( Rajpurkar et al , 2016 ) 10.3 M 175 K 55.9 SQuAD 2.0 ( Rajpurkar et al , 2018 ) 15.4 M 188 K 82.0 QUOREF ( Dasigi et al , 2019 ) 6.52 M 38.6 K 169", "entities": [[8, 10, "MethodName", "dice loss"], [26, 28, "MethodName", "Dice loss"], [34, 35, "MetricName", "Dice"], [115, 117, "MetricName", "F1 score"], [121, 123, "MethodName", "dice loss"], [177, 178, "DatasetName", "CoNLL03"], [186, 189, "TaskName", "named entity recognition"], [193, 196, "TaskName", "machine reading comprehension"], [197, 199, "TaskName", "paraphrase identification"], [222, 223, "DatasetName", "SQuAD"], [236, 237, "DatasetName", "QUOREF"]]}
{"text": "Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension . Table 1 gives concrete examples : for the Named Entity Recognition ( NER ) task Nadeau and Sekine , 2007 ) , most tokens are backgrounds with tagging class O. Specifically , the number of tokens with tagging class O is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset ; Dataimbalanced issue is more severe for MRC tasks ( Rajpurkar et al , 2016 ; Nguyen et al , 2016 ; Rajpurkar et al , 2018 ; Ko\u010disk\u1ef3 et al , 2018 ; Dasigi et al , 2019 ) with the value of negative - positive ratio being 50 - 200 , which is due to the reason that the task of MRC is usually formalized as predicting the starting and ending indexes conditioned on the query and the context , and given a chunk of text of an arbitrary length , only two tokens are positive ( or of interest ) with all the rest being background . Data imbalance results in the following two issues : ( 1 ) the training - test discrepancy : Without balancing the labels , the learning process tends to converge to a point that strongly biases towards class with the majority label . ( Lample et al , 2016 ; Devlin et al , 2018 ; Yu et al , 2018a ; McCann et al , 2018 ; Ma and Hovy , 2016 ; , handles neither of the issues . To handle the first issue , we propose to replace CE or MLE with losses based on the S\u00f8rensen - Dice coefficient ( Sorensen , 1948 ) or Tversky index ( Tversky , 1977 ) . The S\u00f8rensen - Dice coefficient , dice loss for short , is the harmonic mean of precision and recall . It attaches equal importance to false positives ( FPs ) and false negatives ( FNs ) and is thus more immune to data - imbalanced datasets . Tversky index extends dice loss by using a weight that trades precision and recall , which can be thought as the approximation of the F \u03b2 score , and thus comes with more flexibility . Therefore , we use dice loss or Tversky index to replace CE loss to address the first issue . Only using dice loss or Tversky index is not enough since they are unable to address the dominating influence of easy - negative examples . This is intrinsically because dice loss is actually a soft version of the F1 score . Taking the binary classification task as an example , at test time , an example will be classified as negative as long as its probability is smaller than 0.5 , but training will push the value to 0 as much as possible . The rest of this paper is organized as follows : related work is presented in Section 2 . We describe different proposed losses in Section 3 . Experimental results are presented in Section 4 . We perform ablation studies in Section 5 , followed by a brief conclusion in Section 6 . 2 Related Work", "entities": [[16, 19, "TaskName", "machine reading comprehension"], [28, 31, "TaskName", "Named Entity Recognition"], [32, 33, "TaskName", "NER"], [72, 73, "DatasetName", "CoNLL03"], [291, 292, "MetricName", "Dice"], [310, 311, "MetricName", "Dice"], [313, 315, "MethodName", "dice loss"], [357, 359, "MethodName", "dice loss"], [379, 380, "HyperparameterName", "\u03b2"], [393, 395, "MethodName", "dice loss"], [401, 402, "MetricName", "loss"], [410, 412, "MethodName", "dice loss"], [437, 439, "MethodName", "dice loss"], [446, 448, "MetricName", "F1 score"], [486, 487, "DatasetName", "0"]]}
{"text": "The background - object label imbalance issue is severe and thus well studied in the field of object detection ( Li et al , 2015 ; Girshick , 2015 ; Girshick et al , 2013 ; . The idea of hard negative mining ( HNM ) ( Girshick et al , 2013 ) has gained much attention recently . Pang et al ( 2019 ) proposed a novel method called IoU - balanced sampling and designed a ranking model to replace the conventional classification task with an average - precision loss to alleviate the class imbalance issue . The efforts made on object detection have greatly inspired us to solve the data imbalance issue in NLP . Sudre et al ( 2017 ) addressed the severe class imbalance issue for the image segmentation task . They proposed to use the class re - balancing property of the Generalized Dice Loss as the training objective for unbalanced tasks . Shen et al ( 2018 ) investigated the influence of Dice - based loss for multi - class organ segmentation using a dataset of abdominal CT volumes . Kodym et al ( 2018 ) 3 Losses", "entities": [[17, 19, "TaskName", "object detection"], [70, 74, "MethodName", "IoU - balanced sampling"], [90, 91, "MetricName", "loss"], [102, 104, "TaskName", "object detection"], [148, 150, "MethodName", "Dice Loss"], [168, 169, "MetricName", "Dice"], [171, 172, "MetricName", "loss"]]}
{"text": "The vanilla cross entropy ( CE ) loss is given by : CE = \u2212 1 N i j { 0 , 1 } y ij log p ij ( 1 ) As can be seen from Eq.1 , each x i contributes equally to the final objective . Two strategies are normally used to address the the case where we wish that not all x i are treated equally : associating different classes with different weighting factor \u03b1 or resampling the datasets . For the former , Eq.1 is adjusted as follows : Weighted CE = \u2212 1 N i \u03b1 i j { 0 , 1 } y ij log p ij ( 2 ) where \u03b1 i [ 0 , 1 ] may be set by the inverse class frequency or treated as a hyperparameter to set by cross validation . In this work , we use lg ( n\u2212nt nt + K ) to calculate the coefficient \u03b1 , where n t is the number of samples with class t and n is the total number of samples in the training set . K is a hyperparameter to tune . Intuitively , this equation assigns less weight to the majority class and more weight to the minority class . The data resampling strategy constructs a new dataset by sampling training examples from the original dataset based on human - designed criteria , e.g. extracting equal training samples from each class . Both strategies are equivalent to changing the data distribution during training and thus are of the same nature . Empirically , these two methods are not widely used due to the trickiness of selecting \u03b1 especially for multi - class classification tasks and that inappropriate selection can easily bias towards rare classes ( Valverde et al , 2017 ) .", "entities": [[7, 8, "MetricName", "loss"], [20, 21, "DatasetName", "0"], [78, 79, "HyperparameterName", "\u03b1"], [101, 102, "HyperparameterName", "\u03b1"], [105, 106, "DatasetName", "0"], [118, 119, "HyperparameterName", "\u03b1"], [121, 122, "DatasetName", "0"], [161, 162, "HyperparameterName", "\u03b1"], [168, 171, "HyperparameterName", "number of samples"], [179, 182, "HyperparameterName", "number of samples"], [279, 280, "HyperparameterName", "\u03b1"], [282, 286, "TaskName", "multi - class classification"]]}
{"text": "S\u00f8rensen - Dice coefficient ( Sorensen , 1948 ; Dice , 1945 ) , dice coefficient ( DSC ) for short , is an F1oriented statistic used to gauge the similarity of two sets . Given two sets A and B , the vanilla dice coefficient between them is given as follows : DSC ( A , B ) = 2 | A \u2229 B | | A | + | B | ( 3 ) In our case , A is the set that contains all positive examples predicted by a specific model , and B is the set of all golden positive examples in the dataset . When applied to boolean data with the definition of true positive ( TP ) , false positive ( FP ) , and false negative ( FN ) , it can be then written as follows : DSC = 2TP 2TP + FN + FP = 2 TP TP+FN TP TP+FP TP TP+FN + TP TP+FP = 2Pre \u00d7 Rec Pre+Rec = F 1 ( 4 ) For an individual example x i , its corresponding dice coefficient is given as follows : DSC ( x i ) = 2p i1 y i1 p i1 + y i1 ( 5 ) As can be seen , a negative example ( y i1 = 0 ) does not contribute to the objective . For smoothing purposes , it is common to add a \u03b3 factor to both the nominator and the denominator , making the form to be as follows ( we simply set \u03b3 = 1 in the rest of Loss Formula ( one sample x i ) CE \u2212 j { 0 , 1 } y ij log p ij WCE \u2212\u03b1 i j { 0 , 1 } y ij log p ij DL 1 \u2212 2p i1 y i1 + \u03b3 p 2 i1 + y 2 i1 + \u03b3 TL 1 \u2212 p i1 y i1 + \u03b3 p i1 y i1 + \u03b1 p i1 y i0 + \u03b2 p i0 y i1 + \u03b3 DSC 1 \u2212 2 ( 1\u2212p i1 ) p i1 y i1 + \u03b3 ( 1\u2212p i1 ) p i1 + y i1 + \u03b3 FL \u2212\u03b1 i j { 0 , 1 } ( 1 \u2212 p ij ) \u03b3 log p ij DSC ( x i ) = 2p i1 y i1 + \u03b3 p i1 + y i1 + \u03b3 ( 6 ) As can be seen , negative examples whose DSC is \u03b3 p i1 + \u03b3 , also contribute to the training . Additionally , Milletari et al ( 2016 ) proposed to change the denominator to the square form for faster convergence , which leads to the following dice loss ( DL ) : DL = 1 N i 1 \u2212 2p i1 y i1 + \u03b3 p 2 i1 + y 2 i1 + \u03b3 ( 7 ) Another version of DL is to directly compute setlevel dice coefficient instead of the sum of individual dice coefficient , which is easier for optimization : DL = 1 \u2212 2 i p i1 y i1 + \u03b3 i p 2 i1 + i y 2 i1 + \u03b3 ( 8 ) Tversky index ( TI ) , which can be thought as the approximation of the F \u03b2 score , extends dice coefficient to a more general case . Given two sets A and B , tversky index is computed as follows : TI = | A \u2229 B | | A \u2229 B | + \u03b1 | A\\B | + \u03b2 | B\\A | ( 9 ) Tversky index offers the flexibility in controlling the tradeoff between false - negatives and falsepositives . It degenerates to DSC if \u03b1 = \u03b2 = 0.5 . The Tversky loss ( TL ) is thus given as follows : TL = 1 N i 1 \u2212 pi1yi1 + \u03b3 pi1yi1 + \u03b1 pi1yi0 + \u03b2 pi0yi1 + \u03b3 ( 10 )", "entities": [[2, 3, "MetricName", "Dice"], [9, 10, "MetricName", "Dice"], [221, 222, "DatasetName", "0"], [240, 241, "HyperparameterName", "\u03b3"], [261, 262, "HyperparameterName", "\u03b3"], [280, 281, "DatasetName", "0"], [294, 295, "DatasetName", "0"], [311, 312, "HyperparameterName", "\u03b3"], [320, 321, "HyperparameterName", "\u03b3"], [329, 330, "HyperparameterName", "\u03b3"], [335, 336, "HyperparameterName", "\u03b1"], [341, 342, "HyperparameterName", "\u03b2"], [347, 348, "HyperparameterName", "\u03b3"], [361, 362, "HyperparameterName", "\u03b3"], [372, 373, "HyperparameterName", "\u03b3"], [378, 379, "DatasetName", "0"], [388, 389, "HyperparameterName", "\u03b3"], [403, 404, "HyperparameterName", "\u03b3"], [410, 411, "HyperparameterName", "\u03b3"], [424, 425, "HyperparameterName", "\u03b3"], [428, 429, "HyperparameterName", "\u03b3"], [462, 464, "MethodName", "dice loss"], [480, 481, "HyperparameterName", "\u03b3"], [489, 490, "HyperparameterName", "\u03b3"], [530, 531, "HyperparameterName", "\u03b3"], [541, 542, "HyperparameterName", "\u03b3"], [561, 562, "HyperparameterName", "\u03b2"], [600, 601, "HyperparameterName", "\u03b1"], [605, 606, "HyperparameterName", "\u03b2"], [633, 634, "HyperparameterName", "\u03b1"], [635, 636, "HyperparameterName", "\u03b2"], [641, 642, "MetricName", "loss"], [660, 661, "HyperparameterName", "\u03b3"], [663, 664, "HyperparameterName", "\u03b1"], [666, 667, "HyperparameterName", "\u03b2"], [669, 670, "HyperparameterName", "\u03b3"]]}
{"text": "Consider a simple case where the dataset consists of only one example x i , which is classified as positive as long as p i1 is larger than 0.5 . The computation of F 1 score is actually as follows : The derivative of DSC approaches zero right after p exceeds 0.5 , and for the other losses , the derivatives reach 0 only if the probability is exactly 1 , which means they will push p to 1 as much as possible . F1 ( x i ) = 2 I ( p i1 > 0.5 ) y i1 I ( p i1 > 0.5 ) + y i1 ( 11 ) Comparing Eq.5 with Eq.11 , we can see that Eq.5 is actually a soft form of F 1 , using a continuous p rather than the binary I ( p i1 > 0 . To address this issue , we propose to multiply the soft probability p with a decaying factor ( 1 \u2212 p ) , changing Eq.11 to the following adaptive variant of DSC : DSC ( x i ) = 2 ( 1 \u2212 p i1 ) p i1 y i1 + \u03b3 ( 1 \u2212 p i1 ) p i1 + y i1 + \u03b3 ( 12 ) One can think ( 1 \u2212 p i1 ) as a weight associated with each example , which changes as training proceeds . The intuition of changing p i1 to ( 1 \u2212 p i1 ) p i1 is to push down the weight of easy examples . For easy examples whose probability are approaching 0 or 1 , ( 1 \u2212 p i1 ) p i1 makes the model attach significantly less focus to them . A close look at Eq.12 reveals that it actually mimics the idea of focal loss ( FL for short ) ( Lin et al , 2017 ) In Table 2 , we summarize all the aforementioned losses . Figure 1 gives an explanation from the perspective in derivative : The derivative of DSC approaches zero right after p exceeds 0.5 , which suggests the model attends less to examples once they are correctly classified . But for the other losses , the derivatives reach 0 only if the probability is exactly 1 , which means they will push p to 1 as much as possible .", "entities": [[62, 63, "DatasetName", "0"], [84, 85, "MetricName", "F1"], [145, 146, "DatasetName", "0"], [198, 199, "HyperparameterName", "\u03b3"], [211, 212, "HyperparameterName", "\u03b3"], [270, 271, "DatasetName", "0"], [305, 307, "MethodName", "focal loss"], [376, 377, "DatasetName", "0"]]}
{"text": "Settings Part - of - speech tagging ( POS ) is the task of assigning a part - of - speech label ( e.g. , noun , verb , adjective ) to each word in a given text . In this paper , we choose BERT ( Devlin et al , 2018 ) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank ( Xue et al , 2005 ) 5.0/6.0 and UD1.4 and English datasets including Wall Street Journal ( WSJ ) and the dataset proposed by Ritter et al ( 2011 ) . We report the span - level micro - averaged precision , recall and F1 for evaluation . Baselines We used the following baselines : Results", "entities": [[1, 7, "TaskName", "Part - of - speech tagging"], [16, 19, "DatasetName", "part - of"], [45, 46, "MethodName", "BERT"], [67, 69, "DatasetName", "Chinese Treebank"], [114, 115, "MetricName", "F1"]]}
{"text": "Settings Named entity recognition ( NER ) is the task of detecting the span and semantic category of entities within a chunk of text . Our implementation uses the current state - of - the - art model proposed by as the backbone , and changes the MLE loss to DSC loss . Datasets that we use include OntoNotes4.0 ( Pradhan et al , 2011 ) , MSRA ( Levow , 2006 ) , CoNLL2003 ( Sang and Meulder , 2003 and OntoNotes5.0 ( Pradhan et al , 2013 ) . We report span - level micro - averaged precision , recall and F1 . Baselines We use the following baselines : ELMo : a tagging model with pretraining from Peters et al ( 2018 ) . Lattice - LSTM : Zhang and Yang ( 2018 ) Results", "entities": [[1, 4, "TaskName", "Named entity recognition"], [5, 6, "TaskName", "NER"], [48, 49, "MetricName", "loss"], [51, 52, "MetricName", "loss"], [74, 75, "DatasetName", "CoNLL2003"], [103, 104, "MetricName", "F1"], [112, 113, "MethodName", "ELMo"], [129, 130, "MethodName", "LSTM"]]}
{"text": "Settings The task of machine reading comprehension ( MRC ) ( Seo et al , 2016 ; Wang and Jiang , 2016 ; Shen et al , 2017 ; predicts the answer span in the passage given a question and the passage . We followed the standard protocols in Seo et al ( 2016 ) , in which the start and end indexes of answer are predicted . We report Extract Match ( EM ) as well as F1 score on validation set . We use three datasets on this task : SQuAD v1.1 , SQuAD v2.0 ( Rajpurkar et al , 2016 ( Rajpurkar et al , , 2018 and Quoref ( Dasigi et al , 2019 ) . Baselines We used the following baselines : enables learning bidirectional contexts . Results Table 6 shows the experimental results for MRC task . With either BERT or XLNet , our proposed DSC loss obtains significant performance boost on both EM and F1 . For SQuADv1.1 , our proposed method outperforms XLNet by +1.25 in terms of F1 score and +0.84 in terms of EM . For SQuAD v2.0 , the proposed method achieves 87.65 on EM and 89.51 on F1 . On QuoRef , the proposed method surpasses XLNet by +1.46 on EM and +1.41 on F1 .", "entities": [[4, 7, "TaskName", "machine reading comprehension"], [73, 74, "MetricName", "EM"], [78, 80, "MetricName", "F1 score"], [92, 93, "DatasetName", "SQuAD"], [95, 96, "DatasetName", "SQuAD"], [111, 112, "DatasetName", "Quoref"], [145, 146, "MethodName", "BERT"], [147, 148, "MethodName", "XLNet"], [152, 153, "MetricName", "loss"], [159, 160, "MetricName", "EM"], [161, 162, "MetricName", "F1"], [170, 171, "MethodName", "XLNet"], [176, 178, "MetricName", "F1 score"], [183, 184, "MetricName", "EM"], [186, 187, "DatasetName", "SQuAD"], [195, 196, "MetricName", "EM"], [199, 200, "MetricName", "F1"], [202, 203, "DatasetName", "QuoRef"], [208, 209, "MethodName", "XLNet"], [212, 213, "MetricName", "EM"], [216, 217, "MetricName", "F1"]]}
{"text": "Settings Paraphrase identification ( PI ) is the task of identifying whether two sentences have the same meaning or not . We conduct experiments on the two widely - used datasets : MRPC ( Dolan and Brockett , 2005 ) and QQP . F1 score is reported for comparison . We use BERT ( Devlin et al , 2018 ) and XLNet ( Yang et al , 2019 ) as baselines . Results Table 7 shows the results . We find that replacing the training objective with DSC introduces performance boost for both settings , +0.58 for MRPC and +0.73 for QQP .", "entities": [[1, 3, "TaskName", "Paraphrase identification"], [32, 33, "DatasetName", "MRPC"], [41, 42, "DatasetName", "QQP"], [43, 45, "MetricName", "F1 score"], [52, 53, "MethodName", "BERT"], [61, 62, "MethodName", "XLNet"], [97, 98, "DatasetName", "MRPC"], [101, 102, "DatasetName", "QQP"]]}
{"text": "It is interesting to see how differently the proposed objectives affect datasets imbalanced to different extents . We use the paraphrase identification dataset QQP ( 37 % positive and 63 % negative ) for studies . To construct datasets with different imbalance degrees , we used the original QQP dataset to construct synthetic training sets with different positive - negative ratios . Models are trained on these different synthetic sets and then test on the same original test set . Results are shown in Table 8 . We first look at the first line , with all results obtained using the MLE objective . We can see that + positive outperforms original , and + negative underperforms original . This is in line with our expectation since + positive creates a balanced dataset while + negative creates a more imbalanced dataset . Despite the fact that - negative creates a balanced dataset , the number of training data decreases , resulting in inferior performances . DSC achieves the highest F1 score across all datasets . Specially , for + positive , DSC achieves minor improvements ( +0.05 F1 ) over DL . In contrast , it significantly outperforms DL for + negative dataset . This is in line with our expectation since DSC helps more on more imbalanced datasets . The performance of FL and DL are not consistent across different datasets , while DSC consistently performs the best on all datasets .", "entities": [[20, 22, "TaskName", "paraphrase identification"], [23, 24, "DatasetName", "QQP"], [48, 49, "DatasetName", "QQP"], [169, 171, "MetricName", "F1 score"], [187, 188, "MetricName", "F1"]]}
{"text": "As mentioned in Section 3.3 , Tversky index ( TI ) offers the flexibility in controlling the tradeoff between false - negatives and false - positives . In this subsection , we explore the effect of hyperparameters ( i.e. , \u03b1 and \u03b2 ) in TI to test how they manipulate the tradeoff . We conduct experiments on the Chinese OntoNotes4.0 NER dataset and English QuoRef MRC dataset . Experimental results are shown in Table 10 . The highest F1 on Chinese OntoNotes4.0 is 84.67 when \u03b1 is set to 0.6 while for QuoRef , the highest F1 is 68.44 when \u03b1 is set to 0.4 . In addition , we can observe that the performance varies a lot as \u03b1 changes in distinct datasets , which shows that the hyperparameters \u03b1 , \u03b2 acturally play an important role in TI .", "entities": [[40, 41, "HyperparameterName", "\u03b1"], [42, 43, "HyperparameterName", "\u03b2"], [61, 62, "TaskName", "NER"], [65, 66, "DatasetName", "QuoRef"], [79, 80, "MetricName", "F1"], [86, 87, "HyperparameterName", "\u03b1"], [93, 94, "DatasetName", "QuoRef"], [97, 98, "MetricName", "F1"], [101, 102, "HyperparameterName", "\u03b1"], [120, 121, "HyperparameterName", "\u03b1"], [131, 132, "HyperparameterName", "\u03b1"], [133, 134, "HyperparameterName", "\u03b2"]]}
{"text": "In Table 10 : The effect of hyperparameters in Tversky Index . We set \u03b2 = 1 \u2212 \u03b1 and thus we only list \u03b1 here . to achieve significant performance boost without changing model architectures . annotation of grammar ( parts of speech , morphological features , and syntactic dependencies ) across different human languages . In this work , we use UD1.4 for Chinese POS tagging .", "entities": [[14, 15, "HyperparameterName", "\u03b2"], [18, 19, "HyperparameterName", "\u03b1"], [24, 25, "HyperparameterName", "\u03b1"]]}
{"text": "Table 3 presents the classification results for the baselines , the full feature set , and the selected features for both logistic regression and random forests . Results are reported using a weighted F1 score , which is a classification accuracy measure based on the mean between the precision and recall after adjusting for class imbalance . The linguistic and clinical content features improve predictive accuracy above the baselines , yielding a higher F1 score than the strongest baseline ( .67 compared to .59 ) . The reduced feature set does not lead to a meaningful performance drop compared to the full feature set , suggesting that no signal was lost due to feature elimination . Figure 2 reports the eight best - performing features : UMLS phrases count , Unique word count , Polysemic word count , Average noun phrase length , Automated readability index , Prepositional phrases , UMLS distinct terms count , and Concreteness ratio .", "entities": [[21, 23, "MethodName", "logistic regression"], [33, 35, "MetricName", "F1 score"], [40, 41, "MetricName", "accuracy"], [65, 66, "MetricName", "accuracy"], [73, 75, "MetricName", "F1 score"], [126, 127, "DatasetName", "UMLS"], [150, 151, "DatasetName", "UMLS"]]}
{"text": "The results presented in the previous section lead to three main findings : i ) the linguistic characteristics of the items carry signal relevant to response 4 Classes were balanced using the balanced subample setting of the class weight parameter in Scikit - learn 's RandomForrestClassifier process complexity ; ii ) no individual features stand out as strong predictors , and iii ) the most important features were those related to syntax and semantics . The first of these findings relates to the fact that the linguistic characteristics of the items carry signal that is predictive of response process complexity , revealing that the problems posed by lowcomplexity and high - complexity items are described using slightly different language . While this signal outperformed several baselines , the overall low predictive utility of the models suggests that there are other factors , yet to be captured , that have a significant effect on response process complexity . The retention of 56 features indicates that individual linguistic predictors provide a weak classification signal but , taken together , they complement each other in a way that ultimately provides a higher accuracy . The fact that there are many predictive features with none standing out is also a positive evaluation outcome for item writing quality , as it shows that the response process complexity associated with an item is not distributed along a small number of linguistic parameters . The most important features that helped with classification were those related to syntax and semantics ( Figure 2 ) . The poor performance of the Word Count baseline suggests that differences in response process complexity can not be explained solely by item length and that more complex linguistic features capture some of the nuance in the response process . As can be seen in Figure 2 , high - complexity items contain a slightly higher number of UMLS phrases and ( distinct ) medical terms , as well as a higher number of unique words . These features suggest high - complexity items re - peat words less frequently and may contain a higher concentration of new information and specialized terminology than low - complexity items . The individual phrases in high - complexity items are also slightly longer , which naturally influences readability metrics that are based on word and sentence length , such as the Automated Readability Index ( higher values are indicative of a more complex text ) . Prepositional phrases were also identified as more important than other phrase types in distinguishing between response process complexity . Prepositional phrases often serve as modifiers of the primary noun phrase and the higher number of prepositional phrases in the high - complexity items suggests the use of more specific descriptions ( e.g. , \" small cell carcinoma of the ovary \" instead of just \" small cell carcinoma \" ) . The words contained in the high - complexity items also have slightly higher concreteness levels , providing another indication that they may contain more terms , as terms tend to be more concrete than common words . Finally , the words contained in the high - complexity items also tend to have more possible meanings , as indicated by the polysemous word count variable , which results in higher complexity owing to disambiguation efforts . Overall , these features indicate that the language used in the low - complexity items is less ambiguous and descriptive , and potentially contains fewer medical terms . One limitation of the study is the fact that it treats item difficulty and time intensiveness as independent variables . This may not always be the case , as examinees do employ strategies to optimize their time . Given finite time limits , examinees may ig - nore time intensive items if they believe the time needed for such items can be better utilized attempting other , less time intensive items . Therefore , the relationship between difficulty and response time and their association with item text would differ for exams that do not impose strict time limits . When using data - driven approaches to defining item classes , our data did not lend itself to a categorization that would allow investigating high difficulty / low response time items and vice - versa . While the approach taken in this paper has a higher ecological validity , studying such cases in the future may lead to a greater understanding of various aspects of response process complexity and their relationship to item text . Other future work includes exploration of potential item position effects .", "entities": [[189, 190, "MetricName", "accuracy"], [314, 315, "DatasetName", "UMLS"]]}
{"text": "Measuring semantic textual similarity ( STS ) is the task of determining the similarity between two different text passages . The task is important for various natural language processing tasks like topic detection or automated text summarization because languages are versatile and authors can express similar content or even the same content with different words . Predicting semantic textual similarity has been a recurring task in SemEval challenges ( Agirre et al , 2015 ; Agirre et al , 2014 ; Agirre et al , 2013 ; Agirre et al , 2012 ) . As in previous years , the purpose of the STS task is the development of systems that automatically predict the semantic similarity of two sentences in the continuous interval [ 0 , 5 ] where 0 represents a complete dissimilarity and 5 denotes a complete semantic equivalence between the sentences ( Agirre et al , 2015 ) . The organizers provide sentence pairs whose semantic similarities have to be predicted by the contestants . The quality of a system is determined by calculating the Pearson correlation between the predicted values and a human gold standard that has been created by crowdsourcing . The data from previous STS tasks can be used for training supervised methods . The test data consists of text content from different sources . In this year 's shared task , the systems are tested on five different categories with different topics and varying textual characteristics like text length or spelling errors : answer - answer , plagiarism , postediting , headlines , and question - question The remainder of the paper is structured as follows : Section 2 discusses related approaches to automatically determining semantic textual similarity . Section 3 describes our three methods in detail . We discuss their results in section 4 . Finally , we conclude in chapter 5 and outline future work .", "entities": [[1, 4, "TaskName", "semantic textual similarity"], [5, 6, "TaskName", "STS"], [35, 37, "TaskName", "text summarization"], [57, 60, "TaskName", "semantic textual similarity"], [103, 104, "TaskName", "STS"], [114, 116, "TaskName", "semantic similarity"], [124, 125, "DatasetName", "0"], [129, 130, "DatasetName", "0"], [178, 180, "MetricName", "Pearson correlation"], [200, 201, "TaskName", "STS"], [282, 285, "TaskName", "semantic textual similarity"]]}
{"text": "The Overlap method measures the token - based overlap between two sentences . Therefore , we need to define a similarity function for tokens : We first try to identify a textual similarity of 1 by comparing the lower case lemmas of both tokens or by checking if their most common WordNet synsets are the same . We assess their similarity as 0.5 if they share any synset . If this is not the case , we use word2vec ( Mikolov et al , 2013 ) with the 300 - dimensional GoogleNews - vectors - negative300 model . We look up both words ( or their lemmas if the words are not present in the model ) and calculate the cosine similarity of their word embeddings . Otherwise , we return a default value . This yields the following similarity function for two tokens : sim ( t 1 , t 2 ) : = \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 if t 1 .lemma = = t 2 .lemma 1 if t 1 and t 2 have the same most common synset 0.5 if t 1 and t 2 share any synset d ( t 1 , t 2 ) if t 1 and t 2 have word embeddings default otherwise where d ( t 1 , t 2 ) denotes the cosine similarity between the two word embeddings of the tokens . Given a token t from one sentence , we calculate its similarity to another sentence S by taking the maximum similarity between t and all tokens of S : msim ( t , S ) : = max t 2 S sim ( t , t 2 ) We define the similarity score between two sentences in [ 0 , 1 ] as follows : ssim ( s 1 , s 2 ) : = t s 1 msim ( t , s 2 ) 2 | s 1 | + t s 2 msim ( t , s 1 ) 2 | s 2 | To predict the semantic similarity score in [ 0 , 5 ] , we multiply ssim by 5 , however , this does not change our evaluation results because the Pearson correlation is scale invariant : STS ( s 1 , s 2 ) : = 5 ssim ( s 1 , s 2 ) We observed that some samples in the STS 2016 test data consist almost entirely of stopwords . For example , the STS 2016 evaluation data contained a sample with the sentences \" I think you should do both . \" and \" You should do both . \" before the final filtering . After filtering stop words , the first sentence would only contain the word \" think \" and the second sentence would be empty , which would result in a predicted score of zero . To avoid these extreme cases , we do not filter stop words if this would result in a sentence length of less than two tokens in both sentences .", "entities": [[124, 126, "TaskName", "word embeddings"], [227, 229, "TaskName", "word embeddings"], [247, 249, "TaskName", "word embeddings"], [311, 312, "DatasetName", "0"], [318, 319, "TaskName", "ssim"], [362, 364, "TaskName", "semantic similarity"], [367, 368, "DatasetName", "0"], [374, 375, "TaskName", "ssim"], [389, 391, "MetricName", "Pearson correlation"], [395, 396, "TaskName", "STS"], [406, 407, "TaskName", "ssim"], [421, 422, "TaskName", "STS"], [435, 436, "TaskName", "STS"]]}
{"text": "Hidden layer Output layer All samples are preprocessed as described in section 3.1.1 . For each sample ( s 1 , s 2 , gs ) in the training set , we create a vocabulary list of the lowercase lemmas from both sentences . Lemmas that share a most common synset in WordNet are grouped together . Let n be the size of the vocabulary . We create two bag - of - words vectors bow s 1 and bow s 2 . For each lemma l , we calculate the minimum number of times l occurs in each sentence and the delta between the minimum and the maximum : min i : = min ( bow s 1 [ i ] , bow s 2 [ i ] ) | \u2206 i | : = | bow s 1 [ i ] \u2212 bow s 2 [ i ] | As input vectors for the neural net , we build two sums per sample and use them as the two dimensional feature vector ( sameWords , notSameWords ) for the expected output gs : 1 shows an example of the same word neural network method for the two input sentences \" Tim plays the guitar \" and \" Tim likes guitar songs \" , which have the input vector ( 2 , 3 ) . We trained the neural net until the error rate between two iterations did not change more than \u03b5 = 10 \u22125 . sameWords : = n i=1 min i notSameWords : = n i=1 | \u2206 i | Table i Lemma bow s 1 bow s 2 min i | \u2206 i | 1 tim 1 1 1 0 2 play 1 0 0 1 3 guitar 1 1 1 0 4 like 0 1 0 1 5 song 0 1 0 1 2 3", "entities": [[85, 86, "DatasetName", "lemma"], [243, 244, "HyperparameterName", "\u03b5"], [266, 267, "DatasetName", "Lemma"], [284, 285, "DatasetName", "0"], [288, 289, "DatasetName", "0"], [289, 290, "DatasetName", "0"], [296, 297, "DatasetName", "0"], [299, 300, "DatasetName", "0"], [301, 302, "DatasetName", "0"], [305, 306, "DatasetName", "0"], [307, 308, "DatasetName", "0"]]}
{"text": "The surface - level similarity can to some extent ( although not entirely ) capture the semantic similarity between documents . Let s 1 and s 2 be the reference and the candidate documents respectively . We compute the components f 1 , f 2 R as follows : f 1 ( s 1 , s 2 , N ) = m N l s 1 N f 2 ( s 1 , s 2 , N ) = N n=1 m N l s 2 n 1 N where m N is the number of matched N - grams between s 1 and s 2 , l s 1 N denotes the total number of Ngrams in s 1 and l s 2 n is the total number of n - grams in s 2 . f 1 is the common ROUGE ( Lin , 2004 ) metric used in automatic text summarization and f 2 is a modified version of the BLEU ( Papineni et al , 2002 ) metric ( standard machine translation metric ) where the brevity penalty is eliminated . Note that f 1 can be interpreted as the recall - oriented surface similarity and f 2 as the precision - oriented one .", "entities": [[16, 18, "TaskName", "semantic similarity"], [152, 154, "TaskName", "text summarization"], [163, 164, "MetricName", "BLEU"], [174, 176, "TaskName", "machine translation"]]}
{"text": "To model the topical similarity between two documents , we use Latent Dirichlet Allocation ( LDA ) ( Blei et al , 2003 ) to train models on the English Wikipedia . For both documents s 1 and s 2 , we compute the topic distributions \u03b8 1 and \u03b8 2 and use the Hellinger distance to measure the similarity between the documents . This can be formally written as f 4 ( s 1 , s 2 ) = 1 \u2212 1 \u221a 2 k i=1 \u03b8 1 i \u2212 \u03b8 2 i 2 where k represents the number of learned LDA topics .", "entities": [[15, 16, "MethodName", "LDA"], [46, 47, "HyperparameterName", "\u03b8"], [49, 50, "HyperparameterName", "\u03b8"], [87, 88, "HyperparameterName", "\u03b8"], [91, 92, "HyperparameterName", "\u03b8"], [102, 103, "MethodName", "LDA"]]}
{"text": "In this years shared task , 117 runs were submitted . We achieved weighted mean Pearson correlations of 0.71134 , 0.67502 and 0.62078 . In this year 's run , our best result was the Overlap method , followed by the Same Word Neural Network method and the Deep LDA approach . Table 2 shows examples of good and bad results of our Overlap method on the 2016 data . Detailed results of our runs are given in Table 3 per test set . Our three approaches achieved different results . From a semantic point of view , the most obvious value for the default value in our Overlap method is 0 . However , we have discovered that a default value 0.15 returned better results on the STS Core test data from 2015 and also chose this default value for our submission . In the Deep LDA approach , we set the parameter N = 2 , although the use of unigrams did not show any significant statistical difference in the results . We choose the number of topics in the LDA model to be 300 . In the prediction phase of the al - gorithm , we select k = 100 nearest neighbors from the data sets provided from 2012 to 2015 .", "entities": [[49, 50, "MethodName", "LDA"], [111, 112, "DatasetName", "0"], [128, 129, "TaskName", "STS"], [147, 148, "MethodName", "LDA"], [182, 183, "MethodName", "LDA"], [200, 202, "HyperparameterName", "k ="]]}
{"text": "The Statistics for the 6 largest categories used in this paper are shown in Table 1 , containing a snapshot of product details up to January 2019 . Except for mobiles , for other domains , 300 products were sampled . As the number of question - specification pairs is huge , manually labelling a sufficiently large dataset is a tedious task . So , we propose a semisupervised method to create a large training dataset using Dual Embedding Space model ( DESM ) ( Mitra et al , 2016 ) . Suppose a product P has S specifications and Q questions . For a question q i Q and a specification s j S , we find dual embedding score DU AL ( q i , s j ) using Equation 1 , where t q and t s denote the vectors for the question and specification terms , respectively . We consider ( q i , s j ) pair positive if DU AL ( q i , s j ) \u2265 \u03b8 and negative if DU AL ( q i , s j ) < \u03b8 . DU AL ( q i , s j ) = 1 | q i | tq q i t q T s j t q s j ( 1 ) where s j = 1 | s j | ts s j t s t s ( 2 ) We take M obile dataset to create labelled training data since most of the questions come from this vertical . We choose the threshold value ( \u03b8 ) which gives the best accuracy on manually labelled balanced validation dataset consisting of 380 question and specification pairs . We train a word2vec ( Mikolov et al , 2013 ) model on our training dataset to get the embeddings of the words . The word2vec model learns two weight matrices during training . The matrix corresponding to the input space and the output space is denoted as IN and OUT word embedding space respectively . 2 . We analyze the questions in the test datasets and find that the questions can be roughly categorized into three classes - numerical , yes / no and others based upon the answer type of the questions . For a question , we have a number of specifications and only one of them is correct .", "entities": [[174, 175, "HyperparameterName", "\u03b8"], [188, 189, "HyperparameterName", "\u03b8"], [229, 230, "MethodName", "ts"], [265, 266, "HyperparameterName", "\u03b8"], [271, 272, "MetricName", "accuracy"], [276, 278, "DatasetName", "validation dataset"]]}
{"text": "We ing rate 0.01 . The fine - tuning of BERT and XL - Net is done with the same experimental settings as given in the original papers . In all the models , we minimize the cross - entropy loss while training . BERT - 380 and XLNet - 380 models are fine - tuned on the 380 labeled validation dataset that was used for creating the training dataset in Section 5.1 . During evaluation , we sort the question specification pairs according to their relevance score . From this ranked list , we compute whether the correct specification appears within top k , k { 1 , 2 , 3 } positions . The ratio of correctly identified specifications in top 1 , 2 , and 3 positions to the total number of questions is denoted as HIT@1 , HIT@2 and HIT@3 respectively .", "entities": [[10, 11, "MethodName", "BERT"], [40, 41, "MetricName", "loss"], [44, 45, "MethodName", "BERT"], [48, 49, "MethodName", "XLNet"], [60, 62, "DatasetName", "validation dataset"]]}
{"text": "Table 3 shows the performance of the models on different datasets 3 . BERT - 380 and XLNet - 380 perform very poorly , but when we use the train dataset created with DESM , there is a large boost in the models ' performance and it shows the effectiveness of our semi - supervised method in generating labeled dataset . Both BERT and XLNet outperform the baseline Siamese model ( Lai et al , 2018 ) by a large margin , and retrieve the correct specification within top 3 results for most of the queries . For Backpack and AC , both BERT and XLNet are very competitive . XLNet outperforms BERT in Computer , Shoes , and Watches . Only in HIT@1 of AC , BERT has surpassed XLNet with 0.07 points . We see that all the models have performed better in Computer compared to the other datasets . Computer has the highest percentage of yes / no questions and this might be one of the reasons , as some questions might have word overlap with correct specification . Table 4 shows the top three specifications returned by different models for some questions . We see that Siamese architecture returns results which look similar to na\u00efve word match , and retrieve wrong specifications . On the other hand , BERT and XLNet are able to retrieve the correct specifications . Error Analysis : We assume that for each question , there is only one correct specification , but the correct answer may span multiple specifications and our models can not provide a full answer . For example , in Backpack dataset , the dimension of the backpack , i.e. , its height , weight , depth is defined separately . So , when user queries about the dimension , only one specification is provided . Some specifications are given in one unit , but users want the answer in another unit , e.g. , \" what is the width of this bag in cms ? \" . Since the specification is given in inches , the models show the answer in inches . So , the answer is related , but not exactly correct . Users sometimes want to know the difference between certain specification types , what is meant by some specifications . For example , consider the questions \" what is the difference between inverter and non - inverter AC ? \" , \" what is meant by water resistant depth ? \" . While we can find the type of inverter , the water resistant depth of a watch etc . from specifications , the definition of the specification is not given . As we have generated train data labels in semi - supervised fashion , it also contributes to inaccurate classification in some cases .", "entities": [[13, 14, "MethodName", "BERT"], [17, 18, "MethodName", "XLNet"], [62, 63, "MethodName", "BERT"], [64, 65, "MethodName", "XLNet"], [103, 104, "MethodName", "BERT"], [105, 106, "MethodName", "XLNet"], [110, 111, "MethodName", "XLNet"], [112, 113, "MethodName", "BERT"], [127, 128, "MethodName", "BERT"], [130, 131, "MethodName", "XLNet"], [222, 223, "MethodName", "BERT"], [224, 225, "MethodName", "XLNet"], [233, 234, "MetricName", "Error"]]}
{"text": "An important desideratum of natural language generation ( NLG ) is to produce outputs that are not only correct but also diverse ( Tevet and Berant , 2021 ) . The term \" diversity \" in NLG is defined as the ability of a generative model to create a set of possible outputs that are each valid given the input and vary as widely as possible in terms of content , language style , and word variability ( Gupta et al , 2018 ) . This research problem is also referred as one - to - many generation ( Shen et al , 2019 ; Cho et al , 2019 ; Shen et al , 2022 ) . Diversity in NLG has been extensively studied for various tasks in the past few years , such as machine translation ( Shen et al , 2019 ) and paraphrase Codes of our model and baselines are available at https://github.com/DM2 - ND / MoKGE . [ 1 ] [ 4 ] [ 3 ] [ 1 ] [ 3 ] [ 4 ] [ 4 ] [ 1 ] [ 3 ] [ 4 ] [ 2 ] [ 4 ] [ 1 ] ( 1 ) You can produce music when pressing keys on the piano , so it is an instrument . generation ( Gupta et al , 2018 ) . In these tasks , output spaces are constrained by input context , i.e. , the contents of multiple outputs should be similar , and globally , under the same topic . However , many NLG tasks , e.g. , generative commonsense reasoning , pose unique challenges for generating multiple reasonable outputs that are semantically different . Figure 1 shows an example in the commonsense explanation generation ( ComVE ) task . The dataset has collected explanations to counterfactual statements for sense - making from three annotators ( Wang et al , 2020 ) . From the annotations , we observed that different annotators gave explanations to the unreasonable statement from different perspectives to make them diverse in terms of content , e.g. , wrong effect and inappropriate usage . In order to create diversity , existing methods attempted to produce uncertainty by introducing random noise into a latent variable ( Gupta et al , 2018 ) or sampling next token widely from the vo - cabulary . However , these methods were not able to explicitly control varying semantics units and produce outputs of diverse content . Meanwhile , the input text alone contains too limited knowledge to support diverse reasoning and produce multiple reasonable outputs ( Yu et al , 2022c ) . As an example , Table 1 shows the human evaluation results on two GCR tasks . While human annotators were able to produce 2.60 different yet reasonable explanations on the ComVE dataset , one SoTA diversity - promoting method ( i.e. , nucleus sampling ) could produce only 2.15 reasonable explanations . To improve the diversity in outputs for GCR tasks , we investigated the ComVE task and found that 75 % of the concepts ( nouns and verbs ) in human annotations were among 2 - hop neighbors of the concepts contained in the input sequence on the commonsense KG ConceptNet 1 . Therefore , to produce diverse GCR , our idea is enabling NLG models to reason from different perspectives of knowledge on commonsense KG and use them to generate diverse outputs like the human annotators . Thus , we present a novel Mixture of Knowledge Graph Expert ( MoKGE ) method for diverse generative commonsense reasoning on KG . MoKGE contains two major components : ( i ) a knowledge graph ( KG ) enhanced generative reasoning module to reasonably associate relevant concepts into the generation process , and ( ii ) a mixture of expert ( MoE ) module to produce diverse reasonable outputs . Specifically , the generative reasoning module performs compositional operations on KG to obtain structure - aware representations of concepts and relations . Then , each expert uses these representations to seek different yet relevant sets of concepts and sends them into a standard Transformer model to generate the corresponding output . To encourage different experts to specialize in different reasoning abilities , we employ the stochastic hard - EM algorithm by assigning full responsibility of the largest joint probability to each expert . We conducted experiments on two GCR benchmarks , i.e. , commonsense explanation generation and abductive commonsense reasoning . Empirical experiments demonstrated that our proposed MoKGE can outperform existing diversitypromoting generation methods in diversity , while achieving on par performance in quality . To the best of our knowledge , this is the first work to boost diversity in NLG by diversifying knowledge reasoning on commonsense KG .", "entities": [[136, 138, "TaskName", "machine translation"], [294, 296, "TaskName", "explanation generation"], [545, 546, "DatasetName", "ConceptNet"], [696, 697, "MethodName", "Transformer"], [721, 722, "MetricName", "EM"], [747, 749, "TaskName", "explanation generation"]]}
{"text": "Generating multiple valid outputs given a source sequence has a wide range of applications , such as machine translation ( Shen et al , 2019 ) , paraphrase generation ( Gupta et al , 2018 ) , question generation ( Cho et al , 2019 ) , dialogue system ( Dou et al , 2021 ) , and story generation . For example , in machine translation , there are often many plausible and semantically equivalent translations due to information asymmetry between different languages ( Lachaux et al , 2020 ) . Methods of improving diversity in NLG have been explored from various perspectives . Sampling - based decoding is one of the most effective solutions to improve diversity . For example , nucleus sampling samples next tokens from the dynamic nucleus of tokens containing the vast majority of the probability mass , instead of decoding text by maximizing the likelihood . Another line of work focused on introducing random noise ( Gupta et al , 2018 ) or changing latent variables ( Lachaux et al , 2020 ) to produce uncertainty . In addition , Shen et al ( 2019 ) adopted a mixture of experts to diversify machine translation , where a minimum - loss predictor is assigned to each source input . Shi et al ( 2018 ) employed an inverse reinforcement learning approach for unconditional diverse text generation . However , no existing work considered performing diverse knowledge reasoning to generate multiple reasonable outputs of different contents .", "entities": [[17, 19, "TaskName", "machine translation"], [27, 29, "TaskName", "paraphrase generation"], [37, 39, "TaskName", "question generation"], [58, 60, "TaskName", "story generation"], [65, 67, "TaskName", "machine translation"], [199, 201, "TaskName", "machine translation"], [206, 207, "MetricName", "loss"], [230, 232, "TaskName", "text generation"]]}
{"text": "Not all concepts in G appear in the outputs . Thus , we design a concept selection module to choose salient concepts that should be considered during generation . For each concept v V x , we calculate its probability of being selected by taking a multilayer perception ( MLP ) on the top of graph encoder : p v = P r [ v is selected | x ] = MLP ( h L v ) . To supervise the concept selection process , we use the overlapping concepts between concepts appearing in the output sequence C y and concepts in input sequence associated subgraph G x , i.e. , V x \u2229 C y , as a simple proxy for the ground - truth supervision . So , the concept selection loss ( here only for one expert , see MoE loss in Eq . ( 8 ) ) is : L concept = \u2212 v Vx\u2229Cy v log p v ( 4 ) + v Vx\u2212Cy ( 1 \u2212 v ) log ( 1 \u2212 p v ) . Finally , the top - N ranked concepts on the subgraph G x ( denoted as v 1 , ... , v N ) are selected as the additional input to the generation process .", "entities": [[49, 50, "DatasetName", "MLP"], [71, 72, "DatasetName", "MLP"], [133, 134, "MetricName", "loss"], [143, 144, "MetricName", "loss"]]}
{"text": "We utilize a standard Transformer ( Vaswani et al , 2017 ) as our generation model . It takes the concatenation of the sequence x and all the selected concepts v 1 , ... , v N as input and auto - regressively generates the outputs y. We adopt the cross - entropy loss , which can be written as : L generation = \u2212 log p ( y | x , v 1 , , v N ) ( 5 ) = \u2212 | y | t=1 log p ( y t | x , v 1 , , v N , y < t ) . Note that since the selected concepts do not have a rigorous order , we only apply positional encodings ( used in Transformer ) to the input sequence x.", "entities": [[4, 5, "MethodName", "Transformer"], [53, 54, "MetricName", "loss"], [129, 130, "MethodName", "Transformer"]]}
{"text": "We jointly optimizes the following loss : L = L generation + \u03bb L concept . ( 6 ) where \u03bb is a hyperparameter to control the importance of different tasks 2 .", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "To empower the generation model to produce multiple reasonable outputs , we employ a mixture of expert ( MoE ) module to model uncertainty and generate diverse outputs . While the MoE models have primarily been explored as a means of increasing model capacity , they are also being used to boost diverse generation process ( Shen et al , 2019 ; Cho et al , 2019 ) . Formally , the MoE module introduces a multinomial latent variable z { 1 , , K } , and decomposes the marginal likelihood as follows : p ( y | x , G x ) = K z=1 p ( z | x , G x ) p ( y | z , x , G x ) . ( 7 ) Training . We minimize the loss function ( in Eq . ( 6 ) ) using the MoE decomposition , \u2207 log p ( y | x , G x ) ( 8 ) = K z=1 p ( z | x , y , G x ) \u2207 log p ( y , z | x , G x ) , and train the model with the EM algorithm ( Dempster et al , 1977 ) . Ideally , we would like different experts to specialize in different reasoning abilities so that they can generate diverse outputs . The specialization of experts means that given the input , only one element in { p ( y , z | x , G x ) } K z=1 should dominate in value ( Shen et al , 2019 ) . To encourage this , we employ a hard mixture model to maximize max z p ( y , z | x , G x ) by assigning full responsibility to the expert with the largest joint probability . Training proceeds via hard - EM can be written as : E - step : estimate the responsibilities of each expert r z 1 [ z = arg max z p ( y , z | x , G x ) ] using the current parameters \u03b8 ; M - step : update the parameters with gradients of the chosen expert ( r z = 1 ) from E - step . Expert parameterization . Independently parameterizing each expert may exacerbate overfitting since the number of parameters increases linearly with the number of experts ( Shen et al , 2019 ) . We follow the parameter sharing schema in Cho et al ( 2019 ) ; Shen et al ( 2019 ) to avoid this issue . This only requires a negligible increase in parameters over the baseline model that does not uses MoE. In our experiments , we compared adding a unique expert embedding to each input token with adding an expert prefix token before the input text sequence , where they achieved very similar performance . Producing K outputs during inference . In order to generate K different outputs on test set , we follow Shen et al ( 2019 ) to enumerate all latent variables z and then greedily decoding each token by\u0177 t = arg max p ( y | \u0177 1 : t\u22121 , z , x ) . In other words , we ask each expert to seek different sets of concepts on the knowledge graph , and use the selected concepts to generate K different outputs . Notably , this decoding procedure is efficient and easily parallelizable . Furthermore , to make fair comparisons with sampling - based methods , we use greedy decoding without any sampling strategy .", "entities": [[136, 137, "MetricName", "loss"], [199, 200, "MetricName", "EM"], [314, 315, "MetricName", "EM"], [355, 356, "HyperparameterName", "\u03b8"], [393, 396, "HyperparameterName", "number of parameters"]]}
{"text": "All baseline methods were built on the Transformer architecture with 6 - layer encoder and decoder , and initialized with pre - trained parameters from BARTbase ( Lewis et al , 2020 ) , which is one of the stateof - the - art pre - trained Transformer models for natural language generation ( Gehrmann et al , 2021 ) . In our MoKGE , the Transformer parameters were also initialized by BART - base , in order to make fair comparison with all baseline methods . The R - GCN parameters were random initialized . For model training , we used Adam with batch size of 60 , learning rate of 3e - 5 , L2 weight decay of 0.01 , learning rate warm up over the first 10 , 000 steps , and linear decay of learning rate . Our models were trained by one Tesla V100 GPU card with 32 GB memory , and implemented on PyTorch with the Huggingface 's Transformer ( Wolf et al , 2020 ) . All Transformer - based methods were trained with 30 epochs , taken about 4 - 5 hours on the ComVE dataset and 7 - 9 hours on the \u03b1 - NLG dataset . In addition to our MoKGE implementation , we also provide the baseline implementation code on GitHub https://github.com/DM2 - ND / MoKGE .", "entities": [[7, 8, "MethodName", "Transformer"], [47, 48, "MethodName", "Transformer"], [66, 67, "MethodName", "Transformer"], [72, 73, "MethodName", "BART"], [90, 91, "MethodName", "GCN"], [102, 103, "MethodName", "Adam"], [104, 106, "HyperparameterName", "batch size"], [109, 111, "HyperparameterName", "learning rate"], [117, 119, "MethodName", "weight decay"], [122, 124, "HyperparameterName", "learning rate"], [138, 140, "HyperparameterName", "learning rate"], [164, 165, "MethodName", "Transformer"], [174, 175, "MethodName", "Transformer"], [201, 202, "HyperparameterName", "\u03b1"]]}
{"text": "We evaluated the performance of different generation models from two aspects : quality ( or say accuracy ) and diversity . Quality tests the appropriateness of the generated response with respect to the context , and diversity tests the lexical and semantic diversity of the appropriate sequences generated by the model . These evaluation metrics have been widely used in existing work ( Ott et al , 2018 ; Vijayakumar et al , 2018 ; Cho et al , 2019 ; .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "The quality is measured by standard N - gram based metrics , including the BLEU score ( Papineni et al , 2002 ) and the ROUGE score ( Lin , 2004 ) . This measures the highest accuracy comparing the best hypothesis among the top - K with the target ( Vijayakumar et al , 2018 ) . Concretely , we generate hypotheses { \u0176 ( 1 ) , \u0176 ( K ) } from each source X and keep the hypothesis\u0176 best that achieves the best sentencelevel metric with the target Y . Then we calculate a corpus - level metric with the greedily - selected hypotheses { Y ( i ) , best } N i=1 and references { Y ( i ) } N i=1 . The diversity of evaluated by three aspects : concept , pairwise and corpus diversity . Concept diversity . The number of unique concepts ( short as Uni . C ) measures how many unique concepts on the commonsense KG are covered in the generated outputs . A higher value indicates the higher concept diversity . Besides , we also measure the pairwise concept diversity by using Jaccard similarity . It is defined as the size of the intersection divided by the size of the union of two sets . Lower value indicates the higher concept diversity . Pairwise diversity ( \u21d3 ) . Referred as \" self - \" ( e.g. , self - BLEU ) , it measures the within - distribution similarity . This metric computes the average of sentence - level metrics between all pairwise combinations of hypotheses { Y ( 1 ) , , Y ( K ) } generated from each source sequence X. Lower pairwise metric indicates high diversity between generated hypotheses .", "entities": [[14, 16, "MetricName", "BLEU score"], [37, 38, "MetricName", "accuracy"], [243, 244, "MetricName", "BLEU"]]}
{"text": "Comparison with baseline methods . We evaluated our proposed MoKGE and baseline methods based on both quality and diversity . As shown in Table 2 , MoE - based methods achieved the best performance among all baseline methods . MoKGE can further boost diversity by at least 1.57 % and 1.83 % on Self - BLEU - 3 and Self - BLEU - 4 , compared with the vanilla MoE methods . At the same time , MoKGE achieved on par performance with other baseline methods based on the quality evaluation . Specifically , on the ComVE dataset , MoKGE achieved the best performance on BLEU - 4 and ROUGE - L , and on the \u03b1 - NLG dataset , the perfor - mance gap between MoKGE and the best baseline method was always less than 0.5 % on BLEU - 4 . Ablation study . We conducted an ablation study to analyze the two major components in the MoKGE . The experimental results are shown in Table 3 . First , we note that when not using MoE ( line - w/o MoE ) , we used the most basic decoding strategy - beam search - to generate multiple outputs . We observed that the outputs generated by beam search differed only on punctuation and minor morphological variations , and typically only the last few words were different from others . Besides , integrating commonsense knowledge graph into the MoEbased generation model brought both quality and diversity improvement on the ComVE , but might sacrifice a little quality ( less than 0.5 % on BLEU - 4 ) on the \u03b1 - NLG dataset . Overall , our MoKGE benefited from KG and MoE modules , and achieved great performance on both diversity and quality .", "entities": [[55, 56, "MetricName", "BLEU"], [61, 62, "MetricName", "BLEU"], [105, 106, "MetricName", "BLEU"], [109, 112, "MetricName", "ROUGE - L"], [116, 117, "HyperparameterName", "\u03b1"], [140, 141, "MetricName", "BLEU"], [266, 267, "MetricName", "BLEU"], [272, 273, "HyperparameterName", "\u03b1"]]}
{"text": "Automatic diversity evaluation ( e.g. , Self - BLEU , Distinct - k ) can not reflect the content - level diversity . Therefore , we conducted extensive human evaluations to assess both the quality and diversity of outputs generated from different models . The human evaluation was divided into two parts : independent scoring and pairwise comparisons . All evaluations were conducted on Amazon Mechanical Turk ( AMT ) , and each evaluation form was answered by at least three AMT workers . Independent scoring . In this part , human annotators were asked to evaluate the generated outputs from a single model . We first presented top - 3 generated outputs from a certain model to human annotators . The annotators would first evaluate the diversity by answering \" How many different meanings do three outputs express ? \" Then we presented human - written outputs to the annotators . The annotator would evaluate the quality by comparing machine generated outputs and human - written outputs , and answering \" How many machine generated out - puts are correct ? \" The diversity and quality scores are normalized to the range from 0 to 3 . Besides , the annotators need to give a fluency and grammar score from 1 to 4 for each generated output . Pairwise comparisons . In this part , the annotators were given two sets of top - 3 generated explanations from two different methods each time and instructed to pick the more diverse set . The choices are \" win , \" \" lose , \" or \" tie . \" As shown in Table 4 - 5 , our MoKGE can significantly outperform the state - of - the - art samplingbased methods in diversity evaluation ( p - value < 0.05 under paired t - test ) , even slightly better than human performance on the ComVE task . At the same time , we can observe MoKGE is able to obtain on par performance with other methods based on quality evaluation . The p - value is not smaller than 0.05 ( i.e. , not significant difference ) under paired t - test between MoKGE and baseline methods based on the quality evaluation . ( 1 ) Billy 's parents took him to the zoo as a reward .", "entities": [[8, 9, "MetricName", "BLEU"], [194, 195, "DatasetName", "0"]]}
{"text": "[ 4 ] [ 4 ] [ 1 ] [ 1 ] [ 4 ] [ 3 ] [ 4 ] [ 4 ] [ 1 ] [ 1 ] [ 1 ] [ 2 ] [ 1 ] ComVE - - Input : Cars are made of fuel . Goal ( explanation for sense - making ) : [ ] . ( 1 ) Cars are not made of fuel . ( 2 ) Cars burn fuel to produce energy and work . ( 3 ) Fuel is a liquid which can not make cars . meanings , e.g. , \" go to the zoo and see elephants \" and \" took him to the zoo and see elephants \" in the \u03b1 - NLG case . On the contrary , MoKGE can generate semantically richer and more diverse contents than the other two methods by incorporating more commonsense concepts on the knowledge graph .", "entities": [[123, 124, "HyperparameterName", "\u03b1"]]}
{"text": "Improving content diversity in NLG . Most of the existing diversity - promoting work has focused on improving syntactic and lexical diversity , such as different language style in machine translation ( Shen et al , 2019 ) and word variability in paraphrase generation ( Gupta et al , 2018 ) . Nevertheless , methods for improving content diversity in NLG systems have been rarely studied in the existing literature . We believe that generating diverse content is one of the most promising aspects of machine intelligence , which can be applied to a wide range of real - world applications , not only limited to commonsense reasoning . Besides , leveraging knowledge graph is not the only way to promote content diversity as it is a highly knowledge - intensive task . Many existing knowledge - enhanced methods ( Yu et al , 2022c ) can be used to acquire different external knowledge for producing diverse outputs , e.g. , taking different retrieved documents as conditions for generator . Designing neural diversity metrics . In spite of growing interest in NLG models that produce diverse outputs , there is currently no principled neu - ral method for evaluating the diversity of an NLG system . As described in Tevet and Berant ( 2021 ) , existing automatic diversity metrics ( e.g. Self - BLEU ) perform worse than humans on the task of estimating content diversity , indicating a low correlation between metrics and human judgments . Therefore , neural - based diversity metrics are highly demanded . Intuitively , the metrics should include computational comparisons of multiple references and hypotheses by projecting them into the same semantic space , unlike metrics for evaluating the generation quality , e.g. , BERTScore ( Zhang et al , 2020b ) and BLEURT ( Sellam et al , 2020 ) , which only measures the correlation between a pair of reference and hypothesis .", "entities": [[29, 31, "TaskName", "machine translation"], [42, 44, "TaskName", "paraphrase generation"], [224, 225, "MetricName", "BLEU"]]}
{"text": "Multiple - choice questions ( MCQs ) are widely used for student assessments , from high - stakes graduation tests to lower - stakes reading comprehension tests . An MCQ consists of a question ( stem ) , the correct answer ( key ) and a number of wrong , but plausible options ( distractors ) . The problem of automatically generating stems with a key has received a great deal of attention , e.g. , see the survey by Amidei et al ( 2018 ) . By comparison , automatically generating distractors is substantially less researched , although Welbl et al ( 2017 ) report that manually finding reasonable distractors was the most time - consuming part in writing science MCQs . Indeed , reasonable distractors should be grammatically consistent and similar in length compared to the key and within themselves . Given the challenges above , we attempt using machine learning ( ML ) to aid teachers in creating distractors for reading comprehension MCQs . The problem is not new , however most of the prior work has been done for English . In this paper we propose the first such solution for Swedish ( although the proposed method is novel even for English , to the best of our knowledge ) . The key contributions of this work are : proposing a BERT - based method for generating distractors using only a small - scale dataset , releasing SweQUAD - MC 1 , a dataset of Swedish MCQs , and proposing a methodology for conducting human evaluation aimed at assessing the plausibility of distractors . 2 Background 2.1 BERT for NLG Devlin et al ( 2019 ) introduced BERT as the first application of the Transformer architecture ( Vaswani et al , 2017 ) to language modelling . BERT uses only Transformer 's encoder stacks ( with multihead self - attention , MHSA ) , while the NLG community relies more on Transformer 's decoder stacks ( with masked MHSA ) for text generation , e.g. , GPT ( Radford et al , 2018 ) . However , Wang and Cho ( 2019 ) showed that BERT is a Markov random field , meaning that BERT learns a joint probability distribution over all sentences of a fixed length , and one could use Gibbs sampling to generate a new sentence . The authors compared samples generated autoregressively left - to - right by BERT and GPT , and found the perplexity of BERT samples to be higher than GPT 's ( BERT samples are of worse quality ) , but the n - gram overlap between the generated texts and texts from the dataset to be lower ( BERT samples are more diverse ) . Liao et al ( 2020 ) show a way to improve BERT 's generation capabilities via changing the masking scheme to a probabilistic one at training time . Probabilistically masked language models ( PMLMs ) assume that the masking ratio r for each sentence is drawn from a prior distribution p ( r ) . The au - 2.1 \u00b1 0.5 2.1 \u00b1 0.4 2.0 \u00b1 0.2 Len ( Text ) 384.9 \u00b1 330.1 355.1 \u00b1 233.1 357.9 \u00b1 254.3 Len ( A ) 4.2 \u00b1 3.4 4.4 \u00b1 3.5 4.6 \u00b1 4.5 Len ( D ) 4.5 \u00b1 3.9 4.3 \u00b1 4.0 4.0 \u00b1 3.7 | Len ( A ) - Len ( D ) | 1.9 \u00b1 2.4 1.9 \u00b1 2.3 1.9 \u00b1 2.9 Table 1 : Descriptive statistics of SweQUAD - MC dataset splits . A denotes the key , D denotes a distractor , Len ( X ) denotes a length of X in words . x \u00b1 y shows mean x and a standard deviation y thors proposed to train a PMLM with a uniform prior ( referred to as u - PMLM ) . The absence of the left - to - right restriction allows the model to generate sequences in an word arbitrary order . In fact , Liao et al ( 2020 ) propose to generate sentences by randomly selecting the masked position , predicting a token for it , replacing the masked token with the predicted one and repeating the process until no masked tokens are left . The authors showed that the perplexity of the texts generated by u - PMLM is comparable to the ones by GPT .", "entities": [[24, 26, "TaskName", "reading comprehension"], [163, 165, "TaskName", "reading comprehension"], [225, 226, "MethodName", "BERT"], [271, 272, "MethodName", "BERT"], [281, 282, "MethodName", "BERT"], [288, 289, "MethodName", "Transformer"], [298, 300, "TaskName", "language modelling"], [301, 302, "MethodName", "BERT"], [304, 305, "MethodName", "Transformer"], [325, 326, "MethodName", "Transformer"], [335, 337, "TaskName", "text generation"], [340, 341, "MethodName", "GPT"], [359, 360, "MethodName", "BERT"], [368, 369, "MethodName", "BERT"], [406, 407, "MethodName", "BERT"], [408, 409, "MethodName", "GPT"], [413, 414, "MetricName", "perplexity"], [415, 416, "MethodName", "BERT"], [421, 422, "MethodName", "GPT"], [424, 425, "MethodName", "BERT"], [451, 452, "MethodName", "BERT"], [469, 470, "MethodName", "BERT"], [634, 635, "MethodName", "PMLM"], [645, 646, "MethodName", "PMLM"], [720, 721, "MetricName", "perplexity"], [728, 729, "MethodName", "PMLM"], [735, 736, "MethodName", "GPT"]]}
{"text": "Given the small scale of SweQUAD - MC we have decided to fine - tune a pretrained BERT 2 for Swedish ( Malmsten et al , 2020 ) on the task of distractor generation ( DG ) . For achieving this , we have added on top of BERT two linear layers with layer normalization ( Ba et al , 2016 ) in the middle to be trained from scratch ( see architecture in Figure 1 ) . The last linear layer is followed by a softmax activation giving probabilities over the tokens in the vocabulary for each position in the text . We trained the model using cross - entropy loss only for tokens in masked positions . Recall that each MCQ consists of a base text T , the stem Q based on T , the key A and ( on average ) two distractors D1 and D2 . The DG problem is then to generate distractors conditioned on the context , consisting of T , Q and A. We provide all context components as input to the BERT model , separated from each other by the special separator token [ SEP ] . Given that BERT 's maximum input length is 512 tokens , we trim T to the first 384 tokens ( later referred to as T 384 ) , since that is the average text length of the training set . We have explored two different solution variants of DG . The first variant aims at generating distractors autoregressively , left to right . At generation time , the input to BERT consists of a context CTX ( T 384 , Q and A separated by [ SEP ] token ) , a [ SEP ] token , and a [ MASK ] token at the end . After a forward pass through BERT , the [ MASK ] token gets replaced by the word with the highest softmax score , which becomes the first word of the first distractor ( dubbed D11 ) . The generation of the first distractor continues by appending a [ MASK ] token after each forward pass until the network generates a separator token [ SEP ] , which concludes the generation of the first distractor D1 . The next distractor D2 is generated in the same way , except that the CTX is extended by D1 . At training time , we use the same procedure , but with teacher forcing , allowing us to use the correct distractor tokens as targets for the cross - entropy loss ( see example training datapoints for one MCQ in Table 2 ) . The second variant is inspired by u - PMLM , and aims at generating distractors autoregressively , but in an arbitrary word order . At generation time , the input to BERT consists of a context CTX , a [ SEP ] token , and a predefined number of [ MASK ] tokens ( see Section 6.1 ) . The generation proceeds by unmasking the token at the position where the model is most confident . This differs from unmasking a random position , proposed by Liao et al ( 2020 ) . The training procedure largely follows a masking scheme employed by u - PMLM by drawing the masking ratio from the uniform distribution ( see example training datapoints for one MCQ in Table 2 ) . Note that we do not include the [ SEP ] token when training , since we found that the trained model would constantly generate [ SEP ] tokens . Each sampled masking ratio r for the u - PMLM variant means that each token in the distractors from the dataset has a probability r to be masked . Hence , different r will potentially result in different number of masked tokens and at different positions . The number of times we draw r per distractor DX is proposed to be min ( Len ( DX ) , MAX MASKINGS ) .", "entities": [[17, 18, "MethodName", "BERT"], [32, 34, "TaskName", "distractor generation"], [48, 49, "MethodName", "BERT"], [53, 55, "MethodName", "layer normalization"], [80, 82, "MethodName", "linear layer"], [86, 87, "MethodName", "softmax"], [111, 112, "MetricName", "loss"], [119, 120, "MetricName", "Recall"], [180, 181, "MethodName", "BERT"], [198, 199, "MethodName", "BERT"], [266, 267, "MethodName", "BERT"], [308, 309, "MethodName", "BERT"], [323, 324, "MethodName", "softmax"], [429, 430, "MetricName", "loss"], [451, 452, "MethodName", "PMLM"], [474, 475, "MethodName", "BERT"], [548, 549, "MethodName", "PMLM"], [609, 610, "MethodName", "PMLM"]]}
{"text": "Automatic evaluation metrics , such as BLEU ( Papineni et al , 2002 ) , ROUGE ( Lin , 2004 ) , METEOR ( Denkowski and Lavie , 2014 ) , CIDEr ( Vedantam et al , 2015 ) , became popular in NLG in recent years . Essentially , these metrics rely on comparing word overlap between a generated distractor and a reference one . Such metrics can yield a low score even if the generated distractor is valid but just happens to be different from the reference one , or a high score even though the distractor is ungrammatical but happens to have a high word overlap with the reference one ( see the article by Callison - Burch et al ( 2006 ) for a further discussion ) . Furthermore , they do not take into account how well a generated distractor is aligned with the key grammatically or how challenging the whole group of generated distractors would be . To account for the properties mentioned above , we have experimented with a number of quantitative metrics and propose the following set to be used ( the whole list is available in Appendix B ) . In the following list MCQ% means \" Percentage of MCQ \" and DIS means \" generated distractor ( s ) \" . The first group consists of metrics 1 - 3 . The first two metrics count exact matches between generated and reference distractors . The rationale behind metric 3 is our assumption that distractors coming from the same text are more challenging . The higher the values of all these metrics are , the better . The second group contains metrics 4 - 8 , which give an idea of how challenging the whole group of distractors would be . For instance , duplicate distractors or ones with word repetitions could be excluded by students using common sense . The lower the metrics in this group are , the better . The third group consists only of metric 9 , serving as an overfitting indicator . The metric accounts for the distractors appearing as distractors in training data and high percentage indicates an overfitting possibility . The lower the values , the better . The final group ( item 10 ) measures how syntactically aligned generated distractors and the respective keys are . We employ NCPTK to measure the similarity of syntactic structures between each distractor and the respective key . Then we take mean , median and mode of the sequence of NCPTKs obtained in the previous step . The higher the values of these metrics are , the better . Based on these metrics , we performed a model selection on the development set and chose the models performing best on the most of these metrics . Left - to - right model generated distractors token by token until either a [ SEP ] token was generated or the length of the distractor was 20 tokens .", "entities": [[6, 7, "MetricName", "BLEU"], [22, 23, "DatasetName", "METEOR"], [31, 32, "MetricName", "CIDEr"], [450, 452, "TaskName", "model selection"]]}
{"text": "MCQs is that the students should be unable to answer them correctly without reading the actual text . To put more formally , the average number of correctly answered MCQs without reading the actual text ( denoted N s ) should not differ significantly from the average number of correctly answered MCQs when choosing the answer uniformly at random ( denoted N r ) . To test for this property , we have formulated the following two hypotheses . 5 H 0 : N s = N r . H 1 : N s = N r . For N MCQs with 4 options , N r = 0.25N , which for our test set would be equal to N r = 0.25 102 = 25.5 . The appropriate statistical test in this case is one - sample two - tailed t - test with the aim of not being able to reject H 0 . Given that the purpose is to show that the data supports H 0 , we have set both the probability \u03b1 of type I errors and the probability \u03b2 of type II errors to be 0.05 . Then we have used G*Power ( Faul et al , 2009 ) to calculate the required sample size for finding a medium effect size ( 0.5 ) and the given \u03b1 and \u03b2 , which turned out to be 54 subjects . Following the calculations above , we have recruited 54 subjects on the Prolific platform 6 , and instructed them to choose the most plausible answer to a number of reading comprehension MCQs without providing the original texts . The collected data did not violate any assumptions for a one - sample t - test ( see Appendix D.1 for more details ) . On average , the subjects correctly answered a significantly larger number of questions than N r ( N s = 62.26 , SE = 1.09 , t ( 53 ) = 33.51 , p < 0.05 , r = 0.98 ) . To summarize , the chances of this sample to be collected are very low if H 0 were true . However , evidently some of the generated distractors were actually plausible , given that N s = N . To investigate the matter we have plotted the histogram of the frequency of choice of distractors by the subjects in Figure 2 . As suggested by Haladyna and Downing ( 1993 ) , distractors that are chosen by less than 5 % of students should not be used , which in our case amounts to 39 % of the dis - 5 Preregistration is available here 6 https://www.prolific.co/ tractors ( the leftmost bar in Figure 2 ) . If we eliminate these low - frequency distractors ( LF - DIS ) , 68 MCQs ( 66.67 % ) will lose at least one distractor , 10 MCQs ( 9.8 % ) will lose all distractors and thus 34 MCQs ( 33.33 % ) will keep all 3 distractors . A more relaxed question is how many MCQs had at least one plausible distractor , which can be estimated by calculating the entropy for each question as shown in Equation ( 2 ) , where A is the key , D is a distractor , Q is the stem , P Q ( A ) ( P Q ( D ) ) is the probability that the key ( any distractor ) is chosen for Q by a subject . H ( Q ) = \u2212 O { A , D } p Q ( O ) log ( p Q ( O ) ) ( 2 ) The distribution of entropies per question is shown in Figure 3 . Assuming the natural logarithm , the highest theoretically possible value for H ( Q ) is 0.69 , if p Q ( A ) = p Q ( D ) = 0.5 . 32 % of MCQs had an entropy larger than 0.65 , whereas 51 % had an entropy larger than 0.6 , which means that half of MCQs had at least one plausible distractor .", "entities": [[81, 82, "DatasetName", "0"], [154, 155, "DatasetName", "0"], [168, 169, "DatasetName", "0"], [176, 177, "HyperparameterName", "\u03b1"], [184, 185, "HyperparameterName", "\u03b2"], [223, 224, "HyperparameterName", "\u03b1"], [225, 226, "HyperparameterName", "\u03b2"], [264, 266, "TaskName", "reading comprehension"], [356, 357, "DatasetName", "0"]]}
{"text": "Bearing in mind the findings of Section 6.2.1 , it is interesting to see which of the proposed distractors ( especially , among LF - DIS ) teachers would mark as acceptable . Given the complexity of such evaluation , using the whole test set was infeasible . To get a representative sample , we used entropy per question ( shown in Figure 3 ) . All MCQs were divided into 5 equally sized buckets by entropy and 9 MCQs were sampled uniformly at random from each bucket , resulting in 45 MCQs in total . We asked 5 teachers to evaluate each MCQ ( presented in a random order for each of them ) . Each MCQ contained the base text , the stem , the key and the generated distractors . The teachers were instructed to select those of generated distractors ( if any ) deemed suitable for testing reading comprehension . Additionally , we asked to provide their reasons for each rejected distractor in a free - text input . The inter - annotator agreement ( IAA ) was estimated using Goodman - Kruskal 's \u03b3 ( Goodman and Kruskal , 1979 ) , specifically its multirater version \u03b3 N proposed by Kalpakchi and Boye ( 2021 ) . On the scale proposed by Rosenthal ( 1996 ) , we have found a very large agreement ( \u03b3 N = 0.85 , see Appendix D.2.2 for more details on IAA calculations ) . On average , 1.47 distractors per MCQ were accepted by a teacher . Their reasons for rejections are distributed as shown in Figure 4 . All teachers accepted at least one generated distractor for 39 MCQs ( 86.7 % ) , whereas the majority of teachers did so for 27 MCQs ( 60 % ) . Interestingly , there are no MCQs in which all 5 teachers have either accepted or rejected all generated distractors . However , the majority of teachers has accepted or rejected all distractors for 4 MCQs ( 8.9 % ) and 6 MCQs ( 13.3 % ) respectively . Out of 45 MCQs , 31 ( 68.9 % ) had at least one LF - DIS , as defined in Section 6.2.1 . For these 31 MCQs we report a distribution of accepted / rejected LF - DIS by the majority of teachers in Figure 5 . Let us call the 15 MCQs with all LF - DIS accepted by the majority of teachers as mismatch MCQs ( lowest row in Figure 5 ) . Interestingly , 12 of the 15 mismatch MCQs had at least one more distractor in addition to LF - DIS being accepted by the majority of teachers . Furthermore , all mismatch MCQs had entropy higher than 0.3 . This entails that almost a half of LF - DIS should not necessarily be thrown away , since they were accepted by teachers , but the MCQs either happened to have more plausible distractors or subjects might have had relevant background knowledge to answer the questions .", "entities": [[151, 153, "TaskName", "reading comprehension"], [188, 189, "HyperparameterName", "\u03b3"], [201, 202, "HyperparameterName", "\u03b3"], [230, 231, "HyperparameterName", "\u03b3"]]}
{"text": "We employed a systematic process to get a comprehensive overview of DG methods ( see Appendix E for more details ) . Out of the resulting 28 articles ( see an overview in Table 4 ) , only 2 worked with a language other than English ( Chinese and Basque ) . In this paper we work on reading comprehension MCQs , which makes only 12 papers , dealing with factual questions , relevant . Two of these used rule - based approaches . Majumder and Saha ( 2015 ) generated MCQs for cricket domain and used a number of hand - crafted rules based on gazeteers and Wikipedia entries to generate distractors . Mitkov and Ha ( 2003 ) proposed to generate distractors for MCQs on electronic instructional documents using WordNet . Six of these relied on extractive approaches . Liang et al ( 2018 ) , Welbl et al ( 2017 ) , and Ha and Yaneva ( 2018 ) formulated choosing a distractor as a ranking problem from the given candidate set . In the first two articles the candidate set constituted all distractors from the available MCQ dataset . The authors then trained ML - based ranker ( s ) for choosing the best distractors . In the last one , the candidate set was created using content engineers . Distractors with a high similarity of their concept embeddings ( summed for multiple words ) and appearing in the same document as the key are ranked higher . Stasaski and Hearst ( 2017 ) and Araki et al ( 2016 ) worked in the domain of biology . The former used an ontology and the latter employed event graphs containing information about coreferences to generate distractors . Karamanis et al ( 2006 ) used thesaurus and tf - idf to identify key concepts in the given text and then select as distractors those having the same semantic type as the key . The remaining four employed neural methods and are most relevant among the surveyed . Qiu et al ( 2020 ) trained a sequence - to - sequence ( seq2seq ) model with a number of attention layers . Zhou et al ( 2020 ) also employed a seq2seq model , but with a hierarchical attention to capture the interaction between a text and a question , as well as semantic similarity loss . Both articles used a beam search combined with filtering based on Jaccard coefficient at generation time . Offerijns et al ( 2020 ) trained a GPT - 2 model to generate 3 distractors for a given MCQ , and used BERT - based question answering model for quantitative evaluation ( along with human evaluation ) . Finally , Chung et al ( 2020 ) proposed a BERTbased method for English with answer - negative regularization , penalizing distractors for containing Then they rank every triple of distractors based on the entropy of a separately trained QA model . Our method also relies on BERT , but has a number of differences beyond being applied to Swedish . Firstly , we did not include answer - negative regularization , since it is not always a good strategy . For instance , given the stem \" When should you pay a fee if you apply for a visa ? \" and a key \" before you have submitted the application \" , the best distractor would be \" after you have submitted the application \" , which shares most of the words with the key . Secondly , we generate distractors in arbitrary word order compared to left - to - right generation in ( Chung et al , 2020 ) . Thirdly , at generation time , we use previously generated distractors as input for generating next ones , and always take tokens with a maximum probability . This lowers the risk of generating ungrammatical distractors . Finally , our training set is 100 times smaller compared to the training set used by Chung et al ( 2020 ) .", "entities": [[58, 60, "TaskName", "reading comprehension"], [276, 277, "MethodName", "ontology"], [354, 355, "MethodName", "seq2seq"], [361, 363, "HyperparameterName", "attention layers"], [373, 374, "MethodName", "seq2seq"], [395, 397, "TaskName", "semantic similarity"], [397, 398, "MetricName", "loss"], [424, 425, "MethodName", "GPT"], [439, 440, "MethodName", "BERT"], [442, 444, "TaskName", "question answering"], [502, 503, "MethodName", "BERT"]]}
{"text": "To evaluate the inter - annotator agreement ( IAA ) between the teachers , we have reformulated the problem into a ranking problem , where all accepted distractors were given the rank of 1 and those rejected - the rank of 2 . IAA was then estimated using Goodman - Kruskal 's \u03b3 ( Goodman and Kruskal , 1979 ) , specifically its multirater version \u03b3 N proposed by Kalpakchi and Boye ( 2021 ) . The total number of concordant and discordant pairs were summed for each pair of teachers for each MCQ . The resulting \u03b3 N equals to 0.85 , indicating a very large agreement on the scale proposed by Rosenthal ( 1996 ) .", "entities": [[52, 53, "HyperparameterName", "\u03b3"], [65, 66, "HyperparameterName", "\u03b3"], [97, 98, "HyperparameterName", "\u03b3"]]}
{"text": "A number of generated distractors along with the respective stems and keys from the dataset are presented in Figures 11 , 12 , 13 , 14 , 15 . The questions are sampled based on the entropy of student 's an - Thank you for participating in our study ! You will be presented with a number of tests . Each test contains a text , a reading comprehension question based on the text , the explicitly marked correct answer to this question and a number of suggestions for wrong , but plausible alternatives ( distractors ) . Suppose you would like to use the given question for testing reading comprehension of the given text . Your task is to judge which of the suggested distractors ( if any ) you would fit the purpose . Select suitable distractors by simply ticking the respective checkboxes . For the other distractors ( that you did n't select ) , please briefly state your reasons why these distractors were inappropriate in the respective text fields ( max 1 sentence ) . Figure 10 : An English translation of the original instructions given to teachers ( the original instructions in Swedish can be found in the GitHub repository ) swers using the same 5 buckets as in sampling for teachers ' evaluation . Recall that distractors are said to be low frequency ( LF - DIS ) if they were chosen by less than 5 % of students . Hence , a red cross in the column \" F - DIS > 5 % \" entails that a given distractor is in fact an LF - DIS . The MCQ in sample 1 has an entropy of 0 , meaning all students have selected the same option , which in this case was the key . In this case , two of three distractors were accepted by the majority of teachers , although all of them were LF - DIS . This is a good example of an MCQ with plausible distractors , but where the stem is too easy . The MCQ in sample 2 presents an interesting case , when the distractor contains an obvious grammatical error ( comma before the first word in the distractor 3 ) . While the distractor was rightfully rejected by the majority of teachers , it was still selected by more than 5 % of students . The MCQ in sample 3 is a good example of longer distractors . In this case , two distractors were accepted by teachers and two were selected by more than 5 % of students . However , interestingly these sets are disjoint , meaning that all three distractors could potentially be useful . Another more general observation , requiring future research , is that our model seems to struggle more when generating longer distractors in general , resulting in non - finished sentences or repetitions of words . The MCQ in sample 4 is somewhat opposite to sample 3 , since one distractor that was accepted by the teachers turned out to be an LF - DIS . This either means that the stem was too easy or that none of the distractors were potentially useful . The MCQ in sample 5 is the one with a highest theoretically possible entropy between selecting the correct or a wrong option . Note that it might still happen that some of the distractors is LF - DIS , since the entropy is calculated not between all four options , but only between the key and the distractors as a group .", "entities": [[67, 69, "TaskName", "reading comprehension"], [109, 111, "TaskName", "reading comprehension"], [220, 221, "MetricName", "Recall"], [284, 285, "DatasetName", "0"]]}
{"text": "Because of globalization , it is becoming more and more common to use multiple languages in a single utterance , also called codeswitching . This results in special linguistic structures and , therefore , poses many challenges for Natural Language Processing . Existing models for language identification in code - switched data are all supervised , requiring annotated training data which is only available for a limited number of language pairs . In this paper , we explore semi - supervised approaches , that exploit out - of - domain monolingual training data . We experiment with word uni - grams , word n - grams , character ngrams , Viterbi Decoding , Latent Dirichlet Allocation , Support Vector Machine and Logistic Regression . The Viterbi model was the best semi - supervised model , scoring a weighted F1 score of 92.23 % , whereas a fully supervised state - of - the - art BERT - based model scored 98.43 % . 1", "entities": [[45, 47, "TaskName", "language identification"], [117, 120, "MethodName", "Support Vector Machine"], [121, 123, "MethodName", "Logistic Regression"], [138, 140, "MetricName", "F1 score"], [155, 156, "MethodName", "BERT"]]}
{"text": "Social platforms have been the cradle of the internet , driving vast amounts of communication among people from all over the world . As a consequence , the way people communicate in written text has changed , as now it is common to use , for example , abbreviations of words , emoticons , references to other users and use multiple languages within the same utterance . An annotated example sentence of this is the following tweet : Word El online exercise de hoy : Label es en en es es other This phenomenon has caught particular interest in both sociolinguistics and Natural Language Processing ( NLP ) ( Aguilar et al , 2020 ; Khanuja et al , 2020 ) . Classifying the language labels on the word level ( i.e. code - switch detection ) has shown to be beneficial to improve performance on downstream NLP tasks , like dependency parsing ( Bhat et al , 2018 ) or lexical normalization ( Barik et al , 2019 ) . Previous work has shown that high performances can be achieved for this task for many language pairs ( Molina et al , 2016 ; Banerjee et al , 2016 ) . However , to the best of our knowledge , most previous work focused on supervised settings , restraining their usefulness to language pairs for which annotated datasets exist . Recent efforts to unify existing datasets have collected annotation for 4 ( Aguilar et al , 2020 ) and 2 ( Khanuja et al , 2020 ) language pairs , which confirms that annotated data is not available for most language pairs . In supervised settings , recent transformer models ( Vaswani et al , 2017 ; Devlin et al , 2019 ) have reached a new state - of - the - art ( Aguilar et al , 2020 ; Khanuja et al , 2020 ) , outperforming Bi - LSTMS and traditional machine learning methods used earlier ( Molina et al , 2016 ; Banerjee et al , 2016 ) . Yirmibe\u015foglu and Eryigit ( 2018 ) tackled this task in a semi - supervised setup as well , where they used character n - gram language models trained on monolingual data to predict perplexity on the target word for classification . They show that this obtains a micro - average F1 score of 92.9 % , compared to 95.6 % with a supervised CRF - model . To overcome this limitation , we focus on exploiting only mono - lingual datasets for performing word - level language identification in code - switched data . We refer to this setup as semi - supervised , since we have no data annotated for the task at hand ( code - switch detection ) . This enables the possibility to easily train models for new language pairs , and leads to the research question : How do semi - supervised models compare and perform in the task of language identification in English - Spanish code - switched data ? ( RQ1 ) . Since supervised methods have the advantage of learning from annotated data , the second research question is : How much can we reduce the gap in performance between the aforementioned semisupervised models and a supervised state - of - the - art model ? ( RQ2 ) . Previous work in similar setups have automatically generated code - switched data from monolingual datasets ( Santy et al , 2021 ) . We consider this approach to be orthogonal to ours , and Santy et al ( 2021 ) exploit mono - lingual in - domain data , syntactic parsers and parallel sentences .", "entities": [[151, 153, "TaskName", "dependency parsing"], [161, 163, "TaskName", "lexical normalization"], [377, 378, "MetricName", "perplexity"], [393, 395, "MetricName", "average F1"], [407, 408, "MethodName", "CRF"], [430, 432, "TaskName", "language identification"], [499, 501, "TaskName", "language identification"]]}
{"text": "The problem of code - switching can be represented as a Hidden Markov Model ( HMM ) problem , since a sentence can be seen as a Markov chain with hidden states that are the two different languages . We use the Viterbi decoding algorithm ( Forney , 1973 ) to find the most probable sequence of states given the observations - namely , to assign a language label ( state ) to each word ( observation ) . We used eginhard 's implementation 5 of the Viterbi algorithm and modified the starting and transition probabilities to the values specified below , which were found to be optimal using grid search on the development set using the range of initial probabilities for English from 0.1 to 0.9 with step size 0.1 , transition probabilities for English from 0.05 to 0.95 with step size 0.05 . The final hyperparameters are as follows : states : lang1 and lang2 , other tokens are identified based on heuristics ( see Section 2.3 ) ; initial probabilities : 0.6 for English and 0.4 for Spanish ; transition probabilities : 0.15 for transitioning to a different language and 0.85 for transitioning to the same language ; emission probabilities : these are estimated through a relative probability model , the probability of the word being emitted from English , for example , is : P ( w ) = P ( w | EN ) P ( w | EN ) + P ( w | SP A ) , where P ( w | EN ) and P ( w | SP A ) are probabilities given by the dictionaries described in section 3.1 . In case this is 0 ( i.e. the word does not occur in our monolingual data ) , the emission probability is calculated by a relative character bi - gram probability .", "entities": [[128, 130, "HyperparameterName", "step size"], [141, 143, "HyperparameterName", "step size"], [284, 285, "DatasetName", "0"]]}
{"text": "To evaluate the performance of our models , we use weighted F1 score 7 . As found in We use multilingual BERT and all default settings . Results in Table 1 show that there is still a performance gap between the semi - supervised approaches and this state - of - the - art supervised model . When comparing common confusions of our best semi - supervised model ( Viterbi ) to the output of MaChAmp , we found that there was more confusion in the Viterbi model about other , where 213 words were classified as lang1 and 60 as lang2 instead , compared to just 3 and 1 in MaChAmp . Full confusion matrices can be found in the appendix . The majority voting ensembling models do not lead to improved performance . However , the oracle ensemble , which always picks the correct label if it is available from one of the models , shows that there is potential in improving the selection method for ensembling .", "entities": [[11, 13, "MetricName", "F1 score"], [21, 22, "MethodName", "BERT"]]}
{"text": "When inspecting the performances of the models per class ( see also Table 2 in the appendix ) , we found that , for the development dataset , all models have a better F1 score for English than for Spanish and , for the test dataset , the other way around . This might be due to a discrepancy between the label distribution of the two datasets and is a significant aspect to be investigated in future work . Regarding the LDA model , its low performance can be explained by the results of ( Zhang et al , 2014 ) , which show that for the task of language filtering , the performance of LDA decreases when the dominating language decreases under 70 % of the whole text . This is also the case in our experiments , where the test data had a 54 % English and 46 % Spanish ratio . Furthermore , the amount of evidence per sample is rather low compared to the normal use of LDA ( it is commonly used on the document level ) . For character n - grams , we observed that the more we increased the value of n , the better results we got , up until n = 6 . The higher order n - grams performed better with around 12 % difference in validation weighted F1 score , as we can capture groups of letters that are representative for a language , e.g. ' tion ' in English and ' cion ' in Spanish . This model achieves good results also because it addresses the problem of misspelled words . For word n - grams , using tri - grams resulted in worse predictions than using bi - grams with around 11 % difference in validation weighted F1 score . For LDA , SVM and Logistic Regression models we tried to vectorize data with CountVectorizer from Scikit Learn , which gives the termfrequency for each n - gram in a word . However , TfidfVectorizer performed approximately 1 % better in LDA and Logistic Regression and 4 % for SVM in validation data . This was then the preferred vectorizer in all models , as it helps decreasing the impact of very frequent character n - grams that are not expressing much value and gives more importance to less frequent character n - grams . The fact that the oracle model has a 3 % higher weighted F1 score than the best model ( in validation data ) , suggests that there is room for improvement for the ensemble model with other methods than majority voting . Improvements on the single models could be achieved by using bigger monolingual datasets of the same size or selecting a corpus that is more similar to the test set ( social media - like posts ) , which is not as easy to query as Wikipedia articles . The overall performance of the models can also be slightly improved by a more complex method for the other class ( the existing rule - based method scored an F1 of 96.76 , see Table 2 in the appendix ) . The training efficiency of the Viterbi model and the supervised model were measured in a Windows Sub - system for Linux environment on an i7 - 7700 K processor with 16 GB ram . We ran the MaChAmp model in this environment and it completed in 53 , 990 seconds . In comparison , the Viterbi training completed in 1 , 805 seconds , which is an improvement of almost 30 times faster than the MaChAmp model .", "entities": [[33, 35, "MetricName", "F1 score"], [81, 82, "MethodName", "LDA"], [115, 116, "MethodName", "LDA"], [171, 172, "MethodName", "LDA"], [229, 231, "MetricName", "F1 score"], [301, 303, "MetricName", "F1 score"], [305, 306, "MethodName", "LDA"], [307, 308, "MethodName", "SVM"], [309, 311, "MethodName", "Logistic Regression"], [345, 346, "MethodName", "LDA"], [347, 349, "MethodName", "Logistic Regression"], [353, 354, "MethodName", "SVM"], [411, 413, "MetricName", "F1 score"], [518, 519, "MetricName", "F1"], [550, 551, "DatasetName", "Linux"]]}
{"text": "In this study we evaluated different types of models , namely word uni - grams , word n - grams , character n - grams , Viterbi Decoding , Latent Dirichlet Allocation , Support Vector Machine and Logistic Regression , for the task of semi - supervised language identification in English - Spanish codeswitched data . We found that most of the models achieved promising results , however , the Viterbi model performed the best with a weighted F1 score of 95.76 % on validation data and 92.23 % on test data ( RQ1 ) . Using this model , one can potentially train CS - detection for many more language pairs as previously possible . Furthermore , since the majority voting did not lead to improvements , we experimented with an Oracle model , which showed that by combining results form our models , the best score we could achieve is 98.47 % on validation data . Even though the results were good , our models still underperformed compared to the supervised MaChAmp model , that scored 99.24 % weighted F1 score on validation data and 98.43 % on test data ( RQ2 ) . There is also a clear take away that , by using simpler , faster approaches like ours and when top performance is not crucial , one can avoid the extensive process of human - annotation and long training time that are needed by finetuning these large transformer models on supervised data . It can be noted that the confusion matrix for MaChAmp model has more than the three labels we used , because it was trained on part of the original training set presented in Section 2.1 . This set contained 8 classes , and , thus , occasionally , the model mistakenly predicted some of these classes . It can be seen that there was more confusion in Viterbi model about other , where 213 words were classified as lang1 and 60 as lang2 instead , compared to just 3 and 1 in MaChAmp , which also had 7 other misclassifications .", "entities": [[33, 36, "MethodName", "Support Vector Machine"], [37, 39, "MethodName", "Logistic Regression"], [47, 49, "TaskName", "language identification"], [78, 80, "MetricName", "F1 score"], [104, 105, "DatasetName", "CS"], [181, 183, "MetricName", "F1 score"]]}
{"text": "Relation extraction is a core natural language processing task which is concerned with the extraction of relations between entities from text . It has numerous applications ranging from question answering ( Xu et al , 2016 ) to automated knowledge base construction ( Dong et al , 2014 ) . While the vast majority of existing research focuses on extracting binary relations , there exists only few recent approaches to extract n - ary relations , that is , relations among n \u2265 2 entities ( Li et al , 2015 ; Ernst et al , 2018 ) . In n - ary relation extraction , relation mentions tend to span multiple sentences more frequently as n increases . Thus , Peng et al ( 2017 ) recently extended the problem to cross - sentence n - ary relation extraction in which n - ary relations are extracted from multiple sentences . As a motivating example , consider the following text from Wikipedia : \" Revis started off the 2009 season matched up against some of football 's best wide receivers . In Week 1 , he helped limit Houston Texans Pro - bowler Andre Johnson to four receptions for 35 yards . \" In this example , two sentences collectively describes that Andre Johnson is a player of the football team the Texans during 2009 season , and thus we need cross - sentence information to correctly extract this ternary interaction among the three entities , i.e. Player ( Andre Johnson , Texans , 2009 season ) . Previous methods ( Peng et al , 2017 ; Song et al , 2018 ) capture cross - sentence n - ary relation mentions by representing texts with a document graph which consists of both intra - and cross - sentence links between words . With this graphical representation , they applied graph neural networks to predict ternary relations in the medical domain . However , these methods train the neural networks in a supervised manner using distant supervision ( Mintz et al , 2009 ) and , therefore , may suffer from the lack of sufficient positive labels when a well - populated knowledge base is not available . On the other hand , for binary relation extraction , the problem of insufficient positive labels can be mitigated with universal schemas . In a universal schema approach , textual representations ( surface patterns ) of entities and their relations are encoded into the same vector space as the canonical knowledge base relations . Thus , semantically similar surface patterns can share information of relation labels in a semisupervised manner . This reduces the amount of required labeled training data . Applying the universal schema approach to n - ary ( n > 2 ) relation extraction is , however , not straight - forward due to the sparsity of higher - order relation mentions among a specific set of n > 2 entities . 1 This is be - cause the universal schema approach and its extensions ( Toutanova et al , 2015 ; Verga et al , 2016Verga et al , , 2017 utilize co - occurring patterns of relation types between specific pair of entities . Also , prior work has only addressed binary relations , and it is not trivial to define surface patterns among n > 2 entities and to encode these patterns into a vector representation . To mitigate the aforementioned sparsity problem and utilize existing encoders for binary and unary surface patterns , we propose to train universal schema models on more dense lower - arity ( unary and binary ) facts instead of original sparse n - ary facts . Since most n - ary relations can be decomposed into a set of k - ary relations ( k = 1 , 2 ) which are implied by the n - ary relation , 2 we can easily acquire lower - arity facts by decomposing n - ary facts . Our model learns representations of these lower - arity relations using the universal schema framework , and predicts new n - ary facts by aggregating scores of lower - arity facts . To evaluate the proposed method , we create new cross - sentence n - ary relation extraction datasets with multiple ternary relations . 3 The new datasets contain more entity tuples with known relational facts appeared in a knowledge base than the existing dataset ( Peng et al , 2017 ) , and , therefore , these datasets can be used to more effectively evaluate methods which predict relation labels for each individual entity tuple . We show empirically that by jointly training lower - arity models and an nary score aggregation model , the proposed method improves the performance of n - ary relation extraction . To the best of our knowledge , this is the first attempt to apply universal schemas to n - ary relation extraction , taking advantage of the compositionality of higher - arity facts .", "entities": [[0, 2, "TaskName", "Relation extraction"], [28, 30, "TaskName", "question answering"], [103, 105, "TaskName", "relation extraction"], [138, 140, "TaskName", "relation extraction"], [189, 190, "DatasetName", "Houston"], [320, 322, "DatasetName", "medical domain"], [375, 378, "TaskName", "binary relation extraction"], [464, 466, "TaskName", "relation extraction"], [635, 637, "HyperparameterName", "k ="], [714, 716, "TaskName", "relation extraction"], [803, 805, "TaskName", "relation extraction"], [826, 828, "TaskName", "relation extraction"]]}
{"text": "To alleviate the sparsity problem of facts among n entities ( n > 2 ) and to utilize well - studied encoders for binary and unary surface patterns , we decompose a set of original n - ary facts , O , into a set of unary facts O 1 and a set of binary facts O 2 ( Figure 1 ) . Unary Facts : Given an n - ary fact r , ( e 1 , ... , e k ) O , we decompose it into a set of n unary facts { r ( k ) , e k : k = 1 , ... , n } , where r ( k ) is a tentative unary relation w.r.t . the k - th argument of the original relation r. If r is a KB relation , we define unary relation r ( k ) as a new canonicalized relation . If r is section T , we define unary relation r ( k ) as a tuple r ( k ) = ( T , pos ( e k ) ) , where pos ( e k ) is a set of word position indices of entity e k in section T ( Figure 2 ) . We denote a set of all decomposed unary facts by O 1 . Intuitively , these unary relations represent semantic roles or types of corresponding arguments of the original relation r . Binary Facts : Given an n - ary fact r , ( e 1 , ... , e k ) O , we decompose it into a set of n ( n \u2212 1 ) binary facts { r ( k , l ) , ( e k , e l ) : k , l = 1 , ... , n , k = l } , where r ( k , l ) is a tentative binary relation between the k - th and l - th argument of the original relation r. If r is a KB relation , we define binary relation r ( k , l ) as a new canonicalized relation . If r is a section T , we represent it by the shortest path between e k and e l on the document graph ( Quirk and Poon , 2017 ) of T ( Figure 2 ) , and denote it by path ( T ; e k , e l ) . We denote the set of all decomposed binary facts by O 2 .", "entities": [[104, 106, "HyperparameterName", "k ="], [308, 310, "HyperparameterName", "k ="]]}
{"text": "We follow Verga et al ( 2017 ) to train relation representations ( 3.2 ) . We define a score \u03b8 r , p for each lower - arity fact r , p O 1 \u222a O 2 , and minimize the following loss ( 3 ) for each arity i = 1 , 2 . Here , placeholder p refers to either an entity ( if r , p O 1 ) or an entity tuple ( if r , p O 2 ) , and we simply refer to both as entity tuple . The loss functions contrast a score of an original fact r , p + O i and those of K sampled negative facts r , p \u2212 k / O i . We sample negative facts by randomly replacing entity tuple p + in the original fact by different entity tuples p \u2212 k . L i = E r , p + O i r , p \u2212 k / O i [ \u2212 log ( exp ( \u03b8 r , p + ) exp ( \u03b8 r , p + ) + k exp ( \u03b8 r , p \u2212 k ) ) ] . ( 3 ) The score of fact r , p is defined as \u03b8 r , p = v ( r ) T v ( p ; r ) . Entity tuple representations v ( p ; r ) are computed with a weighted average of the representations { v ( r ) : r V ( p ) } as shown in ( 4 ) and ( 5 ) where a ( r , r ; V ( p ) ) is the attention weight for each relation r V ( p ) . 5 v ( p ; r ) = r V ( p ) a ( r , r ; V ( p ) ) v ( r ) , a ( r , r ; V ( p ) ) = exp ( v ( r ) T v ( r ) ) r V ( p ) exp ( v ( r ) T v ( r ) ) . ( 4 ) V ( p ) = { r : r , e k O 1 } ( if p = e k ) { r : r , ( e k , e l ) O 2 } ( if p = ( e k , e l ) ) . ( 5 )", "entities": [[20, 21, "HyperparameterName", "\u03b8"], [43, 44, "MetricName", "loss"], [97, 98, "MetricName", "loss"], [175, 176, "HyperparameterName", "\u03b8"], [183, 184, "HyperparameterName", "\u03b8"], [193, 194, "HyperparameterName", "\u03b8"], [216, 217, "HyperparameterName", "\u03b8"]]}
{"text": "To predict n - ary facts of KB relation r R KB , we compute its score \u03b8 r , ( e 1 , ... , en ) by aggregating lower - arity scores as in ( 6 ) , where w ( ) r is a positive scalar weight defined for each KB relation which sum to one : k w ( k ) r + k = l w ( k , l ) r = 1 . We can set al weights w ( k ) r and w ( k , l ) r to 1 / n 2 , or train these weights to give higher scores to positive n - ary facts by minimizing additional loss function L n . Note that L n directly contrasts n - ary scores associated with KB relations r R KB in a more supervised manner than both L 1 and L 2 . 6 ( 6 ) L n = E r , p + O KB r , p \u2212 / O KB [ max ( 0 , 1 \u2212 \u03b8 r , p + + \u03b8 r , p \u2212 ) ] ( 7 ) The overall loss function is now L = L 1 + L 2 + \u03b1L n . By changing \u03b1 , we can balance the semisupervised effect of lower - arity universal schemas ( L 1 , L 2 ) and that of the supervision with n - ary relation labels ( L n ) .", "entities": [[17, 18, "HyperparameterName", "\u03b8"], [67, 69, "HyperparameterName", "k ="], [121, 122, "MetricName", "loss"], [180, 181, "DatasetName", "0"], [184, 185, "HyperparameterName", "\u03b8"], [190, 191, "HyperparameterName", "\u03b8"], [202, 203, "MetricName", "loss"], [219, 220, "HyperparameterName", "\u03b1"]]}
{"text": "We compared the methods in the held - out evaluation as in ( Mintz et al , 2009 ) and report ( weighted ) mean average precision ( MAP ) . Unless otherwise noted , reported values are average values over six experiments , in which network parameters are randomly initialized . All reported p - values are calculated based on Wilcoxon rank sum test ( Wilcoxon , 1945 ) with", "entities": [[25, 27, "MetricName", "average precision"], [28, 29, "DatasetName", "MAP"]]}
{"text": "Table 1 illustrates the performance of each method . 12 Compared to the baseline methods , our proposed method achieves higher weighted MAP for both datasets . Interestingly , Model F performs well in Verga et al ( 2017 ) baseline , while it shows low performance in Toutanova et al ( 2015 ) baseline . Ablation Study : Table 2 illustrates the performance of various settings of our proposed method . U , B , and N stand for using the loss functions L 1 , L 2 , and \u03b1L n respectively . In the result , U+B performs significantly better ( p < 0.005 ) than U and B , and this shows effectiveness of combining scores of both binary facts and unary facts . On the other hand , there was no significant difference between U+B+N and N ( p > 0.9 ) . Note that we used all positive labels in this experiment , that is , sufficient amount of positive labels are used for calculating the loss N. Data efficiency : Furthermore , we also investigated the influence of the training data size ( the number of positive labels ) of our proposed method and baseline methods . 13 Here , \u03b1 = stands for optimizing L n instead of L 1 + L 2 + \u03b1L n . As shown in Figure 3 , \u03b1 = 1 achieved higher performance than \u03b1 = , showing that introducing lower - arity semi - supervised loss ( L 1 + L 2 ) improves the performance for dataset with few positive labels . On the other hand , the lower performance of \u03b1 = 0 compared to \u03b1 = 0.1 , 1 suggests that information of higher - arity facts introduced from L n is benefitial for n - ary relation extraction .", "entities": [[22, 23, "DatasetName", "MAP"], [82, 83, "MetricName", "loss"], [172, 173, "MetricName", "loss"], [207, 208, "HyperparameterName", "\u03b1"], [231, 232, "HyperparameterName", "\u03b1"], [238, 239, "HyperparameterName", "\u03b1"], [250, 251, "MetricName", "loss"], [277, 278, "HyperparameterName", "\u03b1"], [279, 280, "DatasetName", "0"], [282, 283, "HyperparameterName", "\u03b1"], [305, 307, "TaskName", "relation extraction"]]}
{"text": "We proposed a new method for cross - sentence nary relation extraction that decomposes sparse n - 12 For the proposed method , we set \u03b1 = 10 . 13 In this experiment , we conducted four experiments per each setting and set K = 10 . ary facts into dense unary and binary facts . Experiments on two datasets with multiple ternary relations show that our proposed method can statistically significantly improve over previous works , which suggests the effectiveness of using unary and binary interaction among entities in surface patterns . However , as Fatemi et al ( 2019 ) suggests , there exists cases in which reconstructing n - ary facts from decomposed binary facts induces false positives . Tackling this issue is one important future research direction .", "entities": [[10, 12, "TaskName", "relation extraction"], [25, 26, "HyperparameterName", "\u03b1"], [43, 45, "HyperparameterName", "K ="]]}
{"text": "ZAR as argument selection As illustrated in Figure 3 , the basic idea behind BERT - based ZAR is that given the powerful neural encoder , the joint task of omission detection and antecedent identification can be formalized as argument selection ( Shibata and Kurohashi , 2018 ; Kurita et al , 2018 ; Ueda et al , 2020 ) . Omission detection concerns whether a given predicate has an argument for a given case ( relation ) . If not , the model must point to the special token [ NULL ] . Otherwise the model must identify the antecedent of the zero pronoun by pointing either to a token in the given text or to a special token reserved for exophora . Note that by getting the entire document as the input , the model can handle inter - sentential anaphora as well as intra - sentential anaphora . In practice , the input length limitation of BERT forces us to implement a sliding window approach . Also note that in this formulation , ZAR is naturally subsumed into verbal predicate analysis ( VPA ) , which also covers instances where the predicate and the argument have a dependency relation and only the case marker is absent . Formally , the probability of the token t j being the argument of the predicate t i for case c is : P ( t j | t i , c ) = exp ( s c ( t j , t i ) ) j exp ( s c ( t j , t i ) ) ( 1 ) s c ( t j , t i ) = v tanh ( W c t j + U c t i ) ( 2 ) where t i is the context - aware embedding of t i provided by BERT , W c and U c are case - specific weight matrices , and v is a weight vector shared among cases . We output t j with the highest probability . For each predicate , we repeat this for the nominative ( NOM ) , accusative ( ACC ) , and dative ( DAT ) cases , and another nominative case for the double nominative construction ( NOM2 ) .", "entities": [[14, 15, "MethodName", "BERT"], [159, 160, "MethodName", "BERT"], [311, 312, "MethodName", "BERT"], [360, 361, "MetricName", "ACC"]]}
{"text": "As discussed in Section 2.2 , MT as an intermediate task reportedly harms target - task performance , probably because MT forces the model to forget what it has learned from MLM pretraining ( catastrophic forgetting ) . To overcome this problem , we incorporate the MLM training objective into MT , as suggested by Pruksachatkun et al ( 2020 ) . Specifically , we mask some input tokens on the encoder Web News # of sentences 16 , 038 11 , 276 # of zeros 30 , 852 27 , 062 side and force the model to recover the original tokens , as depicted in the center of Figure 2 . Our masking strategy is the same as BERT 's ( Devlin et al , 2019 ) : We choose 15 % of the tokens at random and 80 % of them are replaced with [ MASK ] , 10 % of them with a random token , and the rest are unchanged . The corresponding losses are added to the MT loss function .", "entities": [[31, 32, "DatasetName", "MLM"], [46, 47, "DatasetName", "MLM"], [119, 120, "MethodName", "BERT"], [173, 174, "MetricName", "loss"]]}
{"text": "BERT We employed a Japanese BERT model with BPE segmentation distributed by NICT . 6 It had the same architecture as Google 's BERT - Base ( Devlin et al , 2019 ) : 12 layers , 768 hidden units , and 12 attention heads . It was trained on the full text of Japanese Wikipedia for approximately 1 million steps . MT We used the Transformer encoder - decoder architecture ( Vaswani et al , 2017 ) . The encoder was initialized with BERT while the decoder was a randomly initialized six - layer Transformer . The numbers of hidden units and heads were set to be the same as BERT 's ( i.e. , 768 units and 12 attention heads ) . We adopted Adam ( Kingma and Ba , 2017 ) as the optimizer . We set the total number of epochs to 50 . In two - stage optimization , the encoder was frozen during the first 15 epochs , then the entire model was updated for the remaining 35 epochs . We set a mini - batch size to about 500 . The details of hyper - parameters are given in Appendix A. ZAR For a fair comparison with Ueda et al ( 2020 ) , we used almost the same configuration as theirs . We dealt with all subtypes of ZAR : intra - sentential anaphora , inter - sentential anaphora , and exophora . For exophora , we targeted [ author ] , [ reader ] , and [ unspecified person ] . We set the maximum sequence length to 128 . 7 All documents from the Web met this limitation . In the News corpus , however , many documents exceeded the sequence length of 128 . For such documents , we divided the document into multiple parts such that it had the longest preceding contexts . The evaluation of ZAR was relaxed using a gold coreference chain . The model was trained on the mixture of both corpora and evaluated on each corpus . We used almost the same", "entities": [[0, 1, "MethodName", "BERT"], [5, 6, "MethodName", "BERT"], [8, 9, "MethodName", "BPE"], [21, 22, "DatasetName", "Google"], [23, 24, "MethodName", "BERT"], [66, 67, "MethodName", "Transformer"], [84, 85, "MethodName", "BERT"], [95, 96, "MethodName", "Transformer"], [111, 112, "MethodName", "BERT"], [126, 127, "MethodName", "Adam"], [136, 137, "HyperparameterName", "optimizer"], [142, 145, "HyperparameterName", "number of epochs"], [179, 183, "HyperparameterName", "mini - batch size"]]}
{"text": "Web News hyper - parameters as Ueda et al ( 2020 ) , which are included in Appendix B. We decided to tune the training epochs for MT since we found that it slightly affected ZAR performance . We collected checkpoints at the interval of 5 epochs out of 45 epochs , in addition to the one with the lowest validation loss . They were all trained on ZAR , and we chose the one with the highest score on the validation set . We ran the model with 3 seeds on MT and with 3 seeds on ZAR , which resulted in 9 seed combinations . We report the mean and the standard deviation of the 9 runs .", "entities": [[61, 62, "MetricName", "loss"], [90, 91, "DatasetName", "seeds"], [96, 97, "DatasetName", "seeds"]]}
{"text": "The experimental results demonstrate that MT helps ZAR , but why does it work ? Unfortunately , conventional evaluation metrics for MT ( e.g. , BLEU ) reveal little about the model 's ability to handle zero anaphora . To address this problem , Shimazu et al ( 2020 ) and Nagata and Morishita ( 2020 ) constructed Japanese - English parallel datasets that were designed to automatically evaluate MT models with regard to the translation of Japanese zero pronouns ( ZPT ) . We used Shimazu et al 's dataset for its larger data size . 10 To facilitate automatic evaluation of ZPT , this dataset paired a correct English sentence with an incorrect one . All we had to do was to calculate the ratio of instances for which the model assigned higher translation scores to the correct candidates . The only difference between the two sentences involved the translation of a Japanese zero pronoun . To choose the correct one , the MT model must sometimes refer to preceding sentences . As in intermediate training , multiple source sentences were fed to the model to generate multiple target sentences . We prepended as many preceding sentences as possible given the limit of 128 tokens . In addition , this dataset recorded d , the sentencelevel distance between the zero pronoun in question and its antecedent . The number of instances with d = 0 was 218 while the number of remaining instances was 506 . We regarded the former as the instances of intra - sentential anaphora and the latter as the instances of inter - sentential anaphora . We chose the model with the best performance ( i.e. , one - stage optimization with MLM ) . For each checkpoint we collected during intermediate training , we ( 1 ) measured the ZPT accuracy and ( 2 ) finetuned it to obtain the F1 score for ZAR . As before , Web News intra - sentential anaphora 0.758 0.763 inter - sentential anaphora 0.871 0.879 5 shows the strong positive correlations between the two performance measures , especially the very strong correlation for inter - sentential anaphora . These results were in line with our speculation that the performance gains in ZAR stemmed from the model 's increased ability to translate zero pronouns .", "entities": [[25, 26, "MetricName", "BLEU"], [236, 237, "DatasetName", "0"], [288, 289, "DatasetName", "MLM"], [307, 308, "MetricName", "accuracy"], [317, 319, "MetricName", "F1 score"]]}
{"text": "Web News F1 F1 + MT 70.5 - 57.7 - + MT w/ masking 71.1 0.6 57.8 0.1 + MT w/ MLM 71.9 1.4 58.3 0.6 To dig into this question , we conducted an ablation study by introducing a model with token masking but without the corresponding loss function ( denoted as + MT w/ masking ) . We assume that this model was largely deprived of the power to mitigate CF while token masking still acted as a data augmenter . Table 6 shows the results . Not surprisingly , + MT w/ masking was beaten by + MT w/ MLM with large margins . However , it did outperform + MT , and the gain was particularly large for the Web . The fact that the contribution of the loss function was larger than that of token masking indicates that the improvements were mainly attributed to CF mitigation , but the contribution of token masking alone should not be overlooked .", "entities": [[2, 3, "MetricName", "F1"], [3, 4, "MetricName", "F1"], [21, 22, "DatasetName", "MLM"], [48, 49, "MetricName", "loss"], [102, 103, "DatasetName", "MLM"], [132, 133, "MetricName", "loss"]]}
{"text": "Various studies have explored paraphrase generation for dialog systems . Bowman et al ( 2016 ) showed that generating sentences from a continuous latent space is possible using a variational autoencoder model and provided guidelines on how to train such a generation model . However , our model uses an encoder - decoder approach which can handle the intent and language as categorical inputs in addition to the sequence input . Malandrakis et al ( 2019 ) explored a variety of controlled paraphrase generation approaches for data augmentation and proposed to use conditional variational autoencoders which they showed obtained the best results . Our method is different as it uses a conditional seq2seq model that can generate text from any sequence of slots and does not require an utterance as an input . Xia et al ( 2020 ) propose a transformer - based conditional variational autoencoder for few shot utterance generation where the latent space represents the intent as two independent parts ( domain and action ) . Our approach is different since it models the language and intent of the generation that can be controlled explicitly . Also , our model is the first to enable zero - shot utterance generation . Cho et al ( 2019 ) generate paraphrases for seed examples with a transformer seq2seq model and self - label them with a baseline intent and slot model . We follow a similar approach but our model generates utterances from a sequence of slots rather than an utterance , which enables an explicitly controlled generation . Also the number of seed utterances we use is merely 20 for the few shot setup unlike around 1 M seed para - carrier phrase pairs in Cho et al ( 2019 ) . Several other studies follow a text - to - text ap - proach and assume training data in the form of paraphrase pairs for training paraphrase generation models in a single language Li et al , 2018Li et al , , 2019 . Our approach is focused towards generating utterances in the dialog domain that can generate utterances from a sequence of slots conditioned on both intent and language . Jolly et al ( 2020 ) showed that an interpretationto - text model can be used with shuffling - based sampling techniques to generate diverse and novel paraphrases from small amounts of seed data , that improve accuracy when augmenting to the existing training data . Our approach is different as our model can generate the slot annotations along with the the utterance , which are necessary for the slot labeling task . Our model can be seen as an extension of the model by Jolly et al ( 2020 ) to a transformer based model , with the added functionality of controlling the language in which the utterance generation is needed , which in turn enables zero shot generation . Using large pre - trained models has also been shown to be effective for paraphrase generation . Chen et al ( 2020 ) for instance show the effectiveness of using GPT - 2 ( Radford et al , 2019 ) for generating text from tabular data ( a set of attributevalue pairs ) . Our model , however , does not rely on pre - trained weights from another model such as GPT - 2 , is scalable , and can be applied to training data from any domain , for instance , dialog domain . Beyond paraphrase generation , several other techniques have been proposed for feature bootstrapping . Machine translation can be used from data - rich to data - scarce languages ( Gaspers et al , 2018 ; Xu et al , 2020 ) . Cross - lingual transfer learning can also leverage use existing data in other languages ( Do and Gaspers , 2019 ) . If a feature is already being actively used , feedback signals from users , such as paraphrases or interruptions , can be used to obtain additional training data ( Muralidharan et al , 2019 ; .", "entities": [[4, 6, "TaskName", "paraphrase generation"], [29, 31, "MethodName", "variational autoencoder"], [82, 84, "TaskName", "paraphrase generation"], [86, 88, "TaskName", "data augmentation"], [94, 95, "MethodName", "autoencoders"], [112, 113, "MethodName", "seq2seq"], [145, 147, "MethodName", "variational autoencoder"], [218, 219, "MethodName", "seq2seq"], [319, 321, "TaskName", "paraphrase generation"], [401, 402, "MetricName", "accuracy"], [499, 501, "TaskName", "paraphrase generation"], [515, 516, "MethodName", "GPT"], [557, 558, "MethodName", "GPT"], [582, 584, "TaskName", "paraphrase generation"], [595, 597, "TaskName", "Machine translation"], [623, 627, "TaskName", "Cross - lingual transfer"]]}
{"text": "Generating the output sequence token - by - token can be done by using greedy decoding where given learned model parameters \u03b8 , the most likely token is picked at each decoding step as x t = argmax p \u03b8 ( x t | x < t ) . Such a generation process is deterministic . For our task of generating paraphrases , we are interested in generating diverse and novel utterances . Non - deterministic sampling methods such as top - k sampling has been used in related work ( Fan et al , 2018 ; Welleck et al , 2020 ; Jolly et al , 2020 ) to achieve this . In top - k random sampling , we first scale the logits z w by using a temperature parameter \u03c4 before applying softmax . p ( x t = w | x < t ) = exp ( z w /\u03c4 ) w V exp ( z w /\u03c4 ) , ( 1 ) where V is the decoder 's vocabulary . Setting \u03c4 > 1 encourages the resulting probability distribution to be less spiky , thereby encouraging diverse choices during sampling . The top - k sampling restricts the size of the most likely candidate pool to k \u2264 | V | .", "entities": [[21, 22, "HyperparameterName", "\u03b8"], [39, 40, "HyperparameterName", "\u03b8"], [135, 136, "MethodName", "softmax"]]}
{"text": "Paraphrase generation training Since the training data is imbalanced , we balanced the training data by oversampling the intents to match the frequency of the most frequent intent . 3 For both the encoder and the decoder , the multi - head attention layers ' hidden dimension was set to 128 and the position - wise feed forward layers ' hidden dimension was set to 256 . The number of encoder and decoder layers was set to 3 each . The number of heads was set to 8 . Dropout of 0.1 was used in both the encoder and the decoder . The model parameters were initialized with Xavier initialization ( Glorot and Bengio , 2010 ) . The model was trained using Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 5e - 4 and a gradient clipping of 1 . The training was stopped when the development loss did not improve for 5 epochs . Generating paraphrases For generating paraphrases in the target intent in the target language , we used the slots appearing in the existing training data in the target intent . We used greedy decoding and top - k sampling with k = 3 , 5 , 10 and \u03c4 = 1.0 , 2.0 . For a given input , we generated using the top - k random sampling three times with different random seeds . We finally combined all generations and ranked the candidates using the baseline downstream system 's prediction probability . The number of paraphrases that are selected is determined as in 3.3 , with 20 as the minimum . Methods for comparison We compare our method against four alternatives : ( a ) Baseline : No data augmentation at all . The downstream model is trained using just the available seed examples for the target intent . ( b ) Oversampling : We oversample the samples per intent uniformly at random to match the size of the augmented training data using the proposed method . This is only applicable to the few shot setup since for the zero shot setup , there are no existing samples in the target intent in the target language to sample from . ( c ) CVAE seq2seq model : We generate paraphrases using the CVAE seq2seq model by Malandrakis et al ( 2019 ) . The original CVAE seq2seq model as proposed by Malandrakis et al ( 2019 ) defines the set { domain , intent , slots } as the signature of an utterance and denotes the carrier phrases for a given signature to be paraphrases . These carrier phrases are then used to create input - output pairs for the CVAE seq2seq model training . Since the original formulation does not take into account the language of generation , we adapt the method for our case by defining the signature as the set { language , intent , slots } . We set the model 's hidden dimension to 128 , used the 100 - dimensional GloVe embeddings ( Pennington et al , 2014 ) pretrained on Wikipedia , and trained the model without freezing embeddings using early stopping with a patience of 5 epochs by monitoring the development loss . Finally we generated 100 carrier phrases for each carrier phrase input in the target intent in the target language . Paraphrases were obtained by injecting the slot values to the generated carrier phrases . The pool of all paraphrases was sorted using the baseline downstream system 's prediction probabilities . The CVAE seq2seq model was only applicable to the few shot setup since in the zero shot setup there are no existing carrier phrases in the target language in the target intent that can be used to sample from . ( d ) Machine translation : We augmented the translations generated from English using the MT+fastalign approach from the MultiATIS++ paper ( Xu et al , 2020 ) . For the few shot setup , we added all the translated utterances except the ones that correspond to those utterances we already picked as the few shot samples . For the zero shot setup , we added all the translated utterances . ( Su et al , 2018 ) did not improve for 3 epochs .", "entities": [[0, 2, "TaskName", "Paraphrase generation"], [39, 43, "MethodName", "multi - head attention"], [89, 90, "MethodName", "Dropout"], [108, 110, "MethodName", "Xavier initialization"], [123, 124, "MethodName", "Adam"], [124, 125, "HyperparameterName", "optimizer"], [134, 136, "HyperparameterName", "learning rate"], [142, 144, "MethodName", "gradient clipping"], [154, 155, "MetricName", "loss"], [201, 203, "HyperparameterName", "k ="], [234, 235, "DatasetName", "seeds"], [290, 292, "TaskName", "data augmentation"], [375, 376, "MethodName", "CVAE"], [376, 377, "MethodName", "seq2seq"], [384, 385, "MethodName", "CVAE"], [385, 386, "MethodName", "seq2seq"], [397, 398, "MethodName", "CVAE"], [398, 399, "MethodName", "seq2seq"], [452, 453, "MethodName", "CVAE"], [453, 454, "MethodName", "seq2seq"], [508, 510, "MethodName", "GloVe embeddings"], [529, 531, "MethodName", "early stopping"], [541, 542, "MetricName", "loss"], [594, 595, "MethodName", "CVAE"], [595, 596, "MethodName", "seq2seq"], [636, 638, "TaskName", "Machine translation"]]}
{"text": "We evaluate the quality of the generated paraphrases using the following metrics . Let S be the set of input slot types and G be the set of generated slot types . All retrieval score The all retrieval score r measures if all the input slots were retrieved in the generation . r = 1 if | S \u2229 G | = | S | 0 otherwise ( 2 ) Exact match The exact match score r measures if all the input slots and output slots exactly match ( Malandrakis et al , 2019 ) . r = 1 if S = G 0 otherwise ( 3 ) Partial match The partial match score r measures if at least one output slot matches an input slot . r = 1 if | S \u2229 G | > 0 0 otherwise ( 4 ) F1 slot score The F1 slot score F 1 measures the set similarity between S and G using precision and recall which are defined for sets as follows . precision = | S \u2229 G | | G | , recall = | S \u2229 G | | S | ( 5 ) Jaccard index Jaccard index measures the set similarity between S and G as their intersection size divided by the union size . Novelty Let P be the set of paraphrases generated from a base utterance u. novelty = 1 | P | u P 1 \u2212 BLEU4 ( u , u ) ( 6 ) Diversity The diversity is computed using the generated paraphrases P . diversity = u P , u P , u = u 1 \u2212 BLEU4 ( u , u ) | P | \u00d7 ( | P | \u2212 1 ) ( 7 ) Language detection score We are interested in quantifying if a generated paraphrase is in the target language . We use langdetect 5 to compute p ( lang = target lang ) . Higher scores denote better language generation . Table 5 : Downstream slot labeling F1 scores ( % ) . Each score shown is the average score of 10 runs . 5 Experimental results", "entities": [[65, 66, "DatasetName", "0"], [70, 72, "MetricName", "Exact match"], [73, 75, "MetricName", "exact match"], [103, 104, "DatasetName", "0"], [137, 138, "DatasetName", "0"], [138, 139, "DatasetName", "0"], [143, 144, "MetricName", "F1"], [147, 148, "MetricName", "F1"], [340, 341, "MetricName", "F1"]]}
{"text": "For both the few shot and zero shot setups , the paraphrases used for intrinsic evaluation are generated in the target intent and the target language only . For the top - k sampling based generation , we generate for each input three times with different random seeds and compute novelty and diversity scores . Table 2 shows intrinsic evaluation results for different generation methods . For the few shot setup , the all retrieval , exact match , partial match , F1 slot and Jaccard index scores decrease upon increasing top - k and temperature . The highest scores for the above metrics are obtained for the greedy generation , which indicates that the generated slot types are most similar to the input slot types in that case . However , it is the opposite for the novelty and diversity metrics where the scores are higher with larger top - k and temperatures . For the zero shot setup , the overall trend is similar to the few shot setup . The slot similarity based metrics are lower in general , which indicates that even as little as 20 samples in the few shot setup improve the generation of desired slots . The novelty scores for the zero shot setup are 1 as we would expect . In Table 3 , we show that the intrinsic evaluation results using the proposed approach are consistently better than the CVAE seq2seq paraphrase generation model ( Malandrakis et al , 2019 ) . The language detection score varies across languages , which may be due to the vocabulary overlap between languages , e.g. , San Francisco appears in both English and German utterances . Interestingly we also observe code switching , i.e. mixedlanguage generations , while using our approach .", "entities": [[47, 48, "DatasetName", "seeds"], [76, 78, "MetricName", "exact match"], [82, 83, "MetricName", "F1"], [238, 239, "MethodName", "CVAE"], [239, 240, "MethodName", "seq2seq"], [240, 242, "TaskName", "paraphrase generation"]]}
{"text": "We evaluate the downstream intent classification using accuracy and the slot labeling using F1 score . Since we are interested in measuring the variation in scores for the target intents , we only report the scores for the test samples in the target intents in Tables 4 and 5 . We run each downstream training experiment 10 times and report the mean scores for each language and also the average across languages in the AVG column in Tables 4 and 5 . We are also interested in tracking the scores for the test samples having intents other than the target intents since we need to ensure that the scores on the other intents does not go down . We found that the effect on the scores ( both intent classification and slot labeling ) for the other intents is negligible using paraphrasing and other methods . 6 In Tables 4 and 5 , our paraphrasing results outperform the baseline scores on average . In the few shot setup , our paraphrasing approach outperforms the CVAE seq2seq approach in 6 ( DE , ES , FR , HI , JA , ZH ) out of 8 languages in intent classification and overall obtains an improvement of 1.9 % intent classification accuracy across all target languages . Both oversampling and MT approaches are competitive . Oversampling performs the best for JA whereas MT performs the best for ES and HI . Our paraphrasing approach results in the best intent classification scores overall ( 78 % ) . In terms of slot F1 scores , we see mixed results with no clear best method ( baseline , oversampling and CVAE all result in 87.6 % F1 score ) . Notably , the MT approach results in the lowest overall slot F1 score of just 84.8 % on average . In the zero shot setup , the MT approach outperforms our paraphrasing approach by a large margin in intent classification ( 62.5 % ) . However we note that the paraphrasing approach requires no dependencies on other models or other data , unlike the MT approach which requires a parallel corpus to train the MT model . In terms of slot F1 scores , our paraphrasing approach and the baseline approach both result in almost similar overall scores ( 85.5 % and 85.4 % ) , both higher than the MT approach . The lower slot F1 scores using the MT approach in few and zero shot setups indicate that the fast align method to align slots in source and translation might result in noisy training data affecting the SL model .", "entities": [[4, 6, "TaskName", "intent classification"], [7, 8, "MetricName", "accuracy"], [13, 15, "MetricName", "F1 score"], [128, 130, "TaskName", "intent classification"], [174, 175, "MethodName", "CVAE"], [175, 176, "MethodName", "seq2seq"], [197, 199, "TaskName", "intent classification"], [207, 209, "TaskName", "intent classification"], [209, 210, "MetricName", "accuracy"], [246, 248, "TaskName", "intent classification"], [259, 260, "MetricName", "F1"], [276, 277, "MethodName", "CVAE"], [282, 284, "MetricName", "F1 score"], [297, 299, "MetricName", "F1 score"], [324, 326, "TaskName", "intent classification"], [367, 368, "MetricName", "F1"], [402, 403, "MetricName", "F1"]]}
{"text": "Word embeddings pre - trained over large texts have demonstrated benefits for many NLP tasks , especially when the task is label - deprived . However , many popular pre - trained sets of word embeddings assume fixed finite - size vocabularies 1 , 2 , which hinders their ability to provide useful word representations for out - of - vocabulary ( OOV ) words . We look into the task of generalizing word embeddings : extrapolating a set of pre - trained word embeddings to words out of its fixed vocabulary , without extra access to contextual information ( e.g. example sentences or text corpus ) . In contrast , the more common task of learning word embeddings , or often just word embedding , is to obtain distributed representations of words directly from large unlabeled text . The motivation here is to extend the usefulness of pre - trained embeddings without expensive retraining over large text . There have been works showing that contextual information can also help generalize word embeddings ( for example , Khodak et al , 2018 ; Schick and Sch\u00fctze , 2019a , b ) . We here , however , focus more on the research question of how much one can achieve from just word compositions . In addition , our proposed way of utilizing word composition information can be combined with the contextual embedding algorithms to further improve the performance of generalized embeddings . The hidden assumption here is that words are made of meaningful parts ( cf . morphemes ) and that the meaning of a word is related to the meaning of their parts . This way , humans are often able to guess the meaning of a word or term they have never seen before . For example , \" postEMNLP \" probably means \" after EMNLP \" . Different models have been proposed for that task of generalizing word embeddings using word compositions , usually under the name of subword ( level ) models . Stratos ( 2017 ) ; Pinter et al ( 2017 ) ; Kim et al ( 2018b ) model words at the character level . However , they have been surpassed by later subword - level models , probably because of putting too much burden on the models to form and discover meaningful subwords from characters . Bag - of - subwords ( BoS ) is a simple yet effective model for learning and generalizing ( Zhao et al , 2018 ) word embeddings . BoS composes a word embedding vector by taking the sum or average of the vectors of the subwords ( character n - grams ) that appear in the given word . However , it ignores the importance of different subwords since all of them are given the same weight . Intuitively , \" farm \" and \" land \" should be more relevant in composing representation for word \" farmland \" than some random subwords like \" armla \" . Even more favorable would be a model 's ability to discover meaningful subword segmentations on its own . Cotterell et al ( 2016 ) bases their model over morphemes but needs help from an external morphological analyzer such as Morfessor ( Virpioja et al , 2013 ) . Sasaki et al ( 2019 ) use trainable self - attention to combine subword vectors . While the attention implicitly facilitates interactions among subwords , there has been no explicit enforcement of mutual exclusiveness from subword segmentation , making it sometimes difficult to rule out less relevant subwords . For example , \" her \" is itself a likely subword , but is unlikely to be relevant for \" higher \" as the remaining \" hig \" is unlikely . We propose the probabilistic bag - of - subwords ( PBoS ) model for generalizing word embedding . PBoS simultaneously models subword segmentation and composition of word representations out of subword representations . The subword segmentation part is a probabilistic model capable of handling ambiguity of subword boundaries and ranking possible segmentations based on their overall likelihood . For each segmentation , we compose a word vector as the sum of all subwords that appear in the segmentation . The final embedding vector is the expectation of the word vectors from all possible segmentations . An alternative view is that the model assigns word - specific weights to subwords based on how likely they appear as meaningful segments for the given word . Coupled with an efficient algorithm , our model is able to compose better word embedding vectors with little computational overhead compared to BoS. Manual inspections show that PBoS is able to produce subword segmentations and subword weights that align with human intuition . Affix prediction experiment quantitatively shows that the subword weights given by PBoS are able to recover most eminent affixes of words with good accuracy . To assess the quality of generated word embeddings , we evaluate with the intrinsic task of word similarity which relates to the semantics ; as well as the extrinsic task of part - of - speech ( POS ) tagging which requires rich information to determine each word 's role in a sentence . English word similarity experiment shows that PBoS improves the correlation scores over previous best models under vari - ous settings and is the only model that consistently improves over the target pre - trained embeddings . POS tagging experiment over 23 languages shows that PBoS improves accuracy compared in all but one language to the previous best models , often by a big margin . We summarize our contributions as follows : We propose PBoS , a subword - level word embedding model that is based on probabilistic segmentation of words into subwords , the first of its kind ( Section 2 ) . We propose an efficient algorithm that leads to an efficient implementation 3 of PBoS with little overhead over previous much simpler BoS. ( Section 3 ) . Manual inspection and affix prediction experiment show that PBoS is able to give reasonable subword segmentations and subword weights ( Section 4.1 and 4.2 ) .", "entities": [[0, 2, "TaskName", "Word embeddings"], [34, 36, "TaskName", "word embeddings"], [73, 75, "TaskName", "word embeddings"], [83, 85, "TaskName", "word embeddings"], [117, 119, "TaskName", "word embeddings"], [171, 173, "TaskName", "word embeddings"], [320, 322, "TaskName", "word embeddings"], [419, 421, "TaskName", "word embeddings"], [819, 820, "MetricName", "accuracy"], [827, 829, "TaskName", "word embeddings"], [837, 839, "TaskName", "word similarity"], [852, 855, "DatasetName", "part - of"], [876, 878, "TaskName", "word similarity"], [921, 922, "MetricName", "accuracy"]]}
{"text": "For a given language , let \u0393 be its alphabet . A word w of length l = | w | is a string made of l letters in \u0393 , i.e. w = c 1 c 2 . . . c l \u0393 l where w [ i ] = c i is the i - th letter . Let p w [ 0 , 1 ] be the probability that w appears in the language . Empirically , this is proportional to the unigram frequency of word w observed in large text in that language . Note that we do not assume a vocabulary . That is , we do not distinguish words from arbitrary strings made out of the alphabet . The implicit assumption here is that a \" word \" in common sense is just a string associated with high probability . In this sense , p w can also be seen as the likelihood of string w being a \" legit word \" . This blurs the boundary between words and non - words , and automatically enables us to handle unseen words , alternative spellings , typos , and nonce words as normal cases . We say a string s \u0393 + is a subword of word w , denoted as s \u2286 w , if s = w [ i : j ] = c i . . . c j for some 1 \u2264 i \u2264 j \u2264 | w | , i.e. s is a substring of w. The probability that subword s appears in the language can then be defined as p s \u221d w \u0393 + p w 1\u2264i\u2264j\u2264 | w | 1 ( s = w [ i : j ] ) ( 1 ) where 1 ( pred ) gives 1 and otherwise 0 only if pred holds . Note that a subword s may occur more than once in the same word w. For example , subword \" ana \" occurs twice in the word \" banana \" . A subword segmentation g of word w of length k = | g | is a tuple ( s 1 , s 2 , . . . , s k ) of subwords of w , so that w is the concatenation of s 1 , . . . , s k .", "entities": [[6, 7, "HyperparameterName", "\u0393"], [29, 30, "HyperparameterName", "\u0393"], [43, 44, "HyperparameterName", "\u0393"], [64, 65, "DatasetName", "0"], [206, 207, "HyperparameterName", "\u0393"], [275, 276, "HyperparameterName", "\u0393"], [306, 307, "DatasetName", "0"], [352, 354, "HyperparameterName", "k ="]]}
{"text": "Based on the above modeling of subword segmentations , we propose the Probabilistic Bag - of - Subword ( PBoS ) model for composing word embeddings . The embedding vector w for word w is the expectation of all its segmentation - based word embedding : w = g Segw p g | w g ( 3 ) where g is the embedding for segmentation g. Given a subword segmentation g , we adopt the Bag - of - Subwords ( BoS ) model Zhao et al , 2018 ) for composing word embedding from subwords . Specifically , we apply BoS 4 over the subword segments in g : g = s g s , ( 4 ) where s is the vector representation for subword s , as if the current segmentation g is the \" golden \" segmentation of the word . In such case , we assume the meaning of the word is the combination of the meaning of all its subword segments . We maintain a look - up table S : \u0393 + R d for all subword vectors ( i.e. s = S ( s ) ) as trainable parameters of the model , where d is the embedding dimension . Combining Eq . ( 3 ) and ( 4 ) , we can compose vector representation for any word w \u0393 + as w = g Segw p g | w s g s. ( 5 ) Given a set of target pre - trained word vectors w * defined for words within a finite vocabulary W , our model can be trained by minimizing the mean square loss : minimize S 1 | W | w W w \u2212 w * 2 2 . ( 6 ) 3 Efficient Algorithm PBoS simultaneously considers all possible subword segmentations and their contributions in composing word representations . However , summing over embeddings of all possible segmentations can be awfully inefficient , as simply enumerating all possible segmentations of w takes number of steps exponential to the length of w ( Proposition 2 ) . We therefore need an efficient way to compute Eq . ( 5 ) .", "entities": [[24, 26, "TaskName", "word embeddings"], [177, 178, "HyperparameterName", "\u0393"], [205, 207, "HyperparameterName", "embedding dimension"], [228, 229, "HyperparameterName", "\u0393"], [276, 277, "MetricName", "loss"]]}
{"text": "Now we can efficiently compute Eq . ( 7 ) if we can efficiently compute a s | w . Here we present an algorithm that computes a s | w for all s \u2286 w in O ( | w | 2 ) time . The specific structure of the subword transition graph means that edges only go from left to right . Thus , we can split every path going through e into three parts : edges left to e , e itself and edges right to e. In terms of subwords , that is , for s = w [ i : j ] , l = | w | , each segmentation g that contains s can be divided into three parts : segmentation g w [ 1 : i\u22121 ] over w [ 1 : i \u2212 1 ] , = p s b 1 , i\u22121 b j+1 , l , ( 10 ) where b i , j = g Seg w [ i : j ] s g p s . Now we can efficiently compute a s | w if we can efficiently compute b 1 , i\u22121 and b j+1 , l for all 1 \u2264 i , j \u2264 l. Fortunately , we can do so for b 1 , i using the following recursive relation b 1 , i = i\u22121 k=0 b 1 , k p w [ k+1 : i ] ( 11 ) for i = 1 , . . . , l with b 1 , 0 = 1 . Similar formulas hold for b j , l , j = 1 , . . . , l with b l+1 , l = 1 . Based on this , we devise Algorithm 1 for computing a s | w for all s \u2286 w. Here we take the alternative view of our model as a weighted average of all possible subwords ( thus the normalization in Line 12 ) , and an extension to the unweighted averaging of subwords as used in Zhao et al ( 2018 ) . Algorithm 1 Computing a s | w . 1 : Input : Word w , p s for all s \u2286 w. l = | w | . 2 : b 1 , 0 1 ; b l+1 , l 1 ; 3 : for i 1 . . . l do 4 : b 1 , i i\u22121 k=0 p w [ k+1 : i ] b 1 , k 5 : b l\u2212i+1 , l l k = l\u2212i+1 p w [ l\u2212i+1 : k ] b k+1 , l6 : end for 7 : \u00e3 s | w 0 for all s \u2286 w 8 : for i 1 . . . l , j i . . . l do 9 : \u00e3 p w [ i : j ] b 1 , i\u22121 b j+1 , l 10 : \u00e3 w [ i : j ] | w \u00e3 w [ i : j ] | w + \u00e3 11 : end for 12 : a s | w \u00e3 s | w / s \u2286w\u00e3 s | w for all s \u2286 w 13 : return a | w Time complexity As we only access each subword once in each for - statement , the number of multiplications and additions involved is bounded by the number of subword locations of w. Each of Line 4 and Line 5 take i multiplications and i \u2212 1 additions respectively . So Line 3 to Line 6 in total takes 2l 2 computations . Line 8 to Line 11 takes 3l ( l+1 ) 2 computations . Thus , the time complexity of Algorithm 1 is O ( l 2 ) . Given a word of length 20 , O ( l 2 ) ( 20 2 = 400 ) is much better than enumerating all O ( 2 l ) ( 2 20 = 1 , 048 , 576 ) segmentations . Using the setting in Section 4.3 , PBoS only takes 30 % more time ( 590 \u00b5s vs 454 \u00b5s ) in average than BoS ( by disabling a s | w computation ) to compose a 300 - dimensional word embedding vector .", "entities": [[262, 263, "DatasetName", "0"], [389, 390, "DatasetName", "0"], [434, 436, "HyperparameterName", "k ="], [457, 458, "DatasetName", "0"]]}
{"text": "We quantitatively evaluate the quality of subword segmentations and subsequent subword weights by testing if our PBoS model is able to discover the most eminent word affixes . Note this has nothing to do with embeddings , so no training is involved in this experiment . The affix prediction task is to predict the most eminent affix for a given word . For example , \" - able \" for \" replaceable \" and \" re - \" for \" rename \" . Models We get affix prediction from our PBoS by taking the top - ranked subword that is one of the possible affixes . To show our advantage , we Word w Top segmentation g ( and their p g | w ) Top subword s ( and their a s | w ) higher higher ( 0.924 ) , high / er ( 0.030 ) , highe / r ( 0.027 ) , h / igher ( 0.007 ) , hig / her ( 0.004 ) . higher ( 0.852 ) , high ( 0.031 ) , er ( 0.029 ) , r ( 0.029 ) , highe ( 0.025 ) . farmland farmland ( 0.971 ) , farmlan / d ( 0.010 ) , farm / land ( 0.006 ) , f / armland ( 0.005 ) . farmland ( 0.941 ) , d ( 0.010 ) , farmlan ( 0.009 ) , farm ( 0.008 ) , land ( 0.007 ) . penpineapplepie pen / pineapple / pie ( 0.359 ) , pen / pineapple / pi / e ( 0.157 ) , pen / pineapple / p / ie ( 0.101 ) . pineapple ( 0.238 ) , pen ( 0.186 ) , pie ( 0.131 ) , p ( 0.101 ) , e ( 0.099 ) . paradichlorobenzene para / dichlorobenzene ( 0.611 ) , par / a / dichlorobenzene ( 0.110 ) , paradi / chlorobenzene ( 0.083 ) . dichlorobenzene ( 0.344 ) , para ( 0.283 ) , a ( 0.061 ) , par ( 0.054 ) , ichlorobenzene ( 0.042 ) . compare it with a BoS - style baseline affix predictor . Because BoS gives same weight to all subwords in a given word , we randomly choose one of the possible affixes that appear as subword of the word . Benchmark We use the derivational morphology dataset 8 from Lazaridou et al ( 2013 ) . The dataset contains 7449 English words in total along with their most eminent affixes . Because no training is needed in this experiment , we use all the words for evaluation . To make the task more challenging , we drop trivial instances where there is only one possible affix appears as a subword in the given word . For example , \" rename \" is dropped because only prefix \" re - \" is present ; on the other hand , \" replaceable \" is kept because both \" re - \" and \" - able \" are present . Besides excluding the trivial cases described above , we also exclude instances labeled with suffix \" - y \" , because it is always included by \" - ly \" and \" - ity \" . Altogether , we acquire 3546 words with 17 possible affixes for this evaluation . Results Affix prediction results in terms of macro precision , recall , and F1 score are shown in Table 2 . We can see a definite advantage of PBoS at predicting most word affixes , where all the metrics boost about 0.4 and F1 almost doubles compared to BoS , providing evidence that PBoS is able to assign meaningful subword weights .", "entities": [[573, 575, "MetricName", "F1 score"], [603, 604, "MetricName", "F1"]]}
{"text": "Given that PBoS is able to produce sensible segmentation likelihood and subword weights , we now turn our focus onto the quality of the generated 8 http://marcobaroni.org/PublicData/ affix_complete_set.txt.gz word embeddings . In this section , we evaluate the word vectors ' ability to capture word senses using the intrinsic task of word similarity . Word similarity aims to test how well word embeddings capture words ' semantic similarity . The task is given as pairs of words , along with their similarity scores labeled by language speakers . Given a set of word embeddings , we compute the similarity scores induced by the cosine distance between the embedding vectors of each pair of words . The performance is then measured in Spearman 's correlation \u03c1 for all pairs . Benchmarks We use WordSim353 ( WS ) from Finkelstein et al ( 2001 ) which mainly consists of common words . To better access models ' ability to generalize word embeddings towards OOV words , we include the rare word datasets RareWord ( RW ) from Luong et al ( 2013 ) and the newer Card - 660 ( Card ) from Pilehvar et al ( 2018 ) . Model Setup PBoS composes word embeddings out of subword vectors exactly as described in Section 3 . Unlike some of previous models , we do not add special characters to indicate word boundaries and do not set any constraint on subword lengths . PBoS is trained 50 epochs using vanilla SGD with initial learning rate 1 and inverse square root decay . For baselines , we compare against the bag - ofsubword model ( BoS ) from Zhao et al ( 2018 ) , and the best attention - based model ( KVQ - FH ) from Sasaki et al ( 2019 ) . For BoS , we use our implementation by disabling subword weight computation . For KVQ - FH , we use the implementation given in the paper . All the hyperparameters are set the same as described in the original papers . We choose to not include the character - RNN model ( MIMICK ) from Pinter et al ( 2017 ) , as it has been shown clearly outperformed by the two . KVQ - FH , PBoS can often match and sometimes surpass it even though PBoS is a much simpler model with better explainability . Compared to the scores by using just the target embeddings ( Table 3 , All pairs ) , PBoS is the only model that demonstrates improvement across all cases . The only case where PBoS is not doing well is with Polyglot vectors and RW benchmark . After many manual inspections , we conjecture that it may be related to the vector norm . Sometimes the vector of a relevant subword can be of a small norm , prone to be overwhelmed by less relevant subword vectors . To counter this , we tried to normalize subword vectors before summing them up into a word vector ( PBoS - n ) . PBoS - n showed good improvement for the Polyglot RW case ( 25 to 32 ) , matching the performance of the other two . One may argue that PBoS has an advantage for using the most number of parameters . However , this is largely because we do not constrain the length of subwords as in BoS or use hashing as in KVQ - FH . In fact , restricting subword length and using hashing helped them for the word similarity task . We found that PBoS is insensitive to subword length constraints and decide to keep the setting simple . Despite being an interesting direction , we decide to not involve hashing in this work to focus on the effect of our unique weighting scheme . FaxtText Comparison Albeit targeted for a different task ( training word embedding ) which have access to contextual information , the popular fast - Text ) also uses a subwordlevel model . We train fastText 12 over the same English corpus on which the Polyglot target vectors are trained , in order to understand the quantitative impact of contextual information . To ensure a fair comparison , we restrict the vocabulary sizes and embedding dimensions to match those of Polyglot vectors . The word similarity scores we get for the trained fastText model are 65/40/14 for WS / RW / Card . We note the great gain for WS and RW , suggesting the helpfulness of contextual information in learning and generalizing word embeddings in the setting of small to moderate OOV rates . Surprisingly , we find that for the case of extremely high OOV rate ( Card ) , PBoS slightly surpasses fastText , suggesting PBoS ' effectiveness in generalizing embeddings to OOV words even without any help from contexts . Multilingual Results To evaluate and compare the effectiveness of PBoS across languages , we further train the models targeting multilingual Wikipedia2Vec vectors ( Yamada et al , 2020 ) and evaluate them on multilingual WordSim353 and SemLex999 from Leviant and Reichart ( 2015 ) which are available in English , German , Italian and Russian . To better access the models ' ability to generalize , we only take the top 10k words from the target vectors for training , which yields decent OOV rates , ranging from 23 % to 84 % . Detailed results can be found in Appendix Section A.3 . In summary , we find 1 ) that PBoS surpasses KVQ - FH for English and German and is comparable to KVQ - FH for Italian ; 2 ) that PBoS and KVQ - FH surpasses BoS for English , German and Italian ; and 3 ) no definitive trend among the three models for Russian .", "entities": [[28, 30, "TaskName", "word embeddings"], [51, 53, "TaskName", "word similarity"], [54, 56, "TaskName", "Word similarity"], [61, 63, "TaskName", "word embeddings"], [66, 68, "TaskName", "semantic similarity"], [92, 94, "TaskName", "word embeddings"], [158, 160, "TaskName", "word embeddings"], [202, 204, "TaskName", "word embeddings"], [248, 249, "MethodName", "SGD"], [251, 253, "HyperparameterName", "learning rate"], [548, 551, "HyperparameterName", "number of parameters"], [591, 593, "TaskName", "word similarity"], [673, 674, "MethodName", "fastText"], [722, 724, "TaskName", "word similarity"], [730, 731, "MethodName", "fastText"], [761, 763, "TaskName", "word embeddings"], [793, 794, "MethodName", "fastText"]]}
{"text": "We follow the evaluation protocol for sequential labeling used by Kiros et al ( 2015 ) and Li et al ( 2017 ) , and use logistic regression classifier 13 as the model for POS tagging . When predicting the tag for the i - th word w i in a sentence , the input to the classifier is the concatenation of the vectors w i\u22122 , w i\u22121 , w i , w i+1 , w i+2 for the word itself and the words in its context . This setup allows a more direct evaluation of the quality of word vectors themselves , and thus gives better discriminative power . 14 Dataset We train and evaluate the performance of generated word embeddings over 23 languages at the intersection of the Polyglot ( Al - Rfou ' et al , 2013 ) pre - trained embedding vectors 15 and the Universal Dependency ( UD , v1.4 16 ) dataset . Polyglot vectors contain 64 - dimensional vectors over 13 https://scikit - learn.org/0.19/ modules / generated / sklearn.linear_model . LogisticRegression.html 14 As a side note , in our early trials , we tried to evaluate using an LSTM model following Pinter et al ( 2017 ) and Zhao et al ( 2018 ) , but found the numbers rather similar across embedding models . One possible explanation is that LSTMs are so good at picking up contextual features that the impact of mild deviations of a single word vector is marginal . an 100k vocabulary for each language and are used as target vectors for each of the subword - level embedding models in this experiment . For PBoS , we use the Polyglot word counts for each language as the base for subword segmentation and subword weights calculation . UD is used as the POS tagging dataset to train and test the POS tagging model . We use the default partition of training and testing set . Statistics vary from language to language . See Appendix A.4 for more details . Results Table 5 shows the POS tagging accuracy over the 23 languages that appear in both Polyglot and UD . All the subword - level embedding models follow the same hyperparameters as in Section 4.3 . Following Sasaki et al ( 2019 ) , we tune the regularization term of the logistic regression model when evaluating KVQ - FH . Even with that , PBoS is able to achieve the best POS tagging accuracy in all but one language regardless of morphological types , OOV rates , and the number of training instances ( Appendix Table 12 ) . Particularly , PBoS improvement accuracy by greater than 0.1 for 9 languages . For the one language ( Tamil ) where PBoS is not the most accurate , the difference to the best is small ( 0.003 ) . KVQ - FH gives no significantly more accurate predictions than BoS despite it is more complex and is the only one tuned with hyperparameters . Overall , Table 5 shows that the word embeddings composed by our PBoS is effective at predicting POS tags for a wide range of languages .", "entities": [[26, 28, "MethodName", "logistic regression"], [121, 123, "TaskName", "word embeddings"], [153, 154, "DatasetName", "UD"], [196, 197, "MethodName", "LSTM"], [299, 300, "DatasetName", "UD"], [348, 349, "MetricName", "accuracy"], [359, 360, "DatasetName", "UD"], [392, 394, "MethodName", "logistic regression"], [414, 415, "MetricName", "accuracy"], [444, 445, "MetricName", "accuracy"], [511, 513, "TaskName", "word embeddings"]]}
{"text": "Table 7 and Table 8 show the hyperparameter values used in the POS tagging experiment ( Section 4.4 ) . For the prediction model , we use the logistic regression classifier from scikit - learn 0.19.1 with the default settings . Following the observation in Sasaki et al ( 2019 ) , we tune the regularization parameter C for KVQ - FH for all values a \u00d7 10 b where a = 1 , . . . , 9 and b = \u22121 , 0 , . . . , 4 . We use the POS tagging accuracy for English as criterion , and choose C = 70 . Table 12 lists some statistics of the datasets used in the POS tagging experiment . PBoS is able to achieve better accuracy over BoS and KVQ - FH in all languages regardless of their morphological type , OOV rate and number of training instances for POS tagging .", "entities": [[28, 30, "MethodName", "logistic regression"], [84, 85, "DatasetName", "0"], [97, 98, "MetricName", "accuracy"], [130, 131, "MetricName", "accuracy"]]}
{"text": "To encode a semantic component p , we take the sequence of both predicate ids and predicate names into consideration . As the example shown in Figure 3 , the i d sequence of the first semantic component is { contained by } , and the predicate word sequence is the concatenation of canonical names for each predicate , that is { \" contained \" , \" by \" } . Given the word sequence { p ( w ) 1 , . . . , p n } , we first use a word embedding matrix E w R | Vw | \u00d7d to convert the original sequence into word embeddings { p ( w ) 1 , . . . , p ( w ) n } , where | V w | denotes the vocabulary size of natural language words , and d denotes the embedding dimension . Then we represent the word sequence using word averaging : p ( w ) = 1 n i p ( w ) i . For the i d sequence { p ( i d ) 1 , . . . , p ( i d ) m } , we simply take it as a whole unit , and directly translate it into vector representation using the embedding matrix E p R | Vp\u00d7d | at path level , where | V p | is the vocabulary size of predicate sequences . There are two reasons for using such path embedding : 1 ) the length of i d sequence is not larger than two , based on our generation method ; 2 ) the number of distinct predicate sequences is roughly the same as the number of distinct predicates . We get the fi - nal vector of the semantic component by elementwise addition : p = p ( w ) + p ( i d ) .", "entities": [[110, 112, "TaskName", "word embeddings"], [148, 150, "HyperparameterName", "embedding dimension"]]}
{"text": "To predict the best query graph from candidates , we calculate the overall association score S ( q , G ) between the question q and each candidate G , which is the weighted sum of features over entity linking , semantic matching and structural level . Table 1 lists the detail features . During training step , we adopt hinge loss to maximize the margin between positive graphs G + and negative graphs G \u2212 : loss = max { 0 , \u03bb \u2212 S ( q , G + ) + S ( q , G \u2212 ) } . ( 2 ) For each question , we pick a candidate graph as positive data , if the F 1 score of its answer is larger than a threshold ( set to 0.1 in our work ) . We randomly sample 20 negative graphs G \u2212 from the candidate set whose F 1 is lower than the corresponding G + .", "entities": [[38, 40, "TaskName", "entity linking"], [61, 62, "MetricName", "loss"], [77, 78, "MetricName", "loss"], [81, 82, "DatasetName", "0"]]}
{"text": "QA datasets : We conduct our experiments on ComplexQuestions ( Bao et al , 2016 ) , We - bQuestions ( Berant et al , 2013 ) and SimpleQuestions ( Bordes et al , 2015 ) . We use CompQ , WebQ and SimpQ as abbreviations of the above datasets , respectively . CompQ contains 2 , 100 complex questions collected from Bing search query log , and the dataset is split into 1 , 300 training and 800 testing questions . WebQ contains 5 , 810 questions collected from Google Suggest API , and is split into 3 , 778 training and 2 , 032 testing QA pairs . Each question is manually labeled with at least one answer entity in both datasets . SimpQ consists of more than 100 K questions , and the gold answer of each question is a gold focus entity paired with a single predicate . This dataset is designed mainly for answering simple questions , and we use it for complementary evaluation . Knowledge bases : For experiments on both CompQ and WebQ , we follow the settings of Berant et al ( 2013 ) and Xu et al ( 2016 ) to use the full Freebase dump 5 as the knowledge base , which contains 46 M entities and 5 , 323 predicates . We host the knowledge base with Virtuoso engine 6 . For the experiments on SimpQ , the knowledge base we use is FB2 M , which is a subset of Freebase provided with the dataset , consisting 2 M entities and 10 M triple facts . Implementation detail : For all experiments in this section , we initialize word embeddings using GloVe ( Pennington et al , 2014 ) word vectors with dimensions set to 300 , and the size of Bi - GRU hidden layer is also set to 300 . We tune the margin \u03bb in { 0.1 , 0.2 , 0.5 } , the ensemble threshold K in { 1 , 2 , 3 , 5 , 10 , + INF } , and the batch size B in { 16 , 32 , 64 } . All the source codes , QA datasets , and detail results can be downloaded from http://202.120.38.146/CompQA/.", "entities": [[28, 29, "DatasetName", "SimpleQuestions"], [90, 91, "DatasetName", "Google"], [280, 282, "TaskName", "word embeddings"], [283, 284, "MethodName", "GloVe"], [305, 306, "MethodName", "GRU"], [350, 352, "HyperparameterName", "batch size"]]}
{"text": "Now we perform KBQA experiments on WebQ and CompQ. We use the average F 1 score over all questions as our evaluation metric . The official evaluation script 7 measures the correctness of output entities at string level . While in CompQ , the annotated names of gold answer entities do n't match the case of their names in Freebase , thus we follow Bao et al ( 2016 ) to lowercase both annotated names and the output answer names before calculating the F 1 score . We set \u03bb = 0.5 , B = 32 , K = 3 for WebQ and K = 5 for CompQ , as reaching the highest average F 1 on the validation set of each dataset . We report the experimental results in Table 2 . The result of Yih et al ( 2015 ) on CompQ is reported by Bao et al ( 2016 ) as their implemented result . Our approach outperforms existing approaches on CompQ dataset , and ranks 2nd on WebQ among a long list of state - of - the - art works . Jain ( 2016 ) achieves highest F 1 score on WebQ using memory networks , which is not semantic parsing based , and thus less interpretable . We point out that Xu et al ( 2016 ) uses Wikipedia texts as the external community knowledge for verifying candidate answers , and achieves a slightly higher F 1 score ( 53.3 ) than our model , but the performance decreases to 47.0 if this step is removed . Besides , Yih et al ( 2015 ) and Bao et al ( 2016 ) used ClueWeb dataset for learning more accurate semantics , while based on the ablation test of Yih , the F 1 score of WebQ drops by 0.9 if ClueWeb information is removed . Our results show that entity enrichment method improves the results on both datasets by a large margin ( 0.8 ) , which is a good help to our approach . We argue that the enriched results are directly comparable with other approaches , as S - MART itself is learned from semi - structured information in Wikipedia , such as anchor texts , redirect links and disambiguation pages , the enrichment step does not bring extra knowledge into our system . In addition , the improvements of the candidate generation step also show a positive effect . If we remove our implicit type filtering in Step 4 and time interval constraints in Step 5 , the F 1 of CompQ slightly drops from 42.84 to 42.37 . Al - though these improvements mainly concern timerelated questions ( around 25 % in CompQ ) , we believe these strategies can be useful tricks in the further researches . As a complementary evaluation , we perform semantic matching experiments on SimpQ. Given the gold entity of each question , we recognize the entity mention in the question , replace it with E , then predict the correct predicate . Table 3 shows the experimental results . The best result is from Qu et al ( 2018 ) , which learns the semantic similarity through both attentive RNN and similarity matrix based CNN . Yu et al ( 2017 ) proposed another approach using multi - layer BiL - STM with residual connections . Our semantic matching model performs slightly below these two systems , since answering simple questions is not the main goal of this paper . Comparing with these approaches , our semantic matching model is lightweighted , with a simpler structure and fewer parameters , thus is easier to tune and remains effective .", "entities": [[97, 99, "HyperparameterName", "K ="], [103, 105, "HyperparameterName", "K ="], [204, 206, "TaskName", "semantic parsing"], [530, 532, "TaskName", "semantic similarity"]]}
{"text": "For our work in our lab ( Orange - Deski\u00f1 ) we needed a robust dependency analysis for written French with the highest Labeled Attachment Score ( LAS ) 1 possible , using a wide range of dependency relations . Having worked in the past on rule based dependency analysis , it became obvious that we need to adopt a more modern approach to dependency analysis . Thus during the last year we tried several freely available open source tools available ( e.g. MaltParser 2 , Google 's SyntaxNet 3 , Standford Dependency Tools 4 , Bist - 1 Since we are interested in semantic relations a good CLAS score ( Nivre and Fang , 2017 ) is even more relevant . 2 http://www.maltparser.org/ 3 https://www.tensorflow.org/versions/ r0.11 / tutorials / syntaxnet/ 4 https://nlp.stanford.edu/software/ stanford - dependencies.shtml Parser 5 and HTParser 6 ) , trained on different Treebanks ( notably French Sequoia ( Candito et al , 2014 ) and Universal Dependencies ( McDonald et al , 2013 ) ) . All combinations of tools and treebanks had some advantages and some inconveniences . For instance , the underlying linguistic models of the treebanks are not the same or some tools would not accept CONLLU input but only raw text and apply their own segmentation and POS tagging . In a next step we enriched the French treebanks with additional information like lemmas , morphological features and more fine - graded XPOS in addition to the about 20 UPOS categories of the treebanks ( UD - French v1.2 does not contain neither lemmas nor morphological features ) and conducted a new training / test / evaluation cycle . Since the initial results for French were encouraging we tried the same approaches with other languages , such as the languages proposed for CoNLL 2017 UD Shared Task ( Zeman et al , 2017 ) . However , for participation at the shared task , we relied exclusively on the data provided by Universal Dependencies ( Nivre et al , 2016 ( Nivre et al , , 2017b , also for French in spite of our previous work . For the shared task we have trained models separately for each language . So strictly speaking , this is not a multilingual but a monolingual multimodel approach .", "entities": [[25, 26, "MetricName", "Score"], [86, 87, "DatasetName", "Google"], [159, 161, "DatasetName", "Universal Dependencies"], [253, 254, "DatasetName", "UD"], [302, 303, "DatasetName", "UD"], [330, 332, "DatasetName", "Universal Dependencies"]]}
{"text": "The biggest challenge were the 4 surprise languages . Having only between 20 and 109 sentences to train on ( even less if we wanted to split it into a train and development corpus ) did not help ( see table 2 for some details ) . Since the word embedding files where also rather small we chose not to train on the languages themselves , but to keep all of the provided sentences for the development corpus . So we first tried three similar approaches in order to be able to predict dependency relations for these languages : In all three cases we replaced the forms of all closed word classes ( i.e. all but nouns , adjectives and verbs ) with the corresponding UPOS in the training and in the test corpus ( for the CoNLL 2017 UD Shared Task we inserted the original forms again after predicting the dependency relations . The \" mix \" is then trained with a hidden layer size of either 100 or 50 , but without word embeddings . We initially tested these models using the test corpus for the Tamil treebank ( UD v2.0 ) . Using the \" mix \" with 23 languages ( 3 ) resulted in the best weighted LAS , 35.2 % ( 35.3 % if using a hidden layer size of 50 ) . The weighted LAS for the surprise languages is shown in table 3 Upper Sorbian is a slavonic language very close to Czech ( and slightly less close to Polish ) . Northern Sami shares quite a lot of typological features with the Finnic branch of the Fenno - Ugric languages ( here Finnish and Estonian ) , and Kurmanji shares at least some typolological feature with Persian ( both are from the Iranian subgroup of the Indo - European language family . However Buryat , a Mongolian language , is not typologically close to any of the shared task 's languages . Even though Turkish seems close enough , to our surprise Hindi was finally the best guess . With Urdu , which is very similar to Hindi apart from the fact that it uses the Arabic alphabet instead of Devanagari , the LAS was less good . As for the language mix , we replaced the forms of the closed word classes in the training corpora by the corresponding UPOS ( except nouns , verbs and adjectives ) and trained the modified treebanks ( cf . tables 4 and 5 , best configuration in bold ) .", "entities": [[139, 140, "DatasetName", "UD"], [163, 166, "HyperparameterName", "hidden layer size"], [174, 176, "TaskName", "word embeddings"], [191, 192, "DatasetName", "UD"], [221, 224, "HyperparameterName", "hidden layer size"], [348, 349, "DatasetName", "Urdu"]]}
{"text": "Our final macro - averaged LAS F1 score on the CoNLL 2017 UD Shared Task test data ( Nivre et al , 2017a ) was 68.61 % , ( 10th out of 33 ) 17 . The details show that our approach worked well for the bigger treebanks and the surprise languages ( where we ended up as 8th ) . In general , the results per language are slightly lower than those we had during training on the development corpora ( cf . table 1 ) . This is due to the fact we did our training on forms , lemmas , UPOS and XPOS of the training corpus , which are gold . In the test data , lemmas , UPOS and XPOS ( if present ) , however , are predicted by UDpipe , and do contain some errors with respect to the gold standard . After the end of the test phase , we discovered a bug in our chain , which concerned languages , which have only UPOS data . In this case the UPOS information was totally discarded by error . Thus all training and testing are done only on the 17 http://universaldependencies.org/ conll17 / results.html forms 18 . Further we made en error uploading the models for the gl TreeGal , fr parTut and sl sst treebanks . During the tests the models trained on the basic gl , fr and sl treebanks were used instead . After the test phase we corrected these errors . Fortunately , their impact was not that hard . Apart from the result for gl TreeGal and sl sst , which went up to 66.13 % ( from 22.46 % ) and to 47.68 ( from 40.25 ) respectively once the correct model was used , the results for the other corpora changed only slightly , the global results could have been 69.38 % . All results are shown in table 6 . The column on the right shows the difference between the results of the development corpora and test data . For some languages , the test results are unexpectedly lower than the results on the development corpora . For gl TreeGal , fr parTut and sl sst , this is due to errors when installing our system on the Tira - platform . The lower performance on languages like Chinese , Ukrainian , Vietnamese or Latin ( both ITTB and PROIEL ) seems to be caused by the nature of the test corpora themselves . Systems of other participants seem to drop in performance as well ; for all these languages our system is still around the 10th position of the global ranking . Perhaps a cause may be the fact that the XPOS we use ( predicted by UDpipe ) contain more errors than average for the Chinese , Ukrainian or Vietnamese treebanks than for languages where our test score is closer to the development score .", "entities": [[6, 8, "MetricName", "F1 score"], [12, 13, "DatasetName", "UD"], [222, 223, "DatasetName", "sst"], [271, 272, "DatasetName", "sst"], [371, 372, "DatasetName", "sst"]]}
{"text": "The MultiWOZ 2.0 dataset has greatly boosted the research on dialogue state tracking ( DST ) . However , substantial noise has been discovered in its state annotations . Such noise brings about huge challenges for training DST models robustly . Although several refined versions , including MultiWOZ 2.1 - 2.4 , have been published recently , there are still lots of noisy labels , especially in the training set . Besides , it is costly to rectify all the problematic annotations . In this paper , instead of improving the annotation quality further , we propose a general framework , named ASSIST ( lAbel noiSe - robuSt dIalogue State Tracking ) , to train DST models robustly from noisy labels . ASSIST first generates pseudo labels for each sample in the training set by using an auxiliary model trained on a small clean dataset , then puts the generated pseudo labels and vanilla noisy labels together to train the primary model . We show the validity of ASSIST theoretically . Experimental results also demonstrate that AS - SIST improves the joint goal accuracy of DST by up to 28.16 % on MultiWOZ 2.0 and 8.41 % on MultiWOZ 2.4 , compared to using only the vanilla noisy labels .", "entities": [[1, 3, "DatasetName", "MultiWOZ 2.0"], [10, 13, "TaskName", "dialogue state tracking"], [47, 49, "DatasetName", "MultiWOZ 2.1"], [108, 111, "TaskName", "dIalogue State Tracking"], [183, 184, "MetricName", "accuracy"], [192, 194, "DatasetName", "MultiWOZ 2.0"], [198, 200, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "Conventionally , all the state labels are assumed to be correct . However , this assumption may not hold . In practice , dialogue state annotations are errorprone ( Han et al , 2020b ) . There are a couple of reasons . First , the states are usually annotated by crowdworkers to improve the labelling efficiency . Due to limited knowledge , crowdworkers can not annotate all the states with 100 % accuracy , which naturally incurs noisy labels ( Han et al , 2020a ) . Second , the dialogue may span multiple domains , which also increases the labelling difficulty . Apparently , the noisy labels are harmful and likely to lead to sub - optimal performance . Therefore , it is crucial to take them into consideration so as to train DST models more robustly . LetB t = { ( s , \u1e7d t ) | s S } denote the noisy state annotations , where\u1e7d t is the noisy label of slot s at turn t. We use B t = { ( s , v t ) | s S } to denote the noise - free state annotations . Here , v t represents the true label of slot s at turn t , which is unknown . In fact , existing DST approaches are only able to learn a sub - optimal dialogue state trackerF : X t B t rather than the optimal state tracker F : X t B t , as none of them have considered the influence of noisy labels . In this work , we aim to learn a robust state tracker F * that can better approximate F from the noisy state annotationsB t .", "entities": [[73, 74, "MetricName", "accuracy"]]}
{"text": "To reduce the influence of noisy labels , we combine the generated pseudo labels and vanilla noisy labels to train the primary model . Letv t and\u1e7d t be the one - hot representation of the pseudo labelv t and vanilla noisy label\u1e7d t , respectively . Then , we can define the combined label as : v c t = \u03b1v t + ( 1 \u2212 \u03b1 ) \u1e7d t , where \u03b1 ( 0 \u2264 \u03b1 \u2264 1 ) L pri = ( s , v c t ) C ( Bt , Bt ) \u2212 log p ( v c t | X t , s ) = \u03b1 ( s , vt ) Bt \u2212 log p ( v t | X t , s ) + ( 1 \u2212 \u03b1 ) ( s , \u1e7dt ) Bt \u2212 log p ( \u1e7d t | X t , s ) = \u03b1L pseudo + ( 1 \u2212 \u03b1 ) L vanilla , where L pseudo and L vanilla correspond to the training objective of using only the pseudo labels and using only the vanilla noisy labels , respectively . By minimizing L pri , the primary model is trained to learn from the vanilla noisy labels and at the same time imitate the predictions of the auxiliary model .", "entities": [[67, 68, "HyperparameterName", "\u03b1"], [73, 74, "HyperparameterName", "\u03b1"], [75, 76, "DatasetName", "0"], [77, 78, "HyperparameterName", "\u03b1"], [111, 112, "HyperparameterName", "\u03b1"], [134, 135, "HyperparameterName", "\u03b1"], [161, 162, "HyperparameterName", "\u03b1"]]}
{"text": "Since the pseudo labels are generated by the auxiliary model that has been trained on a small clean dataset , it can be expected that the combined labels are able to serve as a better approximation to the unknown true labels . Let v t denote the one - hot representation of the unknown true value v t of slot s at turn t. We adopt the mean squared loss to define the approximation error of any corrupted labelsv t associated with the noisy training set D n as : Yv = 1 | D n | | S | Xt Dn s S E Dc [ v t \u2212 v t 2 2 ] , where the expectation ranges over different choices of the clean dataset D c , and | | returns the cardinality of a set . Next , we show that the approximation error of the combined labels can be smaller than that of both the vanilla noisy labels and the generated pseudo labels . The details are presented in Theorem 1 . Theorem 1 . The optimal approximation error with respect to the combined labels v c t is smaller than that of the vanilla labels\u1e7d t and pseudo labelsv t , i.e. , min \u03b1 Y v c < min { Y\u1e7d , Yv } . By setting \u03b1 = Y\u1e7d Y\u1e7d+Yv , Y v c reaches its minimum : min \u03b1 Y v c = Y\u1e7dYv Y\u1e7d + Yv . Proof . The proof is presented in Appendix A. Theorem 1 indicates that if \u03b1 is set properly , the combined labels can approximate the unknown true labels more accurately . Hence , we can potentially train the primary model more robustly . Note that we can not calculate the optimal value of \u03b1 directly .", "entities": [[69, 70, "MetricName", "loss"], [210, 211, "HyperparameterName", "\u03b1"], [224, 225, "HyperparameterName", "\u03b1"], [237, 238, "HyperparameterName", "\u03b1"], [261, 262, "HyperparameterName", "\u03b1"], [300, 301, "HyperparameterName", "\u03b1"]]}
{"text": "We exploit joint goal accuracy and slot accuracy as the evaluation metrics . The joint goal accuracy is 2 Despite this change , we still call the dataset MultiWOZ 2.0 in this paper for ease of exposition . defined as the proportion of dialogue turns in which the values of all slots are correctly predicted . It is the most important metric in the DST task . The slot accuracy is defined as the average of all individual slot accuracies . The accuracy of an individual slot is calculated as the ratio of dialogue turns in which its value is correctly predicted . We also propose a new evaluation metric , termed as joint turn accuracy . We define joint turn accuracy as the proportion of dialogue turns in which the values of all active slots are correctly predicted . A slot becomes active if its value is mentioned in current turn and is not inherited from previous turns . The advantage of joint turn accuracy is that it can tell us in how many turns the turn - level information is fully captured by the model .", "entities": [[4, 5, "MetricName", "accuracy"], [7, 8, "MetricName", "accuracy"], [16, 17, "MetricName", "accuracy"], [28, 30, "DatasetName", "MultiWOZ 2.0"], [69, 70, "MetricName", "accuracy"], [82, 83, "MetricName", "accuracy"], [115, 116, "MetricName", "accuracy"], [121, 122, "MetricName", "accuracy"], [165, 166, "MetricName", "accuracy"]]}
{"text": "For the auxiliary model , the pre - trained BERT - baseuncased model is utilized as the dialogue context encoder . Another pre - trained BERT - base - uncased model with fixed weights is employed to encode the slots and their candidate values . The maximum input length of the BERT model is set to 512 . The number of heads in the slot attention module is set to 4 . The output dimension of the linear transformation layer is set to 768 , which is the same as the dimension of the BERT outputs . Recall that the previous turn dialogue state is treated as part of the input . The ground - truth one is used during training , and the predicted one is used during testing 3 . We train the auxiliary model on the clean validation set and the primary model on the noisy training set . When training the auxiliary model , the noisy training set is leveraged to choose the best model . For all primary models , the parameter \u03b1 is set to 0.6 on MutliWOZ 2.0 and 0.4 on MultiWOZ 2.4 . More training details can be found in Appendix B.", "entities": [[9, 10, "MethodName", "BERT"], [25, 26, "MethodName", "BERT"], [51, 52, "MethodName", "BERT"], [64, 66, "MethodName", "slot attention"], [94, 95, "MethodName", "BERT"], [97, 98, "MetricName", "Recall"], [177, 178, "HyperparameterName", "\u03b1"], [188, 190, "DatasetName", "MultiWOZ 2.4"]]}
{"text": "Table 1 presents the performance scores of the three different primary DST models on the test sets of MultiWOZ 2.0 & 2.4 when they are trained using our proposed framework ASSIST . For comparison , we also include the results when only the vanilla labels or only the pseudo labels are used to train the primary models . As can be seen , ASSIST consistently improves the performance of the three primary models on both datasets . More concretely , compared to the results obtained using only the vanilla labels , AS - SIST improves the joint goal accuracy of SOM - DST , STAR , and AUX - DST on MultiWOZ 2.0 by 25.69 % , 25.82 % , and 28.16 % absolute gains , respectively . On MultiWOZ 2.4 , ASSIST also leads to 8.41 % , 5.79 % , and 7.77 % absolute joint goal accuracy gains . From Table 1 , we further observe that the performance improvements on MultiWOZ 2.4 are lower than on MultiWOZ 2.0 . This is because the training set of MultiWOZ 2.4 is the same as that of MultiWOZ 2.1 ( Eric et al , 2020 ) , in which lots of annotation errors have been fixed . We also observe that all the primary models demonstrate relatively good performance when only the pseudo labels are used . From these results , it can be con - cluded that the pseudo labels are beneficial and they can help us train DST models more robustly . Another observation from Table 1 is that SOM - DST tends to show comparable or even higher joint turn accuracy compared to STAR and AUX - DST , although its performance is worse in terms of joint goal accuracy and slot accuracy . This is because SOM - DST focuses on turn - active slots and copies the values for other slots from previous turns , while both STAR and AUX - DST predict the values of all slots from scratch at each turn . These results show that the joint turn accuracy can help us understand in more depth how different models behave .", "entities": [[18, 20, "DatasetName", "MultiWOZ 2.0"], [98, 99, "MetricName", "accuracy"], [100, 101, "MethodName", "SOM"], [104, 105, "DatasetName", "STAR"], [111, 113, "DatasetName", "MultiWOZ 2.0"], [129, 131, "DatasetName", "MultiWOZ 2.4"], [148, 149, "MetricName", "accuracy"], [163, 165, "DatasetName", "MultiWOZ 2.4"], [169, 171, "DatasetName", "MultiWOZ 2.0"], [179, 181, "DatasetName", "MultiWOZ 2.4"], [187, 189, "DatasetName", "MultiWOZ 2.1"], [261, 262, "MethodName", "SOM"], [273, 274, "MetricName", "accuracy"], [276, 277, "DatasetName", "STAR"], [292, 293, "MetricName", "accuracy"], [295, 296, "MetricName", "accuracy"], [300, 301, "MethodName", "SOM"], [322, 323, "DatasetName", "STAR"], [346, 347, "MetricName", "accuracy"]]}
{"text": "The parameter \u03b1 adjusts the weights of the pseudo labels and vanilla labels in the training phase . Here , we study the effects of \u03b1 by varying its value in the range of 0 to 1 with a step size of 0.1 . Figure 4 shows the results of AUX - DST . As can be seen , \u03b1 plays an important role in balancing the pseudo labels and vanilla labels . The best performance is achieved when \u03b1 is set to 0.6 on MultiWOZ 2.0 and 0.4 on MultiWOZ 2.4 . Since the training set of MultiWOZ 2.0 has more noisy labels than that of MultiWOZ 2.4 , more emphasis should be put on its pseudo labels to obtain the best performance . It is also noted that the performance difference between MultiWOZ 2.0 and MultiWOZ 2.4 dwindles away as \u03b1 increases . This is because the vanilla labels will contribute less to the training of the primary model when \u03b1 is set to be larger .", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [25, 26, "HyperparameterName", "\u03b1"], [34, 35, "DatasetName", "0"], [39, 41, "HyperparameterName", "step size"], [59, 60, "HyperparameterName", "\u03b1"], [79, 80, "HyperparameterName", "\u03b1"], [85, 87, "DatasetName", "MultiWOZ 2.0"], [90, 92, "DatasetName", "MultiWOZ 2.4"], [98, 100, "DatasetName", "MultiWOZ 2.0"], [107, 109, "DatasetName", "MultiWOZ 2.4"], [134, 136, "DatasetName", "MultiWOZ 2.0"], [137, 139, "DatasetName", "MultiWOZ 2.4"], [142, 143, "HyperparameterName", "\u03b1"], [162, 163, "HyperparameterName", "\u03b1"]]}
{"text": "We first investigate whether the pseudo labels are consistent with the true labels . To achieve this goal , we can compute the joint goal accuracy and joint turn accuracy of the auxiliary model on the training set . However , the true labels of the training set are unavailable . As an alternative , we treat the vanilla noisy labels as true labels ( note that only a portion of the vanilla labels are noisy ) . In this experiment , we also vary the number of clean dialogues to train the auxiliary model . Figure 6 presents the results . As shown in Figure 6 , the auxiliary model achieves higher performance when more clean dialogues are utilized to train it . If the entire validation set is used , it achieves around 50 % joint goal accuracy and around 75 % joint turn accuracy . Given that the vanilla noisy labels are regarded as the true labels , we can conjecture that the true performance is actually higher . This experiment shows that the pseudo labels are consistent with the unknown true labels to some extent and can serve as a good complement to the vanilla noisy labels .", "entities": [[25, 26, "MetricName", "accuracy"], [29, 30, "MetricName", "accuracy"], [139, 140, "MetricName", "accuracy"], [146, 147, "MetricName", "accuracy"]]}
{"text": "We further investigate the error rate with respect to each slot . We adopt AUX - DST as the primary model and use AUX - DST ( w/o p ) to denote the case when only the vanilla labels are employed to train the model . The results on the test set of MultiWOZ 2.4 are illustrated in Figure 7 , from which we can observe that the slot \" hotel - type \" has the highest error rate . Even though the error rate is reduced with the aid of the pseudo labels , it is still the highest one among all the slots . This is because the labels of this slot are confusing . It is also observed that the \" name \" - related slots have relatively high error rates . However , when the pseudo labels are used , their error rates reduce remarkably . Besides , we observe that the error rates of some slots are higher when the pseudo labels are leveraged . This is probably due to the fact that we have used the same parameter \u03b1 to combine the pseudo labels and vanilla labels of all slots . In practice , the noise rate with respect to each slot in the vanilla labels may not be exactly the same . This observation in - spires us that more advanced techniques should be developed to combine the pseudo labels and vanilla labels , which we leave as our future work .", "entities": [[53, 55, "DatasetName", "MultiWOZ 2.4"], [184, 185, "HyperparameterName", "\u03b1"]]}
{"text": "In this work , we have presented a general framework ASSIST , aiming to train DST models robustly from noisy labels . ASSIST leverages an auxiliary model that is trained on a small clean dataset to generate pseudo labels for the large noisy training set . The pseudo labels are combined with the vanilla labels to train the primary model . Both theoretical analysis and empirical study have verified the validity of our proposed framework . In the future , we intend to explore more advanced techniques to combine the pseudo labels and vanilla noisy labels in a better way . Considering that the pseudo labels are generated by the auxiliary model that is trained on an extra small clean dataset and this clean dataset is independent of the noisy training set , we can regard the pseudo labels and vanilla labels as independent of each other . Consequently , we obtain : E Dc [ ( \u1e7d t \u2212 v t ) T ( v t \u2212 v t ) ] = [ E Dc [ \u1e7d t \u2212 v t ] ] T E Dc [ v t \u2212 v t ] = [ E Dc [ \u1e7d t \u2212 v t ] ] T E Dc [ v t \u2212 E Dc [ v t ] ] = [ E Dc [ \u1e7d t \u2212 v t ] ] T 0 = 0 . Based on the formula above , we can now calculate the approximation error with respect to the combined label v c t of slot s as below : E Dc [ v c t \u2212 v t 2 2 ] = E Dc [ \u03b1v t + ( 1 \u2212 \u03b1 ) \u1e7d t \u2212 v t 2 2 ] = E Dc [ \u03b1 ( v t \u2212 v t ) + ( 1 \u2212 \u03b1 ) ( \u1e7d t \u2212 v t ) 2 2 ] = \u03b1 2 E Dc [ v t \u2212 v t 2 2 ] + ( 1 \u2212 \u03b1 ) 2 E Dc [ \u1e7d t \u2212 v t 2 2 ] , where the last equality holds because of E Dc [ ( \u1e7d t \u2212 v t ) T ( v t \u2212 v t ) ] = 0 . Then , we have : Y v c = 1 | D n | | S | Xt Dn s S E Dc [ v c t \u2212 v t 2 2 ] = \u03b1 2 | D n | | S | Xt Dn s S E Dc [ v t \u2212 v t 2 2 ] + ( 1 \u2212 \u03b1 ) 2 | D n | | S | Xt Dn s S E Dc [ \u1e7d t \u2212 v t 2 2 ] = \u03b1 2 Yv + ( 1 \u2212 \u03b1 ) 2 Y\u1e7d . Y v c reaches its minimum when \u03b1 = Y\u1e7d Y\u1e7d+Yv , and min \u03b1 Y v c = Y\u1e7dYv Y\u1e7d + Yv , which concludes the proof .", "entities": [[233, 234, "DatasetName", "0"], [235, 236, "DatasetName", "0"], [287, 288, "HyperparameterName", "\u03b1"], [301, 302, "HyperparameterName", "\u03b1"], [313, 314, "HyperparameterName", "\u03b1"], [326, 327, "HyperparameterName", "\u03b1"], [343, 344, "HyperparameterName", "\u03b1"], [385, 386, "DatasetName", "0"], [421, 422, "HyperparameterName", "\u03b1"], [449, 450, "HyperparameterName", "\u03b1"], [475, 476, "HyperparameterName", "\u03b1"], [482, 483, "HyperparameterName", "\u03b1"], [494, 495, "HyperparameterName", "\u03b1"], [501, 502, "HyperparameterName", "\u03b1"]]}
{"text": "Note that the proposed auxiliary model is also applied as one primary model in our experiments . In both cases , AdamW ( Kingma and Ba , 2014 ) is adopted as the optimizer , and a linear schedule with warmup is created to adjust the learning rate dynamically . The peak learning rate is set to 2.5e - 5 . The warmup proportion is fixed at 0.1 . The dropout ( Srivastava et al , 2014 ) probability and word dropout ( Bowman et al , 2016 ) probability are also fixed at 0.1 . When taken as the auxiliary model , the model is trained for at most 30 epochs with a batch size of 8 . When taken as the primary model , the batch size and training epochs are set to 8 and 12 , respectively . The best model is chosen according to the performance on the validation set . We apply left truncation when the input exceeds the maximum input length of BERT . For SOM - DST and STAR , the default hyperparameters are adopted when they are applied as the primary model ( except setting num_workers = 0 ) .", "entities": [[21, 22, "MethodName", "AdamW"], [33, 34, "HyperparameterName", "optimizer"], [46, 48, "HyperparameterName", "learning rate"], [52, 54, "HyperparameterName", "learning rate"], [114, 116, "HyperparameterName", "batch size"], [127, 129, "HyperparameterName", "batch size"], [168, 169, "MethodName", "BERT"], [171, 172, "MethodName", "SOM"], [175, 176, "DatasetName", "STAR"], [195, 196, "DatasetName", "0"]]}
{"text": "We use the large Transformer from Vaswani et al ( 2017 ) with 8 encoder and decoder layers and replicate all the parameters from . The number of parameters in the model are approximately 248 Million and it takes \u223c26 hours on 4 Nvidia V100 ( 32 GB ) GPUs . NMT ( mBART ) For this , we use 12 Transformer encoder and decoder layers , with total number of model parameters \u223c611 Million . We use the pretrained mBART for initializing the model weights . We follow the recommendations of Liu et al ( 2020 ) for the hyperparameter settings . We stop the training after 25 K gradient updates for the model . These updates take \u223c35 hours on 4 Nvidia V100 ( 32 GB ) GPUs .", "entities": [[4, 5, "MethodName", "Transformer"], [26, 29, "HyperparameterName", "number of parameters"], [53, 54, "MethodName", "mBART"], [61, 62, "MethodName", "Transformer"], [80, 81, "MethodName", "mBART"]]}
{"text": "We use case - insensitive BLEU scores ( Papineni et al , 2002 ) calculated using sacreBLEU 2 ( Post , 2018 ) . These scores are calculated on the validation set to decide our primary and contrastive submissions . For evaluating performance on the test set , the organizers use BLEU , TER ( Snover et al , 2006 ) , and RIBES ( Isozaki et al , 2010 ) .", "entities": [[5, 6, "MetricName", "BLEU"], [16, 17, "MetricName", "sacreBLEU"], [51, 52, "MetricName", "BLEU"]]}
{"text": "Results Table 3 shows our results on the test set for our primary and contrastive submissions . We observed the performance of our three model settings on the validation set , and we selected the mBART model as our primary submission and SMT model as the contrastive submission for hi \u2194 mr . Similarly , the mBART model forms our primary submission for es \u2194 pt . Table 4 lists our final results on this shared task . We also list the BLEU scores for the submission that got first rank in each of the language directions . Since the test sets were hidden at the time of submission , we do not report our numbers on the standard Transformer architecture . Analysis Even though Marathi and Portuguese are not present during the pre - training phase of mBART , fine - tuning on these languages provides significant boosts over SMT and standard Transformer . This shows that some level of language independent multilingual embeddings are present in the pre - trained model weights which can be exploited for the transfer task .", "entities": [[35, 36, "MethodName", "mBART"], [56, 57, "MethodName", "mBART"], [82, 83, "MetricName", "BLEU"], [119, 120, "MethodName", "Transformer"], [138, 139, "MethodName", "mBART"], [153, 154, "MethodName", "Transformer"]]}
{"text": "We present a brief overview of the models which we considered for our analysis in this section . Bi - Directional Attention Flow ( BiDAF ) : This model , proposed by Seo et al ( 2016 ) , is a hierarchical multi - stage end - to - end neural network which takes inputs of different granularity ( character , word and phrase ) to obtain a query - aware context representation using memory - less contextto - query ( C2Q ) and query - to - context ( Q2C ) attention . This representation can then be used for different final tasks . Many versions of this model ( with different types of input features ) exist on the SQuAD leaderboard , but the basic architecture 2 ( which we use for our experiments in this paper ) contains character , word and phrase embedding layers , followed by an attention flow layer , a modeling layer and an output layer . Gated Self - Matching Networks ( R - Net ) : This model , proposed by Wang et al ( 2017 ) , is a multilayer end - to - end neural network whose novelty lies in the use of a gated attention mechanism so as to give different levels of importance to different passage parts . It also uses self - matching attention for the context to aggregate evidence from the entire passage to refine the query - aware context representation obtained . The architecture contains character and word embedding layers , followed by question - passage encoding and matching layers , a passage self - matching layer and an output layer . The implementation we used 3 had some minor changes for increased efficiency . Chen et al ( 2017 ) , focuses on answering open - domain factoid questions using Wikipedia , but also performs well on SQuAD ( skipping the document retrieval stage ) . Its implementation 4 has paragraph and question encoding layers , and an output layer . The paragraph encoding is computed by representing each context as a sequence of feature vectors derived from tokens : word embedding , exact match with question word , POS / NER / TF and aligned question embedding , and passing these as inputs to a recurrent neural network . The question encoding is obtained by using word embeddings as inputs to a recurrent neural network . Multi - Paragraph Reading Comprehension ( DocQA ) : This model , proposed by Clark and Gardner ( 2017 ) , aims to answer questions based on entire documents ( multiple paras ) rather than specific paragraphs , but also gives good results for SQuAD ( considering the given paragraph as the document ) . The implementation 5 contains input , embedding ( character and word - level ) , pre - processing ( shared bidirectional GRU between question and passage ) , attention ( similar to BiDAF ) , self - attention ( residual ) and output ( bidirectional GRU and linear scoring ) layers .", "entities": [[121, 122, "DatasetName", "SQuAD"], [314, 315, "DatasetName", "SQuAD"], [360, 362, "MetricName", "exact match"], [368, 369, "TaskName", "NER"], [394, 396, "TaskName", "word embeddings"], [407, 409, "TaskName", "Reading Comprehension"], [448, 449, "DatasetName", "SQuAD"], [479, 481, "MethodName", "bidirectional GRU"], [503, 505, "MethodName", "bidirectional GRU"]]}
{"text": "The span - level performance is measured typically by Exact Match ( EM ) and F1 metrics which are reported with respect to the ground truth answer spans . These results are summarized in Table 1 . The DocQA model gives the best overall performance which aligns well with our expectation , owing to the usage of and improvements in the prior mechanisms introduced in BiDAF and R - Net .", "entities": [[9, 11, "MetricName", "Exact Match"], [12, 13, "MetricName", "EM"], [15, 16, "MetricName", "F1"]]}
{"text": "To investigate trends at different granularities , we also measure sentence retrieval performance . The context given for each question - answer pair is split into sentences using the NLTK sentence tokenizer 7 , and the sentence - level accuracy of each of the models is computed ( Table 1 ) . Since the default sentence tokenizer for English in NLTK is pre - trained on Penn Treebank data which contains formal language ( news articles ) , we expect it to perform reasonably well on Wikipedia articles too . We observe that all the models have high sentencelevel accuracy , with DocQA outperforming the other models with respect to this metric as well . Interestingly , DrQA performs better on sentence retrieval accuracy than both BiDAF and R - Net , but has a worse span - level exact match score , which is probably because of the rich feature vector representation of the passage due to the model 's focus on open domain QA ( and hence retrieval ) . But , none of these neural models have near - perfect ability to identify the correct sentence , and \u223c90 % accuracy indicates that even if we have a perfect answer selection method , this is the best EM score we can achieve . However , incorrect span identification contributes more to errors in prediction for all the models , as seen from the disparity between the sentence - level accuracies and the final spanlevel exact match score values .", "entities": [[39, 40, "MetricName", "accuracy"], [66, 68, "DatasetName", "Penn Treebank"], [99, 100, "MetricName", "accuracy"], [123, 124, "MetricName", "accuracy"], [139, 141, "MetricName", "exact match"], [193, 194, "MetricName", "accuracy"], [202, 204, "TaskName", "answer selection"], [210, 211, "MetricName", "EM"], [247, 249, "MetricName", "exact match"]]}
{"text": "In Table 2 , we analyze the number of erroneous predictions which overlap for different pairs of models , i.e. , which belong to the intersection of the sets of incorrect answers generated by models in each ( row , column ) pair . Thus , the values in the table represent a symmetric matrix with diagonal elements indicating the number of errors which each model commits . This analysis can be useful while determining suitable models for creating meta ensembles since a low incorrect answer overlap indicates that the combined predictive power One way in which this analysis can help in exploring ensemble - based methods is that instead of trying all possible combinations of models , we can adopt a greedy approach based on the incorrect answer overlap metric to decide which model to add to the ensemble ( and only if it leads to a statistically significant difference in this overlap ) . After determining an approximately optimal set of models which such an ensemble should be composed of , each of these models can be trained independently followed by multi - label classification ( to select one of the generated answers ) using techniques like logistic regression , a feed - forward neural network or a recurrent or convolutional neural network with input features based on the question , the passage and their token overlap . The entire network can also be trained end - to - end . Also , all 5 models combined have an error overlap of 13.68 % , i.e. , if we had a mechanism to perfectly choose between these models , we would get an Exact Match score of 86.32 % . This indicates that future work based on ensembling different neural models can give promising results and is worth exploring . An example of a passage - question - answer that all of the models get wrong is : Passage : The University of Warsaw was established in 1816 , when the partitions of Poland separated Warsaw from the oldest and most influential Polish academic center , in Krakow . Warsaw University of Technology is the second academic school of technology in the country , and one of the largest in East - Central Europe , employing 2 , 000 professors . Other institutions for higher education include the Medical University of Warsaw , the largest medical school in Poland and one of the most prestigious , the National Defence University , highest military academic institution in Poland , the Fryderyk Chopin University of Music the oldest and largest music school in Poland , and one of the largest in Europe , the Warsaw School of Economics , the oldest and most renowned economic university in the country , and the Warsaw University of Life Sciences the largest agricultural university founded in 1818 . Question : What is one of the largest music schools in Europe ? Answer : Fryderyk Chopin University of Music This passage - question - answer is difficult for automatic processing because there several entities of the same type ( school / university ) in the passage , and the question is a paraphrase of one segment of a very long , syntactically complicated sentence which contains the information required to be able to infer the correct answer . This presents an interesting challenge , and such qualitative observations can be used to formulate a general technique for effectively testing machine reading systems .", "entities": [[183, 187, "TaskName", "multi - label classification"], [199, 201, "MethodName", "logistic regression"], [275, 277, "MetricName", "Exact Match"]]}
{"text": "For qualitative error analysis , we sample 100 incorrect predictions ( based on EM ) from each model and try to find common error categories . Broadly , the errors observed were either because of incorrect answer span boundaries or inability to infer the meaning of the question / passage . Examples of each error type are shown in Table 3 , and these are further described below .", "entities": [[13, 14, "MetricName", "EM"]]}
{"text": "In this section , we record the main observations from our qualitative error analysis and analyze potential reasons for the error trends observed . Figure 4 shows the different types of errors in predictions by various models . We observe that BiDAF makes many boundarybased errors which indicates that a better output layer ( since this is responsible for span identification - although errors might have percolated from previous layers , most of these are cases where the model almost got the correct answer but not exactly ) or some post - processing of the answer might help improve performance . Paraphrases also contribute to almost 15 % of errors observed which indicates that the question and the relevant parts of the context are not effectively matched in these cases . We observe that R - Net makes fewer boundary errors , perhaps because self - attention enables it to accumulate evidence and return better answer spans , although this leads to more errors of the ' shorter ' answer type than ' longer ' . Also , missing inference contributes to almost 20 % of the observed errors ( not including multiple sentences or paraphrases ) . Paraphrasing is the most frequent error category observed for DrQA , which makes sense if we con - sider the features used to represent each passage , such as exact match with a question word , which depend on lexical overlap between the question and passage . We observe that DocQA makes many boundary errors too , again making more mistakes by pre - dicting shorter answers than expected in most of the observed cases . A better root cause analysis can be performed by visualizing outputs from different layers and evaluating these , and we leave this in - depth investigation to future work . Also , the high number of Soft Correct outputs across all models points to some deficiencies in the SQuAD annotations , which might limit the reliability of the performance evaluation metrics . Although these state - of - the - art deep learning models for machine reading are supposed to have inference capabilities , our error analysis above points to their limitations . These insights can be useful for developing benchmarks and datasets which enable realistic evaluation of systems which aim to ' solve ' the RC task . In Wadhwa et al ( 2018 ) , we take a first step in this direction by proposing a method focused on questions involving referential inference , a setting to which these models fail to generalize well .", "entities": [[227, 229, "MetricName", "exact match"], [322, 323, "DatasetName", "SQuAD"]]}
{"text": "Data and evaluation Our approach requires a large amount of monolingual data that is used for generating synthetic training pairs . We use the publicly available News crawl data 5 released for the WMT shared tasks ( Bojar et al , 2018 ) . For English and German , we limit the size of the data to 100 million sentences ; for Russian , we use all the available 80.5 million sentences . As primary development and test data , we use the following learner corpora ( Table 2 ) : English : the new W&I+LOCNESS corpus Granger , 1998 ) released for the BEA 2019 shared task and representing a diverse cross - section of English language ; German : the Falko - MERLIN GEC corpus ( Boyd , 2018 ) Russian : the recently introduced RULEC - GEC dataset ( Alsufieva et al , 2012 ; Rozovskaya and Roth , 2019 ) containing Russian texts from foreign and heritage speakers . Unless explicitly stated , we do not use the training parts of those datasets . For each language we follow the originally proposed preprocessing and evaluation settings . English and German data are tokenized with Spacy 6 , while Russian is preprocessed with Mystem ( Segalovich , 2003 ) . We additionally normalise punctuation in monolingual data using Moses scripts ( Koehn et al , 2007 ) . During training , we limit the vocabulary size to 32 , 000 subwords computed with SentencePiece using the unigram method ( Kudo and Richardson , 2018 ) . English models are evaluated with ERRANT ( Bryant et al , 2017 ) using F 0.5 ; for German and Russian , the M2Scorer with the MaxMatch metric ( Dahlmeier and Ng , 2012 ) is used . Synthetic data Confusion sets are created for each language for V = 96 , 000 most frequent lexical word forms from monolingual data . We use the Levenshtein distance to generate edit - distance based confusion sets . The maximum considered distance is 2 . Word embeddings are computed with word2vec 7 from monolingual data . To generate spell - broken confusion sets we use Enchant 8 with Aspell dictionaries . 9 The size of confusion sets is limited to top 20 words . Synthetic errors are introduced into monolingual texts to mimic word error rate ( WER ) of about 15 % , i.e. p WER = 0.15 , which resembles error frequency in common ESL error corpora . When confusing a word , the probability p sub is set to 0.7 , other probabilities are set to 0.1 . Training settings We adapt the recent state - ofthe - art GEC system by Junczys - Dowmunt et al ( 2018b ) , an ensemble of sequence - to - sequence Transformer models ( Vaswani et al , 2017 ) and a neural language model . 10 We use the training setting proposed by the authors 11 , but introduce stronger regularization : we increase dropout probabilities of source words to 0.3 , add dropout on transformer self - attention and filter layers of 0.1 , and use larger mini - batches with 2 , 500 sentences . We do not pre - train the decoder parameters with a language model and train directly on the synthetic data . We increase the size of language model used for ensembling to match the Transformer - big configuration ( Vaswani et al , 2017 ) with 16 - head self - attention , embeddings size of 1024 and feed - forward filter size size of 4096 . In experiments with fine - tuning , the training hyperparameters remain unchanged . All models are trained with Marian ( Junczys - Dowmunt et al , 2018a ) . The training is continued for at most 5 epochs or until early - stopping is triggered after 5 stalled validation steps . We found that using 10 , 000 synthetic sentences as validation sets , i.e. a fully unsupervised approach , is as effective as using the development parts of error corpora and does not decrease the final performance .", "entities": [[246, 247, "MethodName", "SentencePiece"], [342, 344, "TaskName", "Word embeddings"], [390, 396, "MetricName", "word error rate ( WER )"], [469, 470, "MethodName", "Transformer"], [570, 571, "MethodName", "Transformer"]]}
{"text": "We first compare the GEC systems with simple baselines using a greedy and context spell - checking ( Table 4 ) ; the latter selects the best correction suggestion based on the sentence perplexity from a Transformer language model . All systems outperform the spell - checker baselines . On German and Russian test sets , single MAGEC models without ensembling with a language model already achieve better performance than reported by Boyd ( 2018 ) Roth ( 2019 ) for their systems that use authentic error - annotated data for training ( Table 4b and 4c ) . Our best unsupervised ensemble systems that combine three Transformer models and a LM 12 outperform the state - of - the - art results for these languages by +7.0 and +11.4 F 0.5 . Our English models do not compete with the top systems ( Grundkiewicz et al , 2019 ) from the BEA shared task trained on publicly available errorannotated corpora ( Table 4a ) . It is difficult to compare with the top low - resource system from the shared task , because it uses additional parallel data from Wikipedia ( Grundkiewicz and Junczys - Dowmunt , 2014 ) , larger ensemble , and n - best list re - ranking with right - to - left models , which can be also implemented in this work . MAGEC systems are generally on par with the results achieved by a recent unsupervised contribution based on finite state transducers by Stahlberg et al ( 2019 ) on the CoNLL - 2014 ( Dahlmeier et al , 2013 ) and JFLEG test sets ( Napoles et al , 2017 All unsupervised systems benefit from domainadaptation via fine - tuning on authentic labelled data ( Miceli Barone et al , 2017 ) . The more authentic high - quality and in - domain training data is used , the greater the improvement , but even as few as~2 , 000 sentences are helpful ( Fig . 1 ) . We found that fine - tuning on a 2:1 mixture of synthetic and oversampled authentic data prevents the model from over - fitting . This is particularly visible for English which has the largest fine - tuning set ( 34 K sentences ) , and the difference of 5.2 F 0.5 between finetuning with and without synthetic data is largest .", "entities": [[33, 34, "MetricName", "perplexity"], [36, 37, "MethodName", "Transformer"], [107, 108, "MethodName", "Transformer"], [269, 270, "DatasetName", "JFLEG"]]}
{"text": "Feature attribution methods , proposed recently , help users interpret the predictions of complex models . Our approach integrates feature attributions into the objective function to allow machine learning practitioners to incorporate priors in model building . To demonstrate the effectiveness our technique , we apply it to two tasks : ( 1 ) mitigating unintended bias in text classifiers by neutralizing identity terms ; ( 2 ) improving classifier performance in a scarce data setting by forcing the model to focus on toxic terms . Our approach adds an L 2 distance loss between feature attributions and task - specific prior values to the objective . Our experiments show that i ) a classifier trained with our technique reduces undesired model biases without a tradeoff on the original task ; ii ) incorporating priors helps model performance in scarce data settings .", "entities": [[93, 94, "MetricName", "loss"]]}
{"text": "Baseline I am gay 0.915 I am straight 0.085 Our Method I am gay 0.141 I am straight 0.144 On the other hand , the amount of research focusing on explainable natural language processing ( NLP ) models ( Li et al , 2016 ; Murdoch et al , 2018 ; Lei et al , 2016 ) is modest as opposed to image explanation techniques . Inherent problems in data emerge in a trained model in several ways . Model explanations can show that the model is not inline with human judgment or domain expertise . A canonical example is model unfairness , which stems from biases in the training data . Fairness in ML models rightfully came under heavy scrutiny in recent years ( Zhang et al , 2018a ; Dixon et al , 2018 ; Angwin et al , 2016 ) . Some examples include sentiment analysis models weighing negatively for inputs containing identity terms such as \" jew \" and \" black \" , and hate speech classifiers leaning to predict any sentence containing \" islam \" as toxic ( Waseem and Hovy , 2016 ) . If employed , explanation techniques help divulge these issues , but fail to offer a remedy . For instance , the sentence \" I am gay \" receives a high score on a toxicity model as seen in Table 1 . The Integrated Gradients ( Sundararajan et al , 2017 ) explanation method attributes the majority of this decision to the word \" gay . \" However , none of the explanations methods suggest next steps to fix the issue . Instead , researchers try to reduce biases indirectly by mostly adding more data ( Dixon et al , 2018 ; , using unbiased word vectors ( Park et al , 2018 ) , or directly optimizing for a fairness proxy with adversarial training ( Madras et al , 2018 ; Zhang et al , 2018a ) . These methods either offer to collect more data , which is costly in many cases , or make a tradeoff between original task performance and fairness . In this paper , we attempt to enable injecting priors through model explanations to rectify issues in trained models . We demonstrate our approach on two problems in text classification settings : ( 1 ) model biases towards protected identity groups ; ( 2 ) low classification performance due to lack of data . The core idea is to add L 2 distance between Path Integrated Gradients attributions for pre - selected tokens and a target attribution value in the objective function as a loss term . For model fairness , we impose the loss on keywords identifying protected groups with target attribution of 0 , so the trained model is penalized for attributing model decisions to those keywords . Our main intuition is that undesirable correlations between toxicity labels and instances of identity terms cause the model to learn unfair biases which can be corrected by incorporating priors on these identity terms . Moreover , our approach allows practitioners to impose priors in the other direction to tackle the problem of training a classifier when there is only a small amount of data . As shown in our experiments , by setting a positive target attribution for known toxic words 1 , one can improve the performance of a toxicity classifier in a scarce data regime . We validate our approach on the Wikipedia toxic comments dataset ( Wulczyn et al , 2017 ) . Our fairness experiments show that the classifiers trained with our method achieve the same performance , if not better , on the original task , while improving AUC and fairness metrics on a synthetic , unbiased dataset . Models trained with our technique also show lower attributions to identity terms on average . Our technique produces much better word vectors as a by - product when compared to the baseline . Lastly , by setting an attribution target of 1 on toxic words , a classifier trained with our objective function achieves better performance when only a subset of the data is present . 1 Full list of identity terms and toxic terms used as priors can be found in supplemental material . Please note the toxic terms are not censored .", "entities": [[112, 113, "TaskName", "Fairness"], [147, 149, "TaskName", "sentiment analysis"], [168, 170, "DatasetName", "hate speech"], [383, 385, "TaskName", "text classification"], [439, 440, "MetricName", "loss"], [449, 450, "MetricName", "loss"], [459, 460, "DatasetName", "0"], [618, 619, "MetricName", "AUC"]]}
{"text": "Problems in data manifest themselves in a trained model 's performance on classification or fairness metrics . Traditionally , model deficiencies were addressed by providing priors through extensive feature engineering and collecting more data . Recently , attributions help uncover deficiencies causing models to perform poorly , but do not offer actionability . To this end , we propose to add an extra term to the objective function to penalize the L 2 distance between model attributions on certain features and target attribution values . This modification allows model practitioners to inject priors . For example , consider a model that tends to predict every sentence containing \" gay \" as toxic in a comment moderation system . Penalizing non - zero attributions on the tokens identifying protected groups would force the model to focus more on the context words rather than mere existence of certain tokens . We give the formal definition of the new objective function that incorporates priors as the follows : Definition 3.1 . Given a vector t of size n , where n is the length of the input sequence and t i is the attribution target value for the ith token in the input sequence . The prior loss for a scalar output is defined as : L prior ( a , t ) = n i ( a i \u2212 t i ) 2 ( 2 ) where a i refers to attribution of the ith token as in Definition 2.1 . For a multi - class problem , we train our model with the following joint objective , L joint = L ( y , p ) + \u03bb C c L prior ( a c , t c ) ( 3 ) where a c and t c are the attribution and attribution target for class c , \u03bb is the hyperparameter that controls the stength of the prior loss and L is the crossentropy loss defined as follows : L ( y , p ) = C c \u2212y c log ( p c ) ( 4 ) where y is an indicator vector of the ground truth label and p c is the posterior probability of class c. The joint objective function is differentiable w.r.t . model parameters when attribution is calculated through Equation 1 and can be trained with most off - the - shelf optimizers . The proposed objective is not dataset - dependent and is applicable to different problem settings such as sentiment classification , abuse detection , etc . It only requires users to specify the target attribution value for tokens of interest in the corpus . We illustrate the effectiveness of our method by applying it to a toxic comment classification problem . In the next section , we first show how we set the target attribution value for identity terms to remove unintended biases while retaining the same performance on the original task . Then , using the same technique , we show how to set target attribution for toxic words to improve classifier performance in a scarce data setting .", "entities": [[28, 30, "TaskName", "feature engineering"], [204, 205, "MetricName", "loss"], [318, 319, "MetricName", "loss"], [324, 325, "MetricName", "loss"], [419, 421, "TaskName", "abuse detection"], [454, 457, "TaskName", "toxic comment classification"]]}
{"text": "We incorporate human prior in model building on two applications . First , we tackle the problem of unintended bias in toxic comment classification ( Dixon et al , 2018 ) with our proposed method . For our experiments , we aim to mitigate the issue of neutral sentences with identity terms being classified as toxic for a given a set of identity terms . A subset of the identity terms are listed in the first column of Table 2 . Second , we force the model to focus on a list of human - selected toxic terms under scarce data scenario to increase model performance . In the following section , we introduce the dataset we train and evaluate on along with a synthetic dataset to further validate our fairness improvements . After that , we describe our experimental setup . Then , we compare our method to a classifier trained without the prior loss and 2 other baselines . Lastly , we show the results demonstrating usefulness of our approach with data scarcity .", "entities": [[21, 24, "TaskName", "toxic comment classification"], [155, 156, "MetricName", "loss"]]}
{"text": "In this work , we use a dataset containing comments from Wikipedia Talk Pages ( Dixon et al , 2018 ) . Number of samples are 95 , 692 / 32 , 128 / 31 , 866 in the train / dev / test sets respectively . The ratio of positive ( toxic ) labels in the training set is 9.7 % . The dataset was annotated by human raters , where toxicity was defined as a \" rude , disrespectful , or unreasonable comment that is likely to make you leave a discussion \" per Dixon et al ( 2018 ) . Please refer to the corresponding paper for more details about collection methodology , biases present in the data , and toxicity distribution per comment length . We also use a synthetically generated dataset to validate our approach on fairness as in Park et al", "entities": [[22, 25, "HyperparameterName", "Number of samples"]]}
{"text": "For the text classifier , we built a convolutional neural network ( CNN ) classifier as in Kim ( 2014 ) . The network contains a convolution layer with 128 2 - , 3 - , 4 - gram filters for a sequence length of 100 followed by a max - pooling layer and softmax function . Embeddings were randomly initialized and their size was set to 128 . Shorter sequences are padded with < pad > token and longer sequences are truncated . Tokens occurring 5 times or more are retained in the vocabulary . We set dropout as 0.2 and used Adam ( Kingma and Ba , 2015 ) as our optimizer with initial learning rate set to 0.001 . We did n't perform extensive network architecture search to improve the performance as it is a reasonably strong classifier with the initial performance of 95.5 % accuracy . The number of interpolating steps for IG is set to 50 ( as in the original paper ) for calculating Riemann approximation of the integral . Since the output of the binary classification can be reduced to a single scalar output by taking the posterior of the positive ( toxic ) class , the prior is only added to the positive class in equation 3 . We set t i = k , if x i I a i , otherwise , ( 5 ) where I is the set of selected terms and x i being the i th token in the sequence . For fairness experiments , we set k to be 0 and I to the set of identity terms with the hope that these terms should be as neutral as possible when making predictions . Hyperparamter \u03bb is searched in the range of ( 1 , 10 8 ) and increased from 1 by a scale of 10 on the dev set and we pick the one with best F - 1 score . \u03bb is set to 10 6 for the final model . For data scarcity experiments , we set k to 1 and I to the set of toxic terms to force the model to make high attributions to these terms . Hyperparameter \u03bb is set to 10 5 across all data size experiments by tuning on the dev set with model given 1 % of training data . Each experiment was repeated for 5 runs with 10 epochs and the best model is selected according to the dev set . Training takes 1 minute for a model with cross - entropy loss and 30 minutes for a model with joint loss on an NVidia V100 GPU . However , reducing the step size in IG for calculating Riemann approximation of the integral to 10 steps reduces the training time to 6 minutes . Lastly , training with joint loss reaches its best performance in later epochs than training with crossentropy loss .", "entities": [[26, 27, "MethodName", "convolution"], [54, 55, "MethodName", "softmax"], [103, 104, "MethodName", "Adam"], [113, 114, "HyperparameterName", "optimizer"], [116, 118, "HyperparameterName", "learning rate"], [148, 149, "MetricName", "accuracy"], [264, 265, "DatasetName", "0"], [429, 430, "MetricName", "loss"], [438, 439, "MetricName", "loss"], [449, 451, "HyperparameterName", "step size"], [476, 477, "MetricName", "loss"], [488, 489, "MetricName", "loss"]]}
{"text": "We compare our work to 3 models with the same CNN architecture , but different training settings : Baseline : A baseline classifier trained with cross - entropy loss . Importance : Classifier trained with crossentropy loss , but the loss for samples containing identity words are weighted in the range ( 1 , 10 8 ) , where the actual coefficient is determined to be 10 on the dev set based on F - 1 score . TOK Replace : Common technique for making models blind to identity terms ( Garg et al , 2018 ) . All identity terms are replaced with a special < i d > token . We also explore a different training schedule for cases where a model has been trained to optimize for a classification loss : Finetuned : An already - trained classifier is finetuned with joint loss for several epochs . The aim of this experiment is to show that our method is also applicable for tweaking trained models , which could be useful if the original had been trained for a long time .", "entities": [[28, 29, "MetricName", "loss"], [36, 37, "MetricName", "loss"], [40, 41, "MetricName", "loss"], [132, 133, "MetricName", "loss"], [145, 146, "MetricName", "loss"]]}
{"text": "We first verify that the prior loss term does not adversely affect overall classifier performance on the main task using general performance metrics such as accuracy and F - 1 . Results are shown in Table 4 . Unlike previous approaches ( Park et al , 2018 ; Dixon et al , 2018 ; Madras et al , 2018 ) , our method does not degrade classifier performance ( it even improves ) in terms of all reported metrics . We also look at samples containing identity terms . Table 5 shows classifier performance metrics for such samples . The importance weighting approach slightly outperforms the baseline classifier . Replacing identity words with a special tokens , on the other hand , hurts the performance on the main task . One of the reasons might be that replacing all identity terms with a token potentially removes other useful information model can rely on . If we were to make an analogy between the token replacement method and hard ablation , then the same analogy can be made between our method and soft ablation . Hence , the information pertaining to identity terms is not completely lost for our method , but come at a cost . Results for fine - tuning experiments show the performance after 2 epochs . It is seen that the model converges to similar performance with joint training after only 2 epochs , albeit being slightly poorer .", "entities": [[6, 7, "MetricName", "loss"], [25, 26, "MetricName", "accuracy"]]}
{"text": "Now we run our experiments on the templatebased synthetic data . As stated , this dataset is used to measure biases in the model since it is unbiased towards identities . We use AUC along with False Positive Equality Difference ( FPED ) and False Negative Equality Difference ( FNED ) , which measure a proxy of Equality of Odds ( Hardt et al , 2016 ) , as in Dixon et al ( 2018 ) ; Park et al ( 2018 ) . FPED sums absolute differences between overall false positive rate and false positive rates for each identity term . FNED calculates the same for false negatives . Results on this dataset are shown in Table 7 . Our method provides substantial improvement on AUC and almost completely eliminates false positive and false negative inequality across identities . The fine - tuned model also outperforms the baseline for mitigating the bias . The token replacement method comes out as a good baseline for mitigating the bias since it treats all identities the same . The importance weighting approach fails to produce an unbiased model .", "entities": [[33, 34, "MetricName", "AUC"], [126, 127, "MetricName", "AUC"]]}
{"text": "We now demonstrate our approach on encouraging higher attributions on toxic words to increase model performance in scarce data regime . We down - sample the dataset with different ratios to simulate a data scarcity scenario . To directly validate the effectiveness of prior loss on attributions , we first show that the attribution of the toxic words have higher values for our method across different data ratios compared to the baseline in Table 8 . We also show that the attribution for these terms increases as training data increases for the baseline method . We then show model performance on testing data for different data size ratios for the baseline and our method in Figure 1 . Our method outperforms the baseline by a big margin in 1 % and 5 % ratio . However , the impact of our approach diminishes after adding more data , since the model starts to learn to focus on toxic words itself for predicting toxicity without the need for prior injection . We can also see that both the baseline and our method start to catch up with the rule based approach , where we give positive prediction if the toxic word is in the sentence , and eventually outperform it .", "entities": [[44, 45, "MetricName", "loss"]]}
{"text": "For explaining ML models , recent research attempts offer techniques ranging from building inherently interpretable models to building a proxy model for explaining a more complex model ( Ribeiro et al , 2016 ; Frosst and Hinton , 2017 ) to explaining inner mechanics of mostly uninterpretable neural networks ( Sundararajan et al , 2017 ; Bach et al , 2015 ) . One family of interpretability methods uses sensitivity of the network with respect to data points ( Koh and Liang , 2017 ) or features ( Ribeiro et al , 2016 ) as a form of explanation . These methods rely on small , local perturbations and check how a network 's response changes . Explaining text models has another layer of complexity due to a lock of proper technique to generate counterfactuals in the form of small perturbations . Hence , interpretability methods tailored for text are quite sparse ( Mudrakarta et al , 2018 ; Jia and Liang , 2017 ; Murdoch et al , 2018 ) . On the other hand , there are many papers criticizing the aforementioned methods by questioning their faithfulness , correctness ( Adebayo et al , 2018 ; Kindermans et al , 2017 ) and usefulness . Smilkov et al ( 2017 ) show that gradient based methods are susceptible to saturation and can be fooled by adversarial techniques . Other sets of papers ( Miller , 2019 ; Gilpin et al , 2018 ) attack model explanation papers from a philosophical perspective . However , the lack of actionability angle is often overlooked . Lipton ( 2018 ) briefly questions the practical benefit of having model explanations from a practitioners perspective . There are several works taking advantage of model explanations . Namely , using model explanations to aid doctors in diagnosing retinopathy patients , and removing minimal features , called pathologies , from neural networks by tuning the model to have high entropy on pathologies ( Feng et al , 2018 ) . The authors of Ross et al ( 2017 ) propose a similar idea to our approach in that they regularize input gradients to alter the decision boundary of the model to make it more consistent with domain knowledge . However , the input gradients technique has been shown to be an inaccurate explanation technique ( Adebayo et al , 2018 ) . Addressing and mitigating bias in NLP models are paramount tasks as the effects on these models adversely affect protected subpopulations ( Schmidt and Wiegand , 2017 ) . One of the earliest works is Calders and Verwer ( 2010 ) . Later , Bolukbasi et al ( 2016 ) proposed to unbias word vectors from gender stereotypes . Park et al ( 2018 ) also try to address gender bias for abusive language detection models by debiasing word vectors , augmenting more data and changing model architecture . While their results seem to show promise for removing gender bias , their method does n't scale for other identity dimensions such as race and religion . The authors of Dixon et al ( 2018 ) highlight the bias in toxic comment classifier models originating from the dataset . They also supplement the training dataset from Wikipedia articles to shift positive class imbalance for sentences containing identity terms to dataset average . Similarly , their approach alleviates the issue to a certain extent , but does not scale to similar problems as their augmentation technique is too data - specific . Also , both methods trade original task accuracy for fairness , while our method does not . Lastly , there are several works ( Davidson et al , 2017 ; Zhang et al , 2018b ) offering methodologies or datasets to evaluate models for unintended bias , but they fail to offer a general framework . One of the main reasons our approach improves the model in the original task is that the model is now more robust thanks to the reinforcement provided to the model builder through attributions . From a fairness angle , our technique shares similarities with adversarial training ( Zhang et al , 2018a ; Madras et al , 2018 ) in asking the model to optimize for an additional objective that transitively unbiases the classifier . However , those approaches work to remove protected attributes from the representation layer , which is unstable . Our approach , on the other hand , works with basic human - interpretable units of information - tokens . Also , those approaches propose to sacrifice main task performance for fairness as well . While our method enables model builders to inject priors to aid a model , it has several limitations . In solving the fairness problem in question , it causes the classifier to not focus on the identity terms even for the cases where an identity term itself is being used as an insult . Moreover , our approach requires prior terms to be manually provided , which bears resemblance to blacklist approaches and suffers from the same drawbacks . Lastly , the evaluation methodology that we and previous papers ( Dixon et al , 2018 ; Park et al , 2018 ) rely on are based on a syntheticallygenerated dataset , which may contain biases of the individuals creating it .", "entities": [[468, 470, "TaskName", "abusive language"], [593, 594, "MetricName", "accuracy"]]}
{"text": "In this paper , we proposed actionability on model explanations that enable ML practitioners to enforce priors on their model . We apply this technique to model fairness in toxic comment classification . Our method incorporates Path Integrated Gradients attributions into the objective function with the aim of stopping the classifier from carrying along false positive bias from the data by punishing it when it focuses on identity words . Our experiments indicate that the models trained jointly with cross - entropy and prior loss do not suffer a performance drop on the original task , while achieving a better performance in fairness metrics on the template - based dataset . Applying model attribution as a fine - tuning step on a trained classifier makes it converge to a more debiased classifier in just a few epochs . Additionally , we show that model can be also forced to focus on pre - determined tokens . There are several avenues we can explore as future research . Our technique can be applied to implement a more robust model by penalizing the attributions falling outside of tokens annotated to be relevant to the predicted class . Another avenue is to incorporate different model attribution strategies such as DeepLRP ( Bach et al , 2015 ) into the objective function . Finally , it would be worthwhile to invest in a technique to extract problematic terms from the model automatically rather than providing prescribed identity or toxic terms .", "entities": [[29, 32, "TaskName", "toxic comment classification"], [84, 85, "MetricName", "loss"]]}
