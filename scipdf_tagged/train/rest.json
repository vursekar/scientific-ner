{"text": "Maoqin @ DravidianLangTech - EACL2021 : The Application of Transformer - Based Model", "entities": [[9, 10, "MethodName", "Transformer"]]}
{"text": "An overall framework and processing pipeline of my solution are shown in Figure 1 . In my job , I use the ALBERT model as my base model and take BiGRU - Attention behind it . My model is shown in Figure 2 .", "entities": [[22, 23, "MethodName", "ALBERT"], [30, 31, "MethodName", "BiGRU"]]}
{"text": "The BiGRU - Attention model ( Cover and Hart , 1967 ) is divided into three parts : text vector input layer , hidden layer , and output layer . Among them , the hidden layer consists of three layers : the BiGRU layer , the attention layer , and the Dense layer ( fully connected layer ) . I set the output of the ALBERT model as the input . After receiving the input , it uses the BiGRU neural network layer to extract features of the deep - level information of the text firstly . Secondly , it uses the attention layer to assign corresponding weights to the deep - level information of the extracted text . Finally , the text feature information with different weights is put into the softmax function layer for classification . The structure of the BiGRU - Attention model is shown in Figure 3 .", "entities": [[1, 2, "MethodName", "BiGRU"], [42, 43, "MethodName", "BiGRU"], [65, 66, "MethodName", "ALBERT"], [79, 80, "MethodName", "BiGRU"], [132, 133, "MethodName", "softmax"], [142, 143, "MethodName", "BiGRU"]]}
{"text": "In this paper , I present my result on Offensive Language Identification in Dravidian Languages - EACL 2021 which includes three tasks of different languages . For this task , I regard it as a multiple classification task , I use the BiGRU - Attention based on the ALBERT model to complete , and my model works very well . I also summarized the possible reasons for classifying only three types of labels . At the same time , I also use some other neural networks for comparative experiments to prove that my model can obtain excellent performance . The result shows that my model ranks 5th in the Malayalam task . Due to the continuous development of the definition of offensive information on the Internet , it is difficult to accurately describe the nature of this information only from the perspective of data mining , which makes it impossible to model this information effectively . In the future , I will use methods based on multidisciplinary discovery to guide model learning . These models are more likely to use limited data to learn more effective models . At the same time , I will also consider whether I can use other transfer learning models to perform better on multi - classification tasks .", "entities": [[10, 12, "TaskName", "Language Identification"], [42, 43, "MethodName", "BiGRU"], [48, 49, "MethodName", "ALBERT"], [202, 204, "TaskName", "transfer learning"]]}
{"text": "Let x = [ x 1 , . . . , x T ] be our input text and y = [ y 1 , . . . , y T ] be per - token output tags . Let D be the domain size of each y i . We predict the most likely y , given a conditional model P ( y | x ) . This paper considers two factorizations of the conditional distribution . First , we have P ( y | x ) = T t=1 P ( y t | F ( x ) ) , ( 1 ) where the tags are conditionally independent given some features for x. Given these features , O ( D ) prediction is simple and parallelizable across the length of the sequence . However , feature extraction may not necessarily be parallelizable . For example , RNN - based features require iterative passes along the length of x. We also consider a linear - chain CRF model that couples all of y together : P ( y | x ) = 1 Z x T t=1 \u03c8 t ( y t | F ( x ) ) \u03c8 p ( y t , y t\u22121 ) , ( 2 ) where \u03c8 t is a local factor , \u03c8 p is a pairwise factor that scores consecutive tags , and Z x is the partition function ( Lafferty et al , 2001 ) . To avoid overfitting , \u03c8 p does not depend on the timestep t or the input x in our experiments . Prediction in this model requires global search using the O ( D 2 T ) Viterbi algorithm . CRF prediction explicitly reasons about interactions among neighboring output tags , whereas prediction in the first model compiles this reasoning into the feature extraction step ( Liang et al , 2008 ) . The suitability of such compilation depends on the properties and quantity of the data . While CRF prediction requires non - trivial search in output space , it can guarantee that certain output constraints , such as for IOB tagging ( Ramshaw and Marcus , 1999 ) , will always be satisfied . It may also have better sample complexity , as it imposes more prior knowledge about the structure of the interactions among the tags ( London et al , 2016 ) . However , it has worse computational complexity than independent prediction .", "entities": [[168, 169, "MethodName", "CRF"], [286, 287, "MethodName", "CRF"], [335, 336, "MethodName", "CRF"]]}
{"text": "We thank Subhransu Maji and Luke Vilnis for helpful discussions , and Brendan O'Connor , Yoav Goldberg , the UMass NLP reading group and many anonymous reviewers for constructive comments on various drafts of the paper . We are also grateful to Guillaume Lample for sharing his pretrained word embeddings . This work was supported in part by the Center for Intelligent Information Retrieval , in part by DARPA under agreement number FA8750 - 13 - 2 - 0020 , in part by Defense Advanced Research Agency ( DARPA ) contract number HR0011 - 15 - 2 - 0036 , in part by the National Science Foundation ( NSF ) grant number DMR - 1534431 , and in part by the National Science Foundation ( NSF ) grant number IIS - 1514053 . The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon . Any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor .", "entities": [[48, 50, "TaskName", "word embeddings"], [62, 64, "TaskName", "Information Retrieval"], [68, 69, "DatasetName", "DARPA"], [88, 89, "DatasetName", "DARPA"]]}
{"text": "Validating Label Consistency in NER Data Annotation", "entities": [[4, 5, "TaskName", "NER"]]}
{"text": "Data annotation plays a crucial role in ensuring your named entity recognition ( NER ) projects are trained with the correct information to learn from . Producing the most accurate labels is a challenge due to the complexity involved with annotation . Label inconsistency between multiple subsets of data annotation ( e.g. , training set and test set , or multiple training subsets ) is an indicator of label mistakes . In this work , we present an empirical method to explore the relationship between label ( in - ) consistency and NER model performance . It can be used to validate the label consistency ( or catch the inconsistency ) in multiple sets of NER data annotation . In experiments , our method identified the label inconsistency of test data in SCIERC and CoNLL03 datasets ( with 26.7 % and 5.4 % label mistakes ) . It validated the consistency in the corrected version of both datasets .", "entities": [[9, 12, "TaskName", "named entity recognition"], [13, 14, "TaskName", "NER"], [92, 93, "TaskName", "NER"], [115, 116, "TaskName", "NER"], [132, 133, "DatasetName", "SCIERC"], [134, 135, "DatasetName", "CoNLL03"]]}
{"text": "problem ] Task , we present a novel technique ... FERRET utilizes a novel approach to [ Q / A ] Method known as predictive questioning which attempts to identify ... FERRET utilizes a novel approach to [ Q / A ] Task known as predictive questioning which attempts to identify ... The goal of this work is the enrichment of [ human - machine interactions ] Task in a natural language environment . The goal of this work is the [ enrichment of human - machine interactions ] Task in a natural language environment . We sample three exclusive subsets ( of size x ) from the training set ( orange , green , and blue ) . We use one subset as the new test set ( orange ) . We apply the SCIIE NER model on the new test set . We build three new training sets : i ) \" TrainTest \" ( blue - red ) , ii ) \" PureTrain \" ( green - blue ) , iii ) \" TestTrain \" ( red - blue ) . Results on SCIERC show that the test set ( red ) is less predictive of training samples ( orange ) than the training set itself ( blue or green ) . This was not observed on two other datasets . Besides the significant correction on the SCI - ERC dataset , our contributions in this work are as follows : i ) an empirical , visual method to identify the label inconsistency between subsets of annotated data ( see Figure 1 ) , ii ) a method to validate the label consistency of corrected data annotation ( see Figure 2 ) . Experiments show that they are effective on the CoNLL03 and SCIERC datasets .", "entities": [[136, 137, "TaskName", "NER"], [186, 187, "DatasetName", "SCIERC"], [294, 295, "DatasetName", "CoNLL03"], [296, 297, "DatasetName", "SCIERC"]]}
{"text": "Suppose the labeling processes on two parts of annotated data were consistent . They are likely to be equivalently predictive of each other . In other words , if we train a model with a set of samples from either part A or part B to predict a different set from part A , the performance should be similar . Take SCIERC as an example . We were wondering whether the labels in the test set were consistent with those in the training set . Our method to identify the inconsistency is presented in Figure 1 . We sample three exclusive subsets ( of size x ) from the training set . We set x = 550 according to the size of the original test set . We use one of the subsets as the new test set . Then we train the SCIIE NER model ( Luan et al , 2018 ) to perform on the new test set . We build three new training sets to feed into the model : \" TrainTest \" : first fed with one training subset and then the original test set ; \" PureTrain \" : fed with two training subsets ; \" TestTrain \" : first fed with the original test set and then one of the training subsets . Results show that \" TestTrain \" performed the worst at the early stage because the quality of the original test set is not reliable . In \" TrainTest \" the performance no longer improved when the model started being fed with the original test set . \" Pure - Train \" performed the best . All the observations conclude that the original test set is less predictive of training samples than the training set itself . It may be due to the issue of label inconsistency . Moreover , we do not have such observations on two other datasets , WikiGold and WNUT16 .", "entities": [[61, 62, "DatasetName", "SCIERC"], [144, 145, "TaskName", "NER"]]}
{"text": "After we corrected the label mistakes , how could we empirically validate the recovery of label consistency ? Again , we use a subset of training data as the new test set . We evaluate the predictability of the original wrong test subset , the corrected test subset , and the rest of the training set . We expect to see that the wrong test subset delivers weaker performance and the other two sets make comparable good predictions . Figure 2 illustrates this idea . Take SCIERC as an example . Suppose we corrected z of y + z sentences in the test set . The original wrong test subset ( \" Mistake \" ) and the corrected test subset ( \" Correct \" ) are both of size z. Here z = 147 and the original good test subset y = 404 ( \" Test \" ) . We sampled three exclusive subsets of size x , y , and w = 804 from the training set ( \" Train \" ) . We use the first subset ( of size x ) as the new test set . We build four new training sets and feed into the SCIIE model . Each new training set has y + w + z = 1 , 355 sentences . \" TestTrainMistake\"/\"TestTrainCorrect \" : the original good test subset , the third sampled training subset , and the original wrong test subset ( or the corrected test subset ) ; \" PureTrainMistake\"/\"PureTrainCorrect \" : the second and third sampled training subsets and the original wrong test subset ( or the corrected test subset ) ; \" MistakeTestTrain\"/\"CorrectTestTrain \" : the original wrong test subset ( or the corrected test subset ) , the original good test subset , and the third sampled training subset ; \" MistakePureTrain\"/\"CorrectPureTrain \" : the original wrong test subset ( or the corrected test subset ) and the second and third sampled training subsets . Results show that the label mistakes ( i.e. , original wrong test subset ) hurt the model performance whenever being fed at the beginning or later . The corrected test subset delivers comparable performance with the original good test subset and the training set . This demonstrates the label consistency of the corrected test set with the training set .", "entities": [[86, 87, "DatasetName", "SCIERC"]]}
{"text": "The visual results of the proposed methods have been presented in Section 2 . Here we deploy five state - of - the - art NER models to investigate their performance on the corrected SCIERC dataset . The NER models are BiLSTM - CRF ( Lample et al , 2016 ) , LM - BiLSTM - CRF ( Liu et al , 2018 ) , singletask and multi - task SCIIE ( Luan et al , 2018 ) , and multi - task DyGIE ( Luan et al , 2019 ) . As shown in Table 2 , all NER models deliver better performance on the corrected SCIERC than the original dataset . So the training set is more consistent with the fixed test set than the original wrong test set . In future work , we will explore more baselines in the leaderboard .", "entities": [[25, 26, "TaskName", "NER"], [34, 35, "DatasetName", "SCIERC"], [38, 39, "TaskName", "NER"], [41, 42, "MethodName", "BiLSTM"], [43, 44, "MethodName", "CRF"], [54, 55, "MethodName", "BiLSTM"], [56, 57, "MethodName", "CRF"], [99, 100, "TaskName", "NER"], [107, 108, "DatasetName", "SCIERC"]]}
{"text": "NER is typically cast as a sequence labeling problem and solved by models integrate LSTMs , CRF , and language models ( Lample et al , 2016 ; Liu et al , 2018 ; Zeng et al , 2019 . Another idea is to generate span candidates and predict their type . Span - based models have been proposed with multitask learning strategies ( Luan et al , 2018 ( Luan et al , , 2019 . The multiple tasks include concept recognition , relation extraction , and co - reference resolution . Researchers notice label mistakes in many NLP tasks ( Manning , 2011 ; Wang et al , 2019 ; Eskin , 2000 ; Kv\u0207to\u0148 and Oliva , 2002 ) . For instance , it is reported that the bottleneck of the POS tagging task is the consistency of the annotation result ( Manning , 2011 ) . People tried to detect label mistakes automatically and minimize the influence of noise in training . The mistake re - weighting mechanism is effective in the NER task ( Wang et al , 2019 ) . We focus on visually evaluating the label consistency .", "entities": [[0, 1, "TaskName", "NER"], [16, 17, "MethodName", "CRF"], [84, 86, "TaskName", "relation extraction"], [176, 177, "TaskName", "NER"]]}
{"text": "We presented an empirical method to explore the relationship between label consistency and NER model performance . It identified the label inconsistency of test data in SCIERC and CoNLL03 datasets ( with 26.7 % and 5.4 % label mistakes ) . It validated the label consistency in multiple sets of NER data annotation on two benchmarks , CoNLL03 and SCIERC .", "entities": [[13, 14, "TaskName", "NER"], [26, 27, "DatasetName", "SCIERC"], [28, 29, "DatasetName", "CoNLL03"], [50, 51, "TaskName", "NER"], [57, 58, "DatasetName", "CoNLL03"], [59, 60, "DatasetName", "SCIERC"]]}
{"text": "Controllable Text Simplification with Explicit Paraphrasing", "entities": [[1, 3, "TaskName", "Text Simplification"]]}
{"text": "Text Simplification improves the readability of sentences through several rewriting transformations , such as lexical paraphrasing , deletion , and splitting . Current simplification systems are predominantly sequence - to - sequence models that are trained end - to - end to perform all these operations simultaneously . However , such systems limit themselves to mostly deleting words and can not easily adapt to the requirements of different target audiences . In this paper , we propose a novel hybrid approach that leverages linguistically - motivated rules for splitting and deletion , and couples them with a neural paraphrasing model to produce varied rewriting styles . We introduce a new data augmentation method to improve the paraphrasing capability of our model . Through automatic and manual evaluations , we show that our proposed model establishes a new state - ofthe art for the task , paraphrasing more often than the existing systems , and can control the degree of each simplification operation applied to the input texts . 1", "entities": [[0, 2, "TaskName", "Text Simplification"], [110, 112, "TaskName", "data augmentation"]]}
{"text": "Text Simplification aims to improve the readability of texts with simpler grammar and word choices while preserving meaning ( Saggion , 2017 ) . It provides reading assistance to children ( Kajiwara et al , 2013 ) , non - native speakers ( Petersen and Ostendorf , 2007 ; Pellow and Eskenazi , 2014 ; Paetzold , 2016 ) , and people with reading disabilities ( Rello et al , 2013 ) . It also helps with downstream natural language processing tasks , such as parsing ( Chandrasekar et al , 1996 ) , semantic role labelling ( Vickrey and Koller , 2008 ) , information extraction ( Miwa et al , 2010 ) , and machine translation ( MT , Chen et al , 2012 ; \u0160tajner and Popovic , 2016 ) . Since 2016 , nearly all text simplification systems have been sequence - to - sequence ( seq2seq ) Table 1 : Output statistics of 500 random sentences from the Newsela test set . Existing systems rely on deletion and do not paraphrase well . OLen , % new , % eq and % split denote the average output length , percentage of new words added , percentage of system outputs that are identical to the inputs , and percentage of sentence splits , respectively . \u2020We used the system outputs shared by their authors . models trained end - to - end , which have greatly increased the fluency of the outputs ( Zhang and Lapata , 2017 ; Nisioi et al , 2017 ; Zhao et al , 2018 ; Kriz et al , 2019 ; Dong et al , 2019 ; . However , these systems mostly rely on deletion and tend to generate very short outputs at the cost of meaning preservation ( Alva - Manchego et al , 2017 ) . Table 1 shows that they neither split sentences nor paraphrase well as reflected by the low percentage of splits ( < 1 % ) and new words introduced ( < 11.2 % ) . While deleting words is a viable ( and the simplest ) way to reduce the complexity of sentences , it is suboptimal and unsatisfying . Professional editors are known to use a sophisticated combination of deletion , paraphrasing , and sentence splitting to simplify texts ( Xu et al , 2015 ) . Another drawback of these end - to - end neural systems is the lack of controllability . Simplification is highly audience dependant , and what constitutes simplified text for one group of users may not be acceptable for other groups ( Xu et al , 2015 ; Lee and Yeung , 2018 ) . An ideal simplification system should be able to generate text with varied characteristics , such as different lengths , readability levels , and number of split sentences , which can be difficult to control in end - to - end systems . To address these issues , we propose a novel hybrid approach that combines linguisticallymotivated syntactic rules with data - driven neural models to improve the diversity and controllability of the simplifications . We hypothesize that the seq2seq generation model will learn lexical and structural paraphrases more efficiently from the parallel corpus , when we offload some of the burden of sentence splitting ( e.g. , split at comma ) and deletion ( e.g. , remove trailing preposition phrases ) decisions to a separate component . Previous hybrid approaches for simplification ( Narayan and Gardent , 2014 ; Siddharthan and Mandya , 2014 ; Sulem et al , 2018c ) used splitting and deletion rules in a deterministic step before applying an MT - based paraphrasing model . In contrast , our approach provides a more flexible and dynamic integration of linguistic rules with the neural models through ranking and data augmentation ( Figure 1 ) . We compare our method to several state - of - theart systems in both automatic and human evaluations . Our model achieves overall better performance measured by SARI ( Xu et al , 2016 ) and other metrics , showing that the generated outputs are more similar to those written by human editors . We also demonstrate that our model can control the extent of each simplification operation by : ( 1 ) imposing a soft constraint on the percentage of words to be copied from the input in the seq2seq model , thus limiting lexical paraphrasing ; and ( 2 ) selecting candidates that underwent a desired amount of splitting and/or deletion . Finally , we create a new test dataset with multiple human references for Newsela ( Xu et al , 2015 ) , the widely used text simplification corpus , to specifically evaluate lexical paraphrasing .", "entities": [[0, 2, "TaskName", "Text Simplification"], [116, 118, "TaskName", "machine translation"], [139, 141, "TaskName", "text simplification"], [150, 151, "MethodName", "seq2seq"], [163, 164, "DatasetName", "Newsela"], [528, 529, "MethodName", "seq2seq"], [641, 643, "TaskName", "data augmentation"], [738, 739, "MethodName", "seq2seq"], [775, 776, "DatasetName", "Newsela"], [787, 789, "TaskName", "text simplification"]]}
{"text": "Figure 1 shows an overview of our hybrid approach . We combine linguistic rules with data - driven neural models to improve the controllability and diversity of the outputs . Given an input complex sentence x , we first generate a set of intermediate simplifications V = { v 1 , v 2 , . . . , v n } that have undergone splitting and deletion ( 2.1 ) . These intermediate sentences are then used for two purposes : ( 1 ) Selected by a pairwise neural ranking model ( 2.2 ) based on the simplification quality and then rewritten by the paraphrasing component ; ( 2 ) Used for data augmentation to improve the diversity of the paraphrasing model ( 2.3 ) .", "entities": [[112, 114, "TaskName", "data augmentation"]]}
{"text": "We leverage the state - of - the - art system for structural simplification , called DisSim ( Niklaus et al , 2019 ) , to generate candidate simplifications that focus on splitting and deletion . 2 The English version of DisSim applies 35 hand - crafted grammar rules to break down a complex sentence into a set of hierarchically organized sub - sentences ( see Figure 1 for an example ) . We choose a rule - based approach for sentence splitting because it works really well . In our pilot experiments , DisSim successfully split 92 % of 100 complex sentences from the training data with more than 20 words , and introduced errors for only 6.8 % of these splits . We consider these sub - sentences as candidate simplifications for the later steps , except those that are extremely short or long ( compression ratio / [ 0.5 , 1.5 ] ) . The compression ratio is calculated as the number of words in a candidate simplification v i ( which may contain one or more sub - sentences ) divided by that of the original sentence x. To further increase the variety of generated candidates , we supplement DisSim with a Neural Deletion and Split module trained on the text simplification corpus ( 3.1 ) . We use a Transformer seq2seq model with the same configuration as the base model for paraphrasing ( 2.3 ) . Given the input sentence x , we constrain the beam search to generate 10 outputs with splitting and another 10 outputs without splitting . Then , we select the outputs that do not deviate substantially from x ( i.e. , Jaccard similarity > 0.5 ) . We add outputs from the two systems to the candidate pool V .", "entities": [[214, 216, "TaskName", "text simplification"], [224, 225, "MethodName", "Transformer"], [225, 226, "MethodName", "seq2seq"]]}
{"text": "We can control our model to concentrate on specific operations . For split - or delete - focused simplification , we select candidates with desirable length or number of splits during the candidate generation step . We perform only the paraphrase generation step for paraphrase - focused simplification . The paraphrasing model is designed specifically to paraphrase with minimal deletion and without splitting . It retains the length and the number of split sentences in the output , thus preserving the extent of deletion and splitting controlled in the previous steps . We control the degree of paraphrasing by changing the copy ratio .", "entities": [[40, 42, "TaskName", "paraphrase generation"]]}
{"text": "We use the following simplification approaches as baselines : ( i ) BERT - Initialized Transfomer ( ? ) , where the encoder is initialized with BERT base checkpoint and the decoder is randomly initialized . It is the current state - of - the - art for text simplification . ( ii ) EditNTS ( Dong et al , 2019 ) , 6 another state - of - the - art model that uses a neural programmer - interpreter ( Reed and de Freitas , 2016 ) to predict the edit operation on each word , and then generates the simplified sentence . ( iii ) LSTM baseline , a vanilla encoderdecoder model used in Zhang and Lapata ( 2017 ) . ( iv ) Hybrid - NG ( Narayan and Gardent , 2014 ) , 7 one of the best existing hybrid systems that performs splitting and deletion using a probabilistic model and lexical substitution with a phrase - based machine translation system . We retrained all the models on the NEWSLA - AUTO dataset . Table 3 : Automatic evaluation results on NEWSELA - TURK that focuses on paraphrasing ( 500 complex sentences with 4 human written paraphrases ) . We control the extent of paraphrasing of our models by specifying the percentage of words to be copied ( cp ) from the input as a soft constraint .", "entities": [[12, 13, "MethodName", "BERT"], [26, 27, "MethodName", "BERT"], [48, 50, "TaskName", "text simplification"], [107, 108, "MethodName", "LSTM"], [162, 164, "TaskName", "machine translation"], [185, 186, "DatasetName", "NEWSELA"]]}
{"text": "We performed two human evaluations : one to measure the overall simplification quality and the other to specifically capture sentence splitting . 11 For the first one , we asked five Amazon Mechanical Turk workers to evaluate fluency , adequacy and simplicity of 100 random simplifications from the NEWSELA - AUTO test set . We supplemented the 2 - 3 readability levels in NEWSELA - AUTO , which contained more lexical overlaps and inflated the scores for EditNTS . 11 We provide instructions in Appendix E. fluency and adequacy ratings with binary questions described in Zhang et al ( 2020a ) for the second evaluation over another 100 simplifications from the NEWSELA - AUTO split - focused test set . We asked if the output sentence exhibits spitting and if the splitting occurs at the correct place . While fluency measures the grammaticality of the output , adequacy captures the extent of meaning preserved when compared to the input . Simplicity evaluates if the output is simpler than the input . Each sentence was rated on a 5 - point Likert scale and we averaged the ratings from the five workers . We chose the majority value for the binary ratings . We used the output of our model that is tailored for sentence splitting for the second evaluation . Table 6 demonstrates that our model achieves the best fluency , simplicity , and overall ratings . The adequacy rating is also very close to that of Transformer bert and EditNTS even though our model is performing more paraphrasing ( Table 2 ) , which verifies that the changes made by our system are meaningful . Our model achieves the most number of correct sentence splits ( 90 % ) , and the highest fluency ( 4.19 ) for syntactic simplification , showing that it can generate more number of coherent sentence splits when compared to other models .", "entities": [[48, 49, "DatasetName", "NEWSELA"], [63, 64, "DatasetName", "NEWSELA"], [111, 112, "DatasetName", "NEWSELA"], [247, 248, "MethodName", "Transformer"]]}
{"text": "We evaluate our key design choices , namely candidate ranking that is based on length - penalized BERTScore and paraphrase generation that uses data augmentation and copy attention . Table 8 summarizes the results . Our pairwise ranking model ( BERTScore len ) achieves an increase of 3.2 points in SARI when compared to choosing a random ( Random ) candidate . Randomly selecting a candidate also performs fairly well , indicating that the", "entities": [[19, 21, "TaskName", "paraphrase generation"], [23, 25, "TaskName", "data augmentation"]]}
{"text": "They float in and out of places that combine stage with the underwater . Compared to our final model ( Our Model ) , its variants without data augmentation ( \u2212 augmentation ) and copy mechanism ( \u2212 copy attn ) suffer a drop of 1.0 and 2.6 points in SARI respectively and a decrease of at least 3.0 % of new words , which demonstrates that these components encourage the system to paraphrase . Our model trained on only DisSim ( \u2212 only DisSim ) and Transformer ( \u2212 only Transformer ) candidates performs close to our best model ( Our Model ) in terms of SARI .", "entities": [[27, 29, "TaskName", "data augmentation"], [87, 88, "MethodName", "Transformer"], [91, 92, "MethodName", "Transformer"]]}
{"text": "To understand the errors generated by our model , we manually classified 200 simplifications from the NEWSELA - AUTO test set into the following categories : ( a ) Good , where the model generated meaningful simplifications , ( b ) Hallucinations , where the model introduced information not in the input , ( c ) Fluency Errors , where the model generated ungrammatical output , ( d ) Anaphora Resolution , where it was difficult to resolve pronouns in the output . ( e ) Bad substitution , where the model inserted an incorrect simpler phrase , and ( e ) Human Reference Errors , where the reference does not reflect the source sentence . Note that a simplification can belong to multiple error categories . Table 7 shows the examples of each category .", "entities": [[16, 17, "DatasetName", "NEWSELA"]]}
{"text": "Before the advent of neural networks , text simplification approaches performed each operation separately in a pipeline manner using either handcrafted rules ( Carroll et al , 1999 ; Siddharthan , 2002 ; Siddharthan et al , 2004 ) or data - driven methods based on parallel corpora ( Zhu et al , 2010 ; Woodsend and Lapata , 2011 ; Narayan and Gardent , 2014 ) . Following neural machine translation , the trend changed to performing all the operations together end - toend ( Zhang and Lapata , 2017 ; Nisioi et al , 2017 ; Zhao et al , 2018 ; Alva - Manchego et al , 2017 they discovered that the ship had been important . EditNTS since 2010 , project researchers have uncovered documents in portugal . have revealed who owned the ship . Our Model ( cp = 0.6 ) scientists have found a secret deal . they have discovered who owned the ship . Our Model ( cp = 0.7 ) scientists have found documents in portugal . they have also found out who owned the ship . Our Model ( cp = 0.8 ) scientists have found documents in portugal . they have discovered who owned the ship .", "entities": [[7, 9, "TaskName", "text simplification"], [70, 72, "TaskName", "machine translation"]]}
{"text": "We proposed a novel hybrid approach for sentence simplification that performs better and produces more diverse outputs than the existing systems . We designed a new data augmentation method to encourage the model to paraphrase . We created a new dataset , NEWSELA - TURK , to evaluate paraphrasing - focused simplifications . We showed that our model can control various attributes of the simplified text , such as number of sentence splits , length , and number of words copied from the input .", "entities": [[26, 28, "TaskName", "data augmentation"], [42, 43, "DatasetName", "NEWSELA"]]}
{"text": "This year , it approved dozens of permits for agricultural drone businesses . Hybrid - NG this year , the government has approved dozens of drone permits for agricultural businesses . LSTM this year , the faa has approved dozens of permits for agricultural drone businesses . Transformer bert this year , the faa has approved dozens of permits for agricultural businesses .", "entities": [[31, 32, "MethodName", "LSTM"], [47, 48, "MethodName", "Transformer"]]}
{"text": "As she spoke , the building echoed with music and the beat of drums . Hybrid - NG echoed the room . LSTM the room echoed with the sounds of song , the voices of young men . Transformer bert the room echoed with the sound of song , the beat of drums , the voices of young men .", "entities": [[22, 23, "MethodName", "LSTM"], [38, 39, "MethodName", "Transformer"]]}
{"text": "We thank the anonymous reviewers for their valuable feedback . We thank Newsela for sharing the data and NVIDIA for providing GPU computing resources . This research is supported in part by the NSF award IIS - 1822754 , ODNI and IARPA via the BETTER program contract 19051600004 . The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies , either expressed or implied , of NSF , ODNI , IARPA , or the U.S. Government . The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein .", "entities": [[12, 13, "DatasetName", "Newsela"]]}
{"text": "This paper describes our official entry LearningToQuestion for SemEval 2017 task 3 community question answer , subtask B. The objective is to rerank questions obtained in web forum as per their similarity to original question . Our system uses pairwise learning to rank methods on rich set of hand designed and representation learning features . We use various semantic features that help our system to achieve promising results on the task . The system achieved second highest results on official metrics MAP and good results on other search metrics .", "entities": [[51, 53, "TaskName", "representation learning"], [81, 82, "DatasetName", "MAP"]]}
{"text": "In online forums question answering is one of the most popular way for users to share information between each other . Due to the unstructured nature of these forums , it 's a problem to find relevant information from the already existing information for users . One way to solve this problem is to design systems to automatically find similar content ( question , answer , comment ) to the user 's posted question . SemEval - 2017 task 3 ( Nakov et al , 2017 ) focuses on solving this problem in community question answer by various subtasks of ranking relevant information in Qatar living forums data . The system presented in this paper focuses on subtask B , to re - rank given set of questions retrieved by search engine , in their similarity to original question . The system is mainly designed by employing learning to rank methods on the rich feature set obtained by text processing of the question text .", "entities": [[3, 5, "TaskName", "question answering"]]}
{"text": "We primarily use the annotated training , development and testing dataset provided by the SemEval - 2017 task 3 organizers . The dataset is collected by organizers from Qatar living forum . It 's in the form of an original question and set of related questions . Each related question in training and development dataset is annotated with one of the 3 possible tags , PerfectMatch , Relevant or Irrelevant . A ranking task is required to rank both Per - fectMatch and Relevant above Irrelevant questions without any distinction between the first two . The train dataset for subtask B consists of 317 original questions and 3169 retrieved questions by search engine roughly 10 related questions per original question . The organizers have also provided annotated test dataset from SemEval - 2016 challenge . Along with these we also used Glove embeddings ( Pennington et al , 2014 ) which were pretrained using 6 billion tokens from Wikipedia - 2014 and Gigaword dataset .", "entities": [[141, 143, "MethodName", "Glove embeddings"]]}
{"text": "Since the task is a ranking task , our system uses learning to rank ( Trotman , 2005 ) to model the ranking of questions . Learning to rank refers to various machine learning techniques used in ranking tasks . These have been studied in information retrieval literature and they power many of the industrial search engines . These systems mainly fall into 3 categories : pointwise , pairwise and listwise as described in . We use pairwise methods for our system with rich feature set . Our feature set is combination of various hand generated features and semantic features learned by neural network . In the following section we first describe these features and then the learning to rank method used .", "entities": [[45, 47, "TaskName", "information retrieval"]]}
{"text": "We use rank given by the search engine as a feature in our system . This gives the system the baseline accuracy of the search engine .", "entities": [[21, 22, "MetricName", "accuracy"]]}
{"text": "Topic modeling is used to generate the salient topics in the text . We use Latent Dirichlet al ocation ( LDA ) ( Blei et al , 2003 ) to compute topic similarity between texts . We train LDA topic model using the whole text ( body and subject ) as corpus . Then a topic distribution over the 50 topics is computed for both the text and cosine similarity is used as a feature in the system .", "entities": [[20, 21, "MethodName", "LDA"], [38, 39, "MethodName", "LDA"]]}
{"text": "We use pairwise learning to rank for ranking task which poses the ranking problem as classification problem to minimize the average number of inversions in ranking . This formulation is more closer to ranking task than predicting relevance as regression and also has theoretical guarantees of maximizing the MAP in ranking ( Chen et al , 2009 ) . First , we create these pairs by taking original question Q o and two candidate questions of which one was relevant and other one not , Q c1 and Q c2 . Then we generate above mentioned feature vectors f ( Q o , Q c1 ) , f ( Q o , Q c2 ) and use feature dif - ference f ( Q o , Q c1 ) \u2212 f ( Q o , Q c2 )", "entities": [[48, 49, "DatasetName", "MAP"]]}
{"text": "This paper presented a system which uses sophisticated learning to rank method with semantic features to obtain promising results on ranking similar questions . The paper shows that semantic features and pairwise learning are essential components to the system by ablation tests . In future , we would like to extend our neural architecture to attention based models which have shown success in recent times . We also plan to use Triplet loss ( Hoffer and Ailon , 2015 ) which captures ranking task in better way . Another direction is to use state - of - art listwise learning to rank methods that can directly optimize MAP .", "entities": [[71, 73, "MethodName", "Triplet loss"], [107, 108, "DatasetName", "MAP"]]}
{"text": "Compressing BERT : Studying the Effects of Weight Pruning on Transfer Learning", "entities": [[1, 2, "MethodName", "BERT"], [10, 12, "TaskName", "Transfer Learning"]]}
{"text": "BERT is a large Transformer encoder ; for background , we refer readers to Vaswani et al ( 2017 ) or one of these excellent tutorials ( Alammar , 2018 ; Klein et al , 2017 ) .", "entities": [[0, 1, "MethodName", "BERT"], [4, 5, "MethodName", "Transformer"]]}
{"text": "Weight Mean Weight STD embeddings word embeddings - 0.0282 0.042 layer 0 attention output FC - 0.0000 0.029 layer 0 self attn key 0.0000 0.043 layer 0 self attn query 0.0000 0.043 layer 0 self attn value - 0.0000 0 .", "entities": [[5, 7, "TaskName", "word embeddings"], [11, 12, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [33, 34, "DatasetName", "0"], [39, 40, "DatasetName", "0"]]}
{"text": "Question Generation for Reading Comprehension Assessment by Modeling How and What to Ask", "entities": [[0, 2, "TaskName", "Question Generation"], [3, 5, "TaskName", "Reading Comprehension"]]}
{"text": "Reading is integral to everyday life , and yet learning to read is a struggle for many young learners . During lessons , teachers can use comprehension questions to increase engagement , test reading skills , and improve retention . Historically such questions were written by skilled teachers , but recently language models have been used to generate comprehension questions . However , many existing Question Generation ( QG ) systems focus on generating literal questions from the text , and have no way to control the type of the generated question . In this paper , we study QG for reading comprehension where inferential questions are critical and extractive techniques can not be used . We propose a two - step model ( HTA - WTA ) that takes advantage of previous datasets , and can generate questions for a specific targeted comprehension skill . We propose a new reading comprehension dataset that contains questions annotated with story - based reading comprehension skills ( SBRCS ) , allowing for a more complete reader assessment . Across several experiments , our results show that HTA - WTA outperforms multiple strong baselines on this new dataset . We show that the HTA - WTA model tests for strong SCRS by asking deep inferential questions .", "entities": [[65, 67, "TaskName", "Question Generation"], [101, 103, "TaskName", "reading comprehension"], [150, 152, "TaskName", "reading comprehension"], [161, 163, "TaskName", "reading comprehension"]]}
{"text": "Reading is an invaluable skill , and is core to communicating in our digital age . Reading also supports other forms of development ; when children read , it sharpens their memory , and improves social skills ( Halliday , 1973 ; Mason , 2017 ) . Yet , statistics show that one out of five children in the U.S. face learning difficulties ( Shaywitz , 2005 ) , especially in reading ( Cornoldi and Oakhill , 2013 ) . The coronavirus pandemic beginning in 2020 had a huge impact on the early reading skills of many children , and threatens to leave a lasting impact on a whole generation of young readers ( Gupta and Jawanda , 2020 ) . The pandemic forced many children to learn online , putting in sharp relief the need for effective online education platforms . In particular , reading games have become popular , and can help fill the gap when teachers can not read in person with students . These platforms present students with short passages and associated comprehension questions . These questions are key to assessing a reader 's comprehension of a passage , and can also enhance learning ( Chua et al , 2017 ) . But , writing diverse and engaging comprehension questions is a nontrivial task . Teachers need to generate new comprehension questions whenever they incorporate new text into a curriculum . New text helps to keep material fresh and topical , and can allow teachers to customize lessons to the interests of a particular student cohort . After finding such custom reading material , teachers must write new comprehension questions to evaluate several reading aspects of comprehension ( e.g. understanding complex words , recalling events , etc . ) . Thus , to improve the educational process , and lighten the load on teachers , we need tools to automate Question Generation ( QG ) : the task of writing questions for a given passage . Generated questions can be either inferential or literal ( extractive ) questions . Literal questions can be answered using only information stated in the text , whereas inferential questions require additional information or reasoning . Previous works focused on this aspect of the questions in reading comprehension and discarded the comprehension skills ( e.g. close reading , predicting , figurative language , etc . ) ( Murakhovs ' ka et al , 2021 ) . We take inspiration from continual learning ( Parisi et al , 2019 ) , which orders a set of learning tasks to improve model performance . We begin by training a model on the general task of QG ( How to ask : HTA ) , and follow with our task of interest : generating a targeted question of a particular type ( What to ask : WTA ) . This paper focuses on the generation of questions for story - based reading comprehension skills ( SBRCS ) , which are varied and cover many aspects of reading comprehension . We create a QG dataset for SBRCS 1 . Although our aim in creating this dataset is to enrich educational applications , this dataset can be considered as a source for general QG and question answering ( QA ) systems in NLP . Our focus here is to build a question generator without answer supervision as the case in a reallife application , where a story only will be given as input . This is a challenging task , as many different questions can be generated from a story when there is no answer supervision . QG with answer supervision is another prevalent research line in the literature ( Zhao et al , 2018 ; Chen and Xu , 2021 ) . The contributions in this work are as follows : We build a novel QG dataset for SBRCS . The dataset contains advanced reading comprehension skills extracted from stories . We propose a two - steps method to generate skill - related questions from a given story . The method takes advantage of previous datasets to improve generalizability , and then , teaches a model how to ask predefined styles of questions . We demonstrate the efficiency of the proposed method after extensive experiments , and we investigate its performance in a few - shot learning setting . The rest of the paper is structured as follows . In the next section , we present an overview of the literature work . In Section 3 , we describe how we built our dataset . Section 4 describes the proposed methodology . The experimental setting is presented in Section 5 . The results and the analysis are presented in Section 6 . Finally , we draw some conclusions and possible future work for this study .", "entities": [[314, 316, "TaskName", "Question Generation"], [375, 377, "TaskName", "reading comprehension"], [409, 411, "TaskName", "continual learning"], [487, 489, "TaskName", "reading comprehension"], [502, 504, "TaskName", "reading comprehension"], [539, 541, "TaskName", "question answering"], [649, 651, "TaskName", "reading comprehension"], [718, 722, "TaskName", "few - shot learning"]]}
{"text": "QG has progressed rapidly due to new datasets and model improvements . Many different QG models have been proposed , starting for simple vanilla Sequence to Sequence Neural Networks models ( seq2seq ) Yuan et al , 2017 ) to the more recent transformer - based models ( Dong et al , 2019 ; Chan and Fan , 2019 ; Varanasi et al , 2020 ; Narayan et al , 2020 ; Bao et al , 2020 ) . Some QG systems use manual linguistic features in their models ( Harrison and Walker , 2018 ; Khullar et al , 2018 ; Liu et al , 2019a ; Dhole and Manning , 2020 ) , some consider how to select question - worthy content Li et al , 2019 ; Scialom et al , 2019 ; , and some systems explicitly model question types ( Duan et al , 2017 ; Sun et al , 2018 ; Kang et al , 2019 ; . The last group focused only on generating questions that start with specific interrogative words ( what , how , etc . ) . QG has been used to solve many real - life problems . For example , QG in conversational dialogue ( Gu et al , 2021 ; Shen et al , 2021 ; Liu et al , 2021b ) where models were taught to ask a series of coherent questions grounded in a QA style , QG based on visual input ( Mostafazadeh et al , 2016 ; Shin et al , 2018 ; Shukla et al , 2019 ) , and QG for deep questions such as mathematical , curiosity - driven , clinical , and examinationtype questions ( Liyanage and Ranathunga , 2019 ; Yue et al , 2020 ; Jia et al , 2021 ) .", "entities": [[24, 27, "MethodName", "Sequence to Sequence"], [31, 32, "MethodName", "seq2seq"]]}
{"text": "Despite the recent efforts for building reading comprehension QA datasets , to the best of our knowledge , none of the available datasets explored SBRCS . Questions in previous datasets ask only either inferential or literal questions from a given passage / story . Rogers et al ( 2020 ) , developed questions with general reasoning types based on text from news and blogs ( e.g. Quora ) . We believe that those texts sources are not rich enough to examine reasoning skills . Advanced reasoning skills ( e.g. Figurative Language ) are usually used in children 's stories to assess comprehension skills . Additionally , we use a extensive set of reading comprehension skills that deeply evaluates the abilities of the readers ( e.g. imagination skill by Visualizing ) . In the following , we will show how we built our dataset . Table 1 gives an overview of the dataset .", "entities": [[6, 8, "TaskName", "reading comprehension"], [112, 114, "TaskName", "reading comprehension"]]}
{"text": "Our stories ( passages ) are multi - genre , selfcontained narratives . This content variety leads annotators towards asking non - localized questions that test for more advanced reading comprehension skills . The stories are generated using several resources : 1 . acquired from free public domain content ( Gutenberg Project 2 ) , 2 . partnerships with a publishing house ( Blue Moon Publishers 3 ) and an educational curriculum development foundation ( The Reimagined Classroom 4 ) , and 3 . authored by two professional writers , ( the majority of the stories are from this last category ) . To provide good lexical coverage and diverse stories , we choose to write and collect stories that come from a varied set of genres ( e.g. science , social studies , fantasy , fairy tale , historical fiction , horror , mystery , adventure , etc . ) . In total , we collect 726 multi - domain stories . The stories ' lengths range from a single sentence to 113 sentences .", "entities": [[29, 31, "TaskName", "reading comprehension"]]}
{"text": "Given the fact that including more data in a reading comprehension system is important for gen - eralization ( Chung et al , 2018 ; Talmor and Berant , 2019 ) , and given that our created dataset has the SBRCS which are missed in previous datasets , we propose a two - steps method to generate skillrelated questions from a given story : HTA followed by WTA . HTA teaches the model the typical format for comprehension questions using large previously released datasets . We use two well - known datasets , SQuAD ( Rajpurkar et al , 2016 ) and Cos - mosQA ( Huang et al , 2019 ) . In Appendix A.3 , we add more details on both of these datasets . These previous datasets are not annotated with the question types outlined in Section 3.1 , so the HTA phase allows us to take advantage of those datasets . WTA guides the model to generate questions to test the specific comprehension skills enumerated in Section 3.1 . Thus , in HTA , we train ( fine - tune ) a model on large QG datasets , and then , we further train the model to teach the model what to ask ( WTA ) . For the generation model , we use the pre - trained Text - to - Text Transfer Transformer T5 ( Raffel et al , 2020 ) , which closely follows the encoder - decoder architecture of the transformer model ( Vaswani et al , 2017 ) . T5 is a SOTA model on multiple tasks , including QA .", "entities": [[9, 11, "TaskName", "reading comprehension"], [93, 94, "DatasetName", "SQuAD"], [228, 229, "MethodName", "Transformer"], [229, 230, "MethodName", "T5"], [258, 259, "MethodName", "T5"]]}
{"text": "Previous works showed that incorporating more data when training a reading comprehension model improves performance and generalizability ( Chung et al , 2018 ; Talmor and Berant , 2019 ) . However , we can not incorporate previously released datasets with our new one , as they do not include compatible question skills information . However , they do contain many well - formed and topical questions . Thus , we train a T5 model on SQuAD and CosmosQA datasets to teach the model how to ask questions . Previous neural question generation models take the passage as input , along with the answer . How - ever , encoders can pass all of the information in the input to the decoder , occasionally causing the generated question to contain the target answer . Since the majority of the questions in our created dataset are inferential questions , the answers are not explicitly given in the passages ( unlike extractive datasets ) . Thus , we feed the stories to the encoder , but withhold the answers . Unlike previous systems , we then train the model to generate the questions and answers . We propose this setting to generate fewer literal questions . During our experiments , we evaluated the effect of excluding the answers , and we found them useful to the system . In Figure 1 we show the input - output format of the model . The encoder input is structured as < STORY_TEXT > < /s > , where < /s > is the end - of - sentence token . The decoder generates multiple questionanswer pairs as < QUESTION_TOKENS>1 < as > < ANSWER_TOKENS>1 < sp > ... < QUESTION_TOKENS > n < as > < ANSWER_TOKENS > n < /s > , where < as > separates a question from its answer , and < sp > separates a question - answer pair from another . The model can generate more than one question - answer pair . We prepare the data to include all of a passage 's question - answer pairs in the decoder . Some passages include single question - answer pair , and some passages have up to fifteen pairs .", "entities": [[10, 12, "TaskName", "reading comprehension"], [73, 74, "MethodName", "T5"], [76, 77, "DatasetName", "SQuAD"], [78, 79, "DatasetName", "CosmosQA"], [91, 93, "TaskName", "question generation"]]}
{"text": "QG models take a passage / story as input and generate a question . The type of generated question is not controlled and is left for the system to decide it . Thus , the generated question is usually an undesired question . Thus , in order to control the style of the generated question , the system needs an indication about the skill that the system is expected to generate a question for . proposed a way to control the style of the generated questions ( e.g. what , how , etc . ) . The authors built a rulebased information extractor to sample meaningful inputs from a given text , and then learn a joint distribution of < answer , clue , question style > before asking the GPT2 model ( Radford et al , 2019 ) to generate questions . However , this distribution can only be learned using an extractive dataset ( e.g. SQuAD ) ; the model can not learn to generate inferential questions . To control the skill of the generated question , we use a specific prompt per skill , by defining a special token < SKILL_NAME > corresponding to the desired target skill , using the collected dataset . This helps us to control what to extract from the pretrained model . Thus , the encoder takes as input < SKILL_NAME > and < STORY_TEXT > , where < SKILL_NAME > indicates to the model for which skill the question should be generated ( see Figure 2 ) . The data format in the decoder is similar to the one in the HTA step , but here the model generates a single question - answer pair . As a result , the encoding of the < STORY_TEXT > will be based on the given < SKILL_NAME > . In this way , the model encodes the same story in a different representation when a different < SKILL_NAME > is given . A similar technique was used in the literature to include persona profiles in dialogue agents to produce more coherent and meaningful conversations .", "entities": [[157, 158, "DatasetName", "SQuAD"]]}
{"text": "To evaluate the performance of our model , we use a set of models that showed state - of - the - art results on several datasets . We obtain the results of those models by running their published GitHub code on our collected dataset . For all of the following baselines , we use SQuAD , CosmosQA , and the collected dataset for training and we test on the test part of the collected dataset : Vanilla Seq2seq ( Sutskever et al , 2014 ) : a basic encoder - decoder sequence learning system for machine translation . This model takes the story as input and generates a question . NQG - Seq : another Seq2seq that implements an attention layer on top of a bidirectional - LSTM encoder . The authors use two encoders , one to encode the sentence that has the answer , and another to encode the whole document . The model then is trained to generate questions . NQG - Max ( Zhao et al , 2018 ) 8 : a QG system with a maxout pointer mechanism and gated self - attention LSTM - based encoder to address the challenges of processing long text input . This model takes a passage and an answer as input and generate a question . The answer must be a sub span of the passage . CGC - QG ( Liu et al , 2019a ) : a Clue Guided Copy network for Question Generation , which is a sequence - to - sequence generative model with a copying mechanism that takes a passage and an answer ( as a span in the text ) and generate the question . The text representation in the encoder ( GRU network ) is represented using a variety of features such as GloVe vectors , POS information , answer position , clue word , etc . AnswerQuest ( Roemmele et al , 2021 ) : a pipeline model that uses as a first step a previous model to retrieve the relevant sentence that has the answer from a document . And then , the sentence is fed to a transformer - based sequence - to - sequence model that is enhanced with a copy mechanism . One - Step : a baseline that uses T5 model trained with all data in one step instead of having separate HTA and WTA steps . Because there is only a single step , the skill name is not included in the encoder 's input . T5 - WTA : the WTA model trained using T5 model as a seed model . The HTA training step is not used here . We use this baseline to evaluate the effect of training WTA using HTA . For all of the previous baselines that require the answer to be a sub - span in the passage , we use the semantic text similarity method that was proposed in ( Ghanem et al , 2019 ) to retrieve the most similar span in the passage . The method extracts several ngrams features from a claim and text spans , and then compute cosine similarity to get the most similar span . In this work , we replace the ngrams features of a text with embeddings extracted from RoBERTa model ( Liu et al , 2019b ) . This process has been done on the inferential questions as their answers are not clearly given in the text .", "entities": [[55, 56, "DatasetName", "SQuAD"], [57, 58, "DatasetName", "CosmosQA"], [78, 79, "MethodName", "Seq2seq"], [96, 98, "TaskName", "machine translation"], [116, 117, "MethodName", "Seq2seq"], [128, 129, "MethodName", "LSTM"], [181, 182, "MethodName", "maxout"], [189, 190, "MethodName", "LSTM"], [246, 248, "TaskName", "Question Generation"], [290, 291, "MethodName", "GRU"], [302, 303, "MethodName", "GloVe"], [384, 385, "MethodName", "T5"], [422, 423, "MethodName", "T5"], [431, 432, "MethodName", "T5"], [485, 487, "TaskName", "text similarity"], [550, 551, "MethodName", "RoBERTa"]]}
{"text": "In this paper , we presented a new reading comprehension dataset to assess reading skills using stories . Unlike previous datasets that focused on either inferential or literal questions , our dataset has nine different SBRCS , each contains inferential and literal questions . In addition to that , we proposed HTA - WTA model which uses two - steps fine - tuning processes to take advantage of previous datasets which have different question formats , and to learn how to ask skill - related questions . We evaluated the model on the collected dataset and compared it to several strong baselines . Our extensive experiments showed the effectiveness of the model . Additionally , HTA - WTA is able to generate high quality questions when only 10 % of the dataset is used ( \u223c240 instances ) . In future work , we plan to extend our dataset with additional skills , and to investigate how our model can be integrated into online educational platforms .", "entities": [[8, 10, "TaskName", "reading comprehension"]]}
{"text": "Data collection and Annotation . We made sure that the sources we use to collect stories do not prevent any kind of copyright infringement . The content distribution licenses were checked before any use . Additionally , we manually examined the stories and the created questions to ensure there are no privacy or ethical concerns , e.g. , toxic language , hate speech , or any bias against underrepresented groups . EyeRead has outreach programs in place to recruit writers from diverse populations , incorporate their writing into the online system , and properly compensate them for their work . Writers that created questions earned comparable hourly wages to those earned by salaried teachers in a summer program . We estimated the amount of time AMT workers need to finish a HIT and then we compensated them so that the payment rate was higher than the local living wage per hour . Each AMT worker received $ 0.41 USD for completing one HIT , which we estimated would take 1 minute . Bias in Language Models . Recently , many research works found that language models have several types of bias , e.g. gender , race , religion , etc . , and this is due to the data used to train them ( Liang et al , 2021 ) . Removing bias from language models completely is difficult , if not impossible ( Gonen and Goldberg , 2019 ) . Thus , here we acknowledge that the QG model we trained might cause ethical concerns , e.g. generating biased questions about stories ' characters . EyeRead is keenly aware of this , and continues to monitor both teacher and modelgenerated questions before they are integrated into their system .", "entities": [[61, 63, "DatasetName", "hate speech"]]}
{"text": "Identifying the place in a story where the author best describes or explains a key point . Also , it includes questions to identify the purpose of a quote or a sentence . This skill requires advanced reading comprehension ability from the reader since its answers can not be extracted directly from the story text , where inferential skills are needed .", "entities": [[37, 39, "TaskName", "reading comprehension"]]}
{"text": "There are three major approaches within literacy education to which teachers or schools subscribe : the whole - language approach ( Froese , 1996 ) ( which is the idea that if teachers simply give kids books , kids will learn how to read ) , the structural literacy approach ( Moats , 2019 ) ( which is the theory that letters sounds , words parts , and grammar rules must all be explicitly taught in order for students to be able to read successfully ) , and the balanced literacy approach ( Asselin , 1999 ) ( which basically blends the aforementioned two theories together , in the sense that students read authentic literature while also receiving targeted instruction in skills or strategies ) . In this work , we chose to use the balanced literacy approach as it benefits from both approaches and as it is the newest approach . At the beginning , we reviewed some of the most commonly used balanced literacy curricula that were released by publishing houses and universities . In particular , we devoted a lot of focus to the Readers and Writers Workshop Model 9 which is developed at Columbia University Teachers College , and to the documentations about reading levels that developed by Scholastic publishing house 10 . The Readers and Writers Workshop curricula were highly instrumental to us in breaking reading comprehension into sub - skills . Also , it is one of the most commonly used and referenced curricula among teachers . We reviewed the workshop materials to create a list of all of the skills that the workshop program highlighted . Then , we matched those against what was offered by Scholastic . This helped us create our primary list of skills . In this study , we are experimenting with nine skills out of around twenty skills . In this phase of the study , we are focusing on the most comprehensive and common skills . In the future , we will expand our work to include the rest of the skills .", "entities": [[230, 232, "TaskName", "reading comprehension"]]}
{"text": "In addition to the collected dataset , we use two well - known datasets , SQuAD and CosmosQA . We choose these two datasets because of their large size , and their focus on literal or inferential questions . SQuAD A reading comprehension dataset , consists of questions created by crowdworkers on a set of Wikipedia articles that cover a large set of topics ( from musical celebrities to abstract concepts ) , where the answer to every question is a span from the corresponding reading passage ( Rajpurkar et al , 2016 ) . This dataset can be considered as an extractive QA dataset . It is one of the largest QA datasets in the literature . In this work , we use SQuAD 2.0 version with discarding the questions that have no answers . The size of the dataset is 100 K paragraph / question / answer triplets . CosmosQA It is another reading comprehension dataset consisting of 35.6 K paragraph / question pairs that require commonsense - based reading comprehension . It is a collection of people 's everyday narratives , and it asks questions about the likely causes of events that require reasoning ( Huang et al , 2019 ) . We discard questions that have no answers in this dataset , resulting in 28 K paragraph / question / answer triplets .", "entities": [[15, 16, "DatasetName", "SQuAD"], [17, 18, "DatasetName", "CosmosQA"], [39, 40, "DatasetName", "SQuAD"], [41, 43, "TaskName", "reading comprehension"], [124, 125, "DatasetName", "SQuAD"], [151, 152, "DatasetName", "CosmosQA"], [155, 157, "TaskName", "reading comprehension"], [171, 173, "TaskName", "reading comprehension"]]}
{"text": "In the following , we elaborate more on the reading comprehension skills :", "entities": [[9, 11, "TaskName", "reading comprehension"]]}
{"text": "We present ADVISER 1 - an open - source , multi - domain dialog system toolkit that enables the development of multi - modal ( incorporating speech , text and vision ) , sociallyengaged ( e.g. emotion recognition , engagement level prediction and backchanneling ) conversational agents . The final Python - based implementation of our toolkit is flexible , easy to use , and easy to extend not only for technically experienced users , such as machine learning researchers , but also for less technically experienced users , such as linguists or cognitive scientists , thereby providing a flexible platform for collaborative research .", "entities": [[36, 38, "TaskName", "emotion recognition"]]}
{"text": "Dialog systems or chatbots , both text - based and multi - modal , have received much attention in recent years , with an increasing number of dialog systems in both industrial contexts such as Amazon Alexa , Apple Siri , Microsoft Cortana , Google Duplex , XiaoIce ( Zhou et al , 2018 ) and Furhat 2 , as well as academia such as MuMMER ( Foster et al , 2016 ) and Alana ( Curry et al , 2018 ) . However , open - source toolkits and frameworks for developing such systems are rare , especially for developing multi - modal systems comprised of speech , text , and vision . Most of the existing toolkits are designed for developing dialog systems focused only on core dialog components , with or without the option to access external speech processing services ( Bohus and Rudnicky , 2009 ; Baumann and Schlangen , 2012 ; Lison and Kennington , 2016 ; Ultes et al , 2017 ; Ortega et al , 2019 ; Lee et al , 2019 ) . To the best of our knowledge , there are only two toolkits , proposed in ( Foster et al , 2016 ) and ( Bohus et al , 2017 ) , that support developing dialog agents using multi - modal processing and social signals ( Wagner et al , 2013 ) . Both provide a decent platform for building systems , however , to the best of our knowledge , the former is not open - source , and the latter is based on the .NET platform , which could be less convenient for non - technical users such as linguists and cognitive scientists , who play an important role in dialog research . In this paper , we introduce a new version of ADVISER - previously a text - based , multi - domain dialog system toolkit ( Ortega et al , 2019 ) - that supports multi - modal dialogs , including speech , text and vision information processing . This provides a new option for building dialog systems that is open - source and Python - based for easy use and fast prototyping . The toolkit is designed in such a way that it is modular , flexible , transparent , and user - friendly for both technically experienced and less technically experienced users . Furthermore , we add novel features to AD - VISER , allowing it to process social signals and to incorporate them into the dialog flow . We believe that these features will be key to developing humanlike dialog systems because it is well - known that social signals , such as emotional states and engagement levels , play an important role in human computer interaction ( McTear et al , 2016 ) . However in contrast to open - ended dialog systems ( Weizenbaum , 1966 ) , our toolkit focuses on task - oriented applications ( Bobrow et al , 1977 ) , such as searching for a lecturer at the university ( Ortega et al , 2019 ) . The purpose we envision for dialog systems developed using our toolkit is not the same as the objective of a social chatbot such as XiaoIce ( Zhou et al , 2018 ) . Rather than promoting \" an AI companion with an emotional connection to satisfy the human need for communication , affection , and social belonging \" ( Zhou et al , 2018 ) , ADVISER helps develop dialog systems that support users in efficiently fulfilling concrete goals , while at the same time considering social signals such as emotional states and engagement levels so as to remain friendly and likeable .", "entities": [[44, 45, "DatasetName", "Google"], [541, 542, "TaskName", "chatbot"]]}
{"text": "The main objective of this work is to develop a multi - domain dialog system toolkit that allows for multi - modal information processing and that provides different modules for extracting social signals such as emotional states and for integrating them into the decision making process . The toolkit should be easy to use and extend for users of all levels of technical experience , providing a flexible collaborative research platform .", "entities": [[43, 45, "TaskName", "decision making"]]}
{"text": "Multi - modality The main challenges in handling multi - modality are a ) the design of a synchronization infrastructure and b ) the large range of different latencies from different modalities . To alleviate the former , we use the publisher / subscriber software pattern presented in section 4 to synchronize signals coming from different sources . This software pattern also allows for services to run in a distributed manner . By assigning computationally heavy tasks such as speech recognition and speech synthesis to a more powerful computing node , it is possible to reduce differences in latency when processing different modalities , therefore achieving more natural interactions . Socially - Engaged Systems Determining the ideal scope of a socially - engaged dialog system is a complex issue , that is which information should be extracted from users and how the system can best react to these signals . Here we focus on two major social signals : emotional states and engagement levels ( see section 3.1 ) , and maintain an internal user state to track them over the course of a dialog . Note that the toolkit is designed in such a way that any social signal could be extracted and leveraged in the dialog manager . In order to react to social signals extracted from the user , we provide an initial affective policy module ( see section 3.5 ) and an initial affective NLG module ( see section 3.7 ) , which could be easily extended to more sophisticated behavior . Furthermore , we provide a backchanneling module that enables the dialog system to give feedback to users during conversations . Utilizing these features could lead to increased trust and enhance the impression of an empathetic system .", "entities": [[79, 81, "TaskName", "speech recognition"], [82, 84, "TaskName", "speech synthesis"]]}
{"text": "We present the three modules of ADVISER for processing social signals : ( a ) emotion recognition , ( b ) engagement level prediction , and ( c ) backchanneling . Figure 1 illustrates an example of our system tracking emotion states and engagement levels .", "entities": [[15, 17, "TaskName", "emotion recognition"], [40, 41, "DatasetName", "emotion"]]}
{"text": "For recognizing a user 's emotional state , all three available modalities - text , audio , and vision - can potentially be exploited , as they can deliver complementary information ( Zeng et al , 2009 ) . Therefore , the emotion recognition module can subscribe to the particular input streams of interest ( see section 4 for details ) and apply emotion prediction either in a time - continuous fashion or discretely per turn . In our example implementation in the toolkit , we integrate speech emotion recognition , i.e. using the acoustic signal as features . Based on the work presented in ( Neumann and Vu , 2017 ) we use log Mel filterbank coefficients as input to convolutional neural networks ( CNNs ) . For the sake of modularity , three separate models are employed for predicting different types of labels : ( a ) basic emotions { angry , happy , neutral , sad } , ( b ) arousal levels { low , medium , high } , and ( c ) valence levels { negative , neutral , positive } . The models are trained on the IEMOCAP dataset ( Busso et al , 2008 ) . The output of the emotion recognition module consists of three predictions per user turn , which can then be used by the user state tracker ( see section 3.4 ) . For future releases , we plan to incorporate multiple training datasets as well as visual features . Engagement Level Prediction User engagement is closely related to states such as boredom and level of interest , with implications for user satisfaction and task success ( Forbes - Riley et al , 2012 ; Schuller et al , 2009 ) . In ADVISER , we assume that eye activity serves as an indicator of various mental states ( Schuller et al , 2009 ; Niu et al , 2018 ) and implement a gaze tracker that monitors the user 's direction of focus via webcam . Using OpenFace 2.2.0 , a toolkit for facial behavior analysis ( Baltrusaitis et al , 2018 ) , we extract the features gaze angle x and gaze angle y , which capture left - right and up - down eye movement , for each frame and compute the deviation from the central point of the screen . If the deviation exceeds a certain threshold for a certain number of seconds , the user is assumed to look away from the screen , thereby disengaging . Thus , the output of our engagement level prediction module is the binary decision { looking , not looking } . Both the spatial and temporal sensitivity can be adjusted , such that developers have the option to decide how far and how long the user 's gaze can stray from the central point until they are considered to be disengaged . In an adaptive system , this information could be used to select re - engagement strategies , e.g. using an affective template ( see section 3.7 ) . Backchanneling In a conversation , a backchannel ( BC ) is a soft interjection from the listener to the speaker , with the purpose of signaling acknowledgment or reacting to what was just uttered . Backchannels contribute to a successful conversation flow ( Clark and Krych , 2004 ) . Therefore , we add an acoustic backchannel module to create a more human - like dialog experience . For backchannel prediction , we extract 13 Mel - frequency - cepstral coefficients from the user 's speech signal , which form the input to the convolutional neural network based on Ortega et al ( 2020 ) . The model assigns one of three categories from the proactive backchanneling theory ( Goodwin , 1986 ) to each user utterance { no - backchannel , backchannel - continuer and backchannel - assessment } . The predicted category is used to add the backchannel realization , such as Right or Uh - huh , to the next system response .", "entities": [[42, 44, "TaskName", "emotion recognition"], [63, 64, "DatasetName", "emotion"], [87, 90, "TaskName", "speech emotion recognition"], [194, 195, "DatasetName", "IEMOCAP"], [208, 210, "TaskName", "emotion recognition"]]}
{"text": "Automatic Speech Recognition ( ASR ) The speech recognition module receives a speech signal as input , which can come from an internal or external microphone , and outputs decoded text . The specific realization of ASR can be interchanged or adapted , for example for new languages or different ASR methods . We provide an end - to - end ASR model for English based on the Transformer neural network architecture . We use the end - to - end speech processing toolkit ESPnet ( Watanabe et al , 2018 ) and the IMS - speech English multi - dataset recipe ( Denisov and Vu , 2019 ) , updated to match the LibriSpeech Transformer - based system in ESPnet ( Karita et al , 2019 ) and to include more training data . Training data comprises the LibriSpeech , Switchboard , TED - LIUM 3 , AMI , WSJ , Common Voice 3 , SWC , VoxForge and M - AILABS datasets with a total amount of 3249 hours . As input features , 80 - dimensional log Mel filterbank coefficients are used . Output of the ASR model is a sequence of subword units , which include single characters as well as combinations of several characters , making the model lexicon independent . Speech Synthesis For ADVISER 's voice output , we use the ESPnet - TTS toolkit , which is an extension of the ESPnet toolkit mentioned above . We use FastSpeech as the synthesis model speeding up mel - spectrogram generation by a factor of 270 and voice generation by a factor of 38 compared to autoregressive Transformer TTS ( Ren et al , 2019 ) . We use a Parallel Wave - GAN ( Yamamoto et al , 2020 ) to generate waveforms that is computationally efficient and achieves a high mean opinion score of 4.16 . The FastSpeech and WaveGAN models were trained with 24 hours of the LJSpeech dataset from a single speaker ( Ito , 2017 ) and are capable of generating voice output in real - time when using a GPU . The synthesis can run on any device in a distributed system . Additionally , we optimize the synthesizer for abbreviations , such as Prof. , Univ . , IMS , NLP , ECTS and PhD , as well as for German proper names , such as street names . These optimizations can be easily extended . Turn Taking To make interacting with the system more natural , we use a naive end - of - utterance detection . Users indicate the start of their turn by pressing a hotkey , so they can choose to pause the interaction . The highest absolute peak of each recording chunk is then compared with a predefined threshold . If a certain number of sequential chunks do not peak above the threshold , the recording stops . We are currenlty in the process of planning more sophisticated turn taking models , such as Skantze et al ( 2015 ) .", "entities": [[0, 3, "TaskName", "Automatic Speech Recognition"], [7, 9, "TaskName", "speech recognition"], [68, 69, "MethodName", "Transformer"], [84, 85, "MethodName", "ESPnet"], [114, 115, "DatasetName", "LibriSpeech"], [115, 116, "MethodName", "Transformer"], [120, 121, "MethodName", "ESPnet"], [139, 140, "DatasetName", "LibriSpeech"], [143, 147, "DatasetName", "TED - LIUM 3"], [152, 154, "DatasetName", "Common Voice"], [158, 159, "DatasetName", "VoxForge"], [216, 218, "TaskName", "Speech Synthesis"], [227, 228, "MethodName", "ESPnet"], [238, 239, "MethodName", "ESPnet"], [272, 273, "MethodName", "Transformer"], [288, 289, "MethodName", "GAN"], [316, 317, "MethodName", "WaveGAN"], [325, 326, "DatasetName", "LJSpeech"]]}
{"text": "The natural language understanding ( NLU ) unit parses the textual user input ( De Mori et al , 2008 ) - or the output from the speech recognition systemand extracts the user action type , generally referred to as intent in goal - oriented dialog systems ( e.g. Inform and Request ) , as well as the corresponding slots and values . The domain - independent , rulebased NLU presented in Ortega et al ( 2019 ) is integrated into ADVISER and adapted to the new domains presented in section 5 .", "entities": [[1, 4, "TaskName", "natural language understanding"], [27, 29, "TaskName", "speech recognition"], [42, 46, "TaskName", "goal - oriented dialog"]]}
{"text": "Belief State Tracking ( BST ) : The BST tracks the history of user informs and the user action types , requests , with one BST entry per turn . This information is stored in a dictionary structure that is built up , as the user provides more details and the system has a better understanding of user intent . User State Tracking ( UST ) : Similar to the BST , the UST tracks the history of the user 's state over the course of a dialog , with one entry per turn . In the current implementation , the user state consists of the user 's engagement level , valence , arousal , and emotion category ( details in section 3.1 ) .", "entities": [[116, 117, "DatasetName", "emotion"]]}
{"text": "Policies To determine the correct system action , we provide three types of policy services : a handcrafted and a reinforcement learning policy for finding entities from a database ( Ortega et al , 2019 ) , as well as a handcrafted policy for looking up information through an API call . Both handcrafted policies use a series of rules to help the user find a single entity or , once an entity has been found ( or directly provided by the user ) , find information about that entity . The reinforcement learning ( RL ) policy 's action - value function is approximated by a neural network which outputs a value for each possible system action , given the vectorized representation of a turn 's belief state as input . The neural network is constructed as proposed in following a duelling architecture ( Wang et al , 2016 ) . It consists of two separate calculation streams , each with its own layers , where the final layer yields the action - value function . For off - policy batch - training , we make use of prioritized experience replay ( Schaul et al , 2015 ) . Affective Policy In addition , we have also implemented a rule - based affective policy service that can be used to determine the system 's emotional response . As this policy is domain - agnostic , predicting the next system emotion output rather than the next system action , it can be used alongside any of the previously mentioned policies . User Simulator To support automatic evaluation and to train the RL policy , we provide a user simulator service outputting at the user acts level . As we are concerned with task - oriented dialogs here , the user simulator has an agenda - based ( Schatzmann et al , 2007 ) architecture and is randomly assigned a goal at the beginning of the dialog . Each turn , it then works to first respond to the system utterance , and then after to fulfill its own goal . When the system utterance also works toward fulfilling the user goal , the RL policy is rewarded by achieving a shorter total dialog turn count ( Ortega et al , 2019 ) .", "entities": [[189, 192, "MethodName", "prioritized experience replay"], [240, 241, "DatasetName", "emotion"]]}
{"text": "In the NLG service , the semantic representation of the system act is transformed into natural language . ADVISER currently uses a template - based approach to NLG in which each possible system act is mapped to exactly one utterance . A special syntax using placeholders reduces the number of templates needed and accounts for correct morphological inflections ( Ortega et al , 2019 ) . Additionally , we developed an affective NLG service , which allows for different templates to be used depending on the user 's emotional state . This enables a more sensitive / adaptive system . For example , if the user is sad and the system does not understand the user 's input , it might try to establish common ground to prevent their mood from getting worse due to the bad news . An example response would be \" As much as I would love to help , I am a bit confused \" rather than the more neutral \" Sorry I am a bit confused \" . One set of NLG templates can be specified for each possible emotional state . At runtime , the utterance is then generated from the template associated with the current system emotion and system action . 4 Software Architecture", "entities": [[204, 205, "DatasetName", "emotion"]]}
{"text": "Our toolkit allows for easy creation of a dialog system within a few lines of code as follows . As a first step , a dialog system object is initialized , which is responsible for coordinating the initialization and graceful termination of all dialog services . Talking about multiple domains in one dialog is enabled by creating a simple keywordbased domain tracker which is introduced as the first argument to the dialog system . To make the dialog multi - modal , speech and vision modules are introduced next , along with modules to extract engagement and emotion . So far , all of these modules are domain - agnostic and can be used as shared resources between all domains . Next , domaindependent services such as NLUs , BSTs and NLGs for weather and mensa , are added . The following shows an example dialog .", "entities": [[97, 98, "DatasetName", "emotion"]]}
{"text": "Other tools for building dialog systems include ConvLab ( Lee et al , 2019 ) , an open - source , textbased dialog system platform that supports both pipelined architectures and an end - to - end neural architecture . ConvLab also provides reusable components and supports multi - domain settings . Other systems are largely text - based , but offer the incorporation of external speech components . In - proTK ( Baumann and Schlangen , 2012 ) , for instance , in which modules communicate by networks via configuration files , uses ASR based on Sphinx - 4 and synthesis based on MaryTTS . Similarly , RavenClaw ( Bohus and Rudnicky , 2009 ) provides a framework for creating dialog managers ; ASR and synthesis components can be supplied , for example , by connecting to Sphinx and Kalliope . OpenDial ( Lison and Kennington , 2016 ) relies on probabilistic rules and provides options to connect to speech components such as Sphinx . Multidomain dialog toolkit - PyDial ( Ultes et al , 2017 ) supports connection to DialPort . As mentioned in the introduction , Microsoft Research 's \\psi is an open and extensible platform that supports the development of multi - modal AI systems ( Bohus et al , 2017 ) . It further offers audio and visual processing , such as speech recognition and face tracking , as well as output , such as synthesis and avatar rendering . And the MuMMER ( multimodal Mall Entertainment Robot ) project ( Foster et al , 2016 ) is based on the SoftBank Robotics Pepper platform , and thereby comprises processing of audio - , visual - and social signals , with the aim to develop a socially engaging robot that can be deployed in public spaces .", "entities": [[227, 229, "TaskName", "speech recognition"], [250, 251, "DatasetName", "Mall"]]}
{"text": "Dependency treebanks for Latin have a history that goes back to 2006 . For it was in that year that the first two projects kicked off : the Latin Dependency Treebank ( ldt ) ( Bamman and Crane , 2006 ) , featuring a small selection of texts by Classical authors ( currently around 50k nodes ) , and the Index Thomisticus Treebank ( \u0131t - tb ) ( Passarotti , 2011 ) , based on works written in the XIIIth century by Thomas Aquinas ( approximately 400k nodes ) . Later on , a third Latin treebank was created in the context of the pro\u0131el project ( Haug and J\u00f8hndal , 2008 ) , which includes the entire New Testament in Latin ( the so called Vulgata by Jerome ) and texts from the Classical era ( for a total of around 250k nodes ) . Most recently , a syntactically annotated corpus of original VIIIth - IXth century charters from Central Italy , called Late Latin Charter Treebank ( llct ; around 250k nodes ) , was made available ( Korkiakangas and Passarotti , 2011 ) . While the ldt , the \u0131t - tb and the llct have shared the same manual for syntactic annotation since the beginning of their respective projects ( Bamman et al , 2007 ) , the pro\u0131el treebank follows a slightly different style ( Haug , 2010 ) . Currently , all the Latin treebanks except the llct are available also in the Universal Dependencies collection ( UD ) ( Nivre et al , 2016 ) . 1 The existence of four treebanks for an ancient language like Latin is not surprising , reflecting the large diachronic ( as well as diatopic ) span of Latin texts , which are spread across a time frame of more than two millennia and in most areas of the Mediterranean and of what is called Europe today . Since Latin has represented for a long time a kind of lingua franca , the variety of its textual typologies is wide , including scientific treaties , literary works , philosophical texts and official documents . This aspect makes it impossible to build one textual corpus that alone can be sufficiently representative of \" Latin \" , just because there are too many varieties of Latin , which can be even very different from each other . 2 In order to cope with such a large variety , several collections of Latin texts are today available in digital format , like for instance the Perseus Digital Library 3 and the collection of Medieval Italian Latinity ALIM . 4 Besides textual resources , the centuries - old tradition of Latin lexicography resulted in the current availability of several digitized dictionaries , like for instance the Lewis - Short dictionary available at Perseus and the Thesaurus Linguae Latinae by the Bayerische Akademie der Wissenschaften in Munich . 5 A small Latin WordNet including around 9 , 000 lemmas is also available ( Minozzi , 2010 ) , as well as a derivational morphology lexicon called Word Formation Latin ( wfl ) ( Litta et al , 2016 ) . Just like for most other ( both modern and ancient ) languages , the interoperability issues imposed by the different formats , tag sets and annotation criteria of the linguistic resources for Latin severely limit their potential for exploitation and use . Indeed , linking linguistic resources to one another would maximize their contribution to linguistic analysis at multiple levels , be those lexical , morphological , syntactic , semantic or pragmatic . Thus , presently there is a growing interest in the interoperability of ( annotated ) corpora , lexical resources and Natural Language Processing ( NLP ) tools ( Ide and Pustejovsky , 2010 ) . So far , this was partially approached by building large infrastructures and databases of linguistic resources , like CLARIN , 6 DARIAH , 7 META - SHARE , 8 and EAGLE . 9 In the treebank area , the UD collection includes more than 100 treebanks sharing the same annotation guidelines and provides different tools for querying the treebanks on - line . 10 A relevant initiative of this kind is the Norwegian Infrastructure for the Exploration of Syntax and Semantics ( \u0131ness ) ( Ros\u00e9n et al , 2012 ) , which offers an open and easy - to - use platform for building , accessing , searching and visualizing treebanks through a web browser . 11 These collections and infrastructures enable to use and query various resources and tools from one common place on the web , but they do not provide a real interconnection between them , thus failing to achieve their interoperability . Instead , making linguistic resources interoperable requires that all types of annotation applied to a particular word / text get integrated into a common representation that enables access to the linguistic information conveyed in a linguistic resource or produced by an NLP tool ( Chiarcos , 2012 , p. 162 ) . Particularly , by applying the principles of Linked Data to linguistic resources 12 \" it is possible to follow links between existing resources to find other , related data and exploit network effects \" ( Chiarcos et al , 2013 , p. iii ) . 13 Despite their rich annotation ( ranging from tokenization to syntactic analysis ) , treebanks alone can not account for the linguistic complexity of the texts they include , which requires that information provided by different ( and currently available ) textual and lexical resources is interlinked and , thus , exploited to the best . To this aim , the LiLa : Linking Latin project ( 2018 - 2023 ) 14 was launched with the objective to interlink the wealth of linguistic resources and NLP tools for Latin developed thus far , in order to bridge the gap between raw language data , NLP and knowledge description ( Declerck et al , 2012 , p. 111 ) . LiLa addresses this challenge by building a collection of several data sets described using the same vocabulary and linked together , namely a Linked ( Open ) Data Knowledge Base of the linguistic resources ( and NLP tools ) for Latin currently available from different providers under various licences . After a brief description of the basic architecture of the LiLa Knowledge Base ( Section 2 ) , this paper focuses on the inclusion of three dependency treebanks for Latin into LiLa ( namely , the \u0131t - tb in two versions , pro\u0131el and the llct ) , presenting an example of a complex query crossing the treebanks and the other linguistic resources included so far in the Knowledge Base ( Section 3 ) .", "entities": [[251, 253, "DatasetName", "Universal Dependencies"], [255, 256, "DatasetName", "UD"], [677, 678, "DatasetName", "UD"]]}
{"text": "In order to achieve interoperability between linguistic resources and NLP tools , the LiLa Knowledge Base makes use of a set of Semantic Web and Linguistic Linked Open Data standards . These include ontologies to describe linguistic annotation ( OLiA ( Chiarcos and Sukhareva , 2015 ) ) , corpus annotation ( NIF ( Hellmann et al , 2013 ) , conll - rdf ( Chiarcos and F\u00e4th , 2017 ) ) and lexical resources ( Lemon ( Buitelaar et al , 2011 ) , Ontolex 15 ) . The Resource Description Framework ( RDF ) ( Lassila et al , 1998 ) is used to encode graphbased data structures to represent linguistic annotations in terms of triples , made of a predicate connecting two nodes ( a subject and its object ) . The SPARQL language is used to query the data recorded in the form of RDF triples ( Prud'Hommeaux et al , 2008 ) . The LiLa Knowledge Base is highly lexically - based , striking a balance between feasibility and granularity : its basic assumption is that textual resources are made of ( occurrences of ) words , lexical resources describe properties of words , and NLP tools process words . Figure 1 presents the basic architecture of the LiLa Knowledge Base , showing its main components and their relations . The Lemma is the key node type in LiLa . A Lemma is an ( inflected ) Form conventionally chosen as the citation form of a lexical item . Lemmas occur in Lexical Resources as canonical forms of lexical entries . Forms , too , can occur in lexical resources , like in a lexicon containing all of the forms of a language , as for instance in Tombeur ( 1998 ) . The occurrences of Forms in real texts are Tokens , which are provided by Textual Resources . Finally , NLP tools process either Forms regardless of their contextual use ( e.g. , a morphological analyzer ) , or Tokens ( e.g. , a PoS - tagger ) , or texts in Textual Resources ( e.g. , a tokenizer ) . Forms , Lemmas and Tokens can be assigned Morphological Features , like part of speech and gender . Since lemmas serve as the optimal interface between lexical resources , ( annotated ) corpora and NLP tools , the core of the LiLa Knowledge Base is a collection of citation forms for Latin . Interoperability can be achieved by linking the entries in lexical resources and the corpus tokens pointing to the same lemma . 16 The collection of citation forms of LiLa is built on top of the set of lemmas used by the morphological analyzer for Latin Lemlat ( Passarotti et al , 2017 ) . 17 Lemlat relies on a lexical basis resulting from the collation of three Latin dictionaries ( Georges andGeorges , 1913 1918 ; Glare , 1982 ; Gradenwitz , 1904 ) for a total of 40 , 014 lexical entries and 43 , 432 lemmas , as more than one lemma can be included in one lexical entry . This lexical basis was recently further enlarged by adding the Onomasticon provided by the 5th edition of Forcellini dictionary ( Budassi and Passarotti , 2016 ) and the entries from a large reference glossary for Medieval Latin , namely the Glossarium Mediae et Infimae Latinitatis ( du Cange et al , 1883 ( du Cange et al , 1887Cecchini et al , 2018 ) , leading to a total of around 150 , 000 lemmas . The linguistic resources currently linked in the LiLa Knowledge Base are stored in a triplestore using the Jena framework . 18 The Fuseki component exposes the data as a SPARQL end - point accessible over HTTP . The current prototype of the LiLa RDF triplestore database connects the following resources for Latin : ( a ) the collection of lemmas provided by Lemlat , ( b ) the wfl lexicon , and ( c ) three treebanks ( four by version ) : ( c.1 ) pro\u0131el in its UD version ( release 2.3 ) , ( c.2 - 3 ) the \u0131t - tb in both its UD 2.3 and original version , and ( c.4 ) a selection of 3 , 900 sentences ( 105 , 380 tokens ) of the llct .", "entities": [[226, 227, "DatasetName", "Lemma"], [236, 237, "DatasetName", "Lemma"], [430, 431, "DatasetName", "lemma"], [514, 515, "DatasetName", "lemma"], [688, 689, "DatasetName", "UD"], [707, 708, "DatasetName", "UD"]]}
{"text": "The Latin treebanks currently integrated into LiLa have been converted into RDF triples . As an example , Figure 2 represents a first result in the conversion and linking process . The figure shows a three - word sentence from the Vulgata ( Matt . 6.10 ) , taken from the UD 2.3 version of the pro\u0131el corpus : veniat regnum tuum ( \" thy kingdom come \" ) . The UD 2.3 tree for this sentence is shown in Figure 3 . 19 Tokens and sentences are defined using the NIF vocabulary . In the current , preliminary stage of the Knowledge Base , some information on the tokens , such as the list of morphological features , is still registered as a simple string of text . For instance , in Figure 2 this is the case of the string \" Case = Nom | Gen - der = Neut | Number = Sing \" , which is linked to the pro\u0131el token with ID s15924_2 ( for the word regnum \" kingdom \" ) via the relation conll : FEAT , linking the morphological features taken from files in the CoNLL - U format of UD . 20 Other types of tagging ( such as syntactic dependencies , or sentence boundaries ) are expressed by links between the nodes for tokens or sentences . For example , in Figure 2 , this is represented by the linking between the token s15924_2 ( regnum ) and the token s15924_1 ( veniat \" come \" ) via the relation conll : HEAD , representing that in the sentence the word veniat is the head of the word regnum , as can be seen from the tree in Figure 3 . Finally , a third group of linguistic annotations , like the part of speech , directly relate tokens to concepts from an ontology of linguistic data ( OLiA ) . 21 In Figure 2 , this is shown by the edge connecting the token s15924_2 ( regnum ) to the concept node olia : CommonNoun . Tokens are connected to the appropriate Lemma nodes recorded in the LiLa Knowledge Base . In Figure 2 , for instance , the token s15924_2 ( regnum ) is linked to lemma 34146 , which has written representation regnum . Via this connection , it becomes possible to access all the other information that is also pointing to that lemma . In the figure , the lemma 34146 is connected to a node for a lexical base ( 1133 ) , the same to which also lemmas rex \" king \" ( 34799 ) and regno \" to rule , to be king \" ( 34145 ) are attached . This means that lemmas regno , regnum and rex belong to the same \" word formation family \" , i.e. a set of lemmas sharing the same lexical base . The lemma regnum is also formed with the suffix \" - n \" ( represented by the node affix:111 in Figure 2 ) , the same found in e.g. fanum \" shrine \" ( not shown here for reasons of space ) . In the collection of citation forms included in LiLa , all the lemmas formed with the suffix \" - n \" are linked to affix:111 via the relation lemlat_base : hasSuffix , thus allowing to retrieve them in the Knowledge Base . The information about lexical bases and affixes is available thanks to the connection of the wfl lexicon in LiLa .", "entities": [[51, 52, "DatasetName", "UD"], [71, 72, "DatasetName", "UD"], [198, 199, "DatasetName", "UD"], [313, 314, "MethodName", "ontology"], [353, 354, "DatasetName", "Lemma"], [378, 379, "DatasetName", "lemma"], [406, 407, "DatasetName", "lemma"], [413, 414, "DatasetName", "lemma"], [488, 489, "DatasetName", "lemma"]]}
{"text": "In this section , we provide an example of the types of queries that the LiLa Knowledge Base can already support . As mentioned , one single query can extract data from all the multiple corpora and lexical resources linked to LiLa 's collection , and can also combine syntactic , lexical and morphological information beyond the type of annotation explicitly recorded in a single corpus . Consider , for instance , the case of a researcher interested in the relation between the syntactic role of subject and the semantic role of agent in Latin . One possible approach to study the question would be to start by collecting and analyzing the sentences where nouns formed with a typical morpheme for agent nouns like \" - ( t ) or \" ( common to several Indo - European languages ) are attested as subject of an active verb . Though the number of linguistic resources currently interlinked in LiLa is still small , it is already possible to design a single SPARQL query to extract this information from our RDF versions of pro\u0131el , \u0131t - tb ( UD version ) and llct . In what follows , we illustrate the results of a query that asks for an active ( or deponent ) verb governing a noun with the syntactic relation of subject in the three treebanks . By leveraging the connection between lemmas and the affixes in wfl , we add the additional constraint that the noun must be formed with the suffix \" - ( t ) or \" . This information , which is not encoded into the original treebanks , is now accessible thanks to the architecture based on Linked Open Data that LiLa adopts . The query allows us to extract 143 passages , with 80 different verbs and 58 agent nouns . One sample of the results , a sentence from Cicero 's Letters to Atticus ( 4.4a.2 ) retrieved from pro\u0131el , is reported in Example ( 1 ) . ( 1 ) gladiatores audio pugnare mirifice . ' I hear that your gladiators fight superbly . ' The subject - verb bigrams resulting from the query highlight interest lexical aspects in the language of the three corpora . As it is to be expected from the documentary nature of the texts provided by the llct treebank , the 10 occurrences found in this corpus all involve legal actors and events : the most frequent subject ( 4 occurrences ) is rector , the priest responsible for a rural church . The other actors are : dispensator \" treasurer \" , fideiussor \" bail \" , genitor \" parent \" and imperator \" emperor \" . In the \u0131ttb , on the other hand , the most frequent couplet is the one formed by the noun commentator \" interpreter \" and the verb dico \" to say \" ( 21 cases ) , where the assertions of a scholar are reported and discussed . Indeed , the verbs pointing to intellectual activities of scholars dominate in the results from the corpus of Thomas Aquinas : in addition to the most frequent dico ( 22 ) , other intellectual verbs include respondeo \" to reply \" ( 3 instances ) , fingo \" to imagine \" ( 2 ) , and intendo \" to mean \" ( 2 ) . Finally , pro\u0131el , which is more balanced between different genres , offers a more varied set of subject - verb couplets in its 57 results . As in Example ( 1 ) , where the noun gladiator \" gladiator \" is coupled with the verb pugnare \" to fight \" , we find several nouns and verbs from everyday life , or from the domain of the professions and human activities . Thus , for instance , we find 4 cases of fossor \" digger , ditcher \" joined with verbs like includo \" to shut in \" and incumbo \" to press upon \" , or 6 cases of pastor \" herdsman , shepherd \" with verbs like fugio \" to flee \" and secludo \" to shut off \" .", "entities": [[92, 93, "DatasetName", "agent"], [121, 122, "DatasetName", "agent"], [188, 189, "DatasetName", "UD"], [306, 307, "DatasetName", "agent"]]}
{"text": "Improving Adversarial Text Generation by Modeling the Distant Future", "entities": [[1, 3, "TaskName", "Adversarial Text"]]}
{"text": "Auto - regressive text generation models usually focus on local fluency , and may cause inconsistent semantic meaning in long text generation . Further , automatically generating words with similar semantics is challenging , and hand - crafted linguistic rules are difficult to apply . We consider a text planning scheme and present a model - based imitation - learning approach to alleviate the aforementioned issues . Specifically , we propose a novel guider network to focus on the generative process over a longer horizon , which can assist next - word prediction and provide intermediate rewards for generator optimization . Extensive experiments demonstrate that the proposed method leads to improved performance .", "entities": [[3, 5, "TaskName", "text generation"], [20, 22, "TaskName", "text generation"]]}
{"text": "Text generation is an important area of investigation within machine learning . Recent work has shown excellent performance on a number of tasks , by combining reinforcement learning ( RL ) and generative models . Example applications include image captioning ( Ren et al , 2017 ; Rennie et al , 2016 ) , text summarization ( Li et al , 2018b ; Paulus et al , 2017 ; Rush et al , 2015 ) , and adversarial text generation ( Guo et al , 2017 ; Lin et al , 2017 ; Zhu et al , 2018 ) . The sequence - to - sequence framework ( Seq2Seq ) ( Sutskever et al , 2014 ) is a popular technique for text generation . However , models from such a setup are typically trained to predict the next token given previous ground - truth tokens as input , causing what is termed exposure bias ( Ranzato et al , 2016 ) . By contrast , sequence - level training with RL provides an effective means of solving this challenge , by treating text generation as a sequential decision - making problem . By directly optimizing an evaluation score ( cumulative rewards ) ( Ranzato et al , 2016 ) , state - of - the - art results have been ob - tained in many text - generation tasks ( Paulus et al , 2017 ; Rennie et al , 2016 ) . However , one problem in such a framework is that rewards in RL training are particularly sparse , since a scalar reward is typically only available after an entire sequence has been generated . Furthermore , the recurrent models focus more on local fluency , and may cause inconsistent semantic meanings for long text generation . For RL - based text generation , most existing works rely on a model - free framework , which has been criticized for its high variance and poor sample efficiency ( Sutton and Barto , 1998 ) . On the other hand , while model - based RL methods do not suffer from these issues , they are usually difficult to train in complex environments . Further , a learned policy is usually restricted by the capacity of an environment model . Recent developments on model - based RL ( Gu et al , 2016 ; Kurutach et al , 2018 ; Nagabandi et al , 2017 ) combine the advantages of these two approaches , and have achieved improved performance by learning a model - free policy , assisted by an environment model . In addition , model - based RL has been employed recently to solve problems with extremely sparse rewards , with curiosity - driven methods ( Pathak et al , 2017 ) . In this paper , we propose a model - based imitation - learning method to overcome the aforementioned issues in text - generation tasks . Our main idea is to employ an explicit guider network to model the generation environment in the feature space of sentence tokens , used to emit intermediate rewards by matching the predicted features from the guider network and features from generated sentences . The guider network is trained to encode global structural information of training sentences , and thus is useful to guide next - token prediction in the generative process . Within the proposed framework , to assist the guider network , we also develop a new type of self - attention mechanism to provide high - level planning - ahead information and maintain consistent semantic meaning . Our experimental results demonstrate the effectiveness of proposed methods .", "entities": [[0, 2, "TaskName", "Text generation"], [38, 40, "TaskName", "image captioning"], [54, 56, "TaskName", "text summarization"], [77, 79, "TaskName", "adversarial text"], [108, 109, "MethodName", "Seq2Seq"], [122, 124, "TaskName", "text generation"], [183, 185, "TaskName", "text generation"], [297, 299, "TaskName", "text generation"], [304, 306, "TaskName", "text generation"]]}
{"text": "The model is illustrated in Figure 1 , with an autoeocoder ( AE ) structure for sentence feature extraction and generation . The encoder is shared for sentences from both training data and generated data , as explained in detail below . Overall , text generation can be formulated as an imitationlearning problem . At each timestep t , the agent , also called a generator ( which corresponds to the LSTM decoder ) , takes the current LSTM state as input , denoted as s t . The policy \u03c0 \u03c6 ( | s t ) parameterized by \u03c6 is a conditional generator , to generate the next token ( action ) given s t , the observation representing the current generated sentence . The objective of text generation is to maximize the total reward as in ( 4 ) . We detail the components for our proposed model in the following subsections .", "entities": [[12, 13, "MethodName", "AE"], [44, 46, "TaskName", "text generation"], [60, 61, "DatasetName", "agent"], [71, 72, "MethodName", "LSTM"], [78, 79, "MethodName", "LSTM"], [128, 130, "TaskName", "text generation"]]}
{"text": "As illustrated in Figure 2 , our framework naturally provides a way for style transfer , where the guider network plays the role of style selection , and the generator only focuses on maintaining content without considering the styles . To make the guider network focus on the guidance of styles , we assign the label l as the initial state s G 0 of the guider network . Specifically , at each step t , we feed the current sentence representation f t and label l into the guider network : O t = g ( s t\u22121 ) , w t = \u03d5 ( G \u03c8 ( s G t\u22121 , [ f t , l ] ) ) , ( 9 ) y t \u223c Multi ( 1 , softmax ( O t w t ) ) . ( 10 ) For the generator , we put an adversarial regularizer on the encoded latent s 0 ( X ) and penalize it if it contains the sentiment information , by maximizing the entropy , i.e. , max l p ( l | s 0 ( X ) ) log p ( l | s 0 ( X ) ) , where p is a pre - trained classifier . Intuitively , the generator gives candidate words represented by O t , while the guider makes a choice implicitly by w t based on the sentiment information . The sentiment information is contained in w t , while the content of the original sentence is represented by O t . To achieve styletransfer , one feeds the original sentence X with the target style label l to get the transferred sentence Y with style l. Following previous work ( Hu et al , 2017 ; Yang et al , 2018 ; Cheng et al , 2020 ) , we adopt a classifier as the discriminator and the soft - argmax approach ( Kusner and Miguel , 2016 ) for the update of generator instead of policy gradient ( Sutton and Barto , 1998 ) .", "entities": [[13, 15, "TaskName", "style transfer"], [63, 64, "DatasetName", "0"], [132, 133, "MethodName", "softmax"], [158, 159, "DatasetName", "0"], [186, 187, "DatasetName", "0"], [197, 198, "DatasetName", "0"]]}
{"text": "We conduct ablation studies on long text generation to investigate the improvements brought by each part of our proposed method . We first test the benefits of using the guider network . Among the methods compared , Guider is the standard MLE model with the guider network . We further compare RL training with i ) only final rewards , ii ) only feature - matching rewards , and iii ) combining both rewards , namely GMGAN . The results are shown in Table 6 . We observe that guider network plays an important role in improving the performance . RL training with final rewards given by a discriminator typically damages the generation quality , but feature - matching reward produces sentences with much better diversity due to the ability of exploration .", "entities": [[6, 8, "TaskName", "text generation"]]}
{"text": "( 1 ) A person and black wooden table . ( 2 ) A closeup of a window at night . ( 1 ) She added on a page where it was made clear more old but public got said . ( 2 ) I think she're guys in four years , and more after it played well enough . LeakGAN ( 1 ) A bathroom with a black sink and a white toilet next to a tub . ( 2 ) A man throws a Frisbee across the grass covered yard . ( 1 ) \" I 'm a fan of all the game , I think if that 's something that I 've not , \" she said , adding that he would not be decided . ( 2 ) The UK is Google ' s largest non - US market , he has added \" 20 , before the best team is amount of fewer than one or the closest home or two years ago .", "entities": [[137, 138, "DatasetName", "Google"]]}
{"text": "Original : the service was slow . Transferred : the service was fast and friendly . Original : i would never eat there again and would probably not stay there either . Transferred : i would definitely eat this place and i would recommend them . Table 8 : Generated samples of guided style transfer .", "entities": [[53, 55, "TaskName", "style transfer"]]}
{"text": "We have proposed a model - based imitationlearning framework for adversarial text generation , by introducing a guider network to model the generation environment . The guider network provides a plan - ahead mechanism for next - word selection . Furthermore , this framework can alleviate the sparse - reward issue , as the intermediate rewards are used to optimize the generator . Our proposed models are validated on both unconditional and conditional text generation , including adversarial text generation and non - parallel style transfer . We achieve improved performance in terms of generation quality and diversity for unconditional and conditional generation tasks .", "entities": [[10, 12, "TaskName", "adversarial text"], [72, 75, "TaskName", "conditional text generation"], [77, 79, "TaskName", "adversarial text"], [84, 86, "TaskName", "style transfer"]]}
{"text": "More Generated Samples of Text Generation Table 13 lists more generated samples on the proposed GMGAN and its baselines . From the experiments , we can see , ( i ) SeqGAN tends to generate shorter sentences , and the readability and fluency is very poor . ( ii ) LeakGAN tends to generate very long sentences , and usually longer than the original sentences . However , even with good locality fluency , its sentences usually are not semantically consistent . By contrast , our proposed GMGAN can generate sentences with similar length to the original sentences , and has good readability and fluency . This is also validated in the Human evaluation experiment .", "entities": [[4, 6, "TaskName", "Text Generation"]]}
{"text": "at time t + t. In the text generation setting , when t = 1 , we can exactly get the feature representation of the current generated sentence if the guider does not help the word selection . If not , we can not exactly get this feature extraction since the guider 's prediction partly determine next token . In practice , we use t = c = 4 , to give the guider planning ability , to help for word selection and guide sentence generation .", "entities": [[7, 9, "TaskName", "text generation"]]}
{"text": "The LSTM state of dimension for the generator is 300 , and the LSTM state of dimension for the guider is 300 . The dimension of word - embedding is 300 .", "entities": [[1, 2, "MethodName", "LSTM"], [13, 14, "MethodName", "LSTM"]]}
{"text": "Acknowledgement The authors would like to thank the anonymous reviewers for their insightful comments . The research was supported in part by DARPA , DOE , NIH , NSF and ONR .", "entities": [[22, 23, "DatasetName", "DARPA"]]}
{"text": "O t < l a t e x i t s h a 1 _ b a s e 6 4 = \" y l D Y v N J 9 i T d 4 H B V D R Q f 3 c g H t 3 s w = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L 9 6 s a G y h D W W z 3 b R L N 5 u w O x F K 6 E / w 4 k H F q / / I m / / G b Z u D V h 8 M P N 6 b Y W Z e m E p h 0 H W / n N L S 8 s r q W n m 9 s r G 5 t b 1 T 3 d 1 7 M E m m G f d Z I h P d D q n h U i j u o 0 D J 2 6 n m N A 4 l b 4 W j q 6 n f e u T a i E T d 4 z j l Q U w H S k S C U b T S 3 U 0 P e 9 W a W 3 d n I H + J V 5 A a F G j 2 q p / d f s K y m C t k k h r T 8 d w U g 5 x q F E z y S a W b G Z 5 S N q I D 3 r F U 0 Z i b I J + d O i F H V u m T K N G 2 F J K Z + n M i p 7 E x 4 z i 0 n T H F o V n 0 p u J / X i f D 6 D z I h U o z 5 I r N F 0 W Z J J i Q 6 d + k L z R n K M e W U K a F v Z W w I d W U o U 2 n Y k P w F l / + S / y T + k X d u z 2 t N S 6 L N M p w A I d w D B 6 c Q Q O u o Q k + M B j A E 7 z A q y O d Z + f N e Z + 3 l p x i Z h 9 + w f n 4 B q B x j Y 4 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" y l D Y v N J 9 i T d 4 H B V D R Q f 3 c g H t 3 s w = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L 9 6 s a G y h D W W z 3 b R L N 5 u w O x F K 6 E / w 4 k H F q / / I m / / G b Z u D V h 8 M P N 6 b Y W Z e m E p h 0 H W / n N L S 8 s r q W n m 9 s r G 5 t b 1 T 3 d 1 7 M E m m G f d Z I h P d D q n h U i j u o 0 D J 2 6 n m N A 4 l b 4 W j q 6 n f e u T a i E T d 4 z j l Q U w H S k S C U b T S 3 U 0 P e 9 W a W 3 d n I H + J V 5 A a F G j 2 q p / d f s K y m C t k k h r T 8 d w U g 5 x q F E z y S a W b G Z 5 S N q I D 3 r F U 0 Z i b I J + d O i F H V u m T K N G 2 F J K Z + n M i p 7 E x 4 z i 0 n T H F o V n 0 p u J / X i f D 6 D z I h U o z 5 I r N F 0 W Z J J i Q 6 d + k L z R n K M e W U K a F v Z W w I d W U o U 2 n Y k P w F l / + S / y T + k X d u z 2 t N S 6 L N M p w A I d w D B 6 c Q Q O u o Q k + M B j A E 7 z A q y O d Z + f N e Z + 3 l p x i Z h 9 + w f n 4 B q B x j Y 4 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" y l D Y v N J 9 i T d 4 H B V D R Q f 3 c g H t 3 s w = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L 9 6 s a G y h D W W z 3 b R L N 5 u w O x F K 6 E / w 4 k H F q / / I m / / G b Z u D V h 8 M P N 6 b Y W Z e m E p h 0 H W / n N L S 8 s r q W n m 9 s r G 5 t b 1 T 3 d 1 7 M E m m G f d Z I h P d D q n h U i j u o 0 D J 2 6 n m N A 4 l b 4 W j q 6 n f e u T a i E T d 4 z j l Q U w H S k S C U b T S 3 U 0 P e 9 W a W 3 d n I H + J V 5 A a F G j 2 q p / d f s K y m C t k k h r T 8 d w U g 5 x q F E z y S a W b G Z 5 S N q I D 3 r F U 0 Z i b I J + d O i F H V u m T K N G 2 F J K Z + n M i p 7 E x 4 z i 0 n T H F o V n 0 p u J / X i f D 6 D z I h U o z 5 I r N F 0 W Z J J i Q 6 d + k L z R n K M e W U K a F v Z W w I d W U o U 2 n Y k P w F l / + S / y T + k X d u z 2 t N S 6 L N M p w A I d w D B 6 c Q Q O u o Q k + M B j A E 7 z A q y O d Z + f N e Z + 3 l p x i Z h 9 + w f n 4 B q B x j Y 4 = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" X / B b P P Q R M 1 p m B h x d K 1 e n S b L + g J w = \" > A A A B 2 H i c b Z D N S g M x F I X v 1 L 8 6 V q 1 r N 8 E i u C o z b t S d 4 M Z l B c c W 2 q F k M n f a 0 E x m S O 4 I p f Q F X L h R f D B 3 v o 3 p z 0 K t B w I f 5 y T k 3 p O U S l o K g i + v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z 8 W S L y g i M R K E K 0 0 u 4 R S U 1 R i R J Y a 8 0 y P N E Y T e Z 3 C 3 y 7 j M a K w v 9 S N M S 4 5 y P t M y k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p + G J Q U z 7 g h K R T O / U F l s e R i w k f Y d 6 h 5 j j a e L c e c s 3 P n p C w r j D u a 2 N L 9 + W L G c 2 u n e e J u 5 p z G 9 m + 2 M P / L + h V l 1 / F M 6 r I i 1 G L 1 U V Y p R g V b 7 M x S a V C Q m j r g w k g 3 K x N j b r g g 1 4 z v O g j / b r w J 0 W X 7 p h 0 + B F C H U z i D C w j h C m 7 h H j o Q g Y A U X u D N G 3 u v 3 v u q q p q 3 7 u w E f s n 7 + A a q K Y o N < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" L Q y c e l W j n / 7 H 2 e E g a t v n P 8 L r W o I = \" > A A A B 3 n i c b Z D N S g M x F I X v + F t r 1 e r W T b A I r s q M G 3 U n u H F n R c c W 2 q F k 0 j t t a C Y z J H e E U v o I b l y o + F j u f B v T n 4 W 2 H g h 8 n J O Q e 0 + c K 2 n J 9 7 + 9 t f W N z a 3 t 0 k 5 5 t 7 K 3 f 1 A 9 r D z Z r D A C Q 5 G p z L R i b l F J j S F J U t j K D f I 0 V t i M h z f T v P m M x s p M P 9 I o x y j l f S 0 T K T g 5 6 + G u S 9 1 q z a / 7 M 7 F V C B Z Q g 4 U a 3 e p X p 5 e J I k V N Q n F r 2 4 G f U z T m h q R Q O C l 3 C o s 5 F 0 P e x 7 Z D z V O 0 0 X g 2 6 o S d O q f H k s y 4 o 4 n N 3 N 8 v x j y 1 d p T G 7 m b K a W C X s 6 n 5 X 9 Y u K L m M x l L n B a E W 8 4 + S Q j H K 2 H R v 1 p M G B a m R A y 6 M d L M y M e C G C 3 L t l F 0 J w f L K q x C e 1 6 / q w b 0 P J T i G E z i D A C 7 g G m 6 h A S E I 6 M M L v M G 7 p 7 x X 7 2 P e 1 p q 3 q O 0 I / s j 7 / A G F 6 Y w 6 < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" L Q y c e l W j n / 7 H 2 e E g a t v n P 8 L r W o I = \" > A A A B 3 n i c b Z D N S g M x F I X v + F t r 1 e r W T b A I r s q M G 3 U n u H F n R c c W 2 q F k 0 j t t a C Y z J H e E U v o I b l y o + F j u f B v T n 4 W 2 H g h 8 n J O Q e 0 + c K 2 n J 9 7 + 9 t f W N z a 3 t 0 k 5 5 t 7 K 3 f 1 A 9 r D z Z r D A C Q 5 G p z L R i b l F J j S F J U t j K D f I 0 V t i M h z f T v P m M x s p M P 9 I o x y j l f S 0 T K T g 5 6 + G u S 9 1 q z a / 7 M 7 F V C B Z Q g 4 U a 3 e p X p 5 e J I k V N Q n F r 2 4 G f U z T m h q R Q O C l 3 C o s 5 F 0 P e x 7 Z D z V O 0 0 X g 2 6 o S d O q f H k s y 4 o 4 n N 3 N 8 v x j y 1 d p T G 7 m b K a W C X s 6 n 5 X 9 Y u K L m M x l L n B a E W 8 4 + S Q j H K 2 H R v 1 p M G B a m R A y 6 M d L M y M e C G C 3 L t l F 0 J w f L K q x C e 1 6 / q w b 0 P J T i G E z i D A C 7 g G m 6 h A S E I 6 M M L v M G 7 p 7 x X 7 2 P e 1 p q 3 q O 0 I / s j 7 / A G F 6 Y w 6 < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" k l y C Z f z i j r C U f s q x X J j p o q f K G D A = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 m 8 q L e i F 2 9 W N L b Q h r L Z b t q l m 0 3 Y n Q g l 9 C d 4 8 a D i 1 X / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e m E p h 0 H W / n d L K 6 t r 6 R n m z s r W 9 s 7 t X 3 T 9 4 N E m m G f d Z I h P d D q n h U i j u o 0 D J 2 6 n m N A 4 l b 4 W j 6 6 n f e u L a i E Q 9 4 D j l Q U w H S k S C U b T S / W 0 P e 9 W a W 3 d n I M v E K 0 g N C j R 7 1 a 9 u P 2 F Z z B U y S Y 3 p e G 6 K Q U 4 1 C i b 5 p N L N D E 8 p G 9 E B 7 1 i q a M x N k M 9 O n Z A T q / R J l G h b C s l M / T 2 R 0 9 i Y c R z a z p j i 0 C x 6 U / E / r 5 N h d B H k Q q U Z c s X m i 6 J M E k z I 9 G / S F 5 o z l G N L K N P C 3 k r Y k G r K 0 K Z T s S F 4 i y 8 v E / + s f l n 3 7 t x a 4 6 p I o w x H c A y n 4 M E 5 N O A G m u A D g w E 8 w y u 8 O d J 5 c d 6 d j 3 l r y S l m D u E P n M 8 f n z G N i g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" y l D Y v N J 9 i T d 4 f N e Z + 3 l p x i Z h 9 + w f n 4 B q B x j Y 4 = < / l a t e x i t > f t < l a t e x i t s h a 1 _ b a s e 6 4 = \" z U m 7 9 l o 9 e Y U U g i 5 D W 6 o p C U a 4 f s s = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R z d R v P X F t R K I e c J z y I K Y D J S L B K F r p P u p h r 1 p z 6 + 4 M Z J l 4 B a l B g W a v + t X t J y y L u U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" z U m 7 9 l o 9 e Y U U g i 5 D W 6 o p C U a 4 f s s = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R z d R v P X F t R K I e c J z y I K Y D J S L B K F r p P u p h r 1 p z 6 + 4 M Z J l 4 B a l B g W a v + t X t J y y L u U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" z U m 7 9 l o 9 e Y U U g i 5 D W 6 o p C U a 4 f s s = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R z d R v P X F t R K I e c J z y I K Y D J S L B K F r p P u p h r 1 p z 6 + 4 M Z J l 4 B a l B g W a v + t X t J y y L u U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" X / B b P o 3 p z 0 K t B w I f 5 y T k 3 p O U S l o K g i + v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z 8 W S L y g i M R K E K 0 0 u 4 R S U 1 R i R J Y a 8 0 y P N E Y T e Z 3 C 3 y 7 j M a K w v 9 S N M S 4 5 y P t M y k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p + G J Q U z 7 g h K R T O / U F l s e R i w k f Y d 6 h 5 j j a e L c e c s 3 P n p C w r j D u a 2 N L 9 + W L G c 2 u n e e J u 5 p z G 9 m + 2 M P / L + h V l 1 / F M 6 r I i 1 G L 1 U V Y p R g V b 7 M x S a V C Q m j r g w k g 3 K x N j b r g g 1 4 z v O g j / b r w J 0 W X 7 p h 0 + B F C H U z i D C w j h C m 7 h H j o Q g Y A U X u D N G 3 u v 3 v u q q p q 3 7 u w E f s n 7 + A a q K Y o N < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" Y t v F 5 e c 6 C w p 4 w r M A A D n k U 2 D S P P I = \" > A A A B 3 n i c b Z D N S g M x F I X v 1 L 9 a q 1 a 3 b o J F c F V m 3 K g 7 w Y 3 L i o 4 t t E P J p H f a 0 E x m S O 4 I p f Q R 3 L h Q 8 b H c + T a m P w t t P R D 4 O C c h 9 5 4 4 V 9 K S 7 3 9 7 p Y 3 N r e 2 d 8 m 5 l r 7 p / c F g 7 q j 7 Z r D A C Q 5 G p z L R j b l F J j S F J U t j O D f I 0 V t i K R 7 e z v P W M x s p M P 9 I 4 x y j l A y 0 T K T g 5 6 y H p U a 9 W 9 x v + X G w d g i X U Y a l m r / b V 7 W e i S F G T U N z a T u D n F E 2 4 I S k U T i v d w m L O x Y g P s O N Q 8 x R t N J m P O m V n z u m z J D P u a G J z 9 / e L C U + t H a e x u 5 l y G t r V b G b + l 3 U K S q 6 i i d R 5 Q a j F 4 q O k U I w y N t u b 9 a V B Q W r s g A s j 3 a x M D L n h g l w 7 F V d C s L r y O o Q X j e t G c O 9 D G U 7 g F M 4 h g E u 4 g T t o Q g g C B v A C b / D u K e / V + 1 i 0 V f K W t R 3 D H 3 m f P 6 f f j F E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" Y t v F 5 e c 6 C w p 4 w r M A A D n k U 2 D S P P I = \" > A A A B 3 n i c b Z D N S g M x F I X v 1 L 9 a q 1 a 3 b o J F c F V m 3 K g 7 w Y 3 L i o 4 t t E P J p H f a 0 E x m S O 4 I p f Q R 3 L h Q 8 b H c + T a m P w t t P R D 4 O C c h 9 5 4 4 V 9 K S 7 3 9 7 p Y 3 N r e 2 d 8 m 5 l r 7 p / c F g 7 q j 7 Z r D A C Q 5 G p z L R j b l F J j S F J U t j O D f I 0 V t i K R 7 e z v P W M x s p M P 9 I 4 x y j l A y 0 T K T g 5 6 y H p U a 9 W 9 x v + X G w d g i X U Y a l m r / b V 7 W e i S F G T U N z a T u D n F E 2 4 I S k U T i v d w m L O x Y g P s O N Q 8 x R t N J m P O m V n z u m z J D P u a G J z 9 / e L C U + t H a e x u 5 l y G t r V b G b + l 3 U K S q 6 i i d R 5 Q a j F 4 q O k U I w y N t u b 9 a V B Q W r s g A s j 3 a x M D L n h g l w 7 F V d C s L r y O o Q X j e t G c O 9 D G U 7 g F M 4 h g E u 4 g T t o Q g g C B v A C b / D u K e / V + 1 i 0 V f K W t R 3 D H 3 m f P 6 f f j F E = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" R o g s 9 b w 4 p c z S b P x 9 x Q I s 8 j H X A R A = \" > A A A B 6 X i c b V B N T 8 J A E J 3 i F + I X 6 t H L R m L i i b R e x B v R i 0 e M V k i g I d t l C x u 2 2 2 Z 3 a k I a f o I X D 2 q 8 + o + 8 + W 9 c o A c F X z L J y 3 s z m Z k X p l I Y d N 1 v p 7 S 2 v r G 5 V d 6 u 7 O z u 7 R 9 U D 4 8 e T Z J p x n 2 W y E R 3 Q m q 4 F I r 7 K F D y T q o 5 j U P J 2 + H 4 Z u a 3 n 7 g 6 L 8 + 5 8 L F p L T j F z D H / g f P 4 A w i S N o Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" z U m 7 9 l o 9 e Y U U g i 5 D W 6 o p C U a 4 f s s = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R z d R v P X F t R K I e c J z y I K Y D J S L B K F r p P u p h r 1 p z 6 + 4 M Z J l 4 B a l B g W a v + t X t J y y L u U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" z U m 7 9 l o 9 e Y U U g i 5 D W 6 o p C U a 4 f s s = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R z d R v P X F t R K I e c J z y I K Y D J S L B K F r p P u p h r 1 p z 6 + 4 M Z J l 4 B a l B g W a v + t X t J y y L u U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" z U m 7 9 l o 9 e Y U U g i 5 D W 6 o p C U a 4 f s s = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R z d R v P X F t R K I e c J z y I K Y D J S L B K F r p P u p h r 1 p z 6 + 4 M Z J l 4 B a l B g W a v + t X t J y y L u U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" z U m 7 9 l o 9 e Y U U g i 5 D W 6 o p C U a 4 f s s = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R z d R v P X F t R K I e c J z y I K Y D J S L B K F r p P u p h r 1 p z 6 + 4 M Z J l 4 B a l B g W a v + t X t J y y L u U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" z U m 7 9 l o 9 e Y U U g i 5 D W 6 o p C U a 4 f s s = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R z d R v P X F t R K I e c J z y I K Y D J S L B K F r p P u p h r 1 p z 6 + 4 M Z J l 4 B a l B g W a v + t X t J y y L u U I m q T E d z 0 0 7 y 1 5 B Q z h / A H z u c P w 2 S N p Q = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" z U m 7 9 l o 9 e Y U U g i 5 D W 6 o p C U a 4 f s s = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R z d R v P X F t R K I e c J z y I K Y D J S L B K F r p P u p h r 1 p z 6 + 4 M Z J l 4 B a l B g W a v + t X t J y y L u U I m q T E d z 0 0 x H a t E r x f + 8 f q 7 j C 7 9 g I s s 1 F W T + U J x z p F N U t o A i J i n R f G I I J p K Z r I i M s M R E m 6 4 a p g R 3 8 c v L x D t t X 7 b d 2 7 N W 5 6 p q o w 6 H c A Q n 4 M I 5 d O A G u u A B g R y e 4 R X e r C f r x X q 3 P u a j N a v a O Y A / s D 5 / A O d + k s o = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" X / B b P U F l s e R i w k f Y d 6 h 5 j j a e L c e c s 3 P n p C w r j D u a 2 N L 9 + W L G c 2 u n e e J u 5 p z G 9 m s y I y w Q o T Y 7 t q 2 B L 8 9 Z U 3 S X D Z 6 X b 8 e w / q c A p n c A E + X M E N 3 E E P A i B Q w A u 8 w b v z 7 L w 6 H 8 u 2 a s 6 q t h P 4 A + f z B 5 X m k W g = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" c X m s y I y w Q o T Y 7 t q 2 B L 8 9 Z U 3 S X D Z 6 X b 8 e w / q c A p n c A E + X M E N 3 E E P A i B Q w A u 8 w b v z 7 L w 6 H 8 u 2 a s 6 q t h P 4 A + f z B 5 X m k W g = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" f s H x Z T 9 a L 9 W 5 9 L E Z r V r V z B H 9 g f f 4 A 5 j 6 S x g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" v 8 T 3 v g o R 2 Q P y V G + h 5 W x O e l G p n 5 I = \" > A A A B 9 3 i c b V B P S 8 M w H P 1 1 / p v z z 6 o e v Q S H 4 G m 0 I q i 3 o R e P E 6 w O t l L S N N 3 C 0 r Q k q T D L P o k X D y p e / S r e / D a m W w + 6 + S D k 8 G B R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 G B R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V m h N N p z c 8 U T T E Z 4 y H t G S p w T F W Q z 6 J P 0 b F R I j R I p D l C o 5 n 6 e y P H s S q y m c k Y 6 5 F a 9 A r x P 6 + X 6 c F F k D O R Z p o K M n 9 o k H G k E 1 T 0 g C I m K d F 8 Y g g m k p m s i I y w x E S b t m q m B H f x y 8 v E O 2 1 e N t 3 b s 0 b r q m y j C o d w B C f g w j m 0 4 A b a 4 A G B R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" X / B b P o 3 p z 0 K t B w I f 5 y T k 3 p O U S l o K g i + v t r W 9 s 7 t X 3 / c P G v 7 h 0 X G z 8 W S L y g i M R K E K 0 0 u 4 R S U 1 R i R J Y a 8 0 y P N E Y T e Z 3 C 3 y 7 j M a K w v 9 S N M S 4 5 y P t M y k 4 O S s z r D Z C t r B U m w T w j W 0 Y K 1 h 8 3 O Q F q L K U Z N Q 3 N p + G J Q U z 7 g h K R T O / U F l s e R i w k f Y d 6 h 5 j j a e L c e c s 3 P n p C w r j D u a 2 N L 9 + W L G c 2 u n e e J u 5 p z G 9 m + 2 M P / L + h V l 1 / F M 6 r I i 1 G L 1 U V Y p R g V b 7 M x S a V C Q m j r g w k g 3 K x N j b r g g 1 4 z v O g j / b r w J 0 W X 7 p h 0 + B F C H U z i D C w j h C m 7 h H j o Q g Y A U X u D N G 3 u v 3 v u q q p q 3 7 u w E f s n 7 + A a q K Y o N < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" k v u 1 4 B G D B I h R l F R I G u 9 Z 3 v M Y U y g = \" > A A A B 7 n i c b V C 9 T s M w G P x S / k o p k L K y W F R I T F X C A m x I L I x F I r R S E 1 W O 4 7 R W H T u y H V A V + i g s D I B 4 H D b e B q f t A C 0 n f f L p z p b v u z j n T B v P + 3 Z q G 5 t b 2 z v 1 3 c Z e c / / g 0 G 0 1 H 7 Q s F K E B k V y q f o w 1 5 U z Q w D D D a T 9 X F G c x p 7 1 4 c l P 5 v U e q N J P i 3 k x z G m V 4 J F j K C D Z W G r q t s A x j y R M 9 z e y B + u F s 6 L a 9 j j c H W i f + k r R h i e 7 Q / Q o T S Y q M C k M 4 1 n r g e 7 m J S q w M I 5 z O G m G h a Y 7 J B I / o w F K B M 6 q j c h 5 9 h k 6 t k q B U K j v C o L n 6 + 0 W J M 1 1 l s z c z b M Z 6 1 a v E / 7 x B Y d L L q G Q i L w w V Z P F R W n B k J K p 6 Q A l T l B g + t Q Q T x W x W R M Z Y Y W J s W w 1 b g r + 6 8 j o J z j t X H f / O g z o c w w m c g Q 8 X c A 2 3 0 I U A C D z B C 7 z B u / P s v D o f i 7 Z q z r K 2 I / g D 5 / M H b U 6 S g A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" k v u 1 4 B G D B I h R l F R I G u 9 Z 3 v M Y U y g = \" > A A A B 7 n i c b V C 9 T s M w G P x S / k o p k L K y W F R I T F X C A m x I L I x F I r R S E 1 W O 4 7 R W H T u y H V A V + i g s D I B 4 H D b e B q f t A C 0 n f f L p z p b v u z j n T B v P + 3 Z q G 5 t b 2 z v 1 3 c Z e c / / g 0 G 0 1 H 7 Q s F K E B k V y q f o w 1 5 U z Q w D D D a T 9 X F G c x p 7 1 4 c l P 5 v U e q N J P i 3 k x z G m V 4 J F j K C D Z W G r q t s A x j y R M 9 z e y B + u F s 6 L a 9 j j c H W i f + k r R h i e 7 Q / Q o T S Y q M C k M 4 1 n r g e 7 m J S q w M I 5 z O G m G h a Y 7 J B I / o w F K B M 6 q j c h 5 9 h k 6 t k q B U K j v C o L n 6 + 0 W J M 1 1 l s z c z b M Z 6 1 a v E / 7 x B Y d L L q G Q i L w w V Z P F R W n B k J K p 6 Q A l T l B g + t Q Q T x W x W R M Z Y Y W J s W w 1 b g r + 6 8 j o J z j t X H f / O g z o c w w m c g Q 8 X c A 2 3 0 I U A C D z B C 7 z B u / P s v D o f i 7 Z q z r K 2 I / g D 5 / M H b U 6 S g A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" S d O L O h y v h a t 7 G h d S U z X L f g 4 p i J 4 = \" > A A A B + X i c b V C 9 T s M w G P z C b y l / K Y w s F h U S U 5 W w A F s F C 2 O R C K 3 U R J X j O K 1 V J 4 5 s B 1 S F P g o L A y B W 3 o S N t 8 F p M 0 D L S Z Z P d 9 8 n n y / M O F P a c b 6 t l d W 1 9 Y 3 N 2 l Z 9 e 2 d 3 b 9 9 u H N w r k U t C P S K 4 k L 0 Q K 8 p Z S j 3 N N K e 9 T F K c h J x 2 w / F 1 6 X c f q F R M p H d 6 k t E g w c O U x Y x g b a S B 3 f A L P x Q 8 U p P E X K j n T w d 2 0 2 k 5 M 6 B l 4 l a k C R U 6 A / v L j w T J E 5 p q w r F S f d f J d F B g q R n h d F r 3 c 0 U z T M Z 4 S P u G p j i h K i h m 0 a f o x C g R i o U 0 J 9 V o p v 7 e K H C i y m x m M s F 6 p B a 9 U v z P 6 + c 6 v g g K l m a 5 p i m Z P d g 7 h D 6 z P H 8 l o k + A = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V m h N N p z c 8 U T T E Z 4 y H t G S p w T F W Q z 6 J P 0 b F R I j R I p D l C o 5 n 6 e y P H s S q y m c k Y 6 5 F a 9 A r x P 6 + X 6 c F F k D O R Z p o K M n 9 o k H G k E 1 T 0 g C I m K d F 8 Y g g m k p m s i I y w x E S b t m q m B H f x y 8 v E O 2 1 e N t 3 b s 0 b r q m y j C o d w B C f g w j m 0 4 A b a 4 A G B R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V m h N N p z c 8 U T T E Z 4 y H t G S p w T F W Q z 6 J P 0 b F R I j R I p D l C o 5 n 6 e y P H s S q y m c k Y 6 5 F a 9 A r x P 6 + X 6 c F F k D O R Z p o K M n 9 o k H G k E 1 T 0 g C I m K d F 8 Y g g m k p m s i I y w x E S b t m q m B H f x y 8 v E O 2 1 e N t 3 b s 0 b r q m y j C o d w B C f g w j m 0 4 A b a 4 A G B R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V m h N N p z c 8 U T T E Z 4 y H t G S p w T F W Q z 6 J P 0 b F R I j R I p D l C o 5 n 6 e y P H s S q y m c k Y 6 5 F a 9 A r x P 6 + X 6 c F F f g w j m 0 4 A b a 4 A G B R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V m h N N p z c 8 U T T E Z 4 y H t G S p w T F W Q z 6 J P 0 b F R I j R I p D l C o 5 n 6 e y P H s S q y m c k Y 6 5 F a 9 A r x P 6 + X 6 c F F f g w j m 0 4 A b a 4 A G B R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V m h N N p z c 8 U T T E Z 4 y H t G S p w T F W Q z 6 J P 0 b F R I j R I p D l C o 5 n 6 e y P H s S q y m c k Y 6 5 F a 9 A r x P 6 + X 6 c F F f g w j m 0 4 A b a 4 A G B R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" U 0 g M h Y y t y h J 9 O i N I M Q F 4 c M z J C Y s = \" > A A A B + X i c b V C 9 T s M w G P x S / k r 5 S 2 F k s a i Q m K o E I Q F b B Q t j k Q i t 1 E S V 4 7 i t V c e J b A d U h T 4 K C w M g V t 6 E j b f B a T N A y 0 m W T 3 f f J 5 8 v T D l T 2 n G + r c r K 6 t r 6 R n W z t r W 9 s 7 t n 1 / f v V Z J J Q j 2 S 8 E R 2 Q 6 w o Z 4 J 6 m m l O u 6 m k O A 4 5 7 Y T j 6 8 L v P F C p W C L u 9 C S l Q Y y H g g 0 Y w d p I f b v u 5 3 6 Y 8 E h N Y n O h r j / t 2 w 2 n 6 c y A l o l b k g a U a P f t L z 9 K S B Z T o Q n H S v V c J 9 V B j q V m h N N p z c 8 U T T E Z 4 y H t G S p w T F W Q z 6 J P 0 b F R I j R I p D l C o 5 n 6 e y P H s S q y m c k Y 6 5 F a 9 A r x P 6 + X 6 c F F f g w j m 0 4 A b a 4 A G B R 3 i G V 3 i z n q w X 6 9 3 6 m I 9 W r H L n A P 7 A + v w B y q i T 5 A = = < / l a t e x i t > s t < l a t e x i t s h a 1 _ b a s e 6 4 = \" 5 n C j d F J 8 J R B I R q G 7 y f D E S c q x v i U = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R z d R v P X F t R K I e c J z y I K Y D J S L B K F r p 3 v S w V 6 2 5 d X c G s k y 8 g t S g Q L N X / e r 2 E 5 b F X C G T 1 J i O 5 6 Y Y 5 F S j Y J J P K t 3 M 8 J S y E R 3 w j q W K x t w E + e z U C T m x S p 9 p e H A B D b i F J v j A Y A D P 8 A p v j n R e n H f n Y 9 5 a c o q Z Q / g D 5 / M H 1 y W N s g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 5 n C j d F J 8 J R B I R q G 7 y f D E S c q x v i U = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R z d R v P X F t R K I e c J z y I K Y D J S L B K F r p 3 v S w V 6 2 5 d X c G s k y 8 g t S g Q L N X / e r 2 E 5 b F X C G T 1 J i O 5 6 Y Y 5 F S j Y J J P K t 3 M 8 J S y E R 3 w j q W K x t w E + e z U C T m x S p 9 p e H A B D b i F J v j A Y A D P 8 A p v j n R e n H f n Y 9 5 a c o q Z Q / g D 5 / M H 1 y W N s g = = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" 5 n C j d F J 8 J R B I R q G 7 y f D E S c q x v i U = \" > A A A B 6 X i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E U G 9 F L x 4 r G l t o Q 9 l s N + 3 S z S b s T o Q S + h O 8 e F D x 6 j / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 L r f T m l l d W 1 9 o 7 x Z 2 d r e 2 d 2 r 7 h 8 8 m i T T j P s s k Y l u h 9 R w K R T 3 U a D k 7 V R z G o e S t 8 L R z d R v P X F t R K I e c J z y I K Y D J S L B K F r p 3 v S w V 6 2 5 d X c G s k y 8 g t S g Q L N X / e r 2 E 5 b F X C G T 1 J i O 5 6 Y Y 5 F S j Y J J P K t 3 M 8 J S y E R 3 w j q W K x t w E + e z U C T m x S p 9 l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" g t a G d G s R i N S q c f e a q 5 f 3 G N U B Z c 0 = \" > A A A B 6 3 i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o Q Y 8 V j C 2 0 s W y 2 m 3 b p Z h N 2 J 0 I J / Q 1 e P K h 4 9 Q 9 5 8 9 + 4 b l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" g t a G d G s R i N S q c f e a q 5 f 3 G N U B Z c 0 = \" > A A A B 6 3 i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o Q Y 8 V j C 2 0 s W y 2 m 3 b p Z h N 2 J 0 I J / Q 1 e P K h 4 9 Q 9 5 8 9 + 4 b l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" g t a G d G s R i N S q c f e a q 5 f 3 G N U B Z c 0 = \" > A A A B 6 3 i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S J 4 K o k I 6 q 3 o Q Y 8 V j C 2 0 s W y 2 m 3 b p Z h N 2 J 0 I J / Q 1 e P K h 4 9 Q 9 5 8 9 + 4 b y 5 a J h J o h J y P x z M u A K m R F T S y h T 3 N 5 K 2 J g q y o z N p 2 x D 8 F Z f X i f t e s 2 7 r t U f b q r N R h F H C c 7 h A q 7 A g 1 t o w j 2 0 w A c G H J 7 h F d 4 c 6 b w 4 7 8 7 H s n X D K W b O 4 A + c z x / 4 8 4 7 C < / l a t e x i t >", "entities": [[86, 87, "DatasetName", "0"], [153, 154, "DatasetName", "0"], [202, 203, "DatasetName", "0"], [247, 248, "DatasetName", "0"], [313, 314, "DatasetName", "0"], [348, 349, "DatasetName", "0"], [356, 357, "DatasetName", "0"], [378, 379, "DatasetName", "0"], [591, 592, "DatasetName", "0"], [658, 659, "DatasetName", "0"], [707, 708, "DatasetName", "0"], [752, 753, "DatasetName", "0"], [818, 819, "DatasetName", "0"], [853, 854, "DatasetName", "0"], [861, 862, "DatasetName", "0"], [883, 884, "DatasetName", "0"], [1096, 1097, "DatasetName", "0"], [1163, 1164, "DatasetName", "0"], [1212, 1213, "DatasetName", "0"], [1257, 1258, "DatasetName", "0"], [1323, 1324, "DatasetName", "0"], [1358, 1359, "DatasetName", "0"], [1366, 1367, "DatasetName", "0"], [1388, 1389, "DatasetName", "0"], [1624, 1625, "DatasetName", "0"], [1649, 1650, "DatasetName", "0"], [1688, 1689, "DatasetName", "0"], [1704, 1705, "DatasetName", "0"], [1705, 1706, "DatasetName", "0"], [1719, 1720, "DatasetName", "0"], [1771, 1772, "DatasetName", "0"], [1928, 1929, "DatasetName", "0"], [1934, 1935, "DatasetName", "0"], [2105, 2106, "DatasetName", "0"], [2146, 2147, "DatasetName", "0"], [2165, 2166, "DatasetName", "0"], [2208, 2209, "DatasetName", "0"], [2235, 2236, "DatasetName", "0"], [2302, 2303, "DatasetName", "0"], [2312, 2313, "DatasetName", "0"], [2313, 2314, "DatasetName", "0"], [2411, 2412, "DatasetName", "0"], [2427, 2428, "DatasetName", "0"], [2471, 2472, "DatasetName", "0"], [2598, 2599, "DatasetName", "0"], [2639, 2640, "DatasetName", "0"], [2658, 2659, "DatasetName", "0"], [2701, 2702, "DatasetName", "0"], [2728, 2729, "DatasetName", "0"], [2795, 2796, "DatasetName", "0"], [2805, 2806, "DatasetName", "0"], [2806, 2807, "DatasetName", "0"], [2904, 2905, "DatasetName", "0"], [2920, 2921, "DatasetName", "0"], [2964, 2965, "DatasetName", "0"], [3072, 3073, "DatasetName", "0"], [3096, 3097, "DatasetName", "0"], [3139, 3140, "DatasetName", "0"], [3188, 3189, "DatasetName", "0"], [3233, 3234, "DatasetName", "0"], [3248, 3249, "DatasetName", "0"], [3327, 3328, "DatasetName", "0"], [3339, 3340, "DatasetName", "0"], [3395, 3396, "DatasetName", "0"], [3648, 3649, "DatasetName", "0"], [3848, 3849, "DatasetName", "0"], [3849, 3850, "DatasetName", "0"], [3967, 3968, "DatasetName", "0"], [4167, 4168, "DatasetName", "0"], [4168, 4169, "DatasetName", "0"], [4286, 4287, "DatasetName", "0"], [4486, 4487, "DatasetName", "0"], [4487, 4488, "DatasetName", "0"], [4551, 4552, "DatasetName", "0"], [4590, 4591, "DatasetName", "0"], [4606, 4607, "DatasetName", "0"], [4607, 4608, "DatasetName", "0"], [4621, 4622, "DatasetName", "0"], [4673, 4674, "DatasetName", "0"], [4830, 4831, "DatasetName", "0"], [4836, 4837, "DatasetName", "0"], [5011, 5012, "DatasetName", "0"], [5110, 5111, "DatasetName", "0"], [5137, 5138, "DatasetName", "0"], [5366, 5367, "DatasetName", "0"], [5504, 5505, "DatasetName", "0"], [5603, 5604, "DatasetName", "0"], [5630, 5631, "DatasetName", "0"], [5859, 5860, "DatasetName", "0"], [5982, 5983, "DatasetName", "0"], [6236, 6237, "DatasetName", "0"], [6436, 6437, "DatasetName", "0"], [6437, 6438, "DatasetName", "0"], [6555, 6556, "DatasetName", "0"], [6755, 6756, "DatasetName", "0"], [6756, 6757, "DatasetName", "0"], [6874, 6875, "DatasetName", "0"], [7074, 7075, "DatasetName", "0"], [7075, 7076, "DatasetName", "0"], [7193, 7194, "DatasetName", "0"], [7393, 7394, "DatasetName", "0"], [7394, 7395, "DatasetName", "0"], [7512, 7513, "DatasetName", "0"], [7712, 7713, "DatasetName", "0"], [7713, 7714, "DatasetName", "0"], [7831, 7832, "DatasetName", "0"], [8031, 8032, "DatasetName", "0"], [8032, 8033, "DatasetName", "0"], [8646, 8647, "DatasetName", "0"], [8667, 8668, "DatasetName", "0"], [8774, 8775, "DatasetName", "0"], [8849, 8850, "DatasetName", "0"], [8963, 8964, "DatasetName", "0"], [9064, 9065, "DatasetName", "0"], [9162, 9163, "DatasetName", "0"], [9225, 9226, "DatasetName", "0"], [9271, 9272, "DatasetName", "0"], [9289, 9290, "DatasetName", "0"], [9378, 9379, "DatasetName", "0"], [9417, 9418, "DatasetName", "0"], [9433, 9434, "DatasetName", "0"], [9434, 9435, "DatasetName", "0"], [9448, 9449, "DatasetName", "0"], [9500, 9501, "DatasetName", "0"], [9657, 9658, "DatasetName", "0"], [9663, 9664, "DatasetName", "0"], [9864, 9865, "DatasetName", "0"], [9902, 9903, "DatasetName", "0"], [9904, 9905, "DatasetName", "0"], [10079, 10080, "DatasetName", "0"], [10186, 10187, "DatasetName", "0"], [10373, 10374, "DatasetName", "0"], [10411, 10412, "DatasetName", "0"], [10413, 10414, "DatasetName", "0"], [10588, 10589, "DatasetName", "0"], [10695, 10696, "DatasetName", "0"], [10881, 10882, "DatasetName", "0"], [10938, 10939, "DatasetName", "0"], [11012, 11013, "DatasetName", "0"], [11061, 11062, "DatasetName", "0"], [11080, 11081, "DatasetName", "0"], [11091, 11092, "DatasetName", "0"], [11185, 11186, "DatasetName", "0"], [11299, 11300, "DatasetName", "0"], [11400, 11401, "DatasetName", "0"], [11498, 11499, "DatasetName", "0"], [11561, 11562, "DatasetName", "0"], [11607, 11608, "DatasetName", "0"], [11625, 11626, "DatasetName", "0"], [11706, 11707, "DatasetName", "0"], [11820, 11821, "DatasetName", "0"], [11921, 11922, "DatasetName", "0"], [12019, 12020, "DatasetName", "0"], [12082, 12083, "DatasetName", "0"], [12128, 12129, "DatasetName", "0"], [12146, 12147, "DatasetName", "0"], [12227, 12228, "DatasetName", "0"], [12341, 12342, "DatasetName", "0"], [12442, 12443, "DatasetName", "0"], [12540, 12541, "DatasetName", "0"], [12589, 12590, "DatasetName", "0"], [12670, 12671, "DatasetName", "0"], [12784, 12785, "DatasetName", "0"], [12885, 12886, "DatasetName", "0"], [12983, 12984, "DatasetName", "0"], [13032, 13033, "DatasetName", "0"], [13113, 13114, "DatasetName", "0"], [13227, 13228, "DatasetName", "0"], [13328, 13329, "DatasetName", "0"], [13426, 13427, "DatasetName", "0"], [13475, 13476, "DatasetName", "0"], [13556, 13557, "DatasetName", "0"], [13670, 13671, "DatasetName", "0"], [13771, 13772, "DatasetName", "0"], [13869, 13870, "DatasetName", "0"], [13918, 13919, "DatasetName", "0"], [14063, 14064, "DatasetName", "0"], [14452, 14453, "DatasetName", "0"], [14841, 14842, "DatasetName", "0"], [15138, 15139, "DatasetName", "0"], [15189, 15190, "DatasetName", "0"], [15203, 15204, "DatasetName", "0"], [15278, 15279, "DatasetName", "0"], [15329, 15330, "DatasetName", "0"], [15343, 15344, "DatasetName", "0"], [15418, 15419, "DatasetName", "0"], [15469, 15470, "DatasetName", "0"], [15483, 15484, "DatasetName", "0"], [15583, 15584, "DatasetName", "0"]]}
{"text": "We test the proposed framework on unconditional and conditional text generation tasks , and analyze the results to understand the performance gained by the guider network . We also perform an ablation investigation on the improvements brought by each part of our proposed method , and consider non - parallel style transfer . All experiments are conducted on a single Tesla P100 GPU and implemented with TensorFlow and Theano . Details of the datasets , the experimental setup and model architectures are provided in the Appendix .", "entities": [[8, 11, "TaskName", "conditional text generation"], [50, 52, "TaskName", "style transfer"]]}
{"text": "Multi - Style Transfer with Discriminative Feedback on Disjoint Corpus", "entities": [[2, 4, "TaskName", "Style Transfer"]]}
{"text": "Style transfer has been widely explored in natural language generation with non - parallel corpus by directly or indirectly extracting a notion of style from source and target domain corpus . A common shortcoming of existing approaches is the prerequisite of joint annotations across all the stylistic dimensions under consideration . Availability of such dataset across a combination of styles limits the extension of these setups to multiple style dimensions . While cascading single - dimensional models across multiple styles is a possibility , it suffers from content loss , especially when the style dimensions are not completely independent of each other . In our work , we relax this requirement of jointly annotated data across multiple styles by using independently acquired data across different style dimensions without any additional annotations . We initialize an encoder - decoder setup with transformerbased language model pre - trained on a generic corpus and enhance its re - writing capability to multiple target style dimensions by employing multiple style - aware language models as discriminators . Through quantitative and qualitative evaluation , we show the ability of our model to control styles across multiple style dimensions while preserving content of the input text . We compare it against baselines involving cascaded state - of - the - art uni - dimensional style transfer models .", "entities": [[0, 2, "TaskName", "Style transfer"], [88, 89, "MetricName", "loss"], [218, 220, "TaskName", "style transfer"]]}
{"text": "Style transfer is a popular task in natural language processing and has been studied on attributes like age or gender ( Subramanian et al , 2018 ) , styles emanating from social construct like formality ( Rao and Tetreault , 2018 ) and politeness ( Madaan et al , 2020 ) , linguistic styles based on author writing style ( Syed et al , 2020 ) , or psycho - linguistic styles based on personality types ( Mairesse and Walker , 2011 ) . While early style transfer frameworks were modeled as a supervised learning task on a parallel corpus , state - of - the - art models are semi - supervised / unsupervised and operate on nonparallel corpus . These models achieve style transfer by aligning source and target distribution of sentences from non - parallel corpus ( Shen et al , 2017 ) , disentangling content space from style space in latent representation ( Hu et al , 2017 ) or employing self - reconstruction ( Dai et al , 2019 ) and back translation ( Lample et al , 2018 ) objectives to achieve pseudo - supervision with non - parallel corpus . Recent works have also modeled this in a self - supervised manner where rewriting ( transfer ) is achieved by utilizing corpus from the target style alone ( Syed et al , 2020 ) . These wide studies have also led to the curation and benchmarking of non - parallel dataset for various style dimensions , such as sentiment ( Li et al , 2018 ) , formality ( Rao and Tetreault , 2018 ) , politeness ( Danescu - Niculescu - Mizil et al , 2013 ) , excitement ( Sancheti et al , 2020 ) , etc . But availability of data with joint tagging across multiple styles is limited and has restricted the ability of existing approaches to scale from single - dimensional transfer to multiple style dimensions . In this paper , we propose a multidimensional style transfer approach that can work off partially labelled data for style transfer across multiple dimensions simultaneously . The work by Subramanian et al ( 2018 ) attempts style transfer with multiple attributes such as age , gender , and sentiment simultaneously . However , their approach avails corpus tagged with each of these three style dimensions . In contrast to this and other similar explorations in multi - style transfer , our approach does not require jointly labelled data across all the stylistic dimensions in source and/or target corpus . We focus on the problem where independent corpus is available across different stylistic dimensions ( say sentiment and formality ) and we achieve style transfer spanning different stylistic dimensions ( say make a sentence more positive and formal ) . While state - of - the - art approaches can be extended to achieve this by sequentially transferring one style after another , it is limited as different style dimensions are not necessarily independent of each other . In aspects that are not independent , changing one style aspect of the text might affect another aspect considered , making a sequential brute - force approach non - ideal . As we show in our experiments later , the cascaded setup also lacks common grounding between the content from different styles leading to erratic changes in content . We circumvent this by grounding our framework on the linguistic understanding of a large language model . Our model builds understanding of interplay between the different styles by incorporating multiple discriminative language models ( LM ) with language model - based encoder - decoder setup . The key contributions of this paper are : 1 ) An encoder - decoder setup with multiple language models as discriminator , with each entity harnessing the language understanding from a large pre - trained transformer model . 2 ) Relaxing the requirement of jointly labelled data for multi - style transfer , by leveraging independently acquired disjoint corpus for different styles . 3 ) Achieving better style control with better content preservation in multi - dimensional style transfer than a cascaded setup of state - of - the - art unidimensional style transfer models .", "entities": [[0, 2, "TaskName", "Style transfer"], [86, 88, "TaskName", "style transfer"], [124, 126, "TaskName", "style transfer"], [337, 339, "TaskName", "style transfer"], [348, 350, "TaskName", "style transfer"], [365, 367, "TaskName", "style transfer"], [406, 408, "TaskName", "style transfer"], [451, 453, "TaskName", "style transfer"], [661, 663, "TaskName", "style transfer"], [688, 690, "TaskName", "style transfer"], [703, 705, "TaskName", "style transfer"]]}
{"text": "An intelligent , rewarding film that I look forward to watching again . ludicrous , shallow film that look forward to watching again . An unintelligent , poor film that I would not look forward to watching again . super friendly staff , quick service and amazing and simple food was done right ! says wait staff , quick not amazing before overcooked food done were okay . dirty staff and slow service and simple food was not done right . Positive+Informal You need to separate the bad thing and move on . need to the great thing and move on . You need to enjoy the good stuff and move on . The evening started out slow . The evening spent in professional show . The evening began amazing . Negative+Informal Great food recommendations steak and tuna were both great . terrible food 9 am steak and were both terrible . Disappointing food recommendations steak and tuna were horrible . That person in hilarious . You person in worse ! That guy in so boring . ( Syed et al ( 2020 ) and our proposed approach ) , we note that content preservation is marginally better for Syed et al ( 2020 ) 's model , however , our model is able to yield much better style transfer owing to feedback on style control by multiple discriminators .", "entities": [[218, 220, "TaskName", "style transfer"]]}
{"text": "We propose an approach to extend currently existing style transfer work to multiple style setting without imposing any extra constraints on availability of dataset . Our method makes use of disjoint corpus from separate styles to enable one step transfer across multiple target styles . We exploit multiple discriminative language models with an encoder - decoder framework , all emerging from large transformer - based language models pretrained on Masked Language Modeling objective and fine - tuned separately for transfer and discriminative purposes . We show that unified single step transfer approach is able to achieve better transfer while offering much better content preservation which is paramount to any style transfer task . Further improvements are in scope for adding modularity to the proposed transfer module . In the current setup , each version of model is trained for a specific combination of target style ( s ) . The utility of such a model increases manifold with added ease of transfer across multiple style combinations within a single model . This could be attempted by employing a controlled language model as a unified discriminator for multiple styles , which would be the subject of further research . Ethics Statement . We recognise the ethical implication of employing large language models trained on data infused with unchecked biases . As with any generative task , style transfer too suffers from the potential misuse for fact distortion , plagiarism and more . The paper aims at establishing academic utility of proposed framework . To meet ethical standards , this solution has to coupled with strict misrepresentation , offensiveness and bias checks .", "entities": [[8, 10, "TaskName", "style transfer"], [69, 72, "TaskName", "Masked Language Modeling"], [109, 111, "TaskName", "style transfer"], [198, 199, "DatasetName", "Ethics"], [225, 227, "TaskName", "style transfer"]]}
{"text": "Improving Numerical Reasoning Skills in the Modular Approach for Complex Question Answering on Text", "entities": [[10, 12, "TaskName", "Question Answering"]]}
{"text": "Complex Question Answering focuses on questions that require capabilities beyond multi - hop reasoning . These capabilities include numerical , logical and discrete reasoning . A number of neural models were recently proposed to address the CQA task , such as BiDAF ( Seo et al , 2017 ) , QANet ( Yu et al , 2018 ) , NMNs ( Gupta et al , 2020 ) and NumNet ( Ran et al , 2019 ) , which achieved high performance on benchmark datasets such as DROP ( Dua et al , 2019 ) . Numerical Reasoning is an essential capability for the CQA task , which is a challenging problem since the numbers and computation procedures are separately extracted and generated from raw text . Dua et al ( 2019 ) modified the output layer of QANet ( Yu et al , 2018 ) and proposed a number - aware model NAQANet that can deal with numerical questions for which the answer can not be directly extracted from the paragraph . In addition to NAQANet , NumNet ( Ran et al , 2019 ) leveraged Graph Neural Network ( GNN ) to design a number - aware deep learning model . Also leveraging GNN , Chen et al ( 2020a ) distinguished number types more precisely by adding the connection with entities and obtained better performance . Chen et al ( 2020b ) searched possible programs exhaustively based on answer numbers and employed these programs as weak supervision to train the whole model . Using dependency parsing of questions , Saha et al ( 2021 ) focused on the numerical part and obtained excellent results on different kinds of numerical reasoning questions . Neural Module Networks ( NMNs ) ( Gupta et al , 2020 ) adopts the programmer - interpreter paradigm and is a fully end - to - end differentiable model , in which the programmer ( responsible for composing programs ) and the interpreter ( responsible for soft execution ) are jointly learned . Specialised modules , such as find and find - num , are predefined to perform different types of reasoning over text and numbers . Compared with those techniques that employ GNNs ( Ran et al , 2019 ; Yu et al , 2018 ) , NMNs is highly interpretable while achieving competitive performance . More details can be found in Appendix A.", "entities": [[1, 3, "TaskName", "Question Answering"], [86, 87, "DatasetName", "DROP"], [256, 258, "TaskName", "dependency parsing"]]}
{"text": "It is highly likely for a paragraph to contain multiple numbers and entities , as shown in Figure 1 . For such paragraphs , the original NMNs allows all numbers to interact with all entities in the computation of number - related modules such as \" find - num \" . This is detrimental to performance as , intuitively , a number far away from an entity is less likely to be related to the entity . As the second example in Figure 1 shows , NMNs connects \" December 1997 \" to the entity \" PUK and KDP \" since \" 2003 \" is far away from it , resulting in wrong predictions eventually . To tackle this issue , we add another computational component , the relation matrix U n , into numberrelated modules . Taking the \" find - num \" module as an example , the following step is added before Equation 2 when computing S n ij : S n ij = U n ij S n ij , ( 8 ) where is element - wise multiplication . In the above equation , the value of S n ij is updated with the relation matrix U n , which constrains the relationship between the i th paragraph token and j th number token . More specifically , let s t be the token index set for the t th sentence in the paragraph . Thus , if both the i th paragraph token and the j th number token belong to the same sentence , element U n ij , in row i and column j , is set to 1 , otherwise 0 : U n ij = 1 , ( i s t ) ( n j s t ) 0 , otherwise ( 9 ) By adding this matrix , the module only keeps the attention values of tokens in close vicinity within a sentence , and learns to find the related numbers that directly interact with entities . Similarly , this relation matrix U n is also applied to other number - related modules to improve performance .", "entities": [[279, 280, "DatasetName", "0"], [298, 299, "DatasetName", "0"]]}
{"text": "In order to solve the complex question answering problem , Gupta et al ( 2020 ) proposed a Neural Module Networks ( NMNs ) model . Consisting of a programmer and an interpreter , NMNs can be more interpretable as shown in Figure 2 . As Figure 2 shows , NMNs takes the question and the paragraph as inputs . The programmer firstly maps the question into corresponding \" discrete \" modules in order . Then , the interpreter executes these generated modules against the corresponding paragraph to produce the final answer . Moreover , all modules are differentiable so that the whole NMNs can be trained in an end - to - end way . For hyper - parameters in our model , we do n't conduct experiments on their search trials since we employ the same settings as Gupta et al ( 2020 ) did , which can be found in Table 3 . Note that they are also the configuration to obtain the best performance . For the added parameter \u03bb in Equation 7 , we leverage an empirical value \u03bb=0.5 without any fine - tuning .", "entities": [[6, 8, "TaskName", "question answering"]]}
{"text": "This research was supported in part by the Future Fellowship FT190100039 from the Australian Research Council . The computational resources for this work were provided by the Multi - modal Australian Sci - enceS Imaging and Visualisation Environment ( MAS - SIVE ) ( www.massive.org.au ) . We would like to thank the anonymous reviewers for their useful comments to improve the manuscript .", "entities": [[39, 40, "MethodName", "MAS"]]}
{"text": "More Identifiable yet Equally Performant Transformers for Text Classification", "entities": [[7, 9, "TaskName", "Text Classification"]]}
{"text": "Interpretability is an important aspect of the trustworthiness of a model 's predictions . Transformer 's predictions are widely explained by the attention weights , i.e. , a probability distribution generated at its self - attention unit ( head ) . Current empirical studies provide shreds of evidence that attention weights are not explanations by proving that they are not unique . A recent study showed theoretical justifications to this observation by proving the non - identifiability of attention weights . For a given input to a head and its output , if the attention weights generated in it are unique , we call the weights identifiable . In this work , we provide deeper theoretical analysis and empirical observations on the identifiability of attention weights . Ignored in the previous works , we find the attention weights are more identifiable than we currently perceive by uncovering the hidden role of the key vector . However , the weights are still prone to be non - unique attentions that make them unfit for interpretation . To tackle this issue , we provide a variant of the encoder layer that decouples the relationship between key and value vector and provides identifiable weights up to the desired length of the input . We prove the applicability of such variations by providing empirical justifications on varied text classification tasks . The implementations are available at https://github.com/declare - lab/ identifiable - transformers .", "entities": [[14, 15, "MethodName", "Transformer"], [223, 225, "TaskName", "text classification"]]}
{"text": "The output of an attention head H is the product of A and T ( eq . ( 2 ) ) . Formally , we define identifiability of attention in a head : Definition 3.1 . For an attention head 's output H , attention weights A are identifiable if there exists a unique solution of A T = H. The above definition can be reformulated as Definition 3.2 . A is unidentifiable if there exist a\u00f1 A , ( \u00c3 = 0 ) , such that ( A + \u00c3 ) is obtainable from phase - 1 of head computations and satisfy ( A + \u00c3 ) T = A T = \u21d2\u00c3 T = 0 . ( constraint - R1 ) Under this constraint , we get\u00e3 i T = 0 where\u00e3 i is the i th row of\u00c3. The set of vectors which when multiplied to T gets mapped to zero describes the left null space of T denoted by LN ( T ) . The dimension of the left null space of T can be obtained by taking the difference of the total number of rows ( d s ) and the number of linearly independent rows , i.e , rank of the matrix T denoted by rank ( T ) . Let dim ( ) denotes the dimension of a vector space , then LN ( T ) = { v | v T T = 0 } ( 3 ) dim LN ( T ) = d s \u2212 rank ( T ) . ( 4 ) 3.1 \" A \" is Identifiable for d s \u2264 d v If dim ( LN ( T ) ) = 0 then LN ( T ) = { 0 } , it leads to the only solution of constraint - R1 that is\u00c3 = 0 . Therefore , the unidentifiabilty condition does not hold . Now we will prove such a situation exists when the number of tokens is not more than the size of value vector . The matrix T in eq . ( 2 ) is product of d s \u00d7 d v value matrix V and d v \u00d7 d e transformation D. We utilize the fact that the rank of product of two matrices P and Q is upper bounded by the minimum of rank ( P ) and rank ( Q ) , i.e. , rank ( P Q ) \u2264 min rank ( P ) , rank ( Q ) . Thus , the upper bound on rank ( T ) in eq . ( 4 ) can be determined by rank ( T ) \u2264 min rank ( V ) , rank ( D ) \u2264 min min ( ds , dv ) , min ( dv , de ) \u2264 min ds , dv , dv , de \u2264 min ds , dv ( as de > dv ) = min ds , 64 where the last inequality is obtained for a head in the regular Transformer for which d v = 64 . Numerical rank . To substantiate the bounds on rank ( T ) as derived above , we set up a model with a single encoder layer ( 6 ) . The model is trained to predict the sentiment of IMDB reviews ( 5 ) . We feed the review tokens to the model and store the values generated in T of the first head . A standard technique for calculating the rank of a matrix with floating - point values and computations is to use singular value decomposition . The rank of the matrix will be computed as the number of singular values larger than the predefined threshold 4 . The fig . 2 illustrates how the rank changes with the sequence length d s . The numerical rank provides experimental support to the theoretical analysis . rank ( T ) = d s if d s \u2264 d v , d v if d s > d v . ( 6 ) Thus , dim LN ( T ) = d s \u2212 rank ( T ) = 0 if d s \u2264 d v , ( d s \u2212 d v ) if d s > d v . = max ( d s \u2212 d v , 0 ) ( 7 ) With this , we infer A is identifiable if d s \u2264 d v = 64 . For the identifiability study , since we focus on a model 's capability of learning unique attention weights , we will assume T has the maximum obtainable rank set by its upper bound .", "entities": [[82, 83, "DatasetName", "0"], [116, 117, "DatasetName", "0"], [132, 133, "DatasetName", "0"], [241, 242, "DatasetName", "0"], [284, 285, "DatasetName", "0"], [292, 293, "DatasetName", "0"], [308, 309, "DatasetName", "0"], [508, 509, "MethodName", "Transformer"], [555, 556, "DatasetName", "IMDB"], [695, 696, "DatasetName", "0"], [726, 727, "DatasetName", "0"]]}
{"text": "( the hidden role of d k ) In this case , from eq . ( 7 ) , we obtain a non zero value of dim LN ( T ) . It allows us to find infi - nite\u00c3 's satisfying ( A + \u00c3 ) T = A T. However , constraint - R1 demands\u00c3 to be obtainable from the first phase of self - attention . As a first step , we focus our analysis on the attention matrix without applying softmax non - linearity , i.e. , A = Q K T \u221a dq . The analysis is crucial to identify constraints coming from the first phase of self - attention in Transformer that impact identifiability . Insights from this will help us analyse softmax version of A.", "entities": [[84, 85, "MethodName", "softmax"], [116, 117, "MethodName", "Transformer"], [128, 129, "MethodName", "softmax"]]}
{"text": "For the empirical analysis of our proposed solutions as mentioned in 4 , we conduct our experiments on the following varied text classification tasks :", "entities": [[21, 23, "TaskName", "text classification"]]}
{"text": "IMDB ( Maas et al , 2011 ) . The dataset for the task of sentiment classification consist of IMDB movie reviews with their sentiment as positive or negative . Each of the train and test sets contain 25 , 000 data samples equally distributed in both the sentiment polarities . TREC ( Voorhees and Tice , 2000 ) . We use the 6 - class version of the dataset for the task of question classification consisting of open - domain , facet - based questions . There are 5 , 452 and 500 samples for training and testing , respectively . SST ( Socher et al , 2013 ) . Stanford sentiment analysis dataset consist of 11 , 855 sentences obtained from movie reviews . We use the 3 - class version of the dataset for the task of sentiment classification . Each review is labeled as positive , neutral , or negative . The provided train / test / valid split is 8 , 544/2 , 210/1 , 101 . 8 ds - max < de as in the regular Transformer setting .", "entities": [[0, 1, "DatasetName", "IMDB"], [19, 22, "DatasetName", "IMDB movie reviews"], [51, 52, "DatasetName", "TREC"], [102, 103, "DatasetName", "SST"], [112, 114, "TaskName", "sentiment analysis"], [182, 183, "MethodName", "Transformer"]]}
{"text": "SNLI ( Bowman et al , 2015 ) . The dataset contain 549 , 367 samples in the training set , 9 , 842 samples in the validation set , and 9 , 824 samples in the test set . For the task of recognizing textual entailment , each sample consists of a premisehypothesis sentence pair and a label indicating whether the hypothesis entails the premise , contradicts it , or neutral . Please refer to Zhang et al ( 2015 ) for more details about the following datasets : Yelp . We use the large - scale Yelp review dataset for the task of binary sentiment classification . There are 560 , 000 samples for training and 38 , 000 samples for testing , equally split into positive and negative polarities . DBPedia . The Ontology dataset for topic classification consist of 14 non - overlapping classes each with 40 , 000 samples for training and 5 , 000 samples for testing . Sogou News . The dataset for news article classification consist of 450 , 000 samples for training and 60 , 000 for testing . Each article is labeled in one of the 5 news categories . The dataset is perfectly balanced . AG News . The dataset for the news articles classification partitioned into four categories . The balanced train and test set consist of 120 , 000 and 7 , 600 samples , respectively . Yahoo ! Answers . The balanced dataset for 10class topic classification contain 1 , 400 , 000 samples for training and 50 , 000 samples for testing . Amazon Reviews . For the task of sentiment classification , the dataset contain 3 , 600 , 000 samples for training and 400 , 000 samples for testing . The samples are equally divided into positive and negative sentiment labels . Except for the SST and SNLI , where the validation split is already provided , we flag 30 % of the train set as part of the validation set and the rest 70 % were used for model parameter learning .", "entities": [[0, 1, "DatasetName", "SNLI"], [133, 134, "DatasetName", "DBPedia"], [136, 137, "MethodName", "Ontology"], [139, 141, "TaskName", "topic classification"], [206, 208, "DatasetName", "AG News"], [240, 243, "DatasetName", "Yahoo ! Answers"], [249, 251, "TaskName", "topic classification"], [312, 313, "DatasetName", "SST"], [314, 315, "DatasetName", "SNLI"]]}
{"text": "This work probed Transformer for identifiability of self - attention , i.e. , the attention weights can be uniquely identified from the head 's output . With theoretical analysis and supporting empirical evidence , we were able to identify the limitations of the existing study by Brunner et al ( 2019 ) . We found the study largely ignored the constraint coming from the first phase of self - attention in the encoder , i.e. , the size of the key vector . Later , we proved how we can utilize d k to make the attention weights more identifiable . To give a more concrete solution , we propose encoder variants that are more identifiable , theoretically as well as experimentally , for a large range of input sequence lengths . The identifiable variants do not show any performance drop when experiments are done on varied text classification tasks . Future works may analyse the critical impact of identifiability on the explainability and interpretability of the Transformer .", "entities": [[3, 4, "MethodName", "Transformer"], [147, 149, "TaskName", "text classification"], [167, 168, "MethodName", "Transformer"]]}
{"text": "The left null space of a m p \u00d7 n p matrix P can be defined as the set of vectors v - LN P = { v T R 1\u00d7mp | v T P = 0 } ( 10 ) If the rows of P are linearly independent ( P is full - row rank ) the left null space of P is zero dimensional . The only solution to the system of equations v P = 0 is trivial , i.e. , v=0 . The dimensions of the null space , known as nullity , of P can be calculated as dim LN ( P ) = m p \u2212 rank ( P ) . The nullity of P sets the dimensions of the space v lies in . In 3 , we utilize our knowledge of appendix A.2 and appendix A.3 to analyse identifiability in a Transformer .", "entities": [[36, 37, "DatasetName", "0"], [78, 79, "DatasetName", "0"], [149, 150, "MethodName", "Transformer"]]}
{"text": "NLQuAD : A Non - Factoid Long Question Answering Data Set", "entities": [[7, 9, "TaskName", "Question Answering"]]}
{"text": "SQuAD ( Rajpurkar et al , 2016 ) is a factoid span detection data set with short answers . Crowdworkers generated the questions given a set of articles . DROP ( Dua et al , 2019 ) makes the problem more challenging by adversarially - created questions requiring discrete reasoning over the text . SQuAD and DROP use Wikipedia pages as context passages whereas SearchQA ( Dunn et al , 2017 ) uses IR approaches to collect context passages . Answer generation based on a set of passages is another approach to address this task . MS MARCO ( Bajaj et al , 2016 ) consists of real - world search queries and retrieved documents corresponding to the queries . There are also different types of QA data sets such as Antique ( Hashemi et al , 2020 ) , which is a data set for answer retrieval for non - factoid ques - tions . There is also a range of multiple - choice QA tasks such as RACE ( Lai et al , 2017 ) , ARC ( Clark et al , 2018 ) , SWAQ ( Zellers et al , 2018 ) , and COS - MOS QA ( Huang et al , 2019 ) that are clustered together with the short - context QA data sets .", "entities": [[0, 1, "DatasetName", "SQuAD"], [29, 30, "DatasetName", "DROP"], [54, 55, "DatasetName", "SQuAD"], [56, 57, "DatasetName", "DROP"], [64, 65, "DatasetName", "SearchQA"], [80, 82, "TaskName", "Answer generation"], [96, 98, "DatasetName", "MS MARCO"], [169, 170, "DatasetName", "RACE"], [178, 179, "DatasetName", "ARC"]]}
{"text": "Factoid QA has been applied to longer documents , however , the nature of factoid questions limits answers to short texts . NewsQA ( Trischler et al , 2017 ) , TriviaQA ( Joshi et al , 2017 ) , NarrativeQA ( Ko\u010disk\u00fd et al , 2018 ) , and DuoRC ( Saha et al , 2018 ) fall into this category and their documents are extracted from news articles , stories , and movie plots , respectively . On the other hand , DQA ( ter Hoeve et al , 2020 ) is a document - centred QA data set aimed at document assistance systems . Along with Yes / No questions , it also includes non - factoid questions with relatively long answers . However , the questions are generated by crowd - workers based on a small set of documents . DuReader ( He et al , 2018 ) consists of real - word Chinese queries and corresponding retrieved documents . It contains both factoid and non - factoid ( 40 % ) questions and consequently has longer average answer length than pure factoid datasets . The multi - hop QA task , requiring multi - hop reasoning over multiple paragraphs , can also be considered as long - context QA if models process paragraphs together . HotpotQA ( Yang et al , 2018 ) is a multi - hop data set , but the answer length of its factoid questions is as limited as that of short - context QA data sets . Natural Questions ( Kwiatkowski et al , 2019 ) is a factoid QA task with much longer documents and two types of answer lengths . answers ( yes / no , entities ) as well as long answers ( bounding boxes with the information to infer the answer ) . However , due to the nature of factoid questions , the majority of long answers are sections containing exactly the short answer or simple facts . ELI5 ( Fan et al , 2019 ) consists of real - world questions with answers provided by the Reddit community . The task is to generate answers given a set of documents retrieved from the Web . However , the documents are not guaranteed to completely address the questions . Furthermore , evaluation metrics for sequence generation tasks such as the ROUGE score ( Lin and Och , 2004 ) are far from perfect to assess the quality of generated answers . Table 1 compares existing long - context question answering data sets along with SQuAD and MS MARCO . We report the average length for data sets with different types of answers .", "entities": [[22, 23, "DatasetName", "NewsQA"], [31, 32, "DatasetName", "TriviaQA"], [40, 41, "DatasetName", "NarrativeQA"], [50, 51, "DatasetName", "DuoRC"], [144, 145, "DatasetName", "DuReader"], [220, 221, "DatasetName", "HotpotQA"], [257, 259, "DatasetName", "Natural Questions"], [333, 334, "DatasetName", "ELI5"], [352, 353, "DatasetName", "Reddit"], [423, 425, "TaskName", "question answering"], [429, 430, "DatasetName", "SQuAD"], [431, 433, "DatasetName", "MS MARCO"]]}
{"text": "To investigate the difficulty level of NLQuAD for state - of - the - art QA systems and to establish baseline results , we evaluate the performance of BERT ( Devlin et al , 2018 ) , RoBERTa ( Liu et al , 2019 ) , and Longformer ( Beltagy et al , 2020 ) . Longformer is a scalable model for processing long documents and has been used for long sequences such as document classification ( Beltagy et al , 2020 ) and document re - ranking ( Sekuli\u0107 et al , 2020 ) . We refer readers to Tay et al ( 2020b ) for a detailed survey on efficient transformers . We train these Transformerbased ( Vaswani et al , 2017 ) models to predict the span of the answer in a context document given a question and document .", "entities": [[28, 29, "MethodName", "BERT"], [37, 38, "MethodName", "RoBERTa"], [47, 48, "MethodName", "Longformer"], [56, 57, "MethodName", "Longformer"], [74, 76, "TaskName", "document classification"]]}
{"text": "The BERT QA model concatenates question and document pairs into a single sequence and predicts the answer span by a dot product between the final hidden vectors , a start vector and an end vector ( Devlin et al , 2018 ) . Due to the memory and computational requirements , BERT can encode sequences with a maximum length of 512 tokens that is less than the average sample length in NLQuAD . Therefore , we adopt a sliding window approach . We split the samples into segments using a sliding window of 512 tokens and a stride of 128 tokens . Each segment is augmented with its corresponding question . The segments can include no answer , a portion of the answer , or the entire answer . We train BERT on the segments independently . Finally , the predicted spans corresponding to a single sample are aggregated to predict the final span that is the span between the earliest start position and the latest end position . The output is considered empty when all segments have empty spans . RoBERTa has the same model architecture and input length limitation as BERT but with a robustly optimized pre - training scheme allowing it to generalize better to downstream tasks such as QA ( Liu et al , 2019 ) . We apply the same sliding window approach for RoBERTa .", "entities": [[1, 2, "MethodName", "BERT"], [51, 52, "MethodName", "BERT"], [131, 132, "MethodName", "BERT"], [181, 182, "MethodName", "RoBERTa"], [192, 193, "MethodName", "BERT"], [229, 230, "MethodName", "RoBERTa"]]}
{"text": "In order to process the question and entire documents at the same time , we use the Longformer model . It employs an attention mechanism scaling linearly with the sequence length which enables Longformer to process up to 4 , 096 tokens . It uses multiple attention heads with different dilation configurations to attend to the entire sequence and includes global attention to question tokens in the sequence . Question and document pairs are packed together into a single sequence without having to use sliding windows and the answer span is calculated by a dot product ( Beltagy et al , 2020 ) .", "entities": [[17, 18, "MethodName", "Longformer"], [33, 34, "MethodName", "Longformer"]]}
{"text": "We study the extent to which emoji can be used to add interpretability to embeddings of text and emoji . To do so , we extend the POLAR - framework that transforms word embeddings to interpretable counterparts and apply it to word - emoji embeddings trained on four years of messaging data from the Jodel social network . We devise a crowdsourced human judgement experiment to study six usecases , evaluating against words only , what role emoji can play in adding interpretability to word embeddings . That is , we use a revised POLAR approach interpreting words and emoji with words , emoji or both according to human judgement . We find statistically significant trends demonstrating that emoji can be used to interpret other emoji very well .", "entities": [[32, 34, "TaskName", "word embeddings"], [84, 86, "TaskName", "word embeddings"]]}
{"text": "Word embeddings create a vector - space representation in which words with a similar meaning are in close proximity . Existing approaches to make embeddings interpretable , e.g. , via contextual ( Subramanian et al , 2018 ) , sparse embeddings ( Panigrahi et al , 2019 ) , or learned ( Senel et al , 2018 ) transformations ( Mathew et al , 2020 ) - all focus on text only . Yet , emoji are widely used in casual communication , e.g. , Online Social Networks ( OSN ) , and are known to extend textual expressiveness , demonstrated to benefit , e.g. , sentiment analysis ( Novak et al , 2015 ; Hu et al , 2017 ) .", "entities": [[0, 2, "TaskName", "Word embeddings"], [106, 108, "TaskName", "sentiment analysis"]]}
{"text": "We raise the question if we can leverage the expressiveness of emoji to make word embeddings - and thus also emoji - interpretable . I.e. , can we adopt word embedding interpretability via leveraging semantic polar opposites ( e.g. , cold / hot ) to emoji ( e.g. , / , or / ) for interpreting words or emoji w.r.t . human judgement . * Timon Mohaupt performed this work during his master thesis at Brandenburg University of Technology and RWTH Aachen University . Approach . Motivated and based upon POLAR ( Mathew et al , 2020 ) , we deploy a revised variant POLAR \u03c1 that transforms arbitrary word embeddings into interpretable counterparts . The key idea is to leverage semantic differentials as a psychometric tool to align embedded terms on a scale between two polar opposites . Employing a projection - based transformation in POLAR \u03c1 , we provide embedding dimensions with semantic information . I.e. , the resulting interpretable embedding space values directly estimate a term 's position on a - priori provided polar opposite scales , while approximately preserving in - embedding structures ( 2 ) . The main contribution of this work is the largescale application of this approach to a social media corpus and especially its evaluation in a crowdsourced human judgement experiment . For studying the role of emoji in interpretability , we create a word - emoji input embedding from on a large social media corpus . The dataset comprises four years of complete data in a single country from the online social network provider Jodel ( 48 M posts of which 11 M contain emoji ) . For subsequent main evaluation , we make this embedding interpretable with word and emoji opposites by deploying our adopted tool POLAR \u03c1 ( 3 ) . Given different expressiveness of emoji , we ask RQ1 ) How does adding emoji to POLAR \u03c1 impact interpretability w.r.t . to human judgement ? I.e. , do humans agree on best interpretable dimensions for describing words or emoji with word or emoji opposites ? And RQ2 ) How well do POLAR \u03c1 - semantic dimensions reflect a term 's position on a scale between word or emoji polar opposites ? Human judgement . We design a crowdsourced human judgement experiment ( 4 ) to study if adding emoji to word embeddings and POLAR \u03c1 in particular increases the interpretability - while also answering how to describe emoji best . Our human judgement experiment involves six campaigns explaining Words ( W/ * ) or Emoji ( E/ * ) with Words , Figure 1 : The POLAR - framework ( Mathew et al , 2020 ) makes word embeddings interpretable leveraging polar opposites . It provides a new interpretable embedding subspace with systematic polar opposite scales : Along six use - cases , we evaluate which role emoji expressiveness plays in adding interpretability to word embeddings . I.e. , how well can our adopted POLAR \u03c1 interpret ( W/ * ) words or ( E/ * ) emoji with words , emoji or both ( * /M ) , Mixed . We test POLAR \u03c1 alignment with human judgement as represented in shown semantic profiles above . Emoji , or both Mixed . We evaluate two test conditions to answer both research questions : ( RQ1 ) a selection test studies if human subjects agree to the POLAR \u03c1 identified differentials ( e.g. , how do emoji affect POLAR \u03c1 interpretability ? ) , and ( RQ2 ) a preference test that studies if the direction on a given differential scale is in line with human judgement ( e.g. , how well does POLAR \u03c1 interpret scales ) . Results . POLAR \u03c1 identifies the best interpretable opposites for describing emoji with emoji , yet generally aligning well with human judgement . Except interpreting words with emoji only probably due to lack of emoji expressiveness indicated by coder agreement . Further , POLAR \u03c1 estimates an embedded terms ' position on a scale between opposites successfully , especially for interpreting emoji . Broader application . Not all emoji have a universally agreed on meaning . Prior work showed that differences in the meaning of emoji exist between cultures ( Guntuku et al , 2019 ; Gupta et al , 2021 ) . Even within the same culture , ambiguity and double meanings of emoji exist ( Reelfs et al , 2020 ) . Currently , no data - driven approach exists to infer the meaning of emoji - to make them interpretable . Our proposed approach can be used to tackle this challenge since it makes emoji interpretable .", "entities": [[14, 16, "TaskName", "word embeddings"], [109, 111, "TaskName", "word embeddings"], [392, 394, "TaskName", "word embeddings"], [449, 451, "TaskName", "word embeddings"], [486, 488, "TaskName", "word embeddings"]]}
{"text": "Semantic Differentials . Based upon the idea of semantic differentials as a psychometric tool to align a word on a scale between two polar opposites ( Fig . 1 ) , POLAR ( Mathew et al , 2020 ) takes a word embedding as input and creates a new interpretable embedding on a polar subspace . This subspace , i.e. , the opposites used for the interpretable embedding are defined by an external source . That is , starting with a corpus and its vocabulary V , a word embedding created by an algorithm a ( e.g. , Word2Vec or GloVe ) assigns vectors \u2212 W a v R d on d dimensions to all words v V according to an optimization function ( usually word co - occurrence ) . This pretraining results in an embedding D = \u2212 W a v , v V R | V | \u00d7d . Such embedding spaces carry a semantic structure between embedded words , whereas the dimensions do not have any specific meaning . However , we can leverage the semantic structure between words to transform the embedding space to carrying over meaning into the dimensions : POLAR uses N semantic differentials / opposites that are itself items within the embedding , i.e. , P = ( p i z , p i \u2212z ) , i [ 1 .. N ] , ( p i z , p i \u2212z ) \u2286 V 2 . As shown in Fig . 2a , given two anchor points for each polar opposite , a line between them represents a differential - which we name POLAR direction ) onto this subspace ( e.g. , left : / , right : / ) yields a direct scale measure between both opposites in the adjacent leg ( green vectors , directed alike the differential ) . ( c ) The resulting interpretable embedding now contains a tangible position estimation along employed polar dimensions for each embedded term ( here : ) . ( red dashed vectors ) : \u2212 \u2212 dir i = \u2212 \u2212 W a p i z \u2212 \u2212 \u2212\u2212 W a p i \u2212z R d Base Change . Naturally , we can use these differentials as a new basis for the interpretable embedding E. Gathering all directions in a matrix dir R N \u00d7d , we obtain for all embedded terms v V : dir T \u2212 E v = \u2212 W a v ,", "entities": [[100, 101, "MethodName", "GloVe"]]}
{"text": "We next propose an approach to improve the interpretability of word embeddings by adding emoji . It uses our extended version POLAR \u03c1 and adds emoji to the POLAR space by creating word embeddings that include emoji .", "entities": [[10, 12, "TaskName", "word embeddings"], [32, 34, "TaskName", "word embeddings"]]}
{"text": "We create a word embedding out of a social media text corpus , since emoji are prominent in communication within Online Social Networks . We decided to use a corpus from the Jodel network , where about one out of four sentences contain emoji ( see ( Reelfs et al , 2020 ) ) . The Jodel Network . We base our study on a country - wide complete dataset of posts in the online social network Jodel , a mobile - only messaging application . It is location - based and establishes local communities relative to the users ' location . Within these communities , users can anonymously post photos from the camera app or content of up to 250 characters length , i.e. , microblogging , and reply to posts forming discussion threads . Corpus . The network operators provided us with data of content created in Germany from 2014 to 2017 . It contains 48 M sentences , of which 11 M contain emoji ( 1.76 emoji per sentence on average ) . Ethics . The dataset contains no personal informa - tion and can not be used to personally identify users except for data that they willingly have posted on the platform . We synchronize with the Jodel operator on analyses we perform on their data .", "entities": [[176, 177, "DatasetName", "Ethics"]]}
{"text": "No universal meaning of emoji . Prior work showed that the interpretation of emoji varies ( Miller et al , 2016 ; Kimura - Thollander and Kumar , 2019 ) , also between cultures ( Guntuku et al , 2019 ; Gupta et al , 2021 ) . Even within the same culture , ambiguity and double meanings of emoji exist ( Reelfs et al , 2020 ) and differences exists on the basis of an individual usage ( Wiseman and Gould , 2018 ) . These observations motivate the need to better understand the meaning of emoji . Currently , no data - driven approach exists to make emoji interpretable - a gap that we aim to close . Interpretable word embeddings . Word embeddings are a common approach to capture meaning ; they are a learned vector space representation of text that carries semantic relationships as distances between the embedded words . A rich body of work aims at making word embeddings interpretable , e.g. , via contextual ( Subramanian et al , 2018 ) , sparse embeddings ( Panigrahi et al , 2019 ) , or learned ( Senel et al , 2018 transformations ( Mathew et al , 2020 ) - all focus on text only . Recently , ( Mathew et al , 2020 ) proposed the PO - LAR that takes a word embedding as input and creates a new interpretable embedding on a polar subspace . The POLAR approach is similar to SEMCAT ( Senel et al , 2018 ) , but is based on the concept of semantic differentials ( Osgood et al , 1957 ) for creating a polar subspace . It measures the meaning of abstract concepts by relying on opposing dimensions associated ( good vs. bad , hot vs. cold , conservative vs. liberal ) . In this work , we extend and use POLAR . Emoji embeddings . Few works focused on using word embeddings for creating emoji representations , e.g. , ( Eisner et al , 2016 ) or ( Reelfs et al , 2020 ) . ( Barbieri et al , 2016 ) used a vector space skip - gram model to infer the meaning of emoji in Twitter data ( Barbieri et al , 2016 ) . Yet , the general question if the interpretability of word embeddings can be improved by adding emoji and if different meaning of emoji can be captured remains still open . In this work , we adapt the POLAR interpretability approach to emoji and study in a human subject experiment if word embeddings can be made interpretable by adding emoji and how emoji can be interpretated by emoji .", "entities": [[26, 27, "DatasetName", "Kumar"], [121, 123, "TaskName", "word embeddings"], [124, 126, "TaskName", "Word embeddings"], [162, 164, "TaskName", "word embeddings"], [249, 250, "DatasetName", "SEMCAT"], [325, 327, "TaskName", "word embeddings"], [391, 393, "TaskName", "word embeddings"], [432, 434, "TaskName", "word embeddings"]]}
{"text": "We raise the question whether we can leverage the expressiveness of emoji to make word embeddings interpretable . Thus , we use the POLAR framework ( Mathew et al , 2020 ) that creates interpretable word embeddings through semantic differentials , polar opposites . We employ a revised POLAR \u03c1 method that transforms arbitrary word embeddings to interpretable counterparts to which we added emoji . We base our evaluation on an off the shelf word - emoji embedding from a large social media corpus , resulting in an interpretable embedding based on semantic differentials , i.e. , antonym lists and polar emoji opposites . Via crowdsourced campaigns , we investigate the interpretable word - emoji embedding quality along six use - cases ( cf . Fig . 1 ) : Using word - & emoji - polar opposites ( or both Mixed ) , to interpret words ( W / W , W / E , W / M ) and emoji ( E / W , E / E , E / M ) , w.r.t . human interpretability . Overall , we find POLAR \u03c1 's interpretations w / wo emoji being well in line with human judgement . We show that explaining emoji with emoji ( E / E ) works statistically significantly best , whereas describing words with emoji ( W / E ) systematically yields the worst performance . We also find good alignment to human judgement estimating a term 's position on differential scales , using the POLAR \u03c1 - projection . That is , emoji can improve POLAR \u03c1 's capability in identifying most interpretable semantic differentials . We have demonstrated how emoji can be used to interpret other emoji using POLAR \u03c1 .", "entities": [[14, 16, "TaskName", "word embeddings"], [35, 37, "TaskName", "word embeddings"], [54, 56, "TaskName", "word embeddings"]]}
{"text": "We thank Felix Dommes , who was instrumental for this work by developing and implementing the POLAR \u03c1 projection approach and the Extremal Word Score in his Master Thesis .", "entities": [[24, 25, "MetricName", "Score"]]}
{"text": "NoahNMT at WMT 2021 : Dual Transfer for Very Low Resource Supervised Machine Translation", "entities": [[12, 14, "TaskName", "Machine Translation"]]}
{"text": "We reproduced the illustration of dual transfer from the original paper ( Zhang et al , 2021 ) , as shown in Figure 1 . This illustration shows the case of general transfer , where the high resource translation direction is A B , and the low resource translation direction is P Q. As discussed in the original paper , in many cases , it is possible to use shared target transfer ( B = Q ) or shared source transfer ( A = P ) . Taking chv ru as an example , we can choose en ru as the high resource translation direction , resulting in an instance of shared target transfer . In this shared task , when training the high resource translation model , we always initialize the shared language side with the pretrained language model BERT ( Devlin et al , 2019 ) .", "entities": [[140, 141, "MethodName", "BERT"]]}
{"text": "In our preliminary experiments , we found it beneficial to use a closely related language as the parent language . It is clear that there are several factors that should be taken into account , such as the degree of closeness , and the amount of resource for training the parent model . For Upper Sorbian , Czech ( cs ) is closely related to it , and Czech - German has a good amount of parallel data , so we directly choose Czech as the parent language . Chuvash , however , is a rather isolated language in the Turkic family . The closest language with usable data is Kazakh ( kk ) , but the amount of parallel data for Kazakh - Russian is relatively small , and we found it to be quite noisy . Therefore , we considered using English ( en ) as the parent language of Chuvash . Even though English is unrelated to Chuvash and they use different scripts , English - Russian has more parallel data that can guarantee the quality of the parent model . We conducted an experiment with Transformer base . Results in Table 2 indicate that English can serve as an eligible parent for Chuvash . Considering that we plan to use Transformer big for which data amount is likely to play a more important role , we decided to use English as the parent language for Chuvash .", "entities": [[189, 190, "MethodName", "Transformer"], [214, 215, "MethodName", "Transformer"]]}
{"text": "The original paper ( Zhang et al , 2021 ) evaluated dual transfer only with Transformer base . In this shared task , we scale up to Transformer big . We also face a more realistic setting where the monolingual data for the low resource languages ( chv and hsb ) are quite scarce . Therefore it is worth testing the effect of scaling up . Results in Table 3 show that Transformer big brings consistent improvements . We also report the runtime of each step in dual transfer for NMT chv ru with Transformer big in Table 4 for reference , but the numbers can vary depending on implementation and data size . In the following experiments and our final submission , we use Transformer big models .", "entities": [[15, 16, "MethodName", "Transformer"], [27, 28, "MethodName", "Transformer"], [72, 73, "MethodName", "Transformer"], [94, 95, "MethodName", "Transformer"], [125, 126, "MethodName", "Transformer"]]}
{"text": "For chv ru and ru chv , we perform selected finetuning starting from the best models from iterative back - translation ( Iteration 2 for chv ru , Iteration 3 for ru chv ) . Note that the selected training subsets are different from those in Section 4.4 because the selection is based on the source side of the blind test sets . We finetune five times with different random seeds for model ensemble . For hsb de and de hsb , we ensemble the five models from iterative back - translation .", "entities": [[70, 71, "DatasetName", "seeds"]]}
{"text": "In this paper , we describe a series of experiments that contribute to our submission to the WMT 2021 shared task of Very Low Resource Supervised Machine Translation . These experiments , as well as the good results of the final submission , show that dual transfer can work in synergy with several widely used techniques in realistic scenarios .", "entities": [[26, 28, "TaskName", "Machine Translation"]]}
{"text": "Towards Generative Aspect - Based Sentiment Analysis *", "entities": [[2, 7, "TaskName", "Aspect - Based Sentiment Analysis"]]}
{"text": "Aspect - based sentiment analysis ( ABSA ) has received increasing attention recently . Most existing work tackles ABSA in a discriminative manner , designing various task - specific classification networks for the prediction . Despite their effectiveness , these methods ignore the rich label semantics in ABSA problems and require extensive task - specific designs . In this paper , we propose to tackle various ABSA tasks in a unified generative framework . Two types of paradigms , namely annotation - style and extraction - style modeling , are designed to enable the training process by formulating each ABSA task as a text generation problem . We conduct experiments on four ABSA tasks across multiple benchmark datasets where our proposed generative approach achieves new state - of - the - art results in almost all cases . This also validates the strong generality of the proposed framework which can be easily adapted to arbitrary ABSA task without additional taskspecific model design . 1", "entities": [[0, 5, "TaskName", "Aspect - based sentiment analysis"], [103, 105, "TaskName", "text generation"]]}
{"text": "Aspect - based sentiment analysis ( ABSA ) , aiming at mining fine - grained opinion information towards specific aspects , has attracted increasing attention in recent years ( Liu , 2012 ) . Multiple fundamental sentiment elements are involved in ABSA , including the aspect term , opinion term , aspect category , and sentiment polarity . Given a simple example sentence \" The pizza is delicious . \" , the corresponding elements are \" pizza \" , \" delicious \" , \" food quality \" and \" positive \" , respectively . The main research line of ABSA focuses on the identification of those sentiment elements such as extracting the aspect term ( Liu et al , 2015 ; Yin et al , 2016 ; Li et al , 2018 ; Ma et al , 2019 ) or classifying the sentiment polarity for a given aspect ( Wang et al , 2016 ; Chen et al , 2017 ; Jiang et al , 2019 ; Zhang and Qian , 2020 ) . To provide more detailed information , many recent studies propose to jointly predict multiple elements simultaneously ( Li et al , 2019a ; Wan et al , 2020 ; Peng et al , 2020 ; Zhao et al , 2020 ) . Taking the Unified ABSA ( UABSA , also called End - to - End ABSA ) task as an example , it tries to simultaneously predict the mentioned aspect terms and the corresponding sentiment polarities ( Luo et al , 2019 ; He et al , 2019 ) . In general , most ABSA tasks are formulated as either sequence - level or token - level classification problems ( Li et al , 2019b ) . By designing taskspecific classification networks , the prediction is made in a discriminative manner , using the class index as labels for training ( Huang and Carley , 2018 ; Wan et al , 2020 ) . However , these methods ignore the label semantics , i.e. , the meaning of the natural language labels , during the training process . Intuitively , knowing the meaning of \" food quality \" and \" restaurant ambiance \" , it can be much easier to identify that the former one is more likely to be the correct aspect category for the concerned aspect \" pizza \" . Such semantics of the label can be more helpful for the joint extraction of multiple sentiment elements , due to the complicated interactions of those involved elements . For example , understanding \" delicious \" is an adjective for describing the food such as \" pizza \" could better lead to the prediction of aspect opinion pair ( \" pizza \" , \" delicious \" ) . Another issue is that different classification models are proposed to suit the need of different ABSA problems , making it difficult to adapt the model from one to another . Motivated by recent success in formulating sev - eral language understanding problems such as named entity recognition , question answering , and text classification as generation tasks ( Raffel et al , 2020 ; Athiwaratkun et al , 2020 ) , we propose to tackle various ABSA problems in a unified generative approach in this paper . It can fully utilize the rich label semantics by encoding the natural language label into the target output . Moreover , this unified generative model can be seamlessly adapted to multiple tasks without introducing additional task - specific model designs . In order to enable the Generative Aspect - based Sentiment analysis ( GAS ) , we tailor - make two paradigms , namely annotation - style and extractionstyle modeling to transform the original task as a generation problem . Given a sentence , the former one adds annotations on it to include the label information when constructing the target sentence ; while the latter directly adopts the desired natural language label of the input sentence as the target . The original sentence and the target sentence produced by either paradigm can then be paired as a training instance of the generation model . Furthermore , we propose a prediction normalization strategy to handle the issue that the generated sentiment element falls out of its corresponding label vocabulary set . We investigate four ABSA tasks including Aspect Opinion Pair Extraction ( AOPE ) , Unified ABSA ( UABSA ) , Aspect Sentiment Triplet Extraction ( ASTE ) , and Target Aspect Sentiment Detection ( TASD ) with the proposed unified GAS framework to verify its effectiveness and generality . Our main contributions are 1 ) We tackle various ABSA tasks in a novel generative manner ; 2 ) We propose two paradigms to formulate each task as a generation problem and a prediction normalization strategy to refine the generated outputs ; 3 ) We conduct experiments on multiple benchmark datasets across four ABSA tasks and our approach surpasses previous state - of - the - art in almost all cases . Specifically , we obtain 7.6 and 3.7 averaged gains on the challenging ASTE and TASD task respectively . 2 Generative ABSA ( GAS )", "entities": [[0, 5, "TaskName", "Aspect - based sentiment analysis"], [508, 511, "TaskName", "named entity recognition"], [512, 514, "TaskName", "question answering"], [516, 518, "TaskName", "text classification"], [598, 603, "TaskName", "Aspect - based Sentiment analysis"], [741, 745, "TaskName", "Aspect Sentiment Triplet Extraction"]]}
{"text": "In this section , we describe the investigated ABSA tasks and the proposed two paradigms , namely , annotation - style and extraction - style modeling . Aspect Opinion Pair Extraction ( AOPE ) aims to extract aspect terms and their corresponding opinion terms as pairs ( Zhao et al , 2020 ; . Here is an illustrative example of our generative formulations for the AOPE task : Input : Salads were fantastic , our server was also very helpful . Target ( Annotation - style ) : [ Salads | fantastic ] were fantastic here , our [ server | helpful ] was also very helpful . Target ( Extraction - style ) : ( Salads , fantastic ) ; ( server , helpful ) In the annotation - style paradigm , to indicate the pair relations between the aspect and opinion terms , we append the associated opinion modifier to each aspect term in the form of [ aspect | opinion ] for constructing the target sentence , as shown in the above example . The prediction of the coupled aspect and opinion term is thus achieved by including them in the same bracket . For the extraction - style paradigm , we treat the desired pairs as the target , which resembles direct extraction of the expected sentiment elements but in a generative manner . Unified ABSA ( UABSA ) is the task of extracting aspect terms and predicting their sentiment polarities at the same time ( Li et al , 2019a ; Chen and Qian , 2020 ) . We also formulate it as an ( aspect , sentiment polarity ) pair extraction problem . For the same example given above , we aim to extract two pairs : ( Salads , positive ) and ( server , positive ) . Similarly , we replace each aspect term as [ aspect | sentiment polarity ] under the annotation - style formulation and treat the desired pairs as the target output in the extraction - style paradigm to reformulate the UABSA task as a text generation problem . As shown above , we annotate each aspect term with its corresponding sentiment triplet wrapped in the bracket , i.e. , [ aspect | opinion | sentiment polarity ] for the annotation - style modeling . Note that we will include all the opinion modifiers of the same aspect term within the same bracket to predict the sentiment polarities more accurately . For the extraction - style paradigm , we just concatenate all triplets as the target output . Target Aspect Sentiment Detection ( TASD ) is the task to detect all ( aspect term , aspect category , sentiment polarity ) triplets for a given sentence ( Wan et al , 2020 ) , where the aspect category belongs to a pre - defined category set . For example , Input : A big disappointment , all around . The pizza was cold and the cheese was n't even fully melted . Similarly , we pack each aspect term , the aspect category it belongs to , and its sentiment polarity into a bracket to build the target sentence for the annotation - style method . Note that we use a bigram expression for the aspect category instead of the original uppercase form \" FOOD#QUALITY \" to make the annotated target sentence more natural . As presented in the example , some triplets may not have explicitly - mentioned aspect terms , we thus use \" null \" to represent it and put such triplets at the end of the target output . For the extraction - style paradigm , we concatenate all the desired triplets , including those with implicit aspect terms , as the target sentence for sequence - to - sequence learning .", "entities": [[347, 349, "TaskName", "text generation"]]}
{"text": "Given the input sentence x , we generate a target sequence y , which is either based on the annotationstyle or extraction - style paradigm as described in the last section , with a text generation model f ( ) . Then the desired sentiment pairs or triplets s can be decoded from the generated sequence y . Specifically , for the annotation - style modeling , we extract the contents included in the bracket \" [ ] \" from y , and separate different sentiment elements with the vertical bar \" | \" . If such decoding fails , e.g. , we can not find any bracket in the output sentence or the number of vertical bars is not as expected , we ignore such predictions . For the extractionstyle paradigm , we separate the generated pairs or triplets from the sequence y and ignore those invalid generations in a similar way . We adopt the pre - trained T5 model ( Raffel et al , 2020 ) as the generation model f ( ) , which closely follows the encoder - decoder architecture of the original Transformer ( Vaswani et al , 2017 ) . Therefore , by formulating these ABSA tasks as a text generation problem , we can tackle them in a unified sequence - to - sequence framework without taskspecific model design .", "entities": [[34, 36, "TaskName", "text generation"], [160, 161, "MethodName", "T5"], [188, 189, "MethodName", "Transformer"], [206, 208, "TaskName", "text generation"]]}
{"text": "We adopt F1 scores as the main evaluation metrics for all tasks . A prediction is correct if and only if all its predicted sentiment elements in the pair or triplet are correct .", "entities": [[2, 3, "MetricName", "F1"]]}
{"text": "We adopt the T5 base model from huggingface Transformer library 2 for 2 https://github.com/huggingface/ transformers", "entities": [[3, 4, "MethodName", "T5"], [8, 9, "MethodName", "Transformer"]]}
{"text": "To better understand the effectiveness of the proposed prediction normalization strategy , we randomly sample some instances from the ASTE task that have different raw prediction and normalized prediction ( i.e. , corrected by our strategy ) . The predicted sentiment elements before and after the normalization , as well as the gold label of some example cases are shown in Table 5 . We find that the normalization mainly helps on two occasions : The first one is the morphology shift where two words have minor lexical differences . For example , the method fixes \" Bbq rib \" to \" BBQ rib \" ( # 1 ) and \" repeat \" to \" repeats \" ( # 2 ) . Another case is orthographic alternatives where the model might generate words with the same etyma but different word types , e.g. , it outputs \" vegetarian \" rather than \" vegan \" ( # 6 ) . Our proposed prediction normalization , which finds the replacement from the corresponding vocabulary set via Levenshtein distance , is a simple yet effective strategy to alleviate this issue . We also observe that our prediction strategy may fail if the raw predictions are quite lexically different or even semantically different from the goldstandard labels ( see Case # 4 , # 7 and # 8 ) . In these cases , the difficulty does not come from the way of performing prediction normalization but the generation of labels close to the ground truths , especially for the examples containing implicit aspects or opinions ( Case # 4 ) .", "entities": [[97, 98, "DatasetName", "Bbq"], [102, 103, "DatasetName", "BBQ"]]}
{"text": "We tackle various ABSA tasks in a novel generative framework in this paper . By formulating the target sentences with our proposed annotation - style and extraction - style paradigms , we solve multiple sentiment pair or triplet extraction tasks with a unified generation model . Extensive experiments on multiple benchmarks across four ABSA tasks show the effectiveness of our proposed method . Our work is an initial attempt on transforming ABSA tasks , which are typically treated as classification problems , into text generation problems . Experimental results indicate that such transformation is an effective solution to tackle various ABSA tasks . Following this direction , designing more effective generation paradigms and extending such ideas to other tasks can be interesting research problems for future work .", "entities": [[83, 85, "TaskName", "text generation"]]}
{"text": "Role - Playing in Open - Domain Dialogue Much recent work has explored training open - domain dialogue models on large and small dialogue corpora , with the former imbuing raw conversational ability and the latter providing necessary conversational skills . Most crowd - sourced datasets require acting out a role to some capacity in conversation ( though indeed Mazar\u00e9 et al ( 2018 ) study extraction of roles from raw data ) . Some involve providing persona lines that a model must assume throughout the conversation ( Zhang et al , 2018 ; ; others require more subtle \" roles \" , such as a listener ( Rashkin et al , 2019 ) , or a teacher and student ( Dinan et al , 2019b ; Gopalakrishnan et al , 2019 ; Zhou et al , 2018 ; Komeili et al , 2021 ) . Zheng et al ( 2020 ) explore using a discriminative model to predict whether model responses contain similarity with their persona , similar to methods we employ in our work . Consistency in Open - Domain Dialogue A common paradigm in the state of the art of open - domain dialogue involves concatenating all relevant contextual information as input to a sequence to sequence neural model ( e.g. , transformers ( Vaswani et al , 2017 ) ) to obtain a conditioned response . Such models can yield human - like and engaging responses ( Adiwardana et al , 2020 ; Roller et al , 2021 ) . Nevertheless , various consistency issues still plague such models . Recent studies have indicated that hallucination of incorrect knowledge is still far from a solved issue Santhanam et al , 2021 ) , with some proposing specific datasets and tools for measuring precisely the levels of this undesired attribute . Another clear example of failure is the short - term memory of state - of - the - art models , sometimes due to the lack of long - form training data or long - context models but often due to simply the modeling itself . To address consistency issues , a variety of methods have been explored . In the context of knowledge - grounded dialogue , different ways to attend most effectively over provided contextual information have been explored ( Zheng and Zhou , 2019 ; Ye et al , 2020 ; Prabhumoye et al , 2021 ; Wang et al , 2019 ) . These works find that considering factual documents separately ( in some capacity ) improves model grounding . We explore such methods , but in the context of character identity . Another general problem is that of contradictions . Nie et al ( 2021 ) collect a dataset of contradictions in dialogue , and train classifiers that help re - rank model outputs at inference time ; explore unlikelihood training to reduce repetition and contradiction , among other undesired traits , in model generations . The character identity issue we study in this work can be seen as an important class of contradictions , but to the best of our knowledge , has not been explicitly focused on .", "entities": [[207, 210, "MethodName", "sequence to sequence"]]}
{"text": "We consider a two - party chat setting . The context provided to a model includes : ( i ) the name of its character and the partner 's character ; ( ii ) an extended description of its own character ; ( iii ) and , information about the area in which the conversation takes place . The responsibility of the model is to engage its conversational partner , with no other goal prescribed ; however , it should stay within character and within the bounds of the defined setting . We operate in the context of LIGHT ( Urbanek et al , 2019 ) , consisting of grounded fantasy roleplaying game conversations . The LIGHT environment involves humans and models interacting with thousands of objects in hundreds of locations , all while assuming the roles of one of hundreds of characters . The dataset consists of roughly 8.5k dialogues spanning 111k utterances . It is an ideal setting for this study because of the rich and varied personas with explicit backstories . To quantify the character identity problem , we take a state - of - the - art dialogue agent ( specifically , BlenderBot ( Roller et al , 2021 ) ) fine - tuned on the LIGHT dialogue dataset and ask human annotators if the agent mistakes its identity based on its utterances in context . The agent conditions its response on the LIGHT context and prior utterances in the dialogue history . We see in Table 4 that in roughly 6.5 percent of utterances the model mistakes its identity ; this corresponds to a mistake in approximately 35 percent of conversations . BlenderBot uses a Byte - Level BPE tokenizer ( Radford et al , 2019 ) ; an artifact from the Blender - Bot pre - training is that it only considers 128 such tokens in the past , and thus has no mechanism for recovering truncated information about the LIGHT context in later conversational turns . Our second baseline lengthens the input context to 1024 BPE tokens , which allows the entire context for every example to fit into the truncation length of the model ; we follow methods employed in to extend the positional embeddings of the model . We see in Table that this actually makes the problem worse , resulting in 7.4 percent of utterances with mistaken identity ( corresponding to a failure in approximately 38 percent of conversations ) .", "entities": [[192, 193, "DatasetName", "agent"], [219, 220, "DatasetName", "agent"], [231, 232, "DatasetName", "agent"], [283, 284, "MethodName", "BPE"], [297, 298, "MethodName", "Blender"], [342, 343, "MethodName", "BPE"]]}
{"text": "In this section we describe several strategies for improving the role - playing accuracy of dialogue agents , specifically ways to improve our transformer baselines .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "We explore utilizing an unlikelihood ( UL ) loss While training on the LIGHT dataset with standard NLL loss , with some fixed probability we consider a candidate model generation for UL loss . The full generation is sent to the RPA classifier ; if the generation is classified as coming from the incorrect character , we examine each partial generated sequence of the output , and send these sequences to the LTR RPA classifier to determine whether the candidate partial sequences match the model 's character . We apply UL loss to tokens that yield the wrong character classification .", "entities": [[8, 9, "MetricName", "loss"], [17, 18, "MetricName", "NLL"], [18, 19, "MetricName", "loss"], [32, 33, "MetricName", "loss"], [91, 92, "MetricName", "loss"]]}
{"text": "The RPA classifiers utilize the LIGHT setting and prior utterances of dialogue history to determine which character generates a candidate response . We hypothesize that the generation models themselves should be able to pick out and utilize these components as well . However , the RPA classifier models are trained explicitly for this task , whereas the seq2seq models are trained only to generate a plausible continuation of a dialogue history . We thus explore a setup in which the generation models are trained to identify the speaker of an utterance as well . To do this , we use the output representations from the model ( either encoder + decoder , or decoder only ) as inputs to n M O additional transformer layers , where we vary n M O { 0 , 2 } . The final outputs are used to compute a character score , similarly to the RPA classifier . The model can then be trained piece - wise . After initializing the model weights with those trained on the LIGHT response generation task , we then train only the extra layers with only the character classification objective ; once the classifier achieves suitable performance on the task , we can begin to back - propagate the character classification objective multi - tasking with the dialogue task itself to the generation model directly , in the hope that the model learns to update its internal representations of the context and/or the decoded response .", "entities": [[57, 58, "MethodName", "seq2seq"], [133, 134, "DatasetName", "0"], [176, 178, "TaskName", "response generation"]]}
{"text": "Maintaining identity relies on the model 's capacity to understand which inputs from the conversational history are pertinent when generating a continuation of the preceding dialogue . In a standard , opendomain chit - chat scenario , the model has free reign to decide which elements of the context it would like to condition on when generating a response , as we are dealing with a nearly unconstrained output space ( so long as the output follows plausibly from the input ) . In LIGHT , however , we want to emphasize certain components of the context more so than others ; specifically , when role - playing as a character , we want the model to always be reminded of its role , so that it can conditionally generate an optimal response while staying in character . In this lens , one can view the task as \" grounding \" on one 's character information when conversing . Profile Grounding Inspired by models demonstrating good performance in knowledge - grounded dialogue ( Zheng and Zhou , 2019 ; Ye et al , 2020 ; Prabhumoye et al , 2021 ; Wang et al , 2019 ) , we propose a simple extension to the transformer seq2seq architecture , specifically the decoder , to ensure the model knows to condition on the pro - file . The standard transformer decoder first uses self - attention over the decoded response , and then cross - attention over the encoder outputs . We add a third attention step , expanded attention , that attends again over an extracted subset of the input context ( encoded separately from the normal context ) . We explore various subsets of the context to determine which are most important for both RPA and other automated metrics , and call this method \" Profile \" grounding as the subsets generally include the character and role description . We utilize the exact same ( shared ) parameters for both the normal cross - attention and the expanded attention ; thus , model size is not affected . Automated Grounding Instead of directly telling the model what to re - attend to , we also explore whether the model can learn to do this automatically , based on its own ( or other ) representations of the context . The first method we consider is examining the decoder attention weights . Specifically , we use the attention weights from the decoder over the full context to choose k tokens to re - attend to . This operation is done on a per - layer basis , and thus allows different decoder layers to re - attend to ( potentially different ) components of the input . The second method we consider is a trainable mask ; this involves feeding the encoded context through a \" mask \" layer to select various tokens to re - attend to . Specifically , we feed the context through a linear projection layer followed by a softmax to select the top - k tokens . This set of tokens is then re - encoded by the encoder and fed to the decoder as the expanded attention context . Finally , we explore using the classifier attention weights over the context from the RPA classifier itself . Intuitively , the RPA classifier has learned what components of the input are necessary for determining which character is speaking ; if we look at these attention weights when considering the model 's character , we know what the classifier thinks is important to use .", "entities": [[161, 162, "DatasetName", "Inspired"], [206, 207, "MethodName", "seq2seq"], [228, 230, "MethodName", "transformer decoder"], [503, 504, "MethodName", "softmax"]]}
{"text": "We first assess the quality of our RPA classifiers . We measure hits@1/427 , where the model must correctly identify the character speaking out of 427 characters from the validation set , comparing the standard and left - to - right ( LTR ) models in Table 2 . We experiment with either 0 , 4 , or All prior context utterances . The LTR classifiers perform nearly as well as the full classifiers on the full datasplit , and outperform them on the LTR split . Given the robustness of the LTR RPA classifiers , we use this model for computing RPA throughout the remaining results , unless otherwise specified . Further results are given in Appendix Table 10 .", "entities": [[53, 54, "DatasetName", "0"]]}
{"text": "We next train baseline models for the dialogue generation task itself . Performance on the LIGHT dataset test split for our baseline models can be found in detailed training and optimization specifications are given in Appendix A.", "entities": [[7, 9, "TaskName", "dialogue generation"]]}
{"text": "All models are trained with the ParlAI 2 framework ( Miller et al , 2017 ) . Due to the large number of experimental setups and computational cost , we do not consider multiple training runs . Base Models RPA classifier Poly - encoders are initialized with the 622 M parameter models from Roller et al ( 2021 ) ; we also use this architecture for dialogue response ( retrieval ) models which we also evaluate ( see Table 19 ) . All generative models are initialized with BlenderBot , also from Roller et al ( 2021 ) , a 2.7B parameter transformer encoder / decoder model . Each model was pre - trained on 1.5B training examples from pushshift.io Reddit ( Baumgartner et al , 2020 ) , with BlenderBot additionally fine - tuned on the BST tasks ( see Roller et al ( 2021 ) for more details ) , before training on LIGHT .", "entities": [[120, 121, "DatasetName", "Reddit"]]}
{"text": "Train Valid Test LIGHT ( Urbanek et al , nation of ( 1 ) the LIGHT context ( set of characters , setting , etc . ) ; ( 2 ) a fixed number of previous utterances in the conversation ; and ( 3 ) a candidate utterance from any point later in the conversation ( a special token separates the candidate utterance from the prior context ) . We experiment with either 0 , 4 , or N \u2212 2 prior utterances ( dubbed \" All \" in relevant tables ) , where N is the total number of utterances ( N \u2212 2 allows the last turn for each speaker to be a candidate utterance ) . The left - toright ( LTR ) data split is built similarly , except each example i becomes w i examples , where w i is the number of tokens in the candidate utterance for example i. Statistics of the training dataset are given in Table 8 . Suppose we choose n as the number of prior utterances to include in the input , and let us denote D = 8538 to represent all the dialogues in the LIGHT train split , and U = 110877 to represent all the utterances in those dialogues . For the RPA classification dataset , each dialogue is presented twice , once from each character 's POV . For any value 0 < n < N \u2212 1 , we build out several examples from several slices of each conversation . Suppose we have dialogue d i with N utterances { u 0 , u 1 , ... , u N } . To build the training data from dialogue d i , we select all continuous subsets of n utterances within d i , forming contexts c i = { u i , ... , u i+n } \u2200 0 \u2264 i \u2264 N \u2212 i Then , we look at all N \u2212 i utterances following utterance u i+n , and use these as target utterances in the task . The goal of this is to build the model to be robust to dataset artifacts ; without this modification , the model could trivially pick out the character just by looking at the number of alternating utterances . These measures force the model to fully understand the task and react accordingly .", "entities": [[73, 74, "DatasetName", "0"], [236, 237, "DatasetName", "0"], [267, 268, "DatasetName", "0"], [315, 316, "DatasetName", "0"]]}
{"text": "Dynamic Sentence Boundary Detection for Simultaneous Translation", "entities": [[2, 4, "TaskName", "Boundary Detection"], [6, 7, "TaskName", "Translation"]]}
{"text": "Simultaneous Translation is a great challenge in which translation starts before the source sentence finished . Most studies take transcription as input and focus on balancing translation quality and latency for each sentence . However , most ASR systems can not provide accurate sentence boundaries in realtime . Thus it is a key problem to segment sentences for the word streaming before translation . In this paper , we propose a novel method for sentence boundary detection that takes it as a multi - class classification task under the endto - end pre - training framework . Experiments show significant improvements both in terms of translation quality and latency .", "entities": [[1, 2, "TaskName", "Translation"], [75, 77, "TaskName", "boundary detection"], [82, 86, "TaskName", "multi - class classification"]]}
{"text": "Recent studies show that the pre - training and finetuning framework achieves significant improvements in various NLP tasks . Generally , a model is first pre - trained on large unlabeled data . After that , on the fine - tuning step , the model is initialized by the parameters obtained by the pre - training step and fine - tuned using labeled data for specific tasks . Devlin et al ( 2019 ) proposed a generalized framework BERT , to learn language representations based on a deep Transformer ( Vaswani et al , 2017 ) encoder . Rather than traditionally train a language model from - left - to - right or from - rightto - left , they proposed a masked language model ( MLM ) that randomly replace some tokens in a sequence by a placeholder ( mask ) and trained the model to predict the original tokens . They also pre - train the model for the next sentence prediction ( NSP ) task that is to predict whether a sentence is the subsequent sentence of the first sentence . Sun et al ( 2019 ) proposed a pre - training framework ERNIE , by integrating more knowledge . Rather than masking single tokens , they proposed to mask a group of words on different levels , such as entities , phrases , etc . The model achieves state - of - theart performances on many NLP tasks . In this paper , we train our model under the ERNIE framework .", "entities": [[78, 79, "MethodName", "BERT"], [88, 89, "MethodName", "Transformer"], [126, 127, "DatasetName", "MLM"]]}
{"text": "For a streaming input x = { x 1 , ... , x t } , our goal is to detect whether there is a sentence boundary till the current word x t from last sentence boundary . Rather than a binary classification that detects whether x t is a sentence boundary , we propose a multi - class method . The classes are as follows : 0 1 2 \u2026 \u22122 \u22121 \u2026 \u210e \u210e 0 1 2 \u2026 \u22122 \u22121 \u2026 \u2026 \u2026 \u03d5 0 \u2212 1 \u2212 2 Classes Masked Language Model \u2026 \u210e \u210e \u2026 \u210e \u210e \" . \" \u2026 \u210e \u210e \" . \" \u2026 \u210e \" . \" \u210e y = \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u03c6 , no sentence boundary detected 0 , x t is the end of a sentence \u22121 , x t\u22121 is the end of a sentence ... \u2212M , x t\u2212M is the end of a sentence where M is the maximum offset size to the current state . Thus , we have M + 2 classes . See Figure 2 for illustration . We set M = 2 , indicating that the model predicts 4 classes for the input stream . If the output class is \u03c6 , meaning that the model does not detect any sentence boundary . Thus the model will continue receiving new words . If the output class is 0 , indicating that the current word x t is the end of a sentence and we put a period after the word . Similarly , class \u2212m denotes to add a sentence boundary after x t\u2212m . While a sentence boundary is detected , the sentence will be extracted from the stream and sent to the MT system as an input for translation . The sentence detection then continues from x t\u2212m+1 . Each time our system receives a new word x t , the classifier predicts probabilities for the last M +1 words as sentence boundaries . If the output class is \u03c6 , the classifier receives a new word x t+1 , and recompute the probabilities for x t+1 , x t , x t\u22121 , ... , x t\u2212M +1 . Generally , more contextual information will help the classifier improve the precision ( Section 4.5 ) .", "entities": [[67, 68, "DatasetName", "0"], [76, 77, "DatasetName", "0"], [86, 87, "DatasetName", "0"], [138, 139, "DatasetName", "0"], [246, 247, "DatasetName", "0"]]}
{"text": "Sentence boundary detection has been explored for years , but the majority of these work focuses on offline punctuation restoration , instead of applied in simultaneous translation . Existing work can be divided into two classes according to the model input .", "entities": [[1, 3, "TaskName", "boundary detection"]]}
{"text": "Nominal ellipsis has been a topic of interest in theoretical linguistics for a very long time ( Halliday and Hasan , 1976 ; Dalrymple et al , 1991 ; Lobeck , 1995 ; Lappin , 1996 ; Hobbs and Kehler , 1997 ; Hardt , 1999 ; Johnson , 2001 ; Wijnen et al , 2003 ; Merchant , 2004 ; Frazier , 2008 ; Chung et al , 2010 ; Mer - chant , 2010 ; Goksun et al , 2010 ; Gunther , 2011 ; Rouveret , 2012 ; Lindenbergh et al , 2015 ; van Craenenbroeck and Merchant , 2013 ; Park , 2017 ; Hyams et al , 2017 ; Kim et al , 2019 ) . Computational approaches to the ellipsis phenomenon majorly focus on the Verb Phrase Ellipsis ( VPE ) along with a few related phenomenon such as gapping , sluicing and do - so anaphora , for instance , the detection of VPE in the Penn Treebank using pattern match ( Hardt , 1992 ) , a transformation learning - based approach to generated patterns for VPE resolution ( Hardt , 1998 ) , the domain independent VPE detection and resolution using machine learning ( Nielsen , 2003 ) , automatically parsed text ( Nielsen , 2004b ) , sentence trimming methods ( McShane et al , 2015 ) , linguistic principles ( McShane and Babkin , 2016 ) , improved parsing techniques that encode elided material dependencies for reconstruction of sentences containing gapping ( Schuster et al , 2018 ) , discriminative and margin infused algorithms ( Dean et al , 2016 ) , Multilayer Perceptrons ( MLP ) and Transformers ( Zhang et al , 2019 ) . In recent times , there has been a surge in the computational research on nominal ellipsis and closely related phenomena ( Khullar et al , 2020 ( Khullar et al , , 2019Lapshinova - Koltunski et al , 2018 ; Menzel , 2017 ; Menzel and Lapshinova - Koltunski , 2014 ) . For the resolution process , we previously proposed a rule based system ( Khullar et al , 2019 ) that detects noun ellipsis using syntactic constraints on licensors of ellipsis and resolves them by matching Part - of - Speech ( POS ) tag similarity between the licensor of ellipsis and the modifier of the antecedent . It later fine tunes these syntactic rules on a small curated dataset that contains 234 instances of noun ellipsis along with some negative samples ( Khullar et al , 2019 ) . For the present paper , we further the research on noun ellipses by using the NoEl corpus annotated by us previously ( Khullar et al , 2020 ) to experiment with state - ofthe - art ML models .", "entities": [[163, 165, "DatasetName", "Penn Treebank"], [276, 277, "DatasetName", "MLP"], [376, 379, "DatasetName", "Part - of"]]}
{"text": "Following the VPE resolution framework presented by ( Zhang et al , 2019 ) , we investigate a similar framework for noun ellipsis resolution in English and present alternative choices of the models at each step as shown in Figure 2 . We use the NoEl corpus ( Khullar et al , 2020 ) that marks noun ellipsis instances as a separate layer ( using the stand - off annotation scheme ) on the Cornell Movie Dialogs corpus ( Danescu - Niculescu - Mizil and Lee , 2011 ) . The corpus marks a total of 946 annotations , of which 438 are described as endophoric , i.e. with a textual antecedent , and 508 exophoric , i.e. without a textual antecedent .", "entities": [[74, 75, "DatasetName", "Cornell"]]}
{"text": "From a given sentence , we first select all words belonging to the syntactic categories that can license noun ellipsis in English , i.e. cardinal and ordinal numbers , determiners and adjectives ( Ross , 1967 ; Lobeck , 1995 ; Mitkov , 1999 ; Saito et al , 2008 ; Kim et al , 2019 ; Khullar et al , 2019 ) using a POS tag filter . The POS tags are obtained from state - ofthe - art spaCy parser ( Honnibal and Johnson , 2015 ) . For simplicity , we refer to words with these categories as noun modifiers ( although in strict linguistic terms , this might be problematic ) . For each of these selected noun modifiers , we follow the task specification for VPE detection used by ( Nielsen , 2004a ; Bos and Spenader , 2011 ; Liu et al , 2016a ; Dean et al , 2016 ) and present noun ellipsis detection as a binary classification task , where given a noun modifier and the sentence in which it occurs as the input , the goal of the classifier is to predict whether the noun modifier licenses a noun ellipsis or not . Formally , for a given licensor word l i is a licensor in a sentence s , the task is represented as follows : f ( l i , s ) \u2212 { 0 , 1 } where 1 denotes that l i is a licensor in s , and 0 otherwise . We experiment with both static and contextualised word embeddings for word and context representation . For the former , we choose pretrained fastText ( FT ) word embeddings ( Bojanowski et al , 2016 ) as they provide representations for rare and unknown words that might be frequent in the movie dialogues . For the latter , we use pretrained BERT embeddings from the BERT base uncased wordpiece model for English ( Devlin et al , 2019 ) , as these currently offer the most powerful embeddings taking into account a large left and right context . fastText We take pretrained FT word embeddings for the noun modifier and sentence in which it is present and sum pool to obtain a single vector that we use to train our classifiers . For the statistical models , we choose Naive Bayes and Linear Support Vector Machine ( SVM ) , and use scikit learn ( Pedregosa et al , 2011 ) with 5 - fold cross validation for training and testing . We choose a BERT We separate the sentence and the licensor with a [ SEP ] token and keep the sequence length to 300 as this is the maximum sentence length in the training data . After creating the concatenated set of tokens , if the number of tokens are greater than 300 , we clip it to 300 , otherwise we add [ PAD ] tokens which correspond to the embedding of 768 dimensional zero - vector . The [ CLS ] output of the BERT model ( Devlin et al , 2019 ) is then fed into Naive Bayes , Linear SVM , MLP and bi - LSTM networks as above . Manual Syntactic Features For each of these models , we additionally experiment with manual syntactic features . We use the lexical features proposed by ( Dean et al , 2016 ) and extended lexical features by ( Zhang et al , 2019 ) , and take the five syntactic constraints on licensors of ellipsis explored by ( Khullar et al , 2019 ) for their rulebased approach as our slot pattern features . We concatenate all these features to the embeddings from the previous step and check if they improve the classification decision .", "entities": [[236, 237, "DatasetName", "0"], [253, 254, "DatasetName", "0"], [263, 265, "TaskName", "word embeddings"], [278, 279, "MethodName", "fastText"], [282, 284, "TaskName", "word embeddings"], [316, 317, "MethodName", "BERT"], [320, 321, "MethodName", "BERT"], [323, 324, "MethodName", "wordpiece"], [353, 354, "MethodName", "fastText"], [358, 360, "TaskName", "word embeddings"], [398, 401, "MethodName", "Support Vector Machine"], [402, 403, "MethodName", "SVM"], [430, 431, "MethodName", "BERT"], [491, 492, "DatasetName", "PAD"], [513, 514, "MethodName", "BERT"], [530, 531, "MethodName", "SVM"], [532, 533, "DatasetName", "MLP"], [536, 537, "MethodName", "LSTM"]]}
{"text": "We define noun ellipsis resolution as a binary classification task where given a licensor , antecedent candidate and their context , the goal of the classifier is to predict whether the antecedent candidate is the resolution of the ellipsis licensed by the licensor . Formally , given a sentence s , the licensor l i from the detection step , and the antecedent candidate a j ; the noun ellipsis resolution task can be defined as follows : f ( a j , l i , s ) \u2212 { 0 , 1 } where 1 denotes that the antecedent candidate a j is the actual resolution of the ellipsis licensed by l i , and 0 otherwise . Embeddings Similar to the detection step , we take pretrained fastText word embeddings for the licensor , antecedent candidate and context , and sum pool to obtain a single vector . In case of BERT , we separate the sentence , the licensor and the antecedent candidate with a [ SEP ] token and follow the same steps as in the detection step .", "entities": [[90, 91, "DatasetName", "0"], [116, 117, "DatasetName", "0"], [129, 130, "MethodName", "fastText"], [130, 132, "TaskName", "word embeddings"], [153, 154, "MethodName", "BERT"]]}
{"text": "We explored statistical and neural models for noun ellipsis detection and resolution , presenting a strong results for this task . As expected , neural classifiers perform significantly better than the statistical with the same input representation . As with several other NLP tasks , the contextual nature of BERT is useful for noun ellipsis resolution too , making robust predictions with simple neural classifiers . Finally , addition of manual features boosts the performance of all classifiers including those that use BERT , highlighting that ellipsis is a syntactically constrained phenomenon .", "entities": [[49, 50, "MethodName", "BERT"], [82, 83, "MethodName", "BERT"]]}
{"text": "Attention mechanisms ( Bahdanau et al , 2014 ) have achieved great success in various natural language processing ( NLP ) tasks . They are introduced to mimic the human eye focusing on important parts in the inputs when predicting labels . The existing studies show attention mechanisms can improve not only the performance but also the interpretability of the models ( Mullenbach et al , 2018 ; Xie et al , 2017 ; Xu et al , 2015 ) . Li et al ( 2016 ) pointed the view : \" Attention provides an important way to explain the workings of neural models \" . Additionally , Wiegreffe and Pinter ( 2019 ) showed that attention mechanisms could help understand the inner workings of a model . The basic assumption of understanding of models with attention scores is that the inputs ( e.g. , words ) with high attentive weights are essential for making decisions . However , as far as we know , it has not been formally verified . Existing research ( Jain and Wallace , 2019 ) also shows that attention is not explicable , and there are a lot of controversy regarding to the result explanations ( Wiegreffe and Pinter , 2019 ; Jain and Wallace , 2019 ) . Moreover , we find that though the attention mechanism can help improve the performance for text classification in our experiments , it may focus on the irrelevant information . For example , in the sentence \" A very funny movie . \" , the long short - term memory model with standard attention ( LSTM - ATT ) infers a correct sentiment label while pays more attention to the irrelevant word \" movie \" , making the result difficult to explain . In general , the attention weights are only optimized to encode the task - relevant information while are not restricted to imitate human behavior . In order to enhance the interpretability of the attention mechanism , recent studies turn to integrate the human provided explanation signals into the attention models . regularized the attention weights with a small amount of word - level annotations . Barrett et al ( 2018 ) ; Bao et al ( 2018 ) improved the explanation of attention by aligning explanations with human - provided rationales . These methods rely on additional labour consuming labelling for enhancing explanations , which is hard to extend to other datasets or tasks . In this paper , we aim to train a more efficient and effective interpretable attention model without any pre - defined annotations or pre - collected explanations . Specifically , we propose a framework consisting of a learner and a compressor , which enhances the performance and interpretability of the attention model for text classification 1 . The learner learns text representations by fine - tuning the encoder . Regarding to the compressor , we are motivated by the effectiveness of the information bottleneck ( IB ) ( Tishby et al , 1999 ) to enhance performance ( Li and Eisner , 2019 ) or detect important features ( Bang et al , 2019 ; Chen and Ji , 2020 ; Jiang et al , 2020 ; Schulz et al , 2020 ) , and present a Variational information bottleneck ATtention ( VAT ) mechanism using IB to keep the most relevant clues and forget the irrelevant ones for better attention explanations . In particular , IB is integrated into attention to minimize the mutual information ( MI ) with the input while preserving as much MI as possible with the output , which provides more accurate and reliable explanations by controlling the information flow . To evaluate the effectiveness of our proposed approach , we adapt two advanced neural models ( LSTM and BERT ) within the framework and conduct experiments on eight benchmark datasets . The experimental results show that our adapted models outperform the standard attention - based models over all the datasets . Moreover , they exhibit great advantages with respect to interpretability by both qualitative and quantitative analyses . Specifically , we obtain significant improvements by applying our model to the semi - supervised word - level sentiment detection task , which detects the sentiment words based on attention weights via only sentencelevel sentiment label . In addition , we provide the case studies and text representation visualization to have an insight into how our model works . The main contributions of this work are summarized as follows . We propose a novel framework to enhance the performance and interpretability of the attention models , where a learner is used to learn good representations by fine - tuning and a compressor is used to obtain good attentive weights by compressing iteratively . We present a Variational information bottleneck ATtention ( VAT ) mechanism for the compressor , which performs compression over the text representation to keep the task related information while reduce the irrelevant noise via information bottleneck . Extensive experiments show the great advantages of our models within the proposed framework , and we perform various qualitative and quantitative analyses to shed light on why our models work in both performance and interpretability .", "entities": [[230, 232, "TaskName", "text classification"], [259, 264, "MethodName", "long short - term memory"], [269, 270, "MethodName", "LSTM"], [465, 467, "TaskName", "text classification"], [634, 635, "MethodName", "LSTM"], [636, 637, "MethodName", "BERT"]]}
{"text": "For LSTM - based models , we use GloVe embedding ( Pennington et al , 2014 ) with 300 - dimension to initialize the word embedding and fine - tune it during the training . We randomly initialize all outof - vocabulary words and weights with the uniform distribution U p\u00b40.1 , 0.1q . For the BERT - based models , we fine - tune pre - trained BERT - base model .", "entities": [[1, 2, "MethodName", "LSTM"], [8, 9, "MethodName", "GloVe"], [56, 57, "MethodName", "BERT"], [68, 69, "MethodName", "BERT"]]}
{"text": "To understand why our proposed VAT model is more effective than the standard attention - based model , we visualize two examples of LSTM - based models using attention heatmaps ( Figure 7 ) . First , the standard attention - based LSTM model focuses on the wrong words ( e.g. , \" this \" , \" work \" ) even though it predicts the right sentiment while our VAT model finds the correct words ( e.g. , \" admired \" , \" lot \" ) . It indicates integrating IB into attention can help it focus on the key words and reduce the noisy information . Second , our proposed model can also improve the attention 's performance by capturing the critical words accurately . For example , in the sentence \" That sucks if you have to take the sats tomorrow . \" , our model predicts the right class label by attending the words \" sucks \" and \" have to . \"", "entities": [[23, 24, "MethodName", "LSTM"], [42, 43, "MethodName", "LSTM"]]}
{"text": "This paper proposes a VAT - based framework to improve the performance and interpretability of attentions via both fine - tuning and compressing . The experimental results on eight benchmark datasets for text classification verify the effectiveness of our models within this framework . In addition , we apply the framework for sentiment detection , which further demonstrates the superiority in terms of interpretability . It is also interesting to find that training the models by fine - tuning and compressing iteratively is effective to improve the text representations . In the future , we will investigate the effectiveness of our proposed attention framework for other tasks and areas , such as machine translation and visual question answering .", "entities": [[32, 34, "TaskName", "text classification"], [112, 114, "TaskName", "machine translation"], [115, 118, "DatasetName", "visual question answering"]]}
{"text": "Current state - of - the - art machine translation systems are based on encoder - decoder architectures , that first encode the input sequence , and then generate an output sequence based on the input encoding . Both are interfaced with an attention mechanism that recombines a fixed encoding of the source tokens based on the decoder state . We propose an alternative approach which instead relies on a single 2D convolutional neural network across both sequences . Each layer of our network recodes source tokens on the basis of the output sequence produced so far . Attention - like properties are therefore pervasive throughout the network . Our model yields excellent results , outperforming state - of - the - art encoderdecoder systems , while being conceptually simpler and having fewer parameters .", "entities": [[8, 10, "TaskName", "machine translation"]]}
{"text": "Deep neural networks have made a profound impact on natural language processing technology in general , and machine translation in particular ( Blunsom , 2013 ; Cho et al , 2014 ; Jean et al , 2015 ; LeCun et al , 2015 ) . Machine translation ( MT ) can be seen as a sequenceto - sequence prediction problem , where the source and target sequences are of different and variable length . Current state - of - the - art approaches are based on encoder - decoder architectures ( Blunsom , 2013 ; Cho et al , 2014 ; Bahdanau et al , 2015 ) . The encoder \" reads \" the variable - length source sequence and maps it into a vector representation . The decoder takes this vector as input and \" writes \" the target sequence , updating its state each step with the most recent word that it generated . The basic encoder - decoder model is generally equipped with an attention model ( Bahdanau et al , 2015 ) , which repetitively re - accesses the source sequence during the decoding process . Given the current state of the decoder , a probability distribution over the elements in the source sequence is computed , which is then used to select or aggregate features of these elements into a single \" context \" vector that is used by the decoder . Rather than relying on the global representation of the source sequence , the attention mechanism allows the decoder to \" look back \" into the source sequence and focus on salient positions . Besides this inductive bias , the attention mechanism bypasses the problem of vanishing gradients that most recurrent architectures encounter . However , the current attention mechanisms have limited modeling abilities and are generally a simple weighted sum of the source representations ( Bahdanau et al , 2015 ; Luong et al , 2015 ) , where the weights are the result of a shallow matching between source and target elements . The attention module re - combines the same source token codes and is unable to re - encode or re - interpret the source sequence while decoding . To address these limitations , we propose an alternative neural MT architecture , based on deep 2D convolutional neural networks ( CNNs ) . The product space of the positions in source and target sequences defines the 2D grid over which the network is defined . The convolutional filters are masked to prohibit accessing information derived from future tokens in the target sequence , obtaining an autoregressive model akin to generative models for images and audio waveforms ( Oord et al , 2016a , b ) . See Figure 1 for an illustration . This approach allows us to learn deep feature hierarchies based on a stack of 2D convolutional layers , and benefit from parallel computation during training . Every layer of our network computes features of the the source tokens , based on the target sequence produced so far , and uses these to predict the next output token . Our model therefore has attention - like capabilities by construction , that are pervasive throughout the layers of the network , Convolutional layers in our model use masked 3\u00d73 filters so that features are only computed from previous output symbols . Illustration of the receptive fields after one ( dark blue ) and two layers ( light blue ) , together with the masked part of the field of view of a normal 3\u00d73 filter ( gray ) . rather than using an \" add - on \" attention model . We validate our model with experiments on the IWSLT 2014 German - to - English ( De - En ) and English - to - German ( En - De ) tasks . We improve on state - of - the - art encoder - decoder models with attention , while being conceptually simpler and having fewer parameters . In the next section we will discuss related work , before presenting our approach in detail in Section 3 . We present our experimental evaluation results in Section 4 , and conclude in Section 5 .", "entities": [[17, 19, "TaskName", "machine translation"], [45, 47, "TaskName", "Machine translation"]]}
{"text": "In this section we present our 2D CNN translation model in detail . Input source - target tensor . Given the source and target pair ( s , t ) of lengths | s | and | t | respectively , we first embed the tokens in d s and d t dimensional spaces via look - up tables . The word embeddings { x 1 , . . . , x | s | } and { y 1 , . . . , y | t | } are then concatenated to form a 3D tensor X R | t | \u00d7 | s | \u00d7f 0 , with f 0 = d t + d s , where X ij = [ y i x j ] . ( 1 ) This joint unigram encoding is the input to our convolutional network .", "entities": [[61, 63, "TaskName", "word embeddings"], [108, 109, "DatasetName", "0"], [112, 113, "DatasetName", "0"]]}
{"text": "Besides pooling we can collapse the source dimension of the feature tensor with an attention mechanism . This mechanism will generate a tensor H att that can be used instead of , or concatenated with , H Pool . We use the self - attention approach of , which for output token i computes the attention vector \u03c1 i R | s | from the activations H L i : \u03c1 i = SoftMax H L i w + b1 | s | , ( 8 ) H att i = | s | \u03c1 i H L i , ( 9 ) where w R f L and b R are parameters of the attention mechanism . Scaling of attention vectors with the square - root of the source length was also used by Gehring et al ( 2017b ) , and we found it effective here as well as in the average - pooling case .", "entities": [[73, 74, "MethodName", "SoftMax"]]}
{"text": "RNNsearch * ( Bahdanau et al , 2015 ) 31.02 1.79 6 M 25.92 7 M Varational attention 33.10 Transformer * * ( Vaswani et al , 2017 ) 32.83 3.53 59 M 27.68 61 M ConvS2S * * ( MLE ) ( Gehring et al , 2017b ) 32 ( Gehring et al , 2017b ) . phrase , and progressively from the part of the source phrase that remains to be decoded .", "entities": [[19, 20, "MethodName", "Transformer"]]}
{"text": "We presented a novel neural machine translation architecture that departs from the encoder - decoder paradigm . Our model jointly encodes the source and target sequence into a deep feature hierarchy in which the source tokens are embedded in the context of a partial target sequence . Max - pooling over this joint - encoding along the source dimension is used to map the features to a prediction for the next target token . The model is implemented as 2D CNN based on DenseNet , with masked convolutions to ensure a proper autoregressive factorization of the conditional probabilities . Since each layer of our model re - encodes the input tokens in the context of the target sequence generated so far , the model has attention - like properties in every layer of the network by construction . Adding an explicit self - attention module therefore has a very limited , but positive , effect . Nevertheless , the max - pooling operator in our model generates implicit sentence alignments that are qualitatively similar to the ones generated by attention mechanisms . We evaluate our model on the IWSLT'14 dataset , translation German to English and vice - versa . We obtain excellent BLEU scores that compare favorably with the state of the art , while using a conceptually simpler model with fewer parameters . We hope that our alternative joint source - target encoding sparks interest in other alternatives to the encoder - decoder model . In the future , we plan to explore hybrid approaches in which the input to our joint encoding model is not provided by tokenembedding vectors , but the output of 1D source and target embedding networks , e.g. ( bi - ) LSTM or 1D convolutional . We also want to explore how our model can be used to translate across multiple language pairs . Our PyTorch - based implementation is available at https://github.com/elbayadm/ attn2d .", "entities": [[5, 7, "TaskName", "machine translation"], [83, 84, "MethodName", "DenseNet"], [203, 204, "MetricName", "BLEU"], [289, 290, "MethodName", "LSTM"]]}
{"text": "In this paper , we describe our participation in the 2021 Workshop on Asian Translation ( team ID : tpt_wat ) . We submitted results for all six directions of the JPC2 patent task . As a first - time participant in the task , we attempted to identify a single configuration that provided the best overall results across all language pairs . All our submissions were created using single base transformer models , trained on only the task - specific data , using a consistent configuration of hyperparameters . In contrast to the uniformity of our methods , our results vary widely across the six language pairs .", "entities": [[14, 15, "TaskName", "Translation"]]}
{"text": "The field of machine translation has seen rapid innovation in the last few years , with new model architectures , pre - training regimens , and computational algorithms emerging at a dizzying pace . However , translation of these techniques into industry practice occurs more slowly . Companies utilizing these techniques must take into account considerations such as deployment costs ( model speed and size ) , scalability , explainability , the complexity of training regimens ( resource constraints limiting independent hyperparameter optimization for all language pairs ) , and risk management , against which advances yielding performance gains must be weighed . For our participation in the 2021 Workshop on Asian Translation shared task on patent translation , we have applied a single , standardized data preparation and model training pipeline as a way of benchmarking the performance of this process . We conducted limited experiments to test different parameters , before 1 http://lotus.kuee.kyoto - u.ac.jp/WAT/patent/ settling on the approach which provided the best overall results across all language pairs . Our NMT systems are standard base Transformer ( Vaswani et al , 2017 ) models , which were trained using only the data resources provided by the task organizers . These models used shared subword vocabularies created with SentencePiece ( Kudo and Richardson , 2018 ) . In contrast to the uniformity of our methods , our results varied widely across the six language pairs . Different scoring metrics prevent the direct comparison of scores from different language pairs , but relative to the top performing model in each language pair , our scores ranged from 98.84 % of the top score for the English Japanese language pair , to 83.89 % of the top score for Korean Japanese . Below , we describe in detail our system architecture , hyperparameter configuration , hardware resources , and results .", "entities": [[3, 5, "TaskName", "machine translation"], [81, 83, "TaskName", "hyperparameter optimization"], [112, 113, "TaskName", "Translation"], [178, 179, "MethodName", "Transformer"], [210, 211, "MethodName", "SentencePiece"]]}
{"text": "The data were encoded using subword encodings learned from the corpora using the unigram model trainer provided by SentencePiece ( Kudo and Richardson , 2018 ) . To avoid the added complexity of using different pre - tokenization strategies for", "entities": [[18, 19, "MethodName", "SentencePiece"]]}
{"text": "Wiktor Stribi\u017cew and Fred Bane and Jos\u00e9 Concei\u00e7\u00e3o and Anna Zaretskaya Transperfect Translations { wstribizew , fbane , jconceicao , azaretskaya } @translations.com different languages , we did not pre - tokenize the data prior to learning the subword model . We tested vocabulary sizes of 8000 and 32000 , as well as using shared or split vocabularies for the source and target languages . Character coverage was set to 0.9995 , the recommended value for languages with extensive character sets such as Chinese and Japanese . For the English Japanese , Korean Japanese , and Chinese Japanese language pairs , we supplemented the corpora with back translation ( from Japanese into each language ) , which is a common data augmentation technique in NMT ( Sennrich et al , 2016 ) . The back translations were produced by the NMT systems trained for the other three directions ( Japanese English , Korean , and Chinese ) .", "entities": [[120, 122, "TaskName", "data augmentation"]]}
{"text": "In this shared task , we set out to identify a single configuration of hyperparameters that provided the best overall performance across all six language pairs . While this approach precluded the possibility of obtaining optimal performance for all language pairs , it afforded the opportunity to investigate which hyperparameters have similar effects on different language pairs , and which have varied effects on different language pairs . As different language pairs require different hyperparameters , any parameter that can be held fixed during the experimentation stage can create significant savings for companies training their own machine translation models . For instance , variation in parameters such as learning rate , dropout , embedding dimensions , and tying the weights of the source and target embedding layers seemed to have similar effects on performance across all language pairs that we tested . Using back translated data to augment the training sets also appeared to be universally beneficial . However , the size of the vocabulary seemed to have quite different effects in different language pairs . We are not aware of any theoretical framework for explaining how the various hyperparameters interact to produce such different results , nor do we know of any way of predicting the optimal hyperparameters for a given language pair other than iterative experimentation . If additional resources are used , several additional steps have also been shown to be effective at boosting performance , but were not employed in these experiments in order to maintain maximum simplicity . These additional steps include using an ensemble of models for decoding , using larger model sizes , performing word segmentation prior to creating the vocabularies , ordering the training data using the output of a language model ( a technique referred to as curriculum learning ) , and employing an additional model for right - to - left re - ranking . With minimal manual intervention , our models achieved results ranging from fair to excellent . The large variance in the relative performance of these systems shows that no \" onesize - fits - all \" yet exists for the problem of machine translation . Despite monumental advances in the field over the past several years , achieving optimal performance requires careful selection of hyperparameters , and different configurations are required for different languages .", "entities": [[96, 98, "TaskName", "machine translation"], [108, 110, "HyperparameterName", "learning rate"], [356, 358, "TaskName", "machine translation"]]}
{"text": "A Transition - based System for Universal Dependency Parsing", "entities": [[7, 9, "TaskName", "Dependency Parsing"]]}
{"text": "In the pipeline of dealing known languages , the second step is to provide several light - weighted syntactical and morphological features for the tokenized texts , which will be utilized as the input features in the final parsing step . In our system , we adopt the tagger in UDPipe , whose tagging method is based on MorphoDita ( Strakov\u00e1 et al , 2014 ) and the training method is the classical Averaged Perceptron ( Collins , 2002 ) , and the training parameters of UDPipe Tagger are provided in Table 2 . In this step , the tagger will provide the following outputs : 1 . Lemma : Lemma or stem of word forms . 2 . UPOS : Universal POS tags . 3 . XPOS : Language - specific POS tags .", "entities": [[108, 109, "DatasetName", "Lemma"], [110, 111, "DatasetName", "Lemma"]]}
{"text": "For the final step , we generate the final dependency outputs with the tokens and features generated by the pre - trained POS taggers . The parser uses Parsito ( Straka et al , 2015b ) . Parsito 4 is a transition - based parser with neural network classifier , which is similar to the one of ( Chen and Manning , 2014 ) . The inputs to the model represent the current configuration of the stack and buffer , including features of the top three nodes on both of them and child nodes of the nodes on the stack . After we projected features to embeddings and concatenated the generated embeddings to representations of features , the vector representations of the input are fed to a hidden layer activated with tanh , and the output layer is softmax indicating the probabilities of each possible transition actions . The parser supports projective and nonprojective dependency parsing , which is configured by the option transition system . In Universal Dependencies release 2.0 , only UD Japanese and UD Galician have no non - projective dependency trees ; while UD Chinese , UD Polish and UD Hebrew have a few non - projective trees , around 1 % in the treebanks . According to the projective tree quantities of the whole treebanks 5 , we train non - projective parsing for most treebanks except UD Japanese and UD Galician . In projective parsing , we use dynamic oracle which usually performs better but more slowly . In non - projective parsing , we use static lazy and search - based oracle ( Straka et al , 2015a ) . Except transition system option , other configurations of Parsito are the same in all the training of different treebanks . For the structured interval option , we kept the default value 8 . To make sure that there is a only single root when parsing , single root option is set to 1 . 3 .", "entities": [[138, 139, "MethodName", "softmax"], [154, 156, "TaskName", "dependency parsing"], [167, 169, "DatasetName", "Universal Dependencies"], [173, 174, "DatasetName", "UD"], [176, 177, "DatasetName", "UD"], [187, 188, "DatasetName", "UD"], [190, 191, "DatasetName", "UD"], [193, 194, "DatasetName", "UD"], [232, 233, "DatasetName", "UD"], [235, 236, "DatasetName", "UD"]]}
{"text": "This sub - system deals with the surprise languages without enough training data . We use a simple delexicalized and cross - lingual method , that is , parsing these low resource languages based on the models learned from other languages . This follows the method of ( Zeman and Resnik , 2008 ) , which shows that transfer learning for another language based on delexicalized parser can perform well . Although different languages may have different word forms , the underlying syntactic information could overlap and the universal POS tags could be utilized to explore the correlations . To achieve this , we train a dependency parser in a close - relation language ( source language ) for a surprise language , and then feed the delexicalized POS tag sequence of the surprise language to the source language parser . We consider language family and close area to find the source language for surprise language .", "entities": [[58, 60, "TaskName", "transfer learning"]]}
{"text": "The Strength of the Weakest Supervision : Topic Classification Using Class Labels", "entities": [[7, 9, "TaskName", "Topic Classification"]]}
{"text": "When developing topic classifiers for realworld applications , we begin by defining a set of meaningful topic labels . Ideally , an intelligent classifier can understand these labels right away and start classifying documents . Indeed , a human can confidently tell if a news article is about science , politics , sports , or none of the above , after knowing just the class labels . We study the problem of training an initial topic classifier using only class labels . We investigate existing techniques for solving this problem and propose a simple but effective approach . Experiments on a variety of topic classification data sets show that learning from class labels can save significant initial labeling effort , essentially providing a \" free \" warm start to the topic classifier .", "entities": [[103, 105, "TaskName", "topic classification"]]}
{"text": "When developing topic classifiers for real - world tasks , such as news categorization , query intent detection , and user - generated content analysis , practitioners often begin by crafting a succinct definition , or a class label , to define each class . Unfortunately , these carefully written class labels are completely ignored by supervised topic classification models . Given a new task , these models typically require a significant amount of labeled documents to reach even a modest initial performance . In contrast , a human can readily understand new topic categories by reading the class definitions and making connections to prior knowledge . Labeling initial examples for every new task can be time - consuming and laborintensive , especially in resource - constrained domains like medicine and law . Therefore it is desirable if a topic classifier can proactively interpret class labels before the training starts , giving itself a \" warm start \" . An imperfect initial model can always be fine - tuned with more labeled documents . As conceptually shown in Figure 1 , a warm start can reduce the total number of training labels for a classifier to reach certain performance level . In this work , we study algorithms that can initialize a topic classifier using class labels only . Since class labels are the starting point of any topic classification task , they can be viewed as the earliest hence weakest supervision signal . We propose a simple and effective approach that combines word embedding and naive Bayes classification . On six topic classification data sets , we evaluate a suite of existing approaches and the proposed approach . Experimental results show that class labels can train a topic classifier that generalizes as well as a classifier trained on hundreds to thousands of labeled documents .", "entities": [[16, 18, "TaskName", "intent detection"], [57, 59, "TaskName", "topic classification"], [228, 230, "TaskName", "topic classification"], [262, 264, "TaskName", "topic classification"]]}
{"text": "Text retrieval . Classifying documents by short labels can be viewed as evaluating textual similarity between a document and a label . Baeza - Yates et al ( 2011 ) called this approach \" naive text classification \" . Treating labels as search queries , we can classify a document into a class if it best matches the label of that class . Well - studied text retrieval methods , such as vector space models and probabilistic models ( Croft et al , 2010 ) , can produce matching scores . To mitigate vocabulary mismatch , such a classifier can be further enhanced by self - training : the classifier assigns pseudo labels to top - ranked documents as done in pseudo relevance feedback ( Rocchio , 1965 ) , and updates itself using those labels . Semi - supervised learning . Our problem setting can be seen as an extreme case of weak supervision : we only use class labels as the ( noisy ) supervision signal , and nothing else . If we view class labels as \" labeled documents \" , one from each class , and to - be - classified documents as unlabeled documents , then we cast the problem as semisupervised learning ( Zhu , 2006 ) . Self - training is one such technique : a generative classifier is trained using only class labels , and then teaches itself using its own predictions on unlabeled data . If we view class labels as \" labeled features \" , then we expect the classifier to predict a class when a document contains the class label words . For instance , Druck et al ( 2008 ) proposed generalized expectation criteria that uses feature words ( class labels ) to train a discriminative classifier . Jagarlamudi et al ( 2012 ) and Hingmire and Chakraborti ( 2014 ) proposed Seeded LDA to incorporate labeled words / topics into statistical topic modeling . The inferred document - topic mixture probabilities can be used to classify documents . Zero - shot learning aims to classify visual objects from a new class using only word descriptions of that class ( Socher et al , 2013 ) . It first learns visual features and their correspondence with word descriptions , and then constructs a new classifier by composing learned features . Most research on zero - shot learning focuses on image classification , but the same principle applies to text classification as well ( Pushp and Srivastava , 2017 ) . Our proposed method constructs a new classifier by composing learned word embeddings in a probabilistic manner . Since the new classifier transfers semantic knowledge in word embedding to topic classification tasks , it is broadly related to transfer learning ( Pan and Yang , 2010 ) . The main difference is that in transfer learning the information about the new task is in the form of labeled data , not class definition words .", "entities": [[35, 37, "TaskName", "text classification"], [315, 316, "MethodName", "LDA"], [341, 345, "TaskName", "Zero - shot learning"], [395, 399, "TaskName", "zero - shot learning"], [401, 403, "TaskName", "image classification"], [410, 412, "TaskName", "text classification"], [432, 434, "TaskName", "word embeddings"], [450, 452, "TaskName", "topic classification"], [459, 461, "TaskName", "transfer learning"], [475, 477, "TaskName", "transfer learning"]]}
{"text": "We compare a variety of methods on six topic classification data sets . The goals are ( 1 ) to study the best classification performance achievable using class labels only , and ( 2 ) to estimate the equivalent amount of true labels needed to achieve the same warm - start performance .", "entities": [[8, 10, "TaskName", "topic classification"]]}
{"text": "We consider six topic classification data sets with different document lengths and application domains . Table 1 summarizes basic statistics of these data sets . Table 4 and Three short text data sets are ( 1 ) Wiki Titles : Wikipedia article titles sampled from 15 main categories ( Wikipedia Main Topic ) . ( 2 ) News Titles : The UCI news title data set ( Lichman , 2013 ) . ( 3 ) Y Questions : User - posted questions in Yahoo Answers ( Yahoo Language Data , 2007 ) . Three long text data sets are ( 1 ) 20 News : The well - known 20 newsgroup data set . ( 2 ) Reuters . The Reuters - 21578 data set ( Lewis ) . We take the articles from the 10 largest topics . ( 3 ) Med WSD : The MeSH word sense disambiguation ( WSD ) data set ( Jimeno - Yepes et al , 2011 ) . Each WSD task aims to tell the sense ( meaning ) of an ambiguous term in a MEDLINE abstract . For instance , the term \" cold \" may refer to Low Temperature , Common Cold , or Chronic Obstructive Lung Disease , depending on its context . These senses are used as the class labels . We use 198 ambiguous words with at least 100 labeled abstracts in the data set , and report the average statistics over 198 independent classification tasks . Although no true labels are used for training , some methods require unlabeled data for retrieval , pseudo - labeling , and re - training . We split unlabeled data into 5 folds , using 4 folds to \" train \" a classifier and 1 fold for test . We use macroaveraged F 1 as the performance metric because not all data sets have a balanced class distribution .", "entities": [[3, 5, "TaskName", "topic classification"], [147, 150, "TaskName", "word sense disambiguation"]]}
{"text": "We studied the problem of training topic classifiers using only class labels . Experiments on six data sets show that class labels can save a significant amount of labeled examples in the beginning . Retrieval - based and semi - supervised methods tend to perform better on long documents , while the proposed method performs better on short documents . This study opens up many interesting avenues for future work . First , we introduce a new perspective on text classification : can we build a text classifier by just providing a short description of each class ? This is a more challenging ( but more user - friendly ) setup than standard supervised classification . Second , future work can investigate tasks such as sentiment and emotion classification , which are more challenging than topic classification tasks . Third , the two approaches - leveraging unlabeled data ( retrievalbased and semi - supervised methods ) and leveraging pretrained models ( the proposed method ) could be combined to give robust performance on both short and long documents . Finally , we can invite users into the training loop : in addition to labeling documents , users can also revise the class definitions to improve the classifier .", "entities": [[79, 81, "TaskName", "text classification"], [127, 129, "TaskName", "emotion classification"], [135, 137, "TaskName", "topic classification"]]}
{"text": "STIL - Simultaneous Slot Filling , Translation , Intent Classification , and Language Identification : Initial Results using mBART on MultiATIS++", "entities": [[3, 5, "TaskName", "Slot Filling"], [6, 7, "TaskName", "Translation"], [8, 10, "TaskName", "Intent Classification"], [12, 14, "TaskName", "Language Identification"], [18, 19, "MethodName", "mBART"]]}
{"text": "Multilingual Natural Language Understanding ( NLU ) , also called cross - lingual NLU , is a technique by which an NLU - based system can scale to multiple languages . A single model is trained on more than one language , and it can accept input from more than one language during inference . In most recent high - performing systems , a model is first pre - trained using unlabeled data for all supported languages and then fine tuned for a specific task using a small set of labeled data ( Conneau and Lample , 2019 ; Pires et al , 2019 ) . Two typical tasks for goal - based systems , such as virtual assistants and chatbots , are intent classification and slot filling ( Gupta et al , 2006 ) . Though intent classification creates a language agnostic output ( the intent of the user ) , slot filling does not . Instead , a slot - filling model outputs the labels for each of input tokens from the user . Suppose the slot - filling model can handle L languages . Downstream components must therefore handle all L languages for the full system to be multilingual across L languages . Machine translation could be performed before the slot filling model at system runtime , though the latency would be fully additive , and some amount of information useful to the slotfilling model may be lost . Similarly , translation could occur after the slot - filling model at runtime , but slot alignment between the source and target language is a non - trivial task ( Jain et al , 2019 ; Xu et al , 2020 ) . Instead , the goal of this work was to build a single model that can simultaneously translate the input , output slotted text in a single language ( English ) , classify the intent , and classify the input language ( See Table 1 ) . The STIL task is defined such that the input language tag is not given to the model as input . Thus , language identification is necessary so that the system can communicate back to the user in the correct language . In all STIL cases , the output is in English . Each token is followed by its BIO - tagged slot label . The sequence of tokens and slots are followed by the intent and then the language . sification , and Language identification ( STIL ) ; ( 2 ) both non - translated and STIL results using the mBART model ( Liu et al , 2020 ) trained using a fully text - to - text data format ; and ( 3 ) public release of source code used in this study , with a goal toward reproducibility and future work on the STIL task 1 .", "entities": [[1, 4, "TaskName", "Natural Language Understanding"], [123, 125, "TaskName", "intent classification"], [126, 128, "TaskName", "slot filling"], [137, 139, "TaskName", "intent classification"], [152, 154, "TaskName", "slot filling"], [206, 208, "TaskName", "Machine translation"], [213, 215, "TaskName", "slot filling"], [353, 355, "TaskName", "language identification"], [414, 416, "TaskName", "Language identification"], [432, 433, "MethodName", "mBART"]]}
{"text": "The Airline Travel Information System ( ATIS ) dataset is a classic benchmark for goal - oriented NLU ( Price , 1990 ; Tur et al , 2010 ) . It contains utterances focused on airline travel , such as how much is the cheapest flight from Boston to New York tomorrow morning ? The dataset is annotated with 17 intents , though the distribution is skewed , with 70 % of intents being the flight intent . Slots are labeled using the Beginning Inside Outside ( BIO ) format . ATIS was localized to Turkish and Hindi in 2018 , forming MultiATIS ( Upadhyay et al , 2018 ) , and then to Spanish , Portuguese , German , French , Chinese , and Japanese in 2020 , forming Multi - ATIS++ ( Xu et al , 2020 ) . In this work , Portuguese was excluded due to a lack of Portuguese pretraining in the publicly available mBART model , and Japanese was excluded due to a current lack of alignment between Japanese and English samples in MultiATIS++ . Hindi and Turkish data were taken from Multi - ATIS , and the training data were upsampled by 3x for Hindi and 7x for Turkish . Prior to any upsampling , there were 4 , 488 training samples for English , Spanish , German , French , and Chinese . The test sets contained 893 samples for all languages except Turkish , which had 715 samples . For English , Spanish , German , French , and Chinese , validation sets of 490 samples were used in all cases . Given the smaller data quantities for Hindi and Turkish , two training and validation set configurations were considered . The first configuration", "entities": [[6, 7, "DatasetName", "ATIS"], [91, 92, "DatasetName", "ATIS"], [159, 160, "MethodName", "mBART"], [190, 191, "DatasetName", "ATIS"]]}
{"text": "Previous approaches for intent classification and slot filling have used either ( 1 ) separate models for slot filling , including support vector machines ( Moschitti et al , 2007 ) , conditional random fields ( Xu and Sarikaya , 2014 ) , and recurrent neural networks of various types ( Kurata et al , 2016 ) or ( 2 ) joint models that diverge into separate decoders or layers for intent classification and slot filling ( Xu and Sarikaya , 2013 ; Guo et al , 2014 ; Liu and Lane , 2016 ; Hakkani - T\u00fcr et al , 2016 ) or that share hidden states ( Wang et al , 2018 ) . In this work , a fully text - to - text approach similar to that of the T5 model was used , such that the model would have maximum information sharing across the four STIL sub - tasks . Encoder - decoder models , first introduced in 2014 ( Sutskever et al , 2014 ) , are a mainstay of neural machine translation . The original transformer model included both an encoder and a decoder ( Vaswani et al , 2017 ) . Since then , much of the work on transformers focuses on models with only an encoder pretrained with autoencoding techniques ( e.g. BERT by Devlin et al ( 2018 ) ) or auto - regressive models with only a decoder ( e.g. GPT by Radford ( 2018 ) ) . In this work , it was assumed that encoder - decoder models , such as BART ( Lewis et al , 2019 ) and T5 ( Raffel et al , 2019 ) , are the best architectural candidates given the translation component of the STIL task , as well as past state of the art advancement by encoder - decoder models on ATIS , cited above . Rigorous architectural comparisons are left to future work .", "entities": [[3, 5, "TaskName", "intent classification"], [6, 8, "TaskName", "slot filling"], [17, 19, "TaskName", "slot filling"], [71, 73, "TaskName", "intent classification"], [74, 76, "TaskName", "slot filling"], [133, 134, "MethodName", "T5"], [177, 179, "TaskName", "machine translation"], [221, 222, "MethodName", "BERT"], [241, 242, "MethodName", "GPT"], [264, 265, "MethodName", "BART"], [273, 274, "MethodName", "T5"], [311, 312, "DatasetName", "ATIS"]]}
{"text": "This preliminary work demonstrates that a single NLU model can perform simultaneous slot filling , translation , intent classification , and language identification across 7 languages using MultiATIS++ . Such an NLU model would negate the need for multiple - language support in some portion of downstream system components . Performance is not irreconcilably worse than traditional slot - filling models , and performance is statistically equivalent with a small amount of additional training data . Looking forward , a more challenging dataset is needed to further develop the translation compo - nent of the STIL task . The English MultiATIS++ test set only contains 455 unique entity - slot pairs . An ideal future dataset would include freeform and varied content , such as text messages , song titles , or open - domain questions . Until then , work remains to achieve parity with English - only ATIS models .", "entities": [[12, 14, "TaskName", "slot filling"], [17, 19, "TaskName", "intent classification"], [21, 23, "TaskName", "language identification"], [149, 150, "DatasetName", "ATIS"]]}
{"text": "We choose the parameterization of the distribution of diplomatic words in order to capture two types of spelling preference : ( 1 ) general preferences for certain character groups ( such as - ie ) and ( 2 ) preferences that only pertain to a particular word and do not indicate a larger pattern . Since it is unknown which of the two behaviors is dominant , we let the model describe both and learn to separate their effects . Using a log - linear parameterization , P ( d | m , c ; w ) \u221d exp ( w c f ( m , d ) ) we introduce features to capture both effects . Here , f ( m , d ) is a feature function defined on modern word m paired with diplomatic word d , while w c is a weight vector corresponding to compositor c. To capture word - specific preferences we add an indicator feature for each pair of modern word m and diplomatic spelling d. We refer to these as WORD features below . To capture general orthographic preferences we introduce an additional set of features based on the edit operations involved in the computation of Levenshtein distance between m and d. In particular , each operation is added as a separate feature , both with and without local context ( previous or next character of the modern word ) . We refer to this group as EDIT features . The weight vector for each compositor represents their unique biases , as shown in the depiction of these parameters in Figure 2 .", "entities": [[178, 179, "DatasetName", "WORD"]]}
{"text": "The media environment has grown increasingly polarized in recent years , creating social , cultural and political divisions ( Prior , 2013 ; Fiorina and Abrams , 2008 ) . Although a diversity of opinions is healthy , and even necessary for democratic discourse , unchecked polarization can paralyze society by suppressing consensus required for effective governance ( Tworzecki , 2019 ) . In more extreme cases , polarization leads to disagreement , conflict and even violence . The COVID - 19 pandemic has exposed many of our vulnerabilities to the pernicious effects of polarization . Public opinions about COVID - 19 ( Jiang et al , 2020 ) , as well as messaging by political elites ( Green et al , 2020 ; Bhanot 1 Code and data are publicly available at https://github.com/ZagHe568/ pacte - polarized - topics - detection . and Hopkins , 2020 ) , are sharply divided along partisan lines . According to a Pew Report ( Jurkowitz et al , 2020 ) , partisanship significantly explains attitudes about the costs and benefits of various mitigation strategies , including non - pharmaceutical interventions and lockdowns , and even explains regional differences in the pandemic 's toll in the US ( Gollwitzer et al , 2020 ) . In mass media a variety of topics is discussed every day , and polarization can form on different topics . Therefore , identifying nascent disagreements and growing controversies of different topics in news media and public discourse would help journalists craft more balanced news coverage ( Lorenz - Spreen et al , 2020 ; . Different from previous works that study polarization from a more coarse - grained perspective , Demszky et al ( 2019 ) were the first to study polarized topics using tweets about 21 mass shootings to show that some topics were more polarized than others . However , their approach to represent semantic information with word frequencies is less expressive than modern methods allow . To better capture the topical polarization among partisan ( liberal vs. conservative ) media sources , we propose Partisanship - aware Contextualized Topic Embeddings ( PaCTE ) . Specifically , given a text corpus containing news articles from both sides , we first extract a set of topics utilizing LDA topic modeling ( Blei et al , 2003 ) . Next , we finetune a pretrained language model ( Devlin et al , 2018 ) to recognize the partisanship of the news articles so as to render it partisanship - aware . Then for each article , we represent its ideology on a topic by a vector , called document - contextualized 2 ( DC ) topic embedding , by aggregating language model representations of the topic keywords contextualized by the article . Such a representation sheds light primarily on the tokens that appear in the topic keywords and thus concentrates on the topic - oriented local semantics in the context of the article , instead of the global semantics from the article that might contain irrelevant and noisy information . We further represent the ideology of the news corpus on the topic , what we call corpus - contextualized ( CC ) topic embedding , by aggregating the DC topic embeddings . As a result , the ideology of the news corpus on a topic is represented by a single vector . Finally , we measure the polarization between two news sources on the topic using the cosine distance between such vectors . For evaluation , we create ground truth by annotating the polarization of pairs of partisan news sources on a variety of topics . We evaluate the topic polarization scores produced by PaCTE against the ground truth on the task of polarized topics retrieval . Experiments on nine pairs of partisan news sources demonstrate that compared to baselines , PaCTE is more effective in capturing topic polarization and retrieving polarized topics . We argue that public media watchdogs and social media platforms can utilize such a simple - yet - effective tool to flag discussions that have grown divisive so that action could be taken to reduce partisan divisions and improve civil discourse .", "entities": [[379, 380, "MethodName", "LDA"]]}
{"text": "The partisan polarization in the US media is a widely studied topic ( Hollander , 2008 ; Stroud , 2011 ) . During the onset of the COVID - 19 pandemic , the polarization among the political elites and the news media causes a lot of confusion . For example , Hart et al ( 2020 ) show that COVID - 19 media coverage is politicized and polarized . Other works have been studying the polarization in media from different perspectives . Focusing on the differences in the languages of liberals and conservatives , KhudaBukhsh et al ( 2020 ) analyze political polarization on YouTube using machine translation tools . To analyze how the news outlets frame the events differently , Fan et al ( 2019 ) have collected and labeled 100 triplets of news articles each discussing the same event from three news sources bearing different political ideologies . In addition to qualitatively analyzing polarization , different approaches to quantifying polarization have also been proposed . propose two different ways , namely the leave - out estimator and the multinomial regression , to measure the trends of partisanship in congressional speech . Green et al ( 2020 ) define the po - larization as one 's ability to identify the partisanship of a tweet 's author based on the contents of tweets and investigate the polarization regarding COVID - 19 among political elites on Twitter . Demszky et al ( 2019 ) first measure topic - wise polarization using the leave - out estimator proposed by ; however , they use a token frequency vector to represent an article , which is less expressive and fails to make use of the rich semantics in the context and the pre - knowledge in pretrained language models ( Devlin et al , 2018 ; Liu et al , 2019 ) or pretrained word embeddings ( Mikolov et al , 2013 ; Pennington et al , 2014 ) ; furthermore , they represent the topic using the token frequency vector of the entire document , thus incurring noisy information that might smooth over the target semantics in the locality of topic keywords . In contrast , our method represents the topic embedding in the context of a document , thus generating topic representations with more attention to the target topic keywords as well as making use of the contextualized semantics from the document , as captured by the contextualized embeddings . Some works have proposed contextualized embeddings to enhance the quality of neural topic models ( Bianchi et al , 2020 ; Chaudhary et al , 2020 ) . However , the scope of this work is to generate better contextualize topic embeddings for articles to capture topic polarization , with a given topic model ; the exploration of other topic modeling techniques is beyond the scope of this work .", "entities": [[106, 108, "TaskName", "machine translation"], [293, 296, "TaskName", "pretrained language models"], [311, 313, "TaskName", "word embeddings"], [421, 423, "TaskName", "topic models"]]}
{"text": "The proposed PaCTE framework consists of four components : 1 ) LDA Topic Modeling , 2 ) Partisanship Learning , 3 ) Partisanship - aware Contextualized Topic Embedding Generation , and 4 ) Measuring Polarization and Ranking Topics . The overall framework is illustrated in Figure 1 . In this section we elaborate on each component in detail .", "entities": [[11, 12, "MethodName", "LDA"]]}
{"text": "We train an LDA topic model using the the combined corpus D C = D L \u222aD R and extract K topics T = { t i } K i=1 , where t i is a topic . The modeled topics T apply to both D L and D R . An example is given in Figure 1 ( a ) . Representing a topic by keywords . A topic t i is represented as a distribution of keywords from the global vocabulary of D C and we only keep the top - m keywords : t i = ( ( p ij , w j ) ) m j=1 , p ij > p ik \u21d4 j < k , ( 2 ) where p ij is the probability of observing keyword w j given topic t i . Representing a topic by documents . A document d D C is represented as a distribution over the K topics . Accordingly , we renormalize the probabilities and represent each topic t i as an ( inverse ) distribution of documents in D C and only keep the top - n most relevant documents , such that t D C i = ( ( q D C ij , d j ) ) n j=1 , q D C ij > q D C ik \u21d4 j < k , ( 3 ) where q D ij is the probability of observing document d j D C given topic t i . Because our goal is to study the polarization between D L and D R , instead of using the global documents in D C , we represent a topic by the top - n documents in D L and D R separately and thus obtain t D L i and t D R i accordingly .", "entities": [[3, 4, "MethodName", "LDA"]]}
{"text": "Denote the ideology embedding of A on B as H A ( B ) , where A represents a news corpus or a document and B represents a topic or a topic keyword . We then represent the ideology of a corpus D on a topic t as corpus - contextualized ( CC ) topic embedding H D ( t ) , the ideology of a document d on a topic t as document - contextualized ( DC ) topic embedding H d ( t ) , and the ideology of a document d on a topic keyword w as DC keyword embedding H d ( w ) . We will elaborate on how the CC topic embedding is obtained from a top - down perspective . According to Equation 3 , in order to compute the CC topic embedding H D ( t i ) , we can rewrite it as H D ( t i ) = n j=1 q D ij H d j ( t i ) . ( 4 ) Hence , we decompose a CC topic embedding into DC topic embeddings from the top - n most relevant documents . To obtain the DC topic embedding , Demszky et al ( 2019 ) use word frequency vectors ; Grootendorst ( 2020 ) takes the [ CLS ] embedding of a pretrained language model that gives a holistic document embedding without encoding the context of a topic . However , while word frequency vectors encode statistical features of words in the document , they neglect their context . In addition , a document is likely to be associated with multiple topics according to the LDA topic model , and therefore using the holistic document embedding as the topic embedding regardless of the specific topic results in identical embeddings for different topics on the same document ; moreover , even if a document is only associated with one topic , it might contain information not relevant to that topic and thus the holistic document embedding will encode noisy information . Therefore , we argue that the DC topic embedding should be both contextualized and topic - specific . In this regard , according to Equation 2 , we rewrite the DC topic embedding as the weighted sum of DC keyword embeddings where only top - m topic keywords are used instead of all the words in the document , as H d j ( t i ) = m k=1 p ik H d j ( w k ) . ( 5 ) Finally , in terms of the DC keyword embedding H d j ( w k ) , as can be told from its name , it is precisely what a pretrained language model ( Devlin et al , 2018 ) is designed for . Therefore , we take the corresponding final - layer token embedding of w k when the input to the language model is d j . Due to the self - attention mechanism ( Vaswani et al , 2017 ) in the pretrained language model , H d j ( t i ) encodes the global context of the document , but since it only takes the sum of topic keyword embeddings , the encoded information is more oriented towards this specific topic t i , which elegantly resonates with its name \" document - contextualized topic embedding \" . The step - by - step illustration of the generation of H d ( w ) , H d ( t ) and H D ( t ) is shown in Figure 1 ( c ) . Because the language model used to generate the embeddings is finetuned to encode partisanship , the generated H D ( t i ) also contains this information and is more precisely called partisanshipaware corpus - contextualized topic embedding . For brevity we call it corpus - contextualized ( CC ) topic embedding .", "entities": [[233, 235, "TaskName", "document embedding"], [279, 280, "MethodName", "LDA"], [288, 290, "TaskName", "document embedding"], [337, 339, "TaskName", "document embedding"]]}
{"text": "We compare PaCTE to the following three baselines . Leave - out estimator ( LOE ) . For a pair of news corpora D L and D R and a given topic t , we take the top - 10 most relevant documents from each corpus and feed the token frequency vectors of the documents into the leave - out estimator ( Demszky et al , 2019 ) , from which we use estimated partisanship as the polarization score ( [ 0 , 1 ] ) of topic t between D L and D R , following the idea of measuring within - topic polarization in their paper . Note that different from their method that extracts topic using embedding - based topic assignment , we use the same LDA topic model in PaCTE to extract topics , so as to ensure fair comparison between PaCTE and LOE . PaCTE\u00acFT . A variant of PaCTE without finetuning the language model . We compare to it to show the effect of finetuning the language model . PaCTE - PLS . A variant of PaCTE where the language model is finetuned on news articles with partisanship labels shuffled and thus is confused about the partisanship . We compare to it in order to show the effect of rendering the language model partisanship - aware .", "entities": [[81, 82, "DatasetName", "0"], [129, 130, "MethodName", "LDA"]]}
{"text": "Breit NYP CNN 1 , 9 , 10 9 , 1 , 11 9 , 10 , 2 Huff 10 , 1 , 8 1 , 11 , 9 10 , 12 , 30 NYT 10 , 33 , 1 11 , 1 , 33 11 , 9 , 10 Analysis of results . The results of polarized topics retrieval using different methods in nine news corpus pairs are shown in Table 3 . The average recall@3 over the nine news source pairs is 0.26 , 0.04 , 0.26 , and 0.52 on LOE , PaCTE\u00acFT , PaCTE - PLS , and PaCTE respectively , where PaCTE outperforms all other baselines . Comparing the results of LOE and PaCTE , we see that in most pairs PaCTE outperforms or ties with LOE . We argue that the inferior performance of LOE stems from its inability to capture document semantics due to the use of word frequency vectors . For example , in Huff vs. NYP , topic 12 is one of the target polarized topics , where documents from both stances spend the bulk of the content on the fact about the primaries and then use a few words to explicitly or implicitly endorse Biden or Sanders . Based on the use of words it is difficult to differentiate documents from the two stances , leading to the failure of LOE . In contrast , PaCTE is able to capture the contextual semantics in addition to the statistics of word usages . Therefore , even when word usages are statistically similar , PaCTE manages to discern the semantic difference and capture polarization . However , in Huff vs. Breit , compared to LOE , PaCTE fails to retrieve topic 1 regarding \" black lives matter \" , which is in the target polarized topics . On topic 1 Huff stresses \" justice \" where the news articles suggest \" police knelt on a black man \" , while Breit stresses \" riot \" where the articles suggest \" the protesters loot stores and attack police \" . As a result , the word usages of the articles from two stances are significantly different , which is trivial for LOE to capture , and thus LOE ranks topic 1 in a high place in the output list . Despite the difference in word usages , articles from both sources mention \" protests \" and \" violence \" a lot and their \" negative \" semantics is captured by PaCTE , leading to the perceived less polarization by PaCTE . The worst - performing method is PaCTE\u00acFT where the language model is not finetuned . On all topics and in all partisan news source pairs , the polarization scores given by PaCTE\u00acFT are below 0.1 ( the full range is [ 0 , 1 ] ) which indicates significant alignment . However , this is contradictory to the well - known polarization in news media . Such a phenomenon demonstrates the necessity of fitting a language model on the target corpus before apply cosine similarity between learned embeddings as a measure of word and topic similarities . In PaCTE - PLS the language model is finetuned on shuffled partisan labels that do not represent real partisanship . Compared to PaCTE\u00acFT where the model is not finetuned at all , the performance of PaCTE - PLS improves significantly , achieving the performance on a par with LOE . However , neither PaCTE\u00acFT nor LOE makes use of information about news partisanship , and compared to PaCTE 0 1/3 1/3 0 1/3 1/3 0 1/3 1/3 2/3 Huff 1/3 1/3 1/3 2/3 2/3 0 1/3 1/3 0 0 1/3 2/3 NYT 1/3 0 1/3 1 1/3 0 1/3 1/3 0 1/3 0 1/3 Table 3 : Recall@3 on polarized topics retrieval in nine partisan news source pairs using different methods , where we use the polarization - based topic ranked list from a model predictions f pred ( D L , D R , T labeled ) to retrieve the top - 3 topics from the ground - truth ranked list l gt ( D L , D R , T labeled ) . The row represents the liberal source and the column represents the conservative source in the news source pair . where partisanship information is leveraged , they are still outperformed . Insights into partisanship learning . We observe that PaCTE , which is finetuned on partisanship labels , outperforms PaCTE\u00acFT and PaCTE - PLS . We hypothesize that during the finetuning process of PaCTE , whereas the direct objective is to separate documents based on partisanship labels , the model implicitly learns the two political stances on each topic in an automatic manner ; just like in human annotating , the annotators were given two groups of documents from two partisan lines , and the annotators were able to discover the two political stances after reading the documents . Therefore , after finetuning , while the model differentiates document embeddings based on partisan divisions , it separates DC topic embeddings according to the implicitly and automatically learned political stances , bearing resemblance to human annotators ' defining two political stances for topics . As a result , we can use the partisanship - aware model to capture topic polarization arising from the partisan divisions .", "entities": [[469, 470, "DatasetName", "0"], [593, 594, "DatasetName", "0"], [596, 597, "DatasetName", "0"], [599, 600, "DatasetName", "0"], [609, 610, "DatasetName", "0"], [612, 613, "DatasetName", "0"], [613, 614, "DatasetName", "0"], [618, 619, "DatasetName", "0"], [622, 623, "DatasetName", "0"], [625, 626, "DatasetName", "0"], [627, 628, "DatasetName", "0"]]}
{"text": "In Section 3.4 we propose to use the DC topic embedding to represent the ideology of a document on a topic , instead of using the holistic document embedding . In this section we study the difference between them . We denote the variant of PaCTE that uses document embeddings ( [ CLS ] token em - beddings ) as PaCTE - DE . First , we show the results of polarized topics retrieval using PaCTE - DE and PaCTE in three partisan news source pairs in Table 4 . Method CNN vs. Fox Huff vs. Breit NYT vs. NYP PaCTE - DE 0 0 0 We observe that PaCTE - DE fails to retrieve any polarized topics in all three pairs of news sources , significantly outperformed by PaCTE . We provide more explanations on the advantages of DC topic embeddings over document embeddings from another perspective , in addition to the capability of DC topic embedding to focus more on the topicspecific semantics in a document . We observe that the polarization scores given by PaCTE - DE in three source pairs on all topics are above 0.98 ( the range is [ 0 , 1 ] ) , suggesting that all topics are highly polarized . Therefore , as the polarization scores cluster within the interval of [ 0.98 , 1 ] , the gaps between different scores are barely discernible , in which case the output ranked list is more susceptible to random noise during the language model finetuning and is thus more unstable and erratic . However , the output polarization scores from PaCTE are more evenly distributed in [ 0 , 1 ] , and thus are more robust to perturbations during partisanship learning ; a small perturbation on a polarization score does not affect the output ranking . As a result , PaCTE enjoys a better chance to outperform PaCTE - DE . PaCTE 1/3 1/3 1/3 As a matter of fact , the large polarization scores from PaCTE - DE on all topics are expected , because the language model is finetuned to directly separate the document embeddings according to partisan line divisions , resulting in low cosine similarities between document embeddings on every topic , as shown in Figure 2 ( Left ) . However , despite the prominent separation of document embeddings , the corresponding DC topic embeddings that are used in PaCTE display more alignment , as shown in Figure 2 ( Right ) , where we see on some topics the DC topic embeddings are separated while on other topics the embeddings are more close . Thus , we argue that during the finetuning process , on a given topic , DC topic embeddings retain their similarity if the two partisan news articles agree on this topic , because in these articles the topic - related se - mantics does not contribute to the forming of the partisanship and thus maintains its position during partisanship learning , while the non - topical semantics ( not captured by DC topic embeddings but captured by document embeddings ) that contribute to the document partisanship keeps moving apart in the embedding space .", "entities": [[27, 29, "TaskName", "document embedding"], [103, 104, "DatasetName", "0"], [104, 105, "DatasetName", "0"], [105, 106, "DatasetName", "0"], [195, 196, "DatasetName", "0"], [275, 276, "DatasetName", "0"]]}
{"text": "We use MALLET 5 topic modeling . The top - 10 keywords of all 39 topics are shown in Table 6 . Among them topic 0 , 3 , 4 , 14 , 16 , 26 , 35 , 36 , 37 are not used in further analysis because after reading relevant articles we find that they are more about advertisements , sport events , gossip news and recipes and etc . , which are more factual and convey limited media ideologies . 30 topics are left after removing the 9 topics . Table 6 lists the top - 10 keywords of the 30 topics .", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "This project was funded in part by DARPA under contract HR001121C0168 . The authors are also grateful to Ves Stoyanov for a productive discussion .", "entities": [[7, 8, "DatasetName", "DARPA"]]}
{"text": "We recruit 3 annotators that work as academic researchers in the areas of NLP and social science . For each one of the 30 topics , the annotators are provided with the top - 10 topic keywords and the summaries of top - 10 most relevant documents from each news corpus ( as a total of 60 documents ) . First , the annotators select 15 topics on which they feel it is straightforward to find two polarized political stances by reading the relevant documents . For example , on topic 12 about Democratic primaries , it is intuitive to perceive the two political stances are \" endorsing Biden \" and \" endorsing Sanders \" after reading relevant articles , and then this topic is likely to be selected . We take the overlap of the 15 selected topics from 3 annotators and obtain 10 topics : T labeled = { t 1 , t 2 , t 8 , t 9 , t 10 , t 11 , t 12 , t 27 , t 30 , t 33 } with defined polarized political stances . In other words , the annotators reach an agreement that it is more clear on these 10 topics that there are two political stances . We find that on each of these 10 topics , the two stances defined by 3 annotators reach a complete agreement . We do not annotate all topics because 1 ) it is difficult for humans to discern the two political stances on some topics , especially when such two stances do not exist at all ; 2 ) we use the vanilla LDA topic modeling which is not the state - of - the - art , so the modeled topics will change using different topic models , in which case the annotating step should be repeated . Nevertheless , we argue that annotating 10 topics is sufficient to quantitatively evaluate the effectiveness of PaCTE . Given a topic t from T labeled , the defined two stances , and its 60 most relevant documents ( 10 from each of the six news sources ) , for each document , we ask the annotators to label which stance it belongs to and label it as 0 or 1 ; if the annotator is not able to perceive a clear political stance , then the annotator will label it as - 1 . For each document , the majority vote of the three labels with be used as the final annotation . If no majority vote is achieved , in other words , the three annotators give three different labels to a document , then a fourth annotator will read the document again and decide the final label . For a complete list of all document labels on the 10 selected topics , please refer to our public repository .", "entities": [[275, 276, "MethodName", "LDA"], [298, 300, "TaskName", "topic models"], [378, 379, "DatasetName", "0"]]}
{"text": "Beyond Laurel / Yanny : An Autoencoder - Enabled Search for Polyperceivable Audio", "entities": [[6, 7, "MethodName", "Autoencoder"]]}
{"text": "How robust is human sensory perception , and to what extent do perceptions differ between individuals ? In May 2018 , an audio clip of a man speaking the word \" laurel \" received widespread attention because a significant proportion of listeners confidently reported hearing not the word \" laurel , \" but rather the quite different sound \" yanny \" ( Salam and Victor , 2018 ) . At first glance , this suggests that the decision boundaries for speech perception vary considerably among individuals . The reality is more surprising : almost everyone has a decision boundary between the sounds \" laurel \" and \" yanny , \" without a significant \" dead zone \" separating these classes . The audio clip in question lies close to this decision boundary , so that if the clip is slightly perturbed ( e.g. by damping certain frequencies or slowing down the playback rate ) , individuals switch from confidently perceiving \" laurel \" to confidently perceiving \" yanny , \" with the exact point of switching varying slightly from person to person . How common is this phenomenon ? Specifically , what fraction of spoken language is \" polyperceivable \" in the sense of evoking a multimodal response in a population of listeners ? In this work , we provide initial results suggesting a significant density of spoken words that , like the original \" laurel / yanny \" clip , lie close to unexpected decision boundaries between seemingly unrelated pairs of words or sounds , such that individual listeners can switch between perceptual modes via a slight perturbation . The clips we consider consist of audio signals synthesized by the Amazon Polly speech synthesis system with a slightly perturbed playback rate ( i.e. a slight slowing - down of the clip ) . Though the resulting audio signals are not \" natural \" stimuli , in the sense that they are very different from the result of asking a human to speak slower ( see Section 5 ) , we find that they are easy to compute and reliably yield compelling polyperceivable instances . We encourage future work to investigate the power of more sophisticated perturbations , as well as to consider natural , ecologically - plausible perturbations . To find our polyperceivable instances , we ( 1 ) devise a metric that correlates with polyperceivability , ( 2 ) use this metric to efficiently sample candidate audio clips , and ( 3 ) evaluate these candidates on human subjects via Amazon Mechanical Turk . We present several compelling new examples of the \" laurel / yanny \" effect , and we encourage readers to listen to the examples included in the supplementary materials ( also available online at https://theory.stanford.edu/ valiant / polyperceivable / index.html ) . Finally , we estimate that polyperceivable clips can be made for > 2 % of English words .", "entities": [[283, 285, "TaskName", "speech synthesis"]]}
{"text": "Are the words we found polyperceivable ? To identify cases where words had multiple perceptual \" modes , \" we looked for clusters in the distribution of responses for each of the 29 candidate words . Concretely , we treated responses as \" bags of phonemes \" and then applied K - means . Though this rough heuristic discards information about the order of phonemes within a word , it works sufficiently well for clustering , especially since most of our words have very few syllables ( more sophisticated models of phonetic similarity exist , but they would not change our results ) . We found that the largest cluster typically contained the original word and rhymes , whereas other clusters represented significantly different perceptual modes . Some examples of clusters and their relative frequency are available in the prevalance of alternate modes among our clips increases . How prevalent are polyperceivable words ? Of our initial sample of 200 words , 11 ultimately yielded compelling demonstrations . To compute the prevalence of polyperceivable words in the population of the top 10k words , we have to account for the importance sampling weights we used when sampling in Section 2.1 . After scaling each word 's contribution by the inverse of the probability of including that word in our nonuniform sample of 200 , we conclude that polyperceivable clips exist for at least 2 % of the population : that is , of the 16 voices under consideration , at least one yields a polyperceivable clip for > 2 % of the top 10k English words . We emphasize that this is a conservative lower bound , because it assumes that there were no other polyperceivable words in the 200 words we sampled , besides the 11 that we selected for the second round . We did not conduct an exhaustive search among those 200 words , instead focusing our Mechanical Turk resources on only the most promising candidates . Is S a good metric ? We consider the metric S to be successful because it allowed us to efficiently find several new polyperceivable instances . If the 200 words were sampled uniformly instead of being importance - sampled based on S , we would only have found 4 polyperceivable words in expectation ( 2 % of 200 ) . Thus , importance sampling increased our procedure 's recall by almost 3\u00d7. For a more quantitative understanding , we analyzed the relationship between \" autoencoder path length \" S and \" perceptual path length \" T . Our measure T of \" perceptual path length \" for a clip is change in average distance between source word and response as we slow the clip down from 0.75\u00d7 to 0.6\u00d7. As with clustering above , distance is measured in bag - of - phonemes space . For each word , we computed the correlation between S and T among the 16 voices ( both S and T vary significantly across voices ) . For all but 5 of our 29 words these metrics correlated positively , though with varying strength ( Figure 3 ) . This suggests that S indeed correlates with polyperceivability . 4 Discussion : Why study quirks of human perception in an ACL paper ? Perceptual instability in human sensory systems offers insight into ML systems . The question of what fraction of natural inputs lie close to decision boundaries for trained ML systems has received enormous attention . The surprising punchline that has emerged over the past decade is that most natural examples ( including points in the training set ) actually lie extremely close to unexpected decision boundaries . For most of these points , a tiny but carefully - crafted perturbation can lead the ML system to change the label . Such perturbations are analogous to the slight perturbation in playback speed for the polyperceivable clips we consider . In the ML literature , these perturbations , referred to as \" adversarial examples \" seem pervasive across complex ML systems ( Szegedy et al , 2013 ; Goodfellow et al , 2014 ; Nguyen et al , 2015 ; Moosavi - Dezfooli et al , 2016 ; Madry et al , 2017 ; Raghunathan et al , 2018 ; Athalye et al , 2017 ) . While the initial work on adversarial examples focused on computer vision , more recent work shows the presence of such examples across other settings , including reinforcement learning ( Huang et al , 2017 ) , reading comprehension ( Jia and Liang , 2017 ) , and speech recognition ( Carlini and Wag - ner , 2018 ; Qin et al , 2019 ) . Studying perceptual illusions would provide a much - needed reference when evaluating ML systems in these domains . For vision tasks , for example , human vision provides the only evidence that current ML models are far from optimal in terms of robustness to adversarial examples . However , while humans are certainly not as susceptible to adversarial examples as ML systems , we lack quantified bounds on human robustness . More broadly , understanding which systems ( both biological and ML ) have decision boundaries that lie surprisingly close to many natural inputs may inform our sense of what settings are amenable to adversarially robust models , and what settings inherently lead to vulnerable classifiers . Perceptual instability in ML systems offers insight into human sensory systems . Recent research on adversarial robustness of ML models has provided a trove of new tools and perspectives for probing classifiers and exploring the geometry of decision boundaries . These tools can not directly be applied to study the decision boundaries of biological classifiers ( e.g. we can not reasonably do \" gradient descent \" on human subjects ) . However , using standard data - driven deep learning techniques to model human perceptual systems can allow us to apply these techniques by proxy . An example can be found in the study of \" transferability . \" Adversarial examples crafted to fool a specific model often also fool other models , even those trained on disjoint training sets ( Papernot et al , 2016a ; Tram\u00e8r et al , 2017 ; Liu et al , 2016 ) . This prompts the question of whether adversarial examples crafted for an ML model might also transfer to humans . Recent surprising work by Elsayed et al ( 2018 ) explores this question for vision . Humans were shown adversarial examples trained for an image classifier for \u2248 70ms , and asked to choose between the correct label and the classifier 's ( incorrect ) predicted label . Humans selected the incorrect label more frequently when shown adversarial examples than when shown unperturbed images . Similarly , Hong et al ( 2014 ) trained a lowdimensional representation of \" perceptual space , \" and used the decision boundaries of the model to find images that confused human subjects .", "entities": [[414, 415, "MethodName", "autoencoder"], [757, 759, "TaskName", "reading comprehension"], [768, 770, "TaskName", "speech recognition"], [918, 920, "TaskName", "adversarial robustness"]]}
{"text": "Priming effects It is possible to use additional stimuli to alter perceptions of the \" laurel / yanny \" audio clip . For example , Bosker ( 2018 ) demonstrates the ability to control a listener 's perception by \" priming \" them with a carefully crafted recording before the polyperceivable clip is played . Similarly , Guan and Valiant ( 2019 ) investigated the \" McGurk effect \" ( McGurk and MacDonald , 1976 ) , where what one \" sees \" affects what one \" hears . \" The work estimated the fraction of spoken words that , when accompanied by a carefully designed video of a human speaker , would be perceived as significantly different words by listeners . Such phenomena raise questions about how our autoencoder - based method can be extended to search for \" priming - sensitive \" polyperceivability . Security implications Just as adversarial examples for DNNs have security implications ( Papernot et al , 2016b ; Carlini and Wagner , 2017 ; Liu et al , 2016 ) , so too might adversarial examples for sensory systems . For example , if a video clip of a politician happens to be polyperceivable , an adversary could lightly edit it with potentially significant ramifications . A thorough treatment of such security implications is left to future work .", "entities": [[129, 130, "MethodName", "autoencoder"]]}
{"text": "In this paper , we leveraged ML techniques to study polyperceivability in humans . By modeling perceptual space as the latent space of an autoencoder , we were able to discover dozens of new polyper - ceivable instances , which were validated with Mechanical Turk experiments . Our results indicate that polyperceivability is surprisingly prevalent in spoken language . More broadly , we suggest that the study of perceptual illusions can offer insight into machine learning systems , and vice - versa .", "entities": [[24, 25, "MethodName", "autoencoder"]]}
{"text": "Synonym , antonym and their relations from unstructured text are fundamental problems in information classification field . These problems can be decomposed into three subtasks : word extraction using regrex , relation extraction ( Zelenko et al , 2003 ) , ( Bunescu and Mooney , 2005 ) , and classifying the logistics between them . However , an end - to - end model , i.e. ERNIE - M model ( Ouyang et al , 2020 ) , is proposed to solve the three tasks . Presupposed Taxonomies - Evaluating Neuralnetwork Semantics ( PreTENS ) ( Zamparelli et al , 2022 ) is a task to predict the acceptability of simple sentences containing constructions whose two arguments are presupposed to be or not to be in an ordered taxonomic relation . In this paper , we first present a simple approach with the ERNIE - M model to solve the task . Although the ERNIE - M model performs unexpectedly impressive , the model has poor robustness . Hence , the additional pre - trained model is introduced to solve the robustness problem . The latest model DeBERTaV3 ( He et al , 2021 ) has outstanding performance on cross - linguistic tasks , which outperforms BERT and DeBERTa on many tasks . The proposed model consists of two parts : the basic ERNIE - M model and the pre - trained model DeBERTaV3 . The De - BERTaV3 model shares the same pre - trained data with ERNIE - M called XNLI ( Conneau et al , 2018 ) , which can improve the performance and robustness as well . The DeBERTaV3 model is trained independently , which has significant improvement for English but somehow brought no improvement for other languages . Based on the above conclusion , we employ the DeBERTaV3 model for English - task only . To better understand the effectiveness of the proposed model , we started a bunch of analyses . The first problem is the data - set limitation . Two additional datasets were imported , i.e. , the translated dataset from Google translation which is translated from three languages , and the XNLI dataset . However , larger datasets do n't lead to better performance . We compared the performance of the ERNIE - M model on four sets of data : the given data , the given data with translated data , the given data with XNLI augmentation , and the given data with both the translated data and XNLI data . We do the same experiments with the DeBERTaV3 model as well . The results show that the combination of ERNIE - M with all the three datasets and DeBERTaV3 with the given English data perform the best . Cross - Attention masked language modeling ( CAMLM ) is to align cross - language semantic representations on parallel corpora . Then , the multilingual representation is enhanced with transferability learned from parallel corpora .", "entities": [[31, 33, "TaskName", "relation extraction"], [207, 208, "MethodName", "BERT"], [209, 210, "MethodName", "DeBERTa"], [253, 254, "DatasetName", "XNLI"], [350, 351, "DatasetName", "Google"], [361, 362, "DatasetName", "XNLI"], [406, 407, "DatasetName", "XNLI"], [419, 420, "DatasetName", "XNLI"], [463, 466, "TaskName", "masked language modeling"]]}
{"text": "Back - Translation masked language modeling ( BTMLM ) is trained to generate pseudo - parallel sentences from monolingual sentences . The generated pairs are then used as the input of the model to further align the cross - lingual semantics , thus enhancing the multilingual representation . DeBERTaV3 presents a new pre - trained language model , which improves the original De - BERTa model by replacing mask language modeling ( MLM ) with replaced token detection ( RTD ) , a more sample - efficient pre - training task . They all come from an important field , multilingual models . Since the related paper was published at the end of 2021 , there are no similar tasks have been done and published .", "entities": [[2, 3, "TaskName", "Translation"], [3, 6, "TaskName", "masked language modeling"], [72, 73, "DatasetName", "MLM"]]}
{"text": "In this section , we first introduce the methods to solving the multi - language problem and then present our work about improving the performance on uni - language . To extenuate over - fitting for a specific language , our team uses a multi - language ensemble learning strategy that includes a pre - trained language model and a multilingual language model . Based on the approach above , it makes the learned representation generalizable across languages and improves the performance in finding the suitable taxonomic relations in two nominal arguments .", "entities": [[47, 49, "TaskName", "ensemble learning"]]}
{"text": "Our key idea of solving multilingual language tasks is to learn the language invariant feature space shared among multiple languages . We tried multilingual masked language modeling ( MMLM ) , translation language modeling ( TLM ) , and crossattention masked language modeling ( CAMLM ) have been tried . However , the scale of the parallel corpus is quite limited , which limits the performance of the model . However , we found that using the transferability learned from parallel corpora to enhance the model 's learning of large - scale monolingual corpora to enhance multilingual semantic representation can achieve a good effect . ERNIE - M does this by making the predictions of tokens depending on tokens in another language , but not on other tokens in this language . Therefore , we choose ERNIE - M as the baseline model for this task and explore on this basis to improve the prediction effect . In the process of using multilingual language models , we mainly adopt random search to finetune the ERNIE - M model and data augmentation methods are used for model training . Cross - lingual natural language inference ( XNLI ) dataset is used and the English training set is translated to Italian ( E2I set ) . Firstly , the English training set is combined with the French and E2I set . Then , the model is fine - tuned with the combined training set . Finally , the augmented task training set in three languages is adopted for fine - tune process .", "entities": [[24, 27, "TaskName", "masked language modeling"], [40, 43, "TaskName", "masked language modeling"], [169, 171, "MethodName", "random search"], [179, 181, "TaskName", "data augmentation"], [188, 194, "TaskName", "Cross - lingual natural language inference"], [195, 196, "DatasetName", "XNLI"]]}
{"text": "To enhance the effect in a single language subtask , we consider using an enhanced mask decoder and a disentangled attention mechanism to improve the effect . DeBERTaV3 meets our needs by using Electra - style pre - training and gradient unwrapping embedding sharing . We have tried to use DeBER - TaV3 for training in each single language subtask respectively .", "entities": [[19, 22, "MethodName", "disentangled attention mechanism"], [33, 34, "MethodName", "Electra"]]}
{"text": "By using the multilingual language model and pretrained language model respectively , we have two groups of validation set results for each language . We adopt the mean of the best - saved models from ERNIE - M and DeBERTaV3 after making predictions on the validation set . After comparing the Figure 1 : The process of 10 - fold cross - validation and ensemble . The training set which includes all three languages is divided randomly 10 times by setting different random seeds . In each division , the training set is divided into 10 parts , of which 9 parts are respectively used as the training set and the remaining 1 part is used as the validation set . And finally , the average of all saved best models predicted on the test set is the final results . combination result , we finally used different strategies in different languages . For the English subtask , we retain the strategy of merging the two types of models . For French and Italian subtasks , the result from cross - validation of the multilingual language model is used directly .", "entities": [[83, 84, "DatasetName", "seeds"]]}
{"text": "As the total number of labeled data in each language is only 5840 , it 's liable to overfit the training data even with pre - trained models . The overfitting phenomenon may be more significant than expected because the data is generated programmatically through manually verified templates . To increase the size of training data , we use the following data augmentation methods : 1 ) translate English data into French and Italian by using Baidu translate 2 ) translate English data into French and Italian by using Google translate 3 ) translate French and Italian data into English by using Google translate . We find that the augmentation can help delay the overfitting occurrence slightly , especially for large models .", "entities": [[61, 63, "TaskName", "data augmentation"], [89, 90, "DatasetName", "Google"], [102, 103, "DatasetName", "Google"]]}
{"text": "Our dataset comes from two parts . The first part is the trial dataset released by organizers , which is composed of English , French and Italian . Each language contains 5838 sentences . Because the trail dataset provided by organizers is only 5838 in each language , to increase the amount of data and make the model better , we use Google translator and Baidu translator to translate the English dataset into French and Italian again . The use of two different translators also increases the diversity of data . The other part is that we use the public dataset - XNLI . We use XNLI dataset because it is often used in similar cross - language tasks . The XNLI dataset contains a total of 15 languages , and each language contains 7500 pairs of data . We used the English and French datasets in this competition . Because the XNLI dataset itself does not contain Italian datasets , we translated the English dataset into Italian and then used the three languages in ERNIE - M model training .", "entities": [[62, 63, "DatasetName", "Google"], [102, 103, "DatasetName", "XNLI"], [106, 107, "DatasetName", "XNLI"], [121, 122, "DatasetName", "XNLI"], [152, 153, "DatasetName", "XNLI"]]}
{"text": "In this task , we mainly use the ERNIE - M model and DeBERTaV3 model . The ERNIE - M model is composed of 24 layers , 1024 hidden , and 16 heads . In terms of parameter selection , we set a set of parameters , as Table 2 shows . We set up 10000 times of ERNIE - M model training , in which the specific values of the above parameters are randomly selected according to the ta - For the comparative analysis of the results of using only ERNIE - M as the baseline model and the ensemble model , we can see that the improvement of the ensemble model in English is relatively obvious , but the improvement in Italian and French is very weak . We think this is due to the following reasons : Firstly , Italian is not included in the original XNLI dataset . In this task , we translate English into Italian . So to a certain extent , the understanding of English by the ERNIE - M model is increased . Secondly , because DeBERTaV3 performs well in English , we only use its results in English , So the results for Italian and French did not get a big boost . This also shows that using the ensemble model can indeed improve the prediction . In the future , we will explore ensemble models that can improve predictions in Italian and French .", "entities": [[149, 150, "DatasetName", "XNLI"]]}
{"text": "To solve the problem of judging whether the meaning of a sentence is self - consistent in multilingual language tasks , that is , the problem raised in task 3 , we propose an ensemble model using ERNIE - M and DeBERTaV3 , and regard this problem as a binary classification problem . Furthermore , to solve the issue of the small dataset , we use various strategies , such as K - ford cross - validation , translating the dataset using different translators , and introducing an external dataset - XNLI , a dataset commonly used in multilingual problems . In future efforts , we plan to further improve our model from these aspects . The first is to enrich the data , especially Italian and French , to help the model learn better . The second is that we could train more models on standard fine - tuning , multi - step fine - tuning ,", "entities": [[91, 92, "DatasetName", "XNLI"]]}
{"text": "multi - task learning , or adversarial training . Then try to ensemble different models to gain a better performance .", "entities": [[0, 4, "TaskName", "multi - task learning"]]}
{"text": "Multilingual Code - Switching for Zero - Shot Cross - Lingual Intent Prediction and Slot Filling", "entities": [[14, 16, "TaskName", "Slot Filling"]]}
{"text": "A cross - lingual setting is typically described as a scenario in which a model trained for a particular task in one source language ( e.g. English ) should be able to generalize well to a different target language ( e.g. Japanese ) . While semi - supervised solutions ( Muis et al , 2018 ; FitzGerald , 2020 , inter alia ) assume some target language data or translators are available , a zero - shot solution ( Eriguchi et al , 2018 ; Srivastava et al , 2018 ; assumes none is available at training time . Having models that generalize well even to unseen languages is crucial for tackling real world problems such as extracting relevant information during a new disaster ( Nguyen et al , 2017 ; Krishnan et al , 2020 ) or detecting hate speech ( Pamungkas and Patti , 2019 ; Stappen et al , 2020 ) , where the target language might be of low - resource or unknown . Intent prediction and slot filling are two NLU tasks , usually solved jointly , which learn to model the intent ( sentence - level ) and slot ( word - level ) labels . Such models are currently used extensively for goal - oriented dialogue systems , such as Amazon 's Alexa , Apple 's Siri , Google Assistant , and Microsoft 's Cortana . Finding the ' intent ' behind the user 's query and identifying relevant ' slots ' in the sentence to engage in a dialogue are essential for effective conversational assistance . For example , users might want to ' play music ' given the slot labels ' year ' and ' artist ' ( Coucke et al , 2018 ) , or they may want to ' book a flight ' given the ' airport ' and ' locations ' slot labels ( Price , 1990 ) . A strong correlation between the two tasks has made jointly trained models successful ( Goo et al , 2018 ; Haihong et al , 2019 ; Hardalov et al , 2020 ; . In a cross - lingual setting , the model should be able to learn this joint task in one language and transfer knowledge to another ( Upadhyay et al , 2018 ; Schuster et al , 2019 ; . This is the premise of our work . Highly effective transformer - based multilingual models such as mBERT ( Devlin et al , 2019 ) and XLM - R ( Conneau et al , 2020a ) have found success across several multilingual tasks in recent years . In the zero - shot cross - lingual transfer setting with an unknown target language , a typical solution is to use pre - trained transformer models and fine - tune to the downstream task using the monolingual source data . However , Pires et al ( 2019 ) showed that existing transformer - based represen - Figure 1 : t - SNE plot of embeddings across the 12 multi - head attention layers of multilingual BERT . Parallelly translated sentences of MutiATIS++ dataset are still clustered according to the languages : English ( black ) , Chinese ( cyan ) , French ( blue ) , German ( green ) , and Japanese ( red ) . Figure 2 : An original example in English from MultiATIS++ dataset and its multilingually code - switched version . In the above code - switching example , the chunks are in Chinese , Punjabi , Spanish , English , Arabic , and Russian . ' atis_airfare ' represents an intent class where the user seeks price of a ticket . tations may exhibit systematic deficiencies for certain language pairs . Figure 1 also verifies that the representations across the 12 multi - head attention layers of mBERT are still not shared across languages , instead forming clearly distinguishable clusters per language . This leads to a fundamental challenge that we address in this work : enhancing the language neutrality so that the fine - tuned model is generalizable across languages for the downstream task . To this goal , we introduce a data augmentation method via multilingual codeswitching , where the original sentence in English is code - switched into randomly selected languages . For example , chunk - level code - switching creates sentences with phrases in multiple languages as shown in Figure 2 . We show that mBERT can be fine - tuned for many languages starting only with monolingual source - language data , leading to better performance in zero - shot settings . Further , we show how code - switching with languages from different language families impacts the model 's performance on individual target languages , even finding some counter - intuitive results . For instance , training on data code - switched between English and Sino - Tibetan languages is as helpful for Hindi ( an Indo - Aryan Indo - European language ) as code - switching with other Indo - Aryan languages , and Turkic languages can be helpful for both Chinese and Japanese .", "entities": [[139, 141, "DatasetName", "hate speech"], [171, 173, "TaskName", "slot filling"], [209, 214, "TaskName", "goal - oriented dialogue systems"], [225, 226, "DatasetName", "Google"], [410, 411, "MethodName", "mBERT"], [419, 420, "MethodName", "XLM"], [442, 449, "TaskName", "zero - shot cross - lingual transfer"], [509, 513, "MethodName", "multi - head attention"], [516, 517, "MethodName", "BERT"], [591, 592, "DatasetName", "Punjabi"], [638, 642, "MethodName", "multi - head attention"], [644, 645, "MethodName", "mBERT"], [700, 702, "TaskName", "data augmentation"], [747, 748, "MethodName", "mBERT"]]}
{"text": "We present a data augmentation method via multilingual code - switching to enhance the language neutrality of transformerbased language models such as mBERT for finetuning to a downstream NLU task of intent prediction and slot filling . b ) By studying different language families , we show how code - switching can be used to aid zero - shot cross - lingual learning for low - resource languages . c ) We release a new human - annotated tweet dataset , collected during Haiti earthquake disaster , for intent prediction and slot filling in English and Haitian Creole .", "entities": [[3, 5, "TaskName", "data augmentation"], [22, 23, "MethodName", "mBERT"], [34, 36, "TaskName", "slot filling"], [91, 93, "TaskName", "slot filling"]]}
{"text": "Multilingual masked language models , such as mBERT ( Devlin et al , 2019 ) , are trained using large datasets of publicly available unlabeled corpora such as Wikipedia . Such corpora largely remain monolingual at the sentence level because the presence of intra - sentence code - switched data in written texts is likely scarce . The masked words that needed to be predicted usually are in the same language as their surrounding words . We study how code - switching can enhance the language neutrality of such language models by augmenting it with artificially code - switched data for fine - tuning it to a downstream task . Algorithm 1 explains this codeswitching process at the chunk - level . When using slot filling datasets , slot labels that are grouped by BIO ( Ramshaw and Marcus , 1999 ) tags constitute natural chunks , as shown in Figure 2 . To summarize the algorithm , we take a sentence , take each chunk from that sentence , perform a translation into a random language using Google 's NMT system ( Wu et al , 2016 ) , and align the slot labels to fit the translation , i.e. , label propagation through alignment as the translated sentence do not preserve the", "entities": [[7, 8, "MethodName", "mBERT"], [124, 126, "TaskName", "slot filling"], [178, 179, "DatasetName", "Google"]]}
{"text": "Afro - Asiatic Arabic ( ar ) , Amharic ( am ) , Hebrew ( he ) , Somali ( so ) Germanic German ( de ) , Dutch ( nl ) , Danish ( da ) , Swedish ( sv ) , Norwegian ( no ) Indo - Aryan Hindi ( hi ) , Bengali ( bn ) , Marathi ( mr ) , Nepali ( ne ) , Gujarati ( gu ) , Punjabi ( pa ) Romance Spanish ( es ) , Portuguese ( pt ) , French ( fr ) , Italian ( it ) , Romanian ( ro ) Sino - Tibetan , Koreanic , & Japonic Chinese ( zh - cn ) , Japanese ( ja ) , Korean ( ko ) Turkic Turkish ( tr ) , Azerbaijani ( az ) , Uyghur ( ug ) , Kazakh ( kk ) number and order of words in the original sentence . At the chunk - level , we use a direct alignment . The BIO - tagged labels are recreated for the translated phrase based on the word tokens . More complex methods could be applied here to improve the alignment of the slot labels such as fast - align ( Dyer et al , 2013 ) or soft - align , but we leave this for future work . Code - Switching at the word - level essentially translates every word randomly , while at the sentence - level translates the entire sentence . During the experimental evaluation process , to build a language - neutral model using monolingual source ( English ) data , all eight target languages are excluded from the code - switching procedure to avoid unfair model comparisons , i.e. removing target languages ( l T ) from lset in Algorithm 1 . Complexity . The augmentation process is repeated k times per sentence producing a new augmented dataset of size k \u00d7 n , where n is the size of the original dataset , i.e. space complexity of O ( k \u00d7 n ) . For T translations per sentence , Algorithm 1 has a runtime complexity of O ( k \u00d7 n \u00d7 T ) assuming constant time for alignment . Word - level requires as many translations as the number of words but sentence - level requires only one . An increase in the dataset size also increases the training time , but the advantage is one model appropriate for many languages .", "entities": [[75, 76, "DatasetName", "Punjabi"]]}
{"text": "Since we assume that target language is not known before hand , Translate - Train ( TT ) method is not a suitable baseline . Rather , we set this to be an upper bound , i.e. translating to the target language and fine - tuning the model should intuitively outperform a generic model . Additionally , we add code - switching to this TT model to assess if augmentation negatively impacts its performance . The zero - shot baselines for the codeswitching experiments use an English - Only ) model , which is fine - tuned over the pre - trained mBERT separately for each task and an English - only Joint model .", "entities": [[102, 103, "MethodName", "mBERT"]]}
{"text": "Cross - Lingual Transfer . Researchers have studied cross - lingual tasks in various settings such as sentiment / sequence classification ( Wan , 2009 ; Eriguchi et al , 2018 ; , named entity recognition ( Zirikly and Hagiwara , 2015 ; Tsai et Xie et al , 2018 ) , parts - of - speech tagging ( Yarowsky et al , 2001 ; T\u00e4ckstr\u00f6m et al , 2013 ; Plank and Agi\u0107 , 2018 ) , and natural language understanding ( He et al , 2013 ; Upadhyay et al , 2018 ; . The methodology for most of the current approaches for cross - lingual tasks can be categorizes as : a ) multilingual representations from pre - trained or fine - tuned models such as mBERT ( Devlin et al , 2019 ) or XLM - R ( Conneau et al , 2020a ) , b ) machine translation followed by alignment ( Shah et al , 2010 ; Yarowsky et al , 2001 ; Ni et al , 2017 ) , or c ) a combination of both . Before transformer models , effective approaches included domain adversarial training to extract language - agnostic features ( Ganin et al , 2016 ; and word alignment methods such as MUSE ( Conneau et al , 2017 ) to align fastText word vectors ( Bojanowski et al , 2017 ) . Recently , Conneau et al , 2020b show that having shared parameters in the top layers of the multilingual encoders can be used to align different languages quite effectively on tasks such as XNLI ( Conneau et al , 2018 ) . Monolingual models for joint slot filling and intent prediction have used attention - based RNN ( Liu and Lane , 2016 ) and attention - based BiLSTM with a slot gate ( Goo et al , 2018 ) on benchmark datasets ( Price , 1990 ; Coucke et al , 2018 ) . These methods have shown that a joint method can enhance both tasks and slot filling can be conditioned on the learned intent . A related approach iteratively learns the relationship between the two tasks ( Haihong et al , 2019 ) . Recently , BERT - based approaches ( Hardalov et al , 2020 ; have improved results . On the other hand , cross - lingual versions of this joint task include a low - supervision based approach for Hindi and Turkish ( Upadhyay et al , 2018 ) , new datasets for Spanish and Thai ( Schuster et al , 2019 ) , and recently creating MultiATIS++ , a comprehensive dataset in 9 languages . The joint task mentioned above in a pure zero - shot setting is one of the motivations for our work . A Zero - shot is the setting where the model sees a new distribution of examples only during test ( prediction ) time ( Xian et al , 2017 ; Srivastava et al , 2018 ; Romera - Paredes and Torr , 2015 ) . Thus , in our setting , we assume that target language is unknown during training , so that our model is generalizable across multiple languages . Code - Switching . Linguistic code - switching is a phenomenon where multilingual speakers alternate between languages . Recently , monolingual models have been adapted to code - switched text in entity recognition ( Aguilar and Solorio , 2019 ) , part - ofspeech tagging ( Soto and Hirschberg , 2018 ; Ball and Garrette , 2018 ) , sentiment analysis ( Joshi et al , 2016 ) and language identification ( Mave et al , 2018 ; Yirmibe\u015foglu and Eryigit , 2018 ; Mager et al , 2019 ) . Recently , KhudaBukhsh et al , 2020 have proposed an approach to sample code - mixed documents using minimal supervision . Qin et al , 2020 allows randomized code - switching to include the target language , as shown in their Figure 3 . In our context for example , if the target language is German , we ensure that there is no code - switching to German during training . We consider this distinction essential to evaluate a true zero - shot learning scenario and prevent any bias when comparing with translate - and - train . present a non - zero - shot approach that performs code - switching to target languages , and Jiang et al ( 2020 ) present a code - switching based method to improve the ability of multilingual language mod - els for factual knowledge retrieval . Contemporary work by Tan and Joty , 2021 makes use of both word and phrase - level code - mixing to switch to a set of languages to perform adversarial training for XNLI . Code - switching and other data augmentation techniques have been applied to the pre - training stage in recent works ( Chaudhary et al , 2020 ; Kale and Siddhant , 2021 ; Dufter and Sch\u00fctze , 2020 ) . However , pre - training is outside the scope of this work . In addition to studying cross - lingual slot filling and language families , another key distinction of our method is that we completely ignore the target language during training to represent a fully zero - shot scenario . The main advantage is that with enhanced cross - lingual generalizability , it can be deployed out - of - the - box , as our training is conducted independently of the target language .", "entities": [[0, 4, "TaskName", "Cross - Lingual Transfer"], [33, 36, "TaskName", "named entity recognition"], [79, 82, "TaskName", "natural language understanding"], [129, 130, "MethodName", "mBERT"], [138, 139, "MethodName", "XLM"], [151, 153, "TaskName", "machine translation"], [208, 210, "TaskName", "word alignment"], [213, 214, "DatasetName", "MUSE"], [223, 224, "MethodName", "fastText"], [267, 268, "DatasetName", "XNLI"], [280, 282, "TaskName", "slot filling"], [302, 303, "MethodName", "BiLSTM"], [342, 344, "TaskName", "slot filling"], [373, 374, "MethodName", "BERT"], [597, 599, "TaskName", "sentiment analysis"], [607, 609, "TaskName", "language identification"], [709, 713, "TaskName", "zero - shot learning"], [805, 806, "DatasetName", "XNLI"], [812, 814, "TaskName", "data augmentation"], [867, 869, "TaskName", "slot filling"]]}
{"text": "Our study shows that augmenting the monolingual input data with multilingual code - switching via random translations at the chunk - level helps a zeroshot model to be language neutral when evaluated on unseen languages . This approach enhanced the generalizability of pre - trained language models such as mBERT when fine - tuning for downstream tasks of intent detection and slot filling . Additionally , we presented an application of this method using a new annotated dataset of disaster tweets . Further , we studied code - switching with language families and their impact on specific target languages . Addressing code - switching with language families during the pre - training phase and releasing a larger dataset of annotated disaster tweets in more languages are planned for future work .", "entities": [[49, 50, "MethodName", "mBERT"], [58, 60, "TaskName", "intent detection"], [61, 63, "TaskName", "slot filling"]]}
{"text": "Automatic Error Analysis for Document - level Information Extraction", "entities": [[1, 2, "MetricName", "Error"]]}
{"text": "Figure 1 : The document - level extraction task from the ProMED dataset on disease outbreaks ( left ) and the automatic error analysis process ( right ) . Our system performs a set of transformations on the predicted templates to convert them into the corresponding gold standard templates . Transformation steps are mapped to corresponding error types to produce informative error statistics . newly developed neural IE methods with that of the largely hand - crafted systems of the 1990s . In this work , we first introduce a framework for automating error analysis for document - level event and relation extraction , casting both as instances of a general role - filling , or template - filling task ( Jurafsky and Martin , 2021 ) . Our approach converts predicted system outputs into their gold standard counterparts through a series of template - level transformations ( Figure 2 ) and then maps combinations of transformations into a collection of IE - based error types . Examples of errors include duplicates , missing and spurious role fillers , missing and spurious templates , and incorrect role and template assignments for fillers . ( See Figure 3 for the full set ) . Next , we employ the error analysis framework in a comparison of two state - of - the - art documentlevel neural template - filling approaches , DyGIE++ and GTT ( Du et al , 2021b ) , across three template - filling datasets ( SciREX , ProMED ( Patwardhan and Riloff , 2009 ) 3 , and MUC - 4 ) . Finally , in an attempt to gauge progress in the information extraction field over the past 30 years , we employ the framework to compare the performance of four of the original MUC - 4 systems with the two newer deep - learning approaches to documentlevel IE . 4 We find that ( 1 ) the best of the early IE models - which strikes a better balance between precision and recall - outperforms modern models that exhibit much higher precision and much lower recall ; ( 2 ) the modern neural models make more mistakes on scientific vs. news - oriented texts , and missing role fillers is universally the largest source of errors ; and ( 3 ) modern models have clear advantages over the early IE systems in terms of accurate span extraction , while the early systems make fewer mistakes assigning role fillers to their roles .", "entities": [[101, 103, "TaskName", "relation extraction"], [248, 249, "DatasetName", "SciREX"]]}
{"text": "Aside from the original MUC - 4 evaluation scoring reports ( Chinchor , 1991 ) , which included counts of missing and spurious role filler errors , there have been very few attempts at understanding the types of errors made by IE systems and grounding those errors linguistically . Valls - Vargas et al ( 2017 ) proposed a framework for studying how different errors propagate through an IE system ; however , the framework can only be used for pipelined systems , not end - to - end ones . On the other hand , automated error analysis with linguistically motivated error types has been used in other sub - fields of NLP such as machinetranslation ( Vilar et al , 2006 ; Zhou et al , 2008 ; Farr\u00fas et al , 2010 ; Kholy and Habash , 2011 ; Zeman et al , 2011 ; Popovi\u0107 and Ney , 2011 ) , coreference resolution ( Uryupina , 2008 ; Kummerfeld and Klein , 2013 ; Martschat and Strube , 2014 ; Martschat et al , 2015 ) and parsing ( Kummerfeld et al , 2012 ) . Recently , generalized automated error analysis frameworks involving human - in - the - loop testing like Errudite ( Wu et al , 2019 ) , CHECK - LIST ( Ribeiro et al , 2020 ) , CrossCheck ( Arendt et al , 2021 ) , and AllenNLP Interpret ( Wallace et al , 2019 ) have successfully been applied to tasks like machine comprehension and relation extraction ( Alt et al , 2020 ) . Closest to our work are Kummerfeld et al ( 2012 ) and Kummerfeld and Klein ( 2013 ) , which use model - agnostic transformationbased mapping approaches to automatically obtain error information in the predicted structured output .", "entities": [[155, 157, "TaskName", "coreference resolution"], [229, 230, "DatasetName", "Arendt"], [256, 258, "TaskName", "relation extraction"]]}
{"text": "Our experiments employ three document - level information extraction datasets . We briefly describe each below . Dataset statistics are summarized in Table 1 . MUC - 4 ( MUC - 4 , 1992 ) consists of newswire describing terrorist incidents in Latin America provided by the FBIS ( Federal Broadcast Information Services ) . We converted the optional templates to required templates and removed the subtypes of the incidents as done in previous work ( Chambers , 2013 ; Du et al , 2021b ) so that the dataset is transformed into standardized templates . The roles chosen from the MUC - 4 dataset are PERPIND ( individual perpetrator ) , PERPORG ( organization perpetrator ) , TARGET ( physical target ) , VICTIM ( human target ) , and WEAPON which are all string - fill roles , as well as INCIDENT We focus specifically on its 4 - ary relation extraction subtask . The roles present in each relation are MATERIAL ( DATASET ) , METRIC , TASK , and METHOD which are all string - fills . We convert the dataset from its original format to templates for our models , and remove individual role fillers ( entities ) that have no mentions in the text . 11 We also remove any duplicate templates . 12 During preprocessing , we remove malformed words longer than 25 characters , as the majority of these consist of concatenated words that are not present in the corresponding text .", "entities": [[149, 154, "TaskName", "4 - ary relation extraction"]]}
{"text": "In our experiments , we train and test two neuralbased IE models , described briefly below , on the MUC - 4 , ProMED , and SciREX datasets . Note that ( 2019 ) for the SciREX dataset and 11 tokens for the ProMED dataset . We use bert - base - cased and allenai / scibert_scivocab_uncased for the base BERT and SciBERT models respectively , which both have a maximum input sequence length of 512 tokens . To aggregate entities detected by DyGIE++ into templates , we use a clustering algorithm . For the SciREX dataset , we adopt a heuristic approach that assumes there is only one template per document , and in that template , we assign the named entities predicted by DyGIE++ for a document to the predicted role types . For the ProMED dataset , we use a different clustering heuristic that ensures that each template has exactly one role filler for the COUNTRY and DISEASE roles , as detailed in the dataset annotation guidelines . Also , since STATUS has the value confirmed in the majority of the templates , every template predicted has its STATUS assigned as confirmed . GTT is an end - to - end document - level templategenerating model . For the MUC - 4 and SciREX datasets , GTT is run for 20 epochs , while for ProMED it is run for 36 epochs , to adjust for the smaller size of the dataset . All other hyperparameters are set as in Du et al ( 2021b ) . We use the same BERT and SciBERT base models as described in the DyGIE++ architecture above , both with a maximum input sequence length of 512 tokens . The computational budget and optimal hyperparameters for these models can be found in Ap - pendix sections D and E , respectively .", "entities": [[26, 27, "DatasetName", "SciREX"], [36, 37, "DatasetName", "SciREX"], [60, 61, "MethodName", "BERT"], [95, 96, "DatasetName", "SciREX"], [216, 217, "DatasetName", "SciREX"], [264, 265, "MethodName", "BERT"]]}
{"text": "We first discuss the results of DyGIE++ and GTT on SciREX , ProMED , and MUC - 4 ; and then examine the performance of these newer neural models on the 1992 MUC - 4 dataset vs. a few of the bestperforming IE systems at the time .", "entities": [[10, 11, "DatasetName", "SciREX"]]}
{"text": "The GTT ( BERT ) model on the MUC - 4 dataset took 1 hour and 21 minutes to train and around 11 minutes to test on Google Colab ( GPU ) . The GTT ( BERT ) model on the ProMED dataset took around 24 minutes to train and 4 minutes to test , while the GTT ( SciBERT ) model on the ProMED dataset took around 13 minutes to train and 4 minutes to test , both on Google Colab ( GPU ) . The DyGIE++ ( BERT ) model on the ProMED dataset took around 50 minutes to train , while the DyGIE++ ( SciBERT ) model on the ProMED dataset took around 1 hour and 30 minutes to train , both on a NVIDIA V100 GPU . For the SciREX dataset , it took around 10 - 20 minutes to run the GTT ( BERT ) and GTT ( SciBERT ) models on a NVIDIA V100 GPU . It is worth noting that since the GTT model embeds all inputs before training and SciREX documents are extremely long , more than 25 GB of memory needs to be allocated at the embedding phrase . The training process has normal memory usage . The DyGIE++ ( BERT ) model took around 2 hours to train , while the DyGIE++ ( SciBERT ) model took around 4 hours to train , both on a NVIDIA V100 GPU . Our error analysis tool can be run completely on a CPU and takes a couple of minutes to run , depending on the size of the dataset and the predicted outputs .", "entities": [[3, 4, "MethodName", "BERT"], [27, 28, "DatasetName", "Google"], [36, 37, "MethodName", "BERT"], [80, 81, "DatasetName", "Google"], [89, 90, "MethodName", "BERT"], [133, 134, "DatasetName", "SciREX"], [148, 149, "MethodName", "BERT"], [177, 178, "DatasetName", "SciREX"], [209, 210, "MethodName", "BERT"]]}
{"text": "We thank the anonymous reviewers and Ellen Riloff for their helpful comments ( ! ) and Sienna Hu for converting the 1992 model outputs to a format compatible with our error analysis tool . Our research was supported , in part , by NSF CISE Grant 1815455 and the Cornell CS Department CSURP grants for undergraduate research .", "entities": [[49, 50, "DatasetName", "Cornell"], [50, 51, "DatasetName", "CS"]]}
{"text": "Intrinsic Evaluation of Summarization Datasets", "entities": [[3, 4, "TaskName", "Summarization"]]}
{"text": "High quality data forms the bedrock for building meaningful statistical models in NLP . Consequently , data quality must be evaluated either during dataset construction or post hoc . Almost all popular summarization datasets are drawn from natural sources and do not come with inherent quality assurance guarantees . In spite of this , data quality has gone largely unquestioned for many recent summarization datasets . We perform the first large - scale evaluation of summarization datasets by introducing 5 intrinsic metrics and applying them to 10 popular datasets . We find that data usage in recent summarization research is sometimes inconsistent with the underlying properties of the datasets employed . Further , we discover that our metrics can serve the additional purpose of being inexpensive heuristics for detecting generically low quality examples .", "entities": [[32, 33, "TaskName", "summarization"], [63, 64, "TaskName", "summarization"], [75, 76, "TaskName", "summarization"], [97, 98, "TaskName", "summarization"]]}
{"text": "Data understanding is fundamentally important in natural language processing ( NLP ) ; for data - driven learning - based methods ( e.g. neural networks ) , the quality of the training data bounds the quality of models learned using it . Therefore , understanding this data is necessary in order to ensure that models learn to perform a given task correctly . Understanding data is a multidimensional problem . One line of inquiry has demonstrated why prominent datasets are insufficiently challenging : many data examples can be solved by alternative heuristics that do not encode an approach that is faithful to the task ( McCoy et al , 2019 ) . From the perspective of datasets , several works have shown that standard datasets in areas such as visual question answering Kafle and Kanan , 2017 ) , natural language inference ( Gururangan et al , 2018 ; Poliak et al , 2018 ) , and reading comprehension ( Kaushik and Lipton , 2018 ) contain annotation artifacts that often give rise to these spurious correlations or reasoning shortcuts . Data understanding can also inform scientific and ethical decision - making ( Bender and Friedman , 2018 ; Gebru et al , 2018 ; Mitchell et al , 2019 ) with recent work studying how social biases encoded in training data propagate to learned models ( Zhao et al , 2019 ; Tan and Celis , 2019 ) . In this work , we extend these efforts towards the setting of summarization . We find this to be particularly timely since several summarization datasets have been released in recent years with little discussion of data quality . While prior work on evaluating NLP datasets has focused on their difficulty , transparency , or bias , we consider broadly the overall quality of the dataset - in our case , for the task of summarization . 1 Our central insight is that desirable properties of a summary can be readily estimated by adapting and applying existing NLP methods . With this in mind , we present a multiaspect large - scale study of summarization datasets that dissects summarization into 5 properties that are evaluated across 10 datasets spanning multiple summarization domains . Our analysis reveals that our metrics can serve as lightweight detectors of generically low quality examples . Most strikingly , we show that quantifiable aspects of summarization datasets are inconsistent with their use by the NLP community in several instances .", "entities": [[129, 132, "DatasetName", "visual question answering"], [139, 142, "TaskName", "natural language inference"], [157, 159, "TaskName", "reading comprehension"], [252, 253, "TaskName", "summarization"], [263, 264, "TaskName", "summarization"], [314, 315, "TaskName", "summarization"], [353, 354, "TaskName", "summarization"], [357, 358, "TaskName", "summarization"], [369, 370, "TaskName", "summarization"], [398, 399, "TaskName", "summarization"]]}
{"text": "Compression scores quantitatively disambiguate summarization tasks . Concretely , we observe GW has the lowest compression scores and while GW is sometimes described as a summarization dataset ( Rush et al , 2015 ; Chopra et al , 2016 ) , it is better seen as a headline generation dataset that is more in the style of sentence compression ( as is suggested by S i = D i = 1 ) . Conversely , AMI and Movi - eScript achieve the highest scores by a substantial margin and are long - document summarization datasets . Classifying new summarization datasets accurately may prove useful given that successful methods from one domain often do not extend to another and this shortcoming in generalization can be attributed to the differences in compression requirements ( Cohan et al , 2018 ) . Given the goals stated in the XSum dataset paper , TL ; DR may be a better choice than XSum . In particular , Narayan et al ( 2018 ) introduce XSum as a large dataset that legitimately requires abstraction . While XSum is more abstractive than other News datasets ( barring GW ) and is relatively large , TL ; DR displays greater abstractivity , similar length summaries , and is 15 times larger . That said , Narayan et al ( 2018 ) explore topic - oriented strategies in their work and such methods may be better suited to XSum given the TS scores . CNN - DM and NYT are suboptimal for studying abstractive / extractive systems respectively . Several recent works ( See et al , 2017 ; Paulus et al , 2018 ; Li et al , 2018 ) have used CNN - DM to build and evaluate abstractive systems . Conversely , NYT has been used to build extractive systems ( Hong and Nenkova , 2014 ; Li et al , 2016 ) . Given our findings , we find both of these trends to be inconsistent with dataset properties and suboptimal given other preferable datasets for these purposes : CNN - DM is one of the least abstractive datasets and there are larger and more extractive alternatives to NYT such as NWS . Especially in the case of CNN - DM , we note that training learning - based systems ( e.g. neural methods ) using data with limited abstractivity implies the resulting summarizers will be limited in their ability to generate genuinely abstractive text . This is validated by empirical findings as both See et al ( 2017 ) and Zhang et al ( 2018 ) observe limited abstractivity in abstractive systems trained on CNN - DM . In light of this , we argue systems should be characterized as abstractive or not based on their empirical behavior rather than their theoretical capability . 9 CNN - DM is not a representative benchmark for summarization as a whole . Recent work ( Kryscinski et al , 2019 ; Raffel et al , 2019 ) has explicitly portrayed CNN - DM as the benchmark dataset for summarization ; the field has implicitly done this for several years ( Kryscinski et al , 2019 ) . While there is clear value in evaluating pretrained representations on summarization datasets , we caution against using CNN - DM as a stand - in for the entire summarization subfield . Instead , we suggest using a diverse group of datasets and not reducing a highly heterogeneous subfield to a single dataset . While this adds additional overhead , this cost is necessary to draw meaningful conclusions about the impact of advances on summarization broadly given the pronounced diversity in summarization datasets ( Table 1 ) . Post - processing methods for mitigating redundancy may be needed for practical systems . While evaluation on standard datasets using ROUGE may not penalize for this , redundancy is clearly undesirable ( Carbonell and Goldstein , 1998 ; Peyrard , 2019a ) and existing datasets ( and thereby systems learned using that data ) display significant amounts of redundancy in their gold - standard summaries ( exceptions are datasets with short summaries where cross - sentence redundancy is constrained to be low ) . Specifically , Nenkova ( 2006 ) argues that redundancy is a clear inhibitor for practical application of summarization systems . Consequently , post hoc methods that reduce redundancy after initial evaluation may be useful in generating summaries that are suitable for human users . Semantic coherence captures observable variation in summary coherence . We observe that the Scientific summaries ( which are abstracts of published papers ) are clearly more coherent than the author - generated summaries in TL ; DR , the fragmented summaries in AMI , and the concatenated bullet - point summaries in CNN - DM . We find that this distinction is captured by the SC measure using BERT . Quantifying semantic coherence is especially important given that the coherence of reference summaries will inform the coherence of system summaries , especially for learning - based approaches . Akin to what we discuss for abstractivity , See et al ( 2017 ) and Paulus et al ( 2018 ) both demonstrate that neural summarizers generate incoherent summaries despite achieving high ROUGE scores .", "entities": [[4, 5, "TaskName", "summarization"], [25, 26, "TaskName", "summarization"], [57, 59, "DatasetName", "sentence compression"], [77, 78, "DatasetName", "Movi"], [92, 94, "TaskName", "document summarization"], [98, 99, "TaskName", "summarization"], [145, 146, "DatasetName", "XSum"], [158, 159, "DatasetName", "XSum"], [170, 171, "DatasetName", "XSum"], [181, 182, "DatasetName", "XSum"], [240, 241, "DatasetName", "XSum"], [243, 244, "MethodName", "TS"], [246, 249, "DatasetName", "CNN - DM"], [285, 288, "DatasetName", "CNN - DM"], [345, 348, "DatasetName", "CNN - DM"], [374, 377, "DatasetName", "CNN - DM"], [441, 444, "DatasetName", "CNN - DM"], [472, 475, "DatasetName", "CNN - DM"], [481, 482, "TaskName", "summarization"], [504, 507, "DatasetName", "CNN - DM"], [508, 510, "DatasetName", "the benchmark"], [512, 513, "TaskName", "summarization"], [541, 542, "TaskName", "summarization"], [548, 551, "DatasetName", "CNN - DM"], [559, 560, "TaskName", "summarization"], [604, 605, "TaskName", "summarization"], [611, 612, "TaskName", "summarization"], [719, 720, "TaskName", "summarization"], [798, 801, "DatasetName", "CNN - DM"], [814, 815, "MethodName", "BERT"]]}
{"text": "While the properties we evaluate for do not exhaust all aspects of summarization that may be of interest , it is unclear to what extent different measures overlap in judgments . To quantify this , in we report pairwise correlations for every pair of metrics . In each case , the value reported is the Spearman rank correlation coefficient \u03c1 computed between the length 10 vectors containing the scores for each dataset . 10 \u03c1 = 1 indicates perfect positive correlation ( which is why we see this for all diagonal entries ) and \u03c1 < 0 indicates the metrics are anti - correlated . Unsurprisingly , the compression metrics are strongly correlated with each other . We further observe that redundancy and topic similarity are correlated whereas abstractivity is anti - correlated with both . In particular , when summaries are considerably redundant , we qualitatively observe that the repeated content in the summary was both important and repeated in the context of the reference document . As a result , this may explain why redundancy and abstractivity are anti - correlated as this would suggest that highly redundant summaries are highly extractive . Additionally , since we measure topic similarity using LDA and unigram count statistics , it is not surprising that extractions may correlate with high topic similarity . In part , this may suggest a deficiency of our measure of topic similarity to accurately consider references to the same topic using substantially different words . We also observe that semantic coherence patterns similarly to redundancy . In particular , while we find the semantic coherence scores are appropriate for most examples we manually inspected , this suggests that BERT relies upon word - level overlaps in making next - sentence judgments ( similar to behaviors seen in other sentence - pair tasks such as natural language inference , c.f Gururangan et al , 2018 )", "entities": [[12, 13, "TaskName", "summarization"], [96, 97, "DatasetName", "0"], [203, 204, "MethodName", "LDA"], [282, 283, "MethodName", "BERT"], [308, 311, "TaskName", "natural language inference"]]}
{"text": "To complement our quantitative dataset - level analysis , we conduct a qualitative study of individual examples by examining outliers . For each ( dataset , metric ) pair , we sample 10 examples from both the top and bottom 10 % of examples for that metric and in that dataset . Since manually considering all of the 1080 examples was not feasible , we began by examining the sampled examples for topic similarity , redundancy , and semantic coherence . Our hypothesis was that example quality would positively correlate with coherence and topic similarity and negatively correlate with redundancy . We found this hypothesis to be validated by our observations as we found that examples with low coherence , low topic similarity , or high redundancy scores were generally low quality examples . Every example which we judged to be low quality demonstrated at least one of the following defects : The summary contains critical disfluencies that severely hinder accurate processing . 11 The summary excludes unambiguously critical information from the reference document . Crucial information in the summary does not appear in the reference document and is not general knowledge . Substantial fractions of the summary involve entities , relations , or events that are ambiguous and that we could not resolve from the 11 We invoked this condition fairly judiciously as we observed that the domain of summaries also could influence the fluency of summaries in terms of grammaticality . In particular , we unsurprisingly found that academic papers in the Science domain generally have highly grammatical summaries whereas the bullet - point summaries in CNN - DM and the author - written summaries in TL ; DR often were ungrammatical but still sufficiently clear to be interpreted correctly . summary alone . In particular , accurate interpretation of the summary would require also reading the reference document to resolve various coreferring expressions ; the summary is not self - contained . 12 The summary is entirely inappropriate as a summary of the reference document . For example , the summary only discusses an event with no obvious relationship to the contents of the reference document . The summary includes an entire sentence or long phrase describing something that appears in the main document but that is clearly an auxiliary detail . We flagged examples as low quality due to this condition quite conservatively , only using it when we could come to no basis for why the sentence / phrase should appear in the summary . On the other hand , we did not find any systematic defects in examples with high coherence , high topic similarity , or low redundancy scores . Instead , almost all of these examples were satisfactory . For the remaining two properties ( compression measured by CMP w , abstractivity measured by ABS 1 ) , we analyzed all of the associated 400 examples . What we observed is that many of these examples tended to be generically low quality and we quantify this in Table 3 . Since this analysis may be difficult to replicate and involves subjective decisions about example quality , we comprehensively enumerate all example IDs we use in Table 8 . Table 4 shows a representative subset of the low quality examples we found in our analysis . We provide further examples in Appendix C and Figures 1 - 9 . Compression . Minimally compressed summaries in NYT , NWS , TL ; DR , and PubMed often are supplementary information to the document rather than a summary of it ; in some cases , we believe this is due to errors in alignment in dataset construction / release . On the other hand , heavily compressed summaries in NWS and XSum often are just category labels ( e.g. Sports ) , in TL ; DR are usually attention - grabbers , and in NYT are nearexact duplicates of reference documents , which themselves are letters to the editor . Abstractivity . Manual inspection reveals highly abstractive summaries in NYT and NWS generally are exceedingly vague or are entirely unrelated to the original document . Highly abstractive summaries in PeerRead are often translated to English from the reference document 's language and discuss results that do not appear in the introduction but likely appear later in the paper . Conversely , extremely extractive summaries in NWS and NYT often are just the lede and can not be understood without the reference document . However , in most other instances , the lede is an effective summary for examples drawn from the News domain . Within the context of our sample of examples , we find that eight of the ten summarization datasets ( all but AMI , MovieScript ) contain at least 8 % low quality examples , the majority contain at least 14 % low quality examples , and that these low quality examples can be detected using our compression and abstractivity metrics . For the worst - offending TL ; DR dataset , we conservatively estimate at least 20 % of examples are of substantially subpar quality . In general , we find that the low quality TL ; DR \" summaries \" we detect often serve a different rhetorical purpose than summarization ( e.g. attention grabbing , responding to a previous post that is not available in the dataset , sarcasm / humor ) .", "entities": [[190, 192, "TaskName", "general knowledge"], [268, 271, "DatasetName", "CNN - DM"], [626, 627, "DatasetName", "XSum"], [694, 695, "DatasetName", "PeerRead"], [785, 786, "TaskName", "summarization"], [879, 880, "TaskName", "summarization"]]}
{"text": "Dataset Analysis . As an alternative to automated evaluation , Chen et al ( 2016 ) and Yatskar ( 2019 ) conduct human evaluations of standard datasets in reading comprehension and question answering . In some cases , dataset creators perform manual analyses of the data they introduce ( e.g. Sandhaus ( 2008 ) and Grusky et al ( 2018 ) for the NYT and Newsroom corpora , respectively ) . Automated and human evaluation provide complementary benefits with respect to their scalability and reliability . Even in the context of human evaluations , we advocate that automatic metrics can be useful in guiding the exploration of data and informing subsampling procedures that provide fine - grained insights . Quality Estimation . Our work bears resemblance both in name and structure to work on quality estimation . Quality estimation , often centered on natural language generation , is the task of measuring system - generated output quality ( Paetzold and Specia , 2016 ; Yuan and Sharoff , 2020 ) . It is closely related to work on unsupervised or reference - free evaluation ( Napoles et al , 2016 ; Ethayarajh and Sadigh , 2020 ) . Within the context of summarization , the special case of quality estimation regarding factual consistency / faithfulness has been of recent interest ( Wang et al , 2020 ; Maynez et al , 2020 ; Durmus et al , 2020 ) since neural abstractive summarizers have been shown to hallucinate / misrepresent facts ( See et al , 2017 ) . In comparison to these settings , our metrics make no use of labelled data ( even in training ) and are entirely intrinsic / unsupervised . Summarization Practices . Several analyses and critiques exist for different aspects of the summarization pipeline . From a modelling perspective , Zhang et al ( 2018 ) assess whether abstractive systems are truly abstractive , Kedzie et al ( 2018 ) evaluate content selection policies in a variety of methods , and Mao et al ( 2020 ) assess the facetlevel performance of extractive summarizers . From an evaluation perspective , several works have discussed the shortcomings of ROUGE / automated evaluation ( Liu and Liu , 2008 ; Chaganty et al , 2018 ; Hashimoto et al , 2019 ; Peyrard , 2019b ) as well proposed alternative metrics for summarization or natural language generation more broadly ( Clark et al , 2019 ; Zhang et al , 2020 ; Sellam et al , 2020 ) . Two recent works are highly related to our own . Kryscinski et al ( 2019 ) provide a critical reevaluation of summarization research . Most relevant to our work , they show that web - scraped datasets , specifically CNN - DM and NWS , contain a nontrivial fraction of examples ( approx . 3.5 % ) with HTML artifacts ( which can be easily detected / removed ) . Jung et al ( 2019 ) provide an aspect - level evaluation of both summarization datasets and systems . In their work , the dataset analyses center on biases in the data ( e.g. positional biases , which are often seen in news summarization ) , which is reminiscent of the annotation artifacts seen in other NLP tasks ( Gururangan et al , 2018 ; Niven and Kao , 2019 ) .", "entities": [[28, 30, "TaskName", "reading comprehension"], [31, 33, "TaskName", "question answering"], [202, 203, "TaskName", "summarization"], [285, 286, "TaskName", "Summarization"], [298, 299, "TaskName", "summarization"], [396, 397, "TaskName", "summarization"], [444, 445, "TaskName", "summarization"], [462, 465, "DatasetName", "CNN - DM"], [507, 508, "TaskName", "summarization"], [535, 537, "TaskName", "news summarization"]]}
{"text": "Open Problems and Future Directions . Our results demonstrate that a sizeable fraction of examples in most summarization datasets are low quality . However , it remains open whether modellers should simply prune these examples , manually / automatically attempt to correct them , or model them without change . We do note that research in the machine learning and learning theory communities shows that models both theoretically and empirically do substantially worse when trained using low quality examples , even when the examples are not strictly adversarially chosen ( Klivans et al , 2009 ; Biggio et al , 2012 ; Koh et al , 2018 ) . These concerns are further compounded by the evidence of Belinkov and Bisk ( 2018 ) that neural models for natural language generation are not robust to naturally noisy data . Our metrics may be repurposed to rank examples in designing curricula for curriculum learning ap - proaches ( Bengio et al , 2009 ) . Alternatively , they can serve as additional metrics for the ( possibly unsupervised ) evaluation of summarization systems , potentially mitigating deficiencies in standard metrics , such as ROUGE , by directly penalizing redundancy and semantic incoherence . Limitations . In this work , we restrict ourselves to single - document single - reference English language summarization datasets . While the datasets we study constitute a considerable fraction of dataset usage in the summarization community , several multi - document summarization datasets have been introduced ( e.g. Fabbri et al , 2019 ; Antognini and Faltings , 2020 ) and multi - reference summarization datasets have often been argued to be desirable due to under - constrained nature of the summarization task ( Kryscinski et al , 2019 ) and the ideal evaluation paradigm for ROUGE ( Lin , 2004 ) . Beyond English , both large summarization datasets ( Nguyen and Daum\u00e9 III , 2019 ; Varab and Schluter , 2020 ) and more general language resources / technologies ( Joshi et al , 2020 ) are less available , which may heighten the need for data quality assurance . More broadly , the measures that we introduce are automated , and therefore non - human , judgments of the quality of summarization data . Therefore , we only envision these measures to be useful as inexpensive first - order approximations of aspectlevel summary quality rather than bona fide replacements for human evaluation . Additionally , since we principally envision applying these metrics to datasets , we make no efforts to make these metrics robust to adversarially - crafted data and they are likely quite susceptible to adversarial attack .", "entities": [[17, 18, "TaskName", "summarization"], [180, 181, "TaskName", "summarization"], [220, 221, "TaskName", "summarization"], [237, 238, "TaskName", "summarization"], [241, 245, "TaskName", "multi - document summarization"], [267, 268, "TaskName", "summarization"], [284, 285, "TaskName", "summarization"], [311, 312, "TaskName", "summarization"], [377, 378, "TaskName", "summarization"], [442, 444, "TaskName", "adversarial attack"]]}
{"text": "In this work , we demonstrate that various aspects of summarization datasets can be intrinsically evaluated for . We specifically show this for 5 properties across 10 popular datasets , uncovering that dataset use is sometimes incongruous with the attributes of the underlying data . We also find that some aspectlevel estimators may be surprisingly effective at detecting low quality dataset examples . Our findings suggest that more intentional and deliberate decisions should be made in selecting summarization datasets for downstream modelling research and that further scrutiny should be placed upon summarization datasets released in the future .", "entities": [[10, 11, "TaskName", "summarization"], [77, 78, "TaskName", "summarization"], [91, 92, "TaskName", "summarization"]]}
{"text": "We evaluate for semantic coherence between successive pairs of sentences , exploiting the auxiliary training objective of BERT beyond its masked language modeling objective . In particular , we were especially interested in this given that many systems are designed with explicit handling of sentence boundaries ( e.g. more extractive systems first rank extractive sentences and then order a thresholded subset ) and datasets such as CNN - DM , which are artificially concatenated , may not be inherently coherent across sentence - boundaries . Our observations regarding the measure of coherence provided by BERT 's next - sentence predictions seem to contradict existing findings . In particular , introduce RoBERTa as a direct followup study to BERT and find that the next - sentence prediction objective is not an effective pretraining objective for improving representations for natural language understanding ; Yang et al ( 2019 ) also provide similar evidence . However , our findings do not contest these conclusions but instead suggest that , nonetheless , BERT is a strong next - sentence predictor and that these predictions are still useful for measuring coherence across sentences . While we considered word or subword measures of coherence , we did not consider alternative pretrained models that are pretrained on other objectives related to inter - sentence coherence such as ALBERT ( Lan et al , 2020 ) . Given the findings of Lan et al ( 2020 , 4.6 ) , it seems likely that the sentence order prediction task they use may be more effective for measuring semantic coherence . Concurrent work by Prabhumoye et al ( 2020 ) also substantiates the usefulness of BERT - based nextsentence prediction for measuring coherence and ranking sentences orders . That said , semantic coherence could also be evaluated using ( neural ) language models , especially in light of results suggest they may be consistent with human judgments regarding grammaticality and acceptability ( Chowdhury and Zamparelli , 2018 ; Warstadt et al , 2019 ) . We did consider this and found language modeling scores ( e.g. surprisal ) assigned via a pretrained high - quality causal lan - guage model ( GPT - 2 ) to be inconsistent with our human judgments . We believe language modeling scores in this sense are likely highly sensitive to the domain ( and even within - domain effects , e.g. lexical variation for XSum which is fairly limited given all articles are sourced from the BBC whereas for Newsroom the variation is greater given the heterogeneous group of publishers with more diversified writing styles ) .", "entities": [[17, 18, "MethodName", "BERT"], [20, 23, "TaskName", "masked language modeling"], [66, 69, "DatasetName", "CNN - DM"], [94, 95, "MethodName", "BERT"], [110, 111, "MethodName", "RoBERTa"], [114, 115, "DatasetName", "followup"], [117, 118, "MethodName", "BERT"], [137, 140, "TaskName", "natural language understanding"], [168, 169, "MethodName", "BERT"], [220, 221, "MethodName", "ALBERT"], [276, 277, "MethodName", "BERT"], [362, 363, "MethodName", "GPT"], [401, 402, "DatasetName", "XSum"]]}
{"text": "We use the versions of GW and CNN - DM dataset released by Gehrmann et al ( 2018 ) . 14 Sentence boundary tokens inserted by Gehrmann et al ( 2018 ) to improve summarization quality were removed to ensure fair comparison in our work . An important distinction in the use of the CNN - DM dataset for modeling is whether the entity - anonymized or non - anonymized version was used . This copy is non - anonymized and it is important to consider the stability of our metrics under this anonymization . We used the released version of the NYT dataset directly as it was released via LDC . We use the released version of the TL ; DR dataset provided by the authors of V\u00f6lske et al ( 2017 ) . 16 We use a version of the NWS dataset that was released via private communication with the authors of Grusky et al ( 2018 ) . We have verified with the authors that the data can be requested with the platform they released in their original work . 17 For all remaining datasets , we use the version released by Jung et al ( 2019 ) . 18 All of our conventions in using these five datasets follow their work .", "entities": [[7, 10, "DatasetName", "CNN - DM"], [34, 35, "TaskName", "summarization"], [54, 57, "DatasetName", "CNN - DM"]]}
{"text": "All datasets were first filtered to remove examples where either the document or summary was empty . We found only examples in CNN - DM failed this criterion and this constituted less than 0.1 % 114 287227 of the dataset . All results were reported then on the standard training set if we were aware of a standard split used consistently in the summarization system literature . Splits in the case of datasets sourced from the work of Jung et al ( 2019 ) followed their work . In all cases , the training set was at least 80 % of the full data collection , so we expect results to generalize to the portions of the collection that were not considered assuming splits were constructed by sampling uniformly at random ( we did not verify this ) . Sentence - level tokenization was performed using NLTK ( Loper and Bird , 2002 ) . Word - level tokenization was performed using SpaCy ( Honnibal and Montani , 2017 ) .", "entities": [[22, 25, "DatasetName", "CNN - DM"], [63, 64, "TaskName", "summarization"]]}
{"text": "We compute semantic coherence by predicting the probability of a sentence conditional on the preceding sentence using BERT . BERT was pretrained with exactly this objective ( beyond its masked language modeling objective ) and we use the released model as - is with no further fine - tuning . We use the bert - base - uncased model along with the associated tokenizer that was implemented in PyTorch ( Paszke et al , 2017 ) by HuggingFace in the transformers repository . 20", "entities": [[17, 18, "MethodName", "BERT"], [19, 20, "MethodName", "BERT"], [29, 32, "TaskName", "masked language modeling"]]}
{"text": "Figure 3 : Dataset : PeerRead . This summary simply is not in the same language and hence achieves a very high abstractivity . Original Text ( truncated ) : from russia with love\"screenplay byrichard maibaumadapted byjohanna harwoodbased on the novel byian fleming . . .", "entities": [[5, 6, "DatasetName", "PeerRead"]]}
{"text": "Figure 8 : Dataset : TL ; DR . We observe this trend quite frequently in TL ; DR . Specifically , since authors on the social discussion platform Reddit choose to provide these summaries at their discretion , we often find the \" summaries \" are attention - grabbing and serve a starkly different rhetorical purpose from how summaries are generally conceived . The mutual information between random variables X and Y is defined as : I ( X ; Y ) H ( X ) \u2212 H ( X | Y ) The entropy measures the uncertainty in the probability mass / density function of a random variable . As such , the mutual information measures how Original Text ( truncated ) : these are external links and will open in a new window1908 - king carlos and eldest son assassinated in lisbon . second son manuel becomes king . 1910 - king manuel ii abdicates amid revolution . . .", "entities": [[29, 30, "DatasetName", "Reddit"]]}
{"text": "Detector : Extremely High Compression Figure 9 : Dataset : XSum . We observe this trend quite frequently in XSum . For articles that are essentially timelines or other types of chronologies discussing historic events diachronically ( which forms a small but distinctive section of the writing style of BBC from our analysis ) , the summary extracted to accompany it is generally this string or a slightly altered version . We argue this summary is fairly unhelpful ( and is likely fairly uninteresting to test models on ; simple rule - based filtering made be preferable to avoid overestimating performance on this dataset because of these examples ) . much the entropy of X is reduced by ( on average ) due to the observation of Y . Intuitively , the claim is that the uncertainty about the summarization task that is reduced by the model ( which is uniquely determined by its training data , pretraining data , and architecture ) is at most what can be cumulatively reduced by the training data , pretraining data , and inductive biases encoded in the model 's architecture . Our hypothesis is that I ( S ; A ) is small for learning - based models with minimal inductive biases , such as neural networks . Further , we hypothesize that while I ( S ; P ) is likely nontrivial for popular pretraining regimes , the dominant term on the right - hand side is likely I ( S ; T ) . We do note that this second hypothesis may be false given the partial evidence of GPT - 3 ( Brown et al , 2020 ) and the successes it enjoys in few - shot learning due to pretraining at unprecedented scale . However , no evaluation is conducted on summarization data in that work .", "entities": [[10, 11, "DatasetName", "XSum"], [19, 20, "DatasetName", "XSum"], [139, 140, "TaskName", "summarization"], [269, 270, "MethodName", "GPT"], [285, 289, "TaskName", "few - shot learning"], [303, 304, "TaskName", "summarization"]]}
{"text": "We thank Anna Huang for her help with analyzing the data . We thank Ge Gao , Esin Durmus , and members of the Cornell and Stanford NLP groups for their valuable advice . We especially thank the reviewers and area chairs for their articulate and constructive feedback .", "entities": [[24, 25, "DatasetName", "Cornell"]]}
{"text": "CUNI - KIT System for Simultaneous Speech Translation Task at IWSLT 2022", "entities": [[7, 8, "TaskName", "Translation"]]}
{"text": "In this paper , we describe our submission to the Simultaneous Speech Translation at IWSLT 2022 . We explore strategies to utilize an offline model in a simultaneous setting without the need to modify the original model . In our experiments , we show that our onlinization algorithm is almost on par with the offline setting while being 3\u00d7 faster than offline in terms of latency on the test set . We also show that the onlinized offline model outperforms the best IWSLT2021 simultaneous system in medium and high latency regimes and is almost on par in the low latency regime . We make our system publicly available . 1", "entities": [[12, 13, "TaskName", "Translation"]]}
{"text": "This paper describes the CUNI - KIT submission to the Simultaneous Speech Translation task at IWSLT 2022 ( Anastasopoulos et al , 2022 ) by Charles University ( CUNI ) and Karlsruhe Institute of Technology ( KIT ) . Recent work on end - to - end ( E2E ) simultaneous speech - to - text translation ( ST ) is focused on training specialized models specifically for this task . The disadvantage is the need of storing an extra model , usually a more difficult training and inference setup , increased computational complexity Liu et al , 2021 ) and risk of performance degradation if used in offline setting ( Liu et al , 2020a ) . In this work , we base our system on a robust multilingual offline ST model that leverages pretrained wav2vec 2.0 ( Baevski et al , 2020 ) and mBART ( Liu et al , 2020b ) . We revise the onlinization approach by Liu et al ( 2020a ) and propose an improved technique with a fully controllable qualitylatency trade - off . We demonstrate that without any change to the offline model , our simultaneous system in the mid - and high - latency regimes is on par with the offline performance . At the same time , the model outperforms previous IWSLT systems in medium and high latency regimes and is almost on par in the low latency regime . Finally , we observe a problematic behavior of the average lagging metric for speech translation when dealing with long hypotheses , resulting in negative values . We propose a minor change to the metric formula to prevent this behavior . Our contribution is as follows : We revise and generalize onlinization proposed by Liu et al ( 2020a ) ; Nguyen et al ( 2021 ) and discover parameter enabling quality - latency trade - off , We demonstrate that one multilingual offline model can serve as simultaneous ST for three language pairs , We demonstrate that an improvement in the offline model leads also to an improvement in the online regime , We propose a change to the average lagging metric that avoids negative values .", "entities": [[12, 13, "TaskName", "Translation"], [48, 49, "DatasetName", "E2E"], [51, 57, "TaskName", "speech - to - text translation"], [146, 147, "MethodName", "mBART"]]}
{"text": "Simultaneous speech translation can be implemented either as a ( hybrid ) cascaded system ( Kolss et al , 2008 ; Elbayad et al , 2020 ; Liu et al , 2020a ; Bahar et al , 2021 ) or an end - to - end model Liu et al , 2021 ) . Unlike for the offline speech translation where cascade seems to have the best quality , the end - to - end speech translation offers a better qualitylatency trade - off ( Ansari et al , 2020 ; Liu et al , 2021 ; Anastasopoulos et al , 2021 ) . End - to - end systems use different techniques to perform simultaneous speech translation . Han et al ( 2020 ) uses wait - k ( Ma et al , 2019 ) model and metalearning to alleviate the data scarcity . Liu et al ( 2020a ) uses a unidirectional encoder with monotonic cross - attention to limit the dependence on future context . Other work ( Liu et al , 2021 ) proposes Cross Attention augmented Transducer ( CAAT ) as an extension of RNN - T ( Graves , 2012 ) . Nguyen et al ( 2021 ) proposed a hypothesis stability detection for automatic speech recognition ( ASR ) . The shared prefix strategy finds the longest common prefix in all beams . Liu et al ( 2020a ) explore such strategies in the context of speech recognition and translation . The most promising is the longest common prefix of two consecutive chunks . The downside of this approach is the inability to parametrize the quality - latency trade - off . We directly address this in our work .", "entities": [[210, 213, "TaskName", "automatic speech recognition"], [243, 245, "TaskName", "speech recognition"]]}
{"text": "Speech recognition and translation use chunking for simultaneous inference with various chunk sizes ranging from 300 ms to 2 seconds ( Liu , 2020 ; Nguyen et al , 2021 ) although the literature suggests that the turn - taking in conversational speech is shorter , around 200 ms ( Levinson and Torreira , 2015 ) . We investigate different chunk sizes in combination with various stable hypothesis detection strategies . As we document later , the chunk size is the principal factor that controls the quality - latency trade - off .", "entities": [[0, 2, "TaskName", "Speech recognition"], [5, 6, "TaskName", "chunking"]]}
{"text": "Committing hypotheses from incomplete input presents a possible risk of introducing errors . To reduce the instability and trade time for quality , we employ a stable hypothesis detection . Formally , we define a function pref ix ( W ) that , given a set of hypotheses ( i.e. , W c all if we want to consider the whole beam or W c best for the single best hypothesis obtained during the beam search decoding of the c - th chunk ) , outputs a stable prefix . We investigate several functions : Hold - n ( Liu et al , 2020a ) Hold - n strategy selects the best hypothesis in the beam and deletes the last n tokens from it : prefix ( W c best ) = W 0 : max ( 0 , | W | \u2212n ) , ( 1 ) where W c best is the best hypothesis obtained in the beam search of c - th chunk . If the hypothesis has only n or fewer tokens , we return an empty string . LA - n Local agreement ( Liu et al , 2020a ) displays the agreeing prefixes of the two consecutive chunks . Unlike the hold - n strategy , the local agreement does not offer any explicit quality - latency trade - off . We generalize the strategy to take the agreeing prefixes of n consecutive chunks . During the first n \u2212 1 chunks , we do not output any tokens . From the n - th chunk on , we identify the longest common prefix of the best hypothesis of the n consecutive chunks : prefix ( W c best ) = , if c < n , LCP ( W c\u2212n+1 best , ... , W c best ) , otherwise , ( 2 ) where LCP ( ) is longest common prefix of the arguments . SP - n Shared prefix ( Nguyen et al , 2021 ) strategy displays the longest common prefix of all the items in the beam of a chunk . Similarly to the LA - n strategy , we propose a generalization to the longest common prefix of all items in the beams of the n consecutive chunks : prefix ( W c all ) = , if c < n , LCP ( W c\u2212n+1 beam 1 ... B , ... , W c beam 1 ... B ) , otherwise , ( 3 ) i.e. , all beam hypotheses 1 , ... , B ( where B is the beam size ) of all chunks c \u2212 n + 1 , ... , c.", "entities": [[133, 134, "DatasetName", "0"], [137, 138, "DatasetName", "0"]]}
{"text": "In our experiments , we use two different models . First , we do experiments with a monolingual Model A , then for the submission , we use a multilingual and more robust Model B. 4 Model A is the KIT IWSLT 2020 model for the Offline Speech Translation task . Specifically , it is an end - to - end English to German Transformer model with relative attention . For more described description , refer to Pham et al ( 2020b ) .", "entities": [[48, 49, "TaskName", "Translation"], [64, 65, "MethodName", "Transformer"]]}
{"text": "For the submission , we use a multilingual Model B. We construct the SLT architecture with the encoder based on the wav2vec 2.0 ( Baevski et al , 2020 ) and the decoder based on the autoregressive language model pretrained with mBART50 ( Tang et al , 2020 ) . wav2vec 2.0 is a Transformer encoder model which receives raw waveforms as input and generates high - level representations . The architecture consists of two main components : first , a convolution - based feature extractor downsamples long audio waveforms into features that have similar lengths with spectrograms . After that , a deep Transformer encoder uses self - attention and feedforward neural network blocks to transform the features without further downsampling . During the self - supervised training process , the network is trained with a contrastive learning strategy ( Baevski et al , 2020 ) , in which the already downsampled features are randomly masked and the model learns to predict the quantized latent representation of the masked time step . During the supervised learning step , we freeze the feature extraction weights to save memory since the first layers are among the largest ones . We fine - tune all of the weights in the Transformer encoder . Moreover , to make the model more robust to the fluctuation in absolute positions and durations when it comes to audio signals , we added the relative position encodings ( Dai et al , 2019 ; Pham et al , 2020a ) to alleviate this problem . 5 Here we used the same pretrained model with the speech recognizer , with the large architecture pretrained with 53k hours of unlabeled data . mBART50 is an encoder - decoder Transformerbased language model . During training , instead of the typical language modeling setting of predicting the next word in the sequence , this model is trained to reconstruct a sequence from its noisy version ( Lewis et al , 2019 ) and later extended to a multilingual version ( Liu et al , 2020b ; Tang et al , 2020 ) in which the corpora from multiple languages are combined during training . mBART50 is the version that is pretrained on 50 languages . The mBART50 model follows the Transformer encoder and decoder ( Vaswani et al , 2017 ) . During fine - tuning , we combine the mBART50 decoder with the wav2vec 2.0 encoder , where both encoder and decoder know one modality . The crossattention layers connecting the decoder with the encoder are the parts that require extensive finetuning in this case , due to the modality mismatch between pretraining and fine - tuning . Finally , we use the model in a multilingual setting , i.e. , for English to Chinese , German , and Japanese language pairs by training on the combination of the datasets . The mBART50 vocabulary contains language tokens for all three languages and can be used to control the language output . For more details on the model refer to Pham et al ( 2022 ) .", "entities": [[54, 55, "MethodName", "Transformer"], [81, 82, "MethodName", "convolution"], [104, 105, "MethodName", "Transformer"], [137, 139, "MethodName", "contrastive learning"], [208, 209, "MethodName", "Transformer"], [237, 240, "MethodName", "relative position encodings"], [379, 380, "MethodName", "Transformer"]]}
{"text": "For the onlinization experiments , we use MuST - C ( Cattoni et al , 2021 ) tst - COMMON from the v2.0 release . We conduct all the experiments on the English - German language pair .", "entities": [[7, 10, "DatasetName", "MuST - C"]]}
{"text": "In this paper , we do not report any computationally aware metrics , as our implementation of Transformers is slow . Later , we implemented the same onlinization approach using wav2vec 2.0 and mBART from Huggingface Transformers ( Wolf et al , 2020 ) . The new implementation reaches faster than realtime inference speed . A ) and the submitted system ( Model B ) on the MuST - C v2 tst - COMMON . We also include the best IWSLT 2021 system ( USTC - NELSLIP ( Liu et al , 2021 ) ) .", "entities": [[33, 34, "MethodName", "mBART"], [67, 70, "DatasetName", "MuST - C"]]}
{"text": "This work has received support from the project \" Grant Schemes at CU \" ( reg . no . CZ.02.2.69/0.0/0.0/19_073/0016935 ) , the grant 19 - 26934X ( NEUREM3 ) of the Czech Science Foundation , the European Union 's Horizon 2020 Research and Innovation Programme under Grant Agreement No 825460 ( ELITR ) , and partly supported by a Facebook Sponsored Research Agreement \" Language Similarity in Machine Translation \" .", "entities": [[68, 70, "TaskName", "Machine Translation"]]}
{"text": "Neural machine translation ( NMT ) witnessed a lot of success in the past few years especially for high resource languages ( Vaswani et al , 2017 ) . Improving the quality of low resource languages is still challenging . Some of the popular techniques are adding high resource helper languages as in multilingual neural machine translation ( MNMT ) ( Dong et al , 2015 ; Firat et al , 2016 ; Ha et al , 2016 ; Johnson et al , 2017 ; Arivazhagan et al , 2019 ) , using monolingual data including pre - training ( Liu et al , 2020 ) , multi - task learning ( Wang et al , 2020 ) , back translation ( Sennrich et al , 2016 ) or any combination of these methods ( Barrault et al , 2020 ) and system combination of multiple systems ( Liu et al , 2018 ) . This paper describes the Microsoft Egypt Development Center ( EgDC ) submission to the WMT21 shared news translation task for three low resource language pairs ( six directions ) , Bengali \u2194 Hindi ( Bn \u2194 Hi ) , English \u2194 Hausa ( En \u2194 Ha ) and Xhosa \u2194 Zulu ( Xh \u2194 Zu ) . We focus on the constrained track because it is easier to compare different systems and it is always possible to improve performance by adding more data . The main features of our approach are as follows : Using a recently proposed multitask and multilingual learning framework to benefit from monolingual data in both the source and target languages ( Wang et al , 2020 ) . Using knowledge distillation ( Freitag et al , 2017 ) to create bilingual baselines from the original multilingual model and combining it with the multilingual model . The paper is organized as follows . Section 2 gives an overview of the data used in the constrained scenario , followed by section 3 that gives a detailed description of our approach . Section 4 presents our experimental evaluation . Finally , our findings are summarized in Section 5 .", "entities": [[1, 3, "TaskName", "machine translation"], [55, 57, "TaskName", "machine translation"], [107, 111, "TaskName", "multi - task learning"], [279, 281, "MethodName", "knowledge distillation"]]}
{"text": "For Bengali , English , Hindi and German , we apply fastText 1 language identification on the monolingual data to remove sentences which are not predicted as the expected language . We do the same for Hausa , Xhosa and Zulu using Polyglot 2 because fastText does not cover these three languages . The resulting size of the monolingual data of each language is shown in Table 2 .", "entities": [[11, 12, "MethodName", "fastText"], [13, 15, "TaskName", "language identification"], [45, 46, "MethodName", "fastText"]]}
{"text": "This subsection describes the individual systems and their training leading to the proposed system combination strategy in the following subsection . We first build bilingual models for the six primary directions using the data shown in Table 1 except the English \u2194 German . These serve as baselines to compare to the developed systems . The models use a transformer base architecture comprising 6 encoder and 6 decoder layers and a 24 K joint vocabulary built for Bengali \u2194 Hindi , a 8 K joint vocabulary built for English \u2194 Hausa and a 4 K joint vocabulary built for Xhosa \u2194 Zulu using sentencepiece ( Kudo and Richardson , 2018 ) to learn these subword units to tokenize the sentences . In addition to the baseline bilingual models , we use knowledge distilled ( KD ) data and back - translated ( BT ) data generated from a multilingual model to build another set of bilingual models for each of the six primary directions . This multilingual model is described below . The purpose of these models is to participate in the ensemble along with the multilingual models . The latter bilingual models follow the same transformer base architecture and joint vocabulary used in the baseline bilingual models . The multilingual model combines the 8 translation directions shown in Table 1 . These are the six primary directions plus English \u2194 German as a helper . The latter is mainly used to improve generation on the English centric directions . The model uses a 64 K joint vocabulary constructed using sentencepiece ( Kudo and Richardson , 2018 ) from a subset of the monolingual data of each language as described in Section 2 . The transformer model has 12 encoder and 6 decoder layers . In addition , a multitask objective is used during training to make use of monolingual data . The objective comprises the usual parallel data likelihood referred to as MT , a masked language model ( MLM ) at the encoder and a denoising auto - encoder ( DAE ) ( similar to mBART ( Liu et al , 2020 ) ) at the decoder side . The latter two objectives help leverage monolingual data for both the encoder and the decoder sides . The three objectives are combined using different proportions according to a schedule during the training . Please refer to ( Wang et al , 2020 ) for details . To summarize we build the following models : Bilingual models trained using parallel data in Table 1 for the 6 primary directions . These are mainly used as baselines . Multilingual models trained using a multitask objective using parallel and monolingual data and comprising 8 directions . Bilingual models trained using KD and BT data generated using our best multilingual model . These are combined with the best multilingual model as described in 3.2 .", "entities": [[103, 104, "MethodName", "sentencepiece"], [260, 261, "MethodName", "sentencepiece"], [330, 331, "DatasetName", "MLM"], [337, 338, "TaskName", "denoising"], [347, 348, "MethodName", "mBART"]]}
{"text": "Knowledge - grounded generation models that utilize retrieved results ( e.g. , relevant documents from Wikipedia ) to generate informative responses have been proposed to perform knowledge - intensive NLP tasks ( e.g. , open - domain question answering ) . The knowledge - grounded generation has a similar form with the exemplar - based generation . However , the main difference is that knowledgegrounded generative models extract the knowledge from external resources to generate the informative response . Guu et al ( 2020 ) show the effectiveness of pre - training a knowledge retriever with the largescale language model for open - domain question answering , and Lewis et al ( 2020 ) demonstrate that knowledge - grounded generative models produce more informative and diverse sentences than vanilla generative models on a wide range of knowledgeintensive NLP tasks . Fan et al ( 2021 ) similarly propose a knowledge - grounded generative model for response generation , but they do not focus on the open - domain conversation . In Method Section , we demonstrate the difference between our approach and knowledge - grounded generative models , and we show that existing knowledge - grounded generative models are not directly applicable to the open - domain conversation in Experiments Section .", "entities": [[34, 39, "TaskName", "open - domain question answering"], [101, 106, "TaskName", "open - domain question answering"], [155, 157, "TaskName", "response generation"]]}
{"text": "As mentioned in Roller et al ( 2021 ) , the primitive exemplar - based generative model tends to ignore the retrieved exemplar dur - ing response generation due to the one - to - many problem in open - domain conversation ( Li et al , 2016 ) . Since its retriever searches an exemplar based on a given context , the retrieved exemplar is often significantly different from a gold response of the generator , although both of the retrieved exemplar and gold response are relevant to the given context , which is shown in Figure 2 ( a ) . As the retrieved exemplar is not helpful for generating the gold response , the generator is trained to ignore the retrieved exemplar and to produce a response using only the given context . To induce the generator to utilize retrieved exemplars more actively , Roller et al ( 2021 ) make use of the gold response , and Cai et al ( 2019b ) use perturbed gold response as an exemplar rather than using retrieved exemplars during the model training . However , since the exemplar z i and the gold response r i are too similar ( as shown in Figure 2 ( b ) ) , the exemplar - based generative model learns to rely overly on the exemplar . Eventually , the generator produces a highly over - fitted response to the exemplar by directly copying the tokens of the exemplar .", "entities": [[26, 28, "TaskName", "response generation"]]}
{"text": "Relevant but Lexically Distanced to the Gold Response We describe how CORGE selects semantically relevant but lexically distanced exemplars to the gold response . Conventionally , the retriever selects the exemplars z based on the relevance score S R ( z , c i ) for the given context c i . However , this searching process could return a significantly different exemplar z from the gold response r i , and it induces the generator G to ignore the retrieved exemplar during response generation . Therefore , we select exemplars based on the gold response r i to ensure that the generator G utilizes the exemplars inspired by Wu et al . We select top - k scoring exemplars based on the score S R \u2032 ( z , r i ) , which we call k - Nearest Exemplars ( kNE ) . 1 These kNE are more semantically related to the gold response r i than the exemplar obtained by using S R ( z , c i ) . However , some of the selected kNE are lexically identical or too close to the gold response r unintentionally since the retriever searches the exemplars based on the gold response . We observe that using these exemplars also causes the overfitting problem of generated responses ; therefore , the generator excessively copies tokens from the exemplars . From this , we are motivated to filter out the exemplars which are lexically too close to the gold response and preserve the exemplars properly distanced to the gold response to mitigate the over - fitting problem . Here , we employ Jaccard similarity to measure the lexical similarity ( Guu et al , 2018 ; Cai et al , 2019a ; Wu et al , 2019 ) between the exemplar and the gold response . Exemplars are filtered out when their Jaccard distance with the gold response r is larger than 0.6 , and we replace them with the randomly chosen responses from the pre - defined response set R. The threshold of filtering is empirically chosen as 0.6 . The set of the final exemplars z obtained through these steps is referred to as Z i = { z i , 1 , z i , 2 , , z i , k } .", "entities": [[83, 85, "TaskName", "response generation"]]}
{"text": "We utilize the following four datasets used in Roller et al ( 2021 ) , which are Blended Skill Talk ( BST ) ( Smith et al , 2020 ) , ConvAI2 ( Zhang et al , 2018 ) , Empathetic Dialogues ( ED ) ( Rashkin et al , 2019 ) , and Wizard of Wikipedia ( WoW ) . To simplify the notation , we denote the concatenated version of these four datasets as BST+ . We split BST+ into train , validation , and test sets following Smith et al ( 2020 ) .", "entities": [[17, 20, "DatasetName", "Blended Skill Talk"], [31, 32, "DatasetName", "ConvAI2"], [54, 57, "DatasetName", "Wizard of Wikipedia"]]}
{"text": "Our retriever follows the architecture of Biencoder ( Mazare et al , 2018 ) , and the score S R ( z , c ) and S R \u2032 ( z , r ) are calculated as follows : S R ( z , c ) = d ( z ) q ( c ) , S R \u2032 ( z , r ) = d ( z ) d ( r ) , d ( z ) = BERT r ( z ) , d ( r ) = BERT r ( r ) , q ( c ) = BERT c ( c ) , ( 3 ) where d ( z ) and d ( r ) are encoded vectors produced by response encoder BERT r and q ( c ) is an encoded vector produced by context encoder BERT c . The notation R \u2032 indicates that it only uses the response encoder instead of using the context encoder together . CORGE is not limited to use Bi - encoder as a retriever and can be applied to other types of a retriever ( e.g. Poly - encoder ( Humeau et al , 2019 ) ) .", "entities": [[79, 80, "MethodName", "BERT"], [90, 91, "MethodName", "BERT"], [101, 102, "MethodName", "BERT"], [127, 128, "MethodName", "BERT"], [142, 143, "MethodName", "BERT"]]}
{"text": "As we mentioned in Section 5.2 , we employ Biencoder 256 M and Blender 90 M as a retriever and a generator of each exemplar - based generative model , respectively . For MatToGen , additional MLP layers are added to the retriever , as follows the details in Cai et al ( 2019b ) . When training the models , weights of the retriever and the generator are initialized with the pre - trained Bi - encoder 256 M and Blender 90 M , respectively , For Blender 90 M , we use the model released by ParlAI ( Miller et al , 2017 )", "entities": [[13, 14, "MethodName", "Blender"], [36, 37, "DatasetName", "MLP"], [81, 82, "MethodName", "Blender"], [88, 89, "MethodName", "Blender"]]}
{"text": "When we generate samples using generative model , exemplar - based generative models , and knowledgegrounded generative models , we adopt a beam decoding strategy which is widely used in generative models ( Graves , 2012 ) . Following ( Roller et al , 2021 ) , we choose a minimum beam length and a beam size as 20 BPE tokens and 10 , respectively , and use tri - gram beam blocking on context and response blocks . During the inference phase , both exemplar - based generative models and knowledgegrounded generative models use the top - 1 scoring candidate as an exemplar chosen from utilizing the relevance score S R ( z , c ) .", "entities": [[59, 60, "MethodName", "BPE"]]}
{"text": "We measure how much time spend when the model generates the responses . When generating the response , Blender 90 M takes 0.481 seconds , and Ret - NRef + CORGE takes 0.523 seconds per instance . There is only an 8.7 % amount of inference time gap between Blender 90 M and RetNRef + CORGE . This tells us that exemplar - based generation can significantly improve the quality of responses regarding appropriateness , informativeness , and diversity without increasing the amount of time to generate answers . We test our model on NVIDIA DGX Station A100 with PyTorch 1.7.1 , CUDA 11.0 , CuDNN 8.0 , and here we adopt the generation strategy we describe above . When we measure the inference time , we only use a single GPU ( NVIDIA A100 GPU , 40 GB Memory ) , and the inference time is measured as the average inference time of 100 response generations .", "entities": [[18, 19, "MethodName", "Blender"], [49, 50, "MethodName", "Blender"]]}
{"text": "Any friends ? My dog is one of mine . Blender 90 M That 's good to hear , I ' ve a dog too , he ' s my best friend .", "entities": [[10, 11, "MethodName", "Blender"]]}
{"text": "Are you doing anything cool for halloween RAG No , I don ' t think I ' m going to . I ' ve never been a fan of halloween .", "entities": [[7, 8, "MethodName", "RAG"]]}
{"text": "Understanding procedural language requires reasoning about both hierarchical and temporal relations between events . For example , \" boiling pasta \" is a sub - event of \" making a pasta dish \" , typically happens before \" draining pasta , \" and requires the use of omitted tools ( e.g. a strainer , sink ... ) . While people are able to choose when and how to use abstract versus concrete instructions , the NLP community lacks corpora and tasks for evaluating if our models can do the same . In this paper , we introduce KIDSCOOK , a parallel script corpus , as well as a cloze task which matches video captions with missing procedural details . Experimental results show that state - of - the - art models struggle at this task , which requires inducing functional commonsense knowledge not explicitly stated in text . * Author now at Google . Work done while unaffiliated . 1 . Take the strainer with the pasta and pour the pasta into the sauce . 2 . Stir the pasta into sauce while it is in the pan . 3 . Let the pasta and sauce simmer for a few minutes .", "entities": [[152, 153, "DatasetName", "Google"]]}
{"text": "The level of detail used in natural language communication varies : descriptive or instructive text for experts may elide over details the reader can seamlessly infer , while text for more novice audiences may be more verbose . A given document typically adheres to a single level of verbosity suited to its presumed audience ( Grice , 1975 ) , so learning correspondences between abstract and detailed descriptions of similar concepts from text is a challenging problem . Commonsense knowledge of how complex events decompose into stereotypical sequences of simpler events is a necessary component of a system that can automatically understand and reason about different types of discourse . Hierarchical correspondences between abstract and detailed representations of concepts and events were an important aspect of the original formulation of scripts for natural language understanding ( Schank and 1 . Put a large pot half full of water on the stove . 2 . Turn the heat on under the pot and wait for the water to boil hard . 3 . Pour the pasta into the boiling water . Cook the pasta 4 . Pick up the strainer and shake it a little bit so more water comes out .", "entities": [[132, 135, "TaskName", "natural language understanding"]]}
{"text": "< l a t e x i t s h a 1 _ b a s e 6 4 = \" F V a Q d j U W Z V L y U U a l b b T R 1 N M D o K M = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / o h 6 9 L B b B U 0 l E 0 G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 H n f T m l t f W N z q 7 x d 2 d n d 2 z 9 w D 4 9 a J s k 0 4 0 2 W y E R 3 Q m q 4 F I o 3 U a D k n V R z G o e S t 8 P x 7 c x v P 3 F t R K I e c Z L y I K Z D J S L B K F r p A f t + 3 6 1 6 N W 8 O s k r 8 g l S h Q K P v f v U G C c t i r p B J a k z X 9 1 I M c q p R M M m n l V 5 m e E r Z m A 5 5 1 1 J F Y 2 6 C f H 7 q l J x Z Z U C i R N t S S O b q 7 4 m c x s Z M 4 t B 2 x h R H Z t m b i f 9 5 3 Q y j 6 y A X K s 2 Q K 7 Z Y F G W S Y E J m f 5 O B 0 J y h n F h C m R b 2 V s J G V F O G N p 2 K D c F f f n m V t C 5 q v l f z 7 y + r 9 Z s i j j K c w C m c g w 9 X U I c 7 a E A T G A z h G V 7 h z Z H O i / P u f C x a S 0 4 x c w x / 4 H z + A A W Y j Z w = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" F V a Q d j U W Z V L y U U a l b b T R 1 N M D o K M = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / o h 6 9 L B b B U 0 l E 0 G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 H n f T m l t f W N z q 7 x d 2 d n d 2 z 9 w D 4 9 a J s k 0 4 0 2 W y E R 3 Q m q 4 F I o 3 U a D k n V R z G o e S t 8 P x 7 c x v P 3 F t R K I e c Z L y I K Z D J S L B K F r p A f t + 3 6 1 6 N W 8 O s k r 8 g l S h Q K P v f v U G C c t i r p B J a k z X 9 1 I M c q p R M M m n l V 5 m e E r Z m A 5 5 1 1 J F Y 2 6 C f H 7 q l J x Z Z U C i R N t S S O b q 7 4 m c x s Z M 4 t B 2 x h R H Z t m b i f 9 5 3 Q y j 6 y A X K s 2 Q K 7 Z Y F G W S Y E J m f 5 O B 0 J y h n F h C m R b 2 V s J G V F O G N p 2 K D c F f f n m V t C 5 q v l f z 7 y + r 9 Z s i j j K c w C m c g w 9 X U I c 7 a E A T G A z h G V 7 h z Z H O i / P u f C x a S 0 4 x c w x / 4 H z + A A W Y j Z w = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" F V a Q d j U W Z V L y U U a l b b T R 1 N M D o K M = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / o h 6 9 L B b B U 0 l E 0 G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 H n f T m l t f W N z q 7 x d 2 d n d 2 z 9 w D 4 9 a J s k 0 4 0 2 W y E R 3 Q m q 4 F I o 3 U a D k n V R z G o e S t 8 P x 7 c x v P 3 F t R K I e c Z L y I K Z D J S L B K F r p A f t + 3 6 1 6 N W 8 O s k r 8 g l S h Q K P v f v U G C c t i r p B J a k z X 9 1 I M c q p R M M m n l V 5 m e E r Z m A 5 5 1 1 J F Y 2 6 C f H 7 q l J x Z Z U C i R N t S S O b q 7 4 m c x s Z M 4 t B 2 x h R H Z t m b i f 9 5 3 Q y j 6 y A X K s 2 Q K 7 Z Y F G W S Y E J m f 5 O B 0 J y h n F h C m R b 2 V s J G V F O G N p 2 K D c F f f n m V t C 5 q v l f z 7 y + r 9 Z s i j j K c w C m c g w 9 X U I c 7 a E A T G A z h G V 7 h z Z H O i / P u f C x a S 0 4 x c w x / 4 H z + A A W Y j Z w = < / l a t e x i t > < l a t e x i t s h a 1 _ b a s e 6 4 = \" F V a Q d j U W Z V L y U U a l b b T R 1 N M D o K M = \" > A A A B 6 n i c b V B N S 8 N A E J 3 U r 1 q / o h 6 9 L B b B U 0 l E 0 G P R i 8 e K 9 g P a U D b b T b t 0 s w m 7 E 6 G E / g Q v H h T x 6 i / y 5 r 9 x 2 + a g r Q 8 G H u / N M D M v T K U w 6 H n f T m l t f W N z q 7 x d 2 d n d 2 z 9 w D 4 9 a J s k 0 4 0 2 W y E R 3 Q m q 4 F I o 3 U a D k n V R z G o e S t 8 P x 7 c x v P 3 F t R K I e c Z L y I K Z D J S L B K F r p A f t + 3 6 1 6 N W 8 O s k r 8 g l S h Q K P v f v U G C c t i r p B J a k z X 9 1 I M c q p R M M m n l V 5 m e E r Z m A 5 5 1 1 J F Y 2 6 C f H 7 q l J x Z Z U C i R N t S S O b q 7 4 m c x s Z M 4 t B 2 x h R H Z t m b i f 9 5 3 Q y j 6 y A X K s 2 Q K 7 Z Y F G W S Y E J m f 5 O B 0 J y h n F h C m R b 2 V s J G V F O G N p 2 K D c F f f n m V t C 5 q v l f z 7 y + r 9 Z s i j j K c w C m c g w 9 X U I c 7 a E A T G A z h G V 7 h z Z H O i / P u f C x a S 0 4 x c w x / 4 H z + A A W Y j Z w = < / l a t e x i t > Figure 1 : An example KIDSCOOK sequence with multiple types of hierarchy and abstraction : the example contains sequences of complex instructions , given both as sentences and sequences of simpler instructions . Abelson , 1977 ; DeJong , 1981 ) but required handwritten data structures encoding world knowledge . However , the automatic induction of such commonsense knowledge from open - domain noisy text corpora remains an open problem ( Chambers , 2013 ; Weber et al , 2018 ; Zellers et al , 2018 ) . As a step towards solving this problem we consider textual descriptions of actions in a cooking domain . We introduce a dataset , KIDSCOOK , targeted at exploring the automatic acquisition of correspondences between abstract and concrete descriptions of actions . The dataset consists of higher - level single - sentence imperative descriptions paired with lower - level descriptions with elided details included . Descriptions come from real grounded actions , built on top of the YouCookII video caption dataset . Figure 1 gives an example annotation from the dataset : the phrase \" drain the pasta , \" presented to an annotator with its corresponding video clip , was annotated as corresponding to four constituent steps appropriate as instruction for a child . The constituent steps are \" simpler \" in the sense that they correspond to more atomic actions , but not necessarily in their linguistic complexity . We identify over 1 , 500 procedures and tools which KIDSCOOK makes explicit but are assumed as commonsense world knowledge by YouCookII . The KIDSCOOK dataset al ows us to learn mappings between abstract and concrete descriptions via sequence - to - sequence prediction . We apply several standard neural sequence - to - sequence models ; however , since these models do not expose explicit , interpretable correspondences between abstract and concrete descriptions , we also propose the application of neural transduction models which capture correspondences with latent hard alignment variables . We define a cloze - style evaluation to complement our dataset , in which models must predict the values of held - out tokens which target knowledge of tool usage , temporal ordering , and kitchen commonsense . We find that our neural transduction models are able to match the predictive power of traditional neural sequence models while providing interpretable alignments between abstract and concrete subsequences useful for our primary goal of analysis of implicit hierarchical script knowledge .", "entities": [[84, 85, "DatasetName", "0"], [87, 88, "DatasetName", "0"], [106, 107, "DatasetName", "0"], [182, 183, "DatasetName", "0"], [184, 185, "DatasetName", "0"], [387, 388, "DatasetName", "0"], [476, 477, "DatasetName", "0"], [589, 590, "DatasetName", "0"], [592, 593, "DatasetName", "0"], [611, 612, "DatasetName", "0"], [687, 688, "DatasetName", "0"], [689, 690, "DatasetName", "0"], [892, 893, "DatasetName", "0"], [981, 982, "DatasetName", "0"], [1094, 1095, "DatasetName", "0"], [1097, 1098, "DatasetName", "0"], [1116, 1117, "DatasetName", "0"], [1192, 1193, "DatasetName", "0"], [1194, 1195, "DatasetName", "0"], [1397, 1398, "DatasetName", "0"], [1486, 1487, "DatasetName", "0"], [1599, 1600, "DatasetName", "0"], [1602, 1603, "DatasetName", "0"], [1621, 1622, "DatasetName", "0"], [1697, 1698, "DatasetName", "0"], [1699, 1700, "DatasetName", "0"], [1902, 1903, "DatasetName", "0"], [1991, 1992, "DatasetName", "0"]]}
{"text": "We construct a task on Amazon 's Mechanical Turk , where workers are asked to explain a video action caption to a child . 2 Every instruction is paired with the original YouTube video and YouCook caption so the annotator could see how 1 Notable exceptions include the hierarchical instructions of ( Regneri et al , 2013 ) and ( Bisk et al , 2016 the action was performed , rather than hallucinating additional details . All captions received three simplifications . The instructions ask users to focus on missing information and allow them up to five steps . Finally , we explicitly asked annotators to simplify complex actions ( e.g. dice ) that can be defined by a series of more basic actions ( e.g. cut ) . Our KIDSCOOK corpus statistics are shown in Table 1 . In total we collected over 10 K action sequences ( \u223c400 K tokens ) . The average caption is approximately 4x longer than a YouCook caption . Most importantly 1 , 536 lemmas and 2 , 316 lexical types from KIDSCOOK 's vocabulary do not appear in any of the original captions . This indicates that there are over 1 , 500 new concepts , tools , and procedures that were assumed by YouCookII but are now explicit in KIDSCOOK .", "entities": [[35, 36, "DatasetName", "YouCook"], [163, 164, "DatasetName", "YouCook"]]}
{"text": "wrap the pizza . Pred find a large piece to put the pizza om . place the pizza in the center for it not to stick around . grab the plastic wrap and start wrapping the entire thing and pizza . wrap all around until completely covered on all corners . put in freezer on a cold water and freeze overnight Gold find a hard surface to put the pizza om . place the pizza in the center for it not to slide around . grab the plastic wrap and start wrapping the hard surface and pizza . wrap all around until fully covered on all corners . put in freezer on a flat surface and freeze overnight of the phenomena we would hope to see ( Table 3 ) . The left - hand side of the table shows words from the abstract YouCook annotations and corresponding phrases in the concrete annotation . For the righthand side we searched for common concrete terms that may be preceded or followed by other terms , and present the abstract terms they were most often generated by . Finally , Table 5 shows three randomly chosen examples ( from the validation set ) of greedy decodings for slot filling with GPT fine - tuned on our dataset . These examples demonstrate that , first , there are cases where GPT is successful or produces a semantically valid answer ( e.g. fully vs completely ) . Second , as is common with greedy decoding , the model can get stuck in a loop ( e.g. cut , cutting , cutting , ... ) . Finally , note there are nonsensical cases where the model appears to have discarded the abstract context ( e.g. knife to add tomato sauce or freezer on a cold water ) .", "entities": [[144, 145, "DatasetName", "YouCook"], [205, 207, "TaskName", "slot filling"], [208, 209, "MethodName", "GPT"], [227, 228, "MethodName", "GPT"]]}
{"text": "Many script learning systems are based on event co - occurrence and language modeling in large text corpora , and can infer implicit events without creating explicit situation - specific frame structures ( Chambers and Jurafsky , 2008 ; Rudinger et al , 2015 ; Pichotta and Mooney , 2016 ) . Other systems induce situation - specific frames from text ( Cheung et al , 2013 ; Balasubramanian et al , 2013 ) . However , these methods do not explicitly target the commonsense correspondence between differing levels of detail of complex events . Most relevant to this paper is the pioneering work of Regneri et al ( 2013 ) as extended by Senina et al ( 2014 ) and . These papers present the TACOS corpus , consisting of natural language descriptions of activities in videos paired with low - level activity labels . Senina et al ( 2014 ) collect an additional level of multi - sentence annotations on the corpus , which allowing for video caption generation at multiple levels of detail . describe a similar corpus of natural descriptions of composite actions , useful for activity recognition in video . These corpora differ in a number of important ways from KIDSCOOK ; in particular , the language has somewhat limited complexity and \" naturalness \" when describing complex scenarios , a phenomenon also observed in the robotics literature ( Scalise et al , 2018 ) . Our data collection process avoids more formulaic language by eliciting \" child - directed \" descriptions .", "entities": [[190, 192, "TaskName", "activity recognition"]]}
{"text": "We model the conditional probability of a concrete sequence y given abstract sequence x through a latent alignment variable a between x and y , which is a sequence of variables a j , with a j = i signifying that y j is aligned to x i . The marginal probability of y given x is p ( y | x ) = a p ( y , a | x ) . ( 1 ) In the following , we use m to denote the length of x and n to denote the length of y. The model formulation restricts alignments to be monotonic , i.e. a j+1 \u2265 a j for all j. The model factorizes over timesteps into alignment and word prediction probabilities , such that the word prediction at each timestep is informed by its alignment : p ( y , a | x ) = j p ( a j | a j\u22121 , x 1 : a j\u22121 , y 1 : j\u22121 ) \u00d7 p ( y j | a j , x 1 : a j , y 1 : j\u22121 ) ( 2 ) The abstract and concrete sequences are both encoded with LSTM Recurrent Neural Networks ( Hochreiter and Schmidhuber , 1997 ) . In contrast to standard attention - based models , the aligned encoder representation is not fed into the decoder RNN state , but only used to make next word predictions . Due to the small size of the training data , words in both sequences are embedded using fixed GloVe embeddings ( Pennington et al , 2014 ) . The word emission probability is then defined as p ( yj | aj , x1 : a j , y1 : j\u22121 ) = softmax ( MLP ( ea j , dj ) ) ( 3 ) with e the encoder hidden states and d the decoder hidden states . The alignment probability factorizes into shift and emit probabilities , where a shift action increments the alignment to the next word in the input sequence , and an emit action generates the next output word . We refer to these as transition probabilities . This formulation enables us to restrict the hard alignment to be monotonic . We consider two parameterizations of this distribution . In the first , the probabilities are parameterized by the neural network , using the encoder and decoder hidden state in a similar manner to how the word emission probability was computed . The alignment probability at a given timestep is therefore parameterized as p ( aj | aj\u22121 , x1 : a j\u22121 , y1 : j\u22121 ) = p ( emit | aj , x1 : a j , y1 : j\u22121 ) \u00d7 a j \u22121 i = a j\u22121 p ( shift | i , x1 : i , y1 : j\u22121 ) , ( 4 ) where p ( shift | i , x1 : i , y1 : j\u22121 ) = \u03c3 ( M LP ( ei , dj ) ) , ( 5 ) p ( emit | i , x1 : i , y1 : j\u22121 ) = 1 \u2212 p ( shift | i , x1 : i , y1 : j\u22121 ) . ( 6 ) We also consider using the simpler , fixed alignment parameterization in Yu , Buys , and Blunsom ( 2016 ) , where the transition probability is conditioned only on sequence length , not on x or y , and can therefore be estimated using the ratio between input and output sentence lengths . The alignment probabilities are not updated during training , and consequently the posterior distribution over the alignments is biased towards this prior , favoring alignments close to the diagonal . The parameterized alignment model contains as special cases two degenerate solutions : ( 1 ) an unconditional language model and ( 2 ) a seq2seq model . These occur if the model performs all emits before shifting or all shifts before emitting , respectively . To prevent the creation of a language model we force the last output word to be aligned to the last word in the abstract sequence , similar to Yu et al ( 2017 ) . However , the parameterized transition model could still in practice revert to a pure sequence - to - sequence model .", "entities": [[203, 204, "MethodName", "LSTM"], [264, 266, "MethodName", "GloVe embeddings"], [298, 299, "MethodName", "softmax"], [300, 301, "DatasetName", "MLP"], [662, 663, "MethodName", "seq2seq"]]}
{"text": "Improving Joint Training of Inference Networks and Structured Prediction Energy Networks", "entities": [[7, 9, "TaskName", "Structured Prediction"]]}
{"text": "We now discuss several methods that simplify and stabilize training SPENs with inference networks . When describing them , we will illustrate their impact by showing training trajectories for the Twitter part - of - speech tagging task .", "entities": [[31, 37, "TaskName", "part - of - speech tagging"]]}
{"text": "We contributed several strategies to stabilize and improve joint training of SPENs and inference networks . Our use of joint parameterizations mitigates the need for inference network fine - tuning , leads to complementarity in the learned inference networks , and yields improved performance overall . These developments offer promise for SPENs to be more easily applied to a broad range of NLP tasks . Future work will explore other structured prediction tasks , such as parsing and generation . We have taken initial steps in this direction , considering constituency parsing with the sequence - to - sequence model of Tran et al ( 2018 ) . Preliminary experiments are positive , 5 but significant challenges remain , specifically in defining appropriate inference network architectures to enable efficient learning .", "entities": [[70, 72, "TaskName", "structured prediction"], [90, 92, "TaskName", "constituency parsing"]]}
{"text": "YNU - HPCC at SemEval - 2021 Task 6 : Combining ALBERT and Text - CNN for Persuasion Detection in Texts and Images", "entities": [[11, 12, "MethodName", "ALBERT"]]}
{"text": "In recent years , memes combining image and text have been widely used in social media , and memes are one of the most popular types of content used in online disinformation campaigns . In this paper , our study on the detection of persuasion techniques in texts and images in SemEval - 2021 Task 6 is summarized . For propaganda technology detection in text , we propose a combination model of both AL - BERT and Text - CNN for text classification , as well as a BERT - based multi - task sequence labeling model for propaganda technology coverage span detection . For the meme classification task involved in text understanding and visual feature extraction , we designed a parallel channel model divided into text and image channels . Our method 1 achieved a good performance on subtasks 1 and 3 . The micro F 1scores of 0.492 , 0.091 , and 0.446 achieved on the test sets of the three subtasks ranked 12th , 7th , and 11th , respectively , and all are higher than the baseline model .", "entities": [[75, 76, "MethodName", "BERT"], [81, 83, "TaskName", "text classification"], [88, 89, "MethodName", "BERT"], [106, 108, "TaskName", "meme classification"]]}
{"text": "The intentional shaping of information to promote a predetermined agenda is called propaganda . Propaganda uses psychological and rhetorical techniques to achieve its purpose . Propaganda techniques generally include the use of logical fallacies and appeal to the emotions of the audience . In recent years , memes combining images and text have been widely used in social media , and the use of memes can easily and effectively attract a large number of users on social platforms . Memes are one of the most popular types of content used in online disinformation campaigns , and memes applied in a disinformation campaign achieve their purpose of influencing users through rhetorical and psychological techniques . Therefore , it is meaningful to research computational techniques for automatically detecting propaganda in particular content . The SemEval 2021 Task 6 ( Dimitrov et al , 2021 ) consists of three subtasks : Subtask 1 - Given only the \" textual content \" of a meme , identify which of the 20 techniques are used . The 20 techniques include appeal to authority , loaded language , and name calling or labeling . Subtask 2 : Given only the \" textual content \" of a meme , identify which of the 20 techniques are used along with the span ( s ) of the text covered by each technique . Subtask 3 : Given a meme , identify which of the 22 techniques are used for both the textual and visual content of the meme . These 22 technologies include the 20 technologies in subtasks 1 and 2 , and 2 technologies , i.e. , transfer and appeal to ( strong ) emotions , are added . The detection of propaganda techniques in texts is similar to a text sentiment analysis , and both can be attributed to text classification tasks . In a previous study , Peng et al ( 2020 ) used the adversarial learning of sentiment word representations for a sentiment analysis . A tree - structured regional CNN - LSTM and dynamic routing in a tree - structured LSTM ( Wang et al , 2019 ) were used for a dimensional sentiment analysis . In previous SemEval competitions , Dao et al ( 2020 ) used GloVe - LSTM and BERT - LSTM models , and Paraschiv et al ( 2020 ) used an ensemble model containing BERT and BiLSTM to detect both spans and categories of propaganda techniques in news articles . In addition , in multimodal analysis combining images and text , Yuan et al ( 2020 ) proposed a parallel channel ensemble model combining BERT embedding , BiLSTM , attention and CNN , and ResNet for a sentiment analysis of memes . Li et al ( 2019 ) proposed a Visual BERT model that aligns and fuses text and image information using transformers ( Vaswani et al , 2017 ) . In this paper , we propose three different systems for the three subtasks in SemEval - 2021 Task 6 . For subtask 1 , we added a Text - CNN layer after the pre - trained model ALBERT to fine - tune it for a multi - label classification of text . For subtask 2 , we used the idea of partitioning to transform the problem into the detection of 20 techniques for each text separately . BERT was used in the model for text feature extraction followed by multi - task sequence labeling , and the results of each task were combined to obtain the final results . For subtask 3 , we built the system using a parallel channel model containing text and image channels . The text channel used both the ALBERT and Text - CNN models to extract features of text in the meme , and the image channel used ResNet and VGGNet for image feature extraction . The information extracted by the two parallel channels was then combined through a fully connected layer after concatenation . Using micro F 1 - scores as metrics , the results of the proposed model in subtasks 1 , 2 , and 3 were 0.625 , 0.215 , and 0.636 , respectively , on the dev set . The remainder of this paper is organized as follows . First , section 2 describes the details of the w1 , w2 , w3 , w4 , \u2026 \u2026 , wn - 1 , wn ALBERT and Text - CNN used in our system . Section 3 then presents the experimental results . Finally , some concluding remarks are presented in section 4 .", "entities": [[32, 34, "TaskName", "logical fallacies"], [294, 296, "TaskName", "sentiment analysis"], [303, 305, "TaskName", "text classification"], [328, 330, "TaskName", "sentiment analysis"], [338, 339, "MethodName", "LSTM"], [347, 348, "MethodName", "LSTM"], [360, 362, "TaskName", "sentiment analysis"], [375, 376, "MethodName", "GloVe"], [377, 378, "MethodName", "LSTM"], [379, 380, "MethodName", "BERT"], [381, 382, "MethodName", "LSTM"], [396, 397, "MethodName", "BERT"], [398, 399, "MethodName", "BiLSTM"], [436, 437, "MethodName", "BERT"], [439, 440, "MethodName", "BiLSTM"], [446, 447, "MethodName", "ResNet"], [449, 451, "TaskName", "sentiment analysis"], [463, 464, "MethodName", "BERT"], [520, 521, "MethodName", "ALBERT"], [528, 532, "TaskName", "multi - label classification"], [560, 561, "MethodName", "BERT"], [617, 618, "MethodName", "ALBERT"], [637, 638, "MethodName", "ResNet"], [737, 738, "MethodName", "ALBERT"]]}
{"text": "Subtask 2 was a multi - label sequence - labeling task . We built the model by converting the problem to detect the coverage of each propagation technique separately for the input sequence , and built a multi - task sequence labeling model based on a fine - tuning of BERT . As illustrated in Figure 3 , the input sequence was first obtained using the pre - trained BERT ( Devlin et al , 2019 ) model with a hidden representation matrix with dimensions of 512 \u00d7 768 . Subsequently , 20 parallel fully connected layers were input separately for the detection of each propaganda technique coverage span ( For each propagation technique , the sequence labeling task is performed separately for the input text ) . For each technique , the intermediate result of each parallel channel output is a 512 \u00d7 41 matrix , and the ensemble layer represents the stacking of 20 matrices from 20 parallel channels , the dimensions of the final output were 20 \u00d7 512 \u00d7 41 , which denote the propaganda technique category , maximum sentence length , and code corresponding to each technique , respectively .", "entities": [[50, 51, "MethodName", "BERT"], [69, 70, "MethodName", "BERT"]]}
{"text": "For subtask 3 , we modeled the problem as a multilabel classification task of the meme text and image content . We used a parallel channel model of text and image channels , and then concatenated the text and image features extracted by the two parallel channels to apply multi - label meme classification . The architecture of the proposed model is shown in Figure 4 . Text Channel . In the text channel , we used the ALBERT - Text - CNN model used in subtask 1 , taking the text part of the meme content as an input to obtain a 768 - dimensional text feature vector as the output . Image Channel . In the image channel , we used ResNet and VGGNet , taking the image part of the meme content as input to obtain a 512 - dimensional image feature vector as the output . The ResNet model ( He et al , 2016 ) is a deep residual learning model for image recognition , and presents the interlayer residual jump connection and solves the deep vanishing gradient problem . VGGNet ( Simonyan and Zisserman , 2015 ) is a deep convolutional neural network with small - sized convolutional kernels and a regular network structure , in which the size of the convolution kernels used in VGG16 in our experiment is 3 \u00d7 3 , and the pooling kernels is 2 \u00d7 2 . Furthermore , only the structures of the ResNet and VGGNet were used in our experiment , and the pre - training weights were not applied .", "entities": [[52, 54, "TaskName", "meme classification"], [78, 79, "MethodName", "ALBERT"], [123, 124, "MethodName", "ResNet"], [151, 152, "MethodName", "ResNet"], [167, 169, "TaskName", "image recognition"], [217, 218, "MethodName", "convolution"], [246, 247, "MethodName", "ResNet"]]}
{"text": "In this paper , we presented our system for the SemEval - 2021 Task 6 , the experimental results in subtasks 1 and 3 show that our proposed ALBERT - Text - CNN model and the parallel channel model achieved a good performance in the detection of persuasion techniques in texts and images . We participated in all three subtasks and achieved the 12th , 7th , and 11th places in the test set , respectively . In a future study , to improve the generalization ability of the model , we will focus on how to deal with the problems caused by unbalanced training data .", "entities": [[28, 29, "MethodName", "ALBERT"]]}
{"text": "PubMedQA : A Dataset for Biomedical Research Question Answering", "entities": [[0, 1, "DatasetName", "PubMedQA"], [7, 9, "TaskName", "Question Answering"]]}
{"text": "A long - term goal of natural language understanding is to build intelligent systems that can reason and infer over natural language . The question answering ( QA ) task , in which models learn how to answer questions , is often used as a benchmark for quantitatively measuring the reasoning and inferring abilities of such intelligent systems . While many large - scale annotated general domain QA datasets have been introduced ( Rajpurkar et al , 2016 ; Lai et al , 2017 ; Ko\u010disk\u1ef3 Question : Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting ? Context : ( Objective ) Recent studies have demonstrated that statins have pleiotropic effects , including anti - inflammatory effects and atrial fibrillation ( AF ) preventive effects [ ... ] ( Methods ) 221 patients underwent CABG in our hospital from 2004 to 2007 . 14 patients with preoperative AF and 4 patients with concomitant valve surgery [ ... ] ( Results ) The overall incidence of postoperative AF was 26 % . Postoperative AF was significantly lower in the Statin group compared with the Non - statin group ( 16 % versus 33 % , p=0.005 ) . Multivariate analysis demonstrated that independent predictors of AF [ ... ] Long Answer : ( Conclusion ) Our study indicated that preoperative statin therapy seems to reduce AF development after CABG . Answer : yes Figure 1 : An instance ( Sakamoto et al , 2011 ) of Pub - MedQA dataset : Question is the original question title ; Context includes the structured abstract except its conclusive part , which serves as the Long Answer ; Human experts annotated the Answer yes . Supporting fact for the answer is highlighted . Yang et al , 2018 ; , the largest annotated biomedical QA dataset , BioASQ ( Tsatsaronis et al , 2015 ) has less than 3k training instances , most of which are simple factual questions . Some works proposed automatically constructed biomedical QA datasets ( Pampari et al , 2018 ; Pappas et al , 2018 ; Kim et al , 2018 ) , which have much larger sizes . However , questions of these datasets are mostly factoid , whose answers can be extracted in the contexts without much reasoning . In this paper , we aim at building a biomedical QA dataset which ( 1 ) has substantial instances with some expert annotations and ( 2 ) requires reasoning over the contexts to answer the questions . For this , we turn to the PubMed 1 , a search engine providing access to over 25 million references of biomedical articles . We found that around 760k articles in PubMed use questions as their titles . Among them , the abstracts of about 120k articles are written in a structured style - meaning they have subsections of \" Introduction \" , \" Results \" etc . Conclusive parts of the abstracts , often in \" Conclusions \" , are the authors ' answers to the question title . Other abstract parts can be viewed as the contexts for giving such answers . This pattern perfectly fits the scheme of QA , but modeling it as abstractive QA , where models learn to generate the conclusions , will result in an extremely hard task due to the variability of writing styles . Interestingly , more than half of the question titles of PubMed articles can be briefly answered by yes / no / maybe , which is significantly higher than the proportions of such questions in other datasets , e.g. : just 1 % in Natural Questions and 6 % in HotpotQA ( Yang et al , 2018 ) . Instead of using conclusions to answer the questions , we explore answering them with yes / no / maybe and treat the conclusions as a long answer for additional supervision . To this end , we present PubMedQA , a biomedical QA dataset for answering research questions using yes / no / maybe . We collected all PubMed articles with question titles , and manually labeled 1k of them for cross - validation and testing . An example is shown in Fig . 1 . The rest of yes / no / answerable QA instances compose of the unlabeled subset which can be used for semisupervised learning . Further , we automatically convert statement titles of 211.3k PubMed articles to questions and label them with yes / no answers using a simple heuristic . These artificially generated instances can be used for pre - training . Unlike other QA datasets in which questions are asked by crowd - workers for existing contexts ( Rajpurkar et al , 2016 ; Yang et al , 2018 ; Ko\u010disk\u1ef3 et al , 2018 ) , in PubMedQA contexts are generated to answer the questions and both are written by the same authors . This consistency assures that contexts are perfectly related to the questions , thus making PubMedQA an ideal benchmark for testing scientific reasoning abilities . As an attempt to solve PubMedQA and provide a strong baseline , we fine - tune BioBERT on different subsets in a multi - phase style with additional supervision of long answers . Though this model generates decent results and vastly outperforms other baselines , it 's still much worse than the single - human performance , leaving significant room for future improvements .", "entities": [[6, 9, "TaskName", "natural language understanding"], [24, 26, "TaskName", "question answering"], [306, 307, "DatasetName", "BioASQ"], [608, 610, "DatasetName", "Natural Questions"], [614, 615, "DatasetName", "HotpotQA"], [660, 661, "DatasetName", "PubMedQA"], [806, 807, "DatasetName", "PubMedQA"], [837, 838, "DatasetName", "PubMedQA"], [852, 853, "DatasetName", "PubMedQA"]]}
{"text": "Biomedical QA : Expert - annotated biomedical QA datasets are limited by scale due to the difficulty of annotations . In 2006 and 2007 , TREC 2 held QA challenges on genomics corpus ( Hersh et al , 2006 ( Hersh et al , , 2007 , where the task is to retrieve relevant documents for 36 and 38 topic questions , respectively . QA4MRE ( Pe\u00f1as et al , 2013 ) included a QA task about Alzheimer 's disease ( Morante et al , 2012 ) . This dataset has 40 QA instances and the task is to answer a question related to a given document using one of five answer choices . The QA task of BioASQ ( Tsatsaronis et al , 2015 ) has phases of ( a ) retrieve question - related documents and ( b ) using related documents as contexts to answer yes / no , factoid , list or summary questions . BioASQ 2019 has a training set of 2 , 747 QA instances and a test set of 500 instances . Several large - scale automatically collected biomedical QA datasets have been introduced : emrQA ( Pampari et al , 2018 ) is an extractive QA dataset for electronic medical records ( EHR ) built by re - purposing existing annotations on EHR corpora . BioRead ( Pappas et al , 2018 ) and BMKC ( Kim et al , 2018 ) both collect cloze - style QA instances by masking biomedical named entities in sentences of research articles and using other parts of the same article as context . Yes / No QA : Datasets such as HotpotQA ( Yang et al , 2018 ) , Natural Questions , ShARC ( Saeidi et al , 2018 ) and BioASQ ( Tsatsaronis et al , 2015 ) contain yes / no questions as well as other types of questions . BoolQ ( Clark et al , 2019 ) specifically focuses on naturally occurring yes / no questions , and those questions are shown to be surprisingly difficult to answer . We add a \" maybe \" choice in PubMedQA to cover uncertain instances . Typical neural approaches to answering yes / no questions involve encoding both the question and context , and decoding the encoding to a class output , which is similar to the well - studied natural language inference ( NLI ) task . Recent breakthroughs of pre - trained language models like ELMo ( Peters et al , 2018 ) and BERT ( Devlin et al , 2018 ) show significant performance i m - provements on NLI tasks . In this work , we use domain specific versions of them to set baseline performance on PubMedQA . 3 PubMedQA Dataset", "entities": [[25, 26, "DatasetName", "TREC"], [118, 119, "DatasetName", "BioASQ"], [159, 160, "DatasetName", "BioASQ"], [192, 193, "DatasetName", "emrQA"], [276, 277, "DatasetName", "HotpotQA"], [285, 287, "DatasetName", "Natural Questions"], [288, 289, "DatasetName", "ShARC"], [297, 298, "DatasetName", "BioASQ"], [318, 319, "DatasetName", "BoolQ"], [356, 357, "DatasetName", "PubMedQA"], [396, 399, "TaskName", "natural language inference"], [413, 414, "MethodName", "ELMo"], [422, 423, "MethodName", "BERT"], [457, 458, "DatasetName", "PubMedQA"], [460, 461, "DatasetName", "PubMedQA"]]}
{"text": "PubMedQA is split into three subsets : labeled , unlabeled and artificially generated . They are denoted as PQA - L ( abeled ) , PQA - U ( nlabeled ) and PQA - A ( rtificial ) , respectively . We show the architecture of PubMedQA dataset in Fig . 2 . Collection of PQA - L and PQA - U : PubMed articles which have i ) a question mark in the titles and ii ) a structured abstract with conclusive part are collected and denoted as pre - PQA - U. Now each instance has 1 ) a question which is the original title 2 ) a context which is the structured abstract without the conclusive part and 3 ) a long answer which is the conclusive part of the abstract . Two annotators 3 labeled 1k instances from pre - PQA - U with yes / no / maybe to build PQA - L using Algorithm 1 . The annotator 1 does n't need to do much reasoning to annotate since the long answer is available . We denote this reasoning - free setting . However , the annotator 2 can not use the long answer , so reasoning over the context is required for 3 Both are qualified M.D. candidates . Algorithm 1 PQA - L data collection procedure Input : pre - PQA - U ReasoningFreeAnnotation { } ReasoningRequiredAnnotation { } GroundTruthLabel { } while not finished do Randomly sample an instance inst from pre - PQA - U if inst is not yes / no / maybe answerable then Remove inst and continue to next iteration end if Annotator 1 annotates inst with l1 { yes , no , maybe } using question , context and long answer Annotator 2 annotates inst with l2 { yes , no , maybe } using question and context if l1 = l2 then la l1 else Annotator 1 and Annotator 2 discuss for an agreement annotation la if not la then Remove inst and continue to next iteration end if end if ReasoningFreeAnnotation [ inst ] l1 ReasoningRequiredAnnotation [ inst ] l2 GroundTruthLabel [ inst ] la end while annotation . We denote such setting as reasoningrequired setting . Note that the annotation process might assign wrong labels when both annotator 1 and annotator 2 make a same mistake , but considering human performance in 5.1 , such error rate could be as low as 1 % 4 . 500 randomly sampled PQA - L instances are used for 10 - fold cross validation and the rest 500 instances consist of Pub - MedQA test set . Further , we include the unlabeled instances in pre - PQA - U with yes / no / maybe answerable questions to build PQA - U. For this , we use a simple rule - based method which removes all questions started with interrogative words ( i.e. wh - words ) or involving selections from multiple entities . This results in over 93 % agreement with annotator 1 in identifying the questions that can be answered by yes / no / maybe . statement titles are converted to questions by simply moving or adding copulas ( \" is \" , \" are \" ) or auxiliary verbs ( \" does \" , \" do \" ) in the front and further revising for coherence ( e.g. : adding a question mark ) . We generate the yes / no answer according to negation status of the VB . Several examples are shown in Table 2 . We collected 211.3k instances for PQA - A , of which 200k randomly sampled instances are for training and the rest 11.3k instances are for validation .", "entities": [[0, 1, "DatasetName", "PubMedQA"], [46, 47, "DatasetName", "PubMedQA"]]}
{"text": "We show the basic statistics of three PubMedQA subsets in Table 1 . Instance Topics : PubMed abstracts are manually annotated by medical librarians with Medical Subject Headings ( MeSH ) 6 , which is a controlled vocabulary designed to describe the topics of biomedical texts . We use MeSH terms to represent abstract topics , and visualize their distribution in Fig . 3 . Nearly all instances are human studies and they cover a wide variety of topics , including retrospective , prospective , and cohort studies , different age groups , and healthcare - related subjects like treatment outcome , prognosis and risk factors of diseases . We use a Sankey diagram to show the proportional relationships between corresponded question type and reasoning type , as well as corresponded reasoning type and whether there are text interpretations of numbers in Fig . 4 .", "entities": [[7, 8, "DatasetName", "PubMedQA"]]}
{"text": "The main metrics of PubMedQA are accuracy and macro - F1 on PQA - L test set using question and context as input . We denote prediction using question and context as a reasoning - required setting , because under this setting answers are not directly expressed in the input and reasoning over the contexts is required to answer the question . Additionally , long answers are available at training time , so generation or prediction of them can be used as an auxiliary task in this setting . A parallel setting , where models can use question and long answer to predict yes / no / maybe answer , is denoted as reasoning - free setting since yes / no / maybe are usually explicitly expressed in the long answers ( i.e. : conclusions of the abstracts ) . Obviously , it 's a much easier setting which can be exploited for bootstrapping PQA - U.", "entities": [[4, 5, "DatasetName", "PubMedQA"], [6, 7, "MetricName", "accuracy"], [8, 11, "MetricName", "macro - F1"]]}
{"text": "Majority : The majority ( about 55 % ) of the instances have the label \" yes \" . We use a trivial baseline denoted as Majority where we simply predict \" yes \" for all instances , regardless of the question and context . Shallow Features : For each instance , we include the following shallow features : 1 ) TF - IDF statistics of the question 2 ) TF - IDF statistics of the context / long answer and 3 ) sum of IDF of the overlapping non - stop words between the question and the context / long answer . To allow multi - phase fine - tuning , we apply a feed - forward neural network on the shallow features instead of using a logistic classifier . BiLSTM : We simply concatenate the question and context / long answer with learnable segment embeddings appended to the biomedical word2vec embeddings ( Pyysalo et al , 2013 ) of each token . The concatenated sentence is then fed to a biL - STM , and the final hidden states of the forward and backward network are used for classifying the yes / no / maybe label . ESIM with BioELMo : Following the state - ofthe - art recurrent architecture of NLI ( Peters et al , 2018 ) , we use pre - trained biomedical contextualized embeddings BioELMo ( Jin et al , 2019 ) for word representations . Then we apply the ESIM model ( Chen et al , 2016 ) , where a biLSTM is used to encode the question and context / long answer , followed by an attentional local inference layer and a biLSTM inference composition layer . After pooling , a softmax output unit is applied for predicting the yes / no / maybe label .", "entities": [[131, 132, "MethodName", "BiLSTM"], [199, 200, "MethodName", "ESIM"], [246, 247, "MethodName", "ESIM"], [258, 259, "MethodName", "biLSTM"], [280, 281, "MethodName", "biLSTM"], [289, 290, "MethodName", "softmax"]]}
{"text": "We present PubMedQA , a novel dataset aimed at biomedical research question answering using yes / no / maybe , where complex quantitative reasoning is required to solve the task . PubMedQA has substantial automatically collected instances as well as the largest size of expert annotated yes / no / maybe questions in biomedical domain . We provide a strong baseline using multi - phase fine - tuning of BioBERT with long answer as additional supervision , but it 's still much worse than just single human performance . There are several interesting future directions to explore on PubMedQA , e.g. : ( 1 ) about 21 % of PubMedQA contexts contain no natural language descriptions of numbers , so how to properly handle these numbers is worth studying ; ( 2 ) we use binary BoW statistics prediction as a simple demonstration for additional supervision of long answers . Learning a harder but more informative auxiliary task of long answer generation might lead to further improvements . Articles of PubMedQA are biased towards clinical study - related topics ( described in Appendix B ) , so PubMedQA has the potential to assist evidence - based medicine , which seeks to make clinical decisions based on evidence of high quality clinical studies . Generally , PubMedQA can serve as a benchmark for testing scientific reasoning abilities of machine reading comprehension models . 7 Acknowledgement", "entities": [[2, 3, "DatasetName", "PubMedQA"], [11, 13, "TaskName", "question answering"], [31, 32, "DatasetName", "PubMedQA"], [98, 99, "DatasetName", "PubMedQA"], [109, 110, "DatasetName", "PubMedQA"], [160, 162, "TaskName", "answer generation"], [170, 171, "DatasetName", "PubMedQA"], [187, 188, "DatasetName", "PubMedQA"], [215, 216, "DatasetName", "PubMedQA"], [227, 230, "TaskName", "machine reading comprehension"]]}
{"text": "Clinical study - related topics are over - represented in PubMedQA : we found proportions of MeSH terms like : \" Pregnancy Outcome \" \" Socioeconomic Factors \" \" Risk Assessment \" \" Survival Analysis \" \" Prospective Studies \" \" Case - Control Studies \" \" Reference Values \" are significantly higher in the PubMedQA articles than those in 200k most recent general PubMed articles ( significance is defined by p < 0.05 in twoproportion z - test ) .", "entities": [[10, 11, "DatasetName", "PubMedQA"], [33, 35, "TaskName", "Survival Analysis"], [55, 56, "DatasetName", "PubMedQA"]]}
{"text": "Misogyny is a problem in many online spaces , making them less welcoming , safe , and accessible for women . Women have been shown to be twice as likely as men to experience gender - based online harassment ( Duggan , 2017 ) . This misogyny can inflict serious psychological harm on women and produce a ' silencing effect ' , whereby women selfcensor or withdraw from online spaces entirely , thus limiting their freedom of expression ( Mantilla , 2013 ; International , 2017 ) . Tackling such content is increasingly a priority for social media platforms and civil society organisations . However , detecting online misogyny remains a difficult task ( Hewitt et al , 2016 ; Nozza et al , 2019 ) . One problem is the lack of high - quality datasets to train machine learning models , which would enable the creation of efficient and scalable automated detection systems . Previous research has primarily used Twitter data and there is a pressing need for other platforms to be researched Lynn et al ( 2019a ) . Notably , de - spite social scientific studies that show online misogyny is pervasive on some Reddit communities , to date a training dataset for misogyny has not been created with Reddit data . In this paper we seek to address the limitations of previous research by presenting a dataset of Reddit content with expert labels for misogyny that can be used to develop more accurate and nuanced classification models . Our contributions are four - fold . First , we develop a detailed hierarchical taxonomy based on existing literature on online misogyny . Second , we create and share a detailed codebook used to train annotators to identify different types of misogyny . Third , we present a dataset of 6 , 383 entries from Reddit . Fourth , we create baseline classification models based on these datasets . All of the research artefacts are made freely available via a public repository for future researchers . 1 The dataset itself has several innovations which differentiate it from previous training datasets for misogyny . First , we use chronological and structured conversation threads , which mean annotators take into account the previous context of each entry before labelling . Second , we distinguish between conceptually distinct types of misogynistic abuse , including gendered personal attacks , use of misogynistic pejoratives , and derogatory and threatening language . Third , we highlight the specific section of text , also known as a ' span ' , on which each label is based . This helps differentiate between multiple labels on one piece of text . Fourth , we use trained annotators , rather than crowd - sourced workers . We also use facilitated meetings to decide the final labels rather than just a majority decision . Both of these factors lead to a high - quality dataset . Additionally , we provide a second dataset with the original labels made by annotators before the final labels were decided .", "entities": [[198, 199, "DatasetName", "Reddit"], [213, 214, "DatasetName", "Reddit"], [233, 234, "DatasetName", "Reddit"], [308, 309, "DatasetName", "Reddit"]]}
{"text": "We collected conversation threads from Reddit . Given that a very small amount of content on social media is hateful , a key difficulty when creating datasets for annotation is collecting enough instances of the ' positive ' class to be useful for machine learning ( Schmidt and Wiegand , 2017 ; Fortuna and Nunes , 2018 ) . However , sampling strategies can introduce biases in the composition and focus of the datasets if overly simplistic methods are used , such as searching for explicitly misogynistic terms ( Wiegand et al , 2019 ) . To ensure that our dataset contains enough misogynistic abuse we began with targeted sampling , taking content from 12 subreddits that were identified as misogynistic in previous research . This includes subreddits such as r / MensRights , r / seduction , and r / TheRedPill . The sources used to identify these subreddits are available in Table 9 in the Appendix . We then identified 22 additional subreddits which had been recommended by the moderators / owners of the original 12 subreddits in the ' sidebar ' . Some of these are not misogynistic but discuss women ( e.g. r / AskFeminists ) and/or are otherwise related to misogyny . For example , r / exredpill is a support group for former members of the misogynistic subreddit r / TheRedPill . Table 9 in the Appendix lists the 34 targeted subreddits and the number of entries and threads for each in the dataset . Over 11 weeks , for each subreddit , we collected the entire threads of the 20 most popular posts that week . Using subreddits to target the sampling rather than keywords should ensure that more linguistic variety is captured , minimising the amount of bias as keywords such as ' slut ' are associated with more explicit and less subtle forms of abuse . Nonetheless , only sampling from suspected misogynistic communities could still lead to classifiers which only identify the forms of misogyny found in those targeted contexts Wiegand et al , 2019 ; Sap et al , 2019 ) . To account for this potential bias , and to enable greater generalisabilty , we sampled content from 71 randomly selected subreddits . They accounted for 18 % of threads and 16 % of entries in our dataset . For each randomly selected subreddit , we collected the thread of the most popular post . All threads were in English with the exception of one thread from the subreddit r / Romania . Posts and comments were collected from February to May 2020 using the python package PRAW , a wrapper for the Reddit API ( Boe , 2020 ) . Posts on Reddit have a text title and a body which can be text , an image , or a link . For posts with a text body we combined this with the post title to create a single unit of text . For the 29 % of posts where the body was an image we also collected the image .", "entities": [[5, 6, "DatasetName", "Reddit"], [445, 446, "DatasetName", "Reddit"], [455, 456, "DatasetName", "Reddit"]]}
{"text": "Content which does not contain misogynistic abuse , pejoratives , or related counter speech as defined in the previous categories . This content is often not related to abuse or to women in general . That said , it can include other forms of abusive language which are not misogynistic .", "entities": [[44, 46, "TaskName", "abusive language"]]}
{"text": "A key difficulty in the formation of abusive language training datasets is producing high quality annotations . Several factors affect this . Deciding between similar categories , such as ' hate speech ' versus ' offensive language ' can be difficult ( Waseem et al , 2017 ) . Determining the right category often requires close scrutiny and sustained critical thinking from annotators . Annotators may face information overload if asked to work with too many categories , both in terms of breadth ( e.g. annotating for different types of abuse ) and depth ( e.g. working with numerous subcategories ) . Further , annotators may have different values and experiences and so make different assessments of the content they observe , especially when context plays a large role . Annotators will also have unconscious social biases which may mean they interpret coding instructions differently to each other , and to how they were intended by the research authors . For instance , found that crowdsourced annotators were more likely to label sexist content as merely ' offensive ' while racist and homophobic content was considered ' hate speech ' . To mitigate such annotator biases , we used expert annotators specifically trained in identifying misogynistic content , as well as a group - based facilitation process to decide final labels . Due to time and resource constraints , the final dataset is smaller than if we had used crowdsourced workers but captures more nuanced and detailed cases of misogyny . Six annotators worked on the dataset . Annotators were trained in the use of a codebook detailing the taxonomy and annotation guidelines . The codebook was updated over time based on feedback from the annotators . Demographic information on the annotators is available in Appendix A.2", "entities": [[7, 9, "TaskName", "abusive language"], [30, 32, "DatasetName", "hate speech"], [187, 189, "DatasetName", "hate speech"]]}
{"text": "Annotators independently marked up each entry for the three levels presented in Section 4 . For all level two categories other than ' None ' , they also highlighted the specific part of the entry which was relevant to the labelled category ( the ' span ' ) . This is particularly important information for long posts which can contain multiple forms of abuse . Each entry was annotated by either two ( 43 % ) or three ( 57 % ) annotators . If all annotators made the exact same annotation ( including all three levels and highlighting ) this was accepted as the final annotation . All other entries were flagged as disagreements . Annotators reviewed the disagreements in weekly meetings which were overseen by an expert facilitator , a PhD researcher who had developed the annotation taxonomy and was familiar with the literature on online misogyny and hate speech classification . The role of the facilitator was to promote discussion between annotators and ensure the final labels reflected the taxonomy . Each disagreement was discussed until the annotators reached a consensus on the final agreed label or labels .", "entities": [[150, 152, "DatasetName", "hate speech"]]}
{"text": "We make use of the more granular secondary labels in our taxonomy to conduct an error analysis for the weighted BERT model . Table 7 shows the confusion matrix for the 1 , 277 entries in the test set . Overall , 137 entries ( 10.7 % ) were misclassified . 86 ( 63 % ) of these errors were false positives and 51 ( 37 % ) were false negatives .", "entities": [[20, 21, "MethodName", "BERT"]]}
{"text": "Precision Recall F1 score Accuracy In many cases , the derogation depends on the context of the earlier conversation thread , thus the BERT - model , which does not explicitly take into account prior entries in the thread , can not recognise the misogyny in isolation . \" It 's funny to see the hamster that starts to act up in their little widdle tiny brains after saying that too . \" is an explicit statement that women are intellectually inferior , but understanding that it refers to women depends on having seen previous entries in the conversation . The next most common source of false negatives is Pejoratives ( n=19 ) . The classifier misses six counts each of ' whore ' and ' stacy ' and five of ' bitch ' . There are seven missed labels for Treatment , five Disrespectful actions and two Threatening language . However , due to the low prevalence of the category in the training data we anticipate some errors . For example , \" I am waiting for such incident to happen to me so that I can beat the shit out of her , and of course it will be all revenge \" details a specific form of violence ( i.e. ' beat the shit out of her ' ) which the model can not know to identify as misogyny without being trained on other uses of the term . The final two errors are for Personal attacks . For example , \" Yeah there s women that I as an Incel would nt even acknowledge and this is one of em [ sic ] \" . This is an implicit attack which requires understanding that considering a woman unworthy of the attention of an incel is a gendered insult . As we can see from these examples the main classification errors are due to context limitations . For false negatives there is usually not enough infor - mation in the entry alone or in the training dataset to identify the misogyny . Conversely , for false positives the classifier appears to overly associate content about women with content that abuses women . These limitations can be addressed by future work drawing on the taxonomy and annotation process presented here to develop larger datasets which can cover a greater range of forms of discourse , including both non - misogynistic discussions of women and a wider variety of misogynistic speech .", "entities": [[0, 1, "MetricName", "Precision"], [1, 2, "MetricName", "Recall"], [2, 4, "MetricName", "F1 score"], [4, 5, "MetricName", "Accuracy"], [23, 24, "MethodName", "BERT"]]}
{"text": "In this paper we have presented a hierarchical granular taxonomy for misogyny and have described a dataset containing high quality , expert labels of misogynistic content from Reddit . We have also provided the detailed coding book we created and a dataset with all of the original labels . The final dataset is small compared to other annotated datasets used for classification . However it benefits from a detailed taxonomy based on the existing literature focused on just one form of online abuse - misogyny . The use of trained annotators and an adjudication process also ensures the quality of the labels . The more granular subcategories in the taxonomy may be too small to classify separately , but they provide insights into the relative frequency of different forms of misogynistic content on Reddit and enable detailed error analysis . They are also useful for other researchers aiming to create larger datasets , who can build on the taxonomic work conducted here . A Short form data statement Following the recommendation of Bender and Friedman ( 2018 ) we include the following short form data statement to summarise the main features of the datasets . Further details on the creation of the datasets are in in Sections 3 and 5 in the main paper .", "entities": [[27, 28, "DatasetName", "Reddit"], [133, 134, "DatasetName", "Reddit"]]}
{"text": "The two datasets include labels for 6 , 383 unique Reddit entries ( i.e. posts or comments ) across 672 conversation threads collected . One dataset is of the 15 , 816 original labels selected by annotators and the second is of the 6 , 567 agreed labels . Table 8 provides a description of each of the variables in the datasets . We also include the accompanying set of images associated with some original post entries . All threads except one are in English . The majority of threads were sampled from a set of 34 subreddits selected for the expected prevalence of misogynistic content , or non - misogynistic discussions about women . Paid annotators received extensive training to apply the taxonomy presented in this paper to label entries . The majority of annotators were White - British , spoke English as a first language , and had or were pursuing a University degree . Two - thirds of annotators were women .", "entities": [[10, 11, "DatasetName", "Reddit"]]}
{"text": "Logistic regression with l1 - regularisation is implemented in R using the ' glmnet ' package ( Friedman et al , 2010 ) on a unigram representation of the data . Lambda is selected using cross - validation and set to 0.015 .", "entities": [[0, 2, "MethodName", "Logistic regression"]]}
{"text": "Model Architecture We implement uncased BERT - base models ( Devlin et al , 2019 ) using the transformers Python library ( Wolf et al , 2020 ) . For sequence classification , we add a linear layer with softmax output .", "entities": [[5, 6, "MethodName", "BERT"], [36, 38, "MethodName", "linear layer"], [39, 40, "MethodName", "softmax"]]}
{"text": "Variable Description entry i d A unique string assigned to every comment and post by Reddit . link i d The i d number of the original post of a thread . parent i d The i d number of parent entry ( i.e. the post or comment this entry responds to ) . subreddit The subreddit community where the entry was made . author The Reddit username of the entry author . body The text body of the entry . For the original posts of threads the title and post body were combined . image Whether the entry has an accompanying image . Only applicable to posts . Images are provided as jpg files . They are named as ' X Y Z ' corresponding to the week ( X ) , group ( Y ) , and thread i d ( Z ) . label date The week commencing date of when the entry was labelled . week The week in the annotation process when the entry as assigned ( 1 to 11 ) . group The weekly group the entry was assigned to . All weeks had two groups except week 7 which only had 1 . sheet order The order of the entry in the weekly annotation sheet . This is a list of numbers referring to the nested structure of comments in threads . It shows the i d number of each level of the thread from the original post to the relevant entry . For example , if an entry has the sheet order ( 1 , 2 , 3 ) it belongs to the first thread ( 1 ) , and replied to the second comment ( 2 ) , to which it is the third reply ( 3 ) . See Fig . 3", "entities": [[15, 16, "DatasetName", "Reddit"], [66, 67, "DatasetName", "Reddit"]]}
{"text": "Discourse parsers recognize the intentional and inferential relationships that organize extended texts . They have had a great influence on a variety of NLP tasks as well as theoretical studies in linguistics and cognitive science . However it is often difficult to achieve good results from current discourse models , largely due to the difficulty of the task , particularly recognizing implicit discourse relations . Recent developments in transformer - based models have shown great promise on these analyses , but challenges still remain . We present a position paper which provides a systematic analysis of the state of the art discourse parsers . We aim to examine the performance of current discourse parsing models via gradual domain shift : within the same corpus , on in - domain texts , and on out - of - domain texts , and discuss the differences between the transformer - based models and the previous models in predicting different types of implicit relations both interand intra - sentential . We conclude by describing several shortcomings of the existing models and a discussion of how future work should approach this problem .", "entities": [[112, 114, "TaskName", "discourse parsing"], [159, 161, "TaskName", "implicit relations"]]}
{"text": "There are various frameworks for studying inferential links between discourse segments , from local shallow relations between discourse segments in PDTB ( Rashmi Prasad , 2008 ) to hierarchical constituent structures in RST ( Carlson et al , 2003 ) or discourse graphs in Segmented Discourse Representation Theory ( SDRT ) ( Asher et al , 2003 ) and the Discourse Graphbank ( Wolf and Gibson , 2005 ) . Rhetorical Structure Theory ( RST ) ( Mann and Thompson , 1987 ) provides a hierarchical structure for analyzing text that describes relations between text spans known as elementary discourse units ( EDUs ) . The RST Discourse Treebank ( Carlson et al , 2003 ) contains 385 Wall Street Journal articles from the Penn Treebank ( Marcus et al , 1993 ) which have been split into elementary discourse units and annotated according to Rhetorical Structure Theory , where discourse relations are annotated in a tree structure across the whole document . A full list of these relations can be found in Carlson and Marcu ( 2001 ) . The Penn Discourse Treebank ( PDTB ) ( Eleni Miltsakaki , 2004 ; Rashmi Prasad , 2008 ; Prasad et al , 2018 ) , which also uses Penn Treebank Wall Street Journal articles , contains discourse relations annotated in a shallow , non - hierarchical manner . For each relation between two arguments , each argument and the discourse connective ( word or phrase that indicates the discourse relation ) are labeled . The PDTB also annotates whether a relation is explicit or non - explicit , the latter type of which has three subtypes : Implicit , AltLex , and EntRel . In this paper , we focus on implicit relations , where a connective can be inserted between the two arguments that indicates a discourse relation . These relations are considered extremely challenging for discourse parsers to automatically identify . There is a need to examine the performance of the proposed discourse parsers , their representational choices , their generalizability , and interpretability both across domains , distributions , and frameworks . One recently developed framework is the PDTB - 3 . Since its release in 2019 , several papers have evaluated the performance of implicit sense classifiers on this new corpus , which includes newly annotated intra - sentential implicit discourse relations . In addition to proposing a new evaluation framework for PDTB , Kim et al ( 2020 ) evaluate the performance of pretrained encoders for implicit sense classification on the PDTB - 2 and the PDTB - 3 . Liang et al ( 2020 ) identify locating the position of relations as a new challenge in the PDTB - 3 , due to the significantly increased number of intra - sentential implicit relations annotated . Techniques of discourse parsing range from supervised Mabona et al , 2019 ; Lin et al , 2019 ; Zhang et al , 2020 ; Kobayashi et al , 2020 ) and weakly supervised and unsupervised approaches ( Lee et al , 2020 ; Nishida and Nakayama , 2020 ; Kurfal\u0131 and\u00d6stling , 2019 ) ; recent developments such as word / contextual embeddings have improved parser performance , although not as significantly as other tasks ( Shi and Demberg , 2019 ; Chen et al , 2019 ) Yet most works have made simplifying assumptions concerning the linguistic annotations for practical purposes that affect their evaluation and generality . For instance , most shallow discourse parsers use only the argument pairs to determine the discourse sense without considering further context . Additionally , in RST parsing , standard practice involves classifying only the 18 top - level RST classes ( Hernault et al , 2010 ; Feng and Hirst , 2014 ; Morey et al , 2017 ) . Thus , all Elaboration relations are lumped together , making it a huge class . We reveal findings about these assumptions in Section 4 . Other works evaluating discourse parsers include DiscoEval ( Chen et al , 2019 ) , a test suite of evaluation tasks that test the effectiveness of different sentence encoders for discourse parsers , and an i m - proved evaluation protocol for the PDTB - 2 ( Kim et al , 2020 ) . In contrast , our work aims to analyze and evaluate existing discourse parsers via gradual domain shift . We provide a comparative genrebased analysis on distributionally shifted text data and present a qualitative analysis of the impact of the practical choices that these models make while doing discourse parsing across frameworks . 3 Where are we in discourse parsing ?", "entities": [[124, 126, "DatasetName", "Penn Treebank"], [208, 210, "DatasetName", "Penn Treebank"], [291, 293, "TaskName", "implicit relations"], [467, 469, "TaskName", "implicit relations"], [473, 475, "TaskName", "discourse parsing"], [767, 769, "TaskName", "discourse parsing"], [777, 779, "TaskName", "discourse parsing"]]}
{"text": "Data . We start by focusing on possible distributional shifts in a shallow parser 's application , by considering different linguistic types of implicit discourse relations ( inter - vs intra - sentential ) ( Liang et al , 2020 ) . To do this , we evaluate performance on the PDTB - 2 and PDTB - 3 , as well as the intrasentential relations in the PDTB - 3 specifically . We then evaluate the performance of three widely used or state - of - the - art models under gradual shift of the domain of texts , noting that users who would want to use a parser will be applying it on data that varies linguistically to different degrees from the parser 's training data ( a fixed 3 - year window of WSJ articles ) . The data we examine is : WSJ texts outside of the Penn Treebank , other news texts , and the GUM corpus ( Zeldes , 2017 ) . Note that none of these texts contain gold PDTB annotations , and only the GUM corpus contains gold RST annotations . Setup . To examine the impact of changing the linguistic distribution by introducing intra - sentential discourse relations , we run the model developed by Chen et al ( 2019 ) using the same train - test split as the authors and training / testing on discourse senses which contain 10 or more examples . To get results for the PDTB - 2 , we train and test the model on the PDTB - 2 ; to get results for the PDTB - 3 and intrasentential relations in the PDTB - 3 , we train the model on the PDTB - 3 and evaluate its performance on both of these sets . To parse plain - text documents for PDTB relations , we use the Wang and Lan ( 2015 ) parser as our end - to - end parser and the Chen et al ( 2019 ) DiscoEval parser as our implicit sense classifier . The former is needed in order to parse unlabeled text , and the latter is a more accurate BERT - based implicit sense classifier ( implicit sense classification is the most difficult PDTB parsing task ) . To evaluate these parsers , we look at quantitative as - pects of their output ( e.g. the distributions ) and qualitative aspects ( manual annotation and inspection of parser output ) . For our RST experiments , we use the state - ofthe - art ( Wang et al , 2017 ) parser . We evaluate the performance of this parser on the standard RST Discourse Treebank test set with a 90 - 10 split ( 347 training documents and 38 test documents ) . We also evaluate it on the gold labels from the GUM corpus ( but trained on the RST ) . Because GUM is annotated with 20 different discourse relations which do not precisely map to the conventional 18 types used in the Wang et al ( 2017 ) parser , we map the ones that do n't match these types or the more fine - grained relations in the following manner , following Braud et al ( 2017 ) : preparation to BACK - GROUND , justify and motivation to EXPLANA - TION , and solutionhood to TOPIC - COMMENT . For the plain - text news articles from outside of the PDTB corpus , we mirror the PDTB experiments on these documents by parsing them with the ( Wang et al , 2017 ) parser , then examining the resulting distributions and manually inspecting the parser output .", "entities": [[150, 152, "DatasetName", "Penn Treebank"], [159, 160, "DatasetName", "GUM"], [181, 182, "DatasetName", "GUM"], [362, 363, "MethodName", "BERT"], [477, 478, "DatasetName", "GUM"], [488, 489, "DatasetName", "GUM"]]}
{"text": "While inspecting the results of the annotations , we found several helpful phenomena for developing future models , including observations regarding the role of context in shallow discourse parsing and errors that current RST parsers are making .", "entities": [[27, 29, "TaskName", "discourse parsing"]]}
{"text": "For the qualitative analysis , we ask two annotators ( a faculty member and a graduate student from linguistics departments ) to provide annotations for the data , as none of the texts contain gold PDTB labels and only the GUM corpus contains gold RST labels . The annotators were trained on , and provided with , the PDTB 2.0 annotation manual ( Prasad et al , 2007 ) . In order for the annotators to annotate this corpus , discourse relations were randomly chosen from Wall Street Journal articles , other news articles , and the GUM corpus . 64 of these discourse relations were implicit , and are the only ones reported in this paper . The annotators were given the sentence ( s ) containing both arguments , with the arguments labeled , and they also had access to the article text if they ever needed to reference back to it . To assess the inter - rater agreement , we determine Cohen 's \u03ba value ( Cohen , 1960 ) . We randomly selected 25 samples from the PDTB and assigned each to the annotators . We obtained a Cohen 's \u03ba of 0.88 , which indicates almost perfect agreement .", "entities": [[40, 41, "DatasetName", "GUM"], [97, 98, "DatasetName", "GUM"]]}
{"text": "Discourse parsing for text has seen a recent surge in experimental approaches . In this work we presented a detailed analysis of the performance of the state of the art discourse parsers and analysed their weaknesses and strength . The conclusions drawn above from these experiments make it clear that discourse parsing , though it has come a long way in the past decade or so , still has a long way to go , particularly with respect to parsing on out - ofdomain texts and addressing issues of class imbalances , although the BERT - based model has made some improvements in this area . Additionally , we investigated how and when PDTB - 3 can help in improving the prediction of intra - sentential implicit relations . There are several promising future directions for the area of discourse parsing . A model that detects intra - sentential implicit relations is necessary in order to be able to parse on the PDTB - 3 . Exploring new neural parsing strategies is also a must . We observed that neural parsers are ignorant about what they do not know and overconfident when they make uninformed predictions . Quantifying prediction uncertainty directly by training the model to output high uncertainty for the data samples close to class boundaries can results in parsers that can make better decisions . One takeaway of our empirical analysis was the importance of the role of context in identifying the correct discourse relations . This observation suggests the need for new computational experiments that can identify the right context window that is required for the model to accurately predict relations . Another useful direction is designing models that can learn discourse relations on their own without the help of annotated corpora . There are several unsupervised models ( Kobayashi et al , 2019 ; Nishida and Nakayama , 2020 ) that are used for determining the structure of discourse parse trees but few that infer the relations themselves .", "entities": [[0, 2, "TaskName", "Discourse parsing"], [50, 52, "TaskName", "discourse parsing"], [94, 95, "MethodName", "BERT"], [126, 128, "TaskName", "implicit relations"], [139, 141, "TaskName", "discourse parsing"], [149, 151, "TaskName", "implicit relations"]]}
{"text": "Importance - based Neuron Allocation for Multilingual Neural Machine Translation", "entities": [[8, 10, "TaskName", "Machine Translation"]]}
{"text": "Multilingual neural machine translation with a single model has drawn much attention due to its capability to deal with multiple languages . However , the current multilingual translation paradigm often makes the model tend to preserve the general knowledge , but ignore the language - specific knowledge . Some previous works try to solve this problem by adding various kinds of language - specific modules to the model , but they suffer from the parameter explosion problem and require specialized manual design . To solve these problems , we propose to divide the model neurons into general and language - specific parts based on their importance across languages . The general part is responsible for preserving the general knowledge and participating in the translation of all the languages , while the language - specific part is responsible for preserving the languagespecific knowledge and participating in the translation of some specific languages . Experimental results on several language pairs , covering IWSLT and Europarl corpus datasets , demonstrate the effectiveness and universality of the proposed method .", "entities": [[2, 4, "TaskName", "machine translation"], [37, 39, "TaskName", "general knowledge"], [117, 119, "TaskName", "general knowledge"]]}
{"text": "In this section , we will give a brief introduction to the Transformer model ( Vaswani et al , 2017 ) and the Multilingual translation .", "entities": [[12, 13, "MethodName", "Transformer"]]}
{"text": "We denote the input sequence of symbols as x = ( x 1 , . . . , x J ) , the ground - truth sequence as y * = ( y * 1 , . . . , y * K * ) and the translation as y = ( y 1 , . . . , y K ) . Transformer is a stacked network with N identical layers containing two or three basic blocks in each layer . For a single layer in the encoder , it consists of a multi - head self - attention and a position - wise feed - forward network . For a single decoder layer , besides the above two basic blocks , a multi - head cross - attention follows multi - head selfattention . The input sequence x will be first converted to a sequence of vectors and fed into the encoder . Then the output of the N - th encoder layer will be taken as source hidden states and fed into decoder . The final output of the N - th decoder layer gives the target hidden states and translate the target sentences .", "entities": [[63, 64, "MethodName", "Transformer"]]}
{"text": "In this section , we describe the datasets using in our experiments on many - to - many and one - to - many multilingual translation scenarios . Many - to - Many For this translation scenario , we test our approach on IWSLT - 17 1 translation datasets , including English , Italian , Romanian , Dutch ( briefly , En , It , Ro , Nl ) . We experimented in eight directions , including It\u2194En , Ro\u2194En , Nl\u2194En , and It\u2194Ro , with 231.6k , 220.5k , 237.2k , and 217.5k data for each language pair . We choose test2016 and test2017 as our development and test set , respectively . Sentences of all languages were tokenized by the Moses scripts 2 and further segmented into subword symbols using Byte - Pair Encoding ( BPE ) rules ( Sennrich et al , 2016 ) with 40 K merge operations for all languages jointly . One - to - Many We evaluate the quality of our multilingual translation models using training data from the Europarl Corpus 3 , Release V7 . Our experiments focus on English to twelve primary languages : Czech , Finnish , Greek , Hungarian , Lithuanian , Latvian , Polish , Portuguese , Slovak , Slovene , Swedish , Spanish ( briefly , Cs , Fi , El , Hu , Lt , Lv , Pl , Pt , Sk , Sl , Sv , Es ) . For each language pair , we randomly sampled 0.6 M parallel sentences as training corpus ( 7.2 M in all ) . The Europarl evaluation data set dev2006 is used as our validation set , while devtest2006 is our test set . For language pairs without available development and test set , we randomly split 1 K unseen sentence pairs from the corresponding training set as the development and test data respectively . We tokenize and truecase the sentences with Moses scripts and apply a jointly - learned set of 90k BPE obtained from the merged source and target sides of the training data for all twelve language pairs .", "entities": [[138, 139, "MethodName", "BPE"], [336, 337, "MethodName", "BPE"]]}
{"text": "To make the evaluation convincing , we reimplement and compare our method with four baseline systems , which can be divided into two categories with respect to the number of models . The multiple - model approach requires maintaining a dedicated NMT model for each language : Individual A NMT model is trained for each language pair . Therefore , there are N different models for N language pairs . The unified model - based methods handle multiple languages within a single unified NMT model : Multilingual ( Johnson et al , 2017 ) Handling multiple languages in a single transformer model which contains one encoder and one decoder with a special language indicator lang added to the input sentence . + TS ( Blackwood et al , 2018 ) This method assigns language - specific attention modules to each language pair . We implement the target - specific attention mechanism because of its excellent performance in the original paper . + Adapter This method injects tiny adapter layers for specific language pairs into the original MNMT model . We set the dimension of projection layer to 128 and train the model from scratch . Our Method - AV Our model is trained just as the Approach section describes . In this system , we adopt the absolute value based method to evaluate the importance of neurons across languages . Our Method - TE This system is implemented the same as the system Our Method - AV except that we adopt the Taylor Expansion based evaluation method as shown in Equation 7 . + Expansion To make a fair comparison , we set the size of Feed Forward Network to 3000 to expand the model capacity up to the level of other baselines , and then apply our Taylor Expansion based method to this model .", "entities": [[122, 123, "MethodName", "TS"], [162, 163, "MethodName", "Adapter"]]}
{"text": "Our work closely relates to language - specific modeling for MNMT and model pruning which we will recap both here . Early MNMT studies focus on improving the sharing capability of individual bilingual models to handle multiple languages , which includes sharing encoders ( Dong et al , 2015 ) , sharing decoders ( Zoph et al , 2016 ) , and sharing sublayers ( Firat et al , 2016 ) . Later , Ha et al ( 2016 ) and Johnson et al ( 2017 ) propose an universal MNMT model with a target language token to indicate the translation direction . While this paradigm fully explores the general knowledge between languages and hard to obtain the specific knowledge of each language ( Tan et al , 2019 ; Aharoni et al , 2019 ) , the subsequent researches resort to Language - specific modeling , trying to find a better trade - off between sharing and specific . Such approaches involve inserting conditional languagespecific routing layer ( Zhang et al , 2021 ) , specific attention networks ( Blackwood et al , 2018 ; Sachan and Neubig , 2018 ) , adding task adapters , and training model with different language clusters ( Tan et al , 2019 ) , and so on . However , these methods increase the capacity of the model which makes the model bloated . Moreover , our method is also related to model pruning , which usually aims to reduce the model size or improve the inference efficiency . Model pruning has been widely investigated for both computer vision ( CV ) ( Luo et al , 2017 ) and natural language processing ( NLP ) tasks . For example , See et al ( 2016 ) examines three magnitude - based pruning schemes , Zhu and Gupta ( 2018 ) demonstrates that large - sparse models outperform comparablysized small - dense models , and Wang et al ( 2020a ) improves the utilization efficiency of parameters by introducing a rejuvenation approach . Besides , Lan et al ( 2020 ) presents two parameter reduction techniques to lower memory consumption and increase the training speed of BERT .", "entities": [[109, 111, "TaskName", "general knowledge"], [364, 365, "MethodName", "BERT"]]}
{"text": "The current standard models of multilingual neural machine translation fail to capture the characteristics of specific languages , while the latest researches focus on the pursuit of specific knowledge while increasing the capacity of the model and requiring fine manual design . To solve the problem , we propose an importance - based neuron allocation method . We divide neurons to general neurons and language - specific neurons to retain general knowledge and capture language - specific knowledge without model capacity incremental and specialized design . The experiments prove that our method can get superior translation results with better general and language - specific knowledge .", "entities": [[7, 9, "TaskName", "machine translation"], [70, 72, "TaskName", "general knowledge"]]}
{"text": "Graph Neural Networks ( GNNs ) that capture the relationships between graph nodes via message passing have been a hot research direction in the natural language processing community . In this paper , we propose Graph Topic Model ( GTM ) , a GNN based neural topic model that represents a corpus as a document relationship graph . Documents and words in the corpus become nodes in the graph and are connected based on document - word cooccurrences . By introducing the graph structure , the relationships between documents are established through their shared words and thus the topical representation of a document is enriched by aggregating information from its neighboring nodes using graph convolution . Extensive experiments on three datasets were conducted and the results demonstrate the effectiveness of the proposed approach .", "entities": [[114, 115, "MethodName", "convolution"]]}
{"text": "Probabilistic topic models ( Blei , 2012 ) are tools for discovering main themes from large corpora . The popular Latent Dirichlet Allocation ( LDA ) ( Blei et al , 2003 ) and its variants ( Lin and He , 2009 ; Zhao et al , 2010 ; Zhou et al , 2014 ) are effective in extracting coherent topics in an interpretable manner , but usually at the cost of designing sophisticated and model - specific learning algorithm . Recently , neural topic modeling that utilizes neuralnetwork - based black - box inference has been the main research direction in this field . Notably , NVDM ( Miao et al , 2016 ) employs variational autoencoder ( VAE ) ( Kingma and Welling , 2013 ) to model topic inference and document generation . Specifically , NVDM consists of an encoder inferring topics from documents and a decoder generating documents from topics , where the latent topics are constrained by a Gaussian prior . Srivastava and Sutton ( 2017 ) argued that Dirichlet distribution is a more appropriate prior for topic modeling than Gaussian in NVDM and proposed ProdLDA that approximates the Dirichlet prior with logistic normal . There are also attempts that directly enforced a Dirichlet prior on the document topics . W - LDA ( Nan et al , 2019 ) models topics in the Wasserstein autoencoders ( Tolstikhin et al , 2017 ) framework and achieves distribution matching by minimizing their Maximum Mean Discrepancy ( MMD ) ( Gretton et al , 2012 ) , while adversarial topic model ( Wang et al , 2019a ( Wang et al , , b , 2020 directly generates documents from the Dirichlet prior and such a process is adversarially trained with a discriminator under the framework of Generative Adversarial Network ( GAN ) ( Goodfellow et al , 2014 ) . Recently , due to the effectiveness of Graph Neural Networks ( GNNs ) ( Li et al , 2015 ; Kipf and Welling , 2016 ; Zhou et al , 2018 ) in embedding graph structures , there is a surge of interests of applying GNN to natural language processing tasks ( Yasunaga et al , 2017 ; Song et al , 2018 ; Yao et al , 2019 ) . For example , GraphBTM ( Zhu et al , 2018 ) is a neural topic model that incorporates the graph representation of a document to capture biterm cooccurrences in the document . To construct the graph , a sliding window over the document is employed and all word pairs in the window are connected . A limitation of GraphBTM is that only word relationships are considered while ignoring document relationships . Since a topic is possessed by a subset of documents in the corpus , we believe that the topical neighborhood of a document , i.e. , documents with similar topics , would help determine the topics of a document . To this end , we propose Graph Topic Model ( GTM ) , a neural topic model that a corpus is represented as a document relationship graph where documents and words in the corpus are nodes and they are connected based on document - word co - occurrences . In GTM , the topical representation of a document node is aggregated from its multi - hop neighborhood , including both document and word nodes , using Graph Convolutional Network ( GCN ) ( Kipf and Welling , 2016 ) . As GCN is able to capture high - order neighborhood relationships , GTM is essentially capable of modeling both word - word and doc - doc relationships . In specific , the relationships between relevant documents are established by their shared words , which is desirable for topic modeling as documents belonging to one topic typically have similar word distributions . The main contributions of the paper are : We propose GTM , a novel topic model that incorporates document relationship graph to enrich document and word representations . We extensively experimented on three datasets and the results demonstrate the effectiveness of the proposed approach . 2 Graph Topic Model", "entities": [[1, 3, "TaskName", "topic models"], [24, 25, "MethodName", "LDA"], [116, 118, "MethodName", "variational autoencoder"], [119, 120, "MethodName", "VAE"], [217, 218, "MethodName", "LDA"], [230, 231, "MethodName", "autoencoders"], [250, 251, "DatasetName", "MMD"], [300, 303, "MethodName", "Generative Adversarial Network"], [304, 305, "MethodName", "GAN"], [572, 575, "MethodName", "Graph Convolutional Network"], [576, 577, "MethodName", "GCN"], [587, 588, "MethodName", "GCN"]]}
{"text": "We have introduced Graph Topic Model , a neural topic model that incorporates corpus - level neighboring context using graph convolutions to enrich document representations and facilitate the topic inference . Both quantitative and qualitative results are presented in the experiments to demonstrate the effectiveness of the proposed approach . In the future , we would like to extend GTM to corpora with explicit doc - doc interactions , e.g. , scientific documents with citations or social media posts with user relationships . Replacing GCN in GTM with more advanced graph neural networks is another promising research direction .", "entities": [[84, 85, "MethodName", "GCN"]]}
{"text": "With the introduction of connotative valid facts , knowledge inference on knowledge graph improves the performance of many downstream applications , such as vertical search and question answering ( Dong et al , 2015 ; Lukovnikov et al , 2017 ) . Existing studies ( Nickel et al , 2016 ; Wang et al , 2017 ) mainly focus on knowledge inference on binary facts with two entities connected with a certain binary relation , represented as triples , ( head entity , relation , tail entity ) . They attempt to infer the unknown head / tail entity or the unknown relation of a given binary fact . However , n - ary facts involving more than two entities are also ubiquitous . For example , in Freebase , more than 1/3 entities participate in n - ary facts ( Wen et al , 2016 ) . The fact that John Bardeen received N obel P rize in P hysics in 1956 together with W alter Houser Brattain and W illiam Shockley 1 is a typical 5ary fact . So far , only a few studies ( Wen et al , 2016 ; Zhang et al , 2018 ; Guan et al , 2019 ) have tried to address knowledge inference on n - ary facts . In existing studies for knowledge inference on nary facts , each n - ary fact is represented as a group of peer attributes and attribute values . In practice , for each n - ary fact , there is usually a primary triple ( the main focus of the n - ary fact ) , and other attributes along with the corresponding attribute values are its auxiliary descriptions . Take the above 5 - ary fact for example , the primary triple is ( John Bardeen , award - received , N obel P rize in P hysics ) , and other attribute - value pairs including point - in - time : 1956 , together - with : W alter Houser Brattain and together - with : W illiam Shockley are its auxiliary descriptions . Actually , in YAGO ( Suchanek et al , 2007 ) and Wikidata ( Vrande\u010di\u0107 and Kr\u00f6tzsch , 2014 ) , a primary triple is identified for each n - ary fact . The above 5 - ary fact is a relatively complete example . In the real - world scenario , many n - ary facts appear as only partial ones , each consisting of a primary triple and a subset of its auxiliary description ( s ) , due to incomplete knowledge acquisition . For example , ( John Bardeen , awardreceived , N obel P rize in P hysics ) with pointin - time : 1956 and it with { together - with : W alter Houser Brattain , together - with : W illiam Shockley } are two typical partial facts corresponding to the above 5 - ary fact . For differentiation , we call those relatively complete facts as whole ones . We noticed that existing studies on n - ary facts infer an unknown element in a welldefined whole fact and have not paid attention to knowledge inference on partial facts . Later on , we refer the former as simple knowledge inference , while the latter as flexible knowledge inference . With these considerations in mind , in this paper , by discriminating the information in the same n - ary fact , we propose a neural network model , called NeuInfer , to conduct both simple and flexible knowledge inference on n - ary facts . Our specific contributions are summarized as : We treat the information in the same n - ary fact discriminatingly and represent each n - ary fact as a primary triple coupled with a set of its auxiliary descriptive attribute - value pair ( s ) . We propose a neural network model , NeuInfer , for knowledge inference on n - ary facts . NeuInfer can particularly handle the new type of task , flexible knowledge inference , which infers an unknown element in a partial fact consisting of a primary triple and any number of its auxiliary description ( s ) . Experimental results validate the significant effectiveness and superiority of NeuInfer . 2 Related Works", "entities": [[26, 28, "TaskName", "question answering"], [357, 358, "DatasetName", "YAGO"]]}
{"text": "They can be divided into tensor / matrix based methods , translation based methods , and neural network based ones . The quintessential one of tensor / matrix based methods is RESCAL ( Nickel et al , 2011 ) . It relates a knowledge graph to a three - way tensor of head entities , relations , and tail entities . The learned embeddings of entities and relations via minimizing the reconstruction error of the tensor are used to reconstruct the tensor . And binary facts corresponding to entries of large values are treated as valid . Similarly , ComplEx ( Trouillon et al , 2016 ) relates each relation to a matrix of head and tail entities , which is decomposed and learned like RESCAL . To improve the embeddings and thus the performance of inference , researchers further introduce the constraints of entities and relations ( Ding et al , 2018 ; Jain et al , 2018 ) . Translation based methods date back to TransE ( Bordes et al , 2013 ) . It views each valid binary fact as the translation from the head entity to the tail entity via their relation . Thus , the score function indicating the validity of the fact is defined based on the similarity between the translation result and the tail entity . Then , a flurry of methods spring up ( Wang et al , 2014 ; Lin et al , 2015b ; Ji et al , 2015 ; Guo et al , 2015 ; Lin et al , 2015a ; Xiao et al , 2016 ; Jia et al , 2016 ; Tay et al , 2017 ; Ebisu and Ichise , 2018 ; Chen et al , 2019 ) . They modify the above translation assumption or introduce additional information and constraints . Among them , TransH ( Wang et al , 2014 ) translates on relationspecific hyperplanes . Entities are projected into the hyperplanes of relations before translating . Neural network based methods model the validity of binary facts or the inference processes . For example , ConvKB ( Nguyen et al , 2018 ) treats each binary fact as a three - column matrix . This matrix is fed into a convolution layer , followed by a concatenation layer and a fully - connected layer to generate a validity score . Nathani et al ( 2019 ) further proposes a generalized graph attention model as the encoder to capture neighborhood features and applies ConvKB as the decoder . ConvE ( Dettmers et al , 2018 ) models entity inference process via 2D convolution over the reshaped then concatenated embedding of the known entity and relation . ConvR ( Jiang et al , 2019 ) further adaptively constructs convolution filters from relation embedding and applies these filters across entity embedding to generate convolutional features . SENN ( Guan et al , 2018 ) models the inference processes of head entities , tail entities , and relations via fullyconnected neural networks , and integrates them into a unified framework .", "entities": [[31, 32, "MethodName", "RESCAL"], [125, 126, "MethodName", "RESCAL"], [161, 162, "TaskName", "Translation"], [167, 168, "MethodName", "TransE"], [376, 377, "MethodName", "convolution"], [437, 438, "MethodName", "convolution"], [462, 463, "MethodName", "convolution"]]}
{"text": "This component estimates the validity of ( h , r , t ) , including the acquisition of its interaction vector and the assessment of its validity , corresponding to \" hrt - FCNs \" and \" FCN 1 \" in Figure 1 , respectively . Detailedly , the embeddings of h , r , and t are concatenated and fed into a fully - connected neural network . After layer - by - layer learning , the last layer outputs the interaction vector o hrt of ( h , r , t ) : o hrt = f ( f ( f ( f ( [ h ; r ; t ] W 1 , 1 + b 1 , 1 ) W 1 , 2 + b 1 , 2 ) ) W 1 , n 1 + b 1 , n 1 ) , ( 1 ) where f ( ) is the ReLU function ; n 1 is the number of the neural network layers ; { W 1 , 1 , W 1 , 2 , . . . , W 1 , n 1 } and { b 1 , 1 , b 1 , 2 , . . . , b 1 , n 1 } are their weight matrices and bias vectors , respectively . With o hrt as the input , the validity score val hrt of ( h , r , t ) is computed via a fully - connected layer and then the sigmoid operation : val hrt = \u03c3 ( o hrt W val + b val ) , ( 2 ) where W val and b val are the weight matrix and bias variable , respectively ; \u03c3 ( x ) = 1 1+e \u2212x is the sigmoid function , which constrains val hrt ( 0 , 1 ) . For simplicity , the number of hidden nodes in each fully - connected layer of \" hrt - FCNs \" and \" FCN 1 \" gradually reduces with the same difference between layers .", "entities": [[37, 38, "MethodName", "FCN"], [156, 157, "MethodName", "ReLU"], [308, 309, "DatasetName", "0"], [335, 336, "MethodName", "FCN"]]}
{"text": "This component estimates the compatibility of F ct . It contains three sub - processes , i.e. , the capture of the interaction vector between ( h , r , t ) and each auxiliary description a i : v i ( i = 1 , 2 , . . . , m ) , the acquisition of the overall interaction vector , and the assessment of the compatibility of F ct , corresponding to \" hrtav - FCNs \" , \" min \" and \" FCN 2 \" in Figure 1 , respectively . Similar to \" hrt - FCNs \" , we obtain the interaction vector o hrta i v i of ( h , r , t ) and a i : v i : o hrta i v i = f ( f ( f ( f ( [ h ; r ; t ; a i ; v i ] W 2 , 1 + b 2 , 1 ) W 2 , 2 + b 2 , 2 ) ) W 2 , n 2 + b 2 , n 2 ) , ( 3 ) where n 2 is the number of the neural network layers ; { W 2 , 1 , W 2 , 2 , . . . , W 2 , n 2 } and { b 2 , 1 , b 2 , 2 , . . . , b 2 , n 2 } are their weight matrices and bias vectors , respectively . The number of hidden nodes in each fully - connected layer also gradually reduces with the same difference between layers . And the dimension of the resulting o hrta i v i is d. All the auxiliary descriptions share the same parameters in this sub - process . The overall interaction vector o hrtav of F ct is generated based on o hrta i v i . Before introducing this sub - process , let us see the principle behind first . Straightforwardly , if F ct is valid , ( h , r , t ) should be compatible with any of its auxiliary description . Then , the values of their interaction vector , measuring the compatibility in many different views , are all encouraged to be large . Therefore , for each dimension , the minimum over it of all the interaction vectors is not allowed to be too small . Thus , the overall interaction vector o hrtav of ( h , r , t ) and its auxiliary description ( s ) is : o hrtav = min m i=1 ( o hrta i v i ) , ( 4 ) where min ( ) is the element - wise minimizing function . Then , similar to \" FCN 1 \" , we obtain the compatibility score comp F ct of F ct : comp F ct = \u03c3 ( o hrtav W comp + b comp ) , ( 5 ) where W comp of dimension d \u00d7 1 and b comp are the weight matrix and bias variable , respectively .", "entities": [[86, 87, "MethodName", "FCN"], [470, 471, "MethodName", "FCN"]]}
{"text": "Simple knowledge inference includes simple entity inference and simple relation inference . For an nary fact , they infer one of the entities / the relation in Method JF17 K WikiPeople MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10 the primary triple or the attribute value / attribute in an auxiliary description , given its other information .", "entities": [[31, 32, "MetricName", "MRR"], [32, 33, "MetricName", "Hits@1"], [33, 34, "MetricName", "Hits@3"], [34, 35, "MetricName", "Hits@10"], [35, 36, "MetricName", "MRR"], [36, 37, "MetricName", "Hits@1"], [37, 38, "MetricName", "Hits@3"], [38, 39, "MetricName", "Hits@10"]]}
{"text": "Knowledge inference methods on n - ary facts are scarce . The representative methods are m - TransH ( Wen et al , 2016 ) and its modified version RAE ( Zhang et al , 2018 ) , and the state - of - the - art one is NaLP ( Guan et al , 2019 ) . As m - TransH is worse than RAE , following NaLP , we do not adopt it as a baseline .", "entities": [[29, 30, "MethodName", "RAE"], [65, 66, "MethodName", "RAE"]]}
{"text": "Since RAE is deliberately developed only for simple entity inference , we compare NeuInfer only with NaLP on simple relation inference .", "entities": [[1, 2, "MethodName", "RAE"]]}
{"text": "Transformer based Natural Language Generation for Question - Answering", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "This paper explores Natural Language Generation within the context of Question - Answering task . The several works addressing this task only focused on generating a short answer or a long text span that contains the answer , while reasoning over a Web page or processing structured data . Such answers ' length are usually not appropriate as the answer tend to be perceived as too brief or too long to be read out loud by an intelligent assistant . In this work , we aim at generating a concise answer for a given question using an unsupervised approach that does not require annotated data . Tested over English and French datasets , the proposed approach shows very promising results . 25 50 rank A1 / Bert / Cmbert - base A2 / Bert / flaubert - small A1 / Bert / xlm - roberta - base A1 / Bert / flaubert - base - unc A1 / Bert / xlm - mlm - enfr - 1024 A1 / Bert / xlm - roberta - large A2 / Bert / gpt2 A1 / Bert / bert - base - mlg - unc A1 / Bert / bert - base - mlg A2 / Bert / xlm - clm - enfr - 1024 A1 / Bert / xlm - clm - enfr - 1024 A2 / Bert / xlm - mlm - enfr - 1024 A2 / Bert / flaubert - large A2 / Bert / xlm - roberta - base A1 / Bert / flaubert - large A1 / Bert / flaubert - small A1 / Bert / openai - gpt A2 / Bert / Cmbert - base A2 / Bert / flaubert - base A2 / Bert / flaubert - base - unc A2 / Bert / xlm - roberta - large A1 / Bert / gpt2 A2 / Bert / bert - base - mlg - unc A2 / Bert / gpt2 - medium A1 / Bert / gpt2 - large A2 / Bert / bert - base - mlg A1 / Bert / gpt2 - medium", "entities": [[142, 143, "MethodName", "xlm"], [160, 161, "MethodName", "xlm"], [162, 163, "DatasetName", "mlm"], [171, 172, "MethodName", "xlm"], [205, 206, "MethodName", "xlm"], [216, 217, "MethodName", "xlm"], [227, 228, "MethodName", "xlm"], [229, 230, "DatasetName", "mlm"], [245, 246, "MethodName", "xlm"], [270, 271, "MethodName", "gpt"], [298, 299, "MethodName", "xlm"]]}
{"text": "Albert Einstein is a German - born theoretical physicist who developed the theory of relativity , one of the two pillars of modern physics . Given the specificity of QAS which extract answers from structured data , users generally receive only a short and limited answer to their questions as illustrated by the example above . This type of answer representation might not meet the user expectations . Indeed , the type of answer given by the first system can be perceived as too brief not recalling the context of the question . The second system returns a passage which contains information that are out of the question 's scope and might be deemed by the user as irrelevant . It is within this framework that we propose in this article an approach which allows to generate a concise answer in natural language ( e.g. The thesis superviser of Albert Einstein was Alfred Kleiner ) that shows very promising results tested over French and English questions . This approach is a component of a QAS that we proposed in Rojas Barahona et al ( 2019 ) and that we will briefly present in this article . In what follows , we detail in section 3 the approach we propose for answer generation in Natural Language and we briefly discuss the QAS developed . We present in section 4 the experiments that we have conducted to evaluate this approach .", "entities": [[210, 212, "TaskName", "answer generation"]]}
{"text": "The huge amount of information available nowadays makes the task of retrieving relevant informa - tion complex and time consuming . This complexity has prompted the development of QAS which help spare the user the search and the information filtering tasks , as it is often the case with search engines , and directly return the exact answer to a question asked in natural language . The QAS cover mainly three tasks : question analysis , information retrieval and answer extraction ( Lopez et al , 2011 ) . These tasks have been tackled in different ways , considering the knowledge bases used , the types of questions addressed ( Iida et al , 2019 ; Zayaraz et al , 2015 ; Dwivedi and Singh , 2013 ; Lopez et al , 2011 ) and the way in which the answer is presented . In this article , we particularly focus on the answer generation process . We generally notice two forms of representation addressed in literature . The answer can take the form of a paragraph selected from a set of text passages retrieved from the web ( Asai et al , 2018 ; Du and Cardie , 2018 ; Wang and Jiang , 2016 ; Wang et al , 2017 ; Oh et al , 2016 ) , as it can also be the exact answer to the question extracted from a knowledge base ( Wu et al , 2003 ; Bhaskar et al , 2013 ; Le et al , 2016 ) . Despite the abundance of work in the field of QAS , the answers generation issue has received little attention . A first approach indirectly addressing this task has been proposed in Brill et al ( 2001Brill et al ( , 2002 . Indeed , the authors aimed at diversifying the possible answer patterns by permuting the question 's words in order to maximise the number of retrieved documents that may contain the answer to the given question . Another answer representation approach based on rephrasing rules has also been proposed in Agichtein and Gravano ( 2000 ) ; Lawrence and Giles ( 1998 ) within the context of query expansion task for document retrieval and not purposely for the question - answering task . The few works that have considered this task within the QAS framework have approached it from a text summary generation perspective ( Ishida et al , 2018 ; Iida et al , 2019 ; Rush et al , 2015 ; Chopra et al , 2016 ; Nallapati et al , 2016 ; Miao and Blunsom , 2016 ; See et al , 2017 ; Oh et al , 2016 ; Sharp et al , 2016 ; . These works consist in generating a summary of a single or various text spans that contain the answer to a question . Most of these works have only considered causality questions like the ones starting with \" why \" and whose answers are para - graphs . To make these answers more concise , the extracted paragraphs are summed up . Other approaches ( Kruengkrai et al , 2017 ; Girju , 2003 ; Verberne et al , 2011 ; Oh et al , 2013 ) have explored this task as a classification problem that consists in predicting whether a text passage can be considered as an answer to a given question . It should be noted that these approaches only intend to diversify as much as possible the answer representation patterns to a given question in order to increase the probability of extracting the correct answer from the Web and do not focus on the answer 's representation itself . It should also be noted that these approaches are only applicable for QAS which extract answers as a text snippet and can not be applied to short answers usually extracted from knowledge bases . The work presented in Pal et al ( 2019 ) tried to tackle this issue by proposing a supervised approach that was trained on a small dataset whose questions / answers pairs were extracted from machine comprehension datasets and augmented manually which make generalization and capturing variation very limited . Our answer generation approach differs from these works as it is unsupervised , can be adapted to any type of factual question ( except for why ) and is based only on easily accessible and unannotated data . Indeed , we build upon the intuitive hypothesis that a concise answer and easily pronounced by an intelligent assistant can in fact consist of a reformulation of the question asked . This approach is a part of a QAS that we have developed in Rojas Barahona et al ( 2019 ) that extracts the answer to a question from structured data . In what follows , we detail in section 3 the approach we propose for answer generation in Natural Language and we briefly discuss the QAS developed . We present in section 4 the experiments that we have conducted to evaluate this approach . and we conclude in section 5 with the limitations noted and the perspectives considered .", "entities": [[76, 78, "TaskName", "information retrieval"], [153, 155, "TaskName", "answer generation"], [703, 705, "TaskName", "answer generation"], [816, 818, "TaskName", "answer generation"]]}
{"text": "The answer generation approach proposed is a component of a system which was developed in Rojas Barahona et al ( 2019 ) and which consists in a spoken conversational question - answering system which analyses and translates a question in natural language ( French or English ) in a formal representation that is transformed into a Sparql query 1 . The Sparql query helps extracting the answer to the given question from an RDF knowledge base , in our case Wikidata 2 . The extracted answer takes the form of a list of URIs or values . Although the QAS that we have developed ( Rojas Barahona et al , 2019 ) is able to find the correct answer to a question , we have noticed that its short representation is not user - friendly . Therefore , we propose an unsupervised approach which integrates the use of Transformer models such as BERT ( Devlin et al , 2019 ) and GPT ( Radford et al , 2018 ) . The choice of an unsupervised approach arises from the fact that there is no available training dataset associating a question with an exhaustive and concise answer at the same time . such dataset could have helped use an End - to - End learning neural architecture that can generate an elaborated answer to a question . This approach builds upon the fact that we have already extracted the short answer to a given question and assumes that a user - friendly answer can consist in rephrasing the question words along with the short answer . This approach is composed of two fundamental phases : The dependency analysis of the input question and the answer generation using Transformer models .", "entities": [[1, 3, "TaskName", "answer generation"], [148, 149, "MethodName", "Transformer"], [152, 153, "MethodName", "BERT"], [161, 162, "MethodName", "GPT"], [283, 285, "TaskName", "answer generation"], [286, 287, "MethodName", "Transformer"]]}
{"text": "During this phase , we first carry out a first test of the set Q to check whether the text fragment which contains a question marker ( exp : what , when , who etc . ) represents the subject nsubj in the analysed question . If so , we simply replace that text fragment with the answer we identified earlier . Let us take the previous example What is the political party of the mayor of Paris ? , the system automatically detects that the text fragment containing the question marker What represents the subject and will therefore be replaced directly by the exact answer The Socialist Party . Therefore , the concise answer generated will be The Socialist Party is the political party of the mayor of Paris . Otherwise , we remove the text fragment containing the question marker that we detected and we add the short answer R to Q : Q = { c 1 , c 2 , . . . , c n\u22121 , R } Using the text fragments set Q , we proceed with a permutation based generation of all possible answer structures that can form the sentence answering the question asked : S = { s 1 ( R , c 1 , c 2 , . . . , c n\u22121 ) , s 2 ( c 1 , R , c 2 , . . . , c n\u22121 ) , . . . , s m ( c 1 , c 2 , . . . , c n\u22121 , R ) } These structures will be evaluated by a Language Model ( LM ) based on Transformer models which will extract the most probable sequence of text fragments that can account for the answer to be sent to the user : structure * = s S ; p ( s ) = argmax s i S p ( s i ) Once the best structure is identified , we initiate the generation process of possible missing words . Indeed , we suppose that there could be some terms which do not necessarily appear in the question or in the short answer but which are , on the other hand , necessary to the generation of a correct grammatical structure of the final answer . This process requires that we set two parameters , the number of possible missing words and their positions within the selected structure . In this paper , we experiment the assumption that one word could be missing and that it is located before the short answer within the identified structure , as it could be the case for a missing article ( the , a , etc . ) or a preposition ( in , at , etc . ) for example . Therefore , to predict this missing word , we use BERT as the generation model ( GM ) for its ability to capture bidirectionally the context of a given word within a sentence . In case when BERT returns a non - alphabetic character sequence , we assume that the optimal structure , as predicted by the LM , does not need to be completed by an additional word . The following example illustrates the different steps of the proposed approach : Question : When did princess Diana die ? 1 . Question parsing and answer extraction using the system proposed in Rojas Barahona et al ( 2019 ) : short answer = { August 31 , 1997 }", "entities": [[280, 281, "MethodName", "Transformer"], [481, 482, "MethodName", "BERT"], [508, 509, "MethodName", "BERT"]]}
{"text": "Long - form answers , consisting of multiple sentences , can provide nuanced and comprehensive answers to a broader set of questions . To better understand this complex and understudied task , we study the functional structure of long - form answers collected from three datasets , ELI5 ( Fan et al , 2019 ) , We - bGPT ( Nakano et al , 2021 ) and Natural Questions ( Kwiatkowski et al , 2019 ) . Our main goal is to understand how humans organize information to craft complex answers . We develop an ontology of six sentence - level functional roles for long - form answers , and annotate 3.9k sentences in 640 answer paragraphs . Different answer collection methods manifest in different discourse structures . We further analyze model - generated answers - finding that annotators agree less with each other when annotating model - generated answers compared to annotating human - written answers . Our annotated data enables training a strong classifier that can be used for automatic analysis . We hope our work can inspire future research on discourselevel modeling and evaluation of long - form QA systems . 1", "entities": [[47, 48, "DatasetName", "ELI5"], [67, 69, "DatasetName", "Natural Questions"], [95, 96, "MethodName", "ontology"]]}
{"text": "While many information seeking questions can be answered by a short text span , requiring a short span answer significantly limits the types of questions that can be addressed as well as the extent of information that can be conveyed . Recent work ( Fan et al , 2019 ; Krishna et al , 2021 ; Nakano et al , 2021 ) explored long - form answers , where answers are free - form texts consisting of multiple sentences . Such long - form answers provide flexible space where the answerer can provide a nuanced answer , incorporating their confidence and sources of their knowledge . Thus the answer sentences form a discourse where the answerers provide information , hedge , explain , provide examples , point to other sources , and more ; these elements need to be structured and organized coherently . We take a linguistically informed approach to understand the structure of long - form answers , designing six communicative functions of sentences in long - form answers ( which we call roles ) . 2 Our framework combines functional structures with the notion of information salience by designating a role for sentences that convey the main message of an answer . Other roles include signaling the organization of the answer , directly answering the question , giving an example , providing background information , and so on . About a half of the sentences in long - form answers we study serve roles other than providing an answer to the question . We collect discourse annotations on three long - form question answering ( LFQA ) datasets , ELI5 ( Fan et al , 2019 ) , WebGPT ( Nakano et al , 2021 ) and Natural Questions ( NQ ) ( Kwiatkowski et al , 2019 ) . Figure 1 contains an example annotation on each dataset . While all three contain paragraph - length answers needed for complex queries , they are collected in distinct mannersanswers in ELI5 are written by Reddit users ; answers in WebGPT are written by annotators who searched documents on a web interface and heavily quoted those documents to form an answer , and answers in NQ are pre - existing paragraphs from Wikipedia corpus . We collect three - way annotations for 3.9k sentences ( \u223c700 question - answer pairs across three datasets ) . We also annotate a small number of model - generated answers from a recent long - form question answering ( LFQA ) system ( Krishna et al , 2021 ) and provide rich analysis of their discourse structure . In all three datasets , we observe appearance of most proposed functional roles , but with different proportions . Answers in ELI5 contains more examples and elaborations , while answers extracted WebGPT : How much money is needed in order to not have to work for the rest of your life ? I worked on a window - washing robot that cleaned acres of rooftops over a huge commercial greenhouse . Worked great , except when it did n't , and would either break down completely or just get lost and start climbing the wrong parts of the structure . Then repair techs and manual window washers still have to be employed . I think this ends up being a cost / benefit problem where the reliability of our robots and price of implementation is n't quite at the point where it m a k e s t h i s c o m m e r c i a l l y v i a b l e f o r skyscrapers . For what it 's worth , I think the Twin Towers actually used a washer robot on the upper floors to limited success . To determine how much money you need to never have to work again for the rest of your life , some calculation is needed to arrive at a dollar number tailored to you [ 2 ] . You need to consider the amount you spend yearly , the effect of inflation on your savings , and the income you need from an investment portfolio to keep ahead of inflation . [ 1 ] [ 3 ] A reliable savings range is your current yearly spending multiplied by 28 to 36 , with more security and comfort the higher the number is . from Wikipedia passages ( NQ ) contain more auxiliary information . Analyzing a subset of ELI5 and WebGPT , we also identify a big gap in lexical overlap between long - form answer and evidence passages across all functional roles . Lastly , we found that human agreement of the discourse roles of model - generated answers are much lower than human - written ones , reflecting the difficulty for humans to process model - generated answers . With the data collected , we present a competitive role classifier , which performs on par with human when trained with our annotated data and can be used for automatic discourse analysis . We further envision using functional roles for controllable long - form generations , concise answer generation , and improved evaluation metrics for LFQA .", "entities": [[265, 267, "TaskName", "question answering"], [272, 273, "DatasetName", "ELI5"], [290, 292, "DatasetName", "Natural Questions"], [293, 294, "DatasetName", "NQ"], [333, 334, "DatasetName", "ELI5"], [337, 338, "DatasetName", "Reddit"], [367, 368, "DatasetName", "NQ"], [414, 416, "TaskName", "question answering"], [457, 458, "DatasetName", "ELI5"], [740, 741, "DatasetName", "NQ"], [751, 752, "DatasetName", "ELI5"], [861, 863, "TaskName", "answer generation"]]}
{"text": "We study the discourse structure of long - form answers based on functional roles of sentences in the paragraph . Functional structures characterize the communicative role a linguistic unit plays ; as such , they vary across genres as the goals of communication also vary . In scientific or technical articles , these roles can be background , method , findings ( Kircz , 1991 ; Liddy , 1991 ; Mizuta et al , 2006 ) , while in news , they can be main event or anecdotes ( Van Dijk , 2013 ; Choubey et al , 2020 ) . These structures are related to , though distinct from , coherence discourse structures ( Hobbs , 1985 ) . The latter characterizes how each unit ( e.g. , adjacent clauses or sentences ) relates to others through semantic relations such as temporal , causal , etc . ; such structures can be trees that hierarchically relate adjacent units ( Mann and Thompson , 1988 ) or graphs ( Lascarides and Asher , 2008 ) . In contrast , functional roles describe how information is organized to serve the communication goal , in our case , providing the answer . We developed our ontology by examining longform answers in online community forums ( subreddit Explain Like I 'm Five ( ELI5 ) ) and Wikipedia passages , hence answers derived from different domains ( e.g. , textbooks ) can contain roles beyond our ontology . We describe our six sentence - level discourse roles for long - form answers here : Answer - Summary ( Sum ) , Answer ( Ans ) . An answer sentence directly addresses the question . Here we distinguish between the the main content of the answer ( henceforth answer summary ) vs. sentences which explain or elaborate on the summary . The summaries play a more salient role than non - summary answer sentences , and can often suffice by themselves as the answer to the question . This is akin to argumentation structure that hierarchically arranges main claims and supporting arguments ( Peldszus and Stede , 2013 ) , and news structure that differentiates between main vs. supporting events ( Van Dijk , 2013 ) . Organizational sentences ( Org . ) Rather than conveying information of the answer , the major role of an organizational sentence is to inform the reader how the answer will be structured . We found two main types of such sentences ; the first signals an upcoming set of items of parallel importance : [ A ] : There are a few reasons candidates with \" no chance \" to win keep running . 1 ) They enjoy campaigning [ ... ] The other type indicates that part of the answer is upcoming amidst an established flow ; in the example below , the answerer used a hypophora : Examples ( Ex . ) Often people provide examples in answers ; these are linguistically distinct from other answer sentences in the sense that they are more specific towards a particular entity , concept , or situation . This pattern of language specificity can also be found in example - related discourse relations ( Louis and Nenkova , 2011 ; Li and Nenkova , 2015 ) , or through entity instantiation ( MacKinlay and Markert , 2011 ) : [ Q ] : What is it about electricity that kills you ? [ A ] : [ ... ] For example , static electricity consists of tens of thousands of volts , but basically no amps . [ ... ] We found that examples in human answers are often not signaled explicitly , and often contain hypothetical situations : [ Q ] : Were major news outlets established with political bias or was it formed over time ? [ A ] : [ ... ] This is impossible due to the problem of \" anchoring . \" Consider a world where people on the right want the tax rate to be 1 % lower and people on the left want the tax rate to be 1 % higher [ ... ] Auxiliary information ( Aux . ) These sentences provide information that are related to what is discussed in the answer , but not asked in the question . It could be background knowledge that the answerer deemed necessary or helpful , e.g. , or related content that extends the question , e.g. , [ Q ] : what is the difference between mandi and kabsa ? [ A ] : [ ... ] A popular way of preparing meat is called mandi . [ ... ] Another way of preparing and serving meat for kabsa is mathbi , where seasoned meat is grilled on flat stones that are placed on top of burning embers . Notably , the removal of auxiliary information would still leave the answer itself intact .", "entities": [[203, 204, "MethodName", "ontology"], [221, 222, "DatasetName", "ELI5"], [244, 245, "MethodName", "ontology"]]}
{"text": "We observe various roles that , although less frequent , show up consistently in human answers . We group them into a miscellaneous role and list them below . ( a ) Some sentences specify the limitation of the answer by narrowing down the scope of the answer to an open - ended question . [ Q ] : Why are there such drastic differences in salaries between different countries ? ( b ) Some sentences state where the answer came from and thus put the answer into context . [ Q ] : Why Does a thermostat require the user to switch between heat and cool modes , as opposed to just setting the desired temperature ? [ A ] : The person who installed my heat pump ( which has all three modes ) explained this to me . [ ... ] ( c ) Some sentences point to other resources that might contain the answers . As our ontology does not provide an exhaustive list of the functional roles , we instructed our annotators to annotate other roles not covered by our ontology as Miscellaneous as well .", "entities": [[161, 162, "MethodName", "ontology"], [185, 186, "MethodName", "ontology"], [187, 188, "TaskName", "Miscellaneous"]]}
{"text": "We randomly sample examples from three LFQA datasets and filter answers with more than 15 sentences and those with less than 3 sentences . 3 We briefly describe each dataset below . 4 ELI5 / ELI5 - model ELI5 consists of QA pairs where the questions and answers are retrieved from the subreddit r / explainlikeimfive . The answers in ELI5 are of varying quality and style . While the original dataset consists of ( question , answer ) pairs , recent benchmark ( Petroni et al , 2021 ) annotated a subset of examples with relevant Wikipedia paragraphs , which we used for analysis in Section 4 . In addition to answers in the original datasets , we annotate a small number of model - generated answers from Krishna et al ( 2021 ) ( we refer this set as ELI5 - model ) , a state - ofthe art LFQA system on ELI5 . WebGPT Nakano et al ( 2021 ) presented a new LFQA dataset and model ; with the goal of building a model that can search and navigate the web to compose a long - form answer . While they reuse questions from ELI5 , they newly collect answers from trained human annotators who were instructed to first search for related documents using a search engine and then construct the answers with reference to those documents . The collected data ( denoted as \" human demonstration \" consisting of question , answer , a set of evidence documents , and mapping from the answer to the evidence document ) are used to finetune GPT - 3 ( Brown et al , 2020 ) to generate long - form answers . Natural Questions ( NQ ) NQ contains questions from Google search queries , which is paired with a relevant Wikipedia article and an answer in the article if the article answers the question . They annotate paragraph - level answer as well as short span answer inside the paragraph answer if it exists . In open retrieval QA , researchers filtered questions with paragraph level answers for its 3 We used Stanza ( Qi et al , 2020 ) to split long - form answers into sentences . This process removes 42 % , 28 % and 34 % from ELI5 , WebGPT and NQ respectively . 4 Our data is sourced from the validation split of ELI5 from the KILT ( Petroni et al , 2021 ) benchmark , the testing portion from WebGPT ( their samples are publicly hosted at https://openaipublic.blob.core.windows . net / webgpt - answer - viewer / index.html , which answers questions from the ELI5 test set ) , and the validation split from Natural Questions . difficulty of evaluation and only look at questions with short span answer . We create a filtered set of NQ that focuses on paragraph - level answers containing complex queries . 5 While many NQ questions can be answered with a short entity ( e.g. , how many episodes in season 2 breaking bad ? ) , many others questions require paragraph length answer ( e.g. , what does the word china mean in chinese ? ) . This provides a complementary view compared to the other two datasets , as the answers are not written specifically for the questions but harvested from pre - written Wikipedia paragraphs . Thus , this simulates scenarios where model retrieves paragraphs instead of generating them .", "entities": [[33, 34, "DatasetName", "ELI5"], [35, 36, "DatasetName", "ELI5"], [38, 39, "DatasetName", "ELI5"], [60, 61, "DatasetName", "ELI5"], [141, 142, "DatasetName", "ELI5"], [154, 155, "DatasetName", "ELI5"], [198, 199, "DatasetName", "ELI5"], [268, 269, "MethodName", "GPT"], [285, 287, "DatasetName", "Natural Questions"], [288, 289, "DatasetName", "NQ"], [290, 291, "DatasetName", "NQ"], [294, 295, "DatasetName", "Google"], [385, 386, "DatasetName", "ELI5"], [389, 390, "DatasetName", "NQ"], [402, 403, "DatasetName", "ELI5"], [405, 406, "DatasetName", "KILT"], [444, 445, "DatasetName", "ELI5"], [454, 456, "DatasetName", "Natural Questions"], [476, 477, "DatasetName", "NQ"], [491, 492, "DatasetName", "NQ"]]}
{"text": "With our annotated data , we study the differences between the three types of long - form answers , namely answers provided by users in online community ( ELI5 ) , answers written by trained annotators through web search ( WebGPT ) , and answers identified in Wikipedia passages ( NQ ) . Q / A Validity Table 2 summarizes the portion of valid answers in the three datasets and the distribution of invalid reasons . NQ has the highest rate of invalid answer ( 15 % ) . Upon manual inspection , we find that passages from Wikipedia written independently of the question often only partially address complex questions . This demonstrates the limitation of a fully extractive approach . Around 10 % of the answers from ELI5 reject presupposition in the question , which is a common phenomena in information - seeking questions . WebGPT boasts the lowest invalid rate , showing the high quality of their collected answers .", "entities": [[28, 29, "DatasetName", "ELI5"], [50, 51, "DatasetName", "NQ"], [76, 77, "DatasetName", "NQ"], [128, 129, "DatasetName", "ELI5"]]}
{"text": "We study the distribution of roles in three datasets ( Table 3 ) . NQ shows the highest proportion of auxiliary information , as the paragraphs are written independent of the questions . In contrast , ELI5 contains more answer sentences and examples which provide explanation . Both ELI5 and WebGPT contain organizational sentences , demonstrating that it is commonly used when answerers assemble answers that cover more than one aspects . In all datasets , around half of the sentences serve roles other than directly answering the questions , such as providing auxiliary information or giving an example , which reflects the wide spectrum of information presented in a long - form answer . sentences . This is partially because both datasets are more extractive and less personal , without sentences which serve the role of various kinds of communication from answerers to question askers ( e.g. expressing sentiments , pointing to other resources ) that are commonly seen in online community forum . Discourse Structure Figure 3 presents the distribution of each role per its relative location in the answer . Despite the significant differences in the proportion of different discourse roles , the positioning of the roles is similar across the datasets . Answer summary and organizational sentences typically locate at the beginning of the paragraph , examples and answers often in the middle , with an increasing portion of auxiliary information towards the end . The sentences belonging to miscellaneous role frequently position at the beginning or the end of the paragraph , instead of intervening in the middle . WebGPT contains a higher portion of auxiliary information locating at the beginning of the passage , followed by the answer summary sentences . Answer Extractiveness One important aspect for long - form answer is whether the answer can be attributed to an external evidence document . While answers from NQ are directly extracted from Wikipedia passages , both ELI5 and WebGPT are written specifically for the question . To help with verification , both datasets provide evidence documents paired with the answer , and yet there are design differences between the two . Answerer ( annotators ) of WebGPT were instructed to answer the question based on the evidence documents returned by a search engine , while answers from ELI5 were written first independently and later paired with relevant Wikipedia passages ( Petroni et al , 2021 ) . We found that such difference leads to different level of extractiveness of the answer , by calculating sentence - level lexical overlap ( after removing stopwords ) with the evidence document . Overall , WebGPT answers exhibit more lexical overlap ( unigram : 0.64 , bigram : 0.36 ) with evidence document than ELI5 answers ( unigram : 0.09 , bigram : 0.01 ) . Answer sentences with different roles also exhibit different levels of extractiveness ( detailed role - level overlap can be found in Table 8 in the appendix ) . For ELI5 answers , sentences belonging to answer and summary roles have the highest overlap while example , auxiliary information and miscellaneous sentences are less grounded to external sources . For WebGPT , organizational sentences are the least extractive among all the roles .", "entities": [[14, 15, "DatasetName", "NQ"], [36, 37, "DatasetName", "ELI5"], [48, 49, "DatasetName", "ELI5"], [313, 314, "DatasetName", "NQ"], [322, 323, "DatasetName", "ELI5"], [383, 384, "DatasetName", "ELI5"], [456, 457, "DatasetName", "ELI5"], [497, 498, "DatasetName", "ELI5"]]}
{"text": "Table 4 reports the results on ELI5 test set . 11 All models outperform the majority and summarylead baselines . The sequential prediction model ( T5 ) significantly outperform classification model ( RoBERTa ) which makes a prediction per sentence . The roles with lower human agreement ( auxiliary , organizational sentence , answer ) also exhibit low model performances , reflecting the subjectivity and ambiguity of roles for some sentences . Overall , with a moderate amount of in - domain annotated data , our best model ( T5 - large ) can reliably classify functional roles of sentences in the long - form answers , showing comparable performances to human lower bound . Table 5 reports the results on the three out - ofdomain datasets , WebGPT , NQ and ELI5 - model ( model - generated answers ) . Human agreement numbers are comparable across all datasets ( 0.53 - 0.59 for lower bound , 0.73 - 0.78 for upper bound ) . While T5 - large still exhibits the best overall performance , all learned models perform worse , partially as the role distribution has changed . Despite trained on the ELI5 dataset , role classification model also perform worse on model - generated answers ( ELI5model ) , echoing our observation that human annotators find it challenging to process the discourse structure of model - generated answers . Our pilot showed that training with in - domain data improved the performances consistently , but the evaluation is on a small subset ( after setting apart some for training ) , so we do not report it here . We anticipate that automatic role classification is feasible given moderate amount of annotation for all three humanwritten long - form answer datasets we study .", "entities": [[6, 7, "DatasetName", "ELI5"], [25, 26, "MethodName", "T5"], [32, 33, "MethodName", "RoBERTa"], [89, 90, "MethodName", "T5"], [130, 131, "DatasetName", "NQ"], [132, 133, "DatasetName", "ELI5"], [167, 168, "MethodName", "T5"], [195, 196, "DatasetName", "ELI5"]]}
{"text": "Discourse structure . Our work is closely related to functional structures defined through content types explored in other domains ; prior work has affirmed the usefulness of these structures in downstream NLP tasks . In news , Choubey et al ( 2020 ) adopted Van Dijk ( 2013 ) 's content schema cataloging events ( e.g. , main event , anecdotal ) , which they showed to improve the performance of event coreference resolution . In scientific writing , content types ( e.g. , background , methodology ) are shown to be useful for summarization ( Teufel and Moens , 2002 ; Cohan et al , 2018 ) , information extraction ( Mizuta et al , 2006 ; Liakata et al , 2012 ) , and information retrieval ( Kircz , 1991 ; Liddy , 1991 ) . The discourse structure of argumentative texts ( e.g. , support , rebuttal ) ( Peldszus and Stede , 2013 ; Becker et al , 2016 ; Stab and Gurevych , 2017 ) has also been applied on argumentation min - ing . To the best of our knowledge , no prior work has studied the discourse structure of long - form answers . Question Answering . Recent work ( Cao and Wang , 2021 ) have investigated the ontology of questions , which includes comparison questions , verification questions , judgement questions , etc . We construct the ontology of functional roles of answer sentences . One of the roles in our ontology is summary , yielding an extractive summarization dataset . This shares motivation with a line of work studying query - focused summarization ( Xu and Lapata , 2020 ) . Concurrent to our work , Su et al ( 2022 ) studies improving faithfulness of long - form answer through predicting and focusing on salient information in retrieved evidence document . Lastly , our work build up on three datasets containing longform answers ( Kwiatkowski et al , 2019 ; Fan et al , 2019 ; Nakano et al , 2021 ) and extends the analysis of long - form answers from earlier studies ( Krishna et al , 2021 ) .", "entities": [[71, 74, "TaskName", "event coreference resolution"], [94, 95, "TaskName", "summarization"], [126, 128, "TaskName", "information retrieval"], [201, 203, "TaskName", "Question Answering"], [216, 217, "MethodName", "ontology"], [236, 237, "MethodName", "ontology"], [250, 251, "MethodName", "ontology"], [256, 258, "TaskName", "extractive summarization"], [272, 273, "TaskName", "summarization"]]}
{"text": "We annotate existing , publicly available long - form question answering datasets which might contain incorrect and outdated information and societal biases . We collected annotations through crowdsourcing platform and also by recruiting undergraduate annotators at our educational institution . We paid a reasonable hourly wage ( $ 13 / hour ) to annotators and documented our data collection process with datasheet ( Gebru et al , 2021 ) . We include studies on the extractiveness of long - form answers ( how much content can be grounded to evidence document ) through a coarse measure of lexical overlap . This is connected to faithfulness and reducing hallucination of QA system . Our study is limited to English sources , and we hope future work can address analysis in other languages . I think one of the biggest ones is that your spouse becomes your legal ' next of kin ' , meaning you can make medical decisions for them , own their property after they die , etc .", "entities": [[9, 11, "TaskName", "question answering"]]}
{"text": "We use pytorch - transformers Wolf et al ( 2019 ) to implement our models . The hyperparameters are manually searched by the authors . Table 9 : Different reasons for invalid question answer pairs for ELI5 - model and annotator agreement . We report both Fleiss kappa and pairwise agreement after reannotation . For reference , we also report agreement for human - written answers annotated . Question : Do animals know they 're going to die ? Role I read an article about this once , I ca n't find it now , but I remember reading about a dog that had been put into a room with a vacuum cleaner , and it did n't notice it was sucking in air , it just started sucking in air as normal .", "entities": [[36, 37, "DatasetName", "ELI5"]]}
{"text": "It was pretty amazing to watch . Disagreed So it was just sucking in air . Example Then , the dog got out of the room and began running around the house , running into things and being hurt . Example It eventually just died of exhaustion . Example So , no , they do n't know . Answer But it is interesting to think about . Miscellaneous It might have just been a part of their routine , or it might have been a learned behavior , or it might have been something they did because it was the only way they could do it , and they figured it out , and it was just a part of their routine , and they thought it was cool .", "entities": [[67, 68, "TaskName", "Miscellaneous"]]}
{"text": "Sentiment Lexicon Construction with Representation Learning Based on Hierarchical Sentiment Supervision", "entities": [[4, 6, "TaskName", "Representation Learning"]]}
{"text": "Sentiment lexicon is an important tool for identifying the sentiment polarity of words and texts . How to automatically construct sentiment lexicons has become a research topic in the field of sentiment analysis and opinion mining . Recently there were some attempts to employ representation learning algorithms to construct a sentiment lexicon with sentiment - aware word embedding . However , these methods were normally trained under documentlevel sentiment supervision . In this paper , we develop a neural architecture to train a sentiment - aware word embedding by integrating the sentiment supervision at both document and word levels , to enhance the quality of word embedding as well as the sentiment lexicon . Experiments on the SemEval 2013 - 2016 datasets indicate that the sentiment lexicon generated by our approach achieves the state - of - the - art performance in both supervised and unsupervised sentiment classification , in comparison with several strong sentiment lexicon construction methods .", "entities": [[31, 33, "TaskName", "sentiment analysis"], [34, 36, "TaskName", "opinion mining"], [44, 46, "TaskName", "representation learning"], [117, 119, "DatasetName", "SemEval 2013"]]}
{"text": "Sentiment lexicon is a set of words ( or phrases ) each of which is assigned with a sentiment polarity score . Sentiment lexicon plays an important role in many practical sentiment analysis and opinion mining tasks . There were some manually annotated universal sentiment lexicons such as General Inquireer ( GI ) and HowNet . However , due to the ubiquitous domain diversity and absence of domain prior knowledge , the automatic construction technique for domain - specific sentiment lex - * The corresponding author of this paper . icons has become a challenging research topic in the field of sentiment analysis and opinion mining ( Wang and Xia , 2016 ) . The early work employed unsupervised learning for sentiment lexicon construction . They normally labelled a set of seed words at first , and then learned the polarity of each candidate word , based on either word conjunction relations ( e.g. , constellation and transition in texts ) ( Hatzivassiloglou and McKeown , 1997 ) , or the word co - occurrence information ( such as pointwise mutual information , PMI ) ( Turney , 2002 ) , between the candidate word and the seed words . However , the unsupervised manner showed limited effect in sentiment prediction , and the performance greatly depends on the quality of the seed words . To fully exploit the sentiment labeling information in texts , a series of supervised learning methods was further proposed to learn the sentiment lexicons . For example , Mohammad et al ( 2013 ) proposed to construct sentiment lexicons by calculating PMI between the word and the distantly supervised sentiment labels ( such as emoticons ) in tweets and the word 's sentiment orientation ( SO ) . The resulting lexicons obtained the best results in SemEval 2013 . More advanced representation learning models were also utilized , with the aim to construct the sentiment lexicons with efficient word embeddings ( Tang et al , 2014a ; Hamilton et al , 2016 ; Vo and Zhang , 2016 ) . The traditional representation learning framework such as Word2Vec only captures the syntactic information in the texts , but ignores the sentiment relations between words . Therefore , some researchers attempted to add sentiment supervision into the network structure , in order to train a sentimentaware word embedding . For example , Tang et al ( 2014a ) exploited a dedicated neural architecture to integrate document - level sentiment supervision and the syntactic knowledge for representation learning . The sentiment - aware word embedding is then used to construct a sentiment lexicon . Vo and Zhang ( 2016 ) proposed to learn a two - dimensional sentiment representation based on a simple neural network . The sentiment lexicons generated by their approach obtained better performance to predict the tweet sentiment labels , in comparison with the PMI - based method ( Mohammad et al , 2013 ) . Although these supervised learning methods can to some extent exploit the sentiment labeling information in the texts and can learn a sentiment - aware word embedding , the manner of using document - level sentiment supervision suffers from some complex linguistic phenomena such as negation , transition and comparative degree , and hence unable to capture the fine - grained sentiment information in the text . For example , in the following tweet \" Four more fake people added me . Is this why people do n't like Twitter ? : ( \" , the document - level sentiment label is negative , but there is a positive word \" like \" in the text . In representation learning , the embeddings of words are summed up to represent the document , and the word \" like \" will be falsely associated with the negative sentiment label . Such linguistic phenomena occur frequently in review texts , and makes sentiment - aware word representation learning less effective . To address this problem , in this paper , we propose a new representation learning framework called HSSWE , to learn sentiment - aware word embeddings based on hierarchical sentiment supervision . In HSSWE , the learning algorithm is supervised under both document - level sentiment labels and word - level sentiment annotations ( e.g. , labeling \" like \" as a positive word ) . By leveraging the sentiment supervision at both document and word level , our approach can avoid the sentiment learning flaws caused by coarse - grained document - level supervision by incorporating finegrained word - level supervision , and improve the quality of sentiment - aware word embedding . Finally , following Tang et al ( 2014a ) , a simple classifier was constructed to obtain the domainspecific sentiment lexicon by using word embeddings as inputs . The main contributions of this work are as follows : 1 . To the best of our knowledge , this is the first work that learns the sentiment - aware word representation under supervision at both document and word levels . 2 . Our approach supports several kinds of wordlevel sentiment annotations such as 1 ) predefined sentiment lexicon ; 2 ) PMI - SO lexicon with hard sentiment annotation ; 3 ) PMI - SO lexicon with soft sentiment annotation . By using PMI - SO dictionary as word - level sentiment annotation , our approach is totally corpus - based , without any external resource . 3 . Our approach obtains the state - of - the - art performance in comparison with several strong sentiment lexicon construction methods , on the benchmark SemEval 2013 - 2016 datasets for twitter sentiment classification .", "entities": [[31, 33, "TaskName", "sentiment analysis"], [34, 36, "TaskName", "opinion mining"], [48, 49, "DatasetName", "General"], [101, 103, "TaskName", "sentiment analysis"], [104, 106, "TaskName", "opinion mining"], [301, 303, "DatasetName", "SemEval 2013"], [306, 308, "TaskName", "representation learning"], [323, 325, "TaskName", "word embeddings"], [347, 349, "TaskName", "representation learning"], [419, 421, "TaskName", "representation learning"], [609, 611, "TaskName", "representation learning"], [655, 657, "TaskName", "representation learning"], [673, 675, "TaskName", "representation learning"], [684, 686, "TaskName", "word embeddings"], [797, 799, "TaskName", "word embeddings"], [935, 937, "DatasetName", "the benchmark"], [937, 939, "DatasetName", "SemEval 2013"]]}
{"text": "In general , sentiment lexicons construction can be classified into two categories , dictionary - based methods and corpus - based methods . Dictionary - based methods generally integrate predefined resources , such as WordNet , to construct sentiment lexicons . Hu and Liu ( 2004 ) exploited WordNet for sentiment lexicon construction . They first labelled two sets of seed words by polarities , then extended the sets by adding the synonyms for each word to the same set and antonyms to the other . For a given new word , Kim and Hovy ( 2004 ) introduced a Naive Bayes model to predict the polarities with .the synonym set obtained from WordNet as features . Kamps et al ( 2004 ) investigated a graph - theoretic model of WordNet 's synonymy relation and measured the sentiment orientation by distance between each candidate word and the seed words with different polarities . Heerschop et al ( 2011 ) proposed a method to propagate the sentiment of seed set words through semantic relations of WordNet . Corpus - based approaches originate from the latent relation hypothesis : \" Pairs of words that cooccur in similar patterns tend to have similar semantic and sentiment relations \" ( Turney , 2008 ) . The primary corpus - based method made the use of PMI . Turney ( 2002 ) built a sentiment lexicon by calculating PMI between the candidate word and seed words . The difference of the PMI score between positive and negative seed words is finally used as the sentiment orientation ( SO ) of each candidate word ( Turney , 2002 ) . Many variants of PMI were proposed afterwards , for example , positive pointwise mutual information ( PPMI ) , second order co - occurrence PMI ( SOC - PMI ) , etc . Hamilton et al ( 2016 ) proposed to build a sentiment lexicon by a propagation method . The key of this method is to build a lexical graph by calculating the PPMI between words . Instead of calculating the PMI between words , Mohammad et al ( 2013 ) proposed to use emoticons as distant supervision and calculate the PMI between words and the distant class labels , and obtained sound performance for tweet sentiment classification . The latest corpus - based approaches normally utilize the up - to - date machine learning models ( e.g. neural networks ) to first learn a sentimentaware distributed representation of words , based on which the sentiment lexicon is then constructed . There were many word representation learning methods such as NNLM ( Bengio et al , 2003 ) and Word2Vec ( Mikolov et al , 2013 ) . However , they mainly consider the syntactic relation of words in the context but ignore the sentiment information . Some work were later proposed to deal with this problem by incorporating the sentiment information during representation learning . For example , Tang et al ( 2014a ) adapted a variant of skip - gram model , which can learn the sentiment information based on distant supervision . Furthermore , Tang et al ( 2014b ) proposed a new neural network approach called SSWE to train sentimentaware word representation . Vo and Zhang ( 2016 ) exploited a simple and fast neural network to train a 2 - dimensional representation . Each dimension is explicitly associated with a sentiment polarity . The sentiment - aware word representation in these methods was normally trained based on only document - level sentiment supervision . In contrast , the learning algorithm in our approach is supervised under both document - level and wordlevel sentiment supervision .", "entities": [[290, 291, "DatasetName", "PPMI"], [300, 301, "DatasetName", "SOC"], [338, 339, "DatasetName", "PPMI"], [430, 432, "TaskName", "representation learning"], [488, 490, "TaskName", "representation learning"]]}
{"text": "Our approach is comprised of three base modules : ( 1 ) Word - level sentiment learning and annotation ; ( 2 ) Sentiment - aware word embedding learning ; ( 3 ) Sentiment lexicon construction . Our approach depends on document - level sentiment labels . The tweet corpus provides a cheap way to get document - level sentiment annotation , owing to the distant sentiment supervision . But it should be noted that our approach is feasible for any corpus provided with document - level sentiment labels ( not merely tweets ) . The first module of our method aims to learn the pseudo sentiment distribution for each word and use it as word - level sentiment annotations to supervise word embedding learning . In the second module , we learn the sentimentaware embeddings for each word in corpus , based on hierarchical sentiment supervision . In the last module , we construct a sentiment lexicon by using the sentiment - aware word embeddings as the basis .", "entities": [[164, 166, "TaskName", "word embeddings"]]}
{"text": "In this part , we follow the method proposed by Tang et al ( 2014a ) to build a classifier to convert the sentiment - aware word representation learned in Section 3.2 to a sentiment lexicon . The word representation is the input of the classifier and word sentiment polarity is the output . Firstly , we utilize the embedding of 125 positive and 109 negative seed words manually labelled by Tang et al ( 2014a ) Thirdly , a traditional logistic regression classifier is trained by using the embeddings of extended sentiment words as the inputs . The sentiment score of a word is the difference between its positive and negative probabilities . Finally , the sentiment lexicon can be collected by using the classifier to predict the other words ' sentiment score .", "entities": [[81, 83, "MethodName", "logistic regression"]]}
{"text": "In this paper , we proposed to construct sentiment lexicons based on a sentiment - aware word representation learning approach . In contrast to traditional methods normally learned based on only the document - level sentiment supervision . We proposed word representation learning via hierarchical sentiment supervision , i.e. , under the supervi - sion at both word and document levels . The wordlevel supervision can be provided based on either predefined sentiment lexicons or the learned PMI - SO based sentiment annotation of words . A wide range of experiments were conducted on several benchmark sentiment classification datasets . The results indicate that our method is quite effective for sentiment - aware word representation , and the sentiment lexicon generated by our approach beats the state - of - the - art sentiment lexicon construction approaches .", "entities": [[17, 19, "TaskName", "representation learning"], [41, 43, "TaskName", "representation learning"]]}
{"text": "Multi - Domain Named Entity Recognition with Genre - Aware and Agnostic Inference", "entities": [[3, 6, "TaskName", "Named Entity Recognition"]]}
{"text": "Accurately identifying named entities and their type in texts is a key processing step for many NLP applications . Named entity recognition ( NER ) is an important component in several tasks including named entity linking ( Cucerzan , 2007 ) , co - reference resolution ( Ng and Cardie , 2002 ) , question answering ( Krishnamurthy and Mitchell , 2015 ) , relation extraction ( Culotta and Sorensen , 2004 ) and usually sits upstream of analytics such as sentiment ( Pang and Lee , 2004 ) or stance ( Mohammad et al , 2016 ) . Building robust NER models to accurately tag and adapt to heterogeneous types of text is thus paramount . Recent research focused on improving the overall performance of NER models on specific data sets . Yet NER models show relatively high variance even when trained on the same data ( Reimers and Gurevych , 2017 ) and poorly generalize when tested on data from different genres 1 , especially if these contain entity mentions unseen in the test data ( Augenstein et al , 2017 ; Agarwal et al , 2020 ) . Despite this , research on NER models robust to different types of input is usually limited to the standard domain adaptation scenario : a single source domain rich in training data and a single target domain with limited or no training data ( Lin and Lu , 2018 ) . We argue that this is an over - simplified experimental setup that is not typical for how NER models are used in real - world applications . Ideally , NER models use all available data , regardless of genre , and perform inference on data from any genre , even if this was not encountered in training . In this scenario , simply pooling all the available data is likely sub - optimal as genre - specific differences in named entity mentions are useful to model . Conversely , models limited to only data from the same genre as the test set are likely to underperform , as using more data is usually beneficial . This work introduces three experimental setups for the NER task where models are trained on data from multiple genres and evaluated as follows : a ) Multi - Domain - evaluation is performed across multiple genres , all seen in training . b ) Multi - Domain with Unknown Domain Labels - evaluation is carried out across multiple genres , all seen in training , but the genre label for each document is unknown at inference time . c ) Zero - shot Domain - evaluation is performed on documents from genres unseen in training . We propose a neural architecture for NER tailored to these three experimental setups , based on the popular BiLSTM - CRF architecture ( Lample et al , 2016 ) . We augment the base architecture to learn both domain - specific and independent features through shared and private domain components including projections and CRFs . Further , we add a multi - task learning objective for domain prediction to guide this separation . This model can perform inference on a text without knowledge of its corresponding domain label by using the shared components . We compare this model with several competitive methods that use a similar base architecture while holding the embeddings constant ( i.e. GloVe embeddings ) . These include models trained on data from each domain independently , models that pool all data and models that use domain identities as features through to source - target domain adaptation methods . Extensive results on all three experimental setups on a collection of data from a total of twelve genres demonstrate that our proposed architecture outperforms all others by a respectable margin . Finally , through an error analysis of our results , we aim to understand the contributions of each proposed component and the margins for future improvements .", "entities": [[19, 22, "TaskName", "Named entity recognition"], [23, 24, "TaskName", "NER"], [34, 36, "TaskName", "entity linking"], [54, 56, "TaskName", "question answering"], [64, 66, "TaskName", "relation extraction"], [101, 102, "TaskName", "NER"], [126, 127, "TaskName", "NER"], [134, 135, "TaskName", "NER"], [196, 197, "TaskName", "NER"], [210, 212, "TaskName", "domain adaptation"], [258, 259, "TaskName", "NER"], [270, 271, "TaskName", "NER"], [364, 365, "TaskName", "NER"], [458, 459, "TaskName", "NER"], [470, 471, "MethodName", "BiLSTM"], [472, 473, "MethodName", "CRF"], [512, 516, "TaskName", "multi - task learning"], [567, 569, "MethodName", "GloVe embeddings"], [600, 602, "TaskName", "domain adaptation"]]}
{"text": "Setups for Domain Adaptation Domain adaptation , formulated as learning a single model for the same task across multiple domains , is a wellstudied research area in NLP ( Chelba and Acero , 2004 ; Florian et al , 2004 ; Blitzer et al , 2006 ; Daum\u00e9 III , 2007 ) . The standard setup for domain adaptation is to maximize performance on data from a single low - resource ( target ) domain , by using data from a single high - resource ( source ) domain ( Blitzer et al , 2007 ; Peng and Dredze , 2017 ) . Extensions consider a single source and multiple different target domains ( Yang and Eisenstein , 2015 ) or multiple sources and a single target domain ( Mansour et al , 2009 ) . The multi - domain text classification task studied in ( Li and Zong , 2008 ; Wu and Huang , 2015 ; Chen and Cardie , 2018 ) is the analogous setup for the text classification task to the first experimental setup we propose for NER . Under this setup , training and evaluation is done across data from multiple domains . Multi - Domain Adaptation Methods for multidomain text classification use data fusion either at the feature or classifier level ( Li and Zong , 2008 ) , decomposing the classifier into a shared one and multiple domain - specific ones ( Wu and Huang , 2015 ) , further guided by a domain discriminator ( Chen and Cardie , 2018 ) which is also used in multi - lingual NER ( Chen et al , 2019 ) . Further , Mc - Closky et al ( 2010 ) explored sequence tagging tasks on data from unknown domains and Chen and Cardie ( 2018 ) experiment with sentiment classification on data from unknown domains , similar to our third experimental setup for NER . To the best of our knowledge , our second setup where the domain label is not available at inference time was never explicitly studied . We note that most of these approaches make use of additional unlabeled data from each domain to learn domain - specific representations . We do not use these resources in our methods , as we assume the end - user of the model is agnostic to the data used in training and wants to run inference without having to provide entire comparable corpora . Domain Adaptation for NER Models for domain adaptation in NER using neural architectures were studied recently , albeit mostly for covering the single - source and single - target setup . The INIT method trains a model using the source domain data , and its parameters are used to initialize a target model which is fine - tuned on the target data ( Mou et al , 2016 ) . The MULT method trains jointly one model for each domain with shared parameters . For sequence tagging , one CRF for each of the two domains is used to obtain the predictions ( Yang et al , 2017 ) . Adaptation can also be made at the embeddings stage ( Lin and Lu , 2018 ) or by using additional unlabeled data from the source domain and out - of - domain annotated data ( He and Sun , 2017 ) . However , as mentioned above , this assumes that unlabeled training data can be provided for each domain , which may not be realistic . The model adds layers between embeddings and the BiLSTM layers , between the BiL - STM and the CRF for the target domain and separate CRF layers , the latter two of which we adapt to our proposed architecture for multi - domain adaptation . A hierarchical Bayesian prior approach is used in ( Finkel and Manning , 2009 ) to tie feature weights across domains when information is sparse and also allow the model to take advantage if substantial data is available in one domain . Their experiments on NER focused only on three data sets : CoNLL , MUC - 6 and MUC - 7 and only the first of our three setups . A multi - task domain adaptation method for NER and word segmentation is used in ( Peng and Dredze , 2017 ) . The proposed architecture learns a shared representation across domains and experiments with linear domain projections for each domain to guide learning of shared representations . The output of these linear layers is fed to a CRF . We adopt the linear domain projection method , but extend this to also include a shared projection , followed by domain - specific CRFs and multi - task learning . Finally , another type of domain adaptation is temporal adaptation of models tested on data that is more recent than the training data , when each temporal slice can be considered as a different domain ( Rijwhani and Preo\u0163iuc - Pietro , 2020 ) .", "entities": [[2, 4, "TaskName", "Domain Adaptation"], [4, 6, "TaskName", "Domain adaptation"], [57, 59, "TaskName", "domain adaptation"], [140, 142, "TaskName", "text classification"], [170, 172, "TaskName", "text classification"], [181, 182, "TaskName", "NER"], [200, 202, "TaskName", "Domain Adaptation"], [205, 207, "TaskName", "text classification"], [267, 268, "TaskName", "NER"], [319, 320, "TaskName", "NER"], [410, 412, "TaskName", "Domain Adaptation"], [413, 414, "TaskName", "NER"], [416, 418, "TaskName", "domain adaptation"], [419, 420, "TaskName", "NER"], [499, 500, "MethodName", "CRF"], [595, 596, "MethodName", "BiLSTM"], [605, 606, "MethodName", "CRF"], [612, 613, "MethodName", "CRF"], [629, 631, "TaskName", "domain adaptation"], [677, 678, "TaskName", "NER"], [707, 709, "TaskName", "domain adaptation"], [711, 712, "TaskName", "NER"], [761, 762, "MethodName", "CRF"], [788, 792, "TaskName", "multi - task learning"], [798, 800, "TaskName", "domain adaptation"]]}
{"text": "This section describes the proposed NER architecture tailored the architecture to our multi - domain experimental setups , which is independent of input embedding representation .", "entities": [[5, 6, "TaskName", "NER"]]}
{"text": "The basic component of our NER models is an architecture which has reached state - of - the - art performance several times over the last few years ( Lample et al , 2016 ; Peters et al , 2018 ; Akbik et al , 2018 ) . Named entity recognition task is a structured prediction task and earlier statistical approaches are based models like Conditional Random Fields ( Lafferty et al , 2001 ) , which rely on features often designed based on domain - specific knowledge ( Luo et al , 2015 ) . The current dominant approach to the NER task consists of neural architectures based on recurrent neural networks with different choices of input representations Ma and Hovy , 2016 ; Lample et al , 2016 ; Peters et al , 2018 ; Akbik et al , 2018Akbik et al , , 2019 . The input consists of a concatenation of pretrained word embeddings and character embeddings . Character embeddings are trained using an LSTM from randomly initialized vectors as in ( Lample et al , 2016 ) . Word embeddings are derived from a combination GloVe ( Pennington et al , 2014 ) and FastText ( Bojanowski et al , 2017 ) pre - trained word embeddings , as used in ( Ma and Hovy , 2016 ) . The choice of embeddings is orthogonal to the architecture and thus , we hold these constant in all experiments . This representation is passed through two LSTM layers that process the input sequence in differ - . The outputs of these layers are concatenated and , in order to map the word representation obtained from the LSTM module into the label distribution , passed to a one - layer feed - forward network . A Conditional Random Field is applied to the class predictions to jointly assign the sequence tags using a transition matrix . This CRF layer improves performance of the model ( Lample et al , 2016 ) as it ensures the output sequence takes into account dependencies between the tags and also models the constraints the output sequence adheres to ( e.g. I - PER can not follow B - LOC ) .", "entities": [[5, 6, "TaskName", "NER"], [48, 51, "TaskName", "Named entity recognition"], [54, 56, "TaskName", "structured prediction"], [102, 103, "TaskName", "NER"], [156, 158, "TaskName", "word embeddings"], [168, 169, "MethodName", "LSTM"], [183, 185, "TaskName", "Word embeddings"], [190, 191, "MethodName", "GloVe"], [199, 200, "MethodName", "FastText"], [210, 212, "TaskName", "word embeddings"], [250, 251, "MethodName", "LSTM"], [280, 281, "MethodName", "LSTM"], [299, 302, "MethodName", "Conditional Random Field"], [320, 321, "MethodName", "CRF"]]}
{"text": "We use a collection of data sets spanning eight genres to evaluate our methods . In addition , in order to test the feasibility of NER tagging in a zero - shot domain setup , we present additional data covering four other genres . Each genre of documents is considered a domain in modelling .", "entities": [[25, 26, "TaskName", "NER"]]}
{"text": "The data set collection used in learning the multidomain models ( denoted as ' Open Data ' in the rest of the paper ) includes the following three data sets : CoNLL 2003 We use the data set released as part of CoNLL 2003 shared task for English ( Tjong Kim Sang and De Meulder , 2003 ) , which is arguably the most popular data set for NER and is regularly used as a benchmark for this task . This data is a collection of news articles from the Reuters Corpus . Twitter The Twitter data set consists of 22 , 000 tweets representative of multiple English - speaking locales and a variety of topics that span 11 years of Twitter posts ( 2009 ) ( 2010 ) ( 2011 ) ( 2012 ) ( 2013 ) ( 2014 ) ( 2015 ) ( 2016 ) ( 2017 ) ( 2018 ) ( 2019 ) . This data was annotated with Organizations ( ORG ) , Persons ( PER ) and Locations ( LOC ) , using the annotation guidelines used in annotating past data sets ( Tjong Kim Sang and De Meulder , 2003 ) supplemented with examples that are specific to Twitter data . OntoNotes ( six genres ) The OntoNotes data set ( Hovy et al , 2006 ) ( Augenstein et al , 2017 ) . Zero Shot Genres Finally , for zero - shot genre NER , we use a collection of internal data sets from four different genres spanning news , closed captions and other documents . All four genres were annotated with the same entity types and using similar guidelines .", "entities": [[31, 33, "DatasetName", "CoNLL 2003"], [42, 44, "DatasetName", "CoNLL 2003"], [68, 69, "TaskName", "NER"], [207, 208, "DatasetName", "OntoNotes"], [213, 214, "DatasetName", "OntoNotes"], [241, 242, "TaskName", "NER"]]}
{"text": "In order to present comparable results across all different data sets , we limit our experiments to three different types of entities that are present in all the above data sets and annotated using similar guidelines : organizations ( including geo - political entities and facilities ) , persons and locations . In case other types of entities exist in the data ( e.g. MISC for CoNLL , dates for OntoNotes ) , these are considered to be not an entity , similar to ( Augenstein et al , 2017 ) . We used the BIO tagging scheme in all our experiments , as this is arguably the most popular and differences in results between this tagging scheme and others , such as the BILOU scheme , are very small in practice ( Ratinov and Roth , 2009 ) .", "entities": [[70, 71, "DatasetName", "OntoNotes"]]}
{"text": "We train our models using the open data sets from CoNLL , Twitter and OntoNotes . The training , development and test splits of CoNLL and OntoNotes follows the standard splits . Similarly , we randomly split the Twitter data set randomly into 70 % for training , 10 % for development and 20 % for testing . The final train , dev and test sets are obtained by joining all the respective splits across the individual data sets .", "entities": [[14, 15, "DatasetName", "OntoNotes"], [26, 27, "DatasetName", "OntoNotes"]]}
{"text": "We evaluate several baseline methods and other competitive methods introduced in past research and compare to our proposed architecture ( MultDomain - SP - Aux ) described in Section 3.2 . These methods focus on different variations of the neural model architecture , while holding the input embeddings constant . InDomain trains an individual NER model using the base architecture for each of the known domains . In inference , the corresponding in - domain model is used . This allows us to establish the baseline individual domain performance when no information is shared between the domains in training . InDomain - DomainClassifier uses the same NER models as the InDomain model . The In - Domain approach is however unable to directly perform inference on sentences where the domain label is unknown at inference time . We thus build a separate domain classifier using a Bi - LSTM recurrent neural network that feeds the final hidden state into a feed - forward network to recognize the domain of a given input sentence and route it to the appropriate InDomain NER model . PoolDomain naively pools all available data , disregarding the domain information and trains a model using the base architecture . This model thus ignores the domain information when training , albeit uses all available training data . Data pooling is the standard baseline in most domain adaptation experiments . PoolDomain - Init uses all available data and uses the domain information to train models on data from one domain at once . After training on data from each domain , the model uses the weights as initialization for training on next domain . This is similar to the INIT strategy for domain adaptation used in ( Mou et al , 2016 ; . We perform this weight initialization and fine - tuning process over all the domains consecutively , where the order is defined by the density of entities , starting with the highest one . PoolDomain - GradRev trains the base architecture using a gradient reversal layer ( Ganin and Lempitsky , 2014 ) . The gradient reversal technique aims to confuse the domain discriminator while learning NER with the combination of the training data from all domains . PoolDomain+DomainFeat trains a base architecture model over all available data and , in addition to the text - based features , the domain information is explicitly represented by passing it through a domain embedding . This is appended to the word - level features that are used as input to the BiLSTM layers . The domain embeddings are randomly initialized . MultDomain - SP extends the MULT method ( Yang et al , 2017 ) to the multi - domain setup . This method uses a domain - specific CRF for each domain and a shared CRF for all domains . Both the BiLSTM and the feed - forward layers are shared across all domains . Inference can be done either through the private layer corresponding to the domain of the input - denoted as MultDomain - MultCRF ( P ) - or through the shared layer - denoted as MultDomain - MultCRF ( S ) - in which case this can be used when the domain label is unknown in inference .", "entities": [[54, 55, "TaskName", "NER"], [106, 107, "TaskName", "NER"], [148, 149, "MethodName", "LSTM"], [180, 181, "TaskName", "NER"], [228, 230, "TaskName", "domain adaptation"], [284, 286, "TaskName", "domain adaptation"], [361, 362, "TaskName", "NER"], [424, 425, "MethodName", "BiLSTM"], [462, 463, "MethodName", "CRF"], [469, 470, "MethodName", "CRF"], [476, 477, "MethodName", "BiLSTM"]]}
{"text": "In this section , we present and compare the results of all the methods introduced previously . Experiments are conducted first on the open data collection introduced in Section 4.1 in the Multi - Domain and Multi - Domain with Unknown Label setups . Following , we evaluate the performance of our model on the data used for zero - shot genre NER . The goal of these experiments is to examine the NER performance across the three proposed experimental setups which focus on model generalizability across multiple domains . We note that the results below can not be directly compared to the state - of - the - art results on each data set , as we restrict the entity types to PER , ORG , LOC , such that these types are constant across all data sets .", "entities": [[62, 63, "TaskName", "NER"], [73, 74, "TaskName", "NER"]]}
{"text": "We first focus on understanding the impact of each component added to our proposed method over the base architecture through an ablation study . Table 4 shows results using the private layer ( MultDomain - SP - Aux ( P ) ) when each of the three components are alternatively turned off : Shared - Private Linear layer , Shared - Private CRF and the domain prediction auxiliary task . Shared vs. Shared - Private CRF With the rest of the architecture fixed , the results show that the shared - private CRF performs close to the shared CRF when the shared linear layer is used ( 80.08 vs. 80.16 ; 82.04 vs. 82.74 ; all comparisons in this section are on macro - average ) . However , once we use a separate linear layer between the BiLSTM and each CRF , the difference between having the shared and the shared - private CRFs increases drastically ( 81.36 vs. 83.11 ; 82.30 vs. 84.68 ) . With only this late separation , the inputs to CRF decoders are still domain - independent features , which makes it hard for the linear CRF to adapt . When the inputs are already domain - dependent , the linear CRF can better use this information in performing the joint inference of the sequence . We note that only using shared - private CRF with the base architecture is equivalent to the MultDomain - SP method ( Yang et al , 2017 ) .", "entities": [[56, 58, "MethodName", "Linear layer"], [62, 63, "MethodName", "CRF"], [75, 76, "MethodName", "CRF"], [92, 93, "MethodName", "CRF"], [98, 99, "MethodName", "CRF"], [102, 104, "MethodName", "linear layer"], [134, 136, "MethodName", "linear layer"], [138, 139, "MethodName", "BiLSTM"], [141, 142, "MethodName", "CRF"], [176, 177, "MethodName", "CRF"], [192, 193, "MethodName", "CRF"], [207, 208, "MethodName", "CRF"], [230, 231, "MethodName", "CRF"]]}
{"text": "The results show that regardless of the other parameters , adding shared and private linear layers between the BiLSTM layers and the CRF ( s ) is always beneficial ( 80.08 vs. 81.36 ; 80.16 vs. 83.11 ; 82.04 vs. 82.30 ; 82.74 vs. 84.68 ) . The improvements are relatively larger when combined with shared and private CRF , as previously seen . Multi - Task Learning of Domain Labels Finally , we compare the impact of adding the multi - task learning objective . We find that , similar to the linear layers , adding the domain prediction task is always beneficial for the model with the increase being larger if is only a shared linear layer . We expect that the two tasks at different levels of granularity rely on shared structure in the original semantic space . The document - level domain labels can help regularize the training , providing generic information about which low - level features are valuable to entity - level recognition .", "entities": [[18, 19, "MethodName", "BiLSTM"], [22, 23, "MethodName", "CRF"], [58, 59, "MethodName", "CRF"], [64, 68, "TaskName", "Multi - Task Learning"], [80, 84, "TaskName", "multi - task learning"], [117, 119, "MethodName", "linear layer"]]}
{"text": "We further study the domains that are selected by the methods above by creating confusion matrices between the domain predictions of three setups : domain classification , domain prediction in the proposed MultDomain - SP - Aux model and the oracle in - domain choice on gold data . Figure 2 shows that the Oracle model relies on the corresponding InDomain model to only a limited extent for each model . In uniformly many cases , predictions from other in - domain models are better than the existing in - domain one , showing the variability of the NER models . The domain classifier predictions align closer to the actual domains . The MultDomain - SP - Aux model also tends to predict the domain correctly , but we see that it better learns the NW , WB and BN domains . Note noting that the MultDomain - SP - Aux model does not use these domain predictions in inference and the model uses the shared components for unknown domains or", "entities": [[98, 99, "TaskName", "NER"]]}
{"text": "Analysis of Zero - Shot Crosslingual Learning between English and Korean for Named Entity Recognition", "entities": [[12, 15, "TaskName", "Named Entity Recognition"]]}
{"text": "Crosslingual representation learning aims to derive embeddings for words ( or sentences ) from multiple languages that can be projected into a shared vector space ( Conneau et al , 2018 ; Schuster et al , 2019b ; Conneau and Lample , 2019 ) . One important application of crosslingual embeddings has been found for transferring models trained on a high - resource language to a low - resource one ( Lin et al , 2019 ; Schuster et al , 2019a ; Artetxe and Schwenk , 2019 ) . The latest multilingual transformer encoders such as BERT ( Devlin et al , 2019 ) and XLM ( Conneau et al , 2020 ) have made it possible to develop robust crosslingual models through zero - shot learning that requires no labeled training data on the target side ( Jebbara and Cimiano , 2019 ; Chidambaram et al , 2019 ; Chi et al , 2020 ) . However , these approaches tend not to work as well for languages whose words can not be easily aligned . Our team is motivated to create a rich crosslingual resource between English and Korean , which are largely different in nature as English is known to be rigid - order , morphologically - poor , and head - initial whereas Korean is flexible - order , morphologicallyrich , and head - final ( Choi et al , 1994 ; Han et al , 2002 ; Hong , 2009 ) . Creation of a high quality parallel dataset to facilitate crosslingual research can reduce the gap between these two languages , and advance NLP techniques in both languages . This paper provides a comprehensive analysis of crosslingual zero - shot learning in English and Korean . We first create a new dataset comprising a large number of parallel sentences and annotate them for named entity recognition ( NER ; Sec . 3 ) . We then adapt the crosslingual approaches and build NER models in Korean through zeroshot learning ( Sec . 4 ) . All models are experimented on our dataset and thoroughly compared to evaluate the feasibility of this work ( Sec . 5 ) . Our results are promising although depicting few challenges in zero - shot learning for English and Korean ( Sec . 6 ) . The contributions of this work can be summarized as follows : To create a crosslingual dataset that enables to develop robust zero - shot NER models in Korean . To present a new data selection scheme that can notably improve zero - shot model performance . To provide a comparative analysis among several crosslingual approaches and establish the initial foundation of this research .", "entities": [[1, 3, "TaskName", "representation learning"], [97, 98, "MethodName", "BERT"], [106, 107, "MethodName", "XLM"], [124, 128, "TaskName", "zero - shot learning"], [284, 288, "TaskName", "zero - shot learning"], [310, 313, "TaskName", "named entity recognition"], [314, 315, "TaskName", "NER"], [329, 330, "TaskName", "NER"], [374, 378, "TaskName", "zero - shot learning"], [412, 413, "TaskName", "NER"]]}
{"text": "For crosslingual representation alignment , Artetxe et al ( 2016 ) and Smith et al ( 2017 ) suggested orthogonality constraints on the embedding transformation that led to better quality translation . Aldarmaki and Diab ( 2019 ) derived a context - aware crosslingual mapping from a parallel corpus using word alignment . Schuster et al ( 2019b ) aligned word embeddings from multilingual transformer encoders using context independent embedding anchors . Recent works based on multilingual pretrained language model aligns representations between languages in a unsupervised fashion . Devlin et al ( 2019 ) proposed multilingual BERT that generates contextualized word embeddings for multiple languages in one vector space by simply sharing all languages ' vocabulary . Conneau and Lample ( 2019 ) extends mBERT by introducing bilingual data and an extra pretraining task ( Translation Language Modeling ) . Luo et al ( 2021 ) adds a crossattention module into the Transformer encoder to explicitly build the interdependence between langauges . For cross - lingual NER , Ni et al ( 2017 ) presented weakly supervised crosslingual models using annotation and representation projection . Huang et al ( 2019 ) made an empirical analysis of how sequential order and multilingual embeddings are used in crosslingual NER . Artetxe and Schwenk ( 2019 ) presented multilingual transfer models that used few - shot learning adapting supervising BEA , ranking and retraining for massive transfer . Wu and Dredze ( 2019 ) and Wu et al ( 2020 ) directly transfers the NER model trained on the source language to the target language using crosslingual representations from multilingual encoders ( Direct model transfer ) . 3 English - Korean Crosslingual Dataset 3.1 Data Collection AI Open Innovation Hub ( AI Hub ) is an integration platform operated by the Korea National Information Society Agency that provides data , software , and computing resources for AI research . It has released the Korean - English AI Training Text Corpus ( KEAT ) 1 containing 1.6 M English - Korean parallel sentences from various sources such as news media , government website / journal , law & administration , conversation and etc . For the present study , 800 K parallel sentences from the news portion of this corpus are extracted .", "entities": [[50, 52, "TaskName", "word alignment"], [60, 62, "TaskName", "word embeddings"], [97, 98, "MethodName", "BERT"], [101, 103, "TaskName", "word embeddings"], [125, 126, "MethodName", "mBERT"], [136, 137, "TaskName", "Translation"], [153, 154, "MethodName", "Transformer"], [164, 168, "TaskName", "cross - lingual NER"], [207, 208, "TaskName", "NER"], [221, 225, "TaskName", "few - shot learning"], [252, 253, "TaskName", "NER"]]}
{"text": "Since KEAT is not organized into documents , each sentence is composed independently although it comes with the URL of its original source . Thus , we group all sentences into news articles based on the URLs . Although there exist news articles with single sentence after the grouping process , we still include them in the train set in order to make full use of the parallel sentences provided , which will be used to train the word alignment model and the transformation matrix in Section 5 . As a result , 757 , 697 sentences are selected , that are composed into 381 , 173 news articles , to create our English - Korean crosslingual dataset . The news articles can be categorized into 9 sections : Business , Lifestyle , Science / Technology , Society , Sports , World , Regional , and Others . Among those , 200 articles are randomly sampled from each of the first 7 categories for our annotation in Section 3.4 and they are split into 50/50 to create the development and test sets for our experiments in Section 5 . Table 1 describes the statistics of our dataset . All sections are uniformly distributed in DEV and TST , enabling to conduct comparative studies among these sections . Table 3 : The statistics of manually annotated named entities on the parallel sentences in the DEV and TST sets . The numbers in the parentheses indicate the percentages of the corresponding tags for each set . EN / KR : # of entities in the English / Korean sentences respectively , E \u2229 K : # of entities existing in both English and Korean sentences .", "entities": [[78, 80, "TaskName", "word alignment"]]}
{"text": "ELIT 2 using the Flair model trained on OntoNotes ( Pradhan et al , 2013 ) . Korean sentences are tagged by a CRF - based model adapting KoBERT ( Korean BERT ) 3 trained on the corpus distributed by Cheon and Kim ( 2018 ) . Note that the named entity types pseudo - annotated on the Korean sentences do n't match with those of the English sentences for now , which will be matched in Section 3.4 in the case of DEV and TST . In addition , Korean sentences are processed by the Mecab morphological analyzer 4 that produces more linguistically sounding tokens than SentencePiece ( Kudo and Richardson , 2018 ) in KoBERT . All named entities from the CRF tagger are then remapped to the tokens produced by the Mecab analyzer using heuristics so they can better reflect the previous morphology work in Korean ( Hong , 2009 ) . Words in every parallel sentence pair , tokenized by the ELIT and Mecab analyzers , are aligned by GIZA++ , that has been adapted by many prior crosslingual studies ( Och and Ney , 2003 ) . Table 2 shows the statistics of pseudo - annotated named entities in our dataset . The detailed descriptions of these tags are provided in Appendix A.1 . The overall statistics are comparable between English and Korean , 2.5 and 2.3 entities per sentence , respectively . GPE e , the 3rd most frequent tag in English , is not supported by the Korean tagger but rather tagged as ORG k or LOC k , explaining why the numbers of these two tags in Korean are much greater than those of ORG e and LOC e , respectively .", "entities": [[8, 9, "DatasetName", "OntoNotes"], [23, 24, "MethodName", "CRF"], [31, 32, "MethodName", "BERT"], [107, 108, "MethodName", "SentencePiece"], [123, 124, "MethodName", "CRF"]]}
{"text": "Three crosslingual learning approaches are adapted to develop zero - shot Korean models . One is direct model transfer method following Wu and Dredze ( 2019 ) . We reproduce the previous work which finetunes mBERT on English NER dataset and transfers the trained model to a target language , in our case , Korean . We fine - tune on OntoNotes , whereas the previous work fine - tuned on CoNLL 2003 NER dataset . The other two approaches that will be experimented are embedding preojection and annotation projection following Ni et al ( 2017 ) , although some modules in the implementation are updated or added : the encoders used to derive the embeddings from sentences , the word alignment tool , the training data selection scheme heuristics . Figure 1 illustrates an overview of two crosslingual learning approaches adapted to develop zero - shot Korean NER models . One is embedding projection ( R1 ) that takes a labeled English sentence ( R2 ) and generates English embeddings , ( R3 ) which are fed into an orthogonal mapping ( R4 ) then transformed into Korean embeddings ( Section 4.2 ) . The other is annotation projection ( A1 ) that aligns words across the two languages and pseudo - annotates the Korean sentence , ( A2 ) which are fed into an encoder ( A3 ) to generate Korean embeddings ( Section 4.3 ) . The Korean embeddings generated by individual approaches are fed into a trainer to build the Korean NER models . No manual annotation is added to the Korean data ; thus they both are zero - shot learning .", "entities": [[35, 36, "MethodName", "mBERT"], [38, 39, "TaskName", "NER"], [61, 62, "DatasetName", "OntoNotes"], [71, 73, "DatasetName", "CoNLL 2003"], [73, 74, "TaskName", "NER"], [120, 122, "TaskName", "word alignment"], [148, 149, "TaskName", "NER"], [255, 256, "TaskName", "NER"], [272, 276, "TaskName", "zero - shot learning"]]}
{"text": "Let X , Y R n\u00d7d be parallel matrices between the source and target languages , where n is the number of parallel terms ( words or sentences ) in those two languages . Let x i , y i R 1\u00d7d be the i'th rows in X and Y , which are the embeddings of the i'th terms in the source and target languages respectively , that refer to the same content . Then , the transformation matrix W R d\u00d7d can be found by minimizing the distance between XW and Y as follows : argmin W XW \u2212 Y s.t . W T W = I This optimization can be achieved by singular value decomposition as proposed by Artetxe et al ( 2016 ) , where U , V R d\u00d7p , \u03a3 R p\u00d7p : W = U V T s.t . X T Y = U \u03a3V T The transformation matrix W is used to convert any English embedding e i into a Korean embedding k i in Figure 1 such that e i W = k i \u2248 k j where k j is the embedding from the Korean encoder that can be aligned with e i . The NER model is trained on only English sentences represented by the transformed embeddings k * and a pseudo - label annotated with an existing English NER model . During decoding , the model takes Korean sentences represented by the encoded embeddings k * and makes the predictions . Given the latest contextualized encoders that generate different embeddings for the same word type by contexts ( Peters et al , 2018 ; Devlin et al , 2019 ; Liu et al , 2019 ) , the size of X and Y is as large as the number of all aligned words in the training data . It is worth saying that the transformed embedding space may be similar to the actual encoded space in the target language ; however , the word order is still preserved as in the source language . Therefore , the model is limited to learn sequence information of the target language , which can be an issue for languages with very different word orderings .", "entities": [[204, 205, "TaskName", "NER"], [229, 230, "TaskName", "NER"]]}
{"text": "For embedding projection and annotation projection , two types of transformer encoders , mBERT ( Devlin et al 2019 )", "entities": [[13, 14, "MethodName", "mBERT"]]}
{"text": "Two types of transformation matrices are derived by the embedding projection method ( Section 4.2 ) . One is a word - level matrix and the other is a sentence - level matrix . To evaluate the zero - shot Korean NER model performance ( Table 6 ) when different size of parallel sentences are available , we use different subsets of sentences of increasing sizes ( 0 , 1 K , 10 K , 100 K , 200 K , 400 K , 747 K ; 0 to total # of sentences in TRN ) . Size 0 means the embeddings from source language are not transformed when fed into the NER model for training . Word embeddings from the last hidden layer of each transformer encoder are extracted . For every parallel sentence pair , let X i and Y i be lists of word embeddings of the i'th sentence extracted from the last layer in the source and target encoders , respectively . Only embeddings for words that find alignments are included in X i and Y i . If multiple words in the source language , s i and s j , are aligned to one word , t k , in the target language ( e.g. , United States \ubbf8\uad6d in Figure 1 ) , the embeddings of t k are duplicated and added to Y i and vice versa s.t . | X * | = | Y * | . Let x ij and y ij be the j'th embeddings in X i and Y i that are guaranteed to be the embeddings of aligned words ; thus , X i and Y i are completely in parallel . For the word - level transformation matrix W w , X i and Y i from all parallel sentences are appended together to create X w i and Y w i respectively such that X w i W w \u2248 Y w i . Sentence embeddings are simply created by averaging the word embeddings of parallel source and target sentences . Let X i and Y i be lists of word embeddings of the i'th sentence extracted from the last layer in the source and target encoders . Note that words in X i and Y i are not aligned , thus no duplications of word embedding unlike X i and Y i . For the sentence - level matrix W s , the average em - beddings of X i , Y i are appended to create X s i and Y s i such that X s i W s \u2248 Y s i . For each sentence in the source language , embeddings from last hidden layer are transformed by W w | s and fed into the NER model for training ( Section 5.5 ) .", "entities": [[41, 42, "TaskName", "NER"], [67, 68, "DatasetName", "0"], [87, 88, "DatasetName", "0"], [94, 95, "DatasetName", "TRN"], [98, 99, "DatasetName", "0"], [112, 113, "TaskName", "NER"], [117, 119, "TaskName", "Word embeddings"], [146, 148, "TaskName", "word embeddings"], [330, 332, "TaskName", "Sentence embeddings"], [338, 340, "TaskName", "word embeddings"], [356, 358, "TaskName", "word embeddings"], [467, 468, "TaskName", "NER"]]}
{"text": "For embedding and annotation projections , a bidirectional LSTM - based NER tagger using a CRF decoder is adapted to build our NER models ( Lample et al , 2016 ) . Details of the hyperparameters are described in Appendix A.3", "entities": [[7, 9, "MethodName", "bidirectional LSTM"], [11, 12, "TaskName", "NER"], [15, 16, "MethodName", "CRF"], [22, 23, "TaskName", "NER"]]}
{"text": "Given the results of the best models for embedding and annotation projection approaches ( Section 5.6 ) , a total of 105 parallel sentences ( 15 pairs per news section ) are randomly selected for error analysis . 7 Table 8 shows the distributions of the 5 error types . error types in both models . For example , the Korean entity \" 10\uac1c ( 10 things ) \" comprises the quantity \" 10 \" and the metric \" \uac1c ( things ) \" that is a generic measure word in Korean , whereas in English just write \" 10 \" . The grammatical difference between English and Korean , where Korean uses measure words for quantifying the classes of objects while English does not in general , makes it difficult to accurately predict under the zero - shot learning setting . Wrong Label occurs frequently across all models when dealing with entities referring to nationality . As mentioned in Section 3.5 , a single word in Korean can entail the meaning of both nationality and country . This overloaded word - sense characteristic makes entities that actually refer to nationality be mislabeled as GPE , which should have been labeled as NORP .", "entities": [[136, 140, "TaskName", "zero - shot learning"]]}
{"text": "This paper presents a multilingual dataset that allows researchers to conduct crosslingual research between English and Korean . Our dataset contains high - quality annotation of named entities on parallel sentences from seven popular news sections . Given this dataset , Korean NER models are built by zero - shot learning using multilingual encoders . Our data selection scheme for annotation projection significantly improves the NER performance although it is still suboptimal . Our error analysis depicts unique characteristics in Korean that make it hard for zero - shot learning , challenges that we need to overcome in the future work . 8", "entities": [[42, 43, "TaskName", "NER"], [47, 51, "TaskName", "zero - shot learning"], [65, 66, "TaskName", "NER"], [86, 90, "TaskName", "zero - shot learning"]]}
{"text": "There are 18 named entity tags annotated in the OntoNotes 5.0 as follows ( Pradhan et al , 2013 ) : 9 CARDINAL : Numerical terms not categorized in other categorizations . Numbers that indicate ages are included . DATE : Absolute or relative dates or periods . The period should last longer than ' TIME ' . General expressions of dates are included too such as ' few months ' , ' that day ' , ' Next season ' and ' First quarter ' . EVENT : It means an official or widely known event , war , exhibition . Official events include ministerial meetings , general elections , presidential elections , exams ( SAT ) , and prayers ( U.S. national breakfast prayer ) . Social phenomena also include ( Brexit ) for widely known events . FAC : Objectives referring to facilities include buildings , airports , highways and bridge names . GPE : An object referring to a place or location , including the name of a country and the name of an administrative district , such as a city or state . LANGUAGE : Any named language . LAW : Named documents made into laws . LOC : Refers to the name of a place or location that does not belong to GPE . It also includes expressions covering the entire location of mountains , rivers , ocean names and Europe , Asia , etc . MONEY : Monetary values including units . NORP : It refers to nationality , religious groups and political groups ( party ) . ORDINAL : All ordinal numbers such as first and second . ORG : It refers a community / group of people gathered together . For example , the name of the company , the name of the school , and the name of the sports team . 9 https://catalog.ldc.upenn.edu/docs/ LDC2013T19 / OntoNotes - Release - 5.0.pdf PERCENT : Percentage expressions with % symbol or the word ' percent ' . PERSON : Referring to a last name or full name of a particular person . It also includes nicknames for non - human creatures and characters in cartoons , dramas and movies . PRODUCT : Vehicles , Weapons , foods . IT services ( including SNS ) and medicine names are included . QUANTITY : Measurements as of weights or distances such as km , kg and etc . TIME : This tag indicates time expressions smaller than a day . This tag includes certain time indication , amount of time or any other expressions related to time . Even though an entity does not have numeral expressions but only words related to time ( for instance , ' noon ' ) , the words are tagged as ' Time ' . WORK_OF_ART : Titles of books , songs , TV programs and art pieces . Title of games , awards , theories , records are included .", "entities": [[9, 11, "DatasetName", "OntoNotes 5.0"], [58, 59, "DatasetName", "General"], [194, 195, "DatasetName", "LAW"], [316, 317, "DatasetName", "OntoNotes"]]}
{"text": "There are 10 tags annotated in the copus distributed by the Korea Maritime and Ocean University : 10 DAT : Absolute dates . Public holidays and day of the week is included . DUR : Duration of incidents . Academically clarified periods such as Cretaceous period are also included . LOC : The name of a country and the name of an administrative district , such as a city or state . Words representing certain locations such as tour spot and stadium is also included . When location word becomes compound nouns with other words , it is not included . MNY : Monetary values including units . Bitcoin is not included . NOH : Any numerical expressions such as measurements of heights , temperatures , weights . Ordinal numbers are included . ORG : A group consisting of 2 or more people . The name of the company , the name of the school , and the name of the sports team . PER : Personal name including first and last name . Any name referring to living things and nicknames for non - human creatures and characters in cartoons , dramas and movies are included . PNT : Percentage expressions with % symbol or the word ' percent ' . POH : Product name , medicine , game , event , meeting , movies , songs , drama series , TV channels , daily and weekly magazines , emails , phone numbers are included . TIM : This tag indicates time expressions smaller than a day . This tag includes certain time indication , amount of time or any other expressions related to time . Even though an entity does not have numeral expressions but only words related to time ( for instance , ' noon ' ) , the words are tagged as ' Time ' .", "entities": [[198, 199, "DatasetName", "PNT"]]}
{"text": "Table 9 shows the performance of the NER model in the ELIT toolkit on the English development and evaluation sets in our dataset .", "entities": [[7, 8, "TaskName", "NER"]]}
{"text": "Our zero - shot model yields a better performance than the existing model although it may be difficult to directly compare the two models . In the case of the existing Korean model , the low performance may be caused by the different annotation scheme between the datasets . In the case of our zero - shot model , the improvement of the performance are seen due to the coarse - grained named entities after the mapping . Table 11b describes the proportions of news sections per entity type . CARDINAL , ORDINAL , and EVENT appear the most in Sports that involves many game events and statistics . DATE , ORG , and QUANTITY show fairly even proportions in every section as they are elemental to a variety of topics . GPE , LOC , and NORP give high proportions to both Politics and World . MONEY and PERCENT appear the most in Business that often deals with monetary issues . PERSON show high proportions in Sports and Politics as discussed above . FAC takes good portions in Lifestyle , Business , and World , which often mention facilities that people encounter daily ( e.g. , airports , bridges ) . TIME appears the most in Sports and Society that are full of dynamic events and issues . LANGUAGE is mostly found in Society although the sample size is too small to generalize . LAW , PRODUCT , and WORK_OF_ART appear the most in Politics , Sci / Tech , and Lifestyle , that focus on legal issues , tech products , and entertainment ( e.g. , music , movies , shows ) , respectively .", "entities": [[235, 236, "DatasetName", "LAW"]]}
{"text": "This work was partly supported by the Institute of Information and Communications Technology Planning and Evaluation ( IITP ) grant funded by the Korean government ( MSIT ) ( No . 2020 - 0 - 01361 , Artificial Intelligence Graduate School Program ( Yonsei University ) ) .", "entities": [[33, 34, "DatasetName", "0"]]}
{"text": "Dissecting offensive language detection : does it work , and what can we do with it ? Social media messages are often written to attack specific groups of users based on their religion , ethnicity or social status , and they can be particularly threatening to vulnerable users such as teenagers . It is therefore very important to develop reliable , unbiased and robust detection systems to support stakeholders in fighting online hatred . Although state - of - the - art systems yield very good classification results , the problem is far from being solved . In my talk , I will discuss which issues still affect the development of abusive language detection systems , for example the problem of dealing with annotators ' disagreement in the creation of training data , and the issues related to contextual information in threads . On the other hand , I will show how the output of offensive language detection systems can be integrated with network - based information to study the behavioral patterns of different types of users , also in relation to misinformation .", "entities": [[111, 113, "TaskName", "abusive language"]]}
{"text": "Sara Tonelli holds a PhD in Language Sciences from Universit\u00e0 Ca ' Foscari , Venice . Since 2013 she has been the head of the Digital Humanities research group at Fondazione Bruno Kessler in Trento , Italy . Among many projects in digital humanities , she is currently involved in the H2020 ODEUROPA project ( focusing on olfactory information extraction ) , and in the H2020 PERCEPTIONS project ( online perception and migration narratives related to EU ) . Since January 2021 she is also the scientific coordinator of the KID ACTIONS European project ( addressing cyberbullying among children and adolescents ) . Sara 's main research interests are related to temporal and event - based processing of texts , especially in the historical domain , and social media processing , including the detection of abusive language .", "entities": [[135, 137, "TaskName", "abusive language"]]}
{"text": "Blocking has been widely studied for record linkage and entity disambiguation . Standard blocking is the simplest but most widely used method ( Fellegi and Sunter , 1969 ) . It is done by considering only pairs that meet al blocking predicates . Another is the sorted neighborhood approach ( Hern\u00e1ndez and Stolfo , 1995 ) which sorts the data by a certain blocking predicate , and forms blocks with pairs of those records within a certain window . Yan et al ( 2007 ) further improved this method to adaptively select the size of the window . Aizawa and Oyama ( 2005 ) introduced a suffix array - based indexing method , which uses an inverted index of suffixes to generate candidate pairs . Canopy clustering ( McCallum et al , 2000 ) generates blocks by clustering with a simple similarity measure and use loose & tight thresholds to generate overlapping clusters . Recent surveys ( Christen , 2012 ; Papadakis et al , 2016Papadakis et al , , 2020 imply that there are no clear winners and proper parameter tuning is required for a specific task . Much work optimized the blocking function for standard blocking . The blocking function is typically presented with a logical formula with blocking predicates . Two studies focused on learning a disjunctive normal form ( DNF ) blocking ( Bilenko et al , 2006 ; Michelson and Knoblock , 2006 ) were published in the same year . Making use of manually labeled record pairs , they used a sequential covering algorithm to find the optimal blocking predicates in a greedy manner . Additional unlabeled data was used to estimate the reduction ratio of their cost function ( Cao et al , 2011 ) while an unsupervised algorithm was used to automatically generate labeled pairs with rule - based heuristics used to learn DNF blocking ( Kejriwal and Miranker , 2013 ) . All the work above proposed to learn nondisjoint blocking because of the logical OR terms in the DNF . However , other work learns the blocking function with a pure conjunction , to ensure the generation of disjoint blocks . Das et al ( 2012 ) learns a conjunctive blocking tree , which has different blocking predicates for each branch of the tree . Fisher et al ( 2015 ) produces blocks with respect to a size restriction , by generating candidate blocks with a list of predefined blocking predicates and then performs a merge and split to generate the block with the desired size . Our work proposes a method for learning a nondisjoint blocking function in a conjunctive normal form ( CNF ) . Our method is based on a previous CNF learner ( Mooney , 1995 ) , which uses the fact that a CNF can be a logical dual of a DNF .", "entities": [[9, 11, "TaskName", "entity disambiguation"], [79, 82, "DatasetName", "Yan et al"]]}
{"text": "CNF blocking can be learned with a small modification to DNF blocking . CNF can be presented as the entire negation of a corresponding DNF and vice versa based on De Morgan 's laws . Using this , Mooney proposed CNF learning ( Mooney , 1995 ) , which is a logical dual of DNF learning . This motivated our CNF blocking method . Algorithm 2 illustrates the proposed CNF blocking and has a similar structure to algorithm 1 . Instead of running a sequential covering algorithm to cover all positive samples , CNF blocking tries to cover all negative samples using negated blocking predicates . In other words , a DNF formula is learned that is consistent with a negated predicate , which we designate negated DNF ( N egDN F ) . N egP is the negation of each predicate p in P . LEARNCNF gets 3 inputs , where L are labeled Find T CandN egT erms that maximizes gain function CALCNEG - GAIN ( P os , N eg , T erm ) 28 : if CALCNEGGAIN ( P os , N eg , T ) > 0 then 29 : N egDN F N egDN F \u2228 T 30 : Let P osCov be all l in P os that satisfies T", "entities": [[158, 159, "MethodName", "egT"], [191, 192, "DatasetName", "0"]]}
{"text": "Consider pairs in L only for clustering 9 : end for 10 : end function they filter out all terms with pairwise completeness ( PC ) below threshold t. We use the dual of the original function used as a CNF , which is now gain CN F = p+n P + N if n N > t 0 otherwise . ( 4 )", "entities": [[58, 59, "DatasetName", "0"]]}
{"text": "Figure 1 shows the PC - RR curve tested on three different gain functions . Blocking usually requires a high PC , so that we do not lose matched pairs after it is applied . As such , we focused on experiments with high PC values . As we can see from the results , information gain has highest RR overall . Thus , we use it as the gain function for the rest of the experiments .", "entities": [[6, 7, "DatasetName", "RR"], [59, 60, "DatasetName", "RR"]]}
{"text": "We compare non - disjoint CNF blocking with the DNF blocking ( Bilenko et al , 2006 ; Michelson and Knoblock , 2006 ) and canopy clustering ( McCallum et al , 2000 ) . We used the set of Jaro - Winkler distance attributes for canopy clustering . Figure 2 shows the PC - RR curve for each method . Both CNF and DNF were better than canopy clustering , as was shown in Bilenko et al ( 2006 in ( Khabsa et al , 2015 ) . For PC=0.99 , RR for CNF blocking was 0.882 while DNF blocking was 0.745 . We believe this is due to certain characteristics of scholarly databases . As discussed on the previous section , some attributes are empty for some records . DNF learns a blocking function by adding conjunction terms to gradually cover positive pairs . Although the proposed similarity criterion compatible could catch positive pairs with empty attributes , it allows many negative pairs to pass the criterion , which makes the RR low . On the other hand , CNF learns a blocking function to cover ( and filter out ) negative pairs gradually . Negative pairs are much more obvious to define ( pairs with different values ) , which makes the CNF more effective . Another advantage of using CNF is the processing time . Fast processing time to apply blocking is important for some applications , one example is when we do a online disambiguation ( Khabsa et al , 2015 ) , another is to do an author search which requires to find the relevant cluster quickly ( Kim et al , 2018 ) . We measured the average processing time of applying each blocking method at high PC ( PC=0.99 ) , CNF blocking , DNF blocking , canopy clustering took 1.39s , 2.09s , 0.44s respectively . Canopy clustering was the fastest but generally we saw from the Figure 2 that its RR is much lower in high PC . CNF blocking has a faster processing time compared to DNF blocking . This is because CNF is composed with conjunctions , so it can quickly reject pairs that are not consistent with any terms . On the other hand , DNF consists of disjunction terms , so each pair should check all terms to make the decision . Learned CNF is also simpler than DNF . Learned CNF at this level is as below ( fn , mn , ln is first , middle , last name respectively ) : In addition , we observed that proposed compatible predicate was frequently used in our result . This shows the effectiveness of compatible in dealing with the empty value .", "entities": [[55, 56, "DatasetName", "RR"], [92, 93, "DatasetName", "RR"], [173, 174, "DatasetName", "RR"], [330, 331, "DatasetName", "RR"]]}
{"text": "We evaluate our extension to disjoint blocks with CNF blocking . We compare the blocking learned with a pure conjunction , our proposed method , and the method of Fisher et al ( 2015 ) . Figure 3 shows the reduction ratio pair completion ( RR - PC ) curve for each method . We also plot the original non - disjoint CNF blocking for comparison . We see that our proposed disjoint CNF blocking is the best amongst all disjoint methods . Fisher 's method produced nearly uniformsized blocks , but had limitations in reaching a high PC and had a generally lower RR compared to our method . Disjoint CNF did n't perform as well when compared to non - disjoint CNF because it is forced to use a pure conjunction on its first step . However , this simple extension easily helps parallelize the clustering process , so that the algorithm scales better . Testing our method to all of PubMed , 82.17 % of the pairs are created in 10.5 min with 24 threads . Parallelization is important for disambiguation algorithms to scale to PubMed size scholarly databases ( Khabsa et al , 2014 ) . Processing time for disjoint CNF blocking comparable to the original non - disjoint CNF blocking . The learned disjoint CNF is : { ( fn , first ( 1 ) ) } { ( ln , exact ) } { ( fn , compatible ) \u2228 ( coauth , cos ( 0.8 ) ) } { ( mn , compatible ) } First two terms are from 1 - CNF , and others from 3 - CNF learner . We also tested this function to the whole PubMed .", "entities": [[45, 46, "DatasetName", "RR"], [104, 105, "DatasetName", "RR"]]}
{"text": "Morphological Inflection Generation with Multi - space Variational Encoder - Decoders", "entities": [[0, 2, "TaskName", "Morphological Inflection"]]}
{"text": "In morphologically rich languages , different affixes ( i.e. prefixes , infixes , suffixes ) can be combined with the lemma to reflect various syntactic and semantic features of a word . In many areas of natural language processing ( NLP ) it is important that systems are able to correctly analyze and generate different morphological forms , including previously unseen forms . The ability to accurately analyze and generate morphological forms is crucial to creating applications such as machine translation ( Chahuneau et al , 2013 ) and information retrieval ( Darwish and Oard , 2007 ) . Accordingly , learning morphological reinflection patterns from labeled data is an important challenge . The Universal Morphological Reinflection task at SIGMORPHON 2017 ( Cotterell and Sch\u00fctze , 2017 ) is an evaluation campaign aimed at systems that tackle the task of morphological inflection . It extends the SIGMORPHON 2016 Morphological Reinflection by conducting tasks in 52 languages instead of 10 Cotterell et al ( 2016 ) . In our system submission , we utilize multispace variational encoder - decoders ( MSVEDs ) , which are a varitional encoder - decoder with both continuous and discrete latent variables ( Zhou and Neubig , 2017 ) . The continuous latent variable is expected to reflect the lemma form of a word and the discrete variables are used to induce the desired labels of the inflected word . The whole model is trained in a semi - supervised fashion . For the supervised part we are reducing the reconstruction error of generating the inflected word given the lemma and corresponding tags . For the unsupervised part , we introduce the discrete latent variables representing the morphological tags , and train an auto - encoder over unlabeled corpora . Thus , the training objective includes both the variational lower bound on the marginal log likelihood of the observed parallel training data and the monolingual data . There are two tasks in SIGMORPHON 2017 , which are morphology inflection ( task 1 ) and paradigm completion ( task 2 ) respectively . We participated in task 1 , inflection generation , in which the goal is to output the inflected form of a lemma given a set of desired morphological tags . 1 Experimental results found that our model works relatively well on the shared task 1 without extensive tuning of hyper - parameters and languagespecific features .", "entities": [[20, 21, "DatasetName", "lemma"], [79, 81, "TaskName", "machine translation"], [89, 91, "TaskName", "information retrieval"], [140, 142, "TaskName", "morphological inflection"], [213, 214, "DatasetName", "lemma"], [263, 264, "DatasetName", "lemma"], [367, 368, "DatasetName", "lemma"]]}
{"text": "One challenge in training our model is that discrete random variables in a stochastic computation graph prevent the gradient from being backpropagated due to their non - differentiability , and marginalizing over all label combinations is also infeasible in our case . To alleviate this problem , we use the recently proposed Gumbel - Softmax trick ( Maddison et al , 2014 ; Gumbel and Lieblein , 1954 ) to create a differentiable estimator for categorical variables . In experiments , we start with a relatively large temperature and decrease it gradually .", "entities": [[54, 55, "MethodName", "Softmax"]]}
{"text": "Creating morphosyntactic tag maps : In our model , we treat the inference model on discrete labels in the form of discriminator , thus we need to know which label belongs to which morphosyntactic dimension . For example , V is a label of Part - of - speech - tagging . To obtain such mapping from a specific label to the morphosyntactic dimension , we leverage the Universal Morphological Feature Schema ( Sylak - Glassman , 2016 ) and also add the missing schema from the training data to create the key - value pairs of morphosysntactic dimension and label . Then we reformat the labels provided in the data set into the key - value pairs to train a classifier for each morphosyntactic dimension . Data Augmentation : We augment the data set in the similar way as Kann and Sch\u00fctze ( 2016 ) . By doing so , the training data is not limited to the form of lemma to inflected word but can also be any word pairs that share the same lemma . This helps our model generalize better and learn the latent continuous representations more effectively . The size of training data set after augmentation scales with a factor of 2 to 20 times compared with the original one .", "entities": [[44, 47, "DatasetName", "Part - of"], [127, 129, "TaskName", "Data Augmentation"], [161, 162, "DatasetName", "lemma"], [176, 177, "DatasetName", "lemma"]]}
{"text": "One potential reason for the lack of effectiveness of semi - supervised training is that the semi - supervised data that we used for training was not appropriate for the task at hand , or that we were not able to use it in the most effective way . In order to do so , we analyze the distribution of linguistic tags for words from the training data in the shared task and the Wiki Data provided by the organizer , with the hypothesis that if the distribution of tags for the Wiki Data is very different from the training and test data for the shared task , our predictions may be biased away from the testing distribution by incorporating the unsupervised Wiki data . To perform this examination , we use the tag classifier trained in our model to predict the labels for each word in the Wiki Data . The percentages of each label within each morphosyntactic dimension for Arabic and Persian are listed in Tab . 4 and Tab . 5 . We found that the distribution of the linguistic tags for the Wiki Data and the training data in the shared task are not always consistent . For example , in Arabic , the distributions of predicted tags with respect to case , possession , part - of - speech , and several other classes differ significantly from the original training data . Such difference suggests that either the words in the unlabeled Wiki Data have very different characteristics than our training set , or our tag classifier is not functioning properly to identify the tags . Either case would be detrimental to semi - supervised learning . The problem is even more stark for Persian : in Persian the only labeled words in the training data are verbs , so all nonverb words in the Wiki Data will receive an incorrect analysis , which is obviously not conducive to learning anything useful . As a recommendation for the future , when performing semi - supervised learning for morphology where the labeled data only represents a subset of the phenomena in the language , it is likely necessary to first identify which of the available unlabeled data is appropriate for semi - supervised learning before applying such methods .", "entities": [[219, 222, "DatasetName", "part - of"]]}
{"text": "In Tab . 1 , we notice that the performance on Latin is relatively poor compared with other languages . Latin is a highly inflected languages with three distinct genders , seven noun cases , four verb conjugations , four verb principal parts , six tenses , three persons , three moods , two voices , two aspects and two numbers . In addition to this , we found that the data set size after augmentation was only enlarged 2 times . We examine some errors made by our system on two worst performed languages Latin and Icelandic in Tab . 2 . As shown in the table , we found that the inflections of Latin and Icelandic have more suffix variations from the lemma . We guess our model still lacks the ability to capture more complicated inflections for such languages . We might consider adding the dependencies between different inflections for multiple target labels in our future work .", "entities": [[124, 125, "DatasetName", "lemma"]]}
{"text": "Non - Autoregressive Text Generation with Pre - trained Language Models", "entities": [[3, 5, "TaskName", "Text Generation"]]}
{"text": "Non - autoregressive generation ( NAG ) has recently attracted great attention due to its fast inference speed . However , the generation quality of existing NAG models still lags behind their autoregressive counterparts . In this work , we show that BERT can be employed as the backbone of a NAG model to greatly improve performance . Additionally , we devise mechanisms to alleviate the two common problems of vanilla NAG models : the inflexibility of prefixed output length and the conditional independence of individual token predictions . Lastly , to further increase the speed advantage of the proposed model , we propose a new decoding strategy , ratio - first , for applications where the output lengths can be approximately estimated beforehand . For a comprehensive evaluation , we test the proposed model on three text generation tasks , including text summarization , sentence compression and machine translation . Experimental results show that our model significantly outperforms existing non - autoregressive baselines and achieves competitive performance with many strong autoregressive models . In addition , we also conduct extensive analysis experiments to reveal the effect of each proposed component . 1", "entities": [[42, 43, "MethodName", "BERT"], [137, 139, "TaskName", "text generation"], [142, 144, "TaskName", "text summarization"], [145, 147, "DatasetName", "sentence compression"], [148, 150, "TaskName", "machine translation"]]}
{"text": "In this section , we give a detailed explanation of the proposed model . First , we describe how to utilize BERT as a non - autoregressive generation model . Then we discuss the decoding mechanism which allows the model to determine the output length dynamically . Finally , we introduce the new ratiofirst decoding strategy which further improves the model 's decoding efficiency .", "entities": [[21, 22, "MethodName", "BERT"]]}
{"text": "The architecture of the proposed model is presented in Figure 2 , in which the embedding layer and the stack of transformer layers are initialized with BERT ( Devlin et al , 2019 ) . Input Representation Following the setup of BERT , we first append a [ cls ] and a [ sep ] token on both sides of the source sequence . Then we attach a number of [ pad ] tokens at the end of source sequence to make its length equal to the predefined maximum size ( e.g. , 256 ) . Thus we can make sure the source length is longer than or equal to the output length . As a special case , for tasks like text summarization where the source is known to be longer than the target , we do not attach the [ pad ] tokens when constructing the input . Transformer Layers Given the source sequence X , it is processed by a stack of N transformer ( Vaswani et al , 2017 ) layers . Formally , the Multi - Head Attention is defined as MultiHead ( Q , K , V ) , where Q , K , V denotes the query , key and value respectively . The computation of the first transformer layer is then defined as : V ( 1 ) = MultiHead ( E ( X ) , E ( X ) , E ( X ) ) , ( 3 ) O ( 1 ) = FFN ( V ( 1 ) ) , ( 4 ) FFN ( x ) = max ( 0 , xW 1 + b 1 ) W 2 + b 2 , ( 5 ) where E ( X ) = T E ( X ) + P E ( X ) in which T E ( ) denotes the token embedding and P E ( ) denotes the position embedding . For other layers : V ( n ) = MultiHead ( O ( n\u22121 ) , O ( n\u22121 ) , O ( n\u22121 ) ) , ( 6 ) O ( n ) = FFN ( V ( n ) ) , ( 7 ) where n = 2 , ... , N and N is the total number of transformer layers . The final sequence representation H R T \u00d7d model is the output states of BERT from the last layer , where T is the source sequence length and d model is the model size . CRF Layer Then , H is passed through a linearchain CRF ( Lafferty et al , 2001 ) . Under the CRF framework , the likelihood of the target sequence Y with length T is then modelled as : P CRF ( Y | X ) = e S ( X , Y ) Y e S ( X , Y ) = 1 Z ( X ) exp ( T i=1 \u03a6 y i ( h i ) + T i=2 t ( y i\u22121 , y i ) ) , ( 8 ) where Z ( X ) is the normalizing factor and \u03a6 y i ( h i ) denotes the label score of y i at position i. In practice , \u03a6 is parameterized by a neural network that maps the BERT output state h i into the label ( vocabulary ) space . The t ( y i\u22121 , y i ) = T y i\u22121 , y i denotes the transition score from label y i\u22121 to y i where T R | V | \u00d7 | V | is the transition matrix . Approximation In the context of text generation , the size of the label space ( vocabulary size ) | V | is typically large , e.g. , 32k . Therefore , it is intractable to directly model the transition matrix T and the normalizing factor Z ( X ) . To this end , we adopt the techniques proposed by to approximate these two terms . Specifically , the full transition matrix is approximated by the product of two low - rank matrices T = E 1 E T 2 , where E 1 , E 2 R | V | \u00d7d and d is much smaller than | V | . To compute the normalizing factor Z ( X ) , at each time step , instead of searching through all possible paths , the number of candidates is heuristically truncated to a predefined beam size k. We refer readers to the original paper for further details .", "entities": [[26, 27, "MethodName", "BERT"], [41, 42, "MethodName", "BERT"], [122, 124, "TaskName", "text summarization"], [150, 151, "MethodName", "Transformer"], [179, 183, "MethodName", "Multi - Head Attention"], [271, 272, "DatasetName", "0"], [403, 404, "MethodName", "BERT"], [424, 425, "MethodName", "CRF"], [434, 435, "MethodName", "CRF"], [445, 446, "MethodName", "CRF"], [464, 465, "MethodName", "CRF"], [559, 560, "MethodName", "BERT"], [619, 621, "TaskName", "text generation"]]}
{"text": "In this section , we describe how to let the model determine the output sequence length by itself . Our basic idea is that we want the model to dynamically stop generation via emitting a special [ eos ] token . To achieve this , during training , we manually append two consecutive [ eos ] tokens to the end of the target sequence , as shown in the top left part of Figure 2 . In this way , the model can learn a deterministic transition behaviour between two [ eos ] states , meaning that t ( [ eos ] , [ eos ] ) = max v V t ( [ eos ] , v ) . This is because , during training , the model never sees a transition ( [ eos ] , v ) , where v = [ eos ] . During inference , the result\u1ef8 is acquired as Y = arg max Y S ( X , Y ) , where the CRF scoring function S ( X , Y ) in Equation ( 8 ) can be decomposed as : S ( X , Y ) = T i=1 \u03a6 y i ( h i ) + T i=2 t ( y i\u22121 , y i ) = \u03a6 y 1 ( h 1 ) initial state + T i=2 { label score \u03a6 y i ( h i ) + transition score t ( y i\u22121 , y i ) state transition } . ( 9 ) Once the decoded trajectory enters the [ eos ] state , the state transition term in S ( X , Y ) will be dominated by the transition score term t ( [ eos ] , [ eos ] ) . As a result , the model will keep transitioning to [ eos ] in the remaining steps . An example is provided in the right part of Figure 2 , from which we can see that , at step 5 , the decoded trajectory enters the [ eos ] state and remains at it in the rest of the generation process . In this way , our model can dynamically control the length of output sequence by entering the [ eos ] state during the generation process . After the entire generation process is completed , the final output sequence can be obtained by removing all generated [ eos ] tokens .", "entities": [[170, 171, "MethodName", "CRF"]]}
{"text": "Non - Autoregressive generation was first introduced by Gu et al ( 2018 ) to reduce the inference latency in machine translation . Recent works in this area have investigated ways to mitigate the tradeoff between the decoding speed and generation quality . Gu et al ( 2018 ) utilized fertility as latent variables for better translation performance . Wang et al ( 2019b ) proposed two auxiliary objectives for better modelling the output states and solving the under - translation problem . To better model the intermediate alignments between source and target sides , Ma et al ( 2019 ) proposed a model based on the generative flow framework . Ghazvininejad et al ( 2019 ) proposed to use a masked language objective to train the NAG model . During inference , starting from a fully masked sequence , the output is generated in an iterative refinement manner . Recently , proposed to incorporate a conditional random field into the decoder of a NAG model for better modelling the outputside dependencies . Our work is different from prior works in two aspects : ( 1 ) we directly utilize a pretrained language model ( BERT ) to perform nonautoregressive generation ; ( 2 ) our model can dynamically generate the output sequence without the need of prespecified output length .", "entities": [[20, 22, "TaskName", "machine translation"], [156, 159, "MethodName", "conditional random field"], [195, 196, "MethodName", "BERT"]]}
{"text": "We evaluate the proposed model on three typical text generation tasks : ( 1 ) text summarization ; ( 2 ) sentence compression and ( 3 ) machine translation .", "entities": [[8, 10, "TaskName", "text generation"], [15, 17, "TaskName", "text summarization"], [21, 23, "DatasetName", "sentence compression"], [27, 29, "TaskName", "machine translation"]]}
{"text": "In this section , we present further discussions and empirical analysis of the proposed model . nents , the overall performance decreases . By removing BERT from the model , we observe notable drop across all metrics . This shows that the knowledge of BERT is an important factor of the model 's strong performance . Comparing with results in Table 1 , it still outperforms vanilla NAG - CRF and performs comparably with NAG - CRF using LPD decoding , which demonstrates the merit of the proposed dynamic length decoding mechanism . Another interesting finding is that , by only removing the CRF layer , the most notable drop is observed on the bigram - level metric . This shows that the bigram - level dependencies on the output side are mainly captured by the CRF module . In addition , by removing both BERT and CRF , all metrics further decrease . This confirms that each of these two components positively contributes to the model 's overall performance .", "entities": [[25, 26, "MethodName", "BERT"], [44, 45, "MethodName", "BERT"], [69, 70, "MethodName", "CRF"], [76, 77, "MethodName", "CRF"], [103, 104, "MethodName", "CRF"], [136, 137, "MethodName", "CRF"], [145, 146, "MethodName", "BERT"], [147, 148, "MethodName", "CRF"]]}
{"text": "In this work , we explored the potential of BERT in various text generation tasks under the NAG framework . To address problems from NAG models previously having a prefixed output length , we devised a decoding mechanism which enables the model to determine the output length dynamically . To reduce errors stemming from the assumption of conditional independence of output tokens , we proposed a context - aware objective as well as using a CRF decoding . Furthermore , to maximize the inference speed advantage of our model , we introduced a ratio - first decoding strategy . We evaluated our model on three benchmark datasets and the results show that our model significantly outperforms many strong NAG baselines and performs comparably to many strong AG models .", "entities": [[9, 10, "MethodName", "BERT"], [12, 14, "TaskName", "text generation"], [75, 76, "MethodName", "CRF"]]}
{"text": "Revision is an essential part of the human writing process . It tends to be strategic , adaptive , and , more importantly , iterative in nature . Despite the success of large language models on text revision tasks , they are limited to non - iterative , one - shot revisions . Examining and evaluating the capability of large language models for making continuous revisions and collaborating with human writers is a critical step towards building effective writing assistants . In this work , we present a human - inthe - loop iterative text revision system , Read , Revise , Repeat ( R3 ) , which aims at achieving high quality text revisions with minimal human efforts by reading model - generated revisions and user feedbacks , revising documents , and repeating human - machine interactions . In R3 , a text revision model provides text editing suggestions for human writers , who can accept or reject the suggested edits . The accepted edits are then incorporated into the model for the next iteration of document revision . Writers can therefore revise documents iteratively by interacting with the system and simply accepting / rejecting its suggested edits until the text revision model stops making further revisions or reaches a predefined maximum number of revisions . Empirical experiments show that R3 can generate revisions with comparable acceptance rate to human writers at early revision depths , and the human - machine interaction can get higher quality revisions with fewer iterations and edits . The collected human - model interaction dataset and system code are available at https://github . com / vipulraheja / IteraTeR. Our system demonstration is available at https:// youtu.be/lK08tIpEoaE.", "entities": [[259, 261, "DatasetName", "interaction dataset"]]}
{"text": "Text revision is a crucial part of writing . Specifically , text revision involves identifying discrepan - cies between intended and instantiated text , deciding what edits to make , and how to make those desired edits ( Flower and Hayes , 1981 ; Faigley and Witte , 1981 ; Fitzgerald , 1987 ) . It enables writers to deliberate over and organize their thoughts , find a better line of argument , learn afresh , and discover what was not known before ( Sommers , 1980 ; Scardamalia , 1986 ) . Previous studies ( Flower , 1980 ; Collins and Gentner , 1980 ; Vaughan and McDonald , 1986 ) have shown that text revision is an iterative process since human writers are unable to simultaneously comprehend multiple demands and constraints of the task when producing well - written texts - for instance , covering the content , following linguistic norms and discourse conventions of written prose , etc . Therefore , writers resort to performing text revisions on their drafts iteratively to reduce the number of considerations at each time . Computational modeling of the iterative text revision process is essential for building intelligent and interactive writing assistants . Most prior works on the development of neural text revision systems Botha et al , 2018 ; Ito et al , 2019 ; Faltings et al , 2021 ) do not take the iterative nature of text revision and human feedback on suggested revisions into consideration . The direct application of such revision systems in an iterative way , however , could generate some \" noisy \" edits and require much burden on human writers to fix the noise . Therefore , we propose to collect human feedback at each iteration of revision to filter out those harmful noisy edits and produce revised documents of higher quality . In this work , we present a novel human - in - theloop iterative text revision system , Read , Revise , Repeat ( R3 ) , which reads model - generated revisions and user feedbacks , revises documents , and repeats human - machine interactions in an iterative way , as depicted in Figure 1 . First , users write a document as input to the system or choose one from a candidate document set to edit . Then , the text revision system provides multiple editing suggestions with their edits and intents . Users can accept or reject the editing suggestions in an iterative way and stop revision when no editing suggestions are provided or the model reaches the maximum revision limit . The overall model performance can be estimated by calculating the acceptance rate throughout all editing suggestions . R3 provides numerous benefits over existing writing assistants for text revision . First , R3 improves the overall writing experience for writers by making it more interpretable , controllable , and productive : on the one hand , writers do n't have to ( re - ) read the parts of the text that are already high quality , and this , in turn , helps them focus on larger writing goals ( 4.2 ) ; on the other hand , by showing edit intentions for every suggested edit , which users can further decide to accept or reject , R3 provides them with more fine - grained control over the text revision process compared to other one - shot based text revision systems ( Lee et al , 2022 ) , and are limited in both interpretability and controllability . Second , R3 improves the revision efficiency . The human - machine interaction can help the system produce higher quality revisions with fewer iterations and edits , and the empirical experiments in 4.2 validate this claim . To the best of our knowledge , R3 is the first text revision system in literature that can perform iterative text revision in collaboration by human writers and revision models . In this paper , we make three major contributions : We present a novel human - in - the - loop text revision system R3 to make text revision models more accessible ; and to make the process of iterative text revision efficient , productive , and cognitively less challenging . From an HCI perspective , we conduct experiments to measure the effectiveness of the proposed system for the iterative text revision task . Empirical experiments show that R3 can generate edits with comparable acceptance rate to human writers at early revision depths . We analyze the data collected from humanmodel interactions for text revision and provide insights and future directions for building high - quality and efficient human - in - the - loop text revision systems . We release our code , revision interface , and collected human - model interaction dataset to promote future research on collaborative text revision .", "entities": [[803, 805, "DatasetName", "interaction dataset"]]}
{"text": "Iterativeness . The human - in - the - loop iterative text revision evaluation results are reported in Table 2 . Each document is evaluated by at least 2 users . We find that R3 achieves comparable performances with ground - truth human revisions at revision depth 1 and 2 , and tends to generate less favorable edits at revision depth 3 . At revision depth 1 , R3 is able to generate more edits than ground - truth human edits for each document , and gets more edits accepted by users on average . This shows the potential of R3 in generating appropriate text revisions that are more favorable to users . At revision depth 2 , while R3 generates less edits than human writers on average , it gets a higher acceptance rate than human writers . This result suggests that for the end users , more edits may not necessarily lead to a higher acceptance ratio , and shows that R3 is able to make high - quality edits for effective iterative text revisions . At revision depth 3 , R3 generates even less edits compared both to human writers and its previous revision depths . This result can be attributed to the fact that our models are only trained on static human revision data , while at testing time they have to make predictions conditioned on their revisions generated at the previous depth , which may have a very different distribution of edits than the training data . Table 7 shows an example of iterative text revision in ArXiv domain generated by R3 . We also provide some other iterative revision examples generated by R3 in Appendix A. Edit Intentions . Table 3 demonstrates the distribution of different edit intentions , which can help us further analyze the which type of edits are more likely to be accepted by end users . For humangenerated revisions , we find that FLUENCY edits are most likely to be accepted since they are mainly fixing grammatical errors . For system - generated revisions , we observe that CLARITY edits are the most frequent edits but end users only accept 58.73 % of them , which suggests that our system needs further improvements in learning CLARITY edits . Another interesting observation is that STYLE edits are rarely generated by human writers ( 1.2 % ) and also gets the lowest acceptance rate ( 33.33 % ) than other intentions , while they are frequently generated by our system ( 16.7 % ) and surprisingly gets the highest acceptance rate ( 64.6 % ) than other intentions . This observation indicates that R3 is capable for generating favorable stylistic edits . Table 4 shows some examples of edit suggestions generated by R3 . Role of Human Feedback in Revision Quality . Table 3 : The distribution of different edit intentions . # Edits indicates the total number of applied edits under the current edit intention , # Accepts means the total number of edits accepted by users under the current edit intention , and % Accepts is calculated by dividing the total accepted edits with the total applied edits . final revised documents with and without humanin - the - loop for R3 . We asked another group of three annotators ( English L2 , bachelor 's or higher degree in Computer Science ) to judge whether the overall quality of system - generated final document is better than the ground - truth reference final document . The quality score ranges between 0 and 1 . We evaluated 10 unique documents in ArXiv domain , and took the average score from all 3 annotators . As shown in Table 5 , SYSTEM - HUMAN produces better overall quality score for the final system - generated documents with fewer iterations of revision and fewer edits , which validates the effectiveness of the human - machine interaction proposed in R3 . User Feedback . We also collected qualitative feedback about R3 from the linguistic experts through a questionnaire . The first part of our questionnaire asks participants to recall their experience with the system , and evaluate various aspects of the system ( in Table 6 ) . They were asked to rate how easy it was to get onboarded and use the system ( convenience ) , whether they were satisfied with the system ( revision quality and usage experience ) ( satisfaction ) , whether they felt it improved their productivity for text revision ( productivity ) , and whether they would like to use the system again ( retention ) for performing revisions on their documents . In general , the users gave positive feedback towards the ease of use of the system . However , they were neutral on the potential productivity impact , owing to the lack of domain knowledge of the documents they were evaluating . This issue could be mitigated by asking users to revise their own documents of interest . The retention and satisfaction scores were leaning slightly negative , which was explained as primarily attributed to gaps in the user interface design ( eg . improperly aligned diffs , suboptimal presentation of word - level edits , etc . ) . We also asked them to provide detailed comments on their experience , and the potential impact of the system on their text revision experience . Specifically , upon asking the users whether using the system to evaluate the model - suggested edits would be more time - efficient compared to actually revising the document themselves , we received many useful insights that help better design better interfaces and features of our system in future work , as some users noted : I think it would be faster using the system , but I would still be checking the text myself in case edits were missed . The system made some edits where there were letters and parts of words being added / re - moved / replaced , which sometimes took some time to figure out . That would n't be the case if I were editing a document . Ultimately , I would use the system for grammar / coherence / clarity edits , and then still research ( a lot ) to ensure that meaning was preserved throughout the document . For topics that I was more familiar with / more general topics , using the system would probably reduce my time by a third or so . For topics that required more in - depth research for me , the time saved by using the system might be minimal .", "entities": [[262, 263, "DatasetName", "ArXiv"], [591, 592, "DatasetName", "0"], [601, 602, "DatasetName", "ArXiv"]]}
{"text": "In this work , we develop an interactive iterative text revision system R3 that is able to effectively assist users to make revisions and improve the quality of existing documents . R3 can generate higher quality revisions while minimizing the human efforts . Users are provided with a reviewing interface to accept or reject system suggesting edits . The user - validated edits are then propagated to the next revision depth to get further improved revisions . Empirical results show that R3 can generate iterative text revisions with acceptance rates comparable or even better than human writers at early revision depths . 0 Due to its high lethality amongst the elderly , nursing homes are in the eye of the COVID - 19 storm . Emerging new test procedures , such as antigen or RT - LAMP tests , might enable us to protect nursing home residents by means of preventive screening strategies . Here , we develop a novel agent - based epidemiological model for the spread of SARS - CoV - 2 in nursing homes to identify optimal preventive testing strategiesto curb this spread . The model is microscopically calibrated to high - resolution data from actual nursing homes in Austria , including the detailed networks of social contacts of their residents and information on past outbreaks . Due to its high lethality amongst the elderly , nursing homes are in the eye of the COVID - 19 storm . Emerging new test procedures , such as antigen or RT - LAMP tests , might enable us to protect nursing home residents by means of preventive screening strategies . Here , we develop a novel agent - based epidemiological model for the spread of SARS - CoV - 2 in nursing homes to identify optimal preventive testing strategiesto curb this spread . The model is microscopically calibrated to high - resolution data from actual nursing homes in Austria , including the detailed networks of social contacts of their residents and information on past outbreaks . Here , we develop a novel detailed agent - based epidemiological model for the spread of SARS - CoV - 2 in nursing homes to identify optimal preventive testing strategiesto curb this spread . The model is microscopically calibrated to high - resolution data from actual nursing homes in Austria , including the detailed networks of social contacts of their resident detailed social contact networks and information on past outbreaks . Due to its high lethality amongst the elderly , n N ursing homes are in the eye of the COVID - 19 storm . Emerging new test procedures might enable us to protect nursing home residents by means of preventive screening strategies . Here , we develop a novel agent - based epidemiological model for the spread of SARS - CoV - 2 in nursing homes to identify optimal preventive testing strategies . The model is calibrated to high - resolution data from actual nursing homes in Austria , including the detailed networks of social contacts of their residents and information on past outbreaks .", "entities": [[102, 103, "DatasetName", "0"], [160, 161, "DatasetName", "agent"], [277, 278, "DatasetName", "agent"], [344, 345, "DatasetName", "agent"], [457, 458, "DatasetName", "agent"]]}
{"text": "Due to its high lethality amongst the elderly , nursing homes are in the eye of the COVID - 19 storm . Emerging new test procedures might enable us to protect nursing home residents by means of preventive screening . Here , we develop a novel n agent - based epidemiological model for the spread of SARS - CoV - 2 in nursing homes to identify optimal preventive testing strategies . The model is calibrated to high - resolution data from actual nursing homes in Austria , including detailed networks of social contacts of their residents and information on past outbreaks . A Canadian Forces statement said Cpl . Hornburg was killed during Operation Sadiq Sarbaaz ( Honest Soldier ) approximately 47 kilometres west of Kandahar City in Panjwaii District , a joint Afghan - NATO mission designed to \" set the conditions for a continuous security presence and the establishment of a new police sub - station in the northern part of ( Panjwaii ) . \" . Media reports indicated he died from mortar fire at around 4 : 30 p.m. local time ( 12:00 UTC ) while he was repairing the track on a Canadian Leopard tank near a cluster of villages known as Zangabad . A Canadian soldier serving with the Canadian Forces in Afghanistanwas killed on September 24 , 2007 . Four others were injured in the incident which killed 24 - year - old Corporal Nathan Hornburg of Calgary , Alberta . Nathan Hornburg was killed during Operation Sadiq Sarbaaz ( Honest Soldier ) , approximately 47 kilometres west of Kandahar City in Panjwaii District . Media reports indicated he died from mortar fire at around 4 : 30 p.m. local time ( 12:00 UTC ) while he was repairing the track on a Canadian Leopard tank near a cluster of villages known as Zangabad .", "entities": [[47, 48, "DatasetName", "agent"]]}
{"text": "We present more iterative revision examples generated by R3 in Table 8 and Table 9 . t HUMAN - HUMAN SYSTEM - HUMAN ( ours ) 0 Jecon Gregory is or was a nomadic artist , whose autobiographical fragments and poems , dictated to an acquaintance , were published as the book \" History of a Nation of One \" ( Harcourt Brace , New York , 1969 , andMichael Joseph , London , 1971 ) . Jecon apparently did not know his place , date , language or even name of birth , began his wanderings as a child in Malta ; walked through many lands , barefoot , tall and thin , pulling all his possessions in a basket on wheels , sleeping on the ground , and making a living by drawing portraits . Jecon Gregory is or was a nomadic artist , whose autobiographical fragments and poems , dictated to an acquaintance , were published as the book \" History of a Nation of One \" ( Harcourt Brace , New York , 1969 , andMichael Joseph , London , 1971 ) . Jecon apparently did not know his place , date , language or even name of birth , began his wanderings as a child in Malta ; walked through many lands , barefoot , tall and thin , pulling all his possessions in a basket on wheels , sleeping on the ground , and making a living by drawing portraits . 1 Jecon Gregory is or was a nomadic artist , whose autobiographical fragments and poems , dictated to an acquaintance , were published as the book \" History of a Nation of One : An Unlikely Memoir \" ( Harcourt Brace , New York , 1969 , andMichael Joseph , London , 1971 ) . .. Jecon apparently did not know his place , date , language or even name of birth , began his wanderings as a child in Malta ; walked through many lands , barefoot , tall and thin , pulling all his possessions in a basket on wheels , sleeping on the ground , and making a living by drawing portraits . Jecon Gregory is or was a nomadic artist , whose autobiographical fragments and poems , dictated to an acquaintance , were published as the book \" History of a Nation of One \" ( Harcourt Brace , New York , 1969 , andMichael Joseph , London , 1971 ) . Jecon apparently did not know his place , date , language or even name of birth , began his wanderings as a child in Malta ; walked through many lands , barefoot , tall and thin , pulling all his possessions in a basket on wheels , sleeping on the ground , and making a living by drawing portraits .", "entities": [[26, 27, "DatasetName", "0"]]}
{"text": "Semantic Modelling of Adjective - Noun Collocations Using FrameNet", "entities": [[8, 9, "DatasetName", "FrameNet"]]}
{"text": "In this paper we argue that Frame Semantics ( Fillmore , 1982 ) provides a good framework for semantic modelling of adjective - noun collocations . More specifically , the notion of a frame is rich enough to account for nouns from different semantic classes and to model semantic relations that hold between an adjective and a noun in terms of Frame Elements . We have substantiated these findings by considering a sample of adjectivenoun collocations from German such as enger Freund ' close friend ' and starker Regen ' heavy rain ' . The data sample is taken from different semantic fields identified in the German wordnet GermaNet ( Hamp and Feldweg , 1997 ; Henrich and Hinrichs , 2010 ) . The study is based on the electronic dictionary DWDS ( Klein and Geyken , 2010 ) and uses the collocation extraction tool Wortprofil ( Geyken et al , 2009 ) . The FrameNet modelling is based on the online resource available at http://framenet.icsi.berkeley.edu . Since FrameNets are available for a range of typologically different languages , it is feasible to extend the current case study to other languages .", "entities": [[155, 156, "DatasetName", "FrameNet"]]}
{"text": "Collocations such as to make a mistake and black coffee are multi - word expressions ( MWEs ) in which the choice of one constituent ( base ) is free , and the choice of the other one ( collocate ) is restricted and depends on the base ( Wanner et al , 2006 ) . Collocations are in the grey area between free phrases like black car and idiomatic MWEs such as black sheep , and in some cases it is challenging to draw the line between those concepts . As opposed to mere co - occurrences of words based on their frequencies , collocations show a certain degree of lexical rigidity which results in their partial lexicalization . This creates difficulties for the non - native speakers when interpreting and especially producing such expressions because a substitution of the restricted component with a synonymous word is not allowed by the language ( Bartsch , 2004 ) . Therefore , combinations such as * to do a mistake or * dark coffee are not acceptable and sound unnatural to the native speakers , but they still can be interpreted correctly . Idiomatic MWEs such as black sheep are semantically opaque and belong to the domain of figurative language . In spite of the fact that collocations have been getting more attention in the recent decades , there is a lack of systematic empirical studies on their semantic properties . Most of the previous corpus studies of collocations are concerned with their statistical properties and the ways to improve methods of automatic collocation extraction ( Church et al , 1991 ; Smadja , 1993 ; Evert , 2004 ; Pecina , 2008 ; Bouma , 2009 ) . These authors have shown that automatic and/or manual extraction of collocations is not an easy task . Our research does not attempt to contribute to this growing body of research . Rather , we focus on the classification and modelling of semantic relations that hold between a base and its collocate , e.g. the relation of degree that holds between the collocate heavy and its nominal base rain . More specifically , we will focus on the semantic relations that hold in adjective - noun collocations , since such collocations have received considerably less attention than verb - noun collocations . In our research , we utilize existing lexical resources that reliably identify adjective - noun collocations . For purely opportunistic reasons , we have chosen German as our language of investigation since there are a number of digital resources for German , including the DWDS ( short for the Digitales W\u00f6rterbuch der deutschen Sprache ) ( Klein and Geyken , 2010 ) and GermaNet ( Hamp and Feldweg , 1997 ; Henrich and Hinrichs , 2010 ) , that offer a broad coverage of adjectives and nouns as the two word classes under investigation . The remainder of this paper is structured as follows : Section 2 introduces the notion of collocation in more detail and describes the related work on the semantic classification of collocations . Section 3 presents our own proposal of how to deal with semantics of collocations ; we argue that the notion of a semantic frame in the sense of FrameNet ( Ruppenhofer et al , 2016 ) provides a suitably general semantic framework that is applicable to a wide range of semantic fields . Furthermore , we argue that collocations offer an interesting empirical domain for validating the structure of semantic frames and for further developing the FrameNet framework itself . The paper concludes with summary of our approach and with the discussion of different directions for future work .", "entities": [[544, 545, "DatasetName", "FrameNet"], [592, 593, "DatasetName", "FrameNet"]]}
{"text": "Following the logic of Nesselhauf ( 2003 ) and Mel'\u010duk ( 1998 ) , we consider the following types of statistical co - occurrences true collocations : 1 . the collocate has a specific sense with a limited number of words from different semantic fields , e.g. ' heavy ' as intensifier : heavy smoker , heavy rain , heavy traffic . The adjective 's sense is not prototypical , since it does not refer to the physical weight , but to intensity . 2 . the collocate has a specific sense only with one or very few semantically related bases , e.g. black coffee . The adjective 's sense here is not prototypical , since it does not refer to the colour , but to the fact , that no dairy products are added to the coffee . 3 . the sense of the collocate is so specific that it can be used with only one or very few semantically closely related bases , e.g. aquiline nose / face ( Mel'\u010duk , 1998 ) . That is the adjective 's only sense . As our empirical basis we rely on the electronic dictionary DWDS . The DWDS contains a rich lexicographic treatment of collocations on the basis of the collocation extraction tool Wortprofil ( Geyken et al , 2009 ) . Figure 1 shows an excerpt of the Wortprofil for the German noun Freund ' friend ' . 1 It illustrates the information contained in such a word profile . As Wanner ( 2006 ) emphasizes , collocation extraction typically only results in lists of collocations that are classified according to their morphosyntactic structure , but that do not provide any semantic information about the combinations . Semantic modelling of collocations requires a theoretical framework with a rich inventory that can be used for describing the relations between the base and its collocate . Such an inventory is offered in the form of Lexical Functions ( LFs ) in Mel'\u010duk 's Meaning \u2194Text Theory ( Mel'\u010duk , 1996 ) . A LF is a function in the mathematical sense : f ( x ) = y , where a general and abstract sense f is expressed by a certain lexical unit y depending on the lexical unit x it is associated with ( Mel'\u010duk , 1995 ) . The number of standard LFs is limited to about 60 , and they have fixed names , e.g. for intensifiers the LF Magn is suggested : Magn [ RAIN ] = heavy . For other cases the non - standard LFs are suggested . They are very specific , and their names are formulated in a natural language : e.g. obtained in an illegal way [ MONEY ] = dirty . LFs have been widely used in lexicographic projects on describing French semantic derivations and collocations ( Polguere , 2000 ) , and have also been implemented in the Spanish online dictionary of collocations ( DiCE ) that focuses on describing emotion lexemes ( Vincze et al , 2011 ) . Mel'\u010duk and Wanner ( 1994 ) employ LFs to represent collocation information for German lexemes from the semantic field of emotions . Wanner ( 2004 ) conducts experiments on automatic classification of Spanish verb - noun collocations based on the typology of LFs , and continues to work on this problem using different algorithms ( Wanner et al , 2006 ) . The works by Wanner ( 2004 ; 2006 ) mostly concentrate on verbal collocations , for which the Meaning - Text Theory provides at least 24 simple verbal LFs that can further be combined into complex LFs . By comparison , adjective - noun collocations have received less attention and the set of proposed adjectival LFs is relatively small : there are six simple adjectival LFs ( Mel'\u010duk , 2015 ) . Thus , our main objective is to find a suitable framework for describing adjectival collocations . Jousse ( 2007 ) proposes a way of formalizing non - standard adjectival LFs through assign - ing attributes to the base word , e.g. shape , size , colour , function . These attributes can be compared to Frame Elements in Frame Semantics ( Fillmore , 1982 ) and to the Qualia Roles in the theory of Generative Lexicon by J. Pustejovsky ( 1991 ) . Qualia roles have been implemented as the underlying framework in the construction of SIMPLE lexicon ( Bel et al , 2000 ) . While they are easily applicable for the treatment of concrete nouns , they fail to suitably generalize the semantics of abstract nouns . By contrast , the concept of semantic roles in Frame Semantics is not restricted to concrete nouns , but applies equally well to other semantic fields as well ( for details see section 3 below ) . The main idea of Frame Semantics is that word meanings are defined relative to a set of semantic frames , which represent non - linguistic entities such as events , states of affairs , beliefs , and emotions , and which are evoked by the use of corresponding words in a particular language . Semantic Frames for English are described in the lexical database FrameNet ( FrameNet - Database ) in terms of Frame Elements ( FEs ) ( Ruppenhofer et al , 2016 ) . The database provides a rich coverage of nouns and adjectives from different semantic fields , currently there are 5558 nouns and 2396 adjectives , and the resource is under further development . The further advantage of FrameNet is that it can be adapted for other languages . As demonstrated by Boas ( 2005 ) and Pad\u00f3 ( 2007 ) , a transfer of existing frame annotations from English to other languages is possible : there is a high degree of cross - lingual parallelism both for frames ( 70 % ) and for Frame Elements ( 90 % ) ( Pad\u00f3 , 2007 ) . For the reasons outlined above , we will use Frame El - ements in the sense of FrameNet for the semantic modelling of adjective - noun collocations .", "entities": [[500, 501, "DatasetName", "emotion"], [874, 875, "DatasetName", "FrameNet"], [876, 877, "DatasetName", "FrameNet"], [932, 933, "DatasetName", "FrameNet"], [1018, 1019, "DatasetName", "FrameNet"]]}
{"text": "As motivated in the previous section , the main objective of this study is to develop a framework for semantic modelling of German adjectivenoun collocations . To assess the applicability of FrameNet for modelling of collocations , we have investigated eleven frames for nouns from various semantic fields ( see Table 1 ) . The corresponding semantic fields were assigned according to the information from the German wordnet GermaNet , and the estimates about the degree of concreteness of the chosen nouns are provided by the MRC Psycholinguistic Database ( Wilson , 1988 ) . The nominal bases have been chosen on the basis of frequency and richness of collocates . The stage of choosing the candidates for modelling showed that there are significant differences in the behaviour of concrete and abstract nouns : the latter ones have a greater number and a richer variety of collocates ( see Table 2 ) . As explained in the previous section , we employ English FrameNet for German collocations . Semantic Frames in FrameNet describe non - linguistic concepts and deal with meanings rather than with particular lexical units in a language . Thus , a correct translation of the target German word into English makes it possible to apply the information contained in the English FrameNet to German data . In collocations , it is only the collocate ( the adjective ) that is language specific , and thus is problematic to translate . However , we consider the semantically transparent base ( noun ) to be the frame - evoking word , and such words do not cause any difficulties for translation .", "entities": [[31, 32, "DatasetName", "FrameNet"], [163, 164, "DatasetName", "FrameNet"], [171, 172, "DatasetName", "FrameNet"], [214, 215, "DatasetName", "FrameNet"]]}
{"text": "The number of true collocates for concrete nouns is relatively small due to several reasons . First of all , when combined with concrete nouns , most adjectives retain their prototypical meaning : enge Stra\u00dfe ' narrow street ' , gro\u00dfes Haus ' big house ' , hoher Turm ' tall tower ' , such expressions are considered free phrases . In addition , there are a lot of cases where a concrete noun is part of an idiomatic expression . 2 ( Wilson , 1988 ) indicate the level of concreteness of the nouns ( in the range 100 to 700 ) . When concrete nouns do form true collocations , the sense of their collocates is not prototypical , yet it is highly conventionalized . Consider the following collocates of the word Schokolade ' chocolate ' : schwarz lit . 'black ' , dunkel ' dark ' , wei\u00df ' white ' . In FrameNet the lexical unit ( LU ) ' chocolate ' evokes the frame \" Food \" with Frame Elements ( FEs ) FOOD , CONSTITUENT PARTS , DESCRIPTOR , and TYPE . Although it is true that dark chocolate has a darker colour than milk chocolate , when we use the expression dunkle Schokolade , we do not refer to the colour of the product , but to the fact that it contains a high percentage of cocoa and little or no milk . The same is true for wei\u00dfe Schokolade ' white chocolate ' : it indeed has a very light colour , but it is due to the fact that such type of chocolate is made of cocoa butter and does not contain cocoa powder . FrameNet offers a suitable FE TYPE for describing the relation that holds between these adjectives and the noun . It is defined in FrameNet as follows : \" This FE identifies a particular Type of the food item \" ( FrameNet - Database ) . A similar logic is applied to the collocates of the noun Droge ' drug ' : the collocates hart ' hard ' , weich ' soft ' , and leicht ' light ' are accommodated by the FE TYPE within the frame \" Intoxicants \" . 3 In the case of the artefact Schuh ' shoe ' , there are only two collocates ( hochhackig ' high - heeled ' , flach ' flat ' ) and the corresponding frame \" Clothing \" offers a suit - When a noun is less concrete , e.g. Regen ' rain ' that is a natural phenomenon and thus is a process , the list of its collocates is longer . The noun evokes the frame \" Precipitation \" and all the collocates are accommodated by the suitable frame elements . For example , under QUANTITY the following attributes are found : sintflutartig ' torrential ' , stark ' heavy ' , kr\u00e4ftig ' heavy ' , leicht ' light ' . All those adjectives describe rain in terms of the amount of water that falls in the process . The same is true for the modifier str\u00f6mend ' pouring ' , however , it carries an extra meaning of the manner in which it can rain and is therefore assigned to the FE MANNER .", "entities": [[157, 158, "DatasetName", "FrameNet"], [285, 286, "DatasetName", "FrameNet"], [308, 309, "DatasetName", "FrameNet"], [325, 326, "DatasetName", "FrameNet"]]}
{"text": "Abstract concepts have a complex meaning which is reflected in the amount of semantic roles describing the corresponding frame and in the amount of attributes through which the semantic roles are realised in the language . For instance , according to the FrameNet Database ( FrameNet - Database ) , the frame \" Personal relationship \" evoked by the noun Freund ' friend ' has the following non - core FEs : Relationship : The Relationship between Partners . Source of relationship : The source of the relationship . The semantic roles as well as the name of the frame suggest that , in many contexts , the word ' friend ' does not refer to a person as a human being of certain age , appearance , ethnicity , etc . , but to the relationship people are engaged in . In German , the adjectives eng lit . ' narrow ' or dick lit . ' thick ' are both used with Freund in the sense ' close ' , thus describing the DEGREE of friendship . The collocate alt ' old ' implies that the friendship has lasted for some time to the moment of speaking and can therefore be accommodate by the FE DURATION . When using wahr ' true ' , echt ' real ' , falsch ' fake ' in connection with friendship , we refer to its quality , the most suitable FE of that kind in this case is MANNER . There are also borderline cases , when the suitable FE is not obvious , as in the case of the word fest ' steady ' ( lit . ' solid ' ) . At first glance , the modifier characterizes MAN - NER ; however , in German , the expression fester Freund means ' boyfriend ' that actually refers to the nature of the relationship between the partners . Therefore , the most suitable FE for that adjective is RELATIONSHIP . All the adjectival modifiers find corresponding semantic roles , however , not all the FEs are realised through adjectives and some of the slots such as MEANS or DEPICTIVE are left empty . Such unrealised FEs are not listed in Table 2 . An accurate mapping of collocates to corresponding FEs is possible for other semantic fields as well . Consider an example from the field of cognition : Interesse ' interest ' . In FrameNet it evokes the frame \" Emotion directed \" . It has an EXPERIENCER referred to by the adjectives ureigen ' vested ' and widerstreitend ' conflicting ' ; MANNER ( rege ' active ' , lebhaft ' lively ' , vital ' lively ' , echt ' genuine ' , and wahr ' genuine ' ) ; TOPIC ( materiell ' material ' ) ; PARAMETER ( breit ' wide ' , handfest ' concrete ' , elementar ' fundamental ' , and vital ' vital ' ) ; and CIRCUMSTANCES ( unmittelbar ' direct ' ) . It also has a property of intensity described in the frame as DEGREE . This FE accommodates the collocates gro\u00df ' strong ' , stark ' strong ' , hoch ' strong ' , and massiv ' massive ' . A similar pattern is found for the emotion noun Angst ' fear ' . Consider its collocates : gro\u00df ' strong ' , nackt ' pure ' , h\u00f6llisch ' hellish ' , panisch ' panic ' , pur ' pure ' , unterschwellig ' subconscious ' , blank ' sheer ' , diffus ' vague ' , tief ' deep ' , dumpf ' vague ' , existenziell ' existential ' , krankhaft ' pathological ' The identified relevant FEs are as follows ( FrameNet - Database ) : Degree : The extent to which the Experiencer 's emotion deviates from the norm for the emotion . Circumstances : The Circumstances is the condition ( s ) under which the Stimulus evokes its response . In some cases it may appear without an explicit Stimulus . Quite often in such cases , the Stimulus can be inferred from the Circumstances . Manner : Any description of the way in which the Experiencer experiences the Stimulus which is not covered by more specific FEs , including secondary effects ( quietly , loudly ) , and general descriptions comparing events ( the same way ) . Manner may also describe a state of the Experiencer that affects the details of the emotional experience . The interpretation of some collocates is straightforward : the adjective existenziell ' existential ' indicates the area of the stimulus and is modelled as TOPIC . The collocates gro\u00df ' strong ' and tief ' deep ' are used as intensifiers and are , therefore , assigned to the FE DEGREE . The word h\u00f6llisch ' hellish ' is frequently used as an intensifier with Schmerz ' pain ' and carries the same meaning with ' fear ' , thus it is also assigned to DE - GREE . The other adjectives do not reveal any information about the intensity of the experienced emotion : blank ' sheer ' , pur ' pure ' , and nackt ' pure ' rather imply that , at a particular moment , fear is the only emotion guiding the behaviour of a person . This interpretation fits the definition of MANNER , and so do the collocates diffus ' vague ' and dumpf ' vague ' . The remaining three adjectives ( panisch , unterschwellig , krankhaft ) could also be assigned to MANNER , however , there is more information in their meaning than it may seem . These collocations are very close to psychological terms , as well as ' existential ' , but they refer to certain conditions under which fear might be experienced rather than to the area of the stimulus . In such cases context is helpful ; consider the following examples from the DWDS - Wortprofil for the noun Angst 4 : 1 . Deshalb habe die Frau panische Angst vor ihrem sehr dominanten Mann gehabt . eng . ' That is why the woman had a panic fear of her dominant husband ' . 2 . Dann spricht man von Erythrophobie , der krankhaften Angst zu err\u00f6ten . eng . ' This is referred to as erythrophobia , a pathological fear of blushing ' . 3 . Es ist eine unterschwellige , allt\u00e4gliche Angst , mit der die B\u00fcrger leben . eng . ' It is a subconscious everyday fear the citizens live with ' . The examples illustrate that these three collocates describe a certain kind of fear triggered by a particular stimulus , but the stimulus itself can only be derived from the context . Thus , the most suitable semantic role for accommodating the collocates is CIRCUMSTANCES . All the above described cases demonstrate that semantic roles present in abstract collocations are quite diverse , and the relations can well be generalized using FrameNet 's inventory of frame elements . There are , however , nouns , that seem to be less diverse when in comes to the number of attributes realized through adjectives . This is the case when a noun has a certain kind of scale at the core of its meaning . For instance , the noun Strafe ' punishment / penalty ' is mostly modified in terms of how strict the inflicted punishment is : drakonisch ' draconian ' , mild ' mild ' , hart ' harsh ' , empfindlich ' severe ' , hoch ' high ' , niedrig ' weak ' , saftig ' stiff ' , streng ' strict ' , scharf ' harsh ' , unmenschlich ' inhumane ' , schwer ' heavy ' , symbolisch ' symbolic ' , deftig ' severe ' They can all be accomodated by the FE DEGREE . However , two adjectives from this list stand out in their meaning : symbolisch ' symbolic ' and unmenschlich ' inhumane ' , they carry an extra meaning describing a kind of penalty , which is reflected in the FE INSTRUMENT ( \" The Instrument with which the reward or punishment is carried out \" ( FrameNet - Database ) ) . A similar situation holds for nouns from other semantic fields . Consider the noun ' price ' : it is defined in FrameNet as \" the amount of money expected , required , or given in payment for something \" ( FrameNet - Database ) . The list of its collocates contains the following adjectives : horrend ' horrendous ' , vern\u00fcnftig ' reasonable ' , erschwinglich ' affordable ' , stolz ' stiff ' , hoch ' high ' , niedrig ' low ' , fest ' fixed ' , stabil ' stable ' They all refer to the scale \" the amount of money \" , the latter two emphasize that there are no changes on the scale , whereas the others show the degree of how high the certain amount is from the point of view of the customer . The noun ' price ' evokes the frame ' Commerce scenario \" with the following FEs : BUYER , SELLER , GOODS , MONEY , MEANS , PURPOSE , RATE , UNIT . The most suitable FE in this case is RATE that according to FrameNet describes price or payment per unit of Goods and is therefore the closest to the concept of a scale in this frame . The examples illustrate that frame semantics offers a varied inventory for modelling semantic relations between the constituents of collocations independently of the semantic field of the noun , either concrete or abstract . FrameNet provides frame semantic information about many lexical units ; however , it is still under development and there are cases , when the frame evoked by a noun does not reflect all the aspects of its meaning . This issue is discussed in more detail in the next subsection .", "entities": [[42, 43, "DatasetName", "FrameNet"], [45, 46, "DatasetName", "FrameNet"], [291, 292, "TaskName", "NER"], [406, 407, "DatasetName", "FrameNet"], [552, 553, "DatasetName", "emotion"], [630, 631, "DatasetName", "FrameNet"], [644, 645, "DatasetName", "emotion"], [651, 652, "DatasetName", "emotion"], [861, 862, "DatasetName", "emotion"], [891, 892, "DatasetName", "emotion"], [1178, 1179, "DatasetName", "FrameNet"], [1384, 1385, "DatasetName", "FrameNet"], [1412, 1413, "DatasetName", "FrameNet"], [1431, 1432, "DatasetName", "FrameNet"], [1578, 1579, "DatasetName", "FrameNet"], [1635, 1636, "DatasetName", "FrameNet"]]}
{"text": "More than one thousand frames are described in FrameNet , thus providing a rich coverage of the lexicon . However , there is always the fundamental issue of granularity that affects the groupings of LUs into frames . There are cases when adjectival collocates provide additional information about a word 's semantics , but where there are no suitable FEs to accommodate this additional aspect of a word 's meaning . The following examples illustrate the issue . Consider the collocates of the noun Zukunft ' future ' : nah ' near ' , unmittelbar ' immediate ' , fern ' distant ' , weit ' distant ' , entfernt ' distant ' , rosig ' rosy ' , gl\u00e4nzend ' bright ' , licht ' bright ' , golden ' golden ' , strahlend ' bright ' , hell ' bright ' , bl\u00fchend ' prosper - ous ' , leuchtend ' bright ' , gro\u00df ' great ' , glanzvoll ' bright ' , dunkel ' dark ' , d\u00fcster ' dark ' , stabil ' stable ' Some of them refer to the temporal proximity of future , the others are evaluative descriptors ( mostly positive ones ) . The frame evoked by ' future ' in FrameNet is \" Alternatives \" with the following FEs ( FrameNet - Database ) : Agent : An individual involved in the Event . Salient entity : An entity intimately involved in the Event . Situation : Something that may happen in the future , or at least whose factual status is unresolved . - Number of possibilities : The number of different future Events under consideration . Purpose : The state - of - affairs that the Agent hopes to bring about which is associated with some of the possible Events but not others . None of the FEs reflects the evaluative or the temporal aspect of the meaning of the noun ' future ' expressed by the collocates above . This means that additional FEs need to be inserted into the frame \" Alternatives \" . The most appropriate FEs appear to be DESCRIPTOR which in FrameNet refers to descriptive characteristics and properties , and TIME . Consider another example : the frame \" Calendric unit \" is evoked by LUs denoting seasons , days of the week , months , times of the day , etc . The FEs describing this frame refer to different aspects of time . However , some , but not all of the LUs that evoke this frame have collocates referring to the weather or the state of nature : winter can be ' mild ' or ' harsh ' ( in the sense of temperature / weather ) , autumn , and September or October are ' golden ' . Such LUs should be accommodated by a subframe that inherits from the frame \" Calendric unit \" and contains additional FEs referring to weather and/or state of nature .", "entities": [[8, 9, "DatasetName", "FrameNet"], [210, 211, "DatasetName", "FrameNet"], [220, 221, "DatasetName", "FrameNet"], [225, 226, "DatasetName", "Agent"], [288, 289, "DatasetName", "Agent"], [358, 359, "DatasetName", "FrameNet"]]}
{"text": "In this paper we have argued that Frame Semantics provides a good framework for semantic modelling of adjective - noun collocations . More specifically , the notion of a frame is rich enough to account for nouns from different semantic classes and to model semantic relations that hold between an adjective and a noun in terms of Frame Elements . We have substantiated these findings by considering a sample of adjective - noun collocations from German that are taken from different semantic fields identified in the German wordnet GermaNet . We are grateful to the anonymous reviewer for raising an interesting question concerning the applicability of FrameNet 's semantic relations to adjective - noun free phrases as well . In future research , we plan to perform the modelling on a larger scale . For this purpose , we are currently preparing a large dataset containing more than 2000 German adjective - noun collocations . We will continue to use the dictionary DWDS and its collocation extraction tool Wortprofil as the empirical basis for obtaining the data . The resulting data sample will cover nouns and adjectives from all the semantic classes identified in GermaNet . We will use this dataset to examine FrameNet 's coverage of lexical units from different semantic fields . But even if a lexical frame exists for a given noun , the Frame Elements included in the lexical frame may not suffice . As described in the previous subsection , the structure of some semantic frames lacks important FEs , which therefore need to be added . Therefore , the overall objective in the future work is to examine various semantic frames and their Frame Elements in terms of their comprehensiveness and applicability for modelling diverse relations that hold between collocation constituents . A second important objective of our future research will be to address the question of reliability of annotations for the semantics of collocations on the basis of FrameNet . To this end , we plan to conduct an inter - annotator agreement study . This study will be informed by detailed instructions to the annotators in the form of written guidelines on how to identify the correct Frame Elements for a given collocation . As mentioned in Section 2 , one of the advantages of FrameNet is that it can be adapted for other languages . Therefore , it is worthwhile to conduct a comparative study on semantic annotation of collocations based on FrameNet for languages other than German . We plan to conduct such a study for Russian and English , since relevant resources and points of comparison are available for each of those two languages . For Russian , the Explanatory Combinatorial Dictionary of Russian ( Mel'cuk and Zholkovsky , 1984 ) describes collocations in terms of Lexical Functions\u00e0 la Mel'\u010duk . The Macmillan Collocations Dictionary for Learners of English ( Macmillan , 2010 ) provides a rich coverage of English lexicon with semantic grouping of collocates for each base word and uses short definitions to describe such semantic sets . We plan to evaluate the relative merits of different annotation schemes and expect that it will be of further benefit for our research on collocations as MWEs . Extending the present study to Russian will also provide an opportunity to compare the present approach that classifies collocations in terms of Frame Elements with Mel'\u010duk 's classification according to Lexical Functions . One noteworthy difference that is apparent already at this point is that FrameNet 's semantic relations can also be applied to describe free phrases , whereas the application of LFs is limited to lexically restricted combinations ( Mel'\u010duk , 1995 ; Mel'\u010duk , 2015 ) . 5", "entities": [[106, 107, "DatasetName", "FrameNet"], [203, 204, "DatasetName", "FrameNet"], [325, 326, "DatasetName", "FrameNet"], [383, 384, "DatasetName", "FrameNet"], [411, 412, "DatasetName", "FrameNet"], [584, 585, "DatasetName", "FrameNet"]]}
{"text": "As social distancing , self - quarantines , and travel restrictions have shifted a lot of pandemic conversations to social media so does the spread of hate speech . While recent machine learning solutions for automated hate and offensive speech identification are available on Twitter , there are issues with their interpretability . We propose a novel use of learned feature importance which improves upon the performance of prior state - of - the - art text classification techniques , while producing more easily interpretable decisions . We also discuss both technical and practical challenges that remain for this task .", "entities": [[26, 28, "DatasetName", "hate speech"], [60, 62, "TaskName", "feature importance"], [76, 78, "TaskName", "text classification"]]}
{"text": "In the day and age of social media , a person 's thoughts and feelings can enter the public discourse at the click of a mouse or tap of a screen . With billions of individuals active on social media , the task of finding reviewing and classifying hate speech online quickly grows to a scale not achievable without the use of machine learning . Additionally , the definition of hate - speech can be broad and include many nuances , but in general hate speech is defined as communication which disparages or incites violence towards an individual or group based on that person or groups ' cultural / ethnic background , gender or sexual orientation . ( Schmidt and Wiegand , 2017 ) . In the context of Covid - 19 , the United Nations has released guidelines on Covid - 19 related hatespeech Guidance on COVID - 19 related Hate Speech cautioning that Member States and Social Media companies that with the rise of Covid - 19 cases there has also been an increase of hate speech . The UN warns that such communication could be used for scapegoating , stereotyping , racist and xenophobic purposes . The tweets above displays an example of hate speech used for scapegoating by @realDonaldTrump and the response of @ajRAFAEL highlighting the impact this hate speech has on Asian Americans . The importance of identifying hate speech combined with the magnitude of the data makes this an area in which innovations achieved in NLP and AI research can make an impact . However , the datasets we use reflect their environments and even their annotators ( Waseem , 2016 ) ( Sap et al , 2019 ) , there are inherent cues contained by the data which can bias the predictions of models developed from these data ( Davidson et al , 2019 ) . In the context of detecting hate speech detection , this can lead to predictions be largely the outcome of a few key terms ( Davidson et al , 2017 ) . Being able to explain how underlying data impacts AI decision outcomes has real world applications , and social media companies ignorant to this fact could face a multitude of ethical and legal repercussions ( Samek et al , 2017 ) . Our Contribution : In this research , we merge feature importance with text classification to help decrease false positives . Our method combines the global representation of a term 's feature importance to a predicted class with the local term feature importance of an individual observation . Each term 's Figure 2 : This is an example of a Covid - 19 Tweet incorrectly classified by our baseline model as \" hate speech towards immigrants \" . After the applying our prediction enhancement method , the tweet was correctly classified as \" not hate speech \" . The first two sentence combinations show differences in local and global term importance impacting the Term Difference Multiplier . The intensity of grey represents the importance of each term to the denoted label . The last sentence pair provides the term difference for each local and global term pair as described in our experimental design . global feature importance is collected from our training dataset and baseline model . Then local feature importance is calculated for each observation on which our trained model makes a prediction . Our algorithm , uses the term level global feature importance to penalize model predictions when an observation 's local term feature importance differs from the global feature importance .", "entities": [[48, 50, "DatasetName", "hate speech"], [84, 86, "DatasetName", "hate speech"], [135, 136, "DatasetName", "Nations"], [151, 153, "DatasetName", "Hate Speech"], [177, 179, "DatasetName", "hate speech"], [206, 208, "DatasetName", "hate speech"], [222, 224, "DatasetName", "hate speech"], [233, 235, "DatasetName", "hate speech"], [318, 321, "TaskName", "hate speech detection"], [394, 396, "TaskName", "feature importance"], [397, 399, "TaskName", "text classification"], [415, 417, "TaskName", "feature importance"], [425, 427, "TaskName", "feature importance"], [456, 458, "DatasetName", "hate speech"], [478, 480, "DatasetName", "hate speech"], [539, 541, "TaskName", "feature importance"], [553, 555, "TaskName", "feature importance"], [577, 579, "TaskName", "feature importance"], [589, 591, "TaskName", "feature importance"], [595, 597, "TaskName", "feature importance"]]}
{"text": "In the same vein of our research , others have leveraged explainability derived with integrated gradients ( Sundararajan et al , 2017 ) and subject matter experts to create priors for use in text classification . In this research , they showed a decrease in undesired model bias and an increase in model performance when using scarce data ( Liu and Avci , 2019 ) . Overall , our method appears to have similar results and lessens the impacts of specific key terms to the the overall model prediction . One of the more commonly utilized explainability methods , SHAP provides a framework within the feature contrubutions to a a model 's output can be derived by borrowing Aultman - Shapely values from cooperative game theory ( Lundberg and Lee , 2017 ) . While there are several \" explainer \" implementations included with SHAP , Gradint Explainer allowed us to leverage our entire training dataset as the background dataset which allows our global average term values described in our experimental design to represent all terms in the training corpus . SHAP 's Gradient Explainer builds off of integrated gradients and leverages what are called expected gradients . This feature attribution method takes the integral from integrated gradients and reformulates it as an expectation usable in calculating the Shapely values . The resulting attributions sum to the difference between the expected and current model output . However , this method does assume independence of the input features , so it would violate this assumption if we were to leverage any sequence models in classification .", "entities": [[33, 35, "TaskName", "text classification"], [99, 100, "MethodName", "SHAP"], [144, 145, "MethodName", "SHAP"], [181, 182, "MethodName", "SHAP"]]}
{"text": "For this research , our intent to score unlabeled tweets called for a robust dataset which could be generalize to Covid - 19 tweets . This lead us to combining three datasets in the domain of hate and offensive speech : the collection of racist and sexist tweets presented by Waseem and Hovy ( Waseem and Hovy , 2016 ) , the Offensive Language Identification Dataset ( OLID ) ( Zampieri et al , 2019 ) , and Multilingual Detection of Hate Speech Against Immigrants and Women in Twitter ( HatEval ) ( Basile et al , 2019 ) . All hate or offensive labels contained within these three datasets were combined and given a sub - classification based on terms contained within each example . The sub - classes focus on hate and offensive speech targeting or directed towards immigrants , sexist , or political topics and/or individuals . The terms which divided positive labeled data into these three sub classes were derived through analysis of the terms contained in positive labeled tweets and through the terms used in extracting the tweets for the original data datasets .", "entities": [[63, 65, "TaskName", "Language Identification"], [67, 68, "DatasetName", "OLID"], [81, 83, "DatasetName", "Hate Speech"]]}
{"text": "In order to leverage this research in the context of Covid - 19 , we collected tweets using two different sources . First , we leveraged data collected by the Texas Advanced Computing Center ( TAAC ) at the University of Texas at Austin . This dataset was important for us to use due to \" Chinese Virus \" being one of the term pairs the TACC team used to collect data . In the context of hate speech in Covid - 19 , terms which target countries or ethnicity 's in the labeling of the virus clearly disregard the aforementioned UN guidance on hate speech . The second dataset we leveraged was provided by Georgia State University 's Panacea Lab ( Banda et al , 2020 ) . For this dataset , we specifically hydrated tweets from the days following the murder of George Floyd and begging of civil unrest in America . The intent behind limiting to these dates was to increase the chance of capturing tweets containing racial or ethnic terms .", "entities": [[30, 31, "DatasetName", "Texas"], [41, 42, "DatasetName", "Texas"], [77, 79, "DatasetName", "hate speech"], [104, 106, "DatasetName", "hate speech"]]}
{"text": "To achieve this , we apply SHAP 's Gradient Explainer to our baseline model . The Gradient Explainer output has the same dimensions as our Glove embedded data , so to reduce the dimensionality to that of the input text sequence , we sum the expected gradients across the axis corresponding to a term in each sequence . s 1 xn x 1 x i = x 1 + ... + x n s n xn x 1 Where s is a token in each sequence and x i is the summation of all expected gradients for the embedding dimensions . The values of these summations can be both positive and negative . Since our method requires positive inputs to measure the percentage difference , these values for every s step across all sequences in the training dataset are scaled between 0 and 1 via min / max scaling . We then create a dictionary of terms from the training corpus and store the \" global average \" feature importance of each term . This dictionary of global average feature importance values is used to calculate how far a particular prediction strays from the feature importance represented in our training data .", "entities": [[6, 7, "MethodName", "SHAP"], [141, 142, "DatasetName", "0"], [168, 170, "TaskName", "feature importance"], [179, 181, "TaskName", "feature importance"], [194, 196, "TaskName", "feature importance"]]}
{"text": "Difference Multiplier Now that we have the global importance of each term ( feature token ) to each class , we calculate the percentage difference of each term 's local feature importance to that term 's global feature importance by each class . 1 \u2212 \uf8eb \uf8ec \uf8ec \uf8ed | s g \u2212 s l | ( s g + s l ) 2 \uf8f6 \uf8f7 \uf8f7 \uf8f8 As you can see above the percentage difference between each local ( s l ) and global ( s g ) feature importance is subtracted from 1 . This outputs the difference multiplier for each term in a sequence . These values are averaged for each sequence , and the predicted probability for each class is multiplied by it 's local Term Difference Multiplier for each sequence . This outputs a new predicted probability score which has been penalized based on how much it 's local attribution values differ from the global mean of each term in the input sequence .", "entities": [[30, 32, "TaskName", "feature importance"], [37, 39, "TaskName", "feature importance"], [89, 91, "TaskName", "feature importance"]]}
{"text": "We found that the enhanced predictions predominately help to correct false positive classifications and shift predictions towards the negative class . We hypothesize this is due to the diversity of language and relatively neutral feature importance most terms have on the negative class . The relative neutrality in both global and local feature importance scores can be seen in figure 2 , and this results in a higher overall average for the aggregation of the Term Difference Multiplier . When we applied our model to Covid - 19 tweets we found similar results as described above . Quite often , we found that tweets providing information about specific ethnic groups or migrants were labeled as hateful or toxic towards immigrants by our baseline model and then correctly labeled as not hateful or offensive speech when we applied the Term Difference Multiplier . An example of this exact scenario is provided in figure 2 .", "entities": [[34, 36, "TaskName", "feature importance"], [52, 54, "TaskName", "feature importance"]]}
{"text": "Here we have experimented with a novel method to leverage the global feature importance from a model 's training dataset to reinforce or even penalize new predictions when their local feature importance varies from this learned global value . This novel algorithm marries the field of XAI and NLP in a manner which allows prior knowledge obtained in model training to impact present predictions . Overall , we believe this technique is especially applicable in scenarios like Covid - 19 where little to no pre - existing labeled data are available . By training this method on a similar corpus it can be used to detract from incorrect predictions made due to a few highly influential terms in Covid - 19 datasets . At present due to this method 's ability to decrease false positives , we believe one application of this research is increasing the efficiency of systems monitoring for hateful and toxic communication . However , this research is ongoing . We intend to explore further scenarios such as altering the equation used in our Term Difference Multiplier and the datasets used since the global feature importance can greatly influence the multiplier combined with our model 's original predicted probabilities .", "entities": [[12, 14, "TaskName", "feature importance"], [30, 32, "TaskName", "feature importance"], [187, 189, "TaskName", "feature importance"]]}
{"text": "Bi - Directional Recurrent Neural Ordinary Differential Equations for Social Media Text Classification", "entities": [[11, 13, "TaskName", "Text Classification"]]}
{"text": "Classification of posts in social media such as Twitter is difficult due to the noisy and short nature of texts . Sequence classification models based on recurrent neural networks ( RNN ) are popular for classifying posts that are sequential in nature . RNNs assume the hidden representation dynamics to evolve in a discrete manner and do not consider the exact time of the posting . In this work , we propose to use recurrent neural ordinary differential equations ( RN - ODE ) for social media post classification which consider the time of posting and allow the computation of hidden representation to evolve in a time - sensitive continuous manner . In addition , we propose a novel model , Bi - directional RNODE ( Bi - RNODE ) , which can consider the information flow in both the forward and backward directions of posting times to predict the post label . Our experiments demonstrate that RNODE and Bi - RNODE are effective for the problem of stance classification of rumours in social media .", "entities": [[0, 1, "TaskName", "Classification"], [168, 170, "TaskName", "stance classification"]]}
{"text": "Information disseminated in social media such as Twitter can be useful for addressing several realworld problems like rumour detection , disaster management , and opinion mining . Most of these problems involve classifying social media posts into different categories based on their textual content . For example , classifying the veracity of tweets as False , True , or unverified allows one to debunk the rumours evolving in social media ( Zubiaga et al , 2018a ) . However , social media text is extremely noisy with informal grammar , typographical errors , and irregular vocabulary . In addition , the character limit ( 240 characters ) imposed by social media such as Twitter make it even harder to perform text classification . Social media text classification , such as rumour stance classification 1 ( Qazvinian et al , 1 Rumour stance classification helps to identify the veracity 2011 ; Zubiaga et al , 2016 ; Lukasik et al , 2019 ) can be addressed effectively using sequence labelling models such as long short term memory ( LSTM ) networks ( Zubiaga et al , 2016 ; Augenstein et al , 2016 ; Kochkina et al , 2017 ; Zubiaga et al , 2018b , a ; Dey et al , 2018 ; Liu et al , 2019 ; Tian et al , 2020 ) . Though they consider the sequential nature of tweets , they ignore the temporal aspects associated with the tweets . The time gap between tweets varies a lot and LSTMs ignore this irregularity in tweet occurrences . They are discrete state space models where hidden representation changes from one tweet to another without considering the time difference between the tweets . Considering the exact times at which tweets occur can play an important role in determining the label . If the time gap between tweets is large , then the corresponding labels may not influence each other but can have a very high influence if they are closer . We propose to use recurrent neural ordinary differential equations ( RNODE ) ( Rubanova et al , 2019 ) and developed a novel approach bidirectional RNODE ( Bi - RNODE ) , which can naturally consider the temporal information to perform time sensitive classification of social media posts . NODE ( Chen et al , 2018 ) is a continuous depth deep learning model that performs transformation of feature vectors in a continuous manner using ordinary differential equation solvers . NODEs bring parameter efficiency and address model selection in deep learning to a great extent . RNODE generalizes RNN by extending NODE for time - series data by considering temporal information associated with the sequential data . Hidden representations are changed continuously by considering the temporal information . We propose to use RNODE for the task of sequence labeling of posts , which considers arrival times of the posts for updating hidden representaof a rumour post by classifying the reply tweets into different stance classes such as Support , Deny , Question , Comment tions and for classifying the post . In addition , we propose a novel model , Bi - RNODE , which considers not only information from the past but also from the future in predicting the label of the post . Here , continuously evolving hidden representations in the forward and backward directions in time are combined and used to predict the post label . We show the effectiveness of the proposed models on the rumour stance classification problem in Twitter using the RumourEval - 2019 ( Derczynski et al , 2019 ) dataset . We found RNODE and Bi - RNODE can improve the social media text classification by effectively making use of the temporal information and is better than LSTMs and gated recurrent units ( GRU ) with temporal features .", "entities": [[17, 19, "TaskName", "rumour detection"], [24, 26, "TaskName", "opinion mining"], [120, 122, "TaskName", "text classification"], [125, 127, "TaskName", "text classification"], [131, 133, "TaskName", "stance classification"], [141, 143, "TaskName", "stance classification"], [177, 178, "MethodName", "LSTM"], [383, 384, "MethodName", "NODE"], [420, 422, "TaskName", "model selection"], [435, 436, "MethodName", "NODE"], [584, 586, "TaskName", "stance classification"], [595, 596, "DatasetName", "Derczynski"], [615, 617, "TaskName", "text classification"], [635, 636, "MethodName", "GRU"]]}
{"text": "We consider the problem of classifying social media posts into different classes . Let D be a collection of N posts , D = { p i } N i=1 . Each post p i is assumed to be a tuple containing information such as textual and contextual features x i , time of the post t i and the label associated with the post y i , thus p i = { ( x i , t i , y i ) } . Our aim is to develop a sequence classification model which considers the temporal information t i along with x i for classifying a social media post . In particular , we consider the rumour stance classification problem in Twitter where one classifies tweets into Support , Query , Deny , and Comment class , thus y i Y= { Support , Query , Deny , Comment } .", "entities": [[119, 121, "TaskName", "stance classification"]]}
{"text": "To demonstrate the effectiveness of the proposed approaches , we consider the stance classification problem in Twitter and RumourEval - 2019 ( Derczynski et al , 2019 data set . This Twitter data set consists of rumours associated with eight events . Each event has a collection of tweets labelled with one of the four labels - Support , Query , Deny and Comment . We picked four major events Charliehebdo , Ferguson , Ottawashooting and Sydneysiege ( each with approximately 1000 tweets per event ) from RumourEval - 2019 to perform experiments . Features : For dataset preparation , each data point x i associated with a Tweet includes text embedding , retweet count , favourites count , punctuation features , negative and positive word count , presence of hashtags , user mentions , URLs etc . obtained from the tweet . The text embedding of the tweet is obtained by concatenating the word embeddings 2 . Each tweet timestamp is converted to epoch time and Min - Max normalization is applied over the time stamps associated with each event to keep the duration of the event in the interval [ 0 , 1 ] .", "entities": [[12, 14, "TaskName", "stance classification"], [22, 23, "DatasetName", "Derczynski"], [113, 114, "DatasetName", "retweet"], [154, 156, "TaskName", "word embeddings"], [192, 193, "DatasetName", "0"]]}
{"text": "We proposed RNODE , Bi - RNODE models for sequence classification of social media posts . These models consider temporal information of the posts and hidden representation are evolved as solution to ODE . Through experiments , we show these models perform better than LSTMs on rumour stance classification problem in Twitter", "entities": [[47, 49, "TaskName", "stance classification"]]}
{"text": "AMR Parsing via Graph Sequence Iterative Inference *", "entities": [[0, 2, "TaskName", "AMR Parsing"]]}
{"text": "On a coarse - grained level , we can categorize existing AMR parsing approaches into two main classes : Two - stage parsing ( Flanigan et al , 2014 ; Lyu and Titov , 2018 ; Zhang et al , 2019a ) uses a pipeline design for concept identification and relation prediction , where the concept decisions precede all relation decisions ; One - stage parsing constructs a parse graph incrementally . For more fine - grained analysis , those one - stage parsing methods can be further categorized into three types : Transitionbased parsing ( Wang et al , 2016 ; Damonte et al , 2017 ; Ballesteros and Al - Onaizan , 2017 ; Peng et al , 2017 ; Guo and Lu , 2018 ; Liu et al , 2018 ; Naseem et al , 2019 ) processes a sentence from left - to - right and constructs the graph incrementally by alternately inserting a new node or building a new edge . Seq2seq - based parsing ( Barzdins and Gosko , 2016 ; Konstas et al , 2017 ; van Noord and Bos , 2017 ; Peng et al , 2018 ) views parsing as sequence - to - sequence transduction by some linearization of the AMR graph . The concept and relation prediction are then treated equally with a shared vocabulary . The third class is graph - based parsing ( Cai and Lam , 2019 ; Zhang et al , 2019b ) , where at each time step , a new node along with its connections to existing nodes are jointly decided , either in order ( Cai and Lam , 2019 ) or in parallel ( Zhang et al , 2019b ) . So far , the recip - The boy must not go The current partial ( solid ) and full ( solid + dashed ) AMR graphs for the sentence \" The boy must no go \" rocal causation of relation prediction and concept prediction has not been closely - studied and wellutilized . There are also some exceptions staying beyond the above categorization . Peng et al ( 2015 ) introduce a synchronous hyperedge replacement grammar solution . Pust et al ( 2015 ) regard the task as a machine translation problem , while Artzi et al ( 2015 ) adapt combinatory categorical grammar . Groschwitz et al ( 2018 ) ; Lindemann et al ( 2019 ) view AMR graphs as the structure AM algebra .", "entities": [[11, 13, "TaskName", "AMR parsing"], [166, 167, "MethodName", "Seq2seq"], [379, 381, "TaskName", "machine translation"]]}
{"text": "Our approach is inspired by the deliberation process when a human expert is deducing a semantic graph from a sentence . The output graph starts from an empty graph and spans incrementally in a node - by - node manner . At any time step of this process , we are distilling the information for the next expansion . We call it expansion because the new node , as an abstract concept of some specific text fragments in the input sentence , is derived to complete some missing elements in the current semantic graph . Specifically , given the input sentence and the current partially constructed graph , we are answering two critical questions : which part of the input sequence to abstract , and where in the output graph to construct the new concept . For instance , Figure 1 ( a ) and ( b ) show two possible choices for the next expansion . In Figure 1 ( a ) , the word \" boy \" is abstracted to the concept boy to complement the subject information of the event go - 02 . On the ( Current Graph ) ( Input Sequence ) The boy wants the girl to believe him .", "entities": [[87, 89, "TaskName", "missing elements"]]}
{"text": "The boy wants the girl to believe him . attention x t y t+ 1 \u2026 initial state x 0 f ( G i , x 0 ) f ( G i , x 1 ) g ( W , y 1 ) g ( W , y 2 ) y 1 x 1 y 2 G i W graph memory Figure 2 : Overview of the dual graph - sequence iterative inference for AMR parsing . Given the current graph G i and input sequence W . The inference starts with an initial concept decision x 0 and follows the inference chain x 0 f ( G i , x 0 ) y 1 g ( W , y 1 ) x 1 f ( G i , x 1 ) y 2 g ( W , y 2 ) . The details of f and g are shown in red and blue boxes , where nodes in graph and tokens in sequence are selected via attention mechanisms . other hand , in Figure 1 ( b ) , a polarity attribute of the event go - 2 is constructed , which is triggered by the word \" not \" in the sentence . We note that the answer to one of the questions can help answer the other . For instance , if we have decided to render the word \" not \" to the graph , then we will consider adding an edge labeled as polarity , and finally determine its attachment to the existing event go - 2 ( rather than an edge labeled ARG0 to the same event go - 2 , though it is also present in the golden graph ) . On the other hand , if we have decided to find the subject ( ARG0 relation ) of the action go - 02 , we are confident to locate the word \" boy \" instead of function words like \" not \" or \" must \" , thus unambiguously predict the right concept boy . Another possible circumstance is that we may make a mistake trying to ask something that is not present in the sentence ( e.g. , the destination of the go - 02 action ) . This attempt will be rejected by a review of the sentence . The rationale is that literally we can not find the destination information in the sentence . Similarly , if we mistakenly propose to abstract some parts of the sentence that are not ready for construction yet , the proposal will be rejected by another inspection on the graph since that there is nowhere to place such a new concept . We believe the mutual causalities , as described above , are useful for action disambiguation and harmonious decision making , which eventually result in more accurate parses . We formulate AMR parsing as a series of dual graph - sequence decisions and design an iterative inference approach to tackle each of them . It is sort of analogous to the cognition procedure of a person , who might first notice part of the important information in one side ( graph or sequence ) , then try to confirm her decision at the other side , which could just refute her former hypothesis and propose a new one , and finally converge to a conclusion after multiple rounds of reasoning .", "entities": [[19, 20, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [74, 76, "TaskName", "AMR parsing"], [97, 98, "DatasetName", "0"], [104, 105, "DatasetName", "0"], [111, 112, "DatasetName", "0"], [466, 468, "TaskName", "decision making"], [479, 481, "TaskName", "AMR parsing"]]}
{"text": "Formally , the parsing model consists of a series of graph expansion procedures { G 0 . . . G i . . . } , starting from an empty graph G 0 . In each turn of expansion , the following iterative inference process is performed : y i t = g ( G i , x i t ) , x i t+1 = f ( W , y i t ) , where W , G i are the input sequence and the current semantic graph respectively . g ( ) , f ( ) seek where to construct ( edge prediction ) and what to abstract ( node prediction ) respectively , and x i t , y i t are the t - th graph hypothesis ( where to construct ) and t - th sequence hypothesis ( what to abstract ) for the i - th expansion step respectively . For clarity , we may drop the superscript i in the following descriptions . Figure 2 depicts an overview of the graphsequence iterative inference process . Our model has four main components : ( 1 ) Sequence Encoder , which generates a set of text memories ( per token ) to provide grounding for concept alignment and abstraction ; ( 2 ) Graph Encoder , which generates a set of graph memories ( per node ) to provide grounding for relation reasoning ; ( 3 ) Concept Solver , where a previous graph hypothesis is used for concept prediction ; and ( 4 ) Graph Solver , where a previous concept hypothesis is used for relation prediction . The last two components correspond to the reasoning functions g ( ) and f ( ) respectively . The text memories can be computed by Sentence Encoder at the beginning of the whole parsing while the graph memories are constructed by Graph Encoder incrementally as the parsing progresses . During the iterative inference , a semantic representation of current state is used to attend to both graph and text memories ( blue and red arrows ) in order to locate the new concept and obtain its relations to the existing graph , both of which subsequently refine each other . Intuitively , after a first glimpse of the input sentence and the current graph , specific sub - areas of both sequence and graph are revisited to obtain a better understanding of the current situation . Later steps typically read the text in detail with specific learning aims , either confirming or overturning a previous hypothesis . Finally , after several iterations of reasoning steps , the refined sequence / graph decisions are used for graph expansion .", "entities": [[15, 16, "DatasetName", "0"], [32, 33, "DatasetName", "0"]]}
{"text": "As mentioned above , we employ a sequence encoder to convert the input sentence into vector representations . The sequence encoder follows the multi - layer Transformer architecture described in Vaswani et al ( 2017 ) . At the bottom layer , each token is firstly transformed into the concatenation of features learned by a character - level convolutional neural network ( charCNN , Kim et al , 2016 ) and randomly initialized embeddings for its lemma , part - of - speech tag , and named entity tag . Additionally , we also include features learned by pre - trained language model BERT ( Devlin et al , 2019 ) . 2 Formally , for an input sequence w 1 , w 2 , . . . , w n with length n , we insert a special token BOS at the beginning of the sequence . For clarity , we omit the detailed transformations ( Vaswani et al , 2017 ) and denote the final output from our sequence encoder as { h 0 , h 1 , . . . , h n } R d , where h 0 corresponds the special token BOS and serves as an overall rep - resentation while others are considered as contextualized word representations . Note that the sequence encoder only needs to be invoked once , and the produced text memories are used for the whole parsing procedure .", "entities": [[26, 27, "MethodName", "Transformer"], [76, 77, "DatasetName", "lemma"], [78, 81, "DatasetName", "part - of"], [103, 104, "MethodName", "BERT"], [175, 176, "DatasetName", "0"], [192, 193, "DatasetName", "0"]]}
{"text": "We use a similar idea in Cai and Lam ( 2019 ) to encode the incrementally expanding graph . Specifically , a graph is simply treated as a sequence of nodes ( concepts ) in the chronological order of when they are inserted into the graph . We employ multi - layer Transformer architecture with masked self - attention and source - attention , which only allows each position in the node sequence to attend to all positions up to and including that position , and every position in the node sequence to attend over all positions in the input sequence . 3 While this design allows for significantly more parallelization during training and computation - saving incrementality during testing , 4 it inherently neglects the edge information . We attempted to alleviate this problem by incorporating the idea of Strubell et al ( 2018 ) that applies auxiliary supervision at attention heads to encourage them to attend to each node 's parents in the AMR graph . However , we did not see performance improvement . We attribute the failure to the fact that the neural attention mechanisms on their own are already capable of learning to attend to useful graph elements , and the auxiliary supervision is likely to disturb the ultimate parsing goal . Consequently , for the current graph G with m nodes , we take its output concept sequence c 1 , c 2 , . . . , c m as input . Similar to the sequence encoder , we insert a special token BOG at the beginning of the concept sequence . Each concept is firstly transformed into the concatenation of feature vector learned by a char - CNN and randomly initialized embedding . Then , a multi - layer Transformer encoder with masked self - attention and sourceattention is applied , resulting in vector representations { s 0 , s 1 , . . . , s m } R d , where s 0 represents the special concept BOG and serves as a dummy node while others are considered as contextualized node representations .", "entities": [[52, 53, "MethodName", "Transformer"], [297, 298, "MethodName", "Transformer"], [315, 316, "DatasetName", "0"], [332, 333, "DatasetName", "0"]]}
{"text": "Datasets Our evaluation is conducted on two AMR public releases : AMR 2.0 ( LDC0217T10 ) and AMR 1.0 ( LDC2014T12 ) . AMR 2.0 is the latest and largest AMR sembank that was extensively used in recent works . AMR 1.0 shares the same development and test set with AMR , while the size of its training set is only about one - third of AMR 2.0 , making it a good testbed to evaluate our model 's sensitivity for data size . 6 Implementation Details We use Stanford CoreNLP ( Manning et al , 2014 ) for tokenization , lemmatization , part - of - speech , and named entity tagging . The hyper - parameters of our models are chosen on the development set of AMR 2.0 . Without explicit specification , we perform N = 4 steps of iterative inference . Other hyper - parameter settings can be found in the Appendix . Our models are trained using ADAM ( Kingma and Ba , 2014 ) for up to 60 K steps ( first 50 K with the random sibling order and last 10 K with deterministic order ) , with early stopping based on development set performance . We fix BERT parameters similar to Zhang et al ( 2019a , b ) due to the GPU memory limit . During testing , we use a beam size of 8 for the highest - scored graph approximation . 7 AMR Pre - and Post - processing We remove senses as done in Lyu and Titov ( 2018 ) ; Zhang et al ( 2019a , b ) and simply assign the most frequent sense for nodes in post - processing . Notably , most existing methods including the state - the - ofart parsers ( Zhang et al , 2019a , b ; Lyu and Titov , 2018 ; Guo and Lu , 2018 , inter alia ) often rely on heavy graph re - categorization for reducing the complexity and sparsity of the original AMR graphs . For graph re - categorization , specific subgraphs of AMR are grouped together and assigned to a single node with a new compound category , which usually involves non - trivial expert - level manual efforts for hand - crafting rules . We follow the exactly same pre - and post - processing steps of those of Zhang et al ( 2019a , b ) for graph re - categorization . More details can be found in the Appendix . Ablated Models As pointed out by Cai and Lam ( 2019 ) , the precise set of graph re - categorization rules differs among different works , making it difficult to distinguish the performance improvement from model optimization and carefully designed rules . In addition , only recent works ( Zhang et al , 2019a , b ; Lindemann et al , 2019 ; Naseem et al , 2019 ) have started to utilize the large - scale pretrained language model , BERT ( Devlin et al , 2019 ; Wolf et al , 2019 ) . Therefore , we also include ablated models for addressing two questions : ( 1 ) How dependent is our model on performance from handcrafted graph re - categorization rules ? ( 2 ) How much does BERT help ? We accordingly implement three ablated models by removing either one of them or removing both . The ablation study not only reveals the individual effect of two model components but also helps facilitate fair comparisons with prior works .", "entities": [[101, 102, "TaskName", "lemmatization"], [103, 106, "DatasetName", "part - of"], [162, 163, "DatasetName", "ADAM"], [195, 197, "MethodName", "early stopping"], [205, 206, "MethodName", "BERT"], [504, 505, "MethodName", "BERT"], [555, 556, "MethodName", "BERT"]]}
{"text": "In order to investigate how our parser performs on individual sub - tasks , we also use the fine - grained evaluation tool ( Damonte et al , 2017 ) and compare to systems which reported these scores . 8 As shown in the right block of Table 1 , our best model obtains the highest scores on almost all sub - tasks . The improvements in all sub - tasks are consistent and uniform ( around 2%\u223c3 % ) compared to the previous state - of - the - art performance ( Zhang et al , 2019b ) , partly confirming that our model boosts performance via consolidated and harmonious decisions rather than fixing particular phenomena . By our ablation study , 8 We only list the results on AMR 2.0 since there are few results on AMR 1.0 to compare . it is worth noting that the NER scores are much lower when using graph re - categorization . This is because the rule - based system for NER in graph recategorization does not generalize well to unseen entities , which suggest a potential improvement by adapting better NER taggers .", "entities": [[149, 150, "TaskName", "NER"], [170, 171, "TaskName", "NER"], [190, 191, "TaskName", "NER"]]}
{"text": "We presented the dual graph - sequence iterative inference method for AMR Parsing . Our method constructs an AMR graph incrementally in a nodeby - node fashion . Each spanning step is explicitly characterized as answering two questions : which parts of the sequence to abstract , and where in the graph to construct . We leverage the mutual causalities between the two and design an iterative inference algorithm . Our model significantly advances the state - of - the - art results on two AMR corpora . An interesting future work is to make the number of inference steps adaptive to input sentences . Also , the idea proposed in this paper may be applied to a broad range of structured prediction tasks ( not only restricted to other semantic parsing tasks ) where the complex output space can be divided into two interdependent parts with a similar iterative inference process to achieve harmonious predictions and better performance .", "entities": [[11, 13, "TaskName", "AMR Parsing"], [121, 123, "TaskName", "structured prediction"], [130, 132, "TaskName", "semantic parsing"]]}
{"text": "We follow exactly the same pre - and postprocessing steps of those of Zhang et al ( 2019a , b ) for graph re - categorization . In preprocessing , we anonymize entities , remove wiki links and polarity attributes , and convert the resultant AMR graphs into a compact format by compressing certain subgraphs . In post - processing , we recover the original AMR format from the compact format , restore Wikipedia links using the DBpedia Spotlight API ( Daiber et al , 2013 ) , add polarity attributes based on rules observed from the training data . More details can be found in Zhang et al ( 2019a ) .", "entities": [[77, 78, "DatasetName", "DBpedia"]]}
{"text": "Using Type Information to Improve Entity Coreference Resolution", "entities": [[6, 8, "TaskName", "Coreference Resolution"]]}
{"text": "Coreference resolution ( CR ) is an extensively studied problem in computational linguistics and NLP ( Hobbs , 1978 ; Lappin and Leass , 1994 ; Mitkov , 1999 ; Ng , 2017 ; Clark and Manning , 2016 ; Lee et al , 2017 ) . Solutions to this problem allow us to make meaningful links between concepts and entities within a discourse and therefore serves as a valuable pre - processing step for downstream tasks like summarization and questionanswering ( Steinberger et al , 2007 ; Dasigi et al , 2019 ; Sukthanker et al , 2020a ) . Recently , multiple datasets including Ontonotes ( Pradhan et al , 2012 ) , Litbank ( Bamman et al , 2020 ) , EmailCoref ( Dakle et al , 2020 ) , and WikiCoref ( Ghaddar and Langlais , 2016 ) have been proposed as benchmark datasets for CR , especially in the sub - area of entity anaphora ( Sukthanker et al , 2020b ) . Entity anaphora is a simpler starting place for work on anaphora because unlike abstract anaphora ( Webber , 1991 ) , entity anaphora are pronouns or noun phrases that refer to an explicitly mentioned entity in the discourse rather than an abstract idea that must be constructed from a repackaging of information revealed over an extended text . An affordance of entity anaphora is that they have easily articulated semantic types . Most of the entity CR datasets are extensively annotated for syntactic features ( like constituency parse etc . ) and semantic features ( like entity - types ) . However , none of the published SOTA methods ( Lee et al , 2017 ; Joshi et al , 2019Joshi et al , , 2020 ) explicitly leverage the type information . In this paper , we present a proof of concept to portray the benefits of using type information in neural approaches for CR . Named entities are generally divided generically ( e.g. person , organization etc . ) or in a domain - specific manner ( e.g. symptom , drug , test etc . ) . In this work , we consider CR datasets that contain generic entitytypes . One challenge is that the different corpora do not utilize the same set of type tags . For example , OntoNotes includes 18 types while EmailCoref includes only 4 . Thus , we evaluate the performance of the proposed modeling approach on each dataset both with the set of type tags germaine to the dataset as well as a common set of four basic types ( person , org , location , facility ) inspired from research on Named Entity Recognition ( NER ) ( Tjong Kim Sang , 2002 ; Tjong Kim Sang and De Meulder , 2003 ) . Our motivation is similar to ( Durrett and Klein , 2014 ) , which used a structured CRF with handcurated features to jointly - model the tasks of CR , entity typing , and entity linking . Their joint architecture showed an improved performance on CR over the independent baseline . However , our work differs from there 's as we show the benefits of entity - type information in neural models that use contextualized representations like BERT ( Peters et al , 2018 ) . Some prior art ( Petroni et al , 2019 ; Roberts et al , 2020 ) argues that contextual - Figure 1 : We improve Bamman et al ( 2020 ) for entity coreference resolution by incorporating type information at two levels . ( 1 ) Type information is concatenated with the mention span representation created by their model ; and ( 2 ) A consistency check is incorporated that compares the types of two mentions under consideration to calculate the coreference score . Please refer to Section 3 for details . ized embeddings implicitly capture facts and relationships between real - world entities . However , in this work , we empirically show that access to explicit knowledge about entity - types benefits neural models that use BERT for CR . We show a consistent improvement in performance on four different coreference datasets from varied domains . Our contribution is that we evaluate the impact of the introduction of type information in neural entity coreference at two different levels of granularity ( which we refer to as original vs common ) , demonstrating their utility both in the case where gold standard type information is available , and the more typical case where it is predicted .", "entities": [[0, 2, "TaskName", "Coreference resolution"], [78, 79, "TaskName", "summarization"], [106, 107, "DatasetName", "Ontonotes"], [115, 116, "DatasetName", "Litbank"], [134, 135, "DatasetName", "WikiCoref"], [390, 391, "DatasetName", "OntoNotes"], [448, 451, "TaskName", "Named Entity Recognition"], [452, 453, "TaskName", "NER"], [488, 489, "MethodName", "CRF"], [501, 503, "TaskName", "entity typing"], [505, 507, "TaskName", "entity linking"], [548, 549, "MethodName", "BERT"], [590, 592, "TaskName", "coreference resolution"], [685, 686, "MethodName", "BERT"]]}
{"text": "Neural Coreference Resolution : Recently , neural approaches to coreference ( Joshi et al , 2020 ( Joshi et al , , 2019Lee et al , , 2017 have begun to show their prowess . The SOTA models show impressive performance on state - of - the - art datasets like OntoNotes ( Pradhan et al , 2012 ) and GAP ( Webster et al , 2018 ) . The notable architecture proposed by Lee et al ( 2017 ) scores pairs of entity mentions independently and later uses a clustering algorithm to find coreference clusters . On the other hand , improve upon this foundation by introducing an approximated higher - order inference that iteratively updates the existing span representation using its antecedent distribution . Moreover , they propose a coarseto - fine grained approach to pairwise scoring for tackling the computational challenges caused due to the iterative higher - order inference . More recently , Joshi et al ( 2019Joshi et al ( , 2020 showed that use of contextual representations instead of wordembeddings like GloVe ( Pennington et al , 2014 ) can further boost the results over and above those just mentioned . Our work offers additional improvement by building on the model proposed in Bamman et al ( 2020 ) , which is based on Lee et al ( 2017 ) , and adds additional nuanced information grounded in semantic types . Type Information : Named Entity Recognition datasets ( Tjong Kim Sang , 2002 ; Tjong Kim Sang and De Meulder , 2003 ; Li et al , 2016 ) often group entity mentions into different types ( or categories ) depending on the domain and the potential downstream applications of the corpus . For example , the medical corpus used in the i2b2 Challenge 2010 ( Uzuner et al , 2011 ) annotates domain - specific types like problem , test , symptom etc . , whereas , a more general - domain dataset like CoNLL - 2002 ( Tjong Kim Sang , 2002 uses generic types like person , organization , and location . Type information as a predictive signal has been shown to be beneficial for NLP tasks like relation extraction ( Soares et al , 2019 ) and entitylinking . It affords some level of disambiguation , which assists models with filtering out some incorrect predictions in order to increase the probability of a correct prediction . In this work , we evaluate the benefits of using explicit type information for CR . We show that a model that leverages entity types associated with the anaphoric/ antecedent mentions significantly reduces the problem of type inconsistency in the output coreference clusters and thus improves the overall performance of the neural baseline on four datasets . Type Information for CR : Multiple prior works have shown type - information to be a useful feature for shallow coreference resolution classifiers ( Soon et al , 2001 ; Bengtson and Roth , 2008 ; Ponzetto and Strube , 2006 ; Haghighi and Klein , 2010 ; Durrett and Klein , 2014 ) . ( Soon et al , 2001 ) take the most frequent sense for each noun in WordNet as the semantic class for that noun and use a decision - tree for pairwise classification of whether two samples co - refer each other . ( Bengtson and Roth , 2008 ) use a hypernym tree to extract the type information for different common nouns , and compare the proper names against a predefined list to determine if the mention is a person . They , then , pass this and many other features ( like distance , agreement , etc . ) through a regularized average perceptron for pairwise classification . This paper expands on these studies to show that entity - type information is also beneficial for neural models that use contextualized representations like BERT ( Peters et al , 2018 ) , which have been argued to implicitly capture facts and relationships between real - world entities ( Petroni et al , 2019 ; Roberts et al , 2020 ) .", "entities": [[1, 3, "TaskName", "Coreference Resolution"], [51, 52, "DatasetName", "OntoNotes"], [60, 61, "DatasetName", "GAP"], [177, 178, "MethodName", "GloVe"], [240, 243, "TaskName", "Named Entity Recognition"], [368, 370, "TaskName", "relation extraction"], [484, 486, "TaskName", "coreference resolution"], [653, 654, "MethodName", "BERT"]]}
{"text": "We use the model proposed in Bamman et al ( 2020 ) as our baseline . The model gives stateof - the - art scores on the LitBank corpus ( Bamman et al , 2020 ) and is an end - to - end mention ranking system based on Lee et al ( 2017 ) , which has shown competitive performance on the OntoNotes dataset . However , this model differs from Lee et al ( 2017 ) as it uses BERT embeddings , omits author and genre information , and only focuses on the task of mention - linking . Since our main goal is to evaluate the benefits of type information , we too separate mention - linking from mentionidentification and only show results computed over gold - standard mentions . This controls for the effects of the mention - identification module 's performance on our experiments . Impact of typeinformation incorporation in the real - world endto - end CR setting ( mention identification + linking ) is left as future work . The BERT embeddings for each token i are passed through a bi - directional LSTM ( x i ) . To represent a mention m with start and end positions s , e respectively , x s , x e , attention over x s , ... , x e , and features to represent the width ( wi ) and inclusion within quotations ( qu ) are concatenated . m = [ x s ; x e ; Att ( x s , .. , x e ) ; wi ; qu ] ( 1 ) Finally , given the representation of two mentions m j and m k , their coreference score S ( m j , m k ) is computed by concatenating m j , m k , m j m k , distance ( d ) between the mentions and whether one mention is nested ( n ) within the other , which are then passed through fully - connected layers ( FC ) . S ( m j , m k ) = FC ( [ m j ; m k ; m j m k ; d ; n ] ) ( 2 ) We refer the reader to ( Bamman et al , 2020 ; Lee et al , 2017 ) for more details about the architecture .", "entities": [[27, 28, "DatasetName", "LitBank"], [63, 64, "DatasetName", "OntoNotes"], [81, 82, "MethodName", "BERT"], [177, 178, "MethodName", "BERT"], [190, 191, "MethodName", "LSTM"]]}
{"text": "We improve the above model by including entitytype information on two levels ( Figure 1 ) . First , we concatenate the entity - type t of the mention to m ( in Eq . 1 ) to improve the mention representation . m = [ m ; t ] ( 3 ) This allows the model access to the entity type of the mention as an additional feature . We call this + ET - self . Second , to check the type consistency ( softly ) between any two mentions under consideration as possibly coreferent , we append a feature ( tc ) in Eq . 2 , which takes the value 0 if both mentions have the same type , and 1 otherwise . For example , in Figure 1 , since Los Angeles and it have the same entity - type PLACE , tc jk = 0 . S ( m j , m k ) = FC ( [ m j ; m k ; m j m k ; d ; n ; tc jk ] ) ( 4 ) This part of the approach is referred to as + ETcross throughout the remainder of the paper . We decide against the use of a hard consistency check ( which would filter out mentions which do not have the same type ) as it might not generalize well to bridging anaphora ( Clark , 1975 ) where the anaphor refers to an object that is associated with , but not identical to , the antecedent ( Poesio et al , 2018 ) . In such cases , the type of the anaphora and its antecedent may not match . Finally , our architecture combines both components together as + ET ( ET = ET - self + ET - cross ) .", "entities": [[114, 115, "DatasetName", "0"], [150, 151, "DatasetName", "0"]]}
{"text": "We gauge the benefits of using entity - type information on the four datasets discussed below . LitBank . This dataset ( Bamman et al , 2020 ) contains coreference annotations for 100 literary texts . 1 This dataset limits the markable mentions to six entity - types , where majority of the mentions ( 83.1 % ) point to a person . EmailCoref . This dataset ( Dakle et al , 2020 ) comprises of 46 email threads with a total of 245 email messages . 2 Similar to LitBank , it considers a mention to be a span of text that refers to a real - world entity . In this work , we filter out pronouns that point towards multiple entities in the email ( e.g. we , they ) thus only focusing on singular mentions . Ontonotes . From this multi - lingual dataset , we evaluate on the subset ( english ) from OntoNotes that was used in the CoNLL - 2012 shared task ( Pradhan et al , 2012 ) . 3 It contains 2802 training , 343 development , and 348 test documents . The dataset differs from LitBank in its annotation scheme with the biggest difference being the fact that it does not annotate singletons . It contains annotations for 18 different entitytypes . However , unlike LitBank and EmailCoref , not all mentions have an associated entity - type . For example , none of the pronoun mentions are given a type even if they act as anaphors to typed entities . We partially ameliorate this issue by extracting gold coreference clusters that contain at least one typed mention and assigning the majority type in that cluster to all of its elements . For example , in Figure 1 , if Los Angeles is typed PLACE , and it is in the gold coreference cluster of Los Angeles ( no other element in the cluster ) , then it is also assigned the type PLACE . WikiCoref . This corpus , released by ( Ghaddar and Langlais , 2016 ) , comprises 30 documents from wikipedia annotated for coreference resolution . 4 The annotations contain additional metadata , like the associated freebase rdf link for each mention ( if available ) . We use this rdf entry to extract the mention 's entity types from freebase dump . Mentions that do not get any type are marked NA . The first 24 documents are chosen for training , the next 3 for development , and the rest for testing . The above - discussed datasets differ in the number as well as the categories of entity - types they originally annotate ( Table 1 ) . Apart from a common list of types ( like PER , ORG , LOC ) , they also include corpus - specific categories like DIGital ( EmailCoref ) , MONey , and LANG ( OntoNotes ) . We carry out experiments with two sets of types - original and common - for each dataset . The common set of types include the following 5 categories : PER , ORG , LOC , FAC , OTHER .", "entities": [[17, 18, "DatasetName", "LitBank"], [90, 91, "DatasetName", "LitBank"], [140, 141, "DatasetName", "Ontonotes"], [158, 159, "DatasetName", "OntoNotes"], [195, 196, "DatasetName", "LitBank"], [225, 226, "DatasetName", "LitBank"], [335, 336, "DatasetName", "WikiCoref"], [357, 359, "TaskName", "coreference resolution"], [489, 490, "DatasetName", "OntoNotes"]]}
{"text": "The previous experiment leverages the original entity - types assigned by dataset annotators . Due to the differences in domain and annotation guidelines among these datasets , the annotators introduce several domain - specific entity types ( e.g. DIGital , Work Of Art etc . ) apart from the common four ( PERson , ORGanization , LOCation , FACility ) that are often used in the Named Entity Recognition literature ( Tjong Kim Sang , 2002 ) . The former can prove to be much more difficult to obtain/ learn due to dearth of relevant data . Therefore , to assess the worth of using a common entitytype list for all datasets , we map the original types ( Table 1 ) to the above - mentioned four common types . 6 Categories that do not map to any common type are assigned Other . + ET ( com ) rows in Table 2 show the results for this experiment . Models trained with common types as features perform worse than + ET ( orig ) which was expected as several original types are now clubbed into a single category ( e.g. LAW - > OTHER , LANG - > OTHER ) thus somewhat reducing the effectiveness of the feature . One surprising observation is the small difference between the performance on OntoNotes dataset , despite the fact that the number of type categories reduce from 18 + Other ( + ET ( orig ) ) to 4 + Other ( + ET ( com ) ) . This could either be because ( 1 ) the entities with corpus - specific types occur less frequently in Ontonotes , or ( 2 ) the baseline model does a good job in resolving them . Further research is required to understand this case which is out of scope for this work . Figure 2 : Type - prediction model .", "entities": [[66, 69, "TaskName", "Named Entity Recognition"], [192, 193, "DatasetName", "LAW"], [222, 223, "DatasetName", "OntoNotes"], [277, 278, "DatasetName", "Ontonotes"]]}
{"text": "Results shown in the previous section assume the presence of gold standard types during training as well as inference , which is often impractical in the real - world . Most of the new samples that a CR model would encounter would not include type information about the candidate mentions . Therefore , we set up an additional experiment to gauge the benefits of type information using predicted types . We introduce a baseline approach to infer the type of the mentions and then use these predictions in the + ET models , in place of the gold types , for coreference resolution .", "entities": [[101, 103, "TaskName", "coreference resolution"]]}
{"text": "Given the mention and its immediate context , i.e. the sentence it occurs in ( S = ... , c \u22122 , c \u22121 , e 1 , e 2 , ... , e n , c 1 , c 2 , ... ) , we add markers < ENT_START>/ < ENT_END > before/ after the beginning/ ending of the mention in the sentence . The new sequence ( S = ... , c \u22122 , c \u22121 , < ENT_START > , e 1 , e 2 , ... , e n , < ENT_END > , c 1 , c 2 , ... ) is tokenized using BERT tokenizer and passed through the BERT encoder . The output from which is then mean - pooled and passed through a fully - connected layer for classification . This architecture is motivated from ( Soares et al , 2019 ) who show that adding markers around entities before passing the sentence through BERT performs better for relation extraction .", "entities": [[109, 110, "MethodName", "BERT"], [115, 116, "MethodName", "BERT"], [162, 163, "MethodName", "BERT"], [166, 168, "TaskName", "relation extraction"]]}
{"text": "Table 6 shows the most frequently occurring entity - types for each of the genres in OntoNotes . In line with our intuition , we find that enity - type information helps the baseline in bc , bn , wb , and mz genres which have less skew in their entity - type distribution . Genres like bc , bn and wb , although dominated by PER entities , contain a substantial minority of other entity - types like ORG and GPE . Along the same lines , mz contains a majority of GPE entities but also enough entities with type PER and ORG to make type information a potentially useful feature for CR . However , two exceptions to this are the improved performance of + ET ( orig ) on tc ( highest skew ) and no significant improvement on nw ( lowest skew ) . These findings prompt further research in the future .", "entities": [[16, 17, "DatasetName", "OntoNotes"]]}
{"text": "Entity coreference in discourse often takes the surface form of pronouns ( PRP ) ( like she , they , that , it etc . ) or noun phrases ( NP ) ( like LA , John 's brother etc . ) In Table 4 , we compare the performance of our type prediction model on different types of pronouns , and noun phrases of varying length . We find that the model does well in predicting types for personal pronouns ( PRP ( pers . ) ) like she , he and noun phrases ( NP ) . However , it consistently underperforms on demonstrative pro - nouns ( PRP ( dem . ) ) like this , that , and it across all datasets . This reduced performance could be due to the fact that demonstrative pronouns do not contain any signal about the type of the entity they refer to . Therefore , the type prediction model has to solely rely on the context to make that decision . However , this is not the case with PRPs ( pers . ) and NPs where the mention string is usually a strong indicator of the type . This problem is worsened by the imbalance due to the small presence of PRP ( dem . ) mentions in difference CR datasets . Since , the model does not encounter enough PRPs ( dem . ) , it might not be able to learn to give high importance to context in these cases . This could be partially alleviated by creating a separate type - prediction path for PRP ( dem . ) where the mention span is masked before it is passed through the model . A model that is trained with masked mentions would focus more on the context for type prediction and thus could lead to better performance on PRPs ( dem . ) . One could also experiment with training the type - prediction model on all of the mentions across the four datasets . The common list of types introduced in this work would allow for the creation of a larger training - set that includes mentions from multiple corpora ( including external NER datasets ) which could provide enough signal for the model to better learn the common types for PRPs ( dem . ) . Both these approaches could further boost the results for CR with predicted entity - types , ultimately , reducing the gap between the scores in Table 2 and 5 . However , they are left as future work as they are out of scope for this paper .", "entities": [[52, 54, "TaskName", "type prediction"], [157, 159, "TaskName", "type prediction"], [303, 305, "TaskName", "type prediction"], [369, 370, "TaskName", "NER"]]}
{"text": "Table 7 provides an excerpt of an email from EmailCoref corpus . As shown , the baseline model predicts the coreference clusters for an organizer ( DIG ) and PricewaterhouseCoopers Calgary ( ORG ) incorrectly . For the former , the model mistakes it as a reference to your current home address ( LOC ) which is corrected by the entity - type aware models . For the latter , the baseline considers PricewaterhouseCoopers Calgary ( PCC ) as part of a new coreference cluster , even though it refers to the organization of the email 's sender which was previously referred to as we in the email . Models with access to gold type information ( + ET ( orig ) and + ET ( com ) ) are able to make that connection . + ET - pred ( orig ) , however , is unable to cluster PCC correctly which could be due to the fact that the type - prediction model incorrectly classifies the type of we as PER rather than ORG . This could lead to the CR model considering PCC ( ORG ) as a new entity in the discourse rather than a postcedent of we . This example demonstrates that sentencelevel context might not be sufficient in some cases for mention type - disambiguation . We intend to experiment with models that capture long - term context and leverage external knowledge in the future .", "entities": [[76, 77, "DatasetName", "PCC"], [150, 151, "DatasetName", "PCC"], [185, 186, "DatasetName", "PCC"]]}
{"text": "In this work , we show the importance of using entity - type information in neural coreference resolution ( CR ) models with contextualized embeddings like BERT . Models which leverage type information , annotated in the corpus , substantially outperform the baseline on four CR datasets by reducing the number of type mismatches in detected coreference clusters . Since , these datasets vary in number and categories of the types they define , we also experiment with mapping the original corpus types to four common types ( PER , ORG , LOC , FAC ) based on previous NER research that can be learnt more easily through large NER datasets . Models which use these common types perform slightly worse than original types but still show significant improvements over the baseline systems . The presence of gold standard types during CR inference is unlikely in practice . Therefore , we propose a model that infers the type of a mention given the mention span and its immediate context to use along side the proposed CR approach . In our evaluation , we find that using types predicted by our model for CR still performs significantly better than the baseline , thus offering stronger evidence that type information holds the potential for practical improvements for CR . Dropout 0.2 ( com ) experiments . These types are annotated in most of the named - entity recognition datasets and therefore are easier to model and learn via machine learning approaches . Tables A2 , A3 , A4 , A5 show the mapping between the original types of each coreference dataset used in our study to the reduced common types . The most drastic difference occurs for OntoNotes ( 19 - > 5 ) and Wi - kiCoref ( 8 - > 5 ) . OTHER type in WikiCoref is for freebase links that did not have an associated type stored in freebase , whereas NA is used for mentions which do not have a freebase link . For OntoNotes , NA refers to the mentions that did not get any type assigned to them even after the use of our cluster based type - propagation approach ( explained in Section 4 ) .", "entities": [[16, 18, "TaskName", "coreference resolution"], [26, 27, "MethodName", "BERT"], [99, 100, "TaskName", "NER"], [109, 110, "TaskName", "NER"], [217, 218, "MethodName", "Dropout"], [285, 286, "DatasetName", "OntoNotes"], [306, 307, "DatasetName", "WikiCoref"], [337, 338, "DatasetName", "OntoNotes"]]}
{"text": "A Position - aware Bidirectional Attention Network for Aspect - level Sentiment Analysis", "entities": [[11, 13, "TaskName", "Sentiment Analysis"]]}
{"text": "Aspect - level sentiment analysis aims to distinguish the sentiment polarity of each specific aspect term in a given sentence . Both industry and academia have realized the importance of the relationship between aspect term and sentence , and made attempts to model the relationship by designing a series of attention models . However , most existing methods usually neglect the fact that the position information is also crucial for identifying the sentiment polarity of the aspect term . When an aspect term occurs in a sentence , its neighboring words should be given more attention than other words with long distance . Therefore , we propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional GRU . PBAN not only concentrates on the position information of aspect terms , but also mutually models the relation between aspect term and sentence by employing bidirectional attention mechanism . The experimental results on SemEval 2014 Datasets demonstrate the effectiveness of our proposed PBAN model .", "entities": [[3, 5, "TaskName", "sentiment analysis"], [119, 121, "MethodName", "bidirectional GRU"]]}
{"text": "Sentiment analysis , also known as opinion mining ( Liu , 2012 ; Pang et al , 2008 ) , is a vital task in Natural Language Processing ( NLP ) . It divides the text into two or more classes according to the affective states and the subjective information of the text , and has received plenty of attention from both industry and academia . In this paper , we address the aspect - level sentiment analysis , which is a fine - grained task in the field of sentiment analysis . For instance , given the mentioned aspect terms { menu , server , specials } , and the sentence is \" The menu looked good , except for offering the Chilean Sea Bass , but the server does not offer up the specials that were written on the board outside . \" . For aspect term menu , the sentiment polarity is positive , but for aspect term server , the polarity is negative while for specials , the polarity is neutral . One important challenge in aspect - level sentiment analysis is how to model the semantic relationship between aspect terms and sentences . Traditional approaches have defined rich features about content and syntactic structures so as to capture the sentiment polarity ( Jiang et al , 2011 ) . However this kind of feature - based method is labor - intensive and highly depends on the quality of the features . Compared with these methods , neural network architectures are capable of learning features without feature engineering , and have been widely used in a variety of NLP tasks such as machine translation , question answering ( Andreas et al , 2016 ) and text classification ( Lai et al , 2015 ) . Recently , with the development of the neural networks , they are also applied to target - dependent sentiment analysis 1 , such as Target - Dependent LSTM ( TD - LSTM ) ( Tang et al , 2015 ) and Target - Connection LSTM ( TC - LSTM ) ( Tang et al , 2015 ) . However , these neural network - based methods can not effectively identify which words in the sentence are more important . Fortunately , attention mechanisms are an effective way to solve this problem . Attention , which is widely applied to Computer Vision ( CV ) and NLP fields , is an effective mechanism and has been demonstrated in image recognition ( Mnih et al , 2014 ) , machine translation Luong et al , 2015 ) and reading comprehension ( Hermann et al , 2015 ; Cui et al , 2016 ) . Therefore , some researchers have designed attention networks to address the aspect - level sentiment analysis and have obtained comparable results , such as AE - LSTM , ATAE - LSTM and IAN ( Ma et al , 2017 ) . However , these existing work ignores or does not explicitly model the position information of the aspect term in a sentence , which has been studied for improving performance in information retrieval ( IR ) . In , the occurrence positions of the query terms were modeled via kernel functions and then integrated into traditional IR models to boost the retrieval performance . By analyzing this aspect - level sentiment analysis task and the corresponding dataset , we find that when an aspect term occurs in a sentence , its neighboring words in the sentence should be given more attention than other words with long distance . Let us take \" It 's a perfect place to have an amazing indian food . \" as an example , when the aspect term is indian food , its corresponding sentiment polarity is positive . Intuitively , we can see that the neighboring word of the indian food ( i.e. \" amazing \" ) has a greater contribution to judge the sentiment polarity of the aspect term than other words with long distance such as \" to \" and \" have \" . Sometimes this intuitive idea of judging the sentiment polarity may be interpreted as a cognitive activity , which also can be rephrased in a quantum - like language model ( Niu et al , 2017 ) . To be specific , sentiment polarity may be interpreted as a quantum - like cognition state . Inspired by this , we go one step further and propose a position - aware bidirectional attention network ( PBAN ) based on bidirectional Gated Recurrent Units ( Bi - GRU ) . In addition to utilizing the position information , PBAN also mutually models the relationship between the sentence and different words in the aspect term by adopting a bidirectional attention mechanism . To be specific , our model consists of three components : 1 ) Obtaining position information of each word in corresponding sentence based on the current aspect term , then converting the position information into position embedding . 2 ) The PBAN composes of two Bi - GRU networks focusing on extracting the aspectlevel features and sentence - level features respectively . 3 ) Using the bidirectional attention mechanism to model the mutual relation between aspect term and its corresponding sentence . We evaluate our models on SemEval 2014 Datasets , and the results show that our models are more effective than other previous methods . The main contributions of our work can be summarized as follows : ( 1 ) We attempt to explicitly investigate the effectiveness of the position information of aspect term for aspect - level sentiment analysis . ( 2 ) We propose a position - aware bidirectional attention network ( PBAN ) based on Bi - GRU , which has been proved to be effective to improve the sentiment analysis performance . ( 3 ) We apply a bidirectional attention mechanism , which can enhance the mutual relation between the aspect term and its corresponding sentence , and prevent the irrelevant words from getting more attention .", "entities": [[0, 2, "TaskName", "Sentiment analysis"], [6, 8, "TaskName", "opinion mining"], [76, 78, "TaskName", "sentiment analysis"], [90, 92, "TaskName", "sentiment analysis"], [183, 185, "TaskName", "sentiment analysis"], [260, 262, "TaskName", "feature engineering"], [276, 278, "TaskName", "machine translation"], [279, 281, "TaskName", "question answering"], [289, 291, "TaskName", "text classification"], [317, 319, "TaskName", "sentiment analysis"], [326, 327, "MethodName", "LSTM"], [330, 331, "MethodName", "LSTM"], [343, 344, "MethodName", "LSTM"], [347, 348, "MethodName", "LSTM"], [416, 418, "TaskName", "image recognition"], [426, 428, "TaskName", "machine translation"], [435, 437, "TaskName", "reading comprehension"], [465, 467, "TaskName", "sentiment analysis"], [475, 476, "MethodName", "AE"], [477, 478, "MethodName", "LSTM"], [481, 482, "MethodName", "LSTM"], [483, 484, "MethodName", "IAN"], [522, 524, "TaskName", "information retrieval"], [561, 563, "TaskName", "sentiment analysis"], [737, 738, "DatasetName", "Inspired"], [767, 768, "MethodName", "GRU"], [848, 849, "MethodName", "GRU"], [940, 942, "TaskName", "sentiment analysis"], [962, 963, "MethodName", "GRU"], [974, 976, "TaskName", "sentiment analysis"]]}
{"text": "In this section , we describe the proposed model position - aware bidirectional attention network ( PBAN ) for aspect - level sentiment analysis and PBAN is shown in Figure 1 . In this paper , the set of sentiment polarity of the aspect term is { positive , negative , neutral } .", "entities": [[22, 24, "TaskName", "sentiment analysis"]]}
{"text": "As for how to model the position information of the aspect term in its corresponding sentence , inspired by the position encoding vectors used in ( Collobert et al , 2011 ; Zeng et al , 2014 ) , we define a position index sequence whose length is equal to the length of corresponding sentence . Suppose that if a word in the aspect term occurs in the sentence , then its position index will be marked as \" 0 \" , and the position index of other words will be represented as the relative distance to the current aspect term . p i = \uf8f4 \uf8f2 \uf8f4 \uf8f3 | i \u2212 j s | , i < j s 0 , j s \u2264 i \u2264 j e | i \u2212 j e | , i > j e ( 1 ) \u2026 Word embedding Position embedding \u2026 \u2026 N w 1 w 1 p N p Bi - GRU 1 h 2 h \u2026 N h Bi - GRU 1 t h 2 t h \u2026 M t h M t 1 t Mean Pool Attention Mechanism 1 \uf067 R h 11 \uf061 12 \uf061 1N \uf061 1 M \uf061 2 M \uf061 MN \uf061 2 \uf067 M \uf067 \u2026 \u2026 \u2026 \u2026 \u2a00 \u2026 \u2026 \u2a00 dot product dot product 1 s 2 s M s Term embedding \uf0c4 Figure 1 : The architecture of position - aware bidirectional attention network for aspect - level sentiment analysis ( PBAN ) . { w 1 , w 2 , ... , w N } represents the word embedding in a sentence whose length is N , and { t 1 , t 2 , ... , t M } represents the aspect term embedding whose length is M . { p 1 , p 2 , ... , p N } is the position embedding of the aspect term , which is concatenated to the word embedding . { h 1 , h 2 , ... , h N } denotes the hidden representation of inputs and { h t 1 , h t 2 , ... , h t M } indicates the hidden representation of aspect term . where , j s and j e denote the starting and ending indices of the aspect term respectively , and p i can be viewed as the relative distance of the i - th word in sentence to the aspect term . For example , given a sentence \" not only was the food outstanding but the little perks were great . \" , and the aspect term is food , then the position index sequence is represented as p = [ 4 , 3 , 2 , 1 , 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 ] . And its corresponding position embedding are obtained by looking up a position embedding matrix P R dp\u00d7N , which is randomly initialized , and updated during the training process . Here , d p denotes the dimension of position embedding and N indicates the length of the sentence . After the position index is converted to the embedding , the position embedding can model the different weights of words with different distance . From this example , it is obvious that the words with smaller relative distances ( such as \" outstanding \" ) play an more important role in judging the sentiment polarity of food . We can find that this process is basically consistent with the way people judge the sentiment polarity of the aspect term . Because we usually first observe the neighboring words of the aspect term , judging whether the neighboring words can show its sentiment polarity , after that we will focus on those words with long distance .", "entities": [[79, 80, "DatasetName", "0"], [120, 121, "DatasetName", "0"], [160, 161, "MethodName", "GRU"], [170, 171, "MethodName", "GRU"], [248, 250, "TaskName", "sentiment analysis"], [462, 463, "DatasetName", "0"]]}
{"text": "Bidirectional LSTMs have been successfully applied to various NLP tasks , and it models the context dependency with the forward LSTM and the backward LSTM . The forward L - STM handles the sentence from left to right , and the backward LSTM processes it in the reverse order . Therefore , we can obtain two hidden representation , and then concatenate the forward hidden state and backward hidden state of each word . In this paper , we choose to use bidirectional GRU since it performs similarly to bidirectional LSTM but has fewer parameters and lower computational complexity . Concretely , we firstly obtain the representation of each word in aspect term and sentence , and formalize the notations in our work . We suppose that a sentence consists of N words [ w 1 , w 2 , ... , w N ] and an aspect term contains M words [ t 1 , t 2 , ... , t M ] , then we get sentence embedding and aspect term embedding by looking up a word embedding matrix E R d\u00d7v respectively , where d denotes the dimension of the embedding , and v indicates the vocabulary size . Then we input aspect term embeddings into the left Bi - GRU to get the hidden contextual representa - tion , which consists of forward hidden state \u2212 h t i R d h and backward hidden state \u2212 h t i R d h , where d h denotes the number of hidden units . Finally , the hidden contextual representation of aspect term h t i is obtained by concatenating \u2212 h t i and \u2212 h t i , i.e. , h t i = [ \u2212 h t i ; \u2212 h t i ] R 2d h . For the right Bi - GRU structure , we take the concatenation of the position embedding and word embedding as the inputs , then we can obtain the final hidden contextual representation of the inputs , i.e. , h i = [ \u2212 h i ; \u2212 h i ] R 2d h .", "entities": [[20, 21, "MethodName", "LSTM"], [24, 25, "MethodName", "LSTM"], [42, 43, "MethodName", "LSTM"], [82, 84, "MethodName", "bidirectional GRU"], [89, 91, "MethodName", "bidirectional LSTM"], [168, 170, "TaskName", "sentence embedding"], [213, 214, "MethodName", "GRU"], [310, 311, "MethodName", "GRU"]]}
{"text": "In this section , we design a series of models to demonstrate the effectiveness of our PBAN model . Firstly , we design an ATAE - Bi - GRU model , whose structure is similar with ATAE - LSTM . The only difference between these two models is that ATAE - Bi - GRU uses the Bi - GRU structure rather than LSTM , and other design is the same as ATAE - LSTM . Next we design a BAN model without modeling position embedding , and it just utilizes the representation of aspect term and sentence . In BAN , we still adopt bidirectional attention mechanism to model the relation between aspect term and sentence as PBAN does . The only difference between BAN and PBAN is that BAN without taking the position embedding as a part of inputs . Moreover , we also design a PAN model , whose structure is similar with the ATAE - Bi - GRU model . PAN takes the concatenation of the aspect term embedding and the word embedding as the inputs of the Bi - GRU structure to obtain the hidden contextual representation , and then PAN utilizes this representation and the position embedding of the aspect term to calculate the attention weights , so as to effectively judge the sentiment polarity of an aspect term . From Table 3 , we can find that PBAN achieves the best performance among these models . Because Bi - GRU structure has a big advantage over LSTM , it is obvious that ATAE - Bi - GRU model performs better than ATAE - LSTM model . For PAN model , it outperforms ATAE - LSTM and ATAE - Bi - GRU models , but it is worse than BAN model . Compared with ATAE - Bi - GRU , the most difference is that PAN utilizes the position embedding to calculate the attention weights rather than the aspect term embedding like ATAE - Bi - GRU . Therefore , according to these three experimental results , we can prove the importance of the position information in aspect - level sentiment analysis task . As for BAN model , it outperforms IAN model while performs worse than PBAN model . Because Aspect term Sentence Polarity pizza This is one great place to eat pizza more out but not a good place for take - out pizza . positive take - out pizza This is one great place to eat pizza more out but not a good place for take - out pizza . negative compared with IAN model , BAN model can learn more semantic relationship between aspect term and sentence via bidirectional attention mechanism . However , it ignores the position information of aspect term when compared with PBAN model . As we expect , PBAN achieves the best performance among all these models . This is because in addition to fully considering the position information of the aspect term in its corresponding sentence , PBAN also considers the mutual relationship between aspect term and sentence , which is mainly achieved by a bidirectional attention mechanism .", "entities": [[28, 29, "MethodName", "GRU"], [38, 39, "MethodName", "LSTM"], [53, 54, "MethodName", "GRU"], [58, 59, "MethodName", "GRU"], [62, 63, "MethodName", "LSTM"], [73, 74, "MethodName", "LSTM"], [160, 161, "MethodName", "GRU"], [183, 184, "MethodName", "GRU"], [245, 246, "MethodName", "GRU"], [252, 253, "MethodName", "LSTM"], [262, 263, "MethodName", "GRU"], [269, 270, "MethodName", "LSTM"], [280, 281, "MethodName", "LSTM"], [286, 287, "MethodName", "GRU"], [303, 304, "MethodName", "GRU"], [331, 332, "MethodName", "GRU"], [355, 357, "TaskName", "sentiment analysis"], [366, 367, "MethodName", "IAN"], [431, 432, "MethodName", "IAN"]]}
{"text": "In this section , we will briefly review some research on sentiment analysis in recent years . The previous research can be divided into three directions : traditional machine learning methods , neural network methods and attention network methods .", "entities": [[11, 13, "TaskName", "sentiment analysis"]]}
{"text": "With the successful application of the attention mechanism in machine translation and reading comprehension , it is also applied to aspect - level sentiment analysis in recent years . examined the latent relatedness of the aspect term and sentiment polarity for aspect - level sentiment analysis . They designed an attention - based LSTM to learn aspect term embedding , and let the aspect term embedding participate in calculating the attention weights . Ma et al ( 2017 ) proposed a new attention model IAN , which considered the separate modeling of aspect terms and could interactively learn attention in the contexts and aspect terms . Despite the effectiveness of these attention mechanisms , they are coarse - grained and it is still challenging to identify different sentiment polarity at a fine - grained aspect level . However , our PBAN model makes full use of the position information of the aspect term , and PBAN uses a fine - grained bidirectional attention mechanism to model the mutual relationship between the sentence and each word in the aspect term , identifying the importance of the word in the aspect term to obtain a more effective sentence representation as described in Section 1 .", "entities": [[9, 11, "TaskName", "machine translation"], [12, 14, "TaskName", "reading comprehension"], [23, 25, "TaskName", "sentiment analysis"], [44, 46, "TaskName", "sentiment analysis"], [53, 54, "MethodName", "LSTM"], [84, 85, "MethodName", "IAN"]]}
{"text": "In this paper , we have proposed a position - aware bidirectional network ( PBAN ) based on Bi - GRU for aspect - level sentiment analysis . The main idea of PBAN is to utilize the position embedding of aspect term for calculating the attention weights . Moreover , PBAN adopts a bidirectional attention mechanism , which is not only capable of mutually modeling the relation between sentence and different words in aspect term , but also takes advantage of the position information to better judge the sentiment polarity of aspect term . Experimental results on SemEval 2014 Datasets demonstrate that our proposed models can learn effective features and obtain superior performance over the baseline models .", "entities": [[20, 21, "MethodName", "GRU"], [25, 27, "TaskName", "sentiment analysis"]]}
{"text": "Adversarial Multi - lingual Neural Relation Extraction", "entities": [[5, 7, "TaskName", "Relation Extraction"]]}
{"text": "Multi - lingual relation extraction aims to find unknown relational facts from text in various languages . Existing models can not well capture the consistency and diversity of relation patterns in different languages . To address these issues , we propose an adversarial multi - lingual neural relation extraction ( AMNRE ) model , which builds both consistent and individual representations for each sentence to consider the consistency and diversity among languages . Further , we adopt an adversarial training strategy to ensure those consistent sentence representations could effectively extract the language - consistent relation patterns . The experimental results on real - world datasets demonstrate that our AMNRE model significantly outperforms the state - of - the - art models . The source code of this paper can be obtained from https://github.com/thunlp/AMNRE .", "entities": [[3, 5, "TaskName", "relation extraction"], [47, 49, "TaskName", "relation extraction"]]}
{"text": "Relation extraction ( RE ) is a crucial task in NLP , which aims to extract semantic relations between entity pairs from the sentences containing them . For example , given an entity pair ( Bill Gates , Microsoft ) and a sentence \" Bill Gates is the co - founder and CEO of Microsoft \" , we want to figure out the relation Founder between the two entities . RE can potentially benefit many applications , such as knowledge base construction ( Zhong et al , 2015 ; Han et al , 2018 ) and question answering ( Xiang et al , 2017 ) . Recently , neural models have shown their great abilities in RE . Zeng et al ( 2014 ) introduce a convolutional neural network ( CNN ) to extract relational facts with automatically learning features from text . To address the issue of lack of data , Zeng et al ( 2015 ) incorporate multi - instance learning with a piece - wise convolutional neural network ( PCNN ) to extract relations in distantly supervised data . Because distant supervision suffer from wrong labeling problems , Lin et al ( 2016 ) further employ a sentence - level selective attention to filter out those noisy sentences in distantly supervised data and achieve state - of - the - art performance . All these neural relation extraction ( NRE ) models merely focus on extracting relational facts from mono - lingual data , ignoring the rich information in multi - lingual data . propose a multi - lingual attention - based neural relation extraction ( MNRE ) model , which considers the consistency and complementarity in multi - lingual data . MNRE builds a sentence representation for each sentence in various languages and employs a multi - lingual attention to capture the pattern consistency and complementarity among languages . Although MNRE achieves great success in multi - lingual RE , it still has some problems . MNRE learns a single representation for each sentence in various languages , which can not well capture both the consistency and diversity of relation patterns in different languages . Moreover , MNRE simply utilizes a multi - lingual attention mechanism and a global relation predictor to capture the consistent relation patterns among multiple languages . From the experimental data , we find that the sentence representations in different languages are still far from each other and linearly separable . Therefore , it is hard for the multi - To address these issues , we propose an adversarial multi - lingual NRE ( AMNRE ) model . As shown in Figure 1 , for an entity pair , we encode its corresponding sentences in various languages through neural sentence encoders . For each sentence , we build an individual representation to grasp its individual language features and a consistent representation to encode its substantially consistent features among languages . Further , we adopt an adversarial training strategy to ensure AMNRE can extract the language - consistent relation patterns from the consistent representations . Orthogonality constraints are also adopted to enhance differences between individual representations and consistent representations for each language . x1 x 2 x1 x 1 1 E I 1 E I 1 E C 1 E C x 2 x2 x 1 x2 E I 2 E C 2 E C 2 E I 2 1 2 1 1 1 2 2 2 2 1 s1 s2 In experiments , we take Chinese and English to show the effectiveness of AMNRE . The experimental results show that AMNRE outperforms all baseline models significantly by explicitly encoding the consistency and diversity among languages . And we further give a case study and an ablation study to demonstrate the adversarial training strategy could help AMNRE to capture language - consistent relation patterns .", "entities": [[0, 2, "TaskName", "Relation extraction"], [96, 98, "TaskName", "question answering"], [229, 231, "TaskName", "relation extraction"], [266, 268, "TaskName", "relation extraction"]]}
{"text": "Traditional supervised RE models ( Zelenko et al , 2003 ; Socher et al , 2012 ; Santos et al , 2015 ) heavily rely on abundant amounts of high - quality annotated data . Hence , Mintz et al ( 2009 ) propose a distantly supervised model for RE . Distant supervision aligns knowledge bases ( KBs ) and text to automatically annotate data , and thus distantly supervised models inevitably suffer from wrong labeling problems . To alleviate the noise issue , Riedel et al ( 2010 ) and Hoffmann et al ( 2011 ) propose multi - instance learning ( MIL ) mechanisms for single - label and multi - label problems respectively . Then , Zeng et al ( 2015 ) attempt to integrate neural models into distant supervision . Lin et al ( 2016 ) further propose a sentence - level attention to jointly consider all sentences containing same entity pairs for RE . The attention - based neural relation extraction ( NRE ) model has become a foundation for some recent works ( Ji et al , 2017 ; Zeng et al , 2017 ; Liu et al , 2017b ; Wu et al , 2017 ; Feng et al , 2018 ; Zeng et al , 2018 ) . Most existing RE models are devoted to extracting relations from mono - lingual data and ignore information lying in text of multiple languages . Faruqui and Kumar ( 2015 ) and Verga et al ( 2016 ) first attempt to adopt multi - lingual transfer learning for RE . However , both of these works learn predictive models on a new language for existing KBs , without fully leveraging semantic information in text . Then , construct a multi - lingual NRE ( MNRE ) model to jointly represent text of multiple languages to enhance RE . In this paper , we propose a novel multi - lingual NRE framework to explicitly encode language consistency and diversity into different semantic spaces , which can achieve more effective representations for RE . et al ( 2015 ) propose adversarial training for image classification tasks . Afterwards , Goodfellow et al ( 2014 ) propose a mature adversarial training framework and use the framework to train generative models . Adversarial networks have recently been used as methods to narrow probability distributions and proven effective in some tasks . In domain adaptation , Ganin et al ( 2016 ) and Bousmalis et al ( 2016 ) adopt adverarial training strategies to transfer the features of one source domain to its corresponding target domain .", "entities": [[164, 166, "TaskName", "relation extraction"], [242, 243, "DatasetName", "Kumar"], [260, 262, "TaskName", "transfer learning"], [356, 358, "TaskName", "image classification"], [403, 405, "TaskName", "domain adaptation"]]}
{"text": "Inspired by Ganin et al ( 2016 ) , adversarial training has also been explored in some typical NLP tasks for multi - feature fusion . Park and I m ( 2016 ) propose a multi - modal representation learning model based on adversarial training . Then , Liu et al ( 2017a ) employ adversarial training to construct a multi - task learning model for text classification by extending the original binary adversarial training to the multiclass version . And a similar adversarial framework is also adapted by to learn features from different datasets for chinese word segmentation . In this paper , we adopt adversarial training to boost feature fusion to grasp the consistency among different languages .", "entities": [[0, 1, "DatasetName", "Inspired"], [38, 40, "TaskName", "representation learning"], [60, 64, "TaskName", "multi - task learning"], [66, 68, "TaskName", "text classification"], [96, 99, "TaskName", "chinese word segmentation"]]}
{"text": "The input layer transforms all input words in the sentence into corresponding input embeddings by concatenating their word embeddings and position embeddings . The word embeddings are pre - trained by Skip - Gram ( Mikolov et al , 2013 ) . The position embeddings are a widely - used technique in RE proposed by Zeng et al ( 2014 ) , representing each word 's relative distances to the two entities into two k p - dimensional vectors . The input layer represents the input sentence as a k i - dimensional embedding sequence x = { w 1 , w 2 , . . . } , where k i = k w + k p \u00d72 , k w and k p are the dimensions of word embeddings and position embeddings respectively .", "entities": [[17, 19, "TaskName", "word embeddings"], [24, 26, "TaskName", "word embeddings"], [129, 131, "TaskName", "word embeddings"]]}
{"text": "After representing the input sentence as a k i - dimensional embedding sequence , we select both CNN ( Zeng et al , 2014 ) and RNN to encode the input embedding sequence x = { w 1 , w 2 , . . . } to its sentence embedding . CNN slides a convolution kernel with the window size m to extract the k h - dimensional local features , hi = CNN w i\u2212 m\u22121 2 , . . . , w i+ m\u22121 2 . ( 1 ) A max - pooling is then adopted to obtain the final sentence embedding y as follows , [ y ] j = max { [ h1 ] j , . . . , [ hn ] j } . ( 2 ) RNN is mainly designed for modeling sequential data . In this paper , we adopt bidirectional RNN ( Bi - RNN ) to incorporate information from both sides of the sentence sequence as follows , \u2212 h i = RNN f ( xi , \u2212 h i\u22121 ) , \u2212 h i = RNN b ( xi , \u2212 h i+1 ) , ( 3 ) where \u2212 h i and \u2212 h i are the k h - dimensional hidden states at the position i of the forward and backward RNN respectively . RNN ( ) is the recurrent unit and we select gated recurrent unit ( GRU ) ( Cho et al , 2014 ) as the recurrent unit in this paper . We concatenate both the forward and backward hidden states as the sentence embedding y , y = [ \u2212 h n ; \u2212 h 1 ] . ( 4 ) For simplicity , we denote such a sentence encoding operation as the following equation , y = E ( x ) . ( 5 ) For each sentence x i j S j , we adopt the individual sentence encoder E I j and the consistent sentence encoder E C j to embed the sentence into its individual and consistent representations respectively , { y 1 j , y 2 j , . . . } = { E I j ( x 1 j ) , E I j ( x 2 j ) , . . . } , { \u0233 1 j , \u0233 2 j , . . . } = { E C j ( x 1 j ) , E C j ( x 2 j ) , . . . } . ( 6 )", "entities": [[48, 50, "TaskName", "sentence embedding"], [54, 55, "MethodName", "convolution"], [102, 104, "TaskName", "sentence embedding"], [237, 240, "MethodName", "gated recurrent unit"], [241, 242, "MethodName", "GRU"], [269, 271, "TaskName", "sentence embedding"]]}
{"text": "Following the settings of previous works , we use the pre - trained word embeddings learned by Skip - Gram as the initial word embeddings . We implement the MNRE framework proposed by by ourselves . For fair comparision , we set most of the hyperparameters following . We list the best setting of hyperparameters in Table 2 .", "entities": [[13, 15, "TaskName", "word embeddings"], [23, 25, "TaskName", "word embeddings"]]}
{"text": "To further show the effectiveness of our proposed model to extract the language - consistent semantic information , we give an example in Table 6 . We adopt the cosine similarity to measure the similarity between sentence embeddings encoded by consistent encoders . The first sentence in the middle column is the standard Chinese translation of the left sentence , thus they share the same semantic information . We observe that in our proposed model , the feature embedding similarity between these two sentences are significantly higher than the other English sentences sharing entity pair and relational fact but differing in semantics . It indicates that sentences in different languages containing similar semantics can be indeed encoded into adjacent places of the consistent space in our framework .", "entities": [[36, 38, "TaskName", "sentence embeddings"]]}
{"text": "In this paper , we introduce a novel adversarial multi - lingual neural relation extraction model ( AMNRE ) . AMNRE builds both individual and consistent representations for each sentence to consider the consistency and diversity of relation patterns among languages . It also employs an adversarial training strategy and orthogonality constraints to ensure the consistent representations could extract the languageconsistent features to extract relations . The experimental results on real - world datasets demonstrate that", "entities": [[13, 15, "TaskName", "relation extraction"]]}
{"text": "our AMNRE could effectively encode the consistency and diversity among languages , and achieves state - of - the - art performance in relation extraction . We will explore the following directions as our future work : ( 1 ) AMNRE can be also implemented in the scenario of multiple languages , and this paper shows the effectiveness of AMNRE on the dataset with two languages ( English and Chinese ) . In the future , we will explore AMNRE in much more other languages such as French , Spanish , and so on . ( 2 ) AMNRE simply aligns the sentences with similar semantics in different languages with an adversarial training strategy . In fact , machine translation is a typical approach to align sentences in various languages . In the future , we will combine machine translation with our model to further improve the extraction performance .", "entities": [[23, 25, "TaskName", "relation extraction"], [118, 120, "TaskName", "machine translation"], [138, 140, "TaskName", "machine translation"]]}
{"text": "It is well understood that recognizing whether a speaker is ironic or sarcastic is essential to understanding their actual sentiments and beliefs . For instance , the utterance \" pictures of holding animal carcasses are so flattering \" is an expression of verbal irony , where the speaker has a negative sentiment towards \" pictures of holding animal carcasses \" , but uses the positive sentiment word \" flattering \" . This inherent characteristic of verbal irony is called semantic incongruity - incongruity between the literal evaluation and the context ( e.g. , between the positive sentiment words and the negative situation in this example ) . Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context , author context , visual context , or cognitive features ( Davidov et al , 2010 ; Maynard and Greenwood , 2014 ; Wallace et al , 2014 ; Joshi et al , 2015 ; Bamman and Smith , 2015 ; Muresan et al , 2016 ; Amir et al , 2016 ; Mishra et al , 2016 ; Ghosh and Veale , 2017 ; Felbo et al , 2017 ; Hazarika et al , 2018 ; Tay et al , 2018 ; Oprea and Magdy , 2019 ) . Such approaches have focused their analysis on the speakers ' beliefs and intentions for using irony ( Attardo , 2000 ) . However , sarcasm and verbal irony are types of interactional phenomena with specific perlocutionary effects on the hearer ( Haverkate , 1990 ) . Thus , we argue that , besides recognizing the speaker 's sarcastic / ironic intent , it is equally important to understand how the hearer interprets the speaker 's sarcastic / ironic message . For the above utterance , the strength of negative sentiment perceived by the hearer depends on whether they interpret the speaker 's actual meaning as \" picture . . . are not flattering \" vs. \" pictures . . . are so gross \" ( Table 1 ) . The intensity of negative sentiment is higher in the latter interpretation than in the former . Kreuz ( 2000 ) noted that most studies in linguistics and psychology have conducted experiments analyzing reaction times ( Gibbs , 1986 ; Katz et al , 2004 ) or situational context ( Ivanko and Pexman , 2003 ) , featuring a setup with in vitro data aimed at testing the validity of specific theories of irony . Instead , our study adopts a naturalistic approach to understand hearers ' reception of irony looking at what linguistic strategies are recurrently used by hearers to interpret the non - literal meaning underlying ironic utterances . We leverage the crowdsourcing task introduced by Ghosh et al ( 2015 ) for their work on detecting whether a word has a literal or sarcastic in - terpretation , later adopted by Peled and Reichart ( 2017 ) . The task is framed as follows : given a speaker 's ironic message , five annotators ( e.g. , Turkers on Amazon Mechanical Turk ( MTurk ) ) are asked to verbalize their interpretation of the speaker 's ironic message ( i.e. , their understanding of the speaker 's intended meaning ) ( see Table 1 ; S i m denotes the speaker 's ironic message , while H int denotes the hearer 's interpretation of that ironic message ) . The crowdsourcing experiments are reported in Section 2 . The paper makes three contributions . First , we propose a data - driven typology of linguistic strategies that hearers use to interpret ironic messages and discuss its relevance in verifying theoretical frameworks of irony ( Section 4 ) . Second , we propose computational models to capture these strategies ( Section 5 ) . Third , we present two studies that aim to answer two questions : ( 1 ) does the type of semantic incongruity in the ironic message ( explicit vs. implicit ; see Section 3 ) influence the choice of interpretation strategies by the hearers ? ( Section 6.2 ) ; ( 2 ) do interpretation strategies of verbal irony vary by hearers ? We make all datasets and code available . 1", "entities": [[121, 123, "TaskName", "sarcasm detection"]]}
{"text": "To generate a parallel dataset of speakers ' ironic messages and hearers ' interpretations we conduct a crowdsourcing experiment . Given a speaker 's ironic message ( S i m ) , five Turkers ( hearers ) on MTurk are asked to verbalize their interpretation of the speaker 's ironic message ( i.e. , their understanding of the speaker 's intended meaning ) ( H int ) . The design of the MTurk task was first introduced by Ghosh et al ( 2015 ) , who use the resulting dataset to identify words that can have both a literal and a sarcastic sense . Peled and Reichart ( 2017 ) employed similar design to generate a parallel dataset to use for generating interpretations of sarcastic messages using machine translation approaches . They use skilled annotators in comedy writing and literature paraphrasing and give them the option not to rephrase ( we refer to Peled and Reichart ( 2017 ) 's dataset as SIGN ) . We perform this new crowdsourcing task and do not rely entirely on the above two datasets for two reasons : ( 1 ) we focus on verbal irony , and ( 2 ) we always require an interpretation from the Turkers . Un - like the above two studies , the main goal of our research is to analyze the linguistics strategies employed by hearers in interpreting verbal irony . We collected messages that express verbal irony from Twitter using the hashtags # irony , # sarcastic , and # sarcasm . We chose Twitter as a source since the presence of the hashtags allows us to select sentences where the speaker 's intention was to be ironic . Furthermore , even though Twitter users can not be considered representative of the entire population , they are unlikely to be skewed with respect to topics or gender . We manually checked and kept 1 , 000 tweets that express verbal irony . We do not draw any theoretical distinction between sarcasm and irony since we can not assume that Twitter users also differentiate between # irony and # sarcasm , blurred even in scholarly literature . The Turkers were provided with detailed instructions and examples of the task including the standard definition of verbal irony taken from the Merriam - Webster dictionary ( \" use of words to express something other than and especially the opposite of the literal meaning \" ) . We decided to suggest them a guiding definition for two reasons . First , hearers do not usually focus on literal vs. non literal meaning , as shown by studies measuring processing times for both types of statements ( Inhoff et al , 1984 ) . Therefore , when asked to rephrase the speakers ' intended meaning , hearers would have probably come up with sentences expressing the speaker 's imagined discursive goals , rather than disclosing their perceived literal meaning . Second , it is reasonable to assume that Turkers would have looked up the standard meaning of ironic utterance given by an online dictionary to ease up their task , possibly coming up with biased definitions . The Turkers were instructed to consider the entire message in their verbalization to avoid asymmetry in length between the S i m and H int . We obtained a dataset of 5 , 000 S i m - H int pairs where five Turkers rephrase each S i m . A total of 184 Turkers participated in the rephrasing task . Table 1 shows examples of speaker 's ironic messages ( S i m ) and their corresponding hearers ' interpretations ( H i int ) . Next , we ran a second MTurk task to verify whether the generated H int messages are plausible interpretations of the ironic messages . This time we employ three Turkers per task and only Turkers who were not involved in the content generation task were allowed to perform this task . We observe that Turkers labeled 5 % ( i.e. , 238 verbalizations ) of H int s as invalid and low quality ( e.g. , wrong interpretation ) . For both tasks , we allowed only qualified Turkers ( i.e. , at least 95 % approval rate and 5 , 000 approved HITs ) , paid 7 cents / task and gave sixty minutes to complete each task . The final dataset contains 4 , 762 pairs S i m - H int . Sim H 1 int H 2 int H 3 int 1 .", "entities": [[127, 129, "TaskName", "machine translation"]]}
{"text": "Most NLP research on verbal irony or sarcasm has focused on the task of sarcasm detection treating it as a binary classification task using either the utterance in isolation or adding contextual information such as conversation context , author context , visual context , or cognitive features ( Gonz\u00e1lez - Ib\u00e1\u00f1ez et al , 2011 ; Liebrecht et al , 2013 ; Wallace et al , 2014 ; Zhang et al , 2016 ; Ghosh and Veale , 2016 ; Schifanella et al , 2016 ; Xiong et al , 2019 ; Castro et al , 2019 ) . Unlike this line of work , our research focuses on how the hearer interprets an ironic message . The findings from our study could have multiple impacts on the sarcasm detection task . First , interpretation strategies open up a scope of \" graded interpretation \" of irony instead of only a binary decision ( i.e. , predicting the strength of irony ) . Second , nature of semantic incongruence and stereotype irony situations can be useful features in irony detection . Recently , Peled and Reichart ( 2017 ) proposed a computational model based on SMT to generate interpretations of sarcastic messages . We aim to deepen our understanding of such interpretations by introducing a typology of linguistic strategies . We study the distribution of these strategies via both hearer - dependent and messagedependent interpretations . Psycholinguistics studies that have dealt with the hearers ' perception , have mainly focused on how ironic messages are processed : through the analysis of reaction times ( Gibbs , 1986 ; Katz et al , 2004 ) , the role of situational context ( Ivanko and Pexman , 2003 ) and in tackling speaker - hearer social relations by annotating ironic texts from different genres ( Burgers , 2010 ) . However , no attention has been paid to correlations between how ironic message is expressed and how it is interpreted by the hearer , including what linguistic strategies the hearers employ .", "entities": [[14, 16, "TaskName", "sarcasm detection"], [128, 130, "TaskName", "sarcasm detection"]]}
{"text": "We thank Rituparna Mukherjee , Daniel Chaparro , Pedro P\u00e9rez S\u00e1nchez , and Renato Augusto Vieira Nishimori who helped us in annotating as well as in running experiments . This paper partially based on the work supported by the DARPA - DEFT program . The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government .", "entities": [[39, 40, "DatasetName", "DARPA"]]}
{"text": "The current automated event understanding task has been overly simplified to be local and sequential . Real world events , such as disease outbreaks and terrorist attacks , have multiple actors , complex timelines , intertwined relations and multiple possible outcomes . Understanding such events requires knowledge in the form of a library of event schemas , capturing the progress of time , and performing global inference for event prediction . For example , regarding the 2019 protest in Hong Kong International Airport , a typical question from analysts would be \" How long will the flights being canceled ? \" This requires an event understanding system to match events to schema representations and reason about what might happen next . The airport protest schema would be triggered by \" protest \" and \" flight cancellation \" , and evidence of protesters ( e.g. , the number of protesters , the instruments being used , etc ) will suggest a CEO resignation event , or a flight rescheduling event , or continuous flight cancellation events with respective probabilities . Comprehending such a news story requires following a timeline , identifying key events and tracking characters . We refer to such a \" story \" as a complex event , e.g. , the Kabul ambulance bombing event . Its complexity comes from the inclusion of multiple atomic events ( and their arguments ) , relations and temporal order . A complex event schema can be used to define the typical structure of a particular type of complex event , e.g. , carbombing . This leads us to the new task that we address in this paper : temporal complex event schema induction . Figure 1 shows an example schema about car - bombing with multiple temporal dependencies between events . Namely , the occurrence of one event may depend on multiple events . For example , the ASSEMBLE event happens after buying both the bomb materials and the vehicle . Also , there may be multiple events following an event , such as the multiple consequences of the ATTACK event in Figure 1 . That is to say , \" the future is not one - dimensional \" . Our automatically induced probabilistic complex event schema can be used to forecast event abstractions into the future and thus provide a comprehensive understanding of evolving situations , events , and trends . For each type of complex event , we aim to induce a schema library that is probabilistic , temporally organized and semantically coherent . Low level atomic event schemas are abundant , and can be part of multiple , sparsely occurring , higher - level schemas . We propose a Temporal Event Graph Model , an auto - regressive graph generation model , to reach this goal . Given a currently extracted event graph , we generate the next event type node with its potential arguments , such as the ARREST event in Figure 2 , and then propagate edge - aware information following temporal orders . After that , we employ a copy mechanism to generate coreferential arguments , such as the DETAINEE argument is the ATTACKER of the previous ATTACK event , and build relation edges for them , e.g. , PART WHOLE relation between the PLACE arguments . Finally , temporal dependencies are determined with argument connections considered , such as the temporal edge showing that ARREST is after ATTACK . Our generative model serves as both a schema library and a predictive model . Specifically , we can probe the model to generate event graphs unconditionally to obtain a set of schemas . We can also pass partially instantiated graphs to the model and \" grow \" the graph either forward or backward in time to predict missing events , arguments or relations , both from the past and in the future . We propose a set of schema matching metrics to evaluate the induced schemas by comparing with human - created schemas and show the power of the probabilistic schema in the task of future event prediction as an extrinsic evaluation , to predict event types that are likely to happen next . We make the following novel contributions : This is the first work to induce probabilistic temporal graph schemas for complex events Symbol Meaning G G Instance graph of a complex event S S Schema graph of a complex event type e E Event node in an instance graph v V Entity node in an instance graph ei , e l Temporal ordering edge between events ei and e l , indicating ei is before e l ei , a , vj Argument edge , indicating vj plays argument role a in the event ei vj , r , v k Relation edge between entities vj and v k , and r is the relation type A ( e ) Argument role set of event e , defined by the IE ontology \u03a6E The type set of events \u03a6V The type set of entities \u03c6 ( ) A mapping function from a node to its type", "entities": [[82, 83, "DatasetName", "Airport"], [459, 461, "TaskName", "graph generation"], [828, 829, "MethodName", "ontology"]]}
{"text": "To induce schemas for a complex event type , such as car - bombing , we construct a set of instance graphs , where each instance graph is about one complex event , such as Kabul ambulance bombing . We first identify a cluster of documents that describes the same complex event . In this paper , we treat all documents linked to a single Wikipedia page as belonging to the same complex event , detailed in 4.1 . We use OneIE , a state - of - the - art Information Extraction system , to extract entities , relations and events , and then perform crossdocument entity ( Pan et al , 2015 ( Pan et al , , 2017 and event coreference resolution ( Lai et al , 2021 ) over the document cluster of each complex event . We further conduct event - event temporal relation extraction ( Ning et al , 2019 ; Wen et al , 2021b ) to determine the order of event pairs . We run the entire pipeline following ( Wen et al , 2021a ) 3 , and the detailed extraction performance is reported in the paper . After extraction , we construct one instance graph for each complex event , where coreferential events or entities are merged . We consider the isolated events as irrelevant nodes in schema induction , so they are excluded from the instance graphs during graph construction . Considering schema graphs focus on type - level abstraction , we use type label and node index to represent each node , ignoring the mention level information in these instance graphs .", "entities": [[122, 125, "TaskName", "event coreference resolution"], [147, 150, "TaskName", "temporal relation extraction"], [239, 241, "TaskName", "graph construction"]]}
{"text": "Given an instance graph G , we regard the schema as the hidden knowledge to guide the generation of these graphs . To this end , we propose a temporal event graph model that maximizes the probability of each instance graph , parameterized by G G p ( G ) . At each step , based on the previous graph G < i , we predict one event node e i with its arguments to generate the next graph G i , p ( G ) = | E | i=0 p ( G i | G < i ) . We factorize the probability of generating new nodes and edges as : p ( G i | G < i ) = p ( e i | G < i ) a j A ( e i ) p ( e i , a j , v j | e i , a j ) v k G < i p ( v j , r , v k | v j , v k ) e l G < i p ( e i , e l | e i , e l ) . ( 1 ) As shown in Figure 2 , an event node e i is generated first according to the probability p ( e i | G < i ) . We then add argument nodes based on the IE ontology . We also predict relation v j , r , v k between the newly generated node v j and the existing nodes v k G < i . After knowing the shared and related arguments , we add a final step to predict the temporal relations between the new event e i and the existing events e l G < i . In the traditional graph generation setting , the order of node generation can be arbitrary . However , in our instance graphs , event nodes are connected through temporal relations . We order events as a directed acyclic graph ( DAG ) . Considering each event may have multiple events both \" before \" and \" after \" , we obtain the generation order by traversing the graph using Breadth - First Search . We also add dummy START / END event nodes to indicate the starting / ending of the graph generation . At the beginning of the generation process , the graph G 0 has a single start event node e [ SOG ] . We generate e [ EOG ] to signal the end of the graph .", "entities": [[237, 238, "MethodName", "ontology"], [304, 306, "TaskName", "graph generation"], [392, 394, "TaskName", "graph generation"], [406, 407, "DatasetName", "0"]]}
{"text": "To determine the event type of the newly generated event node e i , we apply a graph pooling over all events to get the current graph representation g i , g i = Pooling ( { e 0 , , e i\u22121 } ) . We use bold to denote the latent representations of nodes and edges , which will be initialized as zeros and updated at each generation step via message passing in 3.4 . We adopt a mean - pooling operation in this paper . After that , the event type is predicted through a fully connected layer , p ( e i | G < i ) = exp ( W \u03c6 ( e i ) g i ) \u03c6 \u03a6 E \u222a [ EOG ] exp ( W \u03c6 g i ) . Once we know the event type of e i , we add all of its arguments in A ( e i ) defined in the IE ontology as new entity nodes . For example , in Figure 2 , the new event e i is an ARREST event , so we add three argument nodes for DETAINEE , JAILOR , and PLACE respectively . The edges between these arguments and event e i are also added into the graph .", "entities": [[38, 39, "DatasetName", "0"], [164, 165, "MethodName", "ontology"]]}
{"text": "After updating the node representations , we detect the entity type of each argument , and also predict whether the argument is coreferential to existing entities . Inspired by copy mechanism ( Gu et al , 2016 ) , we classify each argument node v j to either a new entity with entity type \u03c6 ( v j ) , or an existing entity node in the previous graph G < i . For example , in Figure 2 , the DETAINEE should be classified to the existing ATTACKER node , while JAILOR node is classified as PERSON . Namely , p ( e i , a j , v j | e i , a j ) = p ( e i , a j , v j , g | e i , a j ) if v j is new , p ( e i , a j , v j , c | e i , a j ) otherwise , where p ( e i , a j , v j , g | e i , a j ) is the generation probability , classifying the new node to its entity type \u03c6 ( v j ) : p ( e i , a j , v j , g | e i , a j ) = exp ( W \u03c6 ( v j ) v j ) Z The copy probability p ( e i , a j , v j , c | e i , a j ) selects the coreferential entity v from the entities in existing graph , denoted by V < i , p ( e i , a j , v j , c | e i , a j ) = exp ( W v v j ) Z. Here , Z is the shared normalization term , Z = \u03c6 \u03a6 V exp ( W \u03c6 v j ) + v V < i exp ( W v v j ) If determined to copy , we merge coreferential entities in the graph .", "entities": [[27, 28, "DatasetName", "Inspired"]]}
{"text": "In this phase , we determine the virtual edges to be kept and assign relation types to them , such as PARTWHOLE relation in Figure 2 . We model the relation edge generation probability as a categorical distribution over relation types , and add [ O ] ( OTHER ) to the typeset R to represent that there is no relation edge : p ( v j , r , v k | v j , v k ) = exp ( MLP r ( v j \u2212 v k ) ) r R\u222a [ O ] exp ( MLP r ( v j \u2212 v k ) ) We use two hidden layers with ReLU activation functions to implement the MLP .", "entities": [[82, 83, "DatasetName", "MLP"], [99, 100, "DatasetName", "MLP"], [115, 116, "MethodName", "ReLU"], [121, 122, "DatasetName", "MLP"]]}
{"text": "We conduct experiments on two datasets for both the general scenario and a more specific scenario . We adopt the DARPA KAIROS 6 ontology , a newly defined fine - grained ontology for Schema Learning , with 24 entity types , 46 relation types , 67 event types , and 85 argument roles . 7 Our schema induction method does not rely on any specific ontology , only the IE system is trained on a given ontology to create the instance event graphs . General Schema Learning Corpus : The Schema Learning Corpus , released by LDC ( LDC2020E25 ) , includes 82 types of complex events , such as Disease Outbreak , Presentations and Shop Online . Each complex event is associated with a set of source documents . This data set al o includes ground - truth schemas created by LDC annotators , which were used for our intrinsic evaluation . IED Schema Learning Corpus : The same type of complex events may have many variants , which depends on the different types of conditions and participants . In order to evaluate our model 's capability at capturing uncertainty and multiple hypotheses , we decided to dive deeper into one scenario and chose the improvised explosive device ( IED ) as our case study . We first collected Wikipedia articles that describe 4 types of complex events , i.e. , Car - bombing IED , Drone Strikes IED , Suicide IED and General IED . Then we followed ( Li et al , 2021 ) to exploit the external links to collect the additional news documents with the corresponding complex event type . The ground - truth schemas for this IED corpus are created manually , through a schema curation tool ( Mishra et al , 2021 ) . Only one human schema graph was created for each complex event type , resulting in 4 schemas . In detail , for each complex event type , we presented example instance graphs and the ranked event sequences to annotators to create human ( ground truth ) schemas . The event sequences are generated by traversing the instance graphs , and then sorted by frequency and the number of arguments . Initially we assigned three annotators ( IE experts ) to each create a version of the schema and then the final schema was merged through discussion . After that , two annotators ( linguists ) performed a two - pass revision . Human curation focuses on merging and trimming steps by validating them using the reference instance graphs . Also , temporal dependencies between steps were further refined , and coreferential entities and their relations were added during the curation process . To avoid bias from the event sequences , linguists in the second round revision were not presented with the event sequences . All annotators were trained and disagreements were resolved through discussion .", "entities": [[20, 21, "DatasetName", "DARPA"], [23, 24, "MethodName", "ontology"], [31, 32, "MethodName", "ontology"], [65, 66, "MethodName", "ontology"], [76, 77, "MethodName", "ontology"], [84, 85, "DatasetName", "General"], [244, 245, "DatasetName", "General"]]}
{"text": "We compare the generated schemas with the ground truth schemas based on the overlap between them . The following evaluation metrics were employed : 8 Event Match : A good schema must contain the events crucial to the complex event scenario . Fscore is used to compute the overlap of event nodes . Event Sequence Match : A good schema is able to track events through a timeline . So we obtain event sequences following temporal order , and evaluate F - score on the overlapping sequences of lengths l = 2 and l = 3 . Event Argument Connection Match : Our complex event graph schema includes entities and their relations and captures how events are connected through arguments , in addition to their temporal order . We categorize these connections into three categories : ( 1 ) two events are connected by shared arguments ; ( 2 ) two events have related arguments , i.e. , their arguments are connected through entity relations ; ( 3 ) there are no direct connections between two events . For every pair of overlapped events , we calculate F - score based on whether these connections are predicted correctly . The human schemas of the General dataset do not contain arguments and the relations between arguments , so we only compute this metric for the IED dataset .", "entities": [[204, 205, "DatasetName", "General"]]}
{"text": "To explore schema - guided probabilistic reasoning and prediction , we perform an extrinsic evaluation of event prediction . Different from traditional event prediction tasks , the temporal event graphs contain arguments with relations , and there are type labels assigned to nodes and edges . We create a graph - based event prediction dataset using our testing graphs . The task aims to predict ending events of each graph , i.e. , events that have no future events after it . An event is predicted correctly if its event type matches one of the ending events in the graph . Considering that there can be multiple ending events in one instance graph , we rank event type prediction scores and adopt MRR ( Mean Reciprocal Rank ) and HITS@1 as evaluation metrics .", "entities": [[117, 119, "TaskName", "type prediction"], [122, 123, "MetricName", "MRR"], [129, 130, "MetricName", "HITS@1"]]}
{"text": "The definition of a complex event schema separates us from related lines of work , namely schema induction and script learning . Previous work on schema induction aims to characterize event triggers and participants of individual atomic events ( Chambers , 2013 ; Cheung et al , 2013 ; Nguyen et al , 2015 ; Sha et al , 2016 ; Yuan et al , 2018 ) , ignoring inter - event relations . Work on script learning , on the other hand , originally limited attention to event chains with a single protagonist ( Chambers andJurafsky , 2008 , 2009 ; Rudinger et al , 2015 ; Jans et al , 2012 ; Granroth - Wilding and Clark , 2016 ) and later extended to multiple participants Mooney , 2014 , 2016 ; Weber et al , 2018 ) . Recent efforts rely on distributed representations encoded from the compositional nature of events ( Modi , 2016 ; Granroth - Wilding and Clark , 2016 ; Weber et al , 2018Weber et al , , 2020 , and language modeling ( Rudinger et al , 2015 ; Pichotta and Mooney , 2016 ; Peng and Roth , 2016 ) . All of these methods still assume that events follow linear order in a single chain . They also overlook the relations between participants which are critical for understanding the complex event . However , we induce a comprehensive event graph schema , capturing both the temporal dependency and the multi - hop argument dependency across events . Recent work on event graph schema induction ( Li et al , 2020 ) only considers the connections between a pair of two events . Similarly , their event prediction task is designed to automatically generate a missing event ( e.g. , a word sequence ) given a single or a sequence of prerequisite events ( Nguyen et al , 2017 ; Hu et al , 2017 ; Li et al , 2018b ; Kiyomaru et al , 2019 ; Lv et al , 2019 ) , or predict a pre - condition event given the current events ( Kwon et al , 2020 ) . In contrast , we leverage the automatically discovered temporal event schema as guidance to forecast the future events . Existing script annotations ( Chambers andJurafsky , 2008 , 2010 ; Wanzare et al , 2016 ; Mostafazadeh et al , 2016a , b ; Kwon et al , 2020 ) can not support a comprehensive graph schema induction due to the missing of critical event graph structures , such as argument relations . Furthermore , in real - world applications , complex event schemas are expected to be induced from large - scale historical data , which is not feasible to annotate manually . We propose a data - driven schema induction approach , and choose to use IE systems instead of using manual annotation , to induce schemas that are robust and can tolerate extraction errors . Our work is also related to recent advances in modeling and generation of graphs ( Li et al , 2018a ; Jin et al , 2018 ; Grover et al , 2019 ; Simonovsky and Komodakis , 2018 ; Liu et al , 2019 ; Fu et al , 2020 ; Dai et al , 2020 ; You et al , 2018 ; Liao et al , 2019 ; Yoo et al , 2020 ; Shi et al , 2020 ) . We are the first to perform graph generation on event graphs .", "entities": [[590, 592, "TaskName", "graph generation"]]}
{"text": "This research is based upon work supported by U.S. DARPA KAIROS Program Nos . FA8750 - 19 - 2 - 1004 and Air Force No . FA8650 - 17 - C - 7715 . The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies , either expressed or implied , of DARPA , or the U.S. Government . The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein .", "entities": [[9, 10, "DatasetName", "DARPA"], [63, 64, "DatasetName", "DARPA"]]}
{"text": "An important desideratum of natural language generation ( NLG ) systems is to produce outputs that are not only correct , but also diverse . For example , a dialog system ( Adiwardana et al , 2020 ) should permit many responses for the prompt \" How are you today ? \" . Similarly , we expect diverse responses in tasks such as story generation ( Li et al , 2018 ) , question generation ( Pan et al , 2019 ) and question answering ( Fan et al , 2019 ) . Despite growing effort to produce more diverse models ( Li et al , 2016c , a ; Holtzman et al , 2019 ; Du and Black , 2019 ) , there is no standard evaluation metric for measuring diversity . Thus , different papers evaluate diversity differently ( if at Set A Pretty much everything . Nothing , really . You wo n't believe what happened ! Why do you even care ? What were you doing that was more important than this ?", "entities": [[63, 65, "TaskName", "story generation"], [73, 75, "TaskName", "question generation"], [83, 85, "TaskName", "question answering"]]}
{"text": "We now describe our framework for evaluating diversity metrics . Diversity has many facets : for in - stance , a set of sentences can be diverse in terms of their content , while another may have similar content , but diverse form ( Figure 1 ) . Our framework provides a way to evaluate metrics for different aspects of diversity under moderate assumptions . We define a diversity metric m div ( S c ) R as a function that takes a set of generated responses S c as an input , and outputs a diversity score . Each response s S c is generated for the same input context c , hence S c is a sample from a generative distribution P gen ( s | c ) . The overall diversity score of a generative model can be obtained by averaging m div over sets S c sampled from the model given multiple contexts c C. To evaluate m div ( ) , we assume access to some deterministic diversity parameter d that controls an aspect of diversity in S c . We test the relation between m div and the parameter d. By varying d and measuring m div , we can compute the correlation \u03c1 between m div and an aspect of diversity represented by d. Because our goal is to have metrics that rank the diversity of generated texts , we use Spearman 's \u03c1 rank correlation as our test score . Figure 2 illustrates the flow of a test in our framework . In practice , to control the diversity level of S c using d , we use a tester : a generative model that takes a context c and a diversity parameter d as input , and outputs a response set S c , d . We stress that the tester can be either a neural model or a human . A good tester should reliably represent the diversity level quantified by d. As a hypothetical example , c can be a movie name and d represent sentiment diversity , that is , the number of different sentiments in a collection of reviews S c . A human tester can observe c and d , and produce reviews accordingly ( such data can be easily mined from IMDB ) . A collection of such ( d , S c , d ) makes a test , in which the correlation between m div ( S c , d ) and d measures the sensitivity of m div to sentiment diversity . We now describe two tests that instantiate this framework , roughly corresponding to the two main aspects of diversity : form diversity and content diversity .", "entities": [[386, 387, "DatasetName", "IMDB"]]}
{"text": "The diversity of a NLG system constructed from a LM depends on both the LM but also the decoding algorithm on top of it . For example , beam search approximates the most probable output , and dramatically reduces diversity . Conversely , sampling from the LM leads to high diversity , but low quality output ( Holtzman et al , 2019 ) . A popular method to control diversity in NLG systems is to vary some decoding parameter . Variations include ( a ) softmax temperature ( Ackley et al , 1985 ) , where a parameter \u03c4 controls the skewness of the softmax distribution at each step , ( b ) Nucleus ( Top - p ) sampling ( Holtzman et al , 2019 ) , where one samples at each step from the minimal set of most probable tokens whose cumulative probability is at least p , and ( c ) Top - k sampling , which samples from the top - k most probable tokens at each step . All methods skew the LM distribution in a way that avoids low - probability tokens and leads to higher quality ( Holtzman et al , 2019 ) , providing a decoding parameter that trades off quality and diversity ( Caccia et al , 2018 ) . In the decoding test ( decTest ) , we define the tester to be a LM , such as GPT - 2 ( Radford et al , 2019 ) , and the diversity parameter d to be a decoding parameter such as temperature . We check how different diversity metrics m div correlate with decoding parameters . This can shed light on the quality of the metrics , but also on how decoding parameters affect the output of a NLG system . The decoding test uses automatically - generated data that is cheap to produce , and decoding parameters that are well - known to control diversity . Thus , we view this test as a warm - up test to explore the strengths of our framework .", "entities": [[85, 86, "MethodName", "softmax"], [104, 105, "MethodName", "softmax"], [238, 239, "MethodName", "GPT"]]}
{"text": "We expand the idea from Zhu et al ( 2018 ) and suggest a method to construct a diversity metric from any 2 - sentence similarity metric . Given m sim ( s 1 , s 2 ) R , a symmetric similarity metric that gets a pair of input sentences ( s 1 , s 2 ) and returns a similarity score , we can define a diversity metricm div as the negation of the mean similarity score across all ( unordered ) pairs of S c : m div ( S c ) = \u2212 1 | Sc | 2 s i , s j Sc , i > j m sim ( s i , s j ) . This reduction allows us to easily define new diversity metrics based on past work on sentence similarity ( Gomaa et al , 2013 ; Devlin et al , 2019 ; Zhang et al , 2019a ; Reimers and Gurevych , 2019 ) . In 6 we show that both n - gram - based similarity metrics and neural semantic similarity metrics provide useful diversity metrics . 6 Experiments", "entities": [[180, 182, "TaskName", "semantic similarity"]]}
{"text": "We apply our evaluation procedure on three different English NLG tasks that require diversity . Story completion ( storyGen ) ; We use the ROC Stories dataset ( Mostafazadeh et al , 2016 ) , in which the context c is the first four sentences of a story , and the response s is a single sentence that ends the story . We use the contexts C from this data and generate response sets S c for each context using our testers . The long contexts characterizing this data narrow down the space of possible responses , making this a \" low - entropy \" generation task , where the output is constrained , but diversity is still essential . Dialog response generation ( respGen ) ; A comment - response pairs dataset extracted from the website reddit.com and pre - processed by Hashimoto et al ( 2019 ) . We use the comments from their data as contexts C and generate response sets S c for each context using our testers . Since comments are single sentences the response is less constrained , making this a \" medium - entropy \" generation task . 3 - words prompt completion ( promptGen ) ; Contexts C are 3 - words prompts , extracted from the Cornell Movie - Dialogs Corpus ( Danescu - Niculescu - Mizil and Lee , 2011 ) by taking the first three words from each original context . The response sets S c are completions of the prompts , generated by our testers . This context provides minimal constraints , making this a \" highentropy \" generation task . Samples of the contexts extracted for each task , along with generated response sets , are presented in Appendix B. We intentionally avoid NLG tasks where diversity is not necessarily desired , such as summarization and machine translation .", "entities": [[15, 17, "TaskName", "Story completion"], [121, 123, "TaskName", "response generation"], [215, 220, "DatasetName", "Cornell Movie - Dialogs Corpus"], [307, 308, "TaskName", "summarization"], [309, 311, "TaskName", "machine translation"]]}
{"text": "In decTest we measure the correlation between diversity metrics ( m div ) and the softmax temperature decoding parameter ( d ) . The tester generating the response sets ( S c ) is a neural NLG model . Response set ( \u03c4 = 0.25 ) It was a minor fire and they put it out . It was a fire . It was a fire . It was a fire . It was a fire . Response set ( \u03c4 = 0.8 ) They arrived and put out the fire . It was a fire . It was a fire . It turned out to be a fire . It was a minor fire night . Response set ( \u03c4 = 1.1 ) It turned out to be a mechanic . Before the fire was put out it was a fire . It was a fire . They co - worker matter how bad the fire was . Several shells , the fire department came just in time . deviation . HDS metrics are computed over one experiment of 200 sets , due to their high cost . Data for storyGen and respGen was generated by the MASS model , fine - tuned on each dataset . Data for promptGen was generated by GPT - 2 - large ( Radford et al , 2019 ) without fine - tuning . We provide examples for how story endings change as a function of temperature in Table 1 . Examples for all tasks along with additional reproducibility details are in the Appendix B. For each HDS metric , we collected 10 ratings per query from Amazon Mechanical Turk ( AMT ) workers . While absHDS demands one query per response set , in order to perform simHDS at a reasonable cost , we chose | S c | = 5 , resulting in 5 2 = 10 crowdsourcing queries instead of 10 2 = 45 per set . We evaluate simHDS only for respGen due to the metric 's high cost and low performance . Results Table 2 presents results of absHDS , simHDS , and all automatic metrics . In general , ngram based metrics capture the diversity induced by a temperature sweep , beating HDS and neural metrics . Figure 3 provides a more detailed analysis . Each point represents a single set of responses generated at some temperature . While rank correlation for cosine similarity is high , it is While our framework is meant to evaluate diversity metrics , the results of the test let us reflect on the decoding parameters themselves . This result shows that humans perform worse than automatic metrics in this experimental setup , hinting that temperature mostly controls superficial changes to the generated text . Additionally , simHDS performs worse than absHDS although it is 3x more expensive , showing that rating the entire set rather than averaging over pairs is useful . Other decoding parameters To compare the robustness of our conclusions to other decoding parameters , we repeat it with two additional decoding methods : ( a ) in Nucleus ( Top - p ) sampling we swept linearly over 100 values of p in the range [ 0.1 , 1.0 ] ; ( b ) In Top - k sampling we swept k in logarithmic scale over 100 values in the range [ 1 , 30 K ] and present the correlation between the metrics and log 10 ( k ) . While softmax temperature enables skewing P LM to a more diverse P gen using \u03c4 > 1 , both Top - p and Top - k enable only skewing P LM to a more sharp ( hence less diverse ) P gen . Table 3 presents results for all automatic metrics using the three decoding methods over prompt - Gen. Results for other tasks are in Appendix C. We find that Top - p correlates well with temperature along all three generation tasks , whereas Topk does not correlate with any of them .", "entities": [[15, 16, "MethodName", "softmax"], [214, 215, "MethodName", "GPT"], [584, 585, "MethodName", "softmax"]]}
{"text": "In conTest , we measure the correlation between diversity metrics ( m div ) and content diversity , represented by a binary parameter d { 0 , 1 } . The testers are AMT workers , guided to create sets with high level of form diversity and high or low content diversity according to d.", "entities": [[25, 26, "DatasetName", "0"]]}
{"text": "Tables 11 to 19 present data samples from sto - ryGen , respGen and promptGen with the neural testers of decTest , as detailed in 6 . Each table presents two contexts and three response sets per context . Each response set was generated with a different value of decoding parameter for the three decoding methods : softmax temperature , Nucleus sampling , and Top - k.", "entities": [[57, 58, "MethodName", "softmax"]]}
{"text": "We applied conTest for all the collected data for each of the three NLG tasks ( see Tables 8 and 9 10 ) . Compared to Table 4 , The gap between the best performing neural metrics ( sent - BERT ) and absHDS was increased in favor to HDS ( 0.04 compared to 0.1 difference in Spearman 's \u03c1 ) .", "entities": [[40, 41, "MethodName", "BERT"]]}
{"text": "Collected data and code All the collected data , metric scores per samples for each of decTest and conTest , as well as code for running and visualizing the tests , are publicly available 8 . The collection methods are elaborated in Section 6 . Original data We provide additional data for the original three datasets used in Section 6 . ROC Stories dataset 9 ( Mostafazadeh et al , 2016 ) used for storyGen task contains 96K/1K/1 K train / validation / test titles and five - sentence stories . We used the samples without pre - processing for both fine - tuning MASS model and generate samples for our tests . Reddit comment - response dataset used for respGen task contains 37 M /1 M /1 M train / validation / test comment - response pairs , extracted from the social website reddit.com scraped by pushshift.io followed by the pre - process described in ( Hashimoto et al , 2019 ) . We used the samples without further processing for both fine - tuning MASS model and generate samples for our tests . To the best of our knowledge , this dataset is not publicly available at the moment . CMDC dataset 10 ( Danescu - Niculescu - Mizil and Lee , 2011 ) contains 108K/30 K train / test sentence - response pairs extracted from movie scripts . We extracted the first three words from the sentences ( used as contexts for the original task ) to be the context of our task . We did not use this data for training since we used GPT - 2 without fine - tuning for promptGen . Auto - generated data For decTest , we used two pre - trained generative models for generating responses given the contexts : For storyGen and respGen tasks , we used MASS 11 He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He did not get to the meeting anymore . He missed his meeting . He passed out and failing the meeting He missed his meeting . He missed his meeting . He missed his meeting . He missed his meeting . He passed out and was kicked out of the meeting . He missed his meeting . He missed his meeting . Family Night Food . Tonight , my mom ordered Mexican food for family night . She got it from my favorite Mexican place in town . When it arrived , it was hot and smelled wonderful . We devoured it with gusto . After a few hours of take it home we all enjoyed its night . After a few hours of take it home we all enjoyed its night . After a few hours of take it home we all enjoyed its night . After a few hours of eating everyone was satisfied . After a few hours of take it home we all enjoyed its night . After a few hours of eating everyone was satisfied . After a few hours of take it home we all enjoyed its night . After a few hours of take it home we all enjoyed its night . After a few hours of take it home we all enjoyed its night . After a few hours of eating everyone was satisfied . After dinner , we all went home to cook Mexican food . After a few hours of cooking she was tired and ready to eat . After dinner , I always put got ready for Christmas . After dinner , I helped her do the dishes . After a few hours of dinner , the food was amazing . After a few hours of take it home we all enjoyed margaria . After dinner , I was ready to take on work the next day of After dinner , I was sad to say goodbye to her After a few hours of take it home we all enjoyed one bite . After a few hours of eating everyone was satisfied . Even though my stomach was gone , I was sad it was finally pockets After dinner , I alone . All in all the family while my mom finished the food . After a few hours of dancing , she pianed . Afterwards I 'd never finish single night a week . Afterwards we all went to sleep in my woods . Afterwards I helped her do much better than my wife . Afterwards my mom helped me finish my dinner . After a few hours we all enjoyed storm blood from the Italy . After dinner , we all enjoyed some good food together . The worker is asked to generate response of hers / his own and rate the quality of the tester 's response . So that 's the first time you want to punch somebody , not miss before . \" The Seahawks would ! So that 's the science behind the Broadwell - E processors from Intel that Intel launched last fall ! So that 's the instinct from other teams , that they 're a headache . - Ramsay MacDonald , ! So that 's the white whale right there about too much debt . And then what you ! So that 's the end of our discussion about the causes . What happens when we look at the ! So that 's the cover of inhibition against \" chronic \" or \" adaptive \" stimulants ! So that 's the way the story goes , but exactly how is cloud providers going to restrict Their ! So that 's the beginning , the beginning of the show , I guess five minutes . \" ! So that 's the Indie Mobile Game Week Honoring Winners ! ! ! ! ! ! ! ! ! So that 's the reason I 'm writing , that 's why you do n't understand why people know ! do you listen to the music ? \" \" I do n't know . I do n't listen ! do you listen to them ? \" \" I do , \" he said . \" I 'm not ! do you listen to the voices of the people ? \" \" I do , \" said the king ! do you listen to the song ? \" \" I do n't know . I do n't know ! do you listen to the music ? \" \" I do . \" \" You 're not ! do you listen to the news ? I do . I 'm a big fan of the ! do you listen to me ? \" \" Yes , I do . \" \" I 'm ! do you listen to the other side ? \" \" I do n't know . I do n't ! do you listen to the other side ? \" \" I do , \" said the boy . \" ! do you listen to the news ? No , I do n't . I do n't listen ! do you listen to the current draft ? I listen to the current draft . I 'm ! do you listen to it ? \" It 's easy to hear the \" why ? \" but when ! do you listen to the people that come here ? \" \" No , I 'm too busy ! do you listen to the thing ? \" \" Of course I do .", "entities": [[113, 114, "DatasetName", "Reddit"], [268, 269, "MethodName", "GPT"]]}
{"text": "Automatic Generation of Contrast Sets from Scene Graphs : Probing the Compositional Consistency of GQA", "entities": [[14, 15, "DatasetName", "GQA"]]}
{"text": "NLP benchmarks typically evaluate in - distribution generalization , where test sets are drawn i.i.d from a distribution similar to the training set . Recent works showed that high performance on test sets sampled in this manner is often achieved by exploiting systematic gaps , annotation artifacts , lexical cues and other heuristics , rather than learning meaningful task - related signal . As a result , 1 Our contrast sets and code are available at https://github.com/yonatanbitton/ AutoGenOfContrastSetsFromSceneGraphs . Figure 1 : Illustration of our approach based on an example from the GQA dataset . Top : QA pairs and an image annotated with bounding boxes from the scene graph . Bottom : relations among the objects in the scene graph . First line at the top is the original QA pair , while the following 3 lines show our pertubated questions : replacing a single element in the question ( a fence ) with other options ( a wall , men , an elephant ) , leading to a change in the output label . For each QA pair , the LXMERT predicted output is shown . the out - of - domain performance of these models is often severely deteriorated ( Jia and Liang , 2017 ; Ribeiro et al , 2018 ; Gururangan et al , 2018 ; Geva et al , 2019 ; McCoy et al , 2019 ; Feng et al , 2019 ; Stanovsky et al , 2019 ) . Recently , Kaushik et al ( 2019 ) and introduced the contrast sets approach to probe out - of - domain generalization . Contrast sets are constructed via minimal modifications to test inputs , such that their label is modified . For example , in Fig . 1 , replacing \" a fence \" with \" a wall \" , changes the answer from \" Yes \" to \" No \" . Since such perturbations introduce minimal additional semantic complexity , robust models are expected to perform similarly on the test and contrast sets . However , a range of NLP models severely degrade in performance on contrast sets , hinting that they do not generalize well . Except two recent exceptions for textual datasets ( Li et al , 2020 ; Rosenman et al , 2020 ) , contrast sets have so far been built manually , requiring extensive human effort and expertise . In this work , we propose a method for automatic generation of large contrast sets for visual question answering ( VQA ) . We experiment with the GQA dataset ( Hudson and Manning , 2019 ) . GQA includes semantic scene graphs ( Krishna et al , 2017 ) representing the spatial relations between objects in the image , as exemplified in Fig . 1 . The scene graphs , along with functional programs that represent the questions , are used to balance the dataset , thus aiming to mitigate spurious dataset correlations . We leverage the GQA scene graphs to create contrast sets , by automatically computing the answers to question perturbations , e.g. , verifying that there is no wall near the puddle in Fig . 1 . We create automatic contrast sets for 29 K samples or \u224822 % of the validation set . We manually verify the correctness of 1 , 106 of these samples on Mechanical Turk . Following , we evaluate two leading models , LXMERT ( Tan and Bansal , 2019 ) and MAC ( Hudson and Manning , 2019 ) on our contrast sets , and find a 13 - 17 % reduction in performance compared to the original validation set . Finally , we show that our automatic method for contrast set construction can be used to improve performance by employing it during training . We augment the GQA training set with automatically constructed training contrast sets ( adding 80 K samples to the existing 943 K in GQA ) , and observe that when trained with it , both LXMERT and MAC improve by about 14 % on the contrast sets , while maintaining their original validation performance . Our key contributions are : ( 1 ) We present an automatic method for creating contrast sets for VQA datasets with structured input representations ; ( 2 ) We automatically create contrast sets for GQA , and find that for two strong models , performance on the contrast sets is lower than on the original validation set ; and ( 3 ) We apply our method to augment the training data , improving both models ' performance on the contrast sets .", "entities": [[92, 93, "DatasetName", "GQA"], [182, 183, "MethodName", "LXMERT"], [266, 268, "TaskName", "domain generalization"], [417, 420, "DatasetName", "visual question answering"], [421, 422, "TaskName", "VQA"], [428, 429, "DatasetName", "GQA"], [438, 439, "DatasetName", "GQA"], [498, 499, "DatasetName", "GQA"], [572, 573, "MethodName", "LXMERT"], [638, 639, "DatasetName", "GQA"], [658, 659, "DatasetName", "GQA"], [670, 671, "MethodName", "LXMERT"], [708, 709, "TaskName", "VQA"], [724, 725, "DatasetName", "GQA"]]}
{"text": "To construct automatic contrast sets for GQA we first identify a large subset of questions requiring specific reasoning skills ( 2.1 ) . Using the scene graph representation , we perturb each question in a manner which changes its gold answer ( 2.2 ) . Finally , we validate the automatic process via crowdsourcing ( 2.3 ) .", "entities": [[6, 7, "DatasetName", "GQA"]]}
{"text": "The questions in the GQA dataset present a diverse set of modelling challenges , as exemplified in Table 1 , including object identification and grounding , spatial reasoning and color identification . Following the contrast set approach , we create perturbations testing whether models are capable of solving questions which require this skill set , but that diverge from their training distribution . To achieve this , we identify commonly recurring question templates which specifically require such skills . For example , to answer the question \" Are there any cats near the boat ? \" a model needs to identify objects in the image ( cats , boat ) , link them to the question , and identify their relative position . We identify six question templates , testing various skills ( Table 1 ) . We abstract each question template with a regular expression which identifies the question types as well as the physical objects , their attributes ( e.g. , colors ) , and spatial relations . Overall , these regular expressions match 29 K questions in the validation set ( \u224822 % ) , and 80 K questions in the training set ( \u22488 % ) .", "entities": [[4, 5, "DatasetName", "GQA"]]}
{"text": "To verify the correctness of our automatic process , we sampled 553 images , each one with an original and perturbed QA pair for a total of 1 , 106 instances ( \u22484 % of the validation contrast pairs ) . The ( image , question ) pairs were answered independently by human annotators on Amazon Mechanical Turk ( see Fig . 3 in Appendix A.4 ) , oblivious to whether the question originated from GQA or from our automatic contrast set . We found that the workers were able to correctly answer 72.3 % of the perturbed questions , slightly lower than their performance on the original questions ( 76.6 % ) . 2 We observed high agreement between annotators ( \u03ba = 0.679 ) . Our analysis shows that the human performance difference between the perturbed questions and the original questions can be attributed to the scene Figure 2 : GQA image ( left ) with example perturbations for different question templates ( right ) . Each perturbation aims to change the label in a predetermined manner , e.g. , from \" yes \" to \" no \" .", "entities": [[75, 76, "DatasetName", "GQA"], [152, 153, "DatasetName", "GQA"]]}
{"text": "Training graph annotation errors in the GQA dataset : 3.5 % of the 4 % difference is caused by a discrepancy between image and scene graph ( objects appearing in the image and not in the graph , and vice versa ) . Examples are available in Fig . 5 in Appendix A.5 .", "entities": [[6, 7, "DatasetName", "GQA"]]}
{"text": "Our results suggest that both MAC and LXMERT under - perform when tested out of distribution . A remaining question is whether this is due to model architecture or dataset design . claim that both of these models are prone to fail on compositional generalization because they do not decompose the problem into smaller sub - tasks . Our results support this claim . On the other hand , it is possible that a different dataset could prevent these models from finding shortcuts . Is there a dataset that can prevent all shortcuts ? Our automatic method for creating contrast sets allows us to ask those questions , while we believe that future work in better training mechanisms , as suggested in and Jin et al ( 2020 ) , could help in making more robust models . We proposed an automatic method for creating contrast sets for VQA datasets that use annotated scene graphs . We created contrast sets for the GQA dataset , which is designed to be compositional , balanced , and robust against statistical biases . We observed a large performance drop between the original and augmented sets . As our contrast sets can be generated cheaply , we further augmented the GQA training data with additional perturbed questions , and showed that this improves models ' performance on the contrast set . Our proposed method can be extended to other VQA datasets .", "entities": [[7, 8, "MethodName", "LXMERT"], [148, 149, "TaskName", "VQA"], [162, 163, "DatasetName", "GQA"], [206, 207, "DatasetName", "GQA"], [235, 236, "TaskName", "VQA"]]}
{"text": "We created contrast sets automatically , and verified their correctness via the crowdsourcing annotation of a sample of roughly 1 K instances . Section 2.3 describes the annotation process on Amazon Mechanical Turk . The images and original questions were sampled from the public GQA dataset ( Hudson and Manning , 2019 ) , in the English language . Fig . 3 in Appendix A.4 provides example of the annotation task . Overall , the crowdsourcing task resulted in \u22486 hours of work , which paid an average of 11USD per hour per annotator . Reproducibility The augmentations were performed with a MacBook Pro laptop . Augmentations for the validation data takes < 1 hour per question template , and for the training data < 3 hours per question template . Overall process , < 24 hours . The experiments have been performed with the public implementations of MAC ( Hudson and Manning , 2018 ) and LXMERT ( Tan and Bansal , 2019 ) , models : https : //github.com / airsplay / lxmert , https://github.com/stanfordnlp/ mac - network/. The configurations were modified to not include the validation set in the training process . The experiments were performed with a Linux virtual machine with a NVIDIA 's Tesla V100 GPU . The training took \u223c1 - 2 days in each model . Validation took \u223c 30 minutes .", "entities": [[44, 45, "DatasetName", "GQA"], [157, 158, "MethodName", "LXMERT"], [174, 175, "MethodName", "lxmert"], [201, 202, "DatasetName", "Linux"]]}
{"text": "Table 5 reports the basic statistics of automatic contrast sets generation method when applied on the GQA validation dataset . It shows the overall number of images and QA pairs that matched the 6 question types we identified . Tables 6 shows the statistics per question type , indicating how productive each augmentation method is . Tables 7 and 8", "entities": [[16, 17, "DatasetName", "GQA"], [17, 19, "DatasetName", "validation dataset"]]}
{"text": "We thank the reviewers for the helpful comments and feedback . We thank the authors of GQA for building the dataset , and the authors of LXMERT and MAC for sharing their code and making it usable . This work was supported in part by the Center for Interdisciplinary Data Science Research at the Hebrew University of Jerusalem , and research gifts from the Allen Institute for AI .", "entities": [[16, 17, "DatasetName", "GQA"], [26, 27, "MethodName", "LXMERT"]]}
{"text": "Received views of REG suggest that the process contains two steps ( Reiter and Dale , 2000 ) : Step 1 decides what general syntactic type of RE to use ( e.g. , a full description , a PN , a pronoun , or some other type ) ; once this decision is taken , Step 2 ( discussed in section 1 above ) makes more fine - grained decisions , for example , in case of a full description , this step decides what properties should be expressed in the description . The observations of the previous section make this two - step approach problematic , for example because ( in some situations ) no PN may be available for a given referent , or because PNs and descriptions must be combined ( in other situations ) . In what follows , we explore a radical alternative , showing that if a suitable representation scheme is used , it is possible to incorporate all decisions related to PNs within Step 2 . Suppose each individual in the KB comes not just with a number of descriptive properties but with 0 or more PNs as well , where a PN is regarded as a property that is true of all individuals who bear this name . - ( being named ) Joe Klein is a property of all individuals named Joe Klein - ( being named ) Joe is a property of all those individuals named Joe - ( being named ) Klein is a property of all those individuals named Klein The idea that a PN can be viewed as a property of its bearer deviates from a long tradition of work in philosophy and logic that regards PNs as rigid designators ( Kripke , 1980 ) , yet it enjoys considerable support . ( Burge , 1973 ) , for example , observes that PNs can behave like common nouns , as in \" There are relatively few Alfreds in P \" , and \" An Alfred joined the club today \" ( see ( Larson and Segal , 1995 ) and ( Elbourne , 2005 ) for further support ) . A simple KB containing PNs as well as ordinary properties could look like this : JOB : political commentator , commentator NATIONALITY : American NAMES : Mr Joe Klein , Joe Klein , Joe , Klein Because longer versions of a person 's name are applicable to only some of the individuals to whom a shorter version is applicable , the values of the NAMES attribute often subsume each other : all people who are called Mr Joe Klein are also called Joe Klein , and so on . These properties can be dealt with using the mechanism for subsumption in the Incremental Algorithm ( which would also state that all political commentators are commentators , for instance ) ( Dale and Reiter , 1996 ) . Of course if Joe Klein is the only Joe in the room , we can refer unambiguously to him saying \" Joe \" . This is accounted for by making the REG algorithm that operates on the KB above salience aware in one of the standard ways , e.g. , ( Krahmer and Theune , 2002 ) . Salience also suggests a way in which REG can extend beyond one - shot REs to cover reference in extended discourse or dialogue : if x is introduced by means of the PN \" Joe Klein \" in a text , then if x is the only Joe so far mentioned , then this makes x the most salient of all Joe 's , licencing the short RE \" Joe \" . In short : - Each object has an attribute NAMES . - The set of values of NAMES can be empty ( no name is available ) , singleton ( one name ) , or neither ( several names ) . - A subsumption ( i.e. , subset ) relation can be defined among these values . - Different objects can share some or all of their names . If names are the \" canonical \" way of referring to an entity , then standard mechanisms could be invoked to favour names at the expense of other properties . One option is to Dale and Reiter 's Preference Order ( Dale and Reiter , 1996 ) , making NAMES the most highly preferred attribute in an Incremental Algorithm . Alternatively , a new type of brevity - based algorithm might be used that generates the RE that contains the smallest number of syllables . 3 Assuming that PNs are brief ( as they often are ) , this type of approach would favour PNs , and it would favour shorter PNs over longer ones ( e.g. , \" Klein \" over \" Joe Klein \" ) . It would also predict that PNs are avoided where large sets are enumerated ( compare the RE \" the citizens of China \" with an enumeration of all the elements of this set ) . To see how REG could work in an Incremental Algorithm , consider a simple KB , where each individual has 1 name : TYPE : woman { w 1 , w 2 , w 3 } , man { m1 } , dog { d 1 , d 2 } NAMES : mary { w 1 } , shona { w 2 , w 3 } , rover { d 1 } , max { m 1 , d 2 } ACTION : feed { ( w 1 , d 1 ) , ( w 2 , d 2 ) , ( w 2 , d 1 ) } AFFECTION : love { ( w 1 , d 1 ) , ( w 3 , d 1 ) } This approach generates REs such as : d 1 : \" Rover \" d 2 : \" The dog called Max \" w 3 : \" Shona , who loves a dog \" With the above representation scheme in place , classic REG algorithms can be applied without modifications . However , the scheme does not allow PNs to have properties ( e.g. , \" is a posh name \" , \" has 5 characters \" , \" is common in Scotland \" ) . If names are reified , then this becomes possible ; what 's more , PNs themselves could be referred to ( e.g. , \" the name his friends call him \" ) : a name is just another object linked ( on the one hand ) to the things it names and ( on the other hand ) to the ways in which it manifests itself in spelling , pronunciation , etc . For example , n 2 may name both a man and a dog , and it may be written as \" Max \" : Type : woman { w 1 , w 2 , w 3 } , man { m 1 } , dog { d 1 , d 2 } , name { n 1 , n 2 , n 3 , n 4 } Action : feed { ( w 1 , d 1 ) , ( w 2 , d 2 ) , ( w 2 , d 1 ) } Affection : love { ( w 1 , d 1 ) , ( w 3 , d 1 ) } Naming : name { ( d 1 , n 1 ) , ( d 2 , n 2 ) , ( w 1 , n 3 ) , ( w 2 , n 4 ) , ( w 3 , n 4 ) , ( m 1 , n 2 ) } Spelling : written { ( n 1 , Rover ) , ( n 2 , M ax ) , ( n 3 , M ary ) , ( n 4 , Shona ) } Standard REG algorithms can use this KB to generate \" The name shared by a man and a dog \" ( i.e. , \" Max \" ) . If n 4 is Scottish , we obtain \" women with a Scottish name \" as well . A slight drawback of this approach , which treats names as objects , is that subsumption can no longer be used to compare names .", "entities": [[190, 191, "DatasetName", "0"], [431, 432, "DatasetName", "subsume"]]}
{"text": "BRIO : Bringing Order to Abstractive Summarization", "entities": [[6, 7, "TaskName", "Summarization"]]}
{"text": "Abstractive summarization models are commonly trained using maximum likelihood estimation , which assumes a deterministic ( onepoint ) target distribution in which an ideal model will assign all the probability mass to the reference summary . This assumption may lead to performance degradation during inference , where the model needs to compare several system - generated ( candidate ) summaries that have deviated from the reference summary . To address this problem , we propose a novel training paradigm which assumes a non - deterministic distribution so that different candidate summaries are assigned probability mass according to their quality . Our method achieves a new state - of - the - art result on the CNN / DailyMail ( 47.78 ROUGE - 1 ) and XSum ( 49.07 ROUGE - 1 ) datasets . Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality . 1", "entities": [[1, 2, "TaskName", "summarization"], [125, 126, "DatasetName", "XSum"]]}
{"text": "The results are shown in Tab 2 . For CNNDM and NYT we use BART as the backbone model while for XSum we use the pre - trained PEGASUS model as our base model since it achieves better performance than BART . We have the following observations : ( 1 ) BRIO - Ctr outperforms SimCLS , its counterpart as an evaluation model in a two - stage summarization framework . Specifically , both BRIO - Ctr and SimCLS are used to score the candidate summaries generated by a Seq2Seq abstractive model ( BART ) . The final outputs are selected based on those scores . We attribute BRIO - Ctr 's superior performance to its use of the same model architecture ( BART ) for both candidate generation and scoring , while SimCLS uses RoBERTa as the evaluation model . As a result , BRIO - Ctr maximizes the parameter sharing between the two stages , and preserves the power of the Seq2Seq model pre - trained on the same dataset . ( 2 ) BRIO - Mul is able to establish the new stare - of - the - art performance on CNNDM . Notably , the previous state - of - the - art model , GSum , takes additional guidance as input and needs a separate encoder to encode the guidance information , while BRIO - Mul uses the same parameterization of BART . Compared to other methods ( ConSum , SeqCo , GOLD ) that aim to improve upon BART , BRIO - Mul performs much better , showing the effectiveness of our training method . ( 3 ) Since on XSum we use PEGASUS instead of BART as the base model , the result shows that our method is not restricted to the specific choice of the base model .", "entities": [[14, 15, "MethodName", "BART"], [21, 22, "DatasetName", "XSum"], [40, 41, "MethodName", "BART"], [68, 69, "TaskName", "summarization"], [89, 90, "MethodName", "Seq2Seq"], [93, 94, "MethodName", "BART"], [123, 124, "MethodName", "BART"], [135, 136, "MethodName", "RoBERTa"], [163, 164, "MethodName", "Seq2Seq"], [236, 237, "MethodName", "BART"], [254, 255, "MethodName", "BART"], [276, 277, "DatasetName", "XSum"], [282, 283, "MethodName", "BART"]]}
{"text": "The training paradigm proposed in this paper may be extended to any Seq2Seq model . However , it can be a non - trivial overhead to generate the candidate summaries using large neural models on the entire training set . On the other hand , recent work ( Raffel et al , 2020 ; Schick and Sch\u00fctze , System Summary Reference chelsea forward tammy abraham nets first - half double for chelsea . dominic solanke adds a third late on as chelsea look set to win trophy . manchester city struggle without injured star thierry ambrose . read : mourinho warns his young chelsea players he can not play them all . click here to read our match report from man city 's academy stadium . BART tammy abraham scored twice in the first half to give chelsea the lead . isaac buckley - ricketts levelled the game for manchester city . dominic solanke scored late on to put a gloss on the scoreline . click here to read sportsmail 's player ratings from the youth cup final . BRIO - Mul chelsea beat manchester city 3 - 1 in the youth cup final at the etihad stadium . tammy abraham scored twice in the first half to give chelsea the lead . dominic solanke scored late on to seal the win for the home side . Reference alejandro valverde won ahead of julian alaphilippe and michael albasini . chris froome finished 123rd after a crash during the final 12 kilometres . team sky 's sports director gabriel rasch praised froome for finishing . rasch said froome was ' banged up ' but expects to ride tour de romandie . BART movistar rider alejandro valverde won fleche wallonne on wednesday . team sky 's chris froome fell in the final 12 km but finished the race . philippe gilbert pulled out of the race after a bad crash 50 km from the end . click here for more cycling news . BRIO - Mul alejandro valverde defended his fleche wallonne title in belgium on wednesday . movistar rider finished ahead of julian alaphilippe and michael albasini . team sky 's chris froome fell in the final 12 km of the race but finished in 123rd . froome was involved in a crash but finished the race despite being ' banged up ' Reference manuel pellegrini won the premier league and capital one cup last season . city currently sit fourth in the league table - 12 points behind chelsea . pellegrini 's contract expires at the end of the 2015 - 16 season . city players have been impressed with vieira 's work with the youth team . pep guardiola is city 's first - choice to succeed pellegrini at the etihad . BART manuel pellegrini 's future at manchester city is under scrutiny . patrick vieira is highly - respected among the city players . city 's first - choice managerial option is bayern munich boss pep guardiola . click here for all the latest manchester city news . click here for more premier league news . BRIO - Mul manchester city players have backed patrick vieira to replace manuel pellegrini as manager of the club . the frenchman is highly - respected among the players at the etihad stadium . pellegrini 's future at the club is under scrutiny after a disappointing season . city 's first - choice manager is current bayern munich boss pep guardiola .", "entities": [[12, 13, "MethodName", "Seq2Seq"], [126, 127, "MethodName", "BART"], [280, 281, "MethodName", "BART"], [463, 464, "MethodName", "BART"]]}
{"text": "Tab . 10 presents an interesting pattern we observed when comparing the results of BRIO - Mul and BART , which demonstrates that our method helps the abstractive model to filter out noise patterns in the original data . Specifically , some of the reference summaries ( 331/11490 ) in CNNDM contains the phrase \" click here \" , pointing to a hyperlink , and 103 source documents also contain this phrase . BART picked up this pattern , and generates this phrase in 96 output summaries . On the contrary , our model learns to ignore this noise pattern and never generated it across the whole test set , likely because it identified that generated candidates with this pattern rarely achieve a high ROUGE score , and downweighted the probability accordingly .", "entities": [[18, 19, "MethodName", "BART"], [73, 74, "MethodName", "BART"]]}
{"text": "In this work , we presented a new training paradigm that assigns candidate outputs probability mass according to their quality using contrastive learning . While our method has achieved significant improvement on abstractive summarization , we note several directions for the future work to explore . First , since our method makes no assumptions specifically about the summarization task , it can be extended to other conditional text generation tasks such as machine translation . Second , it is possible to apply our method in a reinforcement learning setting , where the candidate summaries are dynamically generated . Finally , in experiments we only used diverse beam search to generate the candidate summaries , but it is likely that other candidate generation methods could yield further improvements .", "entities": [[21, 23, "MethodName", "contrastive learning"], [33, 34, "TaskName", "summarization"], [57, 58, "TaskName", "summarization"], [66, 69, "TaskName", "conditional text generation"], [72, 74, "TaskName", "machine translation"]]}
{"text": "Extremely Small BERT Models from Mixed - Vocabulary Training", "entities": [[2, 3, "MethodName", "BERT"]]}
{"text": "Pretrained language models like BERT have achieved good results on NLP tasks , but are impractical on resource - limited devices due to memory footprint . A large fraction of this footprint comes from the input embeddings with large input vocabulary and embedding dimensions . Existing knowledge distillation methods used for model compression can not be directly applied to train student models with reduced vocabulary sizes . To this end , we propose a distillation method to align the teacher and student embeddings via mixed - vocabulary training . Our method compresses BERT LARGE to a task - agnostic model with smaller vocabulary and hidden dimensions , which is an order of magnitude smaller than other distilled BERT models and offers a better size - accuracy trade - off on language understanding benchmarks as well as a practical dialogue task .", "entities": [[0, 3, "TaskName", "Pretrained language models"], [4, 5, "MethodName", "BERT"], [46, 48, "MethodName", "knowledge distillation"], [51, 53, "TaskName", "model compression"], [92, 93, "MethodName", "BERT"], [117, 118, "MethodName", "BERT"], [125, 126, "MetricName", "accuracy"]]}
{"text": "Recently , pre - trained context - aware language models like ELMo ( Peters et al , 2018 ) , GPT ( Radford et al , 2019 ) , BERT ( Devlin et al , 2018 ) and XLNet ( Yang et al , 2019 ) have outperformed traditional word embedding models like Word2Vec ( Mikolov et al , 2013 ) and GloVe ( Pennington et al , 2014 ) , and achieved strong results on a number of language understanding tasks . However , these models are typically too huge to host on mobile / edge devices , especially for real - time inference . Recent work has explored , inter alia , knowledge distillation ( Ba and Caruana , 2014 ; Hinton et al , 2015 ) to train small - footprint student models by implicit transfer of knowledge from a teacher model . Most distillation methods , however , need the student and teacher output spaces to be aligned . This complicates task - agnostic distillation of BERT to Asterisk ( * ) denotes equal contribution . Research conducted when all authors were at Google . smaller - vocabulary student BERT models since the input vocabulary is also the output space for the masked language modeling ( MLM ) task used in BERT . This in turn limits these distillation methods ' ability to compress the input embedding matrix , that makes up a major proportion of model parameters e.g. the \u223c30 K input WordPiece embeddings of the BERT BASE model make up over 21 % of the model size . This proportion is even higher for most distilled BERT models , owing to these distilled models typically having fewer layers than their teacher BERT counterparts . We present a task and model - agnostic distillation approach for training small , reduced - vocabulary BERT models running into a few megabytes . In our setup , the teacher and student models have incompatible vocabularies and tokenizations for the same sequence . We therefore align the student and teacher WordPiece embeddings by training the teacher on the MLM task with a mix of teacher - tokenized and student - tokenized words in a sequence , and then using these student embeddings to train smaller student models . Using our method , we train compact 6 and 12 - layer reducedvocabulary student models which achieve competitive performance in addition to high compression for benchmark datasets as well as a real - world application in language understanding for dialogue .", "entities": [[11, 12, "MethodName", "ELMo"], [20, 21, "MethodName", "GPT"], [29, 30, "MethodName", "BERT"], [38, 39, "MethodName", "XLNet"], [62, 63, "MethodName", "GloVe"], [114, 116, "MethodName", "knowledge distillation"], [170, 171, "MethodName", "BERT"], [187, 188, "DatasetName", "Google"], [193, 194, "MethodName", "BERT"], [206, 209, "TaskName", "masked language modeling"], [210, 211, "DatasetName", "MLM"], [215, 216, "MethodName", "BERT"], [247, 248, "MethodName", "WordPiece"], [251, 252, "MethodName", "BERT"], [252, 253, "MethodName", "BASE"], [272, 273, "MethodName", "BERT"], [287, 288, "MethodName", "BERT"], [307, 308, "MethodName", "BERT"], [341, 342, "MethodName", "WordPiece"], [349, 350, "DatasetName", "MLM"]]}
{"text": "Work in NLP model compression falls broadly into four classes : matrix approximation , weight quantization , pruning / sharing , and knowledge distillation . The former two seek to map model parameters to low - rank approximations ( Tulloch and Jia , 2017 ) and lower - precision integers / floats ( Chen et al , 2015 ; Zhou et al , 2018 ; Shen et al , 2019 ) respectively . In contrast , pruning aims to remove / share redundant model weights ( Li et al , 2016 ; Lan et al , 2019 ) . More recently , dropout ( Srivastava et al , 2014 ) has been used to cut inference latency by early exit ( Fan et al , 2019 ; Xin et al , 2020 ) . Knowledge distillation focuses on implicit transfer of knowledge as soft teacher predictions ( Tang et al , 2019 ) , attention distributions ( Zagoruyko and Komodakis , 2016 ) and intermediate outputs ( Romero et al , 2014 ) . Approaches close to our work rely on similar methods ( Sanh et al , 2019 ; Sun et al , 2019 ) , while others involve combinations of layer - wise transfer ( Sun et al , 2020 ) , taskspecific distillation ( Jiao et al , 2019 ) , architecture search ( Chen et al , 2020 ) and layer dropout ( Xu et al , 2020 ) ; many of these are specific to the transformer layer ( Vaswani et al , 2017 ) . Another highly relevant line of work focuses on reducing the size of the embedding matrix , either via factorization ( Shu and Nakayama , 2018 ; Lan et al , 2019 ) or vocabulary selection / pruning ( Provilkov et al , 2019 ; Chen et al , 2019b ) .", "entities": [[3, 5, "TaskName", "model compression"], [15, 16, "TaskName", "quantization"], [22, 24, "MethodName", "knowledge distillation"], [134, 136, "MethodName", "Knowledge distillation"]]}
{"text": "WordPiece ( WP ) tokens ( Wu et al , 2016 ) are subword units obtained by applying greedy segmentation to a training corpus . Given such a corpus and a number of desired tokens D , a WordPiece vocabulary is generated by selecting D subword tokens such that the resulting corpus is minimal in the number of WordPiece when segmented according to the chosen WordPiece model . The greedy algorithm for this optimization problem is described in more detail in Sennrich et al ( 2016 ) . Most published BERT models use a vocabulary of 30522 Word - Pieces , obtained by running the above algorithm on the Wikipedia and BooksCorpus ( Zhu et al , 2015 ) corpora with a desired vocabulary size D of 30000 . For our student model , we chose a target vocabulary size D of 5000 WordPiece tokens . Using the same WordPiece vocabulary generation algorithm and corpus as above , we obtain a 4928 - WordPiece vocabulary for the student model . This student vocabulary includes all ASCII characters as separate tokens , ensuring no out - of - vocabulary words upon tokenization with this vocabulary . Additionally , the 30 K teacher BERT vocabulary includes 93.9 % of the WP tokens in this 5 K student vocabulary but does not subsume it . We explore other strategies to obtain a small student vocabulary in Section 6 . For task - agnostic student models , we reuse BERT 's masked language modeling ( MLM ) task : words in context are randomly masked and predicted given the context via softmax over the model 's WP vocabulary . Thus , the output spaces for our teacher ( 30 K ) and student ( 5 K ) models are unaligned . This , coupled with both vocabularies tokenizing the same words differently , means existing distillation methods do not apply to our setting .", "entities": [[0, 1, "MethodName", "WordPiece"], [38, 39, "MethodName", "WordPiece"], [58, 59, "MethodName", "WordPiece"], [65, 66, "MethodName", "WordPiece"], [90, 91, "MethodName", "BERT"], [143, 144, "MethodName", "WordPiece"], [149, 150, "MethodName", "WordPiece"], [163, 164, "MethodName", "WordPiece"], [201, 202, "MethodName", "BERT"], [219, 220, "DatasetName", "subsume"], [245, 246, "MethodName", "BERT"], [247, 250, "TaskName", "masked language modeling"], [251, 252, "DatasetName", "MLM"], [255, 258, "DatasetName", "words in context"], [267, 268, "MethodName", "softmax"]]}
{"text": "For evaluation , we finetune the student model just as one would finetune the original BERT model i.e. , without using the teacher model or any taskspecific distillation . We describe our experiments below , with dataset details left to the appendix .", "entities": [[15, 16, "MethodName", "BERT"]]}
{"text": "We fine - tune and evaluate the distilled student models on two classes of language understanding tasks : MNLI : Multi - Genre Natural Language Inference ( Williams et al , 2018 ) , a 3 - way sentence pair classification task with 393 K training instances . SST - 2 : Stanford Sentiment Treebank ( Socher et al , 2013 ) , a 2 - way sentence classification task with 67 K training instances . Spoken Language Understanding : Since we are also keen on edge device applications , we also evaluate on spoken language understanding , a practical task in dialogue systems . We use the SNIPS dataset ( Coucke et al , 2018 ) of \u223c14 K virtual assistant queries , each comprising one of 7 intents and values for one or more of the 39 pre - defined slots . The intent detection and slot filling subtasks are modeled respectively as 7 - way sentence classification and sequence tagging with IOB slot labels .", "entities": [[18, 19, "DatasetName", "MNLI"], [23, 26, "TaskName", "Natural Language Inference"], [48, 49, "DatasetName", "SST"], [67, 69, "TaskName", "sentence classification"], [76, 79, "TaskName", "Spoken Language Understanding"], [94, 97, "TaskName", "spoken language understanding"], [108, 109, "DatasetName", "SNIPS"], [145, 147, "TaskName", "intent detection"], [148, 150, "TaskName", "slot filling"], [158, 160, "TaskName", "sentence classification"]]}
{"text": "For GLUE , we train student models with 6 and 12 layers , 4 attention heads , and embedding / hidden dimensions fixed to 256 , each using a compact 5 K - WP vocabulary . We also evaluate baselines without knowledge distillation ( NoKD ) , parameterized identically to the distilled student models ( incl . the 5 K vocabulary ) , trained on the MLM teacher objective from scratch . We also compare our models on GLUE with the following approaches : DistilBERT ( Sanh et al , 2019 ) structures for an optimized student model . For SNIPS , we shift our focus to smaller , lowlatency models for on - device use cases . Here , we train student models with 6 layers and embedding / hidden dimensions { 96 , 192 , 256 } . The smaller models here may not be competitive on GLUE but are adequate for practical tasks such as spoken LU . We compare with two strong baselines : BERT BASE ( Chen et al , 2019a ) with intent and IOB slot tags predicted using the [ CLS ] and the first WP tokens of each word respectively , and StackProp ( Qin et al , 2019 ) , which uses a series of smaller recurrent and self - attentive encoders .", "entities": [[1, 2, "DatasetName", "GLUE"], [41, 43, "MethodName", "knowledge distillation"], [66, 67, "DatasetName", "MLM"], [78, 79, "DatasetName", "GLUE"], [84, 85, "MethodName", "DistilBERT"], [100, 101, "DatasetName", "SNIPS"], [149, 150, "DatasetName", "GLUE"], [168, 169, "MethodName", "BERT"], [169, 170, "MethodName", "BASE"]]}
{"text": "We propose a novel approach to knowledge distillation for BERT , focusing on using a significantly smaller vocabulary for the student BERT models . Our mixed - vocabulary training method encourages implicit alignment of the teacher and student Word - Piece embeddings . Our highly - compressed 6 and 12 - layer distilled student models are optimized for on - device use cases and demonstrate competitive performance on both benchmark datasets and practical tasks . Our technique is unique in targeting the student vocabulary size , enabling easy combination with most BERT distillation methods .", "entities": [[6, 8, "MethodName", "knowledge distillation"], [9, 10, "MethodName", "BERT"], [21, 22, "MethodName", "BERT"], [91, 92, "MethodName", "BERT"]]}
{"text": "In the last decade , natural language processing and machine learning - in particular deep learning - have come a long way towards building an automated dialogue system . In a fully automated dialogue system , the goal is to predict an appropriate response given the dialogue history . This problem of response prediction can be formulated in two ways . One is purely generative , where the task is to generate a text response , i.e. generating a sentence or utterance from scratch , whereas the other is Next Utterance Selection , where the task is to select an appropriate response from a set of given candidates . Despite significant research in text generation , a pure generative model capable of generating syntactically and semantically correct text still remains a distant reality . There have been several efforts such as ( Vinyals and Le , 2015 ; Serban et al , 2016a ; Serban et al , 2016b ; Serban et al , 2017b ) for the task of dialogue generation , however these models still do not seem to work in practice ( Liu et al , 2016 ) . This is particularly true for open domain dialogue systems . Dialogue generation in a task - oriented oriented dialogue system , such as flight - booking and troubleshooting , is much easier than in a non - task oriented dialogue system . This level of difficulty arises because a non - task - oriented dialogue system has no predefined goal ( or domain ) , and the vocabulary and possibilities of the dialogues could be endless . Given these challenges , researchers have defined a simpler problem for conversation modeling based on retrieval , i.e. next utterance selection . In this paper we use this second formulation of the problem , and show that using additional information available in the form of dialogue acts help in improving the performance of the underlying model . Dialogue acts ( DA ) are higher level semantic abstractions assigned to utterances in a conversation . An example of a dialogue act for an utterance i 'll give you a call tonight is Inform since speaker is providing information . In a traditional dialogue system , where dialogues are formulated by first sentence planning and then by surface realization , the first step is to understand the dialogue act of the utterance that needs to be generated , and then plan and realize the dialogue accordingly . To better understand the importance of dialogue acts , consider an example of a simple conversation , where if the previous utterance is of type Question then the next utterance is most likely going to be of the type , i.e. Inform , providing information to that question . Knowing that the next utterance is of type Inform , a conversation system with support of dialogue act information can filter a set of candidate responses , and select the most appropriate one . Driven by this intuition , we hypothesize that understanding dialogue acts and using them in the task of next utterance selection should improve the performance irrespective of the underlying model . Driven by this intuition , we hypothesize that understanding dialogue acts and using them in the task of next utterance selection should improve the performance irrespective of the underlying model . Most of the existing literature for the task of next utterance selection can be classified into two categories . First is based on Sequence - to - sequence models ( generative models ) ( Serban et al , 2016a ; Serban et al , 2017a ; Vinyals and Le , 2015 ) , where a model is trained to generate a response given context ; and the other is Siamese models ( discriminative models ) ( Lowe et al , 2017 ) , where a model is trained to discriminate between positive and negative responses for a similar context . In both types of models , at test time , a set of candidate responses is provided consisting of one correct response and several incorrect responses , and the model is evaluated on its ability to assign a higher rank to the true response . In this paper , through the experimentation with both generative and discriminative types of models , we validate the hypothesis that additional information available in the form of dialogue act significantly improves the performance irrespective of the underlying model . In addition to showing the utility of dialogue acts , we propose a novel model that can use the sequential dialogue act information in a natural way . More specifically , we propose a dialogue - act - driven hierarchical Siamese model . Hierarchical models have shown to perform better than non - hierarchical models for the task of dialogue generation , whereas Siamese models have been shown to outperform the encoder - decoder based models for the task of next utterance selection . In this paper , we combine both of these models , and further enhance them with a dialogue act encoder . The proposed model has a hierarchical encoder which encodes the past utterances , and combine them with the representation of additional contextual information , obtained from the dialogue acts associated with the past utterances , to discriminate the correct response from the incorrect ones . Our proposed model provides us the best of both worlds and outperforms the baseline models by a significant margin . Among others , a key contribution of this paper is that we do a deeper analysis of the reasons for the performance improvement due to inclusion of dialogue act and draw several important key insights such as , dialogue acts induce uniformity in the data , they aid in learning the right patterns . We believe that these insights would inspire new research in this field and push the boundary even further . The main contributions of this paper are as follows : 1 . For the task of next utterance selection , we validate the hypothesis that additional information available in the form of dialogue acts improves the performance irrespective of the underlying models . 2 . We propose a novel model that combines the strength of Siamese network with strengths of hierarchical structure inherent in the conversations and dialogue act information . The model gives us the best of all , and outperforms the baseline models by a significant margin on the DailyDialog Dataset . 3 . We perform a deeper analysis of the utility of the dialogue act information and draw three key insights : models learn dominant dialogue act patterns ; dialogue acts induce uniformity ; dialogue acts reinforce correct dialogue act patterns . 4 . We modify the DailyDialog ( Li et al , 2017b ) dataset for the task of next utterance selection , and release it publicly along with the code - base of the proposed model 1 . We believe that this dataset will work as a benchmark dataset for further research on this problem . Similar benchmark datasets have been released earlier , however they do not come with dialogue act information .", "entities": [[113, 115, "TaskName", "text generation"], [170, 172, "TaskName", "dialogue generation"], [202, 204, "TaskName", "Dialogue generation"], [803, 805, "TaskName", "dialogue generation"], [1042, 1044, "MethodName", "Siamese network"], [1078, 1079, "DatasetName", "DailyDialog"], [1127, 1128, "DatasetName", "DailyDialog"]]}
{"text": "A simple encoder - decoder treats the first K utterances as a single long chain of words , and therefore fails to leverage the hierarchical structure , which is an inherent part of a conversation . Hierarchy is important for conversation modeling since it captures the natural dependency among utterances . Several researchers ( Sordoni et al , 2015 ; Serban et al , 2016b ; Serban et al , 2017b ; Dehghani et al , 2017 ; Kumar et al , 2017 ) have shown that hierarchical models outperform standard non - hierarchical models . Hierarchical models use two encoders to capture the hierarchical structure . The first encoder , referred as utterance encoder , operates at the utterance level , encoding each word in each utterance . The second encoder , referred as conversation encoder , operates at the conversation level , encoding each utterance in the conversation , based on the representations of the previous encoder . These two encoders make sure that the output of the conversation encoder captures the dependencies among utterances . For a given conversation , each word w k of each utterance u j is processed by an embedding layer , followed by an RNN which serves as the utterance encoder . Similar to the encoder in equation ( 1 ) , an utterance encoder gives us a sequence of representations v 1 , v 2 , . . . v K , corresponding to the first K utterances u 1 , u 2 , . . . u K in a conversation . These representations are passed on to the conversation encoder , another RNN , which transforms v j to another representation g j . The representation obtained from the last time - step of the conversation - level encoder i.e. g K is considered as the representation of the entire conversation and used to initialize the decoder which works in the same way as Equation 2 .", "entities": [[78, 79, "DatasetName", "Kumar"]]}
{"text": "In our problem setting , we require a dataset that is of reasonable size 2 and has utterances annotated with the corresponding dialogue acts . Although there are several available datasets , such as SwDA ( Switchboard Dialogue Act Corpus ( Jurafsky , 1997 ) ) , MRDA ( Meeting Recorder Dialogue Act corpus ( Janin et al , 2003 ) ) , Ubuntu , OpenSubtitles ( Tiedemann , 2009 ) , etc . , they are not really suitable for our problem setting . Most of these datasets do not come with dialogue acts , and the ones which do ( i.e. SWDA and MRDA ) are small in size . Note that the SwDA and MRDA datasets contain 1003 and 51 conversations , respectively . To the best of our knowledge , a recently released dataset , DailyDialog ( Li et al , 2017b ) , is the only dataset that has utterances annotated with dialogue acts and is large enough for conversation modeling methods to work . Furthermore , in this dataset , conversations are non - task oriented , and each conversation focuses on one topic . Each utterance is annotated with four dialogue acts as described in Table 1 . The dataset has train , validation , and test splits of 11118 , 1000 , and 1000 conversations , respectively . We evaluate and report our results on the DailyDialog dataset . In this paper , we hypothesize that dialogue acts improve conversation modeling . However , it is not always possible that such dialogue acts are available in practice , and it would be ideal to predict dialogue acts first ( Kumar et al , 2017 ) , and then use them for next utterance generation / retrieval ; having a model where both tasks , i.e. prediction and generation , are performed simultaneously may not be ideal for validating the hypothesis . Note that the error from the dialogue act prediction may propagate to the next utterance generation / retrieval . Therefore , we intentionally did not use the predicted dialogue acts ( rather used the available dialogue acts ) to make sure that the insights about the usefulness of the dialogue acts are not corrupted due to the error in the upstream prediction model .", "entities": [[47, 48, "DatasetName", "MRDA"], [65, 66, "DatasetName", "OpenSubtitles"], [105, 106, "DatasetName", "MRDA"], [117, 118, "DatasetName", "MRDA"], [139, 140, "DatasetName", "DailyDialog"], [234, 235, "DatasetName", "DailyDialog"], [277, 278, "DatasetName", "Kumar"]]}
{"text": "A speaker is providing information by means of a question or statement Question A speaker intends to obtain information by asking a question Directive A speaker is requesting , accept / reject offer , or making a suggestion Comissive A speaker accept / reject a request or suggestion Table 1 : Dialogue Acts and their description available in the DailyDialog Dataset .", "entities": [[59, 60, "DatasetName", "DailyDialog"]]}
{"text": "The DailyDialog dataset in its original form is not directly useful for the task of next utterance selection , and hence requires preparation . The dataset has the dialogues from both the speakers . Owing to the different conversational style of human and conversation agent , our objective is to build a model that is specific to the agent , i.e. bot . Therefore , we need to modify the dataset in such a way that we only consider those turns where we need to predict the bot 's utterance . To clarify further , consider the example conversation given in Table 2 . The conversation has 8 utterances , and each utterances is marked with the speaker , i.e. human ( H ) and bot ( B ) . Since we are only interested in building bot - specific model , we only pick those subsequences from this conversation where the last utterance is \" B \" . This gives us three subsequences : 1 , 2 , 3 , 4 ; 3 , 4 , 5 , 6 ; 5 , 6 , 7 , 8 for a context of size 3 . In each of these sub - conversations , the first three utterances constitute the context , while the last utterance is the true response . Our training data consists of such subsequences made up of 4 utterances . In the test data , each subsequence , in addition to these 4 utterances , has 9 more utterances selected randomly from the test pool , therefore a total of 13 utterances . These 9 utterances along with the 4 th response ( i.", "entities": [[1, 2, "DatasetName", "DailyDialog"], [44, 45, "DatasetName", "agent"], [58, 59, "DatasetName", "agent"]]}
{"text": "ED - It is a vanilla sequence to sequence model that uses an utterance encoder to obtain a representation of first K utterances which is then used in a decoder to generate next utterance . HRED - An extension of sequence to sequence model that uses a hierarchical encoder to obtain a representation of first K utterances , which is then used in decoder to generate next utterance . ED - DA - An extension of the ED model which uses dialogue act information . It has a conditional decoder , that conditions the generation of each word on the dialogue acts representation . HRED - DA - An extension of the HRED model which uses dialogue act information . Similar to ED - DA , it also has a conditional decoder that conditions the generation of each word on the dialogue acts representation .", "entities": [[6, 9, "MethodName", "sequence to sequence"], [40, 43, "MethodName", "sequence to sequence"]]}
{"text": "In conversation modeling , the most basic problem is to generate a response given a context . Several efforts have been made towards solving the problem of dialogue generation ( Vinyals and Le , 2015 ; Liu et al , 2016 ; Li et al , 2015 ) , however , due to the inherent difficulty of the problem , these efforts have only had limited success and are known to have issues like generating repetitive and generalized responses such as I do n't know or Ok . For the task of Next Utterance Selection , which is a relatively simpler problem than generation , though existing generative models can be easily adopted , their counterpart discriminative models have shown to have better performance . In generative models , the most notable work is from ( Vinyals and Le , 2015 ) , however this work considers the context as a flat long string of words and ignores the hierarchical structure . Researchers have proposed hierarchical model ( Serban et al , 2016b ) and their variations ( Serban et al , 2017b ; Serban et al , 2017a ; Li et al , 2017a ) but none of these models take into account the dialogue act information . In Discriminative models , such as Siamese , a very notable work by ( Kannan et al , 2016 ) , smart reply , retrieves the most likely response from a set of candidate response clusters . ( Lowe et al , 2017 ) has used a retrieval based Siamese model and shown its results on the Ubuntu corpus . Our proposed model builds upon the strengths of generative and discriminative models , and uses hierarchy along with the dialogue act information to achieve the best performance . A recent work by ( Zhao et al , 2017 ) has used dialogue acts for the task of dialogue generation . Our work complements their findings , and further show that dialogue acts improve the model performance across the board irrespective of underlying model ( i.e. generative or discriminative models ) and for the task of next utterance selection .", "entities": [[27, 29, "TaskName", "dialogue generation"], [316, 318, "TaskName", "dialogue generation"]]}
{"text": "In this paper , we present our systems for the MADAR Shared Task : Arabic Fine - Grained Dialect Identification . The shared task consists of two subtasks . The goal of Subtask - 1 ( S - 1 ) is to detect an Arabic city dialect in a given text and the goal of Subtask - 2 ( S - 2 ) is to predict the country of origin of a Twitter user by using tweets posted by the user . In S - 1 , our proposed systems are based on language modelling . We use language models to extract features that are later used as an input for other machine learning algorithms . We also experiment with recurrent neural networks ( RNN ) , but these experiments showed that simpler machine learning algorithms are more successful . Our system achieves 0.658 macro F 1 - score and our rank is 6 th out of 19 teams in S - 1 and 7 th in S - 2 with 0.475 macro F 1 - score .", "entities": [[18, 20, "TaskName", "Dialect Identification"], [93, 95, "TaskName", "language modelling"]]}
{"text": "In S - 1 , both of our systems used for the official submission take as an input language model features . In our case the objective of a language model in its simplest form is to predict probability p ( S ) of sentence S which is composed from strings ( words or character n - grams ) s 1 , s 2 . . . s N , where N is a number of strings in the sentence . The probability estimation of p ( S ) can be computed as a product of conditional probabilities p ( s i | h i ) of its strings s 1 , s 2 . . . s N , where h i is a history of a string s i . The probability of string s i is conditioned by history h i i.e. n \u2212 1 preceding strings s i\u2212n+1 , s i\u2212n+2 , . . . s i\u22121 which can be rewritten as s i\u22121 i\u2212n+1 . The resulting formula for the p ( S ) estimation looks as follows : p ( S ) = N i=1 p ( s i | h i ) = N i=1 p ( s i | s i\u22121 i\u2212n+1 ) ( 1 ) The conditioned probability p ( s i | h i ) can be estimated with Maximum Likelihood Estimate ( MLE ) which is defined as : p M LE ( s i | h i ) = c ( s i\u2212n+1 , s i\u2212n+2 . . . s i ) c ( s i\u2212n+1 , s i\u2212n+2 . . . s i\u22121 ) ( 2 ) where c ( s i\u2212n+1 , s i\u2212n+2 . . . s i ) is a number of occurrences of string s i with history h i and c ( s i\u2212n+1 , s i\u2212n+2 . . . s i\u22121 ) is a number of occurrences of history h i . These counts are taken from a training corpus . We followed Salameh ) in using the kenlm language modelling tool ( Heafield et al , 2013 ) . kenlm does n't have an option to use character n - grams instead of words , so in order to get character - based language models , we prepared input files with characters separated by spaces . Instead of encoding space as a special word , we surrounded words with a < w></w > pair . This enables noticing strings which occur at the beginning or end of a word ( as would a special sequence for space ) but reduces the possible amount of inter - word information which the language model can keep for a given order , the parameter which indicates to kenlm the largest n - gram to index . We used order 5 for all our kenlm language models . We prebuilt models for each dialect . We prepared six directories , each containing word or character models for each dialect in one of the three corpora . We wrote a LangModel class which quacks like a sklearn classifier , that is , it supports fit ( ) , predict ( ) , and predict proba ( ) , but its choices are based on a directory of language models . predict ( ) returns the dialect name whose model gives the highest score . predict proba ( ) provides a list of languagemodel - score features , adjusted to probabilities .", "entities": [[348, 350, "TaskName", "language modelling"]]}
{"text": "This submission uses a jumble of features and classifiers , most from the sklearn module ( Buitinck et al , 2013 ) . The final classifier is a hard voting classifier with three input streams : 1 . Soft voting classifier on : on language - model - scores for character and language models on the corpus - 6 language models and character language models for the corpus - 26 language models . 2 . Support vector machine , svm . SVC ( gamma='scale ' , kernel = ' poly ' , degree = 2 ) with the same features as item 1e .", "entities": [[75, 78, "MethodName", "Support vector machine"], [79, 80, "MethodName", "svm"]]}
{"text": "Fully Quantized Transformer for Machine Translation", "entities": [[2, 3, "MethodName", "Transformer"], [4, 6, "TaskName", "Machine Translation"]]}
{"text": "In this section , we review a broad spectrum of quantization and pruning methods for neural network compression .", "entities": [[10, 11, "TaskName", "quantization"], [15, 18, "TaskName", "neural network compression"]]}
{"text": "Unlike Jacob et al ( 2017 ) , we do not nudge the domain so that the zero value gets perfectly mapped . The only zero values which we have to deal with are the padding , the Softmax numerator and output , the output of ReLU layers and dropouts . Since padding has no effect on the final output , we completely ignore these values when quantizing . For ReLUs and the Softmax 's numerator and output , we fix their x min to 0 , which guarantees the perfect mapping of the value . Finally , quantization is applied before any dropout operation . Indeed , even though the zeros added to the output of the quantization layer might not be part of the domain , this only happens during training .", "entities": [[38, 39, "MethodName", "Softmax"], [46, 47, "MethodName", "ReLU"], [73, 74, "MethodName", "Softmax"], [85, 86, "DatasetName", "0"], [98, 99, "TaskName", "quantization"], [118, 119, "TaskName", "quantization"]]}
{"text": "In this section , we present the results of our full quantization scheme on various tasks . We first compare our method on a machine translation setup . Then we present the results of numerous ablation studies . We also compare the impact of delaying quantization on translation quality . Finally , we evaluate our method on two language model tasks and experiment with node pruning .", "entities": [[11, 12, "TaskName", "quantization"], [24, 26, "TaskName", "machine translation"], [45, 46, "TaskName", "quantization"]]}
{"text": "TOD - BERT : Pre - trained Natural Language Understanding for Task - Oriented Dialogue", "entities": [[2, 3, "MethodName", "BERT"], [7, 10, "TaskName", "Natural Language Understanding"]]}
{"text": "The underlying difference of linguistic patterns between general text and task - oriented dialogue makes existing pre - trained language models less useful in practice . In this work , we unify nine human - human and multi - turn task - oriented dialogue datasets for language modeling . To better model dialogue behavior during pre - training , we incorporate user and system tokens into the masked language modeling . We propose a contrastive objective function to simulate the response selection task . Our pre - trained task - oriented dialogue BERT ( TOD - BERT ) outperforms strong baselines like BERT on four downstream taskoriented dialogue applications , including intention recognition , dialogue state tracking , dialogue act prediction , and response selection . We also show that TOD - BERT has a stronger few - shot ability that can mitigate the data scarcity problem for task - oriented dialogue .", "entities": [[67, 70, "TaskName", "masked language modeling"], [92, 93, "MethodName", "BERT"], [96, 97, "MethodName", "BERT"], [102, 103, "MethodName", "BERT"], [114, 117, "TaskName", "dialogue state tracking"], [132, 133, "MethodName", "BERT"]]}
{"text": "Pre - trained models with self - attention encoder architectures ( Devlin et al , 2018 ; have been commonly used in many NLP applications . Such models are self - supervised based on a massive scale of general text corpora , such as English Wikipedia or books ( Zhu et al , 2015 ) . By further fine - tuning these representations , breakthroughs have been continuously reported for various downstream tasks , especially natural language understanding . However , previous work ( Rashkin et al , 2018 ; Wolf et al , 2019 ) shows that there are some deficiencies in the performance to apply fine - tuning on conversational corpora directly . One possible reason could be the intrinsic difference of linguistic patterns between human conversations and writing text , resulting in a large gap of data distributions ( Bao et al , 2019 ) . Therefore , pre - training dialogue language models using chit - chat corpora from social media , such as Twitter or Reddit , has been recently investigated , especially for dialogue response generation ( Zhang et al , 2019 ) and retrieval ( Henderson et al , 2019b ) . Although these opendomain dialogues are diverse and easy - to - get , they are usually short , noisy , and without specific chatting goals . On the other hand , a task - oriented dialogue has explicit goals ( e.g. restaurant reservation or ticket booking ) and many conversational interactions . But each dataset is usually small and scattered because obtaining and labeling such data is time - consuming . Moreover , a task - oriented dialogue has explicit user and system behaviors where a user has his / her goal , and a system has its belief and database information , which makes the language understanding component and dialogue policy learning more important than those chit - chat scenarios . This paper aims to prove this hypothesis : selfsupervised language model pre - training using taskoriented corpora can learn better representations than existing pre - trained models for task - oriented downstream tasks . We emphasize that what we care about the most is not whether our pre - trained model can achieve state - of - the - art results on each downstream task since most of the current best models are built on top of pre - trained models , and ours can easily replace them . We avoid adding too many additional components on top of the pre - training architecture when fine - tuning in our experiments . We collect and combine nine human - human and multi - turn task - oriented dialogue corpora to train a task - oriented dialogue BERT ( TOD - BERT ) . In total , there are around 100k dialogues with 1.4 M utterances across over 60 different domains . Like BERT ( Devlin et al , 2018 ) , TOD - BERT is formulated as a masked language model and uses a deep bidirectional Transformer ( Vaswani et al , 2017 ) encoder as its model architecture . Unlike BERT , TOD - BERT incorporates two special tokens for user and system to model the corresponding dialogue behavior . A contrastive objective function of response selection task is combined during pretraining stage to capture response similarity . We select BERT because it is the most widely used model in NLP research recently , and our unified datasets can be easily applied to pre - train any existing language models . We test TOD - BERT on task - oriented dialogue systems on four core downstream tasks , including intention recognition , dialogue state tracking , dialogue act prediction , and response selection . What we observe is : TOD - BERT outperforms BERT and other strong baselines such as GPT - 2 ( Radford et al , 2019 ) and DialoGPT ( Zhang et al , 2019 ) on all the selected downstream tasks , which further confirms its effectiveness for improving dialogue language understanding . We find that response contrastive learning is beneficial , but it is currently overlooked not well - investigated in dialogue pretraining research . More importantly , TOD - BERT has a stronger few - shot ability than BERT on each task , suggesting that it can reduce the need for expensive human - annotated labels . TOD - BERT can be easily leveraged and adapted to a new taskoriented dialogue dataset . Our source code and data processing are released to facilitate future research on pre - training and fine - tuning of task - oriented dialogue 1 .", "entities": [[75, 78, "TaskName", "natural language understanding"], [170, 171, "DatasetName", "Reddit"], [180, 182, "TaskName", "response generation"], [457, 458, "MethodName", "BERT"], [461, 462, "MethodName", "BERT"], [483, 484, "MethodName", "BERT"], [494, 495, "MethodName", "BERT"], [507, 508, "MethodName", "Transformer"], [522, 523, "MethodName", "BERT"], [526, 527, "MethodName", "BERT"], [562, 563, "MethodName", "BERT"], [597, 598, "MethodName", "BERT"], [599, 604, "TaskName", "task - oriented dialogue systems"], [614, 617, "TaskName", "dialogue state tracking"], [633, 634, "MethodName", "BERT"], [635, 636, "MethodName", "BERT"], [642, 643, "MethodName", "GPT"], [683, 685, "MethodName", "contrastive learning"], [707, 708, "MethodName", "BERT"], [716, 717, "MethodName", "BERT"], [737, 738, "MethodName", "BERT"]]}
{"text": "General Pre - trained Language Models , which are trained on massive general text such as Wikipedia and BookCorpus , can be roughly divided into two categories : uni - directional or bidirectional attention mechanisms . GPT ( Radford et al , 2018 ) and GPT - 2 ( Radford et al , 2019 ) are representatives of uni - directional language models using a Transformer decoder , where the objective is to maximize left - to - right generation likelihood . These models are commonly applied in natural language generation tasks . On the other hand , BERT ( Devlin et al , 2018 ) , RoBERTa , and their variances are pre - trained using a Transformer encoder with bi - directional token prediction . These models are usually evaluated on classification tasks such as GLUE benchmark ( Wang et al , 2018 ) or span - based question answering tasks ( Ra - 1 github.com/jasonwu0731/ToD - BERT jpurkar et al , 2016 ) . Some language models can support both unidirectional and bi - directional attention , such as UniLM ( Dong et al , 2019 ) . Conditional language model pre - training is also proposed . For example , CTRL ( Keskar et al , 2019 ) is a conditional Transformer model , trained to condition on control codes that govern style , content , and task - specific behavior . Recently , multi - task language model pretraining with unified sequence - to - sequence generation is proposed . Text - to - text Transformer ( T5 ) ( Raffel et al , 2019 ) unifies multiple text modeling tasks and achieves the promising results in various NLP benchmarks . Dialogue Pre - trained Language Models are mostly trained on open - domain conversational data from Reddit or Twitter for dialogue response generation . Transfertransfo ( Wolf et al , 2019 ) achieves good performance on ConvAI - 2 dialogue competition using GPT - 2 . DialoGPT ( Zhang et al , 2019 ) is an extension of GPT - 2 that is pre - trained on Reddit data for open - domain response generation . Con - veRT ( Henderson et al , 2019a ) pre - trained a dual transformer encoder for response selection task on large - scale Reddit ( input , response ) pairs . PLATO ( Bao et al , 2019 ) uses both Twitter and Reddit data to pre - trained a dialogue generation model with discrete latent variables . All of them are designed to cope with the response generation task for opendomain chatbots . Pretraining for task - oriented dialogues , on the other hand , has few related works . Budzianowski and Vuli\u0107 ( 2019 ) first apply the GPT - 2 model to train on response generation task , which takes system belief , database result , and last dialogue turn as input to predict next system responses . It only uses one dataset to train its model because few public datasets have database information available . Henderson et al ( 2019b ) pre - trained a response selection model for task - oriented dialogues . They first pre - train on Reddit corpora and then fine - tune on target dialogue domains , but their training and fine - tuning code is not released . Peng et al ( 2020 ) focus on the natural language generation ( NLG ) task , which assumes dialogue acts and slot - tagging results are given to generate a natural language response . Pre - training on a set of annotated NLG corpora can improve conditional generation quality using a GPT - 2 model . Name # Dialogue # Utterance Avg . Turn # Domain MetaLWOZ 37 , 884 432 , 036 11.4 47 Schema ( Rastogi et al , 2019 ) 22 , 825 463 , 284 20.3 17 Taskmaster ( Byrne et al , 2019 ) 13 , 215 303 , 066 22.9 6 MWOZ ( Budzianowski et al , 2018 ) 10 , 420 71 , 410 6.9 7 MSR - E2E 10 , 087 74 , 686 7.4 3 SMD ( Eric and Manning , 2017 ) 3 , 031 15 , 928 5.3 3 Frames ( Asri et al , 2017 ) 1 , 369 19 , 986 14.6 3 WOZ ( Mrk\u0161i\u0107 et al , 2016 ) 1 , 200 5 , 012 4.2 1 CamRest676 676 2 , 744 4.1 1", "entities": [[0, 1, "DatasetName", "General"], [18, 19, "DatasetName", "BookCorpus"], [36, 37, "MethodName", "GPT"], [45, 46, "MethodName", "GPT"], [65, 67, "MethodName", "Transformer decoder"], [98, 99, "MethodName", "BERT"], [107, 108, "MethodName", "RoBERTa"], [118, 119, "MethodName", "Transformer"], [137, 138, "DatasetName", "GLUE"], [150, 152, "TaskName", "question answering"], [159, 160, "MethodName", "BERT"], [204, 205, "MethodName", "CTRL"], [215, 216, "MethodName", "Transformer"], [260, 261, "MethodName", "Transformer"], [262, 263, "MethodName", "T5"], [302, 303, "DatasetName", "Reddit"], [307, 309, "TaskName", "response generation"], [328, 329, "MethodName", "GPT"], [344, 345, "MethodName", "GPT"], [353, 354, "DatasetName", "Reddit"], [359, 361, "TaskName", "response generation"], [387, 388, "DatasetName", "Reddit"], [407, 408, "DatasetName", "Reddit"], [414, 416, "TaskName", "dialogue generation"], [431, 433, "TaskName", "response generation"], [464, 465, "MethodName", "GPT"], [471, 473, "TaskName", "response generation"], [538, 539, "DatasetName", "Reddit"], [614, 615, "MethodName", "GPT"], [629, 630, "DatasetName", "MetaLWOZ"], [688, 689, "DatasetName", "E2E"], [697, 698, "DatasetName", "SMD"]]}
{"text": "We collect nine different task - oriented datasets which are English , human - human and multi - turn . In total , there are 100 , 707 dialogues , which dialogue competition . Schema ( Rastogi et al , 2019 ) : Schema - guided dialogue has 22 , 825 dialogues and provides a challenging testbed for several tasks , in particular , dialogue state tracking . Each schema is a set of tracking slots , and each domain could have multiple possible schemas . This allows a single dialogue system to support many services and facilitates the simple integration of new services without requiring much training data . The Schema dataset is used as the dialogue state tracking task for DSTC8 dialogue competition . Taskmaster ( Byrne et al , 2019 ) : This dataset includes 13 , 215 dialogues comprising six do - mains , including 5 , 507 spoken and 7 , 708 written dialogs created with two distinct procedures . One is a two - person Wizard of Oz approach that one person acts like a robot , and the other is a self - dialogue approach in which crowdsourced workers wrote the entire dialog themselves . It has 22.9 average conversational turns in a single dialogue , which is the longest among all taskoriented datasets listed . MWOZ ( Budzianowski et al , 2018 ) : Multi - Domain Wizard - of - Oz dataset contains 10 , 420 dialogues over seven domains , and it has multiple domains in a single dialogue . It has a detailed description of the data collection procedure , user goal , system act , and dialogue state labels . Different from most of the existing corpora , it also provides full database information . MSR - E2E : Microsoft end - toend dialogue challenge has 10 , 087 dialogues in three domains , movie - ticket booking , restaurant reservation , and taxi booking . It also includes an experiment platform with built - in simulators in each domain . SMD ( Eric and Manning , 2017 ) : Stanford multidomain dialogue is an in - car personal assistant dataset , comprising 3 , 301 dialogues and three domains : calendar scheduling , weather information retrieval , and point - of - interest navigation . It is designed to smoothly interface with knowledge bases , where a knowledge snippet is attached with each dialogue as a piece of simplified database information . Frames ( Asri et al , 2017 ) : This dataset comprises 1 , 369 human - human dialogues with an average of 14.6 turns per dialogue , where users are given some constraints to book a trip and assistants who search a database to find appropriate trips . Unlike other datasets , it has labels to keep track of different semantic frames , which is the decision - making behavior of users throughout each dialogue . WOZ ( Mrk\u0161i\u0107 et al , 2016 ) and Cam - Rest676 ( Wen et al , 2016 ) : These two corpora use the same data collection procedure and same ontology from DSTC2 ( Henderson et al , 2014 ) . They are one of the first task - oriented dialogue datasets that use Wizard of Oz style with text input instead of speech input , which improves the model 's capacity for the semantic understanding instead of its robustness to automatic speech recognition errors .", "entities": [[64, 67, "TaskName", "dialogue state tracking"], [117, 120, "TaskName", "dialogue state tracking"], [235, 240, "DatasetName", "Wizard - of - Oz"], [299, 300, "DatasetName", "E2E"], [343, 344, "DatasetName", "SMD"], [377, 379, "TaskName", "information retrieval"], [523, 524, "MethodName", "ontology"], [574, 577, "TaskName", "automatic speech recognition"]]}
{"text": "We pick up several datasets , OOS , DSTC2 , GSIM , and MWOZ , for downstream evaluation . The first three corpora are not included in the pre - trained task - oriented datasets . For MWOZ , to be fair , we do not include its test set dialogues during the pretraining stage . Details of each evaluation dataset are discussed in the following : OOS ( Larson et al , 2019 ) : The out - of - scope intent dataset is one of the largest annotated intent datasets , including 15 , 100/3 , 100/5 , 500 samples for the train , validation , and test sets , respectively . It covers 151 intent classes over ten domains , including 150 in - scope intent and one outof - scope intent . The out - of - scope intent means that a user utterance that does not fall into any of the predefined intents . Each of the intents has 100 training samples . DSTC2 ( Henderson et al , 2014 ) : DSTC2 is a human - machine task - oriented dataset that may include a certain system response noise . It has 1 , 612/506/1117 dialogues for train , validation , and test sets , respectively . We follow to map the original dialogue act labels to universal dialogue acts , which results in 9 different system dialogue acts . GSIM ( Shah et al , 2018a ) : GSIM is a humanrewrote machine - machine task - oriented corpus , including 1500/469/1039 dialogues for the train , validation , and test sets , respectively . We combine its two domains , movie and restaurant domains , into one single corpus . It is collected by Machines Talking To Machines ( M2 M ) ( Shah et al , 2018b ) approach , a functionality - driven process combining a dialogue self - play step and a crowdsourcing step . We map its dialogue act labels to universal dialogue acts , resulting in 6 different system dialogue acts . MWOZ ( Budzianowski et al , 2018 ) : MWOZ is the most common benchmark for task - oriented dialogues , especially for dialogue state tracking . It has 8420/1000/1000 dialogues for train , validation , and test sets , respectively . Across seven different domains , in total , it has 30 ( domain , slot ) pairs that need to be tracked in the test set . We use its revised version MWOZ 2.1 , which has the same dialogue transcripts but with cleaner state label annotation .", "entities": [[368, 371, "TaskName", "dialogue state tracking"]]}
{"text": "Before fine - tuning each pre - trained models , we first investigate their feature extraction ability by probing their output representations . Probing methods are proposed to determine what information is carried intrinsically by the learned embeddings ( Tenney et al , 2019 ) . We probe the output representation using one single - layer perceptron on top of a \" fixed \" pre - trained language model and only finetune that layer for a downstream task with the same hyper - parameters . Table 3 shows the probing results of domain classification on MWOZ , intent identification on OOS , and dialogue act prediction on MWOZ . TOD - BERT - jnt achieves the highest performance in this setting , suggesting its representation contains the most useful information .", "entities": [[111, 112, "MethodName", "BERT"]]}
{"text": "We propose task - oriented dialogue BERT ( TOD - BERT ) trained on nine human - human and multiturn task - oriented datasets across over 60 domains . TOD - BERT outperforms BERT on four dialogue downstream tasks , including intention classification , dialogue state tracking , dialogue act prediction , and response selection . It also has a clear advantage in the few - shot experiments when only limited labeled data is available . TOD - BERT is easy - to - deploy and will be open - sourced , allowing the NLP research community to apply or fine - tune any task - oriented conversational problem .", "entities": [[6, 7, "MethodName", "BERT"], [10, 11, "MethodName", "BERT"], [31, 32, "MethodName", "BERT"], [33, 34, "MethodName", "BERT"], [44, 47, "TaskName", "dialogue state tracking"], [78, 79, "MethodName", "BERT"]]}
{"text": "Transformers are being used extensively across several sequence modeling tasks . Significant research effort has been devoted to experimentally probe the inner workings of Transformers . However , our conceptual and theoretical understanding of their power and inherent limitations is still nascent . In particular , the roles of various components in Transformers such as positional encodings , attention heads , residual connections , and feedforward networks , are not clear . In this paper , we take a step towards answering these questions . We analyze the computational power as captured by Turing - completeness . We first provide an alternate and simpler proof to show that vanilla Transformers are Turing - complete and then we prove that Transformers with only positional masking and without any positional encoding are also Turing - complete . We further analyze the necessity of each component for the Turing - completeness of the network ; interestingly , we find that a particular type of residual connection is necessary . We demonstrate the practical implications of our results via experiments on machine translation and synthetic tasks .", "entities": [[161, 163, "MethodName", "residual connection"], [177, 179, "TaskName", "machine translation"]]}
{"text": "Transformer ( Vaswani et al , 2017 ) is a recent selfattention based sequence - to - sequence architecture which has led to state of the art results across various NLP tasks including machine translation ( Ott et al , 2018 ) , language modeling ( Radford et al , 2018 ) and question answering ( Devlin et al , 2019 ) . Although a number of variants of Transformers have been proposed , the original architecture still underlies these variants . While the training and generalization of machine learning models such as Transformers are the central goals in their analysis , an essential prerequisite to this end is characterization of the computational power of the model : training a model for a certain task can not succeed if the model is computationally incapable of carrying out the task . While the computational capabilities of recurrent networks ( RNNs ) have been studied for decades ( Kolen and Kremer , 2001 ; Siegelmann , 2012 ) , for Transformers we are still in the early stages . The celebrated work of Siegelmann and Sontag ( 1992 ) showed , assuming arbitrary precision , that RNNs are Turing - complete , meaning that they are capable of carrying out any algorithmic task formalized by Turing machines . Recently , P\u00e9rez et al ( 2019 ) have shown that vanilla Transformers with hard - attention can also simulate Turing machines given arbitrary precision . However , in contrast to RNNs , Transformers consist of several components and it is unclear which components are necessary for its Turing - completeness and thereby crucial to its computational expressiveness . The role of various components of the Transformer in its efficacy is an important question for further improvements . Since the Transformer does not process the input sequentially , it requires some form of positional information . Various positional encoding schemes have been proposed to capture order information ( Shaw et al , 2018 ; Dai et al , 2019 ; Huang et al , 2018 ) . At the same time , on machine translation , showed that the performance of Transformers with only positional masking ( Shen et al , 2018 ) is comparable to that with positional encodings . In case of positional masking ( Fig . 1 ) , as opposed to explicit encodings , the model is only allowed to attend over preceding inputs and no additional positional encoding vector is combined with the input vector . Tsai et al ( 2019 ) raised the question of whether explicit encoding is necessary if positional masking is used . Additionally , since P\u00e9rez et al ( 2019 ) 's Turingcompleteness proof relied heavily on residual connections , they asked whether these connections are essential for Turing - completeness . In this paper , we take a step towards answering such questions . Below , we list the main contributions of the paper , We provide an alternate and arguably simpler proof to show that Transformers are Turingcomplete by directly relating them to RNNs . More importantly , we prove that Transformers with positional masking and without positional encoding are also Turing - complete . We analyze the necessity of various components such as self - attention blocks , residual connections and feedforward networks for Turing - completeness . Figure 2 provides an overview . We explore implications of our results on machine translation and synthetic tasks . 1", "entities": [[0, 1, "MethodName", "Transformer"], [33, 35, "TaskName", "machine translation"], [53, 55, "TaskName", "question answering"], [282, 283, "MethodName", "Transformer"], [296, 297, "MethodName", "Transformer"], [349, 351, "TaskName", "machine translation"], [570, 572, "TaskName", "machine translation"]]}
{"text": "Computational Power of neural networks has been studied since the foundational paper Mc - Culloch and Pitts ( 1943 ) ; in particular , among sequence - to - sequence models , this aspect of RNNs has long been studied ( Kolen and Kremer , 2001 ) . The seminal work by Siegelmann and Sontag ( 1992 ) showed that RNNs can simulate a Turing machine by using unbounded precision . Chen et al ( 2018 ) showed that RNNs with ReLU activations are also Turing - complete . Many recent works have explored the computational power of RNNs in practical settings . Several works ( Merrill et al , 2020 ) , ( Weiss et al , 2018 ) recently studied the ability of RNNs to recognize counter - like languages . The capability of RNNs to recognize strings of balanced parantheses has also been studied ( Sennhauser and Berwick , 2018 ; Skachkova et al , 2018 ) . However , such analysis on Transformers has been scarce . Theoretical work on Transformers was initiated by P\u00e9rez et al ( 2019 ) who formalized the notion of Transformers and showed that it can simulate a Turing machine given arbitrary precision . Concurrent to our work , there have been several efforts to understand self - attention based models ( Levine et al , 2020 ; Kim et al , 2020 ) . Hron et al ( 2020 ) show that Transformers behave as Gaussian processes when the number of heads tend to infinity . Hahn ( 2020 ) showed some limitations of Transformer encoders in modeling regular and context - free languages . It has been recently shown that Transformers are universal approximators of sequence - tosequence functions given arbitrary precision ( Yun et al , 2020 ) . However , these are not applicable 2 to the complete Transformer architecture . With a goal similar to ours , Tsai et al ( 2019 ) attempted to study the attention mechanism via a kernel formulation . However , a systematic study of various components of Transformers has not been done .", "entities": [[81, 82, "MethodName", "ReLU"], [245, 247, "TaskName", "Gaussian processes"], [264, 265, "MethodName", "Transformer"], [311, 312, "MethodName", "Transformer"]]}
{"text": "All the numbers used in our computations will be from the set of rational numbers denoted Q. For a sequence X = ( x 1 , . . . , x n ) , we set X j : = ( x 1 , . . . , x j ) for 1 \u2264 j \u2264 n. We will work with an alphabet \u03a3 of size m , with special symbols # and $ signifying the beginning and end of the input sequence , respectively . The symbols are mapped to vectors via a given ' base ' embedding f b : \u03a3 Q d b , where d b is the dimension of the embedding . E.g. , this embedding could be the one used for processing the symbols by the RNN . We set f b ( # ) = 0 d b and f b ( $ ) = 0 d b . Posi - tional encoding is a function pos : N Q d b . Together , these provide embedding for a symbol s at position i given by f ( f b ( s ) , pos ( i ) ) , often taken to be simply f b ( s ) + pos ( i ) . Vector s Q m denotes one - hot encoding of a symbol s \u03a3.", "entities": [[142, 143, "DatasetName", "0"], [152, 153, "DatasetName", "0"]]}
{"text": "We follow Siegelmann and Sontag ( 1992 ) in our definition of RNNs . To feed the sequences s 1 s 2 . . . s n \u03a3 * to the RNN , these are converted to the vectors x 1 , x 2 , . . . , x n where x i = f b ( s i ) . The RNN is given by the recurrence h t = g ( W h h t\u22121 + W x x t + b ) , where t \u2265 1 , function g ( ) is a multilayer feedforward network ( FFN ) with activation \u03c3 , bias vector b Q d h , matrices W h Q d h \u00d7d h and W x Q d h \u00d7d b , and h t Q d h is the hidden state with given initial hidden state h 0 ; d h is the hidden state dimension . After the last symbol s n has been fed , we continue to feed the RNN with the terminal symbol f b ( $ ) until it halts . This allows the RNN to carry out computation after having read the input . A class of seq - to - seq neural networks is Turingcomplete if the class of languages recognized by the networks is exactly the class of languages recognized by Turing machines . Theorem 3.1 . ( Siegelmann and Sontag , 1992 ) Any seq - to - seq function \u03a3 * \u03a3 * computable by a Turing machine can also be computed by an RNN . For details please see section B.1 in appendix .", "entities": [[99, 101, "MethodName", "feedforward network"], [148, 149, "DatasetName", "0"]]}
{"text": "We showed that the class of languages recognized by Transformers and RNNs are exactly the same . This implies that the difference in performance of both the networks across different tasks can be attributed only to their learning abilities . In contrast to RNNs , Transformers are composed of multiple components which are not essential for their com - putational expressiveness . However , in practice they may play a crucial role . Recently , Voita et al ( 2019 ) showed that the decoder - decoder attention heads in the lower layers of the decoder do play a significant role in the NMT task and suggest that they may be helping in language modeling . This indicates that components which are not essential for the computational power may play a vital role in improving the learning and generalization ability . Take - Home Messages . We showed that the order information can be provided either in the form of explicit encodings or masking without affecting computational power of Transformers . The decoder - encoder attention block plays a necessary role in conditioning the computation on the input sequence while the residual connection around it is necessary to keep track of previous computations . The feedforward network in the decoder is the only component capable of performing computations based on the input and prior computations . Our experimental results show that removing components essential for computational power inhibit the model 's ability to perform certain tasks . At the same time , the components which do not play a role in the computational power may be vital to the learning ability of the network . Although our proofs rely on arbitrary precision , which is common practice while studying the computational power of neural networks in theory ( Siegelmann and Sontag , 1992 ; P\u00e9rez et al , 2019 ; Hahn , 2020 ; Yun et al , 2020 ) , implementations in practice work over fixed precision settings . However , our construction provides a starting point to analyze Transformers under finite precision . Since RNNs can recognize all regular languages in finite precision ( Korsky and Berwick , 2019 ) , it follows from our construction that Transformer can also recognize a large class of regular languages in finite precision . At the same time , it does not imply that it can recognize all regular languages given the limitation due to the precision required to encode positional information . We leave the study of Transformers in finite precision for future work .", "entities": [[191, 193, "MethodName", "residual connection"], [205, 207, "MethodName", "feedforward network"], [369, 370, "MethodName", "Transformer"]]}
{"text": "We begin with various definitions and results . We define simulation of Turing machines by RNNs and state the Turing - completeness result for RNNs . We define vanilla and directional Transformers and what it means for Transformers to simulate RNNs . Many of the definitions from the main paper are reproduced here , but in more detail . In Sec . C.1 we discuss the effect of removing a residual connection on computational power of Transformers . Sec . C.2 contains the proof of Turing completeness of vanilla Transformers and Sec . D the corresponding proof for directional Transformers . Finally , Sec . 5 has further details of experiments .", "entities": [[70, 72, "MethodName", "residual connection"]]}
{"text": "Here we summarize , somewhat informally , the Turing - completeness result for RNNs due to ( Siegelmann and Sontag , 1992 ) . We recall basic notions from computability theory . In the main paper , for simplicity we stated the results for total recursive functions \u03c6 : { 0 , 1 } * { 0 , 1 } * , i.e. a function that is defined on every s { 0 , 1 } * and whose values can be computed by a Turing machine . While total recursive functions form a satisfactory formalization of seq - to - seq tasks , here we state the more general result for partial recursive functions . Let \u03c6 : { 0 , 1 } * { 0 , 1 } * be partial recursive . A partial recursive function is one that need not be defined for every s { 0 , 1 } * , and there exists a Turing Machine M with the following property . The input s is initially written on the tape of the Turing Machine M and the output \u03c6 ( s ) is the content of the tape upon acceptance which is indicated by halting in a designated accept state . On s for which \u03c6 is undefined , M does not halt . We now specify how Turing machine M is simulated by RNN R ( M ) . In the RNNs in ( Siegelmann and Sontag , 1992 ) the hidden state h t has the form h t = [ q t , \u03a8 1 , \u03a8 2 ] , where q t = [ q 1 , . . . , q s ] denotes the state of M one - hot form . Numbers \u03a8 1 , \u03a8 2 Q , called stacks , store the contents of the tape in a certain Cantor set like encoding ( which is similar to , but slightly more involved , than binary representation ) at each step . The simulating RNN R ( M ) , gets as input encodings of s 1 s 2 ... s n in the first n steps , and from then on receives the vector 0 as input in each step . If \u03c6 is defined on s , then M halts and accepts with the output \u03c6 ( s ) the content of the tape . In this case , R ( M ) enters a special accept state , and \u03a8 1 encodes \u03c6 ( s ) and \u03a8 2 = 0 . If M does not halt then R ( M ) also does not enter the accept state . Siegelmann and Sontag ( 1992 ) further show that from R ( M ) one can further explicitly produce the \u03c6 ( s ) as its output . In the present paper , we will not deal with explicit production of the output but rather work with the definition of simulation in the previous paragraph . This is for simplicity of exposition , and the main ideas are already contained in our results . If the Turing machine computes \u03c6 ( s ) in time T ( s ) , the simulation takes O ( | s | ) time to encode the input sequence s and 4 T ( s ) to compute \u03c6 ( s ) . Theorem B.1 ( ( Siegelmann and Sontag , 1992 ) ) . Given any partial recursive function \u03c6 : { 0 , 1 } * { 0 , 1 } * computed by Turing machine M \u03c6 , there exists a simulating RNN R ( M \u03c6 ) . In view of the above theorem , for establishing Turing - completeness of Transformers , it suffices to show that RNNs can be simulated by Transformers . Thus , in the sequel we will only talk about simulating RNNs .", "entities": [[50, 51, "DatasetName", "0"], [56, 57, "DatasetName", "0"], [72, 73, "DatasetName", "0"], [120, 121, "DatasetName", "0"], [126, 127, "DatasetName", "0"], [150, 151, "DatasetName", "0"], [371, 372, "DatasetName", "0"], [429, 430, "DatasetName", "0"], [588, 589, "DatasetName", "0"], [594, 595, "DatasetName", "0"]]}
{"text": "Theorem C.2 . RNNs can be simulated by vanilla Transformers and hence the class of vanilla Transformers is Turing - complete . Proof . The construction of the simulating transformer is simple : it uses a single head and both the encoder and decoder have one layer . Moreover , the encoder does very little and most of the action happens in the decoder . The main task for the simulation is to design the input embedding ( building on the given base embedding f b ) , the feedforward network O ( ) and the matrices corresponding to functions Q ( ) , K ( ) , V ( ) . Input embedding . The input embedding is obtained by summing the symbol and positional encodings which we next describe . These encodings have dimension d = 2d h + d b + 2 , where d h is the dimension of the hidden state of the RNN and d b is the dimension of the given encoding f b of the input symbols . We will use the symbol encoding f symb : \u03a3 Q d which is essentially the same as f b except that the dimension is now larger : f symb ( s ) = [ 0 d h , f e ( s ) ; 0 d h , 0 , 0 ] . The positional encoding pos : N Q d is simply pos ( i ) = [ 0 d h , 0 d b , 0 d h , i , 1 ] . Together , these define the combined embedding f for a given input sequence s 0 s 1 s n \u03a3 * by f ( s i ) = f symb ( s i ) + pos ( i ) = [ 0 d h , f b ( s i ) , 0 d h , i , 1 ] . The vectors v Q d used in the computation of our transformer are of the form v = [ h 1 , s ; h 2 , x 1 , x 2 ] , where h 1 , h 2 Q d h , s Q de , and x 1 , x 2 Q. The coordinates corresponding to the h i 's are reserved for computation related to hidden states of the RNN , the coordinates corresponding to s are reserved for base embeddings , and those for x 1 and x 2 are reserved for scalar values related to positional operations . The first two blocks , corresponding to h 1 and s are reserved for computation of the RNN . During the computation of the Transformer , the underlying RNN will get the input st at step t for t = 0 , 1 , . . . , where recall thatt = min { t , n } . This sequence leads to the RNN getting the embedding of the input sequence s 0 , . . . , s n in the first n + 1 steps followed by the embedding of the symbol $ for the subsequent steps , which is in accordance with the requirements of ( Siegelmann and Sontag , 1992 ) . Similar to ( P\u00e9rez et al , 2019 ) we use the following scoring function in the attention mechanism in our construction , f att ( q i , k j ) = \u2212 | q i , k j | ( 8 ) Construction of TEnc . As previously mentioned , our transformer encoder has only one layer , and the computation in the encoder is very simple : the attention mechanism is not utilized , only the residual connections are . This is done by setting the matrix for V ( ) to the all - zeros matrix , and the feedforward networks to always output 0 . The application of appropriately chosen linear transformations for the final K ( ) and V ( ) give the following lemma about the output of the encoder . Lemma C.3 . There exists a single layer encoder denoted by TEnc that takes as input the sequence ( x 1 , . . . , x n , $ ) and generates the tuple ( K e , V e ) where K e = ( k 1 , . . . , k n ) and V e = ( v 1 , . . . , v n ) such that , k i = [ 0 h , 0 s ; 0 h , \u22121 , i ] , v i = [ 0 h , s i ; 0 h , 0 , 0 ] . Construction of TDec . As in the construction of TEnc , our TDec has only one layer . Also like TEnc , the decoder - decoder attention block just computes the identity : we set V ( 1 ) ( ) = 0 identically , and use the residual connection so that p t = y t . For t \u2265 0 , at the t - th step we denote the input to the decoder as y t = \u1ef9 t + pos ( t ) . Let h 0 = 0 h and\u1ef9 0 = 0 . We will show by induction that at the t - th timestep we have y t = [ h t , 0 s ; 0 h , t + 1 , 1 ] . ( 9 ) By construction , this is true for t = 0 : y 0 = [ 0 h , 0 s ; 0 h , 1 , 1 ] . Assuming that it holds for t , we show it for t + 1 . By Lemma C.5 Att ( p t , K e , V e ) = [ 0 h , v t+1 ; 0 h , 0 , 0 ] . ( 10 ) Lemma C.5 basically shows how we retrieve the input s t+1 at the relevant step for further computation in the decoder . It follows that a t = Att ( p t , K e , V e ) + p t = [ h t , s t+1 , 0 h , t + 1 , 1 ] . In the final block of the decoder , the computation for RNN takes place : Lemma C.4 . There exists a function O ( ) defined by feed - forward network such that , O ( a t ) = [ ( h t+1 \u2212 h t ) , \u2212s t+1 , 0 h , \u2212 ( t + 1 ) , \u22121 ] , where W h , W x and b denote the parameters of the RNN under consideration .", "entities": [[89, 91, "MethodName", "feedforward network"], [211, 212, "DatasetName", "0"], [221, 222, "DatasetName", "0"], [225, 226, "DatasetName", "0"], [227, 228, "DatasetName", "0"], [246, 247, "DatasetName", "0"], [250, 251, "DatasetName", "0"], [254, 255, "DatasetName", "0"], [277, 278, "DatasetName", "0"], [304, 305, "DatasetName", "0"], [315, 316, "DatasetName", "0"], [452, 453, "MethodName", "Transformer"], [468, 469, "DatasetName", "0"], [501, 502, "DatasetName", "0"], [653, 654, "DatasetName", "0"], [675, 676, "DatasetName", "lemma"], [683, 684, "DatasetName", "Lemma"], [762, 763, "DatasetName", "0"], [765, 766, "DatasetName", "0"], [768, 769, "DatasetName", "0"], [780, 781, "DatasetName", "0"], [786, 787, "DatasetName", "0"], [789, 790, "DatasetName", "0"], [791, 792, "DatasetName", "0"], [836, 837, "DatasetName", "0"], [842, 844, "MethodName", "residual connection"], [855, 856, "DatasetName", "0"], [884, 885, "DatasetName", "0"], [886, 887, "DatasetName", "0"], [889, 890, "DatasetName", "0"], [891, 892, "DatasetName", "0"], [914, 915, "DatasetName", "0"], [917, 918, "DatasetName", "0"], [939, 940, "DatasetName", "0"], [942, 943, "DatasetName", "0"], [945, 946, "DatasetName", "0"], [948, 949, "DatasetName", "0"], [951, 952, "DatasetName", "0"], [975, 976, "DatasetName", "Lemma"], [990, 991, "DatasetName", "0"], [996, 997, "DatasetName", "0"], [999, 1000, "DatasetName", "0"], [1001, 1002, "DatasetName", "0"], [1007, 1008, "DatasetName", "Lemma"], [1057, 1058, "DatasetName", "0"], [1082, 1083, "DatasetName", "Lemma"], [1119, 1120, "DatasetName", "0"]]}
{"text": "z t = O ( a t ) + a t = [ h t+1 , 0 s ; 0 h , 0 , 0 ] . We choose the function F for our decoder to be the identity function , therefore\u1ef9 t+1 = [ h t+1 , 0 s ; 0 h , 0 , 0 ] , which means y t+1 = \u1ef9 t+1 + pos ( i + 1 ) = [ h t+1 , 0 s ; 0 h , t + 2 , 1 ] , proving our induction hypothesis .", "entities": [[16, 17, "DatasetName", "0"], [19, 20, "DatasetName", "0"], [22, 23, "DatasetName", "0"], [24, 25, "DatasetName", "0"], [48, 49, "DatasetName", "0"], [51, 52, "DatasetName", "0"], [54, 55, "DatasetName", "0"], [56, 57, "DatasetName", "0"], [78, 79, "DatasetName", "0"], [81, 82, "DatasetName", "0"]]}
{"text": "There are a few changes in the architecture of the Transformer to obtain directional Transformer . The first change is that there are no positional encodings and thus the input vector x i only consists of s i . Similarly , there are no positional encodings in the decoder inputs and hence y t = \u1ef9 t . The vector\u1ef9 is the output representation produced at the previous step and the first input vector to the decoder\u1ef9 0 = 0 . Instead of using positional encodings , we apply positional masking to the inputs and outputs of the encoder . Thus the encoder - encoder attention in ( 5 ) is redefined as a ( +1 ) i = Att ( Q ( z ( ) i ) , K ( Z ( ) i ) , V ( Z ( ) i ) ) + z ( ) i , where Z ( 0 ) = X. Similarly the decoder - encoder attention in ( 7 ) is redefined by a ( ) t = Att ( p ( ) t , K e t , V e t ) + p ( ) t , where in a ( ) t denotes the layer and we use v ( , b ) to denote any intermediate vector being used in - th layer and b - th block in cases where the same symbol is used in multiple blocks in the same layer . Theorem D.1 . RNNs can be simulated by vanilla Transformers and hence the class of vanilla Transformers is Turing - complete . Proof . The Transformer network in this case will be more complex than the construction for the vanilla case . The encoder remains very similar , but the decoder is different and has two layers . Embedding . We will construct our Transformer to simulate an RNN of the form given in the definition with the recurrence where h i Q d h , s Q de and x i Q. These blocks reserved for different types of objects . The where W i Q d\u00d7d and b 1 Q d . Define W 1 as h t = g ( W h h t\u22121 + W x x t + b ) . 2d h d e d \u03c9 1 d \u03c9 d \u03c9 d \u03c9 2d h d e d \u03c9 \u2212 1 1 1 d \u03c9 d \u03c9 d \u03c9 \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0 0 0 0 0 0 0 0 0 I 0 0 0 0 0 0 0 0 0 0 I \u2212I 0 0 0 1 2 0 0 0 0 0 0 1 2 0 0 0 0 0 I 0 0 0 0 0 0 0 0 0 I 0 0 0 0 0 0 0 I Proof . Proof is very similar to proof of lemma C.4 . \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb and b 1 = 0 , then \u03c3 ( W 1 a ( 1 ) t + b 1 ) = [ 0 h , 0 h , s 0 : t , \u2206 t , 1 2 t+1 , \u03c9 t , \u03c9 t\u22121 , \u03c9 t\u22121 ] We define W 2 as 2d h d e d \u03c9\u22121 2 d \u03c9 d \u03c9 d \u03c9 2d h d e d \u03c9 \u2212 1 1 1 d \u03c9 d \u03c9 d \u03c9 \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f00", "entities": [[10, 11, "MethodName", "Transformer"], [14, 15, "MethodName", "Transformer"], [77, 78, "DatasetName", "0"], [79, 80, "DatasetName", "0"], [154, 155, "DatasetName", "0"], [271, 272, "MethodName", "Transformer"], [310, 311, "MethodName", "Transformer"], [424, 425, "DatasetName", "0"], [425, 426, "DatasetName", "0"], [426, 427, "DatasetName", "0"], [427, 428, "DatasetName", "0"], [428, 429, "DatasetName", "0"], [429, 430, "DatasetName", "0"], [430, 431, "DatasetName", "0"], [431, 432, "DatasetName", "0"], [433, 434, "DatasetName", "0"], [434, 435, "DatasetName", "0"], [435, 436, "DatasetName", "0"], [436, 437, "DatasetName", "0"], [437, 438, "DatasetName", "0"], [438, 439, "DatasetName", "0"], [439, 440, "DatasetName", "0"], [440, 441, "DatasetName", "0"], [441, 442, "DatasetName", "0"], [442, 443, "DatasetName", "0"], [445, 446, "DatasetName", "0"], [446, 447, "DatasetName", "0"], [447, 448, "DatasetName", "0"], [450, 451, "DatasetName", "0"], [451, 452, "DatasetName", "0"], [452, 453, "DatasetName", "0"], [453, 454, "DatasetName", "0"], [454, 455, "DatasetName", "0"], [455, 456, "DatasetName", "0"], [458, 459, "DatasetName", "0"], [459, 460, "DatasetName", "0"], [460, 461, "DatasetName", "0"], [461, 462, "DatasetName", "0"], [462, 463, "DatasetName", "0"], [464, 465, "DatasetName", "0"], [465, 466, "DatasetName", "0"], [466, 467, "DatasetName", "0"], [467, 468, "DatasetName", "0"], [468, 469, "DatasetName", "0"], [469, 470, "DatasetName", "0"], [470, 471, "DatasetName", "0"], [471, 472, "DatasetName", "0"], [472, 473, "DatasetName", "0"], [474, 475, "DatasetName", "0"], [475, 476, "DatasetName", "0"], [476, 477, "DatasetName", "0"], [477, 478, "DatasetName", "0"], [478, 479, "DatasetName", "0"], [479, 480, "DatasetName", "0"], [480, 481, "DatasetName", "0"], [491, 492, "DatasetName", "lemma"], [511, 512, "DatasetName", "0"], [529, 530, "DatasetName", "0"], [532, 533, "DatasetName", "0"], [536, 537, "DatasetName", "0"]]}
{"text": "Lemma D.2 . There exists a function O ( 1 ) ( . ) defined by feed - forward network such that , Proof . We define the feed - forward network O ( 1 ) ( . ) such that We define the feed - forward network O ( a t ) as follows ,", "entities": [[0, 1, "DatasetName", "Lemma"]]}
{"text": "Sentiment Word Aware Multimodal Refinement for Multimodal Sentiment Analysis with ASR Errors", "entities": [[6, 9, "TaskName", "Multimodal Sentiment Analysis"]]}
{"text": "Multimodal sentiment analysis has attracted increasing attention and lots of models have been proposed . However , the performance of the state - of - the - art models decreases sharply when they are deployed in the real world . We find that the main reason is that real - world applications can only access the text outputs by the automatic speech recognition ( ASR ) models , which may be with errors because of the limitation of model capacity . Through further analysis of the ASR outputs , we find that in some cases the sentiment words , the key sentiment elements in the textual modality , are recognized as other words , which makes the sentiment of the text change and hurts the performance of multimodal sentiment analysis models directly . To address this problem , we propose the sentiment word aware multimodal refinement model ( SWRM ) , which can dynamically refine the erroneous sentiment words by leveraging multimodal sentiment clues . Specifically , we first use the sentiment word position detection module to obtain the most possible position of the sentiment word in the text and then utilize the multimodal sentiment word refinement module to dynamically refine the sentiment word embeddings . The refined embeddings are taken as the textual inputs of the multimodal feature fusion module to predict the sentiment labels . We conduct extensive experiments on the real - world datasets including MOSI - Speechbrain , MOSI - IBM , and MOSI - iFlytek and the results demonstrate the effectiveness of our model , which surpasses the current state - of - the - art models on three datasets . Furthermore , our approach can be adapted for other multimodal feature fusion models easily 1 .", "entities": [[0, 3, "TaskName", "Multimodal sentiment analysis"], [60, 63, "TaskName", "automatic speech recognition"], [127, 130, "TaskName", "multimodal sentiment analysis"], [203, 205, "TaskName", "word embeddings"], [238, 239, "DatasetName", "MOSI"], [242, 243, "DatasetName", "MOSI"], [247, 248, "DatasetName", "MOSI"], [249, 250, "DatasetName", "iFlytek"]]}
{"text": "Multimodal sentiment analysis has gained increasing attention from the community recently and some process has been made . In general , there are three findings presented by previous work . Performing the cross - modal alignment is helpful for multimodal feature fusion . considered that the holistic features mainly contain global information , which may fail to capture local information . Therefore , they applied the force - alignment to align the visual and acoustic features with the words and further obtained the word - level features . To effectively fuse them , they proposed the GME - LSTM ( A ) model , which consists of two modules , the gated multimodal embedding and the LSTM with the temporal attention . However , obtaining the word - level features needs to perform the force - alignment , which is time - consuming . To address it , proposed the MulT model , which uses the crossmodal attention to align different modal features implicitly . Instead of performing the alignment in the time dimension , some works focusing on semantic alignment . Hazarika et al ( 2020 ) considered that the semantic gaps between heterogeneous data could hurt the model performance and proposed the MISA model , which maps the different modal data into a shared space before multimodal feature fusion . first utilized the cross - modal prediction task to distinguish the shared and private semantics of non - textual modalities compared to the textual modality and then fuse them . The above works show that performing the cross - modal alignment is helpful for multimodal feature fusion . Training the MSA models in an end - to - end manner is more effective . Most of the previous studies adopt a two - phase pipeline , first extracting unimodal features and then fusing them . Dai et al ( 2021 ) considered that it may lead to suboptimal performance since the extracted unimodal features are fixed and can not be further improved benefiting from the downstream supervisory signals . Therefore , they proposed the multimodal endto - end sparse model , which can optimize the unimodal feature extraction and multimodal feature fusion jointly . The experimental results on the multimodal emotion detection task show that training the models in an end - to - end manner can obtain better results than the pipeline models . Leveraging the unimodal sentiment labels to learn more informative unimodal representations is useful for multimodal feature fusion . Yu et al ( 2020 ) considered that introducing the unimodal sentiment labels can help the model capture the unimodal sentiment information and model the difference between modalities . Motivated by it , they built the CH - SIMS dataset , which contains not only the multimodal sentiment labels but also unimodal sentiment labels . And based on it , they proposed a multi - task learning framework to leverage two types of sentiment labels simultaneously . However , this method needs unimodal labels , which is absent for most of the existing datasets . To address it , Yu et al ( 2021 ) proposed the Self - MM model , which first generates the unimodal labels by utilizing the relationship between the unimodal and multimodal labels and then uses the multi - task learning to train the model . These two works both address the usefulness of introducing unimodal labels . However , even though lots of models are proposed and obtain promising results on the benchmark datasets , there are few works considering the noisy inputs when the MSA models are deployed in the real world . presented the Gated Multimodal Embedding to filter out the noises from the acoustic and visual data . Pham et al ( 2019 ) considered that visual and acoustic data may be absent and proposed the MCTN model to handle it . and Mittal et al ( 2020 ) also mainly focused on dealing with the noises introduced by the visual and acoustic data , and their models are based on the word - level features , which are obtained by aligning the audios with the gold texts . There is only one work ( Dumpala et al , 2018 ) considering that the texts are output by the ASR models , which may be erroneous . But this work does not study how do the ASR errors affect the MSA models and does not evaluate the SOTA MSA models on the datasets . Besides , the proposed model needs the gold texts when training , which is time - consuming and labor - consuming . Comparing to the above works , we evaluate the SOTA MSA models on the real - world datasets and observe that the performance of models decreases sharply because of the erroneous ASR texts . Through in - depth analysis of the ASR outputs , we find the sentiment word substitution error in the ASR texts could hurt the MSA models directly . To address it , we propose the sentiment word aware multimodal refinement model , which only uses the ASR texts in the training and testing phrases .", "entities": [[0, 3, "TaskName", "Multimodal sentiment analysis"], [98, 99, "MethodName", "LSTM"], [116, 117, "MethodName", "LSTM"], [119, 121, "MethodName", "temporal attention"], [372, 373, "DatasetName", "emotion"], [451, 454, "DatasetName", "CH - SIMS"], [478, 482, "TaskName", "multi - task learning"], [547, 551, "TaskName", "multi - task learning"], [582, 584, "DatasetName", "the benchmark"]]}
{"text": "In this section , we describe the sentiment word aware multimodal refinement model in detail . An illustration of our proposed model is given in Figure 2 . Our model consists of three modules including the sentiment word location module , multimodal sentiment word refinement module , and multimodal feature fusion module . We first use the sentiment word location module to detect the possible positions of sentiment words and then utilize the multimodal sentiment word refinement module to dynamically refine the word embeddings in the detected positions . Finally , the refined word embeddings are fed into the multimodal feature fusion module to predict the final sentiment labels .", "entities": [[82, 84, "TaskName", "word embeddings"], [93, 95, "TaskName", "word embeddings"]]}
{"text": "The core idea of the sentiment word position detection module is to find out the possible positions of sentiment words in the ASR texts . Note that , it is different from locating sentiment words depending on the word semantics , since the ASR models may recognize a sentiment word as a neutral word , which makes it hard to locate correctly . For example , given a gold text \" And I was really upset about it \" , the ASR model recognizes it as \" And I was really set about it \" . It is easy for the model to label the word \" set \" as a neutral word . Therefore , we choose to detect the position of the sentiment words instead of locating them . To achieve it , we consider adopting a powerful language model , since the language model can model the context information of the sentiment words such as syntactic and grammatical information and predict the appropriate words for the target position . Specifically , we choose the BERT model ( Devlin et al , 2019 ) as our language model since the masked language modeling pretraining objective meets our needs perfectly . Given the sentence { w 1 , w 2 , ... , w n l } , we first mask each word w i in the sentence sequentially , and in practice , we replace the word with the special word [ MASK ] . For example , we mask the first word in the sentence and obtain { [ MASK ] , w 2 , ... , w n l } . And then we use the BERT model to predict the possible words in the position of the masked word . We sort the predicted candidate words by the prediction probabilities and get the Top - k candidate words C i = { c i 1 , c i 2 , ... , c i k } . Next , we distinguish the sentiment words from the candidates using the sentiment lexicons ( Hu and Liu , 2004 ; Wilson et al , 2005 ) and k i is the number of selected sentiment words corresponding to the position i. The larger the number is , the more possible the position is . And we obtain the most possible position of sentiment word , s = arg max ( { k 1 , k 2 , ... , k n l } ) . Considering that in some cases there is not a sentiment word in the sentence , we use a sentiment threshold to filter out the impossible ones . In practice , we use the gate mask p to record it , and p is 1 if k s is larger than k/2 and 0 otherwise .", "entities": [[177, 178, "MethodName", "BERT"], [192, 195, "TaskName", "masked language modeling"], [280, 281, "MethodName", "BERT"], [470, 471, "DatasetName", "0"]]}
{"text": "In order to reduce the negative effects of the ASR errors , we propose the multimodal sentiment word refinement module , in which we refine the word embeddings of sentiment words from two aspects . One is that we uses the multimodal gating network to filter out the useless information from the input word embeddings . The other one is that we design the multimodal sentiment attention network to incorporate the useful information from candidate words generated by the BERT model . Given an utterance , which includes three modal unaligned features , word embeddings , acoustic features , and visual features , we denote them as x i = { x i t : 1 \u2264 t \u2264 n i , x i t R d i x } , i { l , v , a } . To obtain the multimodal information corresponding to each word , We utilize the pseudo - alignment method to align the features . We split the the acoustic and visual features into non - overlapping feature groups , of which lengths are na n l and nv n l respectively , and average the features in each group and obtain the aligned features , u i = { u i t : 1 \u2264 t \u2264 n l , u i t R d i x } , i { v , a } . To obtain the context - aware representations , we apply the BERT model and LSTM networks to encode the features , producing h i = { h i t : 1 \u2264 t \u2264 n l , h i t R d i h } , i { v , a , l } . Besides , we also use an LSTM network to fuse the acoustic and visual features for capturing high - level sentiment semantics and obtain h va = { h va . network to filter the word embedding , which is implemented by a non - linear layer . The motivation is that the ASR model may recognize incorrectly the sentiment word resulting in the corrupted sentiment semantics of the text . Therefore , we leverage the multimodal sentiment information to decide how much information of the input word embedding to pass . Specifically , we concatenate the unimodal context - aware representations , h l s , h v s , h a s , and bimodal representation h va s in the detected position s and feed them into a non - linear neural network , producing the gate value g v . And then the gate value is used to filter out the useless information from the word embedding . To make the model ignore the impossible one , we use the gate mask p to achieve it . t : 1 \u2264 t \u2264 n l , h va t R d va h } . h l = BERT ( x l ) h v = LSTM v ( u v ) h a = LSTM a ( u a ) h va = LSTM va ( [ u v ; u a ] ) ( 1 g v = Sigmoid ( W 1 ( [ h l s ; h v s ; h a s ; h va s ] ) + b 1 ) r v = ( 1 \u2212 g v p ) x l s ( 2 ) where W 1 R 1\u00d7 i { l , v , a , va } d i h , b 1 R 1 are the parameters of the multimodal gating network . Furthermore , we propose a novel multimodal sentiment word attention network to leverage the sentiment - related information from the candidate words , more than half of which are sentiment words , generated by the BERT model to complement the word embeddings . For example , the ASR model recognizes the \" upset \" as \" set \" , we first want to remove the useless information of \" set \" and then incorporate the information of negative sentiment words to reconstruct the original sentiment semantics . We use a linear layer to implement the multimodal sentiment word attention network . We first concatenate the word embedding x c s t of the candidate word c s t and multimodal representations , h v s , h a s , and h va s at the most possible time step s. Then , we pass them to the linear layer and obtain the attention score g e t . The attention scores are fed into a softmax function to obtain the attention weights . Finally , we apply the weights to the candidate word embeddings and get the sentiment embedding r e . where W 2 R 1\u00d7 ( d l x + i { v , a , va } d i h ) , b 2 R 1 are the parameters of the multimodal sentiment word attention network . In addition , there may not be suitable words in the candidate words . Hence , we incorporate the embedding of the special word [ MASK ] , x mask , to let the BERT model handle this problem based on the context . We then design an aggregation network to balance the contributions of the special word embedding x mask and the sentiment embedding r e . Finally , we add the r add to the filtered word embedding u l s and obtain the refined word embedding r l for the target word . g mask = Sigmoid ( W 3 ( [ r e ; x mask ] ) + b 3 ) r add = g mask r e + ( 1 \u2212 g mask ) x mask r l = ( g v p ) r add + r v ( 4 ) where W 3 R 1\u00d72d l x , b 3 R 1 are the trainable parameters .", "entities": [[26, 28, "TaskName", "word embeddings"], [53, 55, "TaskName", "word embeddings"], [79, 80, "MethodName", "BERT"], [93, 95, "TaskName", "word embeddings"], [245, 246, "MethodName", "BERT"], [248, 249, "MethodName", "LSTM"], [295, 296, "MethodName", "LSTM"], [334, 336, "MethodName", "linear layer"], [491, 492, "MethodName", "BERT"], [499, 500, "MethodName", "LSTM"], [508, 509, "MethodName", "LSTM"], [517, 518, "MethodName", "LSTM"], [643, 644, "MethodName", "BERT"], [648, 650, "TaskName", "word embeddings"], [698, 700, "MethodName", "linear layer"], [756, 758, "MethodName", "linear layer"], [774, 775, "MethodName", "softmax"], [791, 793, "TaskName", "word embeddings"], [873, 874, "MethodName", "BERT"]]}
{"text": "We describe our multimodal feature fusion module in the section and it is noted that our proposed refinement approach only modifies the textual input token embeddings , which makes it easy to be adapted for other multimodal feature fusion models based on BERT , such as MISA ( Hazarika et al , 2020 ) . We first use the BERT model to encode the refined word embeddings z l = { x l 1 , x l 2 , ... , r l , .. , x l n l } and take the representation of [ CLS ] as the textual representation , which is denoted as v l . And then we use two LSTM networks to encode the visual and acoustic features and take the representations of the first words as the visual representation v v and acoustic representation v a . Finally , we fuse them using a non - linear layer to capture the interactions between them . v l = BERT textual ( z l ) v v = LSTM visual ( x v ) v a = LSTM acoustic ( x a ) v f = Relu ( W 4 ( [ v l ; v v ; v a ] ) + b 4 ) ( 5 ) where W 4 R d f v \u00d7 ( d l v + d a v + d v v ) , b 4 R d f v are the trainable parameters of the fusion network . We utilize a linear layer to predict the final sentiment regression labels . p f = W 5 v f + b 5 ( 6 ) where W 5 R 1\u00d7d f v , b 5 R 1 are the trainable parameters of the prediction network . Besides , to enhance the model to capture unimodal sentiment information , we use the Unimodal Label Generation Module ( ULGM ) ( Yu et al , 2021 ) to generate pseudo unimodal sentiment labels and adopt them to train our model in a multi - task learning manner . For more details , we refer you to Yu et al ( 2021 ) .", "entities": [[42, 43, "MethodName", "BERT"], [59, 60, "MethodName", "BERT"], [65, 67, "TaskName", "word embeddings"], [116, 117, "MethodName", "LSTM"], [154, 156, "MethodName", "linear layer"], [166, 167, "MethodName", "BERT"], [175, 176, "MethodName", "LSTM"], [184, 185, "MethodName", "LSTM"], [193, 194, "MethodName", "Relu"], [256, 258, "MethodName", "linear layer"], [344, 348, "TaskName", "multi - task learning"]]}
{"text": "We compare our proposed model with the following baselines 4 . TFN uses the three - fold Cartesian product to capture unimodal , bimodal , and trimodal interactions . LMF ( Liu et al , 2018 ) uses the low - rank tensors to accelerate the multimodal feature fusion process . MulT uses the cross - modal transformers to fuse multimodal features . MISA ( Hazarika et al , 2020 ) adopts multi - task learning to map different modal features into a shared subspace . Self - MM ( Yu et al , 2021 ) first generates the pseudo unimodal sentiment labels and then adopts them to train the model in a multi - task learning manner . 5 Results and Analysis", "entities": [[72, 76, "TaskName", "multi - task learning"], [113, 117, "TaskName", "multi - task learning"]]}
{"text": "In And we also list the results of the SOTA model , Self - MM , on the original MOSI dataset in the last row of the table for the performance comparison between Self - MM in the ideal world and real world . As we can see from the results , Self - MM obtains the best results on the MOSI - Gold dataset than the other datasets , which demonstrates that the ASR errors hurt the MSA models . We also observe that the better ASR model can help the MSA models achieve better performance . But it should be noted that , according to the analysis in the previous section , current ASR models still can not produce satisfactory results for the MSA models in the real world . Comparison between the feature - based models including TFN , LMF , and MulT and finetuningbased baselines such as MISA and Self - MM , we can find that finetuning - based models obtain better results . We consider that the finetuning - based models can adapt the BERT encoder to the target task and learning more informative textual representations , which also makes them benefit more as the quality of texts increases . Comparing to the baselines especially Self - MM , our model achieves better performance in all evaluation metrics since our model can detect the substitution error of the sentiment words and then refine the word embeddings to reconstruct the sentiment semantics in the textual modality by filtering out useless information from the input words and incorporating useful information from the candidate words generated by the language model . We also observe that the improvement of our model compared with Self - MM on MOSI - iFlytek is smaller . We consider that the main reason is fewer sentiment word substitution errors on MOSI - iFlytek .", "entities": [[19, 20, "DatasetName", "MOSI"], [61, 62, "DatasetName", "MOSI"], [180, 181, "MethodName", "BERT"], [240, 242, "TaskName", "word embeddings"], [289, 290, "DatasetName", "MOSI"], [291, 292, "DatasetName", "iFlytek"], [308, 309, "DatasetName", "MOSI"], [310, 311, "DatasetName", "iFlytek"]]}
{"text": "To have an intuitive understanding of our proposed model , we show a case in Figure 3 . We can see that our model first detects the most possible position based on the context and then finds that the input word in the position may be recognized incorrectly since there is a mismatch between the negative word \" cruel \" and either the smile or the excited tone . Hence our model decides to incorporate the related sentiment information from the candidate words to refine the word embedding . As shown in Figure 3 , our model pays more attention to the candidate words \" special \" , \" cool \" , and \" awesome \" . The word \" cool \" is exactly the gold word and the others have the same sentiment polarity as it . Beneficial from the attended candidate words , our model refines the input word and reconstructs its sentiment semantics . Finally , the refined word embeddings are fed into the multimodal feature fusion module to predict the sentiment label .", "entities": [[161, 163, "TaskName", "word embeddings"]]}
{"text": "In this paper , we observe an obvious performance drop when the SOTA MSA model is deployed in the real world , and through in - depth analysis , we find that the sentiment word substitution error is a very important factor causing it . To address it , we propose the sentiment word aware multimodal refinement model , which can dynamically refine the word embeddings and reconstruct the corrupted sentiment semantics by incorporating the multimodal sentiment information . We evaluate our model on MOSI - SpeechBrain , MOSI - IBM , and MOSI - iFlytek and the results demonstrate the effectiveness of our approach . For future work , we will explore leveraging the multimodal information to detect the sentiment word positions .", "entities": [[64, 66, "TaskName", "word embeddings"], [84, 85, "DatasetName", "MOSI"], [88, 89, "DatasetName", "MOSI"], [93, 94, "DatasetName", "MOSI"], [95, 96, "DatasetName", "iFlytek"]]}
{"text": "GTI at SemEval - 2016 Task 5 : SVM and CRF for Aspect Detection and Unsupervised Aspect - Based Sentiment Analysis", "entities": [[8, 9, "MethodName", "SVM"], [10, 11, "MethodName", "CRF"], [16, 21, "TaskName", "Aspect - Based Sentiment Analysis"]]}
{"text": "This paper describes in detail the approach carried out by the GTI research group for Se - mEval 2016 Task 5 : Aspect - Based Sentiment Analysis , for the different subtasks proposed , as well as languages and dataset contexts . In particular , we developed a system for category detection based on SVM . Then for the opinion target detection task we developed a system based on CRFs . Both are built for restaurants domain in English and Spanish languages . Finally for aspect - based sentiment analysis we carried out an unsupervised approach based on lexicons and syntactic dependencies , in English language for laptops and restaurants domains .", "entities": [[22, 27, "TaskName", "Aspect - Based Sentiment Analysis"], [38, 40, "DatasetName", "and dataset"], [54, 55, "MethodName", "SVM"], [85, 90, "TaskName", "aspect - based sentiment analysis"]]}
{"text": "In the last years , with the growth of Internet , people use it as a means of expressing their opinions and experiences about several subjects . That is the reason why there is a great amount of user generated information available online , through many different platforms , such as blogs , social networks , etc . This information became very valuable for companies , politicians , etc . , who are interested in what users say about them or their products . Due to this , Sentiment Analysis ( SA ) techniques have attracted the interest of researches , trying to process all this amount of information by means of usually supervised methods based on classifiers . Most of these researches focus on extracting the sentiment of a whole review or text ( Liu , 2012 ) . This is enough for many applications and purposes . However , sometimes there is a need for analysing the text in a deeper way , at entity or aspect level . For example , a review in the restaurants domain can include different opinions about different aspects , such as the service or the food quality , so it is interesting to distinguish the different opinions for each of these aspects . This is the reason why some studies emerged about the so - called aspect - based sentiment analysis ( Marcheggiani et al , 2014 ; Lu et al , 2011 ) . Hence this is the subject of the task 5 of the Se - mEval 2016 ( Pontiki et al , 2016 ) , divided into different subtasks . Groups are asked to detect aspect categories in a review or sentence , which are predefined for each domain and formed by an entity and an attribute . Then , there is a subtask which consists of detecting the opinion target expression , which are related to the categories found . Finally , aspect - based sentiment analysis is required for one of the subtasks , associating a polarity , which can be positive , negative or neutral , to each of the categories found in the sentence or review . Datasets in different languages and domains are available for proving the approaches . The remainder of this paper is structured as follows . In Section 2 we make a description of the system developed for all the subtasks . Section 3 contains the results of all the different subtasks , as well as detailed scores for each slot . Finally , in section 4 we summarize the main aspects of our system and extract some final conclusions .", "entities": [[88, 90, "TaskName", "Sentiment Analysis"], [225, 230, "TaskName", "aspect - based sentiment analysis"], [325, 330, "TaskName", "aspect - based sentiment analysis"]]}
{"text": "As a first step for all the subtasks , each preprocessed social media review must first be broken into tokens , in order to derive the syntactic context . Partof - speech ( POS ) tagging and lemmatization are performed to ensure that all the inflected forms of a word are covered . In the case of English , Stanford Tagger is applied due to its better results , however it does not provide lemmatization . That is why using the resulting form and tag , lemma is extracted by means of Freeling Tagger ( Atserias et al , 2006 ; Padr\u00f3 and Stanilovsky , 2012 ) . On the other hand , for Spanish language only Freeling Tagger is used . Freeling is a library that provides multiple languages among which are English and Spanish . Food and drinks recognition is also performed , based on dictionaries 1 , in order to identify words referring to those topics for the subsequent processing of the sentences . POS tagging allows the identification of lexical items that can contribute to the correct recognition of targets in a message . These items are namely adjectives , adverbs , verbs and nouns . The lemmatized and POS - annotated messages are fed to a parser that transforms the output of the tagger into a full parse tree . Finally , the tree is converted to dependencies , and the functions are annotated . The entire process is performed by means of Freeling Parser ( Padr\u00f3 and Stanilovsky , 2012 ) .", "entities": [[37, 38, "TaskName", "lemmatization"], [74, 75, "TaskName", "lemmatization"], [86, 87, "DatasetName", "lemma"]]}
{"text": "Sentiment Analysis ( ABSA ) This subtask contains different slots , having participated in three of them , which are slot 1 , slot 2 and slot 3 . The system for Spanish and English language is exactly the same for both slots 1 and 2 . 1 Taken from the lists available at https://es.speaklanguages.com/ingl\u00e9s/vocabulario/comidas", "entities": [[0, 2, "TaskName", "Sentiment Analysis"]]}
{"text": "The aim of this task is to assign to each sentence a category , which is a tuple ( entity , attribute ) , from a given set of 12 different predefined categories . To do this , we used a linear SVM classifier combined with word lists . These word lists are created from the training file provided by the organization , which was composed of 2000 sentences , grouped in 350 reviews . Different datasets were provided for several languages and topics . Our system was developed for restaurants dataset , both in English and Spanish . The library libsvm ( Chang and Lin , 2011 ) was used to implement the SVM classifier , using the following features for each sentence : Words : those words appearing in the sentence , which are nouns , verbs or adjectives are extracted . Lemmas : lemmas from nouns , verbs and adjectives are selected . POS tags : part of speech from nouns , verbs and adjectives in the sentence . Bigrams : all the bigrams found in the sentence . We developed 12 different binary classifiers , one for each possible category . If the output of one classifier for a particular sentence is \" 1 \" , then we add the related category to the sentence . If more than one category is found for the same sentence , we add all of them to the list of categories . After this , the outputs are improved by means of our word lists , as we can see in Algorithm 1 , executed for each sentence . The word lists were created automatically from the training file , extracting all the nouns and adjectives appearing in sentences from the same category , and manually filtered later in order to remove noisy items . Six different lists are composed , containing terms related to : ambience , service , prices , quality , style options and location . The inputs defined for the following algorithm are the list of categories obtained from SVM for each sentence ( CList ( s ) ) and the six word lists created previously . The output is the new list per sentence , containing the old categories from SVM and the new ones added .", "entities": [[42, 43, "MethodName", "SVM"], [114, 115, "MethodName", "SVM"], [344, 345, "MethodName", "SVM"], [376, 377, "MethodName", "SVM"]]}
{"text": "For this slot , teams were asked to extract the exact expressions or words in the sentence , in which an opinion is expressed . The implementation for this slot is made by means of CRFs , using CRF++ tool ( Kudo , 2005 ) and the training file provided for building the model . A training file is needed to build as input for the CRF , whose structure is as follows . In the first column , all the words for every sentence are written , then in the second column , the corresponding lemma . The third column represents the tag and the last one represents if the word is an aspect or not or if it is included in a multiword aspect . Then for creating the model we take into account all these features , as well as all the possible bigrams in each sentence . In the output , if no target is found , no opinion is returned for that sentence .", "entities": [[66, 67, "MethodName", "CRF"], [96, 97, "DatasetName", "lemma"]]}
{"text": "This slot is implemented only for English language , both restaurants and laptops datasets . Our system is fully unsupervised , this can explain the low results obtained for this slot . An adjustment was made to the system already implemented for sentiment analysis in the whole sentence , which was presented in Semeval 2015 , task 10 : sentiment analysis in Twitter ( Fern\u00e1ndez - Gavilanes et al , 2015 ) , which was also unsupervised . For this dataset , a new polarity lexicon was generated automatically from the training dataset , applying a polarity rank algorithm , as explained in the mentioned article . Then , it was merged with SOCAL ( Taboada et al , 2011 ) and AFINN ( Nielsen , 2011 ) lexicons , which are general context ones , by applying an average for those words which appeared in more than one of them . Our system for the restaurant dataset implements the following syntactic rules : If there is no opinion or only one target expression in the sentence , the system automatically takes the polarity of the whole sentence and assign it to all the categories which appear in this sentence . If there is only one different target expression but appearing more than once , we check if there is an adversative clause in the sentence built with \" but \" particle . If not , we also take the polarity of the whole sentence for all the opinions . If the previous condition is fulfilled , we will take the polarity of the first clause of the sentence , which is the piece of sentence placed before the \" but \" and then apply a polarity linear system , which consists of summing up all the polarities found in the dictionary created . For the next opinions which have the same target , we will follow the same procedure but with the piece of sentence after the \" but \" . For this linear approach , we take negations in account only for adjectives , flipping the polarity of the adjectives which come inmediately after a negation particle , as \" no \" or \" not \" . When there are several different opinion targets , we split the sentence to detect the scope of each target and apply the same linear polarity algorithm explained in the previous point . To detect the scope of the target , we take the words which appear before and after the target , splitting by punctuation marks ( \" ; \" , \" , \" , \" . \" , \" ? \" , \" ! \" , \" - \" ) . For the laptops dataset , since there are no opinion target expressions , we take the polarity of the whole sentence to assign the polarity of each category .", "entities": [[42, 44, "TaskName", "sentiment analysis"], [59, 61, "TaskName", "sentiment analysis"]]}
{"text": "Once we performed aspect category detection at sentence - level , we use this output as input for textlevel detection . All the categories found are grouped at sentence - level and added all of them at reviewlevel . Besides this , if RESTAURANT#GENERAL is not explicitly assigned to any sentence of the review , we add it anyway .", "entities": [[3, 6, "TaskName", "aspect category detection"]]}
{"text": "This paper describes the participation of the GTI group , AtlantTIC Research Center , University of Vigo , in the SemEval 2016 , Task 5 : Aspect - Based Sentiment Analysis . We developed a supervised system based on SVM classifiers for category detection , and CRFs for opinion target detection . Then , for the aspect - based sentiment analysis we submitted a fully unsupervised system , based on syntactic dependencies and context - based polarity lexicons . As we can see in Table 4 , competitive results were obtained for aspect and category detection , being in first position for Spanish language , both in subtask 1 and subtask 2 . Moreover , in subtask 2 , which is aspect detection at review level , we also achieved the first position for English language in restaurants datasets . However , our system did not perform as well as expected in slot 3 , maybe due to the fact of the lack of supervision for our model . It results not competitive against other supervised approaches , although its main advantage is that there is no need of training sets , which is time and resource consuming in order to manually tag them .", "entities": [[26, 31, "TaskName", "Aspect - Based Sentiment Analysis"], [39, 40, "MethodName", "SVM"], [56, 61, "TaskName", "aspect - based sentiment analysis"]]}
{"text": "Welcome to the first Workshop on Human - Computer Question Answering ( HCQA ) ! Question answering is a central task in natural language processing ( NLP ) . Unlike other NLP tasks , it also is easy for non - experts to understand when question answering systems perform well ( or fail ) . The goal of this workshop is to bring the community together to discuss the state of the art of question answering and interactively compete with top human trivia masters . This workshop highlights question answering on the real - world task of quiz bowl , a trivia game in which competitors are asked to identify entities such as battles , novels , and scientific terms . In quiz bowl , a moderator reads a paragraph - long question to two teams , and players are permitted to interrupt the moderator ( or \" buzz in \" ) with a guess if they feel confident . This setting is especially interesting because acquiring more features ( clues ) comes with an added cost ( the other team may buzz in before you ) . While computerized question answering systems have previously had success against humans , this workshop will be the first to pit different systems against each other and then have that winner face off against a top human team . Question answering is a task interesting to both academia and industry . This workshop brings people from both sides to discuss recent progress in QA . We will have a presentation from the IBM Watson team talking about their new Watson Discovery Advisor and the challenges of QA in the industrial setting . Peter Clark will talk about new types of question answering problems that he and his team are solving at the Allen Institute for AI . We also have Zhengdong Lu , Jason Weston , and Richard Socher talking about recent neural network approaches to QA . This year we have nine papers covering a variety of approaches to QA , including neural networks , crowdsourcing , knowledge graph search , and paraphrasing . Besides common QA tasks such as machine comprehension , ( open - domain ) factoid QA , we are also excited to see new topics on error analysis of QA systems by crowdsourcing and alignment between text description and paintings for art questions . At the end of the workshop , we will have a dual computer - human tournament to test entrants ' question answering systems against each other and against the top human trivia masters . Enjoy the match ! Finally , we invite you to enjoy this volume and we are looking forward to seeing you in San Diego !", "entities": [[9, 11, "TaskName", "Question Answering"], [15, 17, "TaskName", "Question answering"], [45, 47, "TaskName", "question answering"], [74, 76, "TaskName", "question answering"], [88, 90, "TaskName", "question answering"], [190, 192, "TaskName", "question answering"], [226, 228, "TaskName", "Question answering"], [287, 289, "TaskName", "question answering"], [416, 418, "TaskName", "question answering"]]}
{"text": "As a prominent attribution - based explanation algorithm , Integrated Gradients ( IG ) is widely adopted due to its desirable explanation axioms and the ease of gradient computation . It measures feature importance by averaging the model 's output gradient interpolated along a straight - line path in the input data space . However , such straight - line interpolated points are not representative of text data due to the inherent discreteness of the word embedding space . This questions the faithfulness of the gradients computed at the interpolated points and consequently , the quality of the generated explanations . Here we propose Discretized Integrated Gradients ( DIG ) , which allows effective attribution along non - linear interpolation paths . We develop two interpolation strategies for the discrete word embedding space that generates interpolation points that lie close to actual words in the embedding space , yielding more faithful gradient computation . We demonstrate the effectiveness of DIG over IG through experimental and human evaluations on multiple sentiment classification datasets . We provide the source code of DIG to encourage reproducible research 1 .", "entities": [[32, 34, "TaskName", "feature importance"]]}
{"text": "In the past few years , natural language processing has seen tremendous progress , largely due to strong performances yielded by pre - trained language models ( Devlin et al , 2019 ; Radford et al , 2019 ; Brown et al , 2020 ) . But even with this impressive performance , it can still be difficult to understand the underlying reasoning for the preferred predictions leading to distrust among end - users ( Lipton , 2018 ) . Hence , improving model interpretability has become a central focus in the community with an increasing effort in developing methods that can explain model behaviors ( Ribeiro et al , 2016 ; Binder et al , 2016 ; Li et al , 2016 ; Sundararajan et 2017 ; Shrikumar et al , 2017 ; Lundberg and Lee , 2017 ; Murdoch et al , 2018 ) . Explanations in NLP are typically represented at a word - level or phrase - level by quantifying the contributions of the words or phrases to the model 's prediction by a scalar score . These explanation methods are commonly referred as attributionbased methods ( Murdoch et al , 2018 ; Ancona et al , 2018 ) . Integrated Gradients ( IG ) ( Sundararajan et al , 2017 ) is a prominent attribution - based explanation method used due to the many desirable explanation axioms and ease of gradient computation . It computes the partial derivatives of the model output with respect to each input feature as the features are interpolated along a straight - line path from the given input to a baseline value . For example , say we want to compute the attribution for the word \" good \" in the sentence \" the movie was good ! \" using IG . The straight - line interpolation path used by IG is depicted in green in Figure 1 . Here , the baseline word is defined as the \" < pad > \" embedding and the green squares are the intermediate interpolation points in the embedding space . While this method can be used for attributing inputs in both continuous ( e.g. , image , audio , etc . ) and discrete ( e.g. , text , molecules , etc . ) domains ( Sundararajan et al , 2017 ) , their usage in the dis - crete domain has some limitations . Since the interpolation is done along a straight - line path joining the input word embedding and the baseline embedding ( \" < pad > \" in Figure 1 ) , the interpolated points are not necessarily representative of the discrete word embedding distribution . Specifically , let a dummy word embedding space be defined by the words represented by black dots in Figure 1 . Then we can see that some of the green squares can be very far - off from any original word in the embedding space . Since the underlying language model is trained to effectively work with the specific word embedding space as input , using these out - of - distribution green interpolated samples as intermediate inputs to calculate gradients can lead to sub - optimal attributions . To mitigate these limitations , we propose a Discretized integrated gradients ( DIG ) formulation by relaxing the constraints of searching for interpolation points along a straight - line path . Relaxing this linear - path constraint leads to a new constraint on the interpolation paths in DIG that points along the path should be monotonically situated between the input word embedding and the baseline embedding . Hence , in DIG , our main objective is to monotonically interpolate between the input word embedding and baseline such that the intermediate points are close to real data samples . This would ensure that the interpolated points are more representative of the word embedding distribution , enabling more faithful model gradient computations . To this end , we propose two interpolation strategies that search for an optimal anchor word embedding in the real data space and then modify it such that it lies monotonically between the input word and baseline ( see Fig . 1 for an illustration ) . We apply DIG using our proposed interpolation algorithms to generate attributions for three pre - trained language models - BERT ( Devlin et al , 2019 ) , DistilBERT ( Sanh et al , 2020 ) , and RoBERTa ( Liu et al , 2019 ) , each fine - tuned separately on three sentiment classification datasets - SST2 ( Socher et al , 2013 ) , IMDB ( Maas et al , 2011 ) , and Rotten Tomatoes ( Pang and Lee , 2005 ) . We find that our proposed interpolation strategies achieve a superior performance compared to integrated gradients and other gradient - based baselines on eight out of the nine settings across different metrics . Further , we also observe that on average , end - users find explanations provided by DIG to be more plausible justifications of model behavior than the explanations from other baselines .", "entities": [[724, 725, "MethodName", "BERT"], [733, 734, "MethodName", "DistilBERT"], [743, 744, "MethodName", "RoBERTa"], [763, 764, "DatasetName", "SST2"], [772, 773, "DatasetName", "IMDB"]]}
{"text": "As described in prior works ( Sundararajan et al , 2017 ; Shrikumar et al , 2017 ) , a good explanation w w2 w6 w1 w5 w3 w4 w ' c ( a ) DIG - GREEDY w w2 [ 7 ] w6 [ 8 ] w1 [ 5 ] w5 [ 12 ] w3 [ 10 ] w4 [ 20 ] w ' c ( b ) DIG - MAXCOUNT Figure 2 : Overview of paths used in DIG and IG . The gray region is the neighborhood of w. Green line depicts the straight - line path used by IG . Left : In DIG - GREEDY , we first monotonize each word in the neighborhood ( red arrow ) and the word closest to its corresponding monotonic point is selected as the anchor ( w 5 since the red arrow of w 5 has the smallest magnitude ) . Right : In DIG - MAXCOUNT we select the word with the highest number of monotonic dimensions ( count shown in [ . ] ) as the anchor word ( w 4 ) , followed by changing the non - monotonic dimensions of w 4 ( red arrow to c ) . Repeating this iteratively gives the non - linear blue path for DIG with the red stars as interpolation points . Please refer to Section 2.1 for more details . Figure best viewed in color . algorithm should satisfy certain desirable axioms which justify the use of the algorithm for generating model explanations . Similar to IG , DIG also satisfies many such desirable axioms . First , DIG satisfies Implementation Invariance which states that attributions should be identical for two functionally equivalent models . Two models are functionally equivalent if they have the same output for the same input , irrespective of any differences in the model 's internal implementation design . Further , DIG satisfies Completeness which states that the sum of the attributions for an input should add up to the difference between the output of the model at the input and the baseline , i.e. , i DIG i ( x ) = F ( x ) \u2212F ( x ) . This ensures that if F ( x ) \u2248 0 then the output is completely attributed to the inputs . Thirdly , DIG satisfies Sensitivity which states that attributions of inputs should be zero if the model does not depend ( mathematically ) on the input . Please refer to Appendix B for further comparisons of DIG with IG .", "entities": [[379, 380, "DatasetName", "0"]]}
{"text": "To further understand the impact of our algorithm on end users , we conduct human evaluations of explanations from our method and the two top baselines - IG and GradShap . We perform the study on the DistilBERT model fine - tuned on SST2 dataset and the BERT model fine - tuned on Rotten Tomatoes dataset . Further , we select the best variant of DIG on each dataset for explanation comparisons . First , we pick 50 sample sentences from each dataset with lengths between 5 and 25 words for easier visualizations . Then , we convert the attributions from each method into word highlights , whose intensity is determined by the magnitude of the attributions . Finally , we show the highlighted sentence and the model 's predicted label to the annotators and ask them to rank the explanations on a scale of 1 - 3 , \" 1 \" being the most comprehensive explanation that best justifies the prediction . Figure 3 shows the mean rank of each explanation algorithm across the two datasets . We find that DIG has a significantly lower mean rank compared to IG ( p < .001 on both SST2 and Rotten Tomatoes 5 ) . Thus , we conclude that explanations generated by DIG are also trustworthy according to humans . Please refer to Appendix G for visualizations and discussion on explanations generated by our methods .", "entities": [[37, 38, "MethodName", "DistilBERT"], [43, 44, "DatasetName", "SST2"], [47, 48, "MethodName", "BERT"], [197, 198, "DatasetName", "SST2"]]}
{"text": "In this section , we report the ablation of AN - CHORSEARCH and the effect of path density on DIG . Please refer to Appendix F for ablations on neighborhood size and discussions on computational complexity . Ablation Study on ANCHORSEARCH . We ablate our methods with two random variants - DIG - RANDOMANCHOR and DIG - 5 We compute the p\u2212value using Wilcoxon signed - rank test . RANDOMNEIGHBOR , in which the AN - CHORSEARCH step uses a random anchor selection heuristic . Specifically , in DIG - RANDOMANCHOR , the anchor is selected randomly from the complete vocabulary . Thus , this variant just ensures that the selected anchor is close to some word in the vocabulary which is not necessarily in the neighborhood . In contrast , the DIG - RANDOMNEIGHBOR selects the anchor randomly from the neighborhood without using our proposed heuristics MAXCOUNT or GREEDY . The log - odds metrics of IG , the two ablations , and our best variant of DIG for DistilBERT fine - tuned individually on all three datasets are reported in Table 4 . We report 5 - seed average for the randomized baselines . We observe that DIG - RANDOMANCHOR improves upon IG on all three datasets . This shows that generating interpolation points close to the words in the vocabulary improve the explanation quality . Further , we observe that DIG - RANDOMNEIGHBOR improves upon DIG - RANDOMANCHOR on log - odds metric . One reason could be that the words in a neighborhood are more semantically relevant to the original word , leading to more coherent perturbations for evaluating model gradients . Finally , we observe that , on average , our proposed method is better compared to selecting a random anchor in the neighborhood . This shows that our search strategies MAXCOUNT and GREEDY are indeed helpful . Effect of Increasing Path Density . In integrated gradients , the completeness axiom ( Section 2.2 ) is used to estimate if the integral approximation ( Equation 6 ) error is low enough . This error is denoted as the Delta % error . If the error is high , users can increase the number of interpolation points m. While DIG also satisfies the completeness axiom , error reduction by increasing m is infeasible . This is because increasing m in Equation 3 implicitly changes the integral path rather than increasing the density . Hence , to achieve an error reduction in DIG , we up - sample the interpolation path P = { w , w 1 , w 2 , . . . , w m\u22122 , w } with an up - sampling factor ( f ) of one as follows : P 1 = { w , w+w 1 2 , w 1 , w 1 + w 2 2 , . . . , w m\u22122 + w 2 , w } , i.e. , we insert the mean of two consecutive points to the path . This essentially doubles the density of points in the path . Similarly , P 2 can be obtained by up - sampling P 1 , etc . DIG ( m , f = 0 ) refers to the standard DIG with no up - sampling . Given that we have two hyperparameters m and f that determine the overall path density , we analyze the effect of each of these in Figure 4 and Table 5 respectively . The results are shown for DIG - MAXCOUNT applied on DistilBERT model finetuned on SST2 dataset . In Figure 4 , we observe that as m increases , the Delta % of IG decreases as expected . But the trend is opposite for DIG . As discussed above , for DIG , the path length increases with increasing m , and hence , we attribute this trend to increasing difficulty in effectively approximating the integral for longer paths . Next , in Table 5 , we observe that as the up - sampling factor f increases , the Delta % consistently decreases . We also find that our up - sampling strategy does not increase the WAE by a significant amount with increasing f , which is desirable . Thus , this confirms that our up - sampling strategy is a good substitute of increasing m for IG to effectively reduce the integral approximation error Delta % . Following Sundararajan et al ( 2017 ) , we choose a threshold of 5 % average Delta to select the hyperparameters . For more discussions , please refer to Appendix F.1 .", "entities": [[170, 171, "MethodName", "DistilBERT"], [538, 539, "DatasetName", "0"], [593, 594, "MethodName", "DistilBERT"], [597, 598, "DatasetName", "SST2"]]}
{"text": "There has been an increasing effort in developing interpretability algorithms that can help understand a neural network model 's behavior by explaining their predictions ( Doshi - Velez and Kim , 2017 ; Gilpin et al , 2019 ) . Attributions are a post - hoc explanation class where input features are quantified by scalar scores indicating the magnitude of contribution of the features toward the predicted label . Explanation algorithms that generate attributions can be broadly classified into two categories - model - agnostic algorithms , like LIME ( Ribeiro et al , 2016 ) , Input occlusion ( Li et al , 2016 ) , Integrated gradients 6 ( Sundararajan et al , 2017 ) , SHAP ( Lundberg and Lee , 2017 ) , etc . and model - dependent algorithms , like LRP ( Binder et al , 2016 ) , DeepLIFT ( Shrikumar et al , 2017 ) , CD ( Murdoch et al , 2018 ) , ACD ( Singh et al , 2019 ) , SOC ( Jin et al , 2020 ) , etc . While the model - agnostic algorithms can be used as blackbox explanation tools that can work for any neural network architecture , for the latter , one needs to understand the network 's architectural details to implement the explanation algorithm . Typically , model - dependent algorithms require specific layer decomposition rules ( Ancona et al , 2018 ; Murdoch et al , 2018 ) which needs to be defined for all the components in the model . Model - agnostic methods usually work directly with the model outputs and gradients which are universally available . Due to the many desirable explanation axioms and ease of gradient computation , there has been several extensions of integrated gradients . For example , Miglani et al ( 2020 ) study the effect of saturation in the saliency maps generated by integrated gradients . Merrill et al ( 2019 ) extend integrated gradients to certain classes of discontinuous functions in financial domains . Further , Jha et al ( 2020 ) use KNNs and auto - encoders to learn latent paths for RNAs . Different from prior work , our focus here is to improve integrated gradients specifically for the discrete textual domain . While the idea of learning latent paths for text data is quite interesting , it brings a significant amount of challenge in successfully modeling such a complex latent space and hence , we leave this for future work .", "entities": [[88, 89, "MethodName", "LIME"], [118, 119, "MethodName", "SHAP"], [172, 173, "DatasetName", "SOC"]]}
{"text": "In Figure 5 , we visualize the effect of changing top - k% on log - odds , comprehensiveness , and sufficiency metrics for DistilBERT model fine - tuned on the SST2 dataset . We compare the two variants of our method : DIG - GREEDY and DIG - MAXCOUNT with Integrated Gradients . We observe that our method outperforms IG for all values of k. Specifically , we note that the gap between DIG and IG is initially non - existent but then gradually increases with increasing k in Figure 5 ( a ) and eventually saturates . This shows that although IG might be equally good as DIG at finding the top - 5 % important words , the explanations from IG are significantly misaligned from true model behavior for higher top - k values .", "entities": [[24, 25, "MethodName", "DistilBERT"], [31, 32, "DatasetName", "SST2"]]}
{"text": "In this section , we present some interesting sentence visualizations based on explanations from DIG and IG for SST2 dataset in Figure 6 . We show the sentence visualization and the model 's predicted sentiment for the sentence for each explanation algorithm . In the visualizations , the red highlighted words denote positive attributions and blue denotes negative attributions . That is , the explanation model suggests that the red highlighted words support the predicted label whereas the blue ones oppose ( or undermine ) the prediction . We observe that in many cases , DIG is able to highlight more plausible explanations . For example , in sentence pairs 1 - 7 , clearly the DIG highlights are more inline with the model prediction . But we want to emphasize that it does not mean that our method always produces more plausible highlights . For example , for sentences 8 - 10 , we observe that highlights from IG are more plausible than those of DIG . Hence , this shows that , while it could be a good exercise to visualize the attributions as a sanity check , we should rely more on automated metrics and human evaluations to correctly compare explanation algorithms . 6 : Some example visualizations of attributions from DIG and IG for the DistilBERT model fine - tuned on SST2 dataset . The sentence visualization is followed by model 's sentiment prediction for the sentence . Here , the red highlighted words denote positive attributions and blue denotes negative attributions . For more details , please refer to Appendix G", "entities": [[18, 19, "DatasetName", "SST2"], [219, 220, "MethodName", "DistilBERT"], [225, 226, "DatasetName", "SST2"]]}
{"text": "Evaluating the state - of - the - art event detection systems on determining spatio - temporal distribution of the events on the ground is performed unfrequently . But , the ability to both ( 1 ) extract events \" in the wild \" from text and ( 2 ) properly evaluate event detection systems has potential to support a wide variety of tasks such as monitoring the activity of sociopolitical movements , examining media coverage and public support of these movements , and informing policy decisions . Therefore , we study performance of the best event detection systems on detecting Black Lives Matter ( BLM ) events from tweets and news articles . The murder of George Floyd , an unarmed Black man , at the hands of police officers received global attention throughout the second half of 2020 . Protests against police violence emerged worldwide and the BLM movement , which was once mostly regulated to the United States , was now seeing activity globally . This shared task asks participants to identify BLM related events from large unstructured data sources , using systems pretrained to extract socio - political events from text . We evaluate several metrics , assessing each system 's ability to evolution of protest events both temporally and spatially . Results show that identifying daily protest counts is an easier task than classifying spatial and temporal protest trends simultaneously , with maximum performance of 0.745 ( Spearman ) and 0.210 ( Pearson r ) , respectively . Additionally , all baselines and participant systems suffered from low recall ( max.5.08 ) , confirming the high impact of media sourcing in the modelling of protest movements .", "entities": [[9, 11, "TaskName", "event detection"], [52, 54, "TaskName", "event detection"], [96, 98, "TaskName", "event detection"]]}
{"text": "The goal of this task is to evaluate the performance of automatic event detection systems on modeling the spatial and temporal pattern of a social protest movement . We evaluate the capability of participant systems to reproduce a manually curated BLM - related protest event data set , by detecting BLM event reports , enriched with location and date attributes , from a news corpus collection , a Twitter collection , and from the union of the two .", "entities": [[12, 14, "TaskName", "event detection"]]}
{"text": "For the Gold Standard data ( i.e. , the BLM events list we wish to automatically detect ) we considered two online sources of Black Lives Matter protest events : Creosote Maps 2 and Race and Policing 3 . Starting with these two data sets , we first checked if the source URL link was still active . If not , we referenced other data sets for the event in question : Wikipedia ( a list of George Floyd protests in and outside of the U.S. ) and the New York Times . If a valid article was not found matching this protest date and location , then we performed a Google search for the specific event . If still nothing was found , then the event was removed from the data set . If at any point , we discovered a valid URL for the event , we ran a validation check . This check asked : ( 1 ) is the source a tweet or Facebook post ; ( 2 ) does the source describe an upcoming event ; ( 3 ) is the source irrelevant to the protest at the location ; ( 4 ) does the source have enough information ; and ( 5 ) is the source not accessible because of a paywall . If the source passed this check , we then scraped the source for the publication date and days of the week in the article text . If the publication date and the day of the week do not match , we then inferred the date of the protest by the mention of the day of the week closest to the publication date . Finally , we manually checked the scraped or inferred dates and record this as the event date . In the end , this produced 3 , 463 distinct U.S. events between May 25 and June 30 , 2020 with date , city , and state information . Of these events , only 537 ( approximately 15 % of the events ) occurred after the first week of June . To compensate for the lack of coverage across all of June , we used the open source data set from the The Crowd Counting Consortium ( CCC ) 4 . From our original data set of 3 , 463 events , 754 events also occurred in the CCC data , matching on ( 1 ) URL or ( 2 ) both date and city . We then combined the two data sets ( i.e. , the CCC events with our original list ) and removed duplicates . This resulted in 7 , 976 protest events in our final Gold Standard data . The U.S. map in Figure 1 shows the spatial distribution of these events ( yellow dots ) .", "entities": [[111, 112, "DatasetName", "Google"], [373, 375, "TaskName", "Crowd Counting"]]}
{"text": "To this purpose , we also measure the correlation coefficients on the absolute event counts with respect to Gold Standard , over each single cell - day . For both analyses , we use two types of correlation coefficients to assess variable 's relationship : Pearson coefficient r and Spearman 's rank correlation coefficient \u03c1 . Moreover , we used Root Mean Squared Error ( RMSE ) to measure the absolute value of the error on estimating cell / event counts from the Gold Standard .", "entities": [[63, 64, "MetricName", "Error"], [65, 66, "MetricName", "RMSE"]]}
{"text": "This system , developed by the Task organizers and denoted NexusDdpl , is an extension of the Baseline system , where an event deduplication has been integrated as a post - processing module . The algorithm uses two metrics based on geographical distance between two event points and semantic distance , respectively . The semantic distance is computed using the cosine between the projections of the sentence embeddings of the texts of the events records . The LASER embeddings ( Schwenk and Douze , 2017 ) were used for that purpose . Twitter data has been cleaned of hashtags , URLs , and accounts names , as these have a negative impact on the semantic similarity measure . In order to be considered duplicate two events must have both distance measures under a fixed threshold , which were set to 2 km for spatial distance , 0.20 for semantic distance on NYT data , 0.30 for semantic distance on Twitter data . The reason of these different threshold depending on the data sets is that Twitter data are noisier than NYT data , with higher variations in text size and style when describing a single event . As such looser threshold was required . When applying on the combination of both data sets , we use a compromise threshold of 0.35 was used .", "entities": [[66, 68, "TaskName", "sentence embeddings"], [114, 116, "TaskName", "semantic similarity"]]}
{"text": "Four teams participated in this event : DaDeFrNi , EventMiner , Handshakes , and NoConflict . We briefly describe the systems below and ask the reader to refer to their systems papers for additional details . DaDeFrNi This team considered two slightly different procedures for this task . For the NYT data set , they first extracted geo - entities from each article using the Python library geography , which was used to classify each entity in one of the three categories \" city \" , \" country \" , and \" region \" . For the cases where an article contained the name of a city but did not provide any region or country reference , DaDeFrNi retrieved the necessary information by checking the city name against a worldwide cities database . When the name of a city was associated with several locations , we filtered the city with the highest population , along with its corresponding \" region \" and \" country \" . For the Twitter data set , given the large size of the data , the above procedure was computationally expensive . Thus , the Python library spaCy ( Honnibal et al , 2020 ) for retrieving NER / GPE entities , given its much smaller computational cost . The complete system details can be found in Ignazio Re et al ( 2021 ) . EventMiner Team EventMiner 's approach for Task 3 is mainly based on transformer models ( Hettiarachchi et al , 2021 ) . This approach involved three steps : ( 1 ) event document identification , ( 2 ) location detail extraction , ( 3 ) and event filtering to identify the spatial and temporal pattern of the targeted social protest movement . Event documents are identified using the winning solution submitted to CASE 2021 Task 1 - Subtask 1 : event document classification ( Hettiarachchi et al , 2021 ) . Next , the location details in event described tweets are extracted . Since this team only focused on the Twitter corpus , they used tweet metadata to extract location details . However , since the majority of the tweets are not geotagged and to extract the location details mentioned in the text , they used a NER approach too . For NER , a transformer model is fine - tuned for token classification using the data set released with the WNUT 2017 Shared Task on Novel and Emerging Entity Recognition ( Derczynski et al , 2017 ) . The BERTweet model is used since it is pretrained on Tweets ( Nguyen et al , 2020 ) . To convert the location details into an unique format and fill the missing details ( e.g. region , country ) , locations are geocoded using the GeoPy library 9 . For the final step , event tweets with location details are grouped based on their created dates and locations and removed the groups with fewer tweets assuming that important events generate a high number of tweets . Three systems were submitted . For the first system , denoted by \u2020 , only the new events are included ( i.e. , events with locations which are identified in the previous day are removed ) . The second system \u2020 \u2020 , includes all the extracted events ( i.e. , no filtering as in \u2020 ) . Finally , the third system \u2020 \u2020 \u2020 further filters the events from \u2020 to include U.S. events only . Please see Hettiarachchi et al ( 2021 ) for more details Handshakes This model is a pretrained XLM - RoBERTa model , fine - tuned on the multi - language article data from Task 1 Subtask 1 and sentence data from Subtask 2 , with a classification head that predicts if the input text is a protest or not . We make use of the provided location data in the data sets , where available . Please see Kalyan et al ( 2021 ) for further details . NoConflict Team NoConflict used their model of protest event sentence classification from the winning submission of the English version of Task 1 Subtask 2 . Their model is based on a RoBERTa ( Liu et al , 2019 ) backbone with a second pretraining ( Gururangan et al , 2020 ) stage done on the POLUSA ( Gebhard and Hamborg , 2020 ) data set before finetuned on Subtask 2 data . For the NYT data set , they first filtered the articles based on the section name . They then ran their model on the abstract of each article to identify ones containing protest events . For each remaining article , they run a transformer - based ( Vaswani et al , 2017 ) named entity recognition from spaCy ( Honnibal et al , 2020 ) to identify the location and date of the events . They covert the location to absolute location using the Geocoder library and convert the date of the event to the absolute date based on the article 's publication date . If the relative location or date is unavailable , they default to those included in the metadata . The event sentence classification system details can be found in Hu and Stoehr ( 2021 ) . Three systems were submitted for the NYT data , denoted , , and . Each system used a set of manually curated keywords applied to different parts of each data point . Theses rules are included in the Appendix . For the Twitter data set , Team NoConflict ran their model on the full text of each tweet to identify protest events . For each potential event tweet , they identify the location and time based on the metadata of the tweet itself and the main tweet if it is a retweet .", "entities": [[202, 203, "TaskName", "NER"], [311, 313, "TaskName", "document classification"], [377, 378, "TaskName", "NER"], [382, 383, "TaskName", "NER"], [392, 394, "TaskName", "token classification"], [401, 403, "DatasetName", "WNUT 2017"], [412, 413, "DatasetName", "Derczynski"], [600, 601, "MethodName", "XLM"], [602, 603, "MethodName", "RoBERTa"], [680, 682, "TaskName", "sentence classification"], [702, 703, "MethodName", "RoBERTa"], [726, 727, "DatasetName", "POLUSA"], [796, 799, "TaskName", "named entity recognition"], [868, 870, "TaskName", "sentence classification"], [974, 975, "DatasetName", "retweet"]]}
{"text": "Honey or Poison ? Solving the Trigger Curse in Few - shot Event Detection via Causal Intervention", "entities": [[12, 14, "TaskName", "Event Detection"]]}
{"text": "Event detection has long been troubled by the trigger curse : overfitting the trigger will harm the generalization ability while underfitting it will hurt the detection performance . This problem is even more severe in few - shot scenario . In this paper , we identify and solve the trigger curse problem in few - shot event detection ( FSED ) from a causal view . By formulating FSED with a structural causal model ( SCM ) , we found that the trigger is a confounder of the context and the result , which makes previous FSED methods much easier to overfit triggers . To resolve this problem , we propose to intervene on the context via backdoor adjustment during training . Experiments show that our method significantly improves the FSED on ACE05 , MAVEN and KBP17 datasets .", "entities": [[0, 2, "TaskName", "Event detection"], [56, 58, "TaskName", "event detection"], [134, 135, "DatasetName", "MAVEN"]]}
{"text": "Event detection ( ED ) aims to identify and classify event triggers in a sentence , e.g. , detecting an Attack event triggered by fire in \" They killed by hostile fire in Iraqi \" . Recently , supervised ED approaches have achieved promising performance ( Chen et al , 2015 ; Nguyen and Grishman , 2015 ; Nguyen et al , 2016 ; Lin et al , 2018Lin et al , , 2019bDu and Cardie , 2020 ; Liu et al , 2020a ; Lu et al , 2021 ) , but when adapting to new event types and domains , a large number of manually annotated event data is required which is expensive . By contrast , fewshot event detection ( FSED ) aims to build effective event detectors that are able to detect new events from instances ( query ) with a few labeled instances ( support set ) . Due to their ability to classify novel types , many few - shot algorithms have been used in FSED , e.g. , metric - based methods like Prototypical Network ( Lai et al , 2020 ; Deng et al , 2020 ; Cong et al , 2021 ) . Unfortunately , there has long been a \" trigger curse \" which troubles the learning of event detec - They were killed by hostile [ MASK ] in Iraqi . They were killed by hostile fire in Iraqi . 1 or 0 tion models , especially in few - shot scenario ( Bronstein et al , 2015 ; Liu et al , 2017 ; Chen et al , 2018 ; Liu et al , 2019 ; Ji et al , 2019 ) . For many event types , their triggers are dominated by several popular words , e.g. , the Attack event type is dominated by war , attack , fight , fire , bomb in ACE05 . And we found the top 5 triggers of each event type cover 78 % of event occurrences in ACE05 . Due to the trigger curse , event detection models nearly degenerate to a trigger matcher , ignore the majority of contextual information and mainly rely on whether the candidate word matches the dominant triggers . This problem is more severe in FSED : since the given support instances are very sparse and lack diversity , it is much easier to overfit the trigger of the support instances . An intuitive solution for the trigger curse is to erase the trigger information in instances and forces the model to focus more on the context . Unfortunately , due to the decisive role of triggers , directly wiping out the trigger information commonly hurts the performance ( Lu et al , 2019 ; Liu et al , 2020b ) . Some previous approaches try to tackle this problem by introducing more di - versified context information like event argument information ( Liu et al , 2017 ( Liu et al , , 2019Ji et al , 2019 ) and document - level information ( Ji and Grishman , 2008 ; Liao and Grishman , 2010 ; Duan et al , 2017 ; Chen et al , 2018 ) . However , rich context information is commonly not available for FSED , and therefore these methods can not be directly applied . Query E T C S Y E C T S Y Q ( In this paper , we revisit the trigger curse in FSED from a causal view . Specifically , we formulate the data distribution of FSED using a trigger - centric structural causal model ( SCM ) ( Pearl et al , 2016 ) shown in Figure 1 ( a ) . Such trigger - centric formulation is based on the fact that , given the event type , contexts have a much lower impact on triggers , compared with the impact of triggers on contexts . This results in the decisive role of triggers in event extraction , and therefore conventional event extraction approaches commonly follow the triggercentric procedure ( i.e. , identifying triggers first and then using triggers as an indicator to find arguments in contexts ) . Furthermore , the case grammar theory in linguistics ( Fillmore , 1967 ) also formulate the language using such trigger / predicate - centric assumption , and have been widely exploited in many NLP tasks like semantic role labeling ( Gildea and Jurafsky , 2002 ) and abstract meaning representation ( Banarescu et al , 2013 ) . From the SCM , we found that T ( trigger set ) is a confounder of the C ( context set ) and the Y ( result ) , and therefore there exists a backdoor path C T Y . The backdoor path explains why previous FSED models disregard contextual information : it misleads the conventional learning procedure to mistakenly regard effects of triggers as the effects of contexts . Consequently , the learning criteria of conventional FSED methods are optimized towards spurious correlation , rather than capturing causality between C and Y . To address this issue , we propose to intervene on context to block the information from trigger to context . Specifically , we apply backdoor adjustment to estimate the interventional distribution that is used for optimizing causality . Furthermore , because backdoor adjustment relies on the unknown prior confounder ( trigger ) distribution , we also propose to estimate it based on contextualized word prediction . We conducted experiments on ACE05 1 , MAVEN 2 and KBP17 3 datasets . Experiments show that causal intervention can significantly alleviate trigger curse , and therefore the proposed method significantly outperforms previous FSED methods .", "entities": [[0, 2, "TaskName", "Event detection"], [120, 122, "TaskName", "event detection"], [189, 192, "DatasetName", "Deng et al"], [243, 244, "DatasetName", "0"], [346, 348, "TaskName", "event detection"], [667, 669, "TaskName", "event extraction"], [673, 675, "TaskName", "event extraction"], [737, 740, "TaskName", "semantic role labeling"], [926, 927, "DatasetName", "MAVEN"]]}
{"text": "Causal Inference . Causal inference aims to make reliable predictions using the causal effect between variables ( Pearl , 2009 ) . Many studies have used causal theory to improve model robustness ( Wang et al , 2020a , b ; Qi et al , 2020 ; Tang et al , 2020b ; Zeng et al , 2020 ) . Recently , backdoor adjustment has been used to remove the spurious association brought by the confounder ( Tang et al , 2020a ; Yue et al , 2020 ; Zhang et al , 2021 ) . Few - shot Event Detection . Few - shot event detection has been studied in many different settings . Bronstein et al ( 2015 ) collect some seed triggers , then detect unseen event with feature - based method . Deng et al ( 2020 ) decompose FSED into two subtasks : trigger identification and few - shot classification . Feng et al ( 2020 ) adopt a sentence - level few - shot classification without triggers . Lai et al ( 2020 ) and Cong et al ( 2021 ) adopt N+1 - way fewshot setting that is closest to our setting .", "entities": [[0, 2, "MethodName", "Causal Inference"], [3, 5, "MethodName", "Causal inference"], [99, 101, "TaskName", "Event Detection"], [105, 107, "TaskName", "event detection"], [136, 139, "DatasetName", "Deng et al"]]}
{"text": "We prove L SG ( \u03b8 ) is equivalent to L ( \u03b8 ) , which indicates that minimizing L SG ( \u03b8 ) is equivalent to minimizing L ( \u03b8 ) . At first , we define a function \u03c6 ( s , q ) \u221d P ( Y | s , q ; \u03b8 ) and then we need to prove that g ( t T s S P ( t | e ) p ( s | C , t ) r s , q ) = f ( t T s S P ( t | e ) P ( s | C , t ) \u03c6 ( s , q ) ) . From Appendix - A , we can obtain :", "entities": [[5, 6, "HyperparameterName", "\u03b8"], [12, 13, "HyperparameterName", "\u03b8"], [22, 23, "HyperparameterName", "\u03b8"], [30, 31, "HyperparameterName", "\u03b8"], [55, 56, "HyperparameterName", "\u03b8"]]}
{"text": "All of our experiments are implemented on one Nvidia TITAN RTX . Our implementation is based on HuggingFace 's Transformers ( Wolf et al , 2019 ) and Allennlp ( Gardner et al , 2018 ) . We tune the hyperparameters based on the dev performance . We train each model 5 times with different random seed , and when evaluating , we sample 4 different support sets .", "entities": [[9, 10, "DatasetName", "TITAN"]]}
{"text": "The hyperparameter is shown in Table 6 . For pretraining , we train a supervised event detection model using the training set . For finetuning , we use the support set to finetune the parameters of the event detection model and then detect the event in query .", "entities": [[15, 17, "TaskName", "event detection"], [37, 39, "TaskName", "event detection"]]}
{"text": "In recent times , multi - modal analysis has been an emerging and highly sought - after field at the intersection of natural language processing , computer vision , and speech processing . The prime objective of such studies is to leverage the diversified information , ( e.g. , textual , acoustic and visual ) , for learning a model . The effective interaction among these modalities often leads to a better system in terms of performance . In this paper , we introduce a recurrent neural network based approach for the multi - modal sentiment and emotion analysis . The proposed model learns the inter - modal interaction among the participating modalities through an auto - encoder mechanism . We employ a context - aware attention module to exploit the correspondence among the neighboring utterances . We evaluate our proposed approach for five standard multi - modal affect analysis datasets . Experimental results suggest the efficacy of the proposed model for both sentiment and emotion analysis over various existing state - of - the - art systems .", "entities": [[97, 98, "DatasetName", "emotion"], [165, 166, "DatasetName", "emotion"]]}
{"text": "In recent past , the world has witnessed tremendous growth of various social media platforms , e.g. , YouTube , Instagram , Twitter , Facebook , etc . People treat these platforms as a communication medium and freely express themselves with the help of a diverse set of input sources , e.g. videos , images , audio , text etc . The amount of information produced daily through these mediums are enormous , and hence , the research on multi - modal information processing has attracted attention to the researchers and developers . A video is a multimodal input which provides visual , acoustic , and textual information . The motivation of multi - modal sentiment and emotion analysis lies in fact to leverage the varieties of ( often distinct ) information from multiple sources for building more efficient systems . For some cases , text can provide a better clue for the prediction , whereas for the others , acoustic or visual sources can be more informative . Similarly , in some situations , a combination of two or more information sources together ensures better and unambiguous classification decision . For example , only text \" shut up \" can not decide the mood of a person but acoustic ( tone of a person ) and visual ( expression of a person ) can reveal the exact mood . Similarly , for some instances visual features such as gesture , postures , facial expression etc . have important roles to play in determining the correctness of the system . However , effectively combining this information is a nontrivial task that researchers often have to face ( Poria et al , 2016 ; Ranganathan et al , 2016 ; Lee et al , 2018 ) . Traditionally , ' text ' has been the key factor in any Natural Language Processing ( NLP ) tasks , including sentiment and emotion analysis . However , with the recent emergence of social media platforms , an interdisciplinary study involving text , visual and acoustic features have drawn a great interest among the research community . Expressing the feelings and emotions through a video is much convenient than the text for a user , and it is the best source to extract all multi - modal information . Not only the visual , it also provides other information such as acoustic and textual representation of spoken language . Additionally , a single video can have multiple utterances based on a speaker 's pause ( speech bounded by breaths ) with different sentiments and emotions . The sentiments and emotions of an utterance often have interdependence on the other contextual utterances . Independently classifying such an utterance poses several challenges to the underlying problem . In contrast , multi - modal sentiment and emotion analysis take inputs from more than one sources e.g. text , visual , acoustic for the analysis . Effectively fusing this diverse information is non - trivial and poses several challenges to the underlying problem . In our current work , we propose an end - to - end Context - aware Interactive Attention ( CIA ) based recurrent neural network for sentiment and emotion analysis . We aim to leverage the interaction between the modalities to increase the confidence of individual task in prediction . The main contributions of our current research are as follows : ( 1 ) We propose an Inter - modal Interactive Module ( IIM ) that aims to learn the interaction among the diverse and distinct features of the input modalities , i.e. , text , acoustic and visual ; ( 2 ) We employ a Context - aware Attention Module ( CAM ) that identifies and assigns the weights to the neighboring utterances based on their contributing features . It exploits the interactive representations of pairwise modalities to learn the attention weights , and ( 3 ) We present new state - of - the - arts for five benchmark datasets for both sentiment and emotion predictions .", "entities": [[117, 118, "DatasetName", "emotion"], [319, 320, "DatasetName", "emotion"], [469, 470, "DatasetName", "emotion"], [534, 535, "DatasetName", "emotion"], [618, 619, "MethodName", "CAM"], [672, 673, "DatasetName", "emotion"]]}
{"text": "Since the utterances in a video are the split units of the break / pause of the speech , their emotions ( or sentiments ) often have relations with their neighboring utterances . Therefore , knowledge of the emotions ( or , sentiments ) of the neighboring utterances is an important piece of information and has the capability to derive the prediction of an utterance , if the available inputs are insufficient for the correct prediction . Our proposed context - aware attention module leverages the contextual information . For each utterance in a video , we compute the attention weights of all the neighboring utterances based on their contributions in predicting the current utterance . It ensures that the network properly utilizes the local contextual information of an utterance as well as the global contextual information of a video together . The aim is to compute the interactive attention weights utilizing a softmax activation for each utterance in the video . Next , we apply a multiplicative gating mechanism following the work of Dhingra et al ( 2016 ) . The attentive representation is , then , forwarded to the upper layers for further processing . We summarize the process of CAM in Algorithm3 .", "entities": [[153, 154, "MethodName", "softmax"], [202, 203, "MethodName", "CAM"]]}
{"text": "One of the key objectives of the multi - modal analysis is to fuse the available input modalities effectively . In general , different modalities represent distinct features despite serving a common goal . For example , in multi - modal sentiment analysis all the three modalities , i.e. , text , acoustic , and visual , aim to predict the expressed polarity of an utterance . The distinctive features in isolation might create an ambiguous scenario for a network to learn effectively . Therefore , we introduce an auto - encoder based inter - modal interactive module whose objective is to learn the interaction between two distinct modalities to serve a common goal . The IIM encodes the feature representation of one modality ( say , text ) , and aims to decode it into the feature representation of another modality ( say , acoustic ) . Similar to an auto - encoder where the input and output are conceptually the same ( or closely related ) , in our case the input and output feature representations of two modalities also intuitively serve a common goal . After training of IIM , the encoded vector signifies a joint representation of the two modalities , which can be further utilized in the network . As the proposed architecture in Figure 1 depicts , our proposed model is an end - to - end system , which takes multi - modal raw features for each utterance in a video and predicts an output . We also train our proposed IIM in the combined framework . For any pair of modalities , e.g. , text - visual , the encoded vector in IIM receives two gradients of errors , i.e. , one error from the IIM output ( visual ) l 1 and another from the task - specific label l 2 . We aggregate the errors ( l 1 + l 2 ) at the encoded vector and backpropagate it to the input ( text ) . Thus , the weights in the encoder part will adjust according to the desired task - specific label as well . However , in contrast , the decoder part does not have such information . Therefore , we employ another IIM to capture the interaction between the visual - text . This time , the visual features are aware of the desired label during the interaction with textual features . A conceptual diagram , depicting the gradient flow in IIM for the text and visual modalities , is shown in Figure 2 .", "entities": [[41, 43, "TaskName", "sentiment analysis"]]}
{"text": "The above datasets offer different dimension of sentiment analysis . We define the following setups for our experiments . Two - class ( pos and neg ) classification : MO - SEI , MOSI , ICT - MMMO , and MOUD . Three - class ( pos , neu , and neg ) classification : YouTube . Five - class ( strong pos , weak pos , neu , weak neg , and strong neg ) classification : MOSEI . Seven - class ( strong pos , moderate pos , weak pos , neu , weak neg , moderate neg , and strong neg ) classification : MOSEI and MOSI . Intensity prediction : MOSEI and MOSI .", "entities": [[7, 9, "TaskName", "sentiment analysis"], [33, 34, "DatasetName", "MOSI"], [109, 110, "DatasetName", "MOSI"], [116, 117, "DatasetName", "MOSI"]]}
{"text": "We analyze our proposed CIA model to understand the importance of the baseline framework CIA - IIM . We study the predictions of both the models and observe that the proposed CIA framework improves the predictions of the baseline CIA - IIM model . It indicates that the CIA framework , indeed , learns the interaction among the input modalities , and the model effectively exploits this interaction for better judgment . In Table 6 , we list the utterances of a CMU - MOSEI video along with their correct and predicted labels for both the proposed and baseline systems . The video in Table 6 has 4 utterances , out of which the correct sentiments of three utterances ( i.e. , u 1 , u 3 , and u 4 ) are positive , while one utterance ( i.e. , u 2 ) is negative . We observe that our proposed CIA model predicts all the 4 utterances correctly , while the CIA - IIM mis - classify the sentiments of the utterances , u 2 and u 3 . We also analyze the context - aware attention module ( CAM ) with the help of heatmaps of the attention weights . The heatmaps , as depicted in Figure 3 , represent the contributing utterances in the neighbourhood for the classification of each utterance . Figures 3a , 3b and 3c show the heatmaps of the pair - wise modality interaction of the proposed model CIA . In Figure 3a , each cell ( i , j ) of the heatmap signifies the weights of utterance ' j ' for the classification of utterance ' i ' . For the utterance u 4 , the model puts more attention weights on the u 2 and u 3 of the text - visual interactions , while for the text - acoustic interaction the model assigns higher weights to the u 4 utterance itself . Similarly , the model assigns the least weight to the u 1 utterance , whereas the utterance u 3 gets the highest weights . We argue that the proposed CAM module captures the diversity in the input modalities of the contextual utterances for the correct prediction . For emotion prediction , the CIA model captures all the emotions correctly , while the CIA - IIM framework fails to predict the correct emotions of the utterances , u 2 and u 3 . For the same video , we also show the attention heatmaps for emotion in Figure 3 . For the utterance u 2 , our proposed model ( CIA ) captures the emotion class ' sad ' as the CAM module assigns higher attention weights on the utterances u 2 and u 3 in Figure 3d , u 4 in Figure 3e , and u 2 in Figure 3f . Since the system finds the contributing neighbours as utterances u 2 , u 3 and u 4 for various combinations , we argue that it utilizes the information of these utterances - which all express the ' sad ' emotion - for the correct prediction of utterance u 2 as ' sad ' .", "entities": [[82, 85, "DatasetName", "CMU - MOSEI"], [191, 192, "MethodName", "CAM"], [261, 262, "MethodName", "heatmap"], [353, 354, "MethodName", "CAM"], [372, 373, "DatasetName", "emotion"], [418, 419, "DatasetName", "emotion"], [437, 438, "DatasetName", "emotion"], [444, 445, "MethodName", "CAM"], [514, 515, "DatasetName", "emotion"]]}
{"text": "In this paper , we have proposed a Context - aware Interactive Attention framework that aims to capture the interaction between the input modalities for the multi - modal sentiment and emotion prediction . We employed a contextual attention module to learn the contributing utterances in the neighborhood by exploiting the interaction among the input modalities . We evaluate our proposed approach on five standard multi - modal datasets . Experiments suggest the effectiveness of the proposed model over various existing systems , for both sentiment and emotion analysis , as we obtained new state - of - the - art for all five datasets . In current work , we undertook the problem of sentiment and emotion analysis for a single - party utterances . In future , we would like to extend our work towards the multi - party dialogue . 6 Acknowledgment", "entities": [[31, 32, "DatasetName", "emotion"], [87, 88, "DatasetName", "emotion"], [117, 118, "DatasetName", "emotion"]]}
{"text": "Improving Graph - based Sentence Ordering with Iteratively Predicted Pairwise Orderings", "entities": [[4, 6, "TaskName", "Sentence Ordering"]]}
{"text": "Dominant sentence ordering models can be classified into pairwise ordering models and set - to - sequence models . However , there is little attempt to combine these two types of models , which inituitively possess complementary advantages . In this paper , we propose a novel sentence ordering framework which introduces two classifiers to make better use of pairwise orderings for graph - based sentence ordering ( Yin et al , 2019 ( Yin et al , , 2021 . Specially , given an initial sentence - entity graph , we first introduce a graph - based classifier to predict pairwise orderings between linked sentences . Then , in an iterative manner , based on the graph updated by previously predicted highconfident pairwise orderings , another classifier is used to predict the remaining uncertain pairwise orderings . At last , we adapt a GRN - based sentence ordering model ( Yin et al , 2019 ( Yin et al , , 2021 on the basis of final graph . Experiments on five commonly - used datasets demonstrate the effectiveness and generality of our model . Particularly , when equipped with BERT ( Devlin et al , 2019 ) and FHDecoder ( Yin et al , 2020 ) , our model achieves state - of - the - art performance . Our code is available at https:// github.com/DeepLearnXMU/IRSEG .", "entities": [[1, 3, "TaskName", "sentence ordering"], [47, 49, "TaskName", "sentence ordering"], [65, 67, "TaskName", "sentence ordering"], [147, 149, "TaskName", "sentence ordering"], [191, 192, "MethodName", "BERT"]]}
{"text": "With the rapid development and increasing applications of natural language processing ( NLP ) , modeling text coherence has become a significant task , since it can provide beneficial information for understanding , evaluating and generating multi - sentence texts . As an important subtask , sentence ordering aims at recovering unordered sentences back to naturally coherent paragraphs . It is required to deal with logic and syntactic consistency , and has increasingly attracted attention due to its wide applications on several tasks such as text generation ( Konstas and Lapata , 2012 ; Holtzman et al , 2018 ) Recently , inspired by the great success of deep learning in other NLP tasks , researchers have resorted to neural sentence ordering models , which can be classified into : pairwise ordering models Agrawal et al , 2016 ; Li and Jurafsky , 2017 ; Moon et al , 2019 ; Kumar et al , 2020 ; Prabhumoye et al , 2020 ; Zhu et al , 2021 ) and set - to - sequence models ( Gong et al , 2016 ; Nguyen and Joty , 2017 ; Logeswaran et al , 2018 ; Mohiuddin et al , 2018 ; Cui et al , 2018 ; Yin et al , 2019 ; Oh et al , 2019 ; Yin et al , 2020 ; Cui et al , 2020 ; Yin et al , 2021 ) . Generally , the former predicts the relative orderings between pairwise sentences , which are then leveraged to produce the final ordered sentence sequence . Its advantage lies in the lightweight pairwise ordering predictions , since the predictions only depend on the semantic representations of involved sentences . By contrast , the latter is mainly based on an encoder - decoder framework , where an encoder is first used to learn contexualized sentence representations by considering other sentences , and then a decoder , such as pointer network ( Vinyals et al , 2015a ) , outputs ordered sentences . Overall , these two kinds of models have their own strengths , which are complementary to each other . To combine their advantages , Yin et al ( 2020 ) propose FHDecoder that is equipped with three pairwise ordering prediction modules to enhance the pointer network decoder . Along this line , Cui et al ( 2020 ) introduce BERT to exploit the deep semantic connection and relative orderings between sentences and achieve SOTA performance when equipped with FHDecoder . However , there still exist two drawbacks : 1 ) their pairwise ordering predictions only depend on involved sentence pairs , without considering other sentences in the same set ; 2 ) their one - pass pairwise ordering predictions are relatively rough , ignoring distinct difficulties in predicting different sentence pairs . Therefore , we believe that the potential of pairwise orderings in neural sentence ordering models has not been fully exploited . In this paper , we propose a novel iterative pairwise ordering prediction framework which introduces two classifiers to make better use of pairwise orderings for graph - based sentence ordering ( Yin et al , 2019 ( Yin et al , , 2021 . As an extension of Sentence - Enity Graph Recurrent Network ( SE - GRN ) ( Yin et al , 2019 ( Yin et al , , 2021 , our framework enriches the graph representation with iteratively predicted orderings between pairwise sentences , which further benefits the subsequent generation of ordered sentences . The basic intuitions behind our work are two - fold . First , learning contextual sentence representations is helpful to predict pairwise orderings . Second , difficulties of predicting ordering vary with respect to different sentence pairs . Thus , it is more reasonable to first predict the orderings of pairwise sentences easily to be predicted , and then leverage these predicted orderings to refine the predictions for other pairwise sentences . Concretely , we propose two graph - based classifiers to iteratively conduct ordering predictions for pairwise sentences . The first classifier takes the sentence - entity graph ( SE - Graph ) ( Yin et al , 2019 ( Yin et al , , 2021 as input and yields relative orderings of linked sentences via corresponding probabilities . Next , in an iterative manner , the second classifier enriches the previous graph representation by converting high - value probabilities into the weights of the corresponding edges , and then reconduct graph encoding to predict orderings for the other pairwise sentences . Based on the final weighted graph representation , we adapt SE - GRN to construct a graph - based sentence ordering model , of which the decoder is also a pointer network . To the best of our knowledge , our work is the first to exploit pairwise orderings to enhance the graph encoding for graph - based set - to - squence sentence ordering . To investigate the effectiveness of our framework , we conduct extensive experiments on several commonly - used datasets . Experimental results and in - depth analyses show that our model enhanced with some proposed technologies ( Devlin et al , 2019 ; Yin et al , 2020 ) achieves the state - of - the - art performance .", "entities": [[46, 48, "TaskName", "sentence ordering"], [85, 87, "TaskName", "text generation"], [120, 122, "TaskName", "sentence ordering"], [151, 152, "DatasetName", "Kumar"], [323, 325, "MethodName", "pointer network"], [381, 383, "MethodName", "pointer network"], [396, 397, "MethodName", "BERT"], [481, 483, "TaskName", "sentence ordering"], [518, 520, "TaskName", "sentence ordering"], [779, 781, "TaskName", "sentence ordering"], [790, 792, "MethodName", "pointer network"], [823, 825, "TaskName", "sentence ordering"]]}
{"text": "Early studies mainly focused on exploring humandesigned features for sentence ordering ( Lapata , 2003 ; Barzilay and Lee , 2004 ; Lapata , 2005 , 2008 ; Elsner and Charniak , 2011 ; Guinaudeau and Strube , 2013 ) . Recently , neural network based sentence ordering models have become dominant , consisting of the following two kinds of models : 1 ) Pairwise models . Generally , they first predict the pairwise orderings between sentences and then use them to produce the final sentence order via ranking algorithms Agrawal et al , 2016 ; Li and Jurafsky , 2017 ; Kumar et al , 2020 ; Prabhumoye et al , 2020 ; Zhu et al , 2021 ) . For example , first framed sentence ordering as a ranking task conditioned on pairwise scores . Agrawal et al ( 2016 ) conducted the same experiments as in the task of image caption storytelling . Similarly , Li and Jurafsky ( 2017 ) 2 ) Set - to - sequence Models . Basically , these models are based on an encoder - decoder framework , where the encoder is used to obtain sentence representations and then the decoder produces ordered sentences progressively . Among them , both Gong et al ( 2016 ) and Logeswaran et al ( 2018 ) Cui et al ( 2018 ) proposed ATTOrderNet that uses self - attention mechanism to learn sentence representations . Inspired by the successful applications of graph neural network ( GNN ) in many NLP tasks Xue et al , 2019 ; , Yin et al ( 2019Yin et al ( , 2021 represented input sentences with a unified SE - Graph and then applied GRN to learn sentence representations . Very recently , we notice that Chowdhury et al ( 2021 ) proposes a BART - based sentence ordering model . Please note that our porposed framework is compatible with BART ( Lewis et al , 2020 ) . For ( Yin et al , 2019 ( Yin et al , , 2021 . example , we can easily adapt the BART encoder as our sentence encoder . With similar motivation with ours , that is , to combine advantages of above - mentioned two kinds of models , Yin et al ( 2020 ) introduced three pairwise ordering predicting modules ( FHDecoder ) to enhance the pointer network decoder of ATTOrder - Net . Recently , Cui et al ( 2020 ) proposed BERSON that is also equipped with FHDecoder and utilizes BERT to exploit the deep semantic connection and relative ordering between sentences . However , significantly different from them , we borrow the idea from the mask - predict framework ( Gu et al , 2018 ; Ghazvininejad et al , 2019 ; Deng et al , 2020 ) to progressively incorporate pairwise ordering information into SE - Graph , which is the basis of our graph - based sentence ordering model . To the best of our knowledge , our work is the first attempt to explore iteratively refined GNN for sentence ordering .", "entities": [[9, 11, "TaskName", "sentence ordering"], [46, 48, "TaskName", "sentence ordering"], [102, 103, "DatasetName", "Kumar"], [126, 128, "TaskName", "sentence ordering"], [240, 241, "DatasetName", "Inspired"], [305, 306, "MethodName", "BART"], [308, 310, "TaskName", "sentence ordering"], [321, 322, "MethodName", "BART"], [352, 353, "MethodName", "BART"], [398, 400, "MethodName", "pointer network"], [424, 425, "MethodName", "BERT"], [467, 470, "DatasetName", "Deng et al"], [493, 495, "TaskName", "sentence ordering"], [516, 518, "TaskName", "sentence ordering"]]}
{"text": "In this section , we give a brief introduction to the SE - GRN ( Yin et al , 2019 ( Yin et al , , 2021 , which is selected as our baseline due to its competitive performance . As shown in Figure 1 , SE - GRN is composed of a Bi - LSTM sentence encoder , GRN paragraph encoder , and a pointer network ( Vinyals et al , 2015b ) decoder . o i . As illustrated in the middle of Figure 1 , each input sentence set is rep - resented as an undirected sentence - entity graph G = ( V , E ) , where V = { v i } I i=1 \u222a { v j } J j=1 and E = { e i , i } I , I i=1 , i = 1 \u222a { \u0113 i , j } I , J i=1 , j=1 \u222a { \u00ea j , j } J , J j=1 , j = 1 represent the nodes and edges respectively . Here , nodes include sentence nodes ( such as v i ) and entity nodes ( such asv j ) , and each edge is 1 ) sentencesentence edge ( ss - edge , such as e i , i ) linking two sentences having the same entity ; or 2 ) sentenceentity edge ( se - edge , such as\u0113 i , j ) connecting an entity to a sentence that contains it . Each se - edge is assigned with a label including subject , object or other , based on the syntactic role of its involved entity ; or 3 ) entity - entity edge ( ee - edge , such as\u00ea j , j ) connecting two semantic related entities . Besides , a virtual global node connecting to all nodes is introduced to capture global information effectively .", "entities": [[55, 56, "MethodName", "LSTM"], [65, 67, "MethodName", "pointer network"]]}
{"text": "Node representations of each sentence and each entity are first initialized with the concatenation of bidirectional last states of the Bi - LSTM sentence encoder and the corresponding GloVe word embedding , respectively . Then , a GRN is adapted to encode the above sentence - entity graph , where node states are updated iteratively . During the process of updating hidden states , the messages for each node are aggregated from its adjacent nodes . Specifically , the sentence - level message m ( l ) i and entity - level messagem ( l ) i for a sentence s i are defined as follows : m ( l ) i = v i N i w ( \u03ba ( l - 1 ) i , \u03ba ( l - 1 ) i ) \u03ba ( l - 1 ) i , m ( l ) i = v j N iw ( \u03ba ( l - 1 ) i , ( l - 1 ) j , rij ) ( l - 1 ) j , ( 1 ) where \u03ba ( l - 1 ) i and ( l - 1 ) j stand for the neighboring sentence and entity representations of the i - th sentence node v i at the ( l \u2212 1 ) - th layer , N i andN i denote the sets of neighboring sentences and entities of v i , and both w ( * ) andw ( * ) are gating functions with single - layer networks , involving associated node states and edge label r ij ( if any ) . Afterwards , \u03ba ( l - 1 ) i is updated by concatenating its original representation \u03ba ( 0 ) i , the messages from neighbours ( m ( l ) i andm ( l ) i ) and the global state g ( l - 1 ) via GRU : \u03be ( l ) i = [ \u03ba ( 0 ) i ; m ( l ) i ; m ( l ) i ; g ( l - 1 ) ] , \u03ba ( l ) i = GRU ( \u03be ( l ) i , \u03ba ( l - 1 ) i ) . ( ) 2 Similar to updating sentence nodes , each entity state ( l - 1 ) j is updated based on its word embedding emb j , hidden states of its connected sentence nodes ( such as \u03ba ( l - 1 ) i ) , and g ( l - 1 ) : m ( l ) j = v i N jw ( ( l - 1 ) j , \u03ba ( l - 1 ) i , rij ) \u03ba ( l - 1 ) i , m ( l ) j = v j N jw ( ( l - 1 ) j , ( l - 1 ) j ) ( l - 1 ) j , \u03be ( l ) j = [ embj ; m ( l ) j ; m ( l ) j ; g ( l - 1 ) ] , ( l ) j = GRU ( \u03be ( l ) j , ( l - 1 ) j ) . ( 3 ) Finally , the messages from both sentence and entity states are used to update global state g ( l - 1 ) via g ( l ) = GRU ( 1 | V | v i V \u03ba ( l - 1 ) i , 1 | V | v j V ( l - 1 ) j , g ( l - 1 ) ) . ( 4 ) The above updating process is iterated for L times . Usually , the top hidden states are considered as fine - grained graph representations , which will provide dynamical context for the decoder via attention mechanism .", "entities": [[22, 23, "MethodName", "LSTM"], [28, 29, "MethodName", "GloVe"], [291, 292, "DatasetName", "0"], [322, 323, "MethodName", "GRU"], [333, 334, "DatasetName", "0"], [363, 364, "MethodName", "GRU"], [537, 538, "MethodName", "GRU"], [584, 585, "MethodName", "GRU"]]}
{"text": "Given the learned hidden states { \u03ba ( L ) i } and g ( L ) , the prediction procedure for order o can be formalized as follows : P ( o | K ( L ) ) = I t=1 P ( o t | o < t , K ( L ) o t\u22121 ) , P ( o t | o < t , K ( L ) o t\u22121 ) = softmax ( q T tanh ( W h d t + U K ( L ) o t\u22121 ) ) , h d t = LSTM ( h d t\u22121 , \u03ba ( 0 ) o t\u22121 ) . Here , q , W and U are learnable parameters , K and the decoder hidden state at the t - th time step , which is initialized by g ( L ) as t=0 , respectively .", "entities": [[76, 77, "MethodName", "softmax"], [101, 102, "MethodName", "LSTM"], [109, 110, "DatasetName", "0"]]}
{"text": "In this section , we give a detailed description to our framework . As shown in Figure 2 , we first introduce two graph - based classifiers to construct an iteratively refined sentence - entity graph ( IRSE - Graph ) . It is a weighted version of SE - Graph , where pairwise ordering inforamtion is iteratively incorporated to update ss - edge weights . Then , we adapt the conventional GRN to establish a neural sentence ordering model based on the final IRSE - Graph .", "entities": [[77, 79, "TaskName", "sentence ordering"]]}
{"text": "Finally , following the conventional SE - GRN ( Yin et al , 2019 ( Yin et al , , 2021 , we construct a graph - based sentence ordering model . Note that the above two classifiers and our sentence ordering model are all based on IRSE - Graph rather than the conventional SE - Graph , which makes the standard GRN unable to be applied directly . To deal with this issue , we slightly adapt GRN to utilize pairwise ordering information for graph encoding . Specifically , we adapt Equation 1 to incorporate ss - edge weights into the message aggregation of sentence - level nodes : m ( l ) i = v i N i w i , i w ( \u03ba ( l - 1 ) i , \u03ba ( l - 1 ) i ) \u03ba ( l - 1 ) i , w ( \u03ba ( l - 1 ) i , \u03ba ( l - 1 ) i ) = \u03c3 ( Wg [ \u03ba ( l - 1 ) i ; \u03ba ( l - 1 ) i ] ) . ( 6 ) Here \u03c3 denotes sigmoid function and W g is learnable parameter matrix . Equation 6 expresses that the sentence - level aggregation should consider not only the semantic representations of the two involved sentences , but also the relative ordering between them . In addition , other Equations are the same as those of conventional GRN , which have been described in Section 3.2 .", "entities": [[28, 30, "TaskName", "sentence ordering"], [40, 42, "TaskName", "sentence ordering"]]}
{"text": "Table 1 reports the overall experimental results of sentence ordering . When incorporating BERT and FHDecoder into IRSE - GRN , our model achieves SOTA performance on most of datasets . Besides , we arrive at the following conclusions : First , IRSE - GRN significantly surpasses SE - GRN on all datasets ( bootstrapping test , p<0.01 ) , indicating that iteratively refining graph representations indeed benefit the ordering of input sentences . Second , IRSE - GRN+FHDecoder exhibits better performance than IRSE - GRN and all non - BERT baselines , which are shown above the upper dotted line of Table 1 , across datasets in different domains . Therefore , we confirm that our framework is orthogonal to the current approach exploiting pairwise ordering information for decoder . Third , when constructing our model based on BERT , IRSE - GRN+BERT+FHDecoder also outperforms all BERT - based baselines , such as Cons - Graph , BERSON , achieving SOTA performance . It can be known that our proposed framework is also effective when combining with pretrained language model . Finally , we note that IRSE - GRN+BERT+FH - Decoder gains relatively marginal improvement on SIND and ROCStory , and performs worse than BERSON in PMR on SIND . We speculate that there exist less ss - edges on these two datasets , resulting in that our proposed framework can not achieve its full potential . Specifically , average edge numbers of SIND and ROCStory are 2.85 and 5.66 respectively , far fewer than 16.60 , 10.86 and 16.73 on NIPS Abstract , ANN Abstract and arXiv Abstract . Besides , since it is a challenge to order longer paragraphs , we investigate the Kendall 's \u03c4 of our models and SE - GRN with respect to different sentence numbers , as shown in Figure 4 . Overall , all models degrade with the increase of sentence number . However , our model and its two enhanced versions always exhibit better performance than SE - GRN .", "entities": [[8, 10, "TaskName", "sentence ordering"], [13, 14, "MethodName", "BERT"], [90, 91, "MethodName", "BERT"], [139, 140, "MethodName", "BERT"], [147, 148, "MethodName", "BERT"], [197, 198, "DatasetName", "SIND"], [209, 210, "DatasetName", "SIND"], [244, 245, "DatasetName", "SIND"], [268, 269, "DatasetName", "arXiv"]]}
{"text": "As mentioned in previous studies ( Gong et al , 2016 ; Cui et al , 2018 ; Oh et al , 2019 ) , the first and last sentences are very important in a paragraph . Following these studies , we compare models by conducting experiments to predict the first and last sentences . As displayed in Table 3 , IRSE - GRN surpasses all non - BERT baselines , and IRSE - GRN+BERT+ FHDecoder wins against BERTSON . These results are consistent with those reported in Table 1 , further demonstrating the effectiveness of our model .", "entities": [[68, 69, "MethodName", "BERT"]]}
{"text": "We conduct several experiments to investigate the impacts of our proposed components on ROCstory dataset and arXiv dataset which are the two largest datasets . All results are provided in Table 4 , where we draw the following conclusions : First , using only iterative classifier , IRSE - GRN ( w/o initial classifier ) performs worse than IRSE - GRN . This result proves that iterative classifier fails to predict well from scratch and the pairwise ordering predicted by initial classifier is beneficial to construct a well - formed graph representation for iterative classifier . Second , when the iteration number k is set as 1 , the performance of IRSE - GRN decreases . Moreover , if we remove iterative classifier , the performance of IRSE - GRN becomes even worse . Therefore , we confirm that the iterative predictions of pairwise ordering indeed benefit the learning of graph representations . Finally , the result in the last line indicates that removing noisy weights leads to a significant performance drop . It suggests that the utilization of noisy weights is useful for the training of iterative classifier , which makes our model more robust .", "entities": [[16, 17, "DatasetName", "arXiv"]]}
{"text": "Following previous studies ( Barzilay and Lapata , 2005 ; Nayeem and Chali , 2017 ) spect the validity of our proposed framework via multi - document summarization . Concretely , we train different neural sentence ordering models on a large - scale summarization corpus ( Fabbri et al , 2019 ) , and then individually use them to reorder the small - scale summarization data of DUC2004 ( Task2 ) . Finally , we use coherence probability proposed by ( Nayeem and Chali , 2017 ) to evaluate the coherence of summaries . In this group of experiments , we conduct experiments using different weights : 0.5 and 0.8 , as implemented in ( Nayeem and Chali , 2017 ) and ( Yin et al , 2020 ) respectively . The results are reported in Table 5 . We can observe that the summaries reordered by IRSE - GRN and its variants achieve higher coherence probabilities than baseline , verifying the effectiveness of our proposed framework in the downstream task .", "entities": [[24, 28, "TaskName", "multi - document summarization"], [35, 37, "TaskName", "sentence ordering"], [43, 44, "TaskName", "summarization"], [64, 65, "TaskName", "summarization"]]}
{"text": "In this work , we propose a novel sentence ordering framework that makes better use of pairwise orderings for graph - based sentence ordering . Specifically , we introduce two classifiers to iteratively predict pairwise orderings , which are gradually incorporated into the graph as edge weights . Then , based on this refined graph , we construct a graph - based sentence ordering model . Experiments on five datasets demonstrate not only the superiority of our model over baselines , but also the compatibility to other modules utilizing pairwise ordering information . Moreover , when equipped with BERT and FHDecoder , our enhanced model achieves SOTA performance across datasets . In the future , we plan to explore more effective GNN for sentence ordering . In particular , we will improve our model by iteratively merging nodes to refine the graph representation .", "entities": [[8, 10, "TaskName", "sentence ordering"], [22, 24, "TaskName", "sentence ordering"], [62, 64, "TaskName", "sentence ordering"], [98, 99, "MethodName", "BERT"], [123, 125, "TaskName", "sentence ordering"]]}
{"text": "In order to train and evaluate machine learning systems to match or correct authors ' names , a dataset of name en - tities containing the different surface forms ( or variants ) of authors ' names is required . The entities should reflect as well as possible the variability that can be found in the RFR dataset , as was illustrated in the case of F. Scott Fitzgerald in Section 1 . For each entity , a canonical name should be elected and correspond to the name that should be preferred for the purpose of e - commerce . Instead of setting these gold spellings by following some predefined rules ( i.e. family name in the first position , initial of first name , etc . ) , for e - commerce applications it is more appropriate that the displayed authors names have the most popular spellings among readers . In agreement with Rakuten catalog analysts we set the most popular spelling of an author name as the one found on Wikipedia 4 or DBpedia ( Lehmann et al , 2015 ) . While Wikipedia seems more pertinent to select canonical names matching the e - commerce user expectations , specialized librarian data services , such as the Library of Congress Name Authority 5 , could be used in future research to enrich the dataset of name entities . Name entities are collected in three distinct ways : 1 . ISBN matching : for each book the different author names found via ISBN search on external sources and the RFR author name field build up an entity . The canonical form is the one that is matched with Wikipedia or DBpedia ; else the one provided by the greatest number of sources . 2 . Matching of Rakuten authors : we build entities using fuzzy search on the author name field on DBpedia and consider the DBpedia value to be canonical . We limit the number of false positives in fuzzy search by tokenizing both names , and keeping only the names where at least one token from the name on RFR is approximately found in the external resource ( Levenshtein distance < 2 ) . 3 . Name variants : DBpedia , BnF , and JRCnames ( Steinberger et al , 2011 ; Maud et al , 2016 ) directly provide data about people ( not limited to book authors ) and their name variants . As an example , by using the wikiPageRedirects field in DBpedia we can build a large entity for the canonical name \" Anton Tchekhov \" , containing \" Anton Tchechov \" , \" Ant\u00f2n P\u00e0vlovi\u010d Ch\u00e9chov \" , \" Checkhov \" , \" Anton Chekov \" , and many more . After creating the name entity dataset , we normalize all names to latin - 1 . We obtain about 750 , 000 entities , for a total of 2.1 million names .", "entities": [[175, 176, "DatasetName", "DBpedia"], [281, 282, "DatasetName", "DBpedia"], [313, 314, "DatasetName", "DBpedia"], [317, 318, "DatasetName", "DBpedia"], [372, 373, "DatasetName", "DBpedia"], [418, 419, "DatasetName", "DBpedia"]]}
{"text": "For any given book with an ISBN and an author 's name , all three techniques shown in Fig . 1 provide one or several candidate canonical names . As we aim at providing an automated tool to enhance the quality of the book products , the final system should provide a ranked list of candidates with a calibrated confidence level . For this purpose we train a logistic regression to estimate the probability that a proposal is the canonical form for an author 's name . This information is then used as a confidence score to rank the different candidate names returned by the three normalization approaches . Specifically , we represent a proposal with a set of 12 features : 11 indicating whether it is found in the bibliographic sources , generated from the seq2seq model , matched with the Siamese network or equal to the input name , and one last feature corresponding to the cosine distance between the representation of the proposal and that of the input name . The selected features reflect that the confidence of the global system should increase with ( i ) the consensus among the different sources , and ( ii ) the similarity of the candidate to the input name . For this component we use the annotated dataset introduced in Section 2.4 , splitting the books between training and test sets , with a ratio of 50 % : 50 % , generating a total of 11185 proposals .", "entities": [[68, 70, "MethodName", "logistic regression"], [136, 137, "MethodName", "seq2seq"], [142, 144, "MethodName", "Siamese network"]]}
{"text": "For each category of constraints introduced in Section 2 , we discuss best practices for both human and automatic evaluation . We leave out overlap due to ease of automatic evaluation . Additionally , we perform a case study , evaluating how well black - box synonym substitution attacks GENETICATTACK and TEXTFOOLER fulfill constraints . Both attacks find adversarial examples by swapping out words for their synonyms until the classifier is fooled . GENETICATTACK uses a genetic algorithm to attack an LSTM trained on the IMDB 6 document - level sentiment classification dataset . TEXTFOOLER uses a greedy approach to attack an LSTM , CNN , and BERT trained on five classification datasets . We chose these attacks because : They claim to create perturbations that preserve semantics , maintain grammaticality , and are not suspicious to readers . However , our inspection of the perturbations revealed that many violated these constraints . They report high attack success rates . 7 They successfully attack two of the most effective models for text classification : LSTM and BERT . To generate examples for evaluation , we attacked BERT using TEXTFOOLER and attacked an LSTM using GENETICATTACK . We evaluate both methods on the IMDB dataset . In addition , we evaluate TEXTFOOLER on the Yelp polarity document - level sentiment classification dataset and the Movie Review ( MR ) sentence - level sentiment classification dataset ( Pang and Lee , 2005 ; Zhang et al , 2015 ) . We use 1 , 000 examples from each dataset . Table 3 shows example violations of each constraint .", "entities": [[81, 82, "MethodName", "LSTM"], [85, 86, "DatasetName", "IMDB"], [102, 103, "MethodName", "LSTM"], [107, 108, "MethodName", "BERT"], [171, 173, "TaskName", "text classification"], [174, 175, "MethodName", "LSTM"], [176, 177, "MethodName", "BERT"], [186, 187, "MethodName", "BERT"], [192, 193, "MethodName", "LSTM"], [202, 203, "DatasetName", "IMDB"], [226, 227, "DatasetName", "MR"]]}
{"text": "Semantics Grammaticality Edit Distance Non - Suspicion Synonym Substitution . ( Alzantot et al , 2018 ; Kuleshov et al , 2018 ; Jin et al , 2019 ; Ren et al , 2019 ) 3 3 3 3 Character Substitution . ( Ebrahimi et al , 2017 ; Gao et al , 2018 ; Li et al , 2018 ) 3 5 3 3 Word Insertion or Removal . ( Liang et al , 2017 ; Samanta and Mehta , 2017 ) 3 3 3 3 General Paraphrase . ( Zhao et al , 2017 ; Ribeiro et al , 2018 ; Iyyer et al , 2018 ) 3 3 5 3 A \" 3 \" indicates that the respective attack is supposed to meet the constraint , and a \" 5 \" means the attack is not supposed to meet the constraint .", "entities": [[87, 88, "DatasetName", "General"]]}
{"text": "To quantify semantic similarity of x and x adv , we asked users whether they agreed that the changes between the two passages preserved meaning on a scale of 1 ( Strongly Disagree ) to 5 ( Strongly Agree ) . We averaged scores for each attack method to determine if the method generally preserves semantics . Perturbations generated by TEXTFOOLER were rated an average of 3.28 , while perturbations generated by GENETICATTACK were rated on average 2.70 . 8 The average rating given for both methods was significantly less than our proposed \u270f sem of 4 . Using a clear survey question illustrates that humans , on average , do n't assess these perturbations as semantics - preserving .", "entities": [[2, 4, "TaskName", "semantic similarity"]]}
{"text": "We propose evaluation of non - suspicion by having judges view a shuffled mix of real and adversarial inputs and guess whether each is real or computer - altered . This is similar to the human evaluation done by Ren et al ( 2019 ) , but we formulate it as a binary classification task rather than on a 1 - 5 scale . A perturbed example x adv is not suspicious if the percentage of judges who identify x adv as computer - altered is at most \u270f ns , where 0 \uf8ff \u270f ns \uf8ff 1 .", "entities": [[92, 93, "DatasetName", "0"]]}
{"text": "In Section 4 , we evaluated how well generated examples met constraints . We found that although attacks in NLP aspire to meet linguistic constraints , in practice , they frequently violate them . Now , we adjust automatic constraints applied during the course of the attack to produce better quality adversarial examples . We set out to find if a set of constraint application methods with appropriate thresholds could produce adversarial examples that are semanticspreserving , grammatical and non - suspicious . We modified TEXTFOOLER to produce TFADJUSTED , a new attack with stricter constraint application . To enforce grammaticality , we added Language - Tool . To enforce semantic preservation , we tuned two thresholds which filter out invalid word substitutions : ( a ) minimum cosine similarity between counter - fitted word embeddings and ( b ) minimum cosine similarity between sentence embeddings . Through human studies , we found threshold values of 0.9 for ( a ) and 0.98 for ( b ) 9 . We implemented TFADJUSTED using TextAttack , a Python framework for implementing adversarial attacks in NLP ( Morris et al , 2020 ) .", "entities": [[134, 136, "TaskName", "word embeddings"], [144, 146, "TaskName", "sentence embeddings"]]}
{"text": "We tested TFADJUSTED to determine the effect of tightening constraint application . We used the IMDB , Yelp , and MR datasets for classifcation as in Section 4 . We added the SNLI and MNLI entailment datasets ( Bowman et al , 2015 ; Williams et al , 2018 ) for the portions not requring human evaluation . Table 5 shows the results . Semantics . TEXTFOOLER generates perturbations for which human judges are on average \" Not sure \" if semantics are preserved . With perturbations generated by TFADJUSTED , human judges on average \" Agree \" that semantics are preserved .", "entities": [[15, 16, "DatasetName", "IMDB"], [20, 21, "DatasetName", "MR"], [32, 33, "DatasetName", "SNLI"], [34, 35, "DatasetName", "MNLI"]]}
{"text": "When an attack 's success rate improves , it may be the result of either ( a ) improvement of the search method for finding adversarial perturbations or ( b ) more lenient constraint definitions or constraint application . TEXTFOOLER achieves a higher success rate than GENETICATTACK , but Jin et al ( 2019 ) did not identify whether the improvement was due to ( a ) or ( b ) . Since TEXTFOOLER uses both a different search method and different constraint application methods than GENETICATTACK , the source of the difference in attack success rates is unclear . To determine which search method is more effective , we used TextAttack to compose attacks from the search method of GENETICATTACK and the constraint application methods of each of TEXTFOOLER and TFADJUSTED ( Morris et al , 2020 ) . With the constraint application held constant , we can identify the source of the difference in attack success rate . Table 7 reveals that the genetic algorithm of GENETICATTACK is more successful than the greedy search of TEXTFOOLER at both constraint application levels . This reveals the source of improvement in attack success rate between GENETICATTACK and TEXTFOOLER to be more lenient constraint application . However , GE - NETICATTACK 's genetic algorithm is far more computationally expensive , requiring over 40x more model queries . Table 7 : Comparison of the search methods from GENETICATTACK and TEXTFOOLER with two sets of constraints ( TEXTFOOLER and TFADJUSTED ) . Attacks were run on 1000 samples against BERT fine - tuned on the MR dataset . GENETICATTACK 's genetic algorithm is more successful than TEXTFOOLER 's greedy strategy , albeit much less efficient . that preserve semantics and grammaticality , NLP models are relatively robust to current synonym substitution attacks . Note that our set of constraints is n't necessarily optimal for every attack scenario . Some contexts may require fewer constraints or less strict constraint application . Decoupling search methods and constraints . It is critical that researchers decouple new search methods from new constraint evaluation and constraint application methods . Demonstrating the performance of a new attack that simultaneously introduces a new search method and new constraints makes it unclear whether empirical gains indicate a more effective attack or a more relaxed set of constraints . This mirrors a broader trend in machine learning where researchers report differences that come from changing multiple independent variables , making the sources of empirical gains unclear ( Lipton and Steinhardt , 2018 ) . This is especially relevant in adversarial NLP , where each experiment depends on many parameters . Towards improved methods for generating textual adversarial examples . As models improve at paraphrasing inputs , we will be able to explore the space of adversarial examples beyond synonym substitutions . As models improve at measuring semantic similarity , we will be able to more rigorously ensure that adversarial perturbations preserve semantics . It remains to be seen how robust BERT is when subject to paraphrase attacks that rigorously preserve semantics and grammaticality .", "entities": [[256, 257, "MethodName", "BERT"], [262, 263, "DatasetName", "MR"], [474, 476, "TaskName", "semantic similarity"], [498, 499, "MethodName", "BERT"]]}
{"text": "The goal of creating adversarial examples that preserve semantics and grammaticality is common in the NLP attack literature ( Zhang et al , 2019 ) . However , previous works use different definitions of adversarial examples , making it difficult to compare methods . We provide a unified definition of an adversarial example based on a goal function and a set of linguistic constraints . Gilmer et al ( 2018 ) laid out a set of potential constraints for the attack space when generating adversarial examples , which are each useful in different real - world scenarios . However , they did not discuss NLP attacks in particular . Michel et al ( 2019 ) defined a framework for evaluating attacks on machine translation models , focusing on meaning preservation constraints , but restricted their definitions to sequence - to - sequence models . Other research on NLP attacks has suggested various constraints but has not introduced a shared vocabulary and categorization that allows for effective comparisons between attacks .", "entities": [[122, 124, "TaskName", "machine translation"]]}
{"text": "Probabilistic Case - based Reasoning for Open - World Knowledge Graph Completion", "entities": [[9, 12, "TaskName", "Knowledge Graph Completion"]]}
{"text": "Given a query , our approach gathers KG path types from entities that are similar to the query entity . Each path type is weighed with respect to an estimate of both its frequency and precision ( 2.2.1 ) . By clustering similar entities together ( 2.2.2 ) , our model obtains robust estimate of the path statistics ( 2.2.3 ) . Our approach is non - parametric because - ( a ) Instead of storing reasoning rules in parameters ( Das et al , 2018 ; Minervini et al , 2020 ) , it derives them dynamically from k - similar entities ( like a non - parametric k - nn classifier ( Cover and Hart , 1967 ) ) . ( b ) We cluster entities together using a non - parametric clustering approach and provide an efficient way of adding / estimating parameters when entities are added to the KG ( 2.3 ) .", "entities": [[109, 112, "MethodName", "k - nn"]]}
{"text": "Our approach first finds k similar entities to the query entity that have atleast an edge of type r q . For example , for the query ( MELINDA GATES , WORKS IN CITY , ? ) , we would consider WAR - REN BUFFET if we observe ( WARREN BUFFET , WORKS IN CITY , OMAHA ) . We refer to these entities as ' contextual entities ' . Each entity is represented as a sparse vector of its outgoing edge types , i.e. e i { 0 , 1 } | R | . If entity e i has m distinct outgoing edge types , then the dimension corresponding to those types are set to 1 . This is an extremely simple and flexible way of representing entities which we find to work well . Also note that , as more data is added about an entity , this sparse representation makes it trivial to update the embeddings . Let E c , q denote the set of contextual entities for the query q. To compute E c , q , we first sort entities with respect to their cosine distance with respect to query entity and select the k entities with the least distance and which have the query relation r q . For each contextual entity e c , we gather the path types ( up to length n ) that connect e c to the entities it is connected by the edge r q ( i.e. P n ( e c , r q ) in 2.1 ) . These extracted path types will be used to reason about the query entity . Let P n ( E c , q , r q ) = e c E c , q P n ( e c , r q ) represent the set of unique path types from the contextual entities . The probability of finding the answer entity e 2 given the query is given by : P ( e 2 | e 1q , r q ) = \u2211 p P n ( E ( c , q ) , r q ) P ( e 2 , p | e 1q , r q ) = \u2211 p P ( p | e 1q , r q ) P ( e 2 | p , e 1q , r q ) ( 1 ) We marginalize the random variable representing the path types obtained from E c , q . P ( p | e 1q , r q ) denotes the probability of finding a path type given the query . This term captures how frequently each path type co - occurs with a query and represents the prior probability for a path type . On the other hand , P ( e 2 | p , e 1q , r q ) captures the proportion of times , when a path type p is traversed starting from the query entity , we reach the correct answer instead of some other entity . This term can be understood as capturing the likelihood of reaching the right answer or the ' precision ' of a reasoning path type . This is crucial in penalizing ' spurious ' path types that sometimes coincidentally find the right answer entity . For example , for the query relation WORKS IN CITY , the path type ( FRIEND LIVES IN CITY ) might have a high prior probability ( since people often have many friends in the city where they work ) . However , this path is ' spurious ' with respect to WORKS IN CITY , since they might have friends living in various cities and hence this path type will not necessarily return the correct answer .", "entities": [[88, 89, "DatasetName", "0"]]}
{"text": "Equation 1 has parameters for each entity in the KG . For large KGs , this can quickly lead to parameter explosion . Also , estimating per - entity parameter leads to noisy estimates due to sparsity . Instead , we choose to cluster similar entities together . Let c be a random variable representing the cluster assignment of the query entity . Then for the pathprior term , we have P ( p | e 1q , r q ) = \u2211 c P ( c | e 1q , r q ) P ( p | c , e 1q , r q ) We assume that each entity is assigned to one cluster , so P ( c | e 1q , r q ) is zero for all clusters except the cluster in which the query entity belongs to . Secondly we assume , that the prior probability of a path given the entity and cluster can be determined from the cluster alone and is independent of each entity in the cluster . In other words , if c e 1q is the cluster in which the e 1 , q has been assigned , then P ( p | c e 1q , e 1q , r q ) = P ( p | c e 1q , r q ) . Instead of perentity parameters , we now aggregate statistics over entities in the same cluster and have per - cluster parameters . We also show that this leads to significantly better performance ( 3.3 ) . A similar argument applies for the path - precision term in which we calculate the proportion of times , a path leads to the correct answer entity starting from each entity in the cluster . To perform clustering , we use hierarchical agglomerative clustering with average linkage with the entity - entity similarity defined in 2.2.1 . We extract a non - parameteric number of clusters from the hierarchy using a threshold on the linkage function . Agglomerative clustering has been shown to be effective in many knowledge - base related tasks such as entity resolution ( Lee et al , 2012 ; Vashishth et al , 2018 ) and in general has shown to outperform flat clustering methods such as K - means ( Green et al , 2012 ; Kobren et al , 2017 ) . A flat clustering is extracted from the hierarchical clustering by using a threshold on the linkage function score . We perform a breadth first search from the root of the tree stopping at nodes for which the linkage is above the given threshold . The nodes where the search stops give a flat clustering ( refer to A.2 for more detail on this ) .", "entities": [[357, 359, "TaskName", "entity resolution"]]}
{"text": "Knowledge Base Completion . Given an entity e 1 and a relation r , our task is retrieve all entities e 2 such that ( e 1 , r , e 2 ) belongs in the edges E in a KG G. This task is known as tail prediction . If the relation is instead the inverse relation r \u22121 , we assume that we are given an e 2 and asked to predict entities e 1 such that ( e 1 , r \u22121 , e 2 ) belongs in the edges E ( head prediction ) . To be exactly comparable to baselines , we report an average of head and tail prediction results 3 . We are given a knowledge graph with three partitions of edges , E train , E dev , E test . For this task , we evaluate against several stateof - the - art embeddings based models such as Dist - Mult ( Yang et al , 2015 ) , ComplEx ( Trouillon et al , 2016 ) , ConvE ( Dettmers et al , 2018 ) , RotatE ( Sun et al , 2019 ) . We also compare against several parametric rule learning methods - NTP , NeuralLP , MINERVA ( Das et al , 2018 ) , GNTP ( Minervini et al , 2020 ) and also the closely related CBR approach of Das et al ( 2020 ) . Open - world Knowledge Base Completion . In this setting , we begin with the top 10 % of the most popular nodes ( with several edges going out from them ) and add more randomly selected nodes such that the initial seed KB contains 50 % of all the entities in V . This is to ensure , that the seed KB is not too sparse and the initial models trained on them are meaningful . Next , any edges between the nodes selected are added to the seed KB . We divide the rest of the entities randomly into 10 batches . Each batch of entities is incrementally added to the KB along with the edges contained in it . The validation and test set are also divided in the same way , i.e. if both the head and tail entity of a triple are present in the KB , only then the triple is put in the corresponding splits . Parametric models for KBC that learn representations for a fixed set of entities can not handle ' open - world ' setting out - of - the - box . We extend the most competitive embedding based model - RotatE ( Sun et al , 2019 ) for this task . For every new entity arriving in a batch , we initialize a new entity embedding for it . We explore two ways of initial - Here , represents the Hadamard ( or element - wise ) product . This initialization minimizes the RotatE objective for the new embedding ensuring that it is \" well - placed \" according to the model in the previous time step . Embeddings for new relations are initialized randomly . Next , the model is further trained on the new batch of triples so that the new entity embeddings get trained . Note , for massive KGs , it might be impractical to re - train on the entire data as new batches of data arrive frequently , however to still prevent the model to forget what it had learned before , we also sample m% of triples that it had already been trained on and re - train on them . We ensure that triples in the neighborhood of the newly added entities are ten times likely to be sampled more than other triples . We also try a setting where we try freezing the initially trained entity embeddings and only training the new entity and relation embeddings .", "entities": [[0, 3, "TaskName", "Knowledge Base Completion"], [186, 187, "MethodName", "RotatE"], [244, 247, "TaskName", "Knowledge Base Completion"], [443, 444, "MethodName", "RotatE"], [497, 498, "MethodName", "RotatE"], [547, 549, "TaskName", "entity embeddings"], [648, 650, "TaskName", "entity embeddings"]]}
{"text": "The results for KBC tasks are presented in Table 2 and 3 4 . Our method does significantly better than parametric rule learning approaches such as MIN - ERVA , GNTPs and the recent case - based approach of Das et al ( 2020 ) . We would like to highlight the difference between the performance of our model and that of Das et al ( 2020 ) on the test - II evaluation of FB122 where triples can be answered by learning logical rules . This results emphasizes the importance of our probabilistic weighing of paths . We also perform comparably to most embedding based models and achieve state - of - the - art results on the overall test sets of FB122 and NELL - 995 . We report the mean over 3 runs for our model . We perform an ablation where we do not cluster entities ( i.e. every entity has its own cluster ) and have per - entity parameters . Table 4 notes the drop in performance due to the noisy estimates of path prior and precision parameters because of sparsity . Table 6 shows an example where our model learns to score different paths based on the type of entities present in the cluster . Effect of path length on WN18RR : On the dev set of WN18RR , out of 2985 queries where our method does not rank the answer in the top - 10 , 2030 queries require a minimum path length greater than 3 . Path - based reasoning models have no power to answer these queries . To correct for this , we perform an experiment with the path length n = 5 ( 950 of 2030 answers are reachable ) . The results in Table 5 show that our method recovers a significant portion of performance when allowed to use longer reasoning paths .", "entities": [[75, 76, "DatasetName", "FB122"], [123, 124, "DatasetName", "FB122"], [125, 126, "DatasetName", "NELL"], [217, 218, "DatasetName", "WN18RR"], [224, 225, "DatasetName", "WN18RR"]]}
{"text": "Open - world KG completion . Shi and Weninger ( 2018 ) consider the task of open - world KG completion . However , they use text descriptions to learn entity representations using convolutional neural networks . Our model does not use additional text data and we use very simple entity representations that helps us to perform well . learns to update a KG with new links by reading news . Even though they handle adding or deleting new edges , they do not observe new entities . Lastly , none of them learn from similar entities using a CBR approach . Inductive representation learning on KGs . Recent works ( Teru et al , 2020 ; Wang et al , 2020 ) learn entity independent relation representations and hence allow them to handle unseen entities . However , they do not perform contextual reasoning by gathering reasoning paths from similar entities . Moreoever , in our open - world setting , we consider the more challenging setting , where new facts and entities are arriving in a streaming fashion and we give an efficient way of updating parameters using online hierarchical clustering . This allows our method to be applicable in settings where the initial KG is small and it grows continuously . Rule induction in knowledge graphs . Classic work in inductive logic programming ( ILP ) ( Muggleton et al , 1992 ; Quinlan , 1990 ) induce rules from grounded facts . However , they need explicit counter - examples which are not present in KBs and they do not scale to large KBs . Recent ILP approaches ( Gal\u00e1rraga et al , 2013 ( Gal\u00e1rraga et al , , 2015 try to fix Figure 3 : Results for open - world setting when trained with 10 % ( top row ) and 30 % ( bottom row ) of already seen edges . Our online method matches the offline version of our approach and outperforms the online variants of RotatE. After all data is observed our online method achieves results closest to the best offline method 's results .", "entities": [[103, 105, "TaskName", "representation learning"], [217, 219, "TaskName", "knowledge graphs"], [223, 226, "TaskName", "inductive logic programming"]]}
{"text": "( politician - us - member - of - political - group , person - belongs - to - organization \u22121 , agent - belongs - to - organization ) ( agent - collaborates - with - agent , agent - belongs - to - organization ) ( Getoor and Taskar , 2007 ; Kok and Domingos , 2007 ; Schoenmackers et al , 2010 ) and probabilistic logic approaches ( Richardson and Domingos , 2006 ; Broecheler et al , 2010 ; Wang et al , 2013 ) combine machine learning and logic to learn rules . However , none of these work derive reasoning rules dynamically from similar entities in the knowledge graph . Bayesian non - parametric approaches for linkprediction . There is a rich body of work in bayesian non - parametrics to automatically learn the latent dimension of entities ( Kemp et al , 2006 ; Xu et al , 2006 ) . Our method does not learn latent dimension of entities , instead our work is nonparametric because it gathers reasoning paths from nearest neighbors and can seamlessly reason with new entities by efficiently updating parameters using online non - parametric hierarchical clustering . Embedding - based approach for link prediction . We also compare to the more popular embeddings based models based on tensor factorization or neural approaches ( Nickel et al , 2011 ; Bordes et al , 2013 ; Dettmers et al , 2018 ; Sun et al , 2019 ) . Our simple approach which needs no iterative opti - mization outperforms most of them and performs comparably to the latest RotatE model . Moreover we outperform RotatE in the online experiments . CBR for KG completion . There has been few attempts to apply CBR for knowledge management ( Dubitzky et al , 1999 ; Bartlmae and Riemenschneider , 2000 ) , however they do not do contextualized reasoning or consider online settings . Our work is most closely related to the recent work of Das et al ( 2020 ) . However , since it does not take in to account the importance of each path , it suffers from low performance , with our model outperforming it in several benchmarks .", "entities": [[22, 23, "DatasetName", "agent"], [31, 32, "DatasetName", "agent"], [37, 38, "DatasetName", "agent"], [39, 40, "DatasetName", "agent"], [205, 207, "TaskName", "link prediction"], [271, 272, "MethodName", "RotatE"], [277, 278, "MethodName", "RotatE"]]}
{"text": "We thank anonymous reviewers and members of UMass IESL and NLP groups for helpful discussion and feedback . This work is funded in part by the Center for Data Science and the Center for Intelligent Information Retrieval , and in part by the National Science Foundation under Grants No . IIS - 1514053 and No . 1763618 , and in part by the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction . Any opinions , findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor .", "entities": [[35, 37, "TaskName", "Information Retrieval"]]}
{"text": "Incorporating related text information has proven successful in stock market prediction . However , it is a huge challenge to utilize texts in the enormous forex ( foreign currency exchange ) market because the associated texts are too redundant . In this work , we propose a BERT - based Hierarchical Aggregation Model to summarize a large amount of finance news to predict forex movement . We firstly group news from different aspects : time , topic and category . Then we extract the most crucial news in each group by the SOTA extractive summarization method . Finally , we conduct interaction between the news and the trade data with attention to predict the forex movement . The experimental results show that the category based method performs best among three grouping methods and outperforms all the baselines . Besides , we study the influence of essential news attributes ( category and region ) by statistical analysis and summarize the influence patterns for different currency pairs . * This work is done when Deli Chen is a intern at Mizuho Securities .", "entities": [[8, 11, "TaskName", "stock market prediction"], [47, 48, "MethodName", "BERT"], [93, 95, "TaskName", "extractive summarization"]]}
{"text": "Deep learning and Natural Language Processing technologies have been widely applied in market prediction tasks ( Strau\u00df et al , 2018 ; Alostad and Davulcu , 2017 ; Li et al , 2015 ; Ni et al , 2019 ) , and the market related finance news has proven very useful for the prediction ( Ding et al , 2016 ; Xu and Cohen , 2018 ) . However , the studies of prediction in forex market , which is the largest market in the world with the highest daily trading volume , is much less than that in the stock market . Figure 1 shows the average numbers per hour of forex related news . There is a large amount of finance news related to forex trading with different influence , so it is a huge challenge to extract the useful semantic information from news . Most of previous works ( Bakhach et al , 2016 ; Shen and Liang , 2016 ; Pradeepkumar and Ravi , 2016 ; Contreras et al , 2018 ; Weeraddana et al , 2018 ) on forex prediction ignore related text totally and focus on the forex trade data only , which loses the important semantic information . Yet existing works ( Seifollahi and Shajari , 2019 ; Nassirtoussi et al , 2015 ) applying finance news in forex prediction mainly rely on manual rules to build feature vectors , which can hardly access the semantic information effectively . To make better use of finance news , we propose a novel neural model : Bert - based Hierarchical Aggregation Model ( BHAM ) to summarize a large amount of finance news for forex movement prediction . We suppose that the finance news is redundant and only a small amount of news plays a crucial role in forex trading . So the key point is how to extract the most important news . In BHAM , we design a hierarchical structure to extract essential news at the group level first and then aggregate the semantic information across all groups . We expect the news is more related intragroup and less related inter - groups to make the extraction more effective . We design three grouping methods from different aspects : time , topic or category . At the group level , we concatenate news headlines in the same group and regard news extraction in each group as an extractive summarization task . We modify the SOTA extractive summarization model proposed in ( Liu , 2019 ) to select the most important news . The connection process can let the selected news both content aware and context aware . Followingly , we conduct multimodal interaction between news data and trade data through attention mechanism to predict the forex prediction . The trade data represents the history movement of the forex , and the news data represents the environment variable . These two types of information are highly related . We conduct experiments on four major currency pairs ( USD - EUR , USD - JPY , USD - RMB , USD - GBP ) , and the experimental results show that the category - based BHAM performs best among all the baselines and proposed methods in all currency pairs . Based on this method , we analyze the influence of input time and prediction time on forex trading . We also analyze the influence of news category and news region and find various influence patterns for different currency pairs , which may be enlightening to the forex investors . The main contributions of this works are summarized as follows : We design a novel neural model to incorporate finance news in forex movement prediction . To the best of our knowledge , this is the first work to use the neural model to summarize a large amount of news for forex movement prediction . We propose three news grouping methods from different aspects : time , topic and category . Experiments show that the category based method performs best and outperforms all the baselines . Based on our experiments , we study the effect of time parameters on forex trading . We also analyze and summarize different influence patterns of finance news ( both category and region ) on different currency pairs .", "entities": [[404, 406, "TaskName", "extractive summarization"], [412, 414, "TaskName", "extractive summarization"]]}
{"text": "BERT ( Devlin et al , 2018 ) is a potent pretrained contextualized sentence representation and has proven obvious improvement for many NLP tasks ( Sun et al , 2019 ; Xu et al , 2019 ) . Liu ( 2019 ) proposes a modified BERT for extractive summarization and achieve the state - of - the - art result in extractive document summarization task . There have been many studies applying the related text in market prediction tasks . Moreover , the text assisted stock movement prediction has attracted many researchers ' interest . Most of these works predict stock movement based on single news : Si et al ( 2014 ) utilize the sentiment analysis to help the prediction . Duan et al ( 2018 ) adopt the summarization of news body instead of headline to predict . Ding et al ( 2016 ) propose the knowledgedriven event embedding method to make the forecast . Yet some others choose multi - news : Hu et al ( 2018 ) propose a hybrid attention network to combine news in different days . However , the number of combined news is still limited and much smaller than that of forex news . Compared to stock prediction , works about forex prediction is much scarce , and most of these works ( Carapu\u00e7o et al , 2018 ; Bakhach et al , 2016 ; Yong et al , 2018 ; Roledene et al , 2016 ; Contreras et al , 2018 ; Weeraddana et al , 2018 ) do not consider the text information . Shen and Liang ( 2016 ) employ stacked autoencoder to get the trade data representation and adopt support vector regression to predict . de Almeida et al ( 2018 ) combine SVM with genetic algorithms to optimize investments in Forex markets based on history price . Tsai et al ( 2018 ) choose the convolutional neural network to process the trading data . Besides , only limited works utilize the forex related text in the prediction process . Nassirtoussi et al ( 2015 ) adopt the WordNet ( Miller , 1995 ) and SentiWordNet ( Baccianella et al , 2010 ) to extract the text semantic and sentiment information and build the text feature vector to forecast forex movement . Following this work , Seifollahi and Shajari ( 2019 ) add word sense disambiguation in the sentiment analysis of news headlines . Vijayan and Potey ( 2016 ) apply the J48 algorithm in analyzing text . This kind of method pays more attention to access a fixed feature vector from news and can only represent news on a shallow level . In this work , we propose a selection and aggregation neural framework to process the larger amount of finance news and employ the powerful pre - trained BERT as text encoder , which can learn the deep semantic information effectively .", "entities": [[0, 1, "MethodName", "BERT"], [45, 46, "MethodName", "BERT"], [47, 49, "TaskName", "extractive summarization"], [61, 64, "TaskName", "extractive document summarization"], [115, 117, "TaskName", "sentiment analysis"], [130, 131, "TaskName", "summarization"], [161, 164, "DatasetName", "multi - news"], [204, 206, "TaskName", "stock prediction"], [272, 273, "MethodName", "autoencoder"], [295, 296, "MethodName", "SVM"], [297, 299, "MethodName", "genetic algorithms"], [395, 398, "TaskName", "word sense disambiguation"], [400, 402, "TaskName", "sentiment analysis"], [472, 473, "MethodName", "BERT"]]}
{"text": "Each sample in the dataset ( x , y , f ) contains the set of news text x , the forex trade data y , and the forex movement label f . x and y happen in the same input time window . To be more specific , x is a list of news groups x = C 1 , C 2 , , C L . L is the number of groups . The methods for dividing groups are introduced in Section 3.5 . Each news group is a sequence of finance news [ news 1 , news 2 , , news K ] in chronological order . y is the trade data embedding accessed by the method introduced in Section 3.6 . And f { 1 , 0 } is the forex movement label telling whether the forex trade price is up or down after a certain time ( we call it prediction delay ) . The forex movement prediction task can be defined as assigning movement label for the news input and trade data input .", "entities": [[130, 131, "DatasetName", "0"]]}
{"text": "The overview of the Bert - based Hierarchical Aggregation Model ( BHAM ) is displayed in Figure 2 . The model can be generally divided into two steps : ( 1 ) Intra - group extraction and ( 2 ) Intergroups aggregation . In the Intra - group extraction step , news in the same group is connected as a continuous paragraph , and we conduct extractive summarization on this paragraph to select the most important news . Specifically , we employ BERT as the encoder to get the contextualized paragraph representation and compute the importance score for each news . Then we select and aggregate the top - k ( k is a hyper - parameters ) news to get the final group representation . In the Inter - groups aggregation step , we first access the trade data representation by a 3 - layer perceptron and then employ the trade data representation as a query to calculate the attention scores of all the news group and obtain the final news representation . Finally , we fuse the final news representation and the trade data representation to predict the forex movement .", "entities": [[66, 68, "TaskName", "extractive summarization"], [82, 83, "MethodName", "BERT"]]}
{"text": "There will be lots of news in the same group , and we suppose that only a small amount of news has the greatest influence on the forex movement . The purpose of this step is to select the essential news from all news in group , which is redundant and full of noise . Inspired by the BERT - based extractive summarization model proposed in ( Liu , 2019 ) , we modify this method to select the most crucial news in each group . All the news in the same group is related to the subject of this group , and the connection of them in chronological order can be regarded as the continuous description of the group subject . The connection can make the news representations realize the context information of this group by passing information among different news . We suppose the context information can help select better news in group . The form of group news input for BERT encoder is illustrated in Figure 3 . We insert a [ CLS ] token before each news and a [ SEP ] token after each news . For the segment embedding , we use the loop of [ E A , E B ] to extend the raw segment embedding of BERT to multi - sentences . After the BERT encoding , all the [ CLS ] tokens cls are regarded as the semantic representations of the corresponding news . The importance score for each news is calculated base on these [ CLS ] tokens : score i = sigmoid ( W 0 * cls i + b 0 ) ( 1 ) t i = TOP k ( score i ) ( 2 ) s i = softmax ( t i ) ( 3 ) Where i { 1 , 2 , , L } , L is the number of groups . cls i is the list of [ CLS ] tokens in the ith group . W 0 and b 0 are the trainable parameters . score i is a list of values indicating the important scores of news . TOP k is an operation to select the top - k pieces of news with the highest scores . Then the group representation is calculated by the weighted sum of the top - k [ CLS ] tokens : G i = k j=1 cls i j * s i j ( 4 ) The G i is the final representation of the i - th news group which contains the semantic information from the most important news in this group .", "entities": [[55, 56, "DatasetName", "Inspired"], [58, 59, "MethodName", "BERT"], [61, 63, "TaskName", "extractive summarization"], [163, 164, "MethodName", "BERT"], [215, 216, "MethodName", "BERT"], [223, 224, "MethodName", "BERT"], [266, 267, "DatasetName", "0"], [272, 273, "DatasetName", "0"], [292, 293, "MethodName", "softmax"], [334, 335, "DatasetName", "0"], [337, 338, "DatasetName", "0"]]}
{"text": "In this method , news is divided into groups by category . The news categories 1 are { Business Sectors , Business General , Business Assets , Business Commodities , Business Organizations , Politics&International Affairs , Arts&Culture&Entertainment&Sports , Science & Technology , Other } . This method supposes that news in the same category is close to each other .", "entities": [[22, 23, "DatasetName", "General"]]}
{"text": "The raw record of forex data includes the open/ close / high / low trade prices for each minute . In order to extract all the possible features , we build the trade data embedding y containing multi aspects : Raw Number : open / close / high / low trade price for each trade minute . Change Rate : change rate of open / close/ high / low price compared to last trade minute . Trade Statistics : mean value , max value , min value , median , variance of all the trade prices in input minutes . The min - max scale is applied for each currency pair 's samples to scale the raw numbers in y to [ 0 , 1 ] according to the maximum and minimum value of each feature .", "entities": [[122, 123, "DatasetName", "0"]]}
{"text": "Here , we introduce the baselines in this work . Since there are few existing works , we modify two advanced models from stock prediction field which adopt multi - news as input for this task . Besides , we design some ablation variations of the proposed model to check the effects of different modules . The baselines are shown below : NoNews : This method considers the forex trade data only and use a 3 - layer perceptron ( the setting is same as full model ) to encode the trade data and make prediction . This is a baseline to check the improvement by adding text information . SVM : This method chooses the support vector machine to predict the result based on the feature vectors extracted by the method introduced in ( Seifollahi and Shajari , 2019 ) . HAN : This method is proposed in ( Hu et al , 2018 ) for stock movement prediction . It includes a hybrid attention mechanism and Gated Recurrent Unit to combine multi - day 's stock news to predict movement . We use every 5 minutes instead of each day as time unit for this method and the StockNet method because there is too much news for forex trading and the experiments show that the latest news has the most influence . StockNet : This method is proposed in ( Xu and Cohen , 2018 ) . It treats the prediction task as a generation task and designs a modified variational auto encoder to process multidays ' tweets to predict stock movement . NoGroup : This method does not group news and select key news directly from all news . NoConnect : This method does not connect news in the same group . Instead , it gets the representation for each news independently using BERT . This method groups news by category . LSTM+Attention : This method uses the bidirectional LSTM and self - attention to replace the BERT as text encoder . The number of LSTM hidden states is 256 , and the hidden - layer is 3 . This method groups news by category . methods perform well , and both BHAM - Topic and BHAM - Category methods outperform all the baselines . The BHAM - Category performs best among these methods , which shows that the semantic information of finance news is mostly aggregated by category . All the methods get improved after introducing the text information , which proves the related finance news is helpful for the prediction . The performance of NoGroup method decreases by a large margin compared to BHAM - Category , which demonstrates that the hierarchical structure works well . Without hierarchical structure , selecting essential news directly from all news has more noise and requires the model to have a stronger fitting ability for a longer paragraph . After removing the news connection , the performance of NoConnect method drops sharply compared to BHAM - Category . Accessing the news representation from the connected paragraph helps the news representation realize the context information in the group . The LSTM+Attention method performs worse than the BERT - based method , which proves that BERT has stronger power of sentence encoding . The two methods borrowed from stock movement prediction are designed to consider all news 's information , but the forex related news is redundant , which can explain the poor performance of these two methods .", "entities": [[23, 25, "TaskName", "stock prediction"], [28, 31, "DatasetName", "multi - news"], [110, 111, "MethodName", "SVM"], [116, 119, "MethodName", "support vector machine"], [168, 171, "MethodName", "Gated Recurrent Unit"], [200, 201, "DatasetName", "StockNet"], [224, 225, "DatasetName", "StockNet"], [306, 307, "MethodName", "BERT"], [321, 323, "MethodName", "bidirectional LSTM"], [330, 331, "MethodName", "BERT"], [338, 339, "MethodName", "LSTM"], [526, 527, "MethodName", "BERT"], [534, 535, "MethodName", "BERT"]]}
{"text": "Polyglot Semantic Parsing in APIs", "entities": [[1, 3, "TaskName", "Semantic Parsing"]]}
{"text": "Traditional approaches to semantic parsing ( SP ) work by training individual models for each available parallel dataset of text - meaning pairs . In this paper , we explore the idea of polyglot semantic translation , or learning semantic parsing models that are trained on multiple datasets and natural languages . In particular , we focus on translating text to code signature representations using the software component datasets of Richardson and Kuhn ( 2017a , b ) . The advantage of such models is that they can be used for parsing a wide variety of input natural languages and output programming languages , or mixed input languages , using a single unified model . To facilitate modeling of this type , we develop a novel graph - based decoding framework that achieves state - of - the - art performance on the above datasets , and apply this method to two other benchmark SP tasks . * Returns the greater of two long values public static long max ( long a , long b ) 2 . ( en , Python ) Documentation max ( self , a , b ) : \" \" \" Compares two values numerically and returns the maximum \" \" \" 3 . ( en , Haskell ) Documentation - - | \" The largest element of a non - empty structure \" maximum : : forall z. Ord a a = > t a - > a 4 . ( de , PHP ) Documentation * gibt den gr\u00f6\u00dferen dieser Werte zur\u00fcck .", "entities": [[3, 5, "TaskName", "semantic parsing"], [39, 41, "TaskName", "semantic parsing"]]}
{"text": "Recent work by Richardson and Kuhn ( 2017a , b ) ; Miceli Barone and Sennrich ( 2017 ) considers the problem of translating source code documentation to lower - level code template representations as part of an effort to model the meaning of such documentation . Example documentation for a number of programming languages is shown in Figure 1 , where each docstring description in red describes a given function ( blue ) in the library . While capturing the semantics of docstrings is in general a difficult task , learning the translation from descriptions to formal code representations ( e.g. , formal representations of functions ) is proposed as a reasonable first step towards learning more general natural language understanding models in the software domain . Under this approach , one can view a software library , or API , as a kind of parallel translation corpus for studying text code or code text translation . Richardson and Kuhn ( 2017b ) extracted the standard library documentation for 10 popular programming languages across a number of natural languages to study the problem of text to function signature translation . Initially , these datasets were proposed as a resource for studying semantic parser induction ( Mooney , 2007 ) , or for building models that learn to translate text to formal meaning representations from parallel data . In followup work ( Richardson and Kuhn , 2017a ) , they proposed using the resulting models to do automated question - answering ( QA ) and code retrieval on target APIs , and experimented with an additional set of software datasets built from 27 open - source Python projects . As traditionally done in SP ( Zettlemoyer and Collins , 2012 ) , their approach involves learning individual models for each parallel dataset or language pair , e.g. , ( en , Java ) , ( de , PHP ) , and ( en , Haskell ) . Looking again at Figure 1 , we notice that while programming languages differ in terms of representation conventions , there is often overlap between the functionality implemented and naming in these different languages ( e.g. , the max function ) , and redundancy in the associated linguistic descriptions . In addition , each English description ( Figure 1.1 - 1.3 ) describes max differently using the synonyms greater , maximum , largest . In this case , it would seem that training models on multiple datasets , as opposed to single language pairs , might make learning more robust , and help to capture various linguistic alternatives . With the software QA application in mind , an additional limitation is that their approach does not allow one to freely translate a given description to multiple output languages , which would be useful for comparing how different programming languages represent the same functionality . The model also can not translate between natural languages and programming languages that are not observed during training . While software documentation is easy to find in bulk , if a particular API is not already documented in a language other than English ( e.g. , Haskell in de ) , it is unlikely that such a translation will appear without considerable effort by experienced translators . Similarly , many individual APIs may be too small or poorly documented to build individual models or QA applications , and will in some way need to bootstrap off of more general models or resources . To deal with these issues , we aim to learn more general text - to - code translation models that are trained on multiple datasets simultaneously . Our ultimate goal is to build polyglot translation models ( cf . Johnson et al ( 2016 ) ) , or models with shared representations that can translate any input text to any output programming language , regardless of whether such language pairs were encountered explicitly during training . Inherent in this task is the challenge of building an efficient polyglot decoder , or a translation mechanism that allows such crossing between input and output languages . A key challenge is ensuring that such a decoder generates well - formed code representations , which is not guaranteed when one simply applies standard decoding strategies from SMT and neural MT ( cf . Cheng et al ( 2017 ) ) . Given our ultimate interest in API QA , such a decoder must also facilitate monolingual translation , or being able to translate to specific output languages as needed . To solve the decoding problem , we introduce a new graph - based decoding and representation framework that reduces to solving shortest path problems in directed graphs . We investigate several translation models that work within this framework , including traditional SMT models and models based on neural networks , and report stateof - the - art results on the technical documentation task of Richardson and Kuhn ( 2017b , a ) . To show the applicability of our approach to more conventional SP tasks , we apply our methods to the Geo - Query domain ( Zelle and Mooney , 1996 ) and the Sportscaster corpus ( Chen et al , 2010 ) . These experiments also provide insight into the main technical documentation task and highlight the strengths and weaknesses of the various translation models being investigated .", "entities": [[119, 122, "TaskName", "natural language understanding"], [229, 230, "DatasetName", "followup"], [599, 601, "TaskName", "code translation"]]}
{"text": "Our approach builds on the baseline models introduced in Richardson and Kuhn ( 2017b ) ( see also Deng and Chrupa\u0142a ( 2014 ) ) . Their work is positioned within the broader SP literature , where traditionally SMT ( Wong and Mooney , 2006a ) and parsing ( Zettlemoyer and Collins , 2009 ) methods are used to study the problem of translating text to formal meaning representations , usually centering around QA applications ( Berant et al , 2013 ) . More recently , there has been interest in using neural network approaches either in place of ( Dong and Lapata , 2016 ; Ko\u010disk\u00fd et al , 2016 ) or in combination with ( Misra and Artzi , 2016 ; Jia and Liang , 2016 ; Cheng et al , 2017 ) these traditional models , the latter idea we look at in this paper . Work in NLP on software documentation has accelerated in recent years due in large part to the availability of new data resources through websites such as StackOverflow and Github ( cf . Allamanis et al ( 2017 ) ) . Most of this recent work focuses on processing large amounts of API data in bulk ( Gu et al , 2016 ; Miceli Barone and Sennrich , 2017 ) , either for learning longer executable programs from text ( Yin and Neubig , 2017 ; Rabinovich et al , 2017 ) , or solving the inverse problem of code to text generation ( Iyer et al , 2016 ; . In contrast to our work , these studies do not look explicitly at translating to target APIs , or at non - English documentation . The idea of polyglot modeling has gained some traction in recent years for a variety of problems ( Tsvetkov et al , 2016 ) and has appeared within work in SP under the heading of multilingual SP ( Jie and Lu , 2014 ; Duong et al , 2017 ) . A related topic is learning from multiple knowledge sources or domains ( Herzig and Berant , 2017 ) , which is related to our idea of learning from multiple APIs . When building models that can translate between unobserved language pairs , we use the term zeroshot translation from Johnson et al ( 2016 ) .", "entities": [[249, 251, "TaskName", "text generation"]]}
{"text": "To improve the baseline translation approach used previously ( Section 3.1 ) , we pursue a graph based approach . Given the formulation above and the finiteness of our prediction space C , our approach exploits the fact that we can represent the complete component search space for any set of APIs as a directed acyclic finite - state automaton ( DAFSA ) , such as the one shown graphically in Figure 2 . The underlying graph is constructed by concatenating all of the component representations for each API of interest and applying standard finite - state construction and minimization techniques ( Mohri , 1996 ) . Each path in the resulting compact automaton is therefore a well - formed component representation . Using an idea from Johnson et al ( 2016 ) , we add to each component representation an artificial token that identifies the output programming language or library . For example , the two edges from the initial state 0 in Figure 2 are labeled as 2C and 2Clojure , which identify the C and Clojure programming languages respectively . All paths starting from the right of these edges are therefore valid paths in each respective programming language . The paths starting from the initial state 0 , in contrast , correspond to all valid component representations in all languages . Decoding reduces to the problem of finding a path for a given text input x. For example , given the input the ceiling of a number , we would want to find the paths corresponding to the component translations numeric math ceil arg ( in C ) and algo math ceil x ( in Clojure ) in the graph shown in Figure 2 . Using the trick above , our setup facilitates both monolingual decoding , i.e. , generating components specific to a particular output language ( e.g. , the C language via the path shown in bold ) , and polyglot decoding , i.e. , generating any output language by starting at the initial state 0 ( e.g. , C and Clojure ) . We formulate the decoding problem using a variant of the well - known single source shortest path ( SSSP ) algorithm for directed acyclic graphs ( DAGs ) ( Johnson ( 1977 ) ) . This involves a graph G = ( V , E ) ( nodes V and labeled edges E , see graph in Figure 2 ) , and taking an off - line topological sort of the graph 's vertices . Using a data structure d R | V | ( initialized as | V | , as shown in Figure 2 ) , the standard SSSP algorithm ( which is the forward update variant of the Viterbi algorithm ( Huang , 2008 ) ) works by searching forward through the graph in sorted order and finding for each node v an incoming labeled edge u , with label z , that solves the following recurrence : d ( v ) = min ( u , z ) : ( u , v , z ) E d ( u ) + w ( u , v , z ) ( 2 ) where d ( u ) is shortest path score from a unique source node b to the incoming node u ( computed recursively ) and w ( u , v , z ) is the weight of the particular labeled edge . The weight of the resulting shortest path is commonly taken to be the sum of the path edge weights as given by w , and the output translation is the sequence of labels associated with each edge . This algorithm runs in linear time over the size of the graph 's adjacency matrix ( Adj ) and can be extended to find k SSSPs . In the standard case , a weighting function w is pro - d [ V [ G ] ] , \u03c0 [ V [ G ] ] N il , d [ b ] o 2 : s [ V [ G ] , n ] 0.0 Shortest path sums at each node 3 : for each vertex u \u2265 b V [ G ] in sorted order do 4 : for each vertex and label ( v , z ) Adj [ u ] do 5 : score \u2212log n i pt ( xi | z ) + s [ u , i ] 6 : if d [ v ] > score then 7 : d [ v ] score , \u03c0 [ v ] u 8 : for i in 1 , .. , n do Update scores 9 : s [ v , i ] pt ( xi | z ) + s [ u , i ] 10 : return FINDPATH ( \u03c0 , | V | , b ) vided by assuming a static weighted graph . In our translation context , we replace w with a translation model , which is used to dynamically generate edge weights during the SSSP search for each input x by scoring the translation between x and each edge label z encountered . Given this general framework , many different translation models can be used for scoring . In what follows , we describe two types of decoders based on lexical translation ( or unigram ) and neural sequence models . Technically , each decoding algorithm involves modifying the standard SSSP search procedure by adding an additional data structure s to each node ( see Figure 2 ) , which is used to store information about translations ( e.g. , running lexical translation scores , RNN state information ) associated with particular shortest paths . By using these two very different models , we can get insight into the challenges associated with the technical documentation translation task . As we show in Section 6 , each model achieves varying levels of success when subjected to a wider range of SP tasks , which reveals differences between our task and other SP tasks .", "entities": [[162, 163, "DatasetName", "0"], [209, 210, "DatasetName", "0"], [340, 341, "DatasetName", "0"]]}
{"text": "Our second set of models use neural networks to compute the weighting function in Equation 2 . We use an encoder - decoder model with global attention ( Bahdanau et al , 2014 ; Luong et al , 2015 ) , which has the following two components : Encoder Model The first is an encoder network , which uses a bi - directional recurrent neural network architecture with LSTM units ( Hochreiter and Schmidhuber , 1997 ) to compute a sequence of forward annotations or hidden states ( \u2212 h 1 , ... , \u2212 h | x | ) and a sequence of backward hid - 1 Details about the approx . are provided as supp . material . den states ( \u2212 h , ... , \u2212 h | x | ) for the input sequence ( x 1 , ... , x | x | ) . Standardly , each word is then represented as the concatenation of its forward and backward states : h j = [ \u2212 h j , \u2212 h j ] .", "entities": [[68, 69, "MethodName", "LSTM"]]}
{"text": "Technical API Docs The first dataset includes the Stdlib and Py27 datasets of Richardson and Kuhn ( 2017b , a ) , which are publicly available via Richardson ( 2017 ) . Stdlib consists of short description and function signature pairs for 10 programming languages in 7 languages , and Py27 contains the same type of data for 27 popular Python projects in English mined from Github . We also built new datasets from the Japanese translation of the Python 2.7 standard library , as well as the Lua stdlib documentation in a mixture of Russian , Portuguese , German , Spanish and English . Taken together , these resources consist of 79 , 885 training pairs , and we experiment with training models on Stdlib and Py27 separately as well as together ( shown as + more in Table 1 ) . We use a BPE subword encoding ( Sennrich et al , 2015 ) of both input and output words to make the representations more similar and transliterated all datasets ( excluding Japanese datasets ) to an 8 - bit latin encoding . Graphs were built by concatenating all function representations into a single word list and compiling this list into a minimized DAFSA . For our global polyglot dataset , this resulted in a graph with 218 , 505 nodes , 313 , 288 edges , and 112 , 107 paths or component representations over an output vocabulary of 9 , 324 words .", "entities": [[146, 147, "MethodName", "BPE"]]}
{"text": "We run experiments on the GeoQuery 880 corpus using the splits from Andreas et al ( 2013 ) , which includes geography queries for English , Greek , Thai , and German paired with formal database queries , as well as a seed lexicon or NP list for each language . In addition to training models on each individual dataset , we also learn polyglot models trained on all datasets concatenated together . We also created a new mixed language test set that was built by re - placing NPs in 803 test examples with one or more NPs from a different language using the NP lists mentioned above ( see examples in Figure 4 ) . The goal in the last case is to test our model 's ability to handle mixed language input . We also ran monolingual experiments on the English Sportscaster corpus , which contains human generated soccer commentary paired with symbolic meaning representation produced by a simulation of four games . For GeoQuery graph construction , we built a single graph for all languages by extracting general rule templates from all representations in the dataset , and exploited additional information and patterns using the Geobase database and the semantic grammars used in ( Wong and Mooney , 2006b ) . This resulted in a graph with 2 , 419 nodes , 4 , 936 edges and 39 , 482 paths over an output vocabulary of 164 . For Sportscaster , we directly translated the semantic grammar provided in Chen and Mooney ( 2008 ) to a DAFSA , which resulted in a graph with 98 nodes , 86 edges and 830 paths .", "entities": [[168, 170, "TaskName", "graph construction"]]}
{"text": "Mixed GeoQuery ( de / gr ) Input : Wie hoch liegt der h\u00f6chstgelegene punkt in \u0391\u03bb\u03b1\u03bc\u03c0\u03ac\u03bc\u03b1 ? Logical Form Translation : answer ( elevation 1 ( highest ( place ( loc 2 ( stateid ( ' alabama ' ) ) ) ) ) ) Figure 4 : Examples of zero - shot translation when running in polyglot mode ( 1 - 3 , function representations shown in a conventionalized format ) , and mixed language parsing ( 4 ) . Semantic Parsing Results SP results are summarized in Table 2 . In contrast , the neural models , especially those with biasing and copying , strongly outperform all other models and are competitive with related work . In the GeoQuery case , we compare against two classic grammar - based models , UBL and TreeTrans , as well as a feature rich , neural hybrid tree model ( nHT ) . We also see that the polyglot Geo achieves the best performance , demonstrating that training on multiple datasets helps in this domain as well . In the Sportscaster case we compare against two PCFG learning approaches , where the second model ( wo - PCFG ) involves a grammar with complex wordorder constraints . The advantage of training a polyglot model is shown on the results related to mixed language parsing ( i.e. , the middle set of results ) . Here we compared against the best performing monolingual English model ( Best Mono . Model ) , which does not have a way to deal with multilingual NPs . We also find the neural model to be more robust than the lexical models with reranking . While the lexical models overall perform poorly on both tasks , the weakness of this model is particularly acute in the Sportscaster case . We found that mistakes are largely related to the ordering of arguments , which these lexical ( unigram ) models are blind to . That these models still perform reasonably well on the Geo task shows that such ordering issues are less of a factor in this domain .", "entities": [[20, 21, "TaskName", "Translation"], [81, 83, "TaskName", "Semantic Parsing"]]}
{"text": "Since the time of the Index Thomisticus by father Roberto Busa ( Busa , 1974 ( Busa , - 1980 , which is usually mentioned among the first electronic ( nowadays called \" digital \" ) annotated corpora available , NLP tools for automatic morphological analysis and lemmatisation of a richly inflected language like Latin were needed . Over the last decades , this need was fulfilled by a number of morphological analysers for Latin . Among the most widespread ones are Morpheus ( Crane , 1991 ) , Whitaker 's Words ( http://archives.nd.edu/words.html ) and Lemlat ( Passarotti , 2004 ) . Over the past ten years , such tools have become essential , in light of a number of projects aimed at developing advanced language resources for Latin , like treebanks . 1 The most recent advances in linguistic annotation of Latin treebanks are moving beyond the level of syntax , by performing semantic - based tasks like semantic role labelling and anaphora and ellipsis resolution ( Passarotti , 2014 ) . In particular , in the area of Digital Humanities there is growing interest in Named Entity Recognition ( NER ) , especially for purposes of geographicalbased analysis of texts . NER is a sub - branch of Information Extraction , whose inception goes back to the Sixth Message Understanding Conference ( MUC - 6 ) ( Grishman and Sundheim , 1996 ) . NER aims at recognising and labelling ( multi ) words , as names of people , things , places , etc . Since MUC - 6 , NER has largely expanded , with several applications also on ancient languages ( see , for example , Depauw and Van Beek , 2009 ) . Although Lemlat provides quite a large coverage of the Latin lexicon , its performance is limited by the absence of an Onomasticon in its lexical basis , which would be helpful for tasks like NER . Given that in Latin proper names undergo morphological inflection , in this paper we describe our work of enhancing Lemlat with an Onomasticon . The paper is organised as follows . Section 2 presents the basic features of Lemlat . Section 3 describes our method to enhance Lemlat with an Onomasticon , by detailing the rules for the automatic enhancement and discussing the most problematic kinds of words . Section 4 evaluates the rules and presents one experiment run on four Latin texts . Section 5 is a short conclusion and sketches the future work .", "entities": [[44, 46, "TaskName", "morphological analysis"], [188, 191, "TaskName", "Named Entity Recognition"], [192, 193, "TaskName", "NER"], [204, 205, "TaskName", "NER"], [237, 238, "TaskName", "NER"], [264, 265, "TaskName", "NER"], [324, 325, "TaskName", "NER"], [333, 335, "TaskName", "morphological inflection"]]}
{"text": "The lexical basis of Lemlat results from the collation of three Latin dictionaries ( Georges andGeorges , 1913 - 1918 ; Glare , 1982 ; Gradenwitz , 1904 ) . It counts 40 , 014 lexical entries and 43 , 432 lemmas , as more than one lemma can be included into the same lexical entry . Given an input wordform that is recognised by Lemlat , the tool produces in output the corresponding lemma ( s ) and a number of tags conveying ( a ) the inflectional paradigm of the lemma ( s ) ( e.g. first declension noun ) and ( b ) the morphological features of the input wordform ( e.g. singular nominative ) , as well as the identification number ( ID ) of the lemma ( s ) in the lexical basis of Lemlat . No contextual disambiguation is performed . For instance , receiving in input the wordform abamitae ( \" great - aunt \" ) , Lemlat outputs the corresponding lemma ( abamita , ID : A0019 ) , the tags for its inflectional paradigm ( N1 : first declension noun ) and those for the morphological features of the input wordform ( feminine singular genitive and dative ; feminine plural nominative and vocative ) . The basic component of the lexical look - up table used by Lemlat to analyse input wordforms is the so - called LES ( \" LExical Segment \" ) . The LES is defined as the invariable part of the inflected form ( e.g. abamit for abamit - ae ) . In other words , the LES is the sequence ( or one of the sequences ) of characters that remains the same in the inflectional paradigm of a lemma ( hence , the LES does not necessarily correspond to the word stem ) . Lemlat includes a LES archive , in which LES are assigned an ID and a number of inflectional features among which are a tag for the gender of the lemma ( for nouns only ) and a code ( called CODLES ) for its inflectional category . According to the CODLES , the LES is compatible with the endings of its inflectional paradigm . For instance , the CODLES for the LES abamit is N1 ( first declension nouns ) and its gender is F ( feminine ) . The wordform abamitae is thus analysed as belonging to the LES abamit because the segmentae is recognised as an ending compatible with a LES with CODLES N1 .", "entities": [[47, 48, "DatasetName", "lemma"], [74, 75, "DatasetName", "lemma"], [92, 93, "DatasetName", "lemma"], [130, 131, "DatasetName", "lemma"], [168, 169, "DatasetName", "lemma"], [262, 263, "MethodName", "ae"], [293, 294, "DatasetName", "lemma"], [338, 339, "DatasetName", "lemma"]]}
{"text": "The bedrock of our work is Busa 's ( 1988 ) Totius Latinitatis Lemmata , which contains the list of the lemmas ( 92 , 052 ) from the 5 th edition of Lexicon Totius Latinitatis ( Forcellini , 1940 ) . In Busa ( 1988 ) , three kinds of metadata are assigned to each lemma : ( a ) a code for the section of the dictionary in which the lemma occurs ( e.g. ON : the lemma occurs in the Onomasticon ) , ( b ) a code for the inflectional paradigm the lemma belongs to and its gender ( e.g. BM : second declension masculine nouns ) and ( c ) the number of lines of the lexical entry for the lemma in Forcellini . In order to enhance Lemlat with Forcellini 's Onomasticon , we first extracted from Busa ( 1988 ) the list of those lemmas that occur in the ON section . This list counts 28 , 178 lemmas . Then , we built a number of rules to automatically include the lemmas of the Onomasticon into the lexical basis of Lemlat .", "entities": [[56, 57, "DatasetName", "lemma"], [72, 73, "DatasetName", "lemma"], [79, 80, "DatasetName", "lemma"], [96, 97, "DatasetName", "lemma"], [125, 126, "DatasetName", "lemma"]]}
{"text": "Including the Onomasticon of Forcellini into Lemlat means converting the list of proper names provided by Busa ( 1988 ) into the same format of the LES archive . In order to perform this task as automatically as possible , we built a number of rules to extract the relevant information for each lemma in the list , namely its LES , CODLES and gender . By exploiting the morphological tagging of Busa ( 1988 ) , which groups sets of lemmas showing common inflectional features , our rules treat automatically such inflectionally regular groups . In total , we wrote 122 rules , which fall into four types . The first type ( 60 rules ) builds the LES by removing one or more characters from the right side of the lemma . Such a removal is constrained by the code for the inflectional paradigm of the lemma , which is then used to create both the CODLES and the tag for the gender . For instance , the lemma marcus ( \" Mark \" ) is assigned the inflectional paradigm BM in Busa ( 1988 ) . One rule states that the LES for BM lemmas ending in - us is built by removing the last two characters from the lemma ( marcus > marc ) The inflectional code BM stands for second declension ( B ) masculine ( M ) nouns : this is converted into the CODLES of Lemlat for second declension nouns ( B > N2 ) and into the tag for masculine gender ( M > m ) . The second type of rules ( 19 ) adds one or more characters on the right side of the lemma to build the LES . Again , this is done according both to the inflectional paradigm and to the ending of the lemma in Busa ( 1988 ) . For instance , the LES for lemmas with inflectional code CM ( third declension masculine nouns ) and ending in - o is built by adding an - n after the last character . One example is the lemma bappo ( \" Bappo \" ) , whose LES is bappon , as third declension imparisyllable nouns are analysed by Lemlat by using the basis for their singular genitive ( bappon - is ) . The third type of rules ( 19 ) replaces one or more characters on the right side of the lemma with others . For instance , the LES of clemens ( \" Clement \" , third declension masculine noun ending in - s , with singular genitive clement - is ) is built by replacing the final - s with a - t ( clement ) . The last type of rules ( 24 ) deals with those lemmas that are equal to their LES ( no change is needed ) . These are uninflected nouns , ( like hamilcar - \" Hamilcar \" ) , which can be easily retrieved because they are assigned a specific inflectional code in Busa ( 1988 ) .", "entities": [[53, 54, "DatasetName", "lemma"], [69, 71, "TaskName", "morphological tagging"], [132, 133, "DatasetName", "lemma"], [148, 149, "DatasetName", "lemma"], [170, 171, "DatasetName", "lemma"], [212, 213, "DatasetName", "lemma"], [284, 285, "DatasetName", "lemma"], [307, 308, "DatasetName", "lemma"], [352, 353, "DatasetName", "lemma"], [407, 408, "DatasetName", "lemma"]]}
{"text": "Not all inflectional paradigms are as much regular as to allow for a fully automatic rule - based treatment . For instance , third declension feminine nouns represent an entangled class . The lemma charybdis , - is ( \" Charybdis \" ) is a third declension parisyllable feminine noun ending in - is . Instead , phegis , - gidis ( \" daughter of Phegeus \" ) is a third declension imparisyllable feminine noun ending in - is . One common rule can not be used for these two kinds of words . We overcome such problem by building two more specific rules : one accounting for third declension feminine nouns ending in - dis and one for third declension feminine nouns ending in - gis . However , there are sub - groups of nouns for which such a solution does not work , like third declension feminine nouns ending in - mis , which can be both imparisyllable nouns ( e.g. salamis , - minis , \" Salamis \" ) and parisyllable nouns ( e.g. tomis , - is , \" Tomis \" ) . For these lemmas we checked manually their inflection in Forcellini and assigned LES and CODLES accordingly . Another group of tricky words includes those lemmas that show two ( or even more ) different inflectional paradigms . For instance , apollonides ( \" Apollonides \" ) shows both a singular genitive of the second declension ( in - i ) and one of the first declension ( in - ae ) . We treated these cases manually by checking their lexical entries in Forcellini . A further problem is represented by graphical variants , which are managed by Lemlat through so - called \" exceptional forms \" . These are wordforms that are hard - coded in the LES archive and are assigned the same ID of the LES used to build their base lemma . For instance , the nominative singular of the lemma jesus ( \" Jesus \" ) is attested also as hiesus , ihesus and zesus . Beside the LES jes ( used for the base lemma jesus ) , in the LES archive also the wordforms hiesus , ihesus and zesus are recorded and assigned the same ID of the LES jes .", "entities": [[33, 34, "DatasetName", "lemma"], [257, 258, "MethodName", "ae"], [322, 323, "DatasetName", "lemma"], [332, 333, "DatasetName", "lemma"], [336, 337, "TaskName", "Jesus"], [358, 359, "DatasetName", "lemma"]]}
{"text": "We evaluated the enhancement of Lemlat with the Onomasticon of Forcellini in two steps . First , we focused on the accuracy of the rules for automatic enhancement . Then , we compared the new version of Lemlat with the previous one by the lexical coverage they provide for four Latin texts .", "entities": [[21, 22, "MetricName", "accuracy"]]}
{"text": "We evaluated the enhancement of Lemlat with the Onomasticon of Forcellini by comparing the lexical coverage provided by the two versions of the tool for four Latin texts of similar size and different genre ( prose and poetry ) and era ( Classical and Late Latin ) . 3 The coverage of Lemlat on the four test texts improved of 4.86 % on average after the enhancement with Forcellini 's Onomasticon . The highest improvement is on Virgil ( +5.7 % ) . Most of the words not analysed by LemlatON are graphical variants ( e.g. cre\u00fcsa for creusa - \" Creusa \" ) or part of the inflectional paradigm of lemmas not available in its lexical basis . Beside these words , there are Roman numbers ( e.g. XV , \" fifteen \" ) , abbreviations ( e.g. kal for kalendae , \" calends \" ) and foreign words ( e.g. \u00b5\u03b7\u03c4\u03ad\u03c1\u03b1 , \" mother \" ) . 4 Table 3 shows the results by category of unknown words ( types ) . Roman numbers are frequent in Caesar 's text ( 1 ) . The fact that Lemlat does not analyse Roman numbers is not a major concern , as their form is regular , easily predictable and interpretable . Only a few of them can raise ambiguity when written lowercase . For instance , vi ( \" six \" ) is homograph with the singular ablative of the third declension noun vis ( \" power \" ) . Homography can hold also between items of of the Onomasticon and the original lexical basis of Lemlat . For instance , the lemma augustus occurs both in the original Lemlat ( a first class adjective , \" solemn \" ) and in the Onomasticon ( a proper name , \" Augustus \" ) . If we look at tokens instead of types , coverage rates remain quite similar , as it is shown by Table 4 It is worth noting that , while the text of Virgil shows the highest improvement in type - based evaluation ( +5.7 % ) , Caesar 's De Bello Gallico is the one that mostly benefits from the extension of Lemlat with the Onomasticon in token - based evaluation ( +6.64 % ) . This is due to the higher number of occurrences of proper names in Caesar than in Virgil . Indeed , although the number of new word types analysed by LemlatON in comparison to Lemlat is lower for Caesar than for Virgil , the opposite holds when tokens are concerned . 6 In more detail , the average number of occurrences ( tokens ) of the new word types analysed by LemlatON for Caesar is 3.59 ( 542/151 ) , while it is 1.71 for Virgil ( 493/288 ) .", "entities": [[273, 274, "DatasetName", "lemma"]]}
{"text": "A prevailing paradigm in neural text generation is one - shot generation , where text is produced in a single step . The one - shot setting is inadequate , however , when the constraints the user wishes to impose on the generated text are dynamic , especially when authoring longer documents . We address this limitation with an interactive text generation setting in which the user interacts with the system by issuing commands to edit existing text . To this end , we propose a novel text editing task , and introduce WikiDocEdits , a dataset of singlesentence edits extracted from Wikipedia revision histories . We show that our Interactive Editor , a transformer - based model trained on this dataset , outperforms baselines and obtains positive results in both automatic and human evaluations . We present empirical and qualitative analyses of this model 's performance . 1", "entities": [[5, 7, "TaskName", "text generation"], [60, 62, "TaskName", "text generation"], [93, 94, "DatasetName", "WikiDocEdits"]]}
{"text": "A long - standing goal of natural language processing research has been to generate long - form text ( Lebowitz , 1985 ; Rashkin et al , 2020 ) . Recent large generative language models such as GPT - 2 ( Radford et al , 2019 ) , and GPT - 3 ( Brown et al , 2020 ) , demonstrate an impressive ability to generate fluent text , but their outputs are difficult to control beyond a prompt , and they manifest a tendency to hallucinate facts ( Wiseman et al , 2017 ) . Much recent work has thus focused on making such models more controllable ( Keskar et al , 2019 ; Hu et al , 2017 ; Zhang et al , 2020 ; Dathathri et al , 2019 ) , and factually grounded ( Guu et al , 2020 ; Liu et al , 2018b ) . * Work done at Microsoft Research . 1 All our code ( including code to recreate our data ) and pre - trained models will be made available at : http://microsoft.com/research/project/ interactive - document - generation Most such work only considers a one - shot generation setting . Given a set of inputs , which may be a prompt , a control code ( Keskar et al , 2019 ) , or a table of data ( Liu et al , 2018b ) for example , the system generates text in a single step . Humans , though , often produce text through an evolutionary process involving multiple draft - edit cycles . This is not simply because they make mistakes when writing , but because they may require multiple iterations to help them shape and even make sense of what they want to express ( Pirolli and Card , 2005 ) . For example , consider a user writing an article about Barack Obama . They might start with a simple sentence such as \" Barack Obama was the 44th President of the United States \" . Next , they may wish to expand on that sentence , adding information , or rephrasing it to integrate it better with the text . Replicating this process in software will mean allowing users to adjust their requirements in response to model outputs . Even an error - free system that meets all of a user 's initial requirements does not obviate the need for iteration , since those constraints are themselves dynamic . While this work focuses on text , we also note that these arguments extend to other settings where a system must generate a complex , structured object for a user , such as image or code generation . The purpose of this paper is to bring into view the task of controllable text editing , as a step beyond one - shot generation towards interactive document generation . A full interactive document generation system will likely comprise multiple components , possibly including one - shot generation to create a first draft . Editing is crucial to interactivity because it allows users to change previously generated text to fit their dynamic constraints . This is a stateful operation , where the state is the current version of the document , as opposed to stateless recasting of text from scratch using a one - shot model . While services like Grammarly or MS Word already offer rewriting suggestions , they mainly focus on syntactic or stylistic edits such as paraphrases ( Gupta et al , 2018 ) . In this work , we are interested in a broader range of edits , particularly those that add or remove content , or change the meaning of text . Figure 1 illustrates this editing setting with an example from our trained model , where a user produces a sentence about Barack Obama over multiple edits . In sum , we make the following contributions : We introduce a challenging new text editing task , wherein a model must learn to edit text in response to a user command , while drawing on grounding to avoid problems of hallucination ( Wiseman et al , 2017 ) . To accompany this task , we release an open - source dataset of sentence - level edits extracted from Wikipedia , including editor comments , which we leverage as natural language commands , together with pre - retrieved grounding documents . We show that a transformer - based editing model trained on our data outperforms \" parrot \" and GPT - 2 baselines , and obtains competitive results compared to gold - standard edits in human evaluations . We then perform an empirical analysis of our model 's performance , showing the importance of the command and grounding , and the varying difficulty of edits in our dataset .", "entities": [[37, 38, "MethodName", "GPT"], [49, 50, "MethodName", "GPT"], [448, 450, "TaskName", "code generation"], [751, 752, "MethodName", "parrot"], [754, 755, "MethodName", "GPT"]]}
{"text": "We now formalize our text editing task . Let D be a document , q a user command 2 , and G some appropriate form of grounding . Moreover , let D be an edited version of D. Then our task is , given a dataset of edits D = { ( D 0 , q 0 , G 0 , D 0 ) , ... , ( D N , q N , G N , D N ) } , learn to produce document D , given D , q , and G. Note that while previous work on text editing usually only considers D as input , we include both a form of control q and grounding G. The command is needed because otherwise the type of edit to be made is undefined , while the grounding provides external knowledge needed to make an edit . In our specific instance of this task , we will only consider sentence - level edits . More formally , we consider edits D \u2212 D , where D and D differ only on a single sentence s D , respectively s D . While , in general , edits can vary in complexity from document - level to character - level changes , sentences are a natural way to break down text into relatively independent units of meaning , so it makes sense to edit text one sentence at a time . More complex , document - level edits can be seen as a composition of multiple sentence - level edits . Additionally , we will consider user commands q written in natural language , e.g. , \" add years in office \" . The command could also take other forms , such as a categorical variable , but natural language allows for the greatest flexibility in specifying what the edit should accomplish . Moreover , natural language commands are a good fit for our model , which we will initialize with pretrained language model weights . For similar reasons , we will also consider corpora of text snippets as our grounding G. Alternatively , the grounding could also consist of structured data such as tables or graphs . In a real user scenario , this grounding might be supplied by the user , or retrieved on the fly . For our dataset , we pre - retrieve groundings by querying a commercial search engine .", "entities": [[53, 54, "DatasetName", "0"], [56, 57, "DatasetName", "0"], [59, 60, "DatasetName", "0"], [62, 63, "DatasetName", "0"]]}
{"text": "We now provide an overview of our dataset . From 667 dump files in the February 1 st 2020 dump of Wikipedia , we extract 11 , 850 , 786 edits , and take a 1 % sample of 118 , 818 edits to run our analyses . Table 1 presents summary statistics for our data , and in the following , we break down the edits by edit type , and present some examples . See also appendix D for an analysis of the quality of the retrieved grounding . Fluency and Content Edits We are interested in the distribution of different edit types within our dataset . In particular , we want to distinguish between fluency edits , which only affect the grammar or structure of a sentence , and content edits , which change the meaning of a sentence . We can lean on previous work to categorize edits on Wikipedia . create 13 edit intention categories , and train a classifier to label revisions according to the categories . We apply their classifier to our data , and group their 13 categories into \" fluency \" , \" content \" , or \" other \" edits , as reported in table 2 . With the caveat that the edits were labelled automatically using a trained classifier , we see that , while fluency edits make up the majority of the edits in our data , a large proportion are content edits . Examples Table 3 presents some examples from our data . These were chosen to illustrate a variety of edits . The first example shows an elaboration edit , appending new information to the end of a sentence . The second example is a simple typo fix , while the third is changing a fact . Finally , the last example is a more complex edit to reword a sentence . We can see that there is a large variety of edits in our dataset . See T5 Encoder < bos > 0 \u2032 1 \u2032 T5 Decoder 0 \u2032 1 \u2032 2 \u2032 \u2026 \u2026", "entities": [[332, 333, "MethodName", "T5"], [337, 338, "DatasetName", "0"], [341, 342, "MethodName", "T5"], [343, 344, "DatasetName", "0"]]}
{"text": "We conducted two rounds of human evaluations , each time across 200 examples from our test set . Annotators were crowd sourced , and each example was rated by seven judges for a total of 1400 judgements . 8 Command and Grounding In our first round of human evaluations we compared our model 's top output from beam search to the reference edit . There were two tasks . In the first task , we asked judges to choose which system better accomplished the command q. In the second , we asked which system was more faithful to the grounding G. 8 : Human Evaluation : comparisons between absolute evaluations of different settings . Raters were asked whether edits were satisfactory . 0 corresponds to strong disagreement , and 5 to strong agreement . Systems are given by model ( full or with the comment ablated ) , and whether the command was shown to the raters ( + or - ) . Bolded numbers indicate significant difference with p < 0.0125 . than the reference . 9 In the grounding task , Interactive Editor demonstrates good correspondence with the background material . 10 Judges were further asked whether the retrieved grounding was relevant to the context D : 92.86 % of judgments recorded the grounding as either \" Somewhat relevant \" or \" Very relevant \" .", "entities": [[122, 123, "DatasetName", "0"]]}
{"text": "Grounded Generation Large language models can generate fluent text ( Radford et al , 2019 ; Brown et al , 2020 ; Raffel et al , 2020 ) , but they have a tendency to hallucinate facts ( Wiseman et al , 2017 ) . Thus , several works have explored using various forms of grounding to enable models to generate factually consistent texts ( Koncel - Kedziorski et al , 2019 ; Liu et al , 2018b ; Prabhumoye et al , 2019 ; Liu et al , 2018a ; Guu et al , 2020 ) . Our work uses grounding to ensure that edits are factually correct , although our task differs from previous work because of the user command , which requires specific information to be retrieved from the grounding during generation . Controllable Generation While grounding can be seen as a way to implicitly control the contents of generated text , other works have explored more explicit forms of control . Hokamp and Liu ( 2017 ) and Zhang et al ( 2020 ) use lexical constraints , while Keskar et al ( 2019 ) and Dathathri et al ( 2019 ) control higher level attributes of text , such as style , tone , or topic . Our task instead uses natural language commands , which can flexibly express different types of constraints , ranging from low - level lexical ones , to high - level topical ones . In this sense , we can also draw the parallel to dialog response generation Dinan et al , 2018 ) , task - oriented dialog , or open domain question answering ( Min et al , 2019 ; Chen et al , 2017 ) , that also involve user responses or queries , although these tasks are not concerned with text generation in the context of document creation .", "entities": [[256, 258, "TaskName", "response generation"], [273, 275, "TaskName", "question answering"], [304, 306, "TaskName", "text generation"]]}
{"text": "The task of Document Generation considered in our work bears similarity with work on generating long - form narratives ( Jain et al , 2017 ) . While earlier work in Story Generation focused more on plan - based architectures ( Lebowitz , 1985 ) , more recent work moved towards end - to - end approaches allowing generation to be unconstrained and creative . As narratives are often aimed at particular goals expressed in terms of outlines and plans , much of the literature in Story Generation is framed as a form of controllable generation , using storylines ( Peng et al , 2018 ) , events ( Martin et al , 2017 , plot words or word skeletons ( Xu et al , 2018 ; Ippolito et al , 2019 ) , plans ( Yao et al , 2019 ) , story ending ( Tambwekar et al , 2019 ) , and outlines ( Rashkin et al , 2020 ) as various forms of constraints . Our work takes a significantly different approach , as we treat document or story generation as an iterative process that allows a human to generate a full document from scratch , but also allows constraints to be more dynamic ( e.g. , add nationality in Table 9 only if the system missed that the first time ) . Text Editing Several previous works have focused on text editing . Guu et al ( 2018 ) generate sentences by editing prototypes taken from their training corpus , although they use editing only as a means for language modeling . expand upon Guu et al ( 2018 ) 's setting , but for dialog . More related to our own setting , Faruqui et al ( 2018 ) propose WikiAtomicEdits , a dataset of edits crawled from Wikipedia . However , they consider a much narrower definition of edits than our data does . Yin et al ( 2018 ) use WikiAtomicEdits and propose the task of learning to represent edits , which Marrese - Taylor et al ( 2020 ) expand using a variational approach . In contrast , we are more interested in generating edits rather than repre - senting them . Related to Wikipedia data , Pryzant et al ( 2020 ) also used Wikipedia revision histories to learn to debias text , whereas we considered general edits . Iso et al ( 2020 ) propose a factbased text editing task , but they do not consider control or other types of edits . Another related task to text editing is text paraphrasing ( Gupta et al , 2018 ) , however paraphrasing usually conserves the meaning of a sentence . While the edits we consider include meaning - preserving edits , we are mostly interested in edits that affect meaning .", "entities": [[31, 33, "TaskName", "Story Generation"], [86, 88, "TaskName", "Story Generation"], [181, 183, "TaskName", "story generation"], [295, 296, "DatasetName", "WikiAtomicEdits"], [327, 328, "DatasetName", "WikiAtomicEdits"]]}
{"text": "In this work we argued that text generation should be interactive , and , as a means towards that end , we proposed a general text editing task , where a system must edit a document in response to a user command . In our specific instance of the task we considered single - sentence edits , and we crawled a dataset of several million edits from Wikipedia that included commands , in the form of editor comments , as well as grounding documents . We then showed that training a transformer - based model on our data , while initializing with pretrained language model weights , yields encouraging results on both automatic and human evaluations . Additionally , our ablation studies showed the crucial role played by the user command and grounding . Breaking down our results by types of edits , we saw that our model not only performs well on easier fluency edits , but also on much harder content edits . Finally , we discussed future research directions for interactive document generation , as well as possible extensions to other domains such as images or code . found that this helps remedy the shortfalls of the markup removal step , since it often leaves behind markup symbols . While there may be valid sentences that use markup punctuation , we do not expect them to make up a significant part of the data , nor do we expect them to be significantly different from regular sentences , except for their use of unusual punctuation .", "entities": [[6, 8, "TaskName", "text generation"]]}
{"text": "We are also interested in knowing how well edits in the data are covered by the inputs ( i.e. D , s , q , or G ) , where an edit is well covered if the information necessary to produce the edit appears somewhere in the inputs . To measure coverage we use word recall : how many words that were inserted in an edit also appear in the grounding ? However , because simple recall fails to account for synonyms , or the context in which words appear , we use the BERTScore ( Zhang et al , 2019a ) recall . This allows for fuzzy matching between BERT embeddings instead of requiring exact word matches . We also use idf scores to weigh words , since we are mostly interested in covering rare words , which are more likely to be meaning - carrying . We can define the BERT recall , R BERT , for a sentence edit The BERT embeddings used to compute R BERT were produced using a pretrained BERT base model . The idf weights were computed from a sample of 500 , 000 Wikipedia pages . In all rows , the considered corpus C corresponds to the grounding . s \u2212 s , with respect to some text corpus C as w s \\s idf ( w ) max w C BERT ( w ) T BERT ( w ) w s \\s idf ( w ) , where s \\s = { w s | w / s } , and idf ( w ) are the inverse document frequency scores computed on a random sample of 500 K Wikipedia pages . Table 13 reports the coverage statistics for our subsample of the data . We used an uncased BERT base model to compute the embeddings . The first row reports the coverage of the target by all of the inputs , namely the command , grounding , context , and source sentence . The second row shows the coverage by the grounding alone . Note that , even with just the grounding , coverage is already fairly high . Finally , the last row presents the coverage by the command alone , which shows that it also provides grounding .", "entities": [[110, 111, "MethodName", "BERT"], [152, 153, "MethodName", "BERT"], [156, 157, "MethodName", "BERT"], [163, 164, "MethodName", "BERT"], [169, 170, "MethodName", "BERT"], [175, 176, "MethodName", "BERT"], [229, 230, "MethodName", "BERT"], [234, 235, "MethodName", "BERT"], [298, 299, "MethodName", "BERT"]]}
{"text": "Neural machine translation ( NMT ) has achieved impressive performance recently by using large - scale parallel corpora . However , it struggles in the low - resource and morphologically - rich scenarios of agglutinative language translation task . Inspired by the finding that monolingual data can greatly improve the NMT performance , we propose a multi - task neural model that jointly learns to perform bi - directional translation and agglutinative language stemming . Our approach employs the shared encoder and decoder to train a single model without changing the standard NMT architecture but instead adding a token before each source - side sentence to specify the desired target outputs of the two different tasks . Experimental results on Turkish - English and Uyghur - Chinese show that our proposed approach can significantly improve the translation performance on agglutinative languages by using a small amount of monolingual data .", "entities": [[1, 3, "TaskName", "machine translation"], [39, 40, "DatasetName", "Inspired"]]}
{"text": "Neural machine translation ( NMT ) has achieved impressive performance on many high - resource machine translation tasks ( Bahdanau et al , 2015 ; Luong et al , 2015a ; Vaswani et al , 2017 ) . The standard NMT model uses the encoder to map the source sentence to a continuous representation vector , and then it feeds the resulting vector to the decoder to produce the target sentence . However , the NMT model still suffers from the low - resource and morphologically - rich scenarios of agglutinative language translation tasks , such as Turkish - English and Uyghur - Chinese . Both Turkish and Uyghur are agglutinative languages with complex morphology . The morpheme structure of the word can be denoted as : prefix1 + \u2026 + prefixN + stem + suffix1 + \u2026 + suffixN ( Ablimit et al , 2010 ) . Since the suffixes have many inflected and morphological variants , the vocabulary size of an agglutinative language is considerable even in small - scale training data . Moreover , many words have different morphemes and meanings in different context , which leads to inaccurate translation results . Recently , researchers show their great interest in utilizing monolingual data to further improve the NMT model performance ( Cheng et al , 2016 ; Ramachandran et al , 2017 ; Currey et al , 2017 ) . Sennrich et al ( 2016 ) pair the target - side monolingual data with automatic back - translation as additional training data to train the NMT model . Zhang and Zong ( 2016 ) use the source - side monolingual data and employ the multi - task learning framework for translation and source sentence reordering . modify the decoder to enable multi - task learning for translation and language modeling . However , the above works mainly focus on boosting the translation fluency , and lack the consideration of morphological and linguistic knowledge . Stemming is a morphological analysis method , which is widely used for information retrieval tasks ( Kishida , 2005 ) . By removing the suffixes in the word , stemming allows the variants of the same word to share representations and reduces data sparseness . We consider that stemming can lead to better generalization on agglutinative languages , which helps NMT to capture the in - depth semantic information . Thus we use stemming as an auxiliary task for agglutinative language translation . In this paper , we investigate a method to exploit the monolingual data of the agglutinative language to enhance the representation ability of the encoder . This is achieved by training a multi - task neural model to jointly perform bi - directional translation and agglutinative language stemming , which utilizes the shared encoder and decoder . We treat stemming as a sequence generation task . Figure 1 : The architecture of the multi - task neural model that jointly learns to perform bi - directional translation between Turkish and English , and stemming for Turkish sentence .", "entities": [[1, 3, "TaskName", "machine translation"], [15, 17, "TaskName", "machine translation"], [277, 281, "TaskName", "multi - task learning"], [294, 298, "TaskName", "multi - task learning"], [330, 332, "TaskName", "morphological analysis"], [339, 341, "TaskName", "information retrieval"]]}
{"text": "Multi - task learning ( MTL ) aims to improve the generalization performance of a main task by using the other related tasks , which has been successfully applied to various research fields ranging from language ( Liu et al , 2015 ; Luong et al , 2015a ) , vision ( Yim et al , 2015 ; Misra et al , 2016 ) , and speech ( Chen and Mak , 2015 ; Kim et al , 2016 ) . Many natural language processing ( NLP ) tasks have been chosen as auxiliary task to deal with the increasingly complex tasks . Luong et al ( 2015b ) employ a small amount of data of syntactic parsing and image caption for English - German translation . Hashimoto et al ( 2017 ) present a joint MTL model to handle the tasks of part - of - speech ( POS ) tagging , dependency parsing , semantic relatedness , and textual entailment for English . Kiperwasser and Ballesteros ( 2018 ) utilize the POS tagging and dependency parsing for English - German machine translation . To the best of our knowledge , we are the first to incorporate stemming task into MTL framework to further improve the translation performance on agglutinative languages . Recently , several works have combined the MTL method with sequence - to - sequence NMT model for machine translation tasks . Dong et al ( 2015 ) follow a one - to - many setting that utilizes a shared encoder for all the source languages with respective attention mechanisms and multiple decoders for the different target languages . Luong et al ( 2015b ) follow a many - to - many setting that uses multiple encoders and decoders with two separate unsupervised objective functions . Zoph and Knight ( 2016 ) follow a many - to - one setting that employs multiple encoders for all the source languages and one decoder for the desired target language . Johnson et al ( 2017 ) propose a more simple method in one - to - one setting , which trains a single NMT model with the shared encoder and decoder in order to enable multilingual translation . The method requires no changes to the standard NMT architecture but instead requires adding a token at the beginning of each source sentence to specify the desired target sentence . Inspired by their work , we employ the standard NMT model with one encoder and one decoder for parameter sharing and model generalization . In addition , we build a joint vocabulary on the concatenation of the source - side and target - side words . Several works on morphologically - rich NMT have focused on using morphological analysis to pre - process the training data ( Luong et al , 2016 ; Huck et al , 2017 ; Tawfik et al , 2019 ) . Gulcehre et al ( 2015 ) segment each Turkish sentence into a sequence of morpheme units and remove any nonsurface morphemes for Turkish - English translation . Ataman et al ( 2017 ) propose a vocabulary reduction method that considers the morphological properties of the agglutinative language , which is based on the unsupervised morphology learning . This work takes inspiration from our previously proposed segmentation method ( Pan et al , 2020 ) that segments the word into a sequence of subword units with morpheme structure , which can effectively reduce language complexity .", "entities": [[0, 4, "TaskName", "Multi - task learning"], [143, 146, "DatasetName", "part - of"], [153, 155, "TaskName", "dependency parsing"], [176, 178, "TaskName", "dependency parsing"], [182, 184, "TaskName", "machine translation"], [231, 233, "TaskName", "machine translation"], [399, 400, "DatasetName", "Inspired"], [456, 458, "TaskName", "morphological analysis"]]}
{"text": "We propose a multi - task neural model for machine translation from and into a low - resource and morphologically - rich agglutinative language . We train the model to jointly learn to perform both the bi - directional translation task and the stemming task on an agglutinative language by using the standard NMT framework . Moreover , we add an artificial token before each source sentence to specify the desired target outputs for different tasks . The architecture of the proposed model is shown in Figure 1 . We take the Turkish - English translation task as example . The \" < MT > \" token denotes the bilingual translation task and the \" < ST > \" token denotes the stemming task on Turkish sentence .", "entities": [[9, 11, "TaskName", "machine translation"]]}
{"text": "Our proposed multi - task neural model on using the source - side monolingual data for agglutinative language translation task can be applied in any NMT structures with encoder - decoder framework . In this work , we follow the NMT model proposed by Vaswani et al ( 2017 ) , which is implemented as Transformer . We will briefly summarize it here . Firstly , the Transformer model maps the source sequence = ( 1 , \u2026 , ) and the target sentence = ( 1 , \u2026 , ) into a word embedding matrix , respectively . Secondly , in order to make use of the word order in the sequence , the above word embedding matrices sum with their positional encoding matrices to generate the source - side and target - side positional embedding matrices . The encoder is composed of a stack of N identical layers . Each layer has two sub - layers consisting of the multi - head self - attention and the fully connected feed - forward network , which maps the source - side positional embedding matrix into a representation vector . The decoder is also composed of a stack of N identical layers . Each layer has three sub - layers : the multi - head self - attention , the multi - head attention , and the fully connected feed - forward network . The multi - head attention attends to the outputs of the encoder and decoder to generate a context vector . The feed - forward network followed by a linear layer maps the context vector into a vector with the original space dimension . Finally , the softmax function is applied on the vector to predict the target word sequence .", "entities": [[55, 56, "MethodName", "Transformer"], [67, 68, "MethodName", "Transformer"], [220, 224, "MethodName", "multi - head attention"], [235, 239, "MethodName", "multi - head attention"], [262, 264, "MethodName", "linear layer"], [280, 281, "MethodName", "softmax"]]}
{"text": "The statistics of the training , validation , and test datasets on Turkish - English and Uyghur - Chinese machine translation tasks are shown in Table 1 . For the Turkish - English machine translation , following ( Sennrich et al , 2015a ) , we use the WIT corpus ( Cettolo et al , 2012 ) and the SETimes corpus ( Tyers and Alperen , 2010 ) as the training dataset , merge the dev2010 and tst2010 as the validation dataset , and use tst2011 , tst2012 , tst2013 , tst2014 from the IWSLT as the test datasets . We also use the talks data from the IWSLT evaluation campaign 1 in 2018 and the news data from News Crawl corpora 2 in 2017 as external monolingual data for the stemming task on Turkish sentences . For the Uyghur - Chinese machine translation , we use the news data from the China Workshop on Machine Translation in 2017 ( CWMT2017 ) as the training dataset and validation dataset , use the news data from CWMT2015 as the test dataset . Each Uyghur sentence has four Chinese reference sentences . Moreover , we use the news data from the Tianshan website 3 as external monolingual data for the stemming task on Uyghur sentences .", "entities": [[19, 21, "TaskName", "machine translation"], [33, 35, "TaskName", "machine translation"], [48, 49, "DatasetName", "WIT"], [80, 82, "DatasetName", "validation dataset"], [142, 144, "TaskName", "machine translation"], [155, 157, "TaskName", "Machine Translation"], [167, 169, "DatasetName", "validation dataset"]]}
{"text": "We normalize and tokenize the experimental data . We utilize the jieba toolkit 4 to segment the Chinese sentences , we utilize the Zemberek toolkit 5 with morphological disambiguation ( Sak et al , 2007 ) and the morphological analysis tool ( Tursun et al , 2016 ) to annotate the morpheme structure of the words in Turkish and Uyghur , respectively . We use our previously proposed morphological segmentation method ( Pan et al , 2020 ) , which segments the word into smaller subword units with morpheme structure . Since Turkish and Uyghur only have a few prefixes , we combine the prefixes with stem into the stem unit . As shown in Figure 2 , the morpheme structure of the Turkish word \" hecelerini \" ( syllables ) is : hece + lerini . Then the byte pair encoding ( BPE ) technique ( Sennrich et al , 2015b ) is applied on the stem unit \" hece \" to segment it into \" he@@ \" and \" ce@@ \" . Thus the Turkish word is segmented into a sequence of subword units : he@@ + ce@@ + lerini .", "entities": [[27, 29, "TaskName", "morphological disambiguation"], [38, 40, "TaskName", "morphological analysis"], [139, 142, "MethodName", "byte pair encoding"], [143, 144, "MethodName", "BPE"]]}
{"text": "Training Sentence Samples En - Tr Translation < MT > We go through initiation rit@@ es . Ba\u015fla@@ ma rit\u00fcel@@ lerini ya\u015f@@ \u0131yoruz . Tr - En Translation < MT > Ba\u015fla@@ ma rit\u00fcel@@ lerini ya\u015f@@ \u0131yoruz . We go through initiation rit@@ es . Turkish Stemming < ST > Ba\u015fla@@ ma rit\u00fcel@@ lerini ya\u015f@@ \u0131yoruz . Ba\u015fla@@ rit\u00fcel@@ ya\u015f@@ In this paper , we utilize the above morphological segmentation method for our experiments by applying BPE on the stem units with 15 K merge operations for the Turkish words and 10 K merge operations for the Uyghur words . The standard NMT model trained on this experimental data is denoted as \" baseline NMT model \" . Moreover , we employ BPE to segment the words in English and Chinese by learning separate vocabulary with 32 K merge operations . Table 2 shows the training sentence samples for multi - task neural model on Turkish - English machine translation task . In addition , to certify the effectiveness of the morphological segmentation method , we employ the pure BPE to segment the words in Turkish and Uyghur by learning a separate vocabulary with 36 K and 38 K merge operations , respectively . The standard NMT model trained on this experimental data is denoted as \" general NMT model \" . Table 3 shows the detailed statistics of using different word segmentation methods on Turkish , English , Uyghur , and Chinese . The \" Vocab \" token denotes the vocabulary size after data preprocessing . The \" Avg . Len \" token denotes the average sentence length .", "entities": [[6, 7, "TaskName", "Translation"], [27, 28, "TaskName", "Translation"], [76, 77, "MethodName", "BPE"], [122, 123, "MethodName", "BPE"], [158, 160, "TaskName", "machine translation"], [179, 180, "MethodName", "BPE"]]}
{"text": "In this paper , we select 4 neural translation models for comparison . More details about the models are shown below : General NMT Model : The standard NMT model trained on the experimental data segmented by BPE . Baseline NMT Model : The standard NMT model trained on the experimental data segmented by morphological segmentation . The following models also use this word segmentation method . Bi - Directional NMT Model : Following Niu et al ( 2018b ) , we train a single NMT model to perform bi - directional machine translation . We concatenate the bilingual parallel sentences in both directions . Since the source and target sentences come from the same language pairs , we share the source and target vocabulary , and tie their word embedding during model training . Multi - Task Neural Model : We simply use the monolingual data of the agglutinative language from the bilingual parallel sentences . We use a joint vocabulary , tie the word embedding as well as the output layer 's weight matrix .", "entities": [[22, 23, "DatasetName", "General"], [37, 38, "MethodName", "BPE"], [92, 94, "TaskName", "machine translation"]]}
{"text": "Many areas of natural language processing have benefited from the existence of tools and frameworks that can be customized to develop specific applications . In the area of dialogue systems , there are few such tools and frameworks and they mostly remain focused on simple tasks that can be encoded in a state - based dialogue model ( see , e.g. , Williams et al , 2016 and the Dialogue State Tracking Challenge 1 ) . In this category some of the more expressive approaches to dialogue modeling are based on the information state ( Cooper , 1997 ) ; notable toolkits include TrindiKit ( Larsson and Traum , 2000 ) and its open - source successor trindikit.py ( Ljungl\u00f6f , 2009 ) , and OpenDial ( Lison and Kennington , 2016 ) . Unfortunately , there is a dearth of tools for developing mixed - initiative dialogue systems that involve complex back - end reasoning systems . Early theoretical work of SharedPlans ( Grosz and Kraus , 1996 ; Lochbaum et al , 1990 ) and planbased dialogue systems ( e.g. , Allen and Perrault , 1980 ; Litman and Allen , 1987 ) laid good foundations . The Collaborative Problem Solving ( CPS ) model ( Allen et al , 2002 ) seemed to promise a solution but that model has never been implemented in a truly domain - independent way . Ravenclaw ( Bohus and Rudnicky , 2009 ) is a plan - based dialog management framework that has been used to develop a number of dialogue systems . Its dialogue engine is task - independent and includes a number of generic conversational skills ; however , its behavior is driven by task - specific dialogue trees , which have to be implemented anew for every application . Dialogue management involves understanding the intention of the user 's contributions to the dialogue , and deciding what to do or say next . It is the core component of a dialogue system , and typically requires significant development effort for every new application domain . We believe that dialogue managers based on models of the collaborative problem solving process offer the highest potential for flexibility and portability . Flexibility refers to the ability to cover the full range of natural dialogues users may want to engage in , and portability refers to how easy it is to customize or modify a system to work in new domains ( Blaylock , 2007 ) . In this paper we describe a new , domainindependent dialogue manager based on the CPS model , and its implementation in an open - source dialog system shell ( Cogent 2 ) . To demonstrate its flexibility , we also describe briefly a few dialogue systems for different domains .", "entities": [[69, 73, "DatasetName", "Dialogue State Tracking Challenge"], [301, 303, "TaskName", "Dialogue management"]]}
{"text": "When agents are engaged in solving problems together , they need to communicate to agree on what goals to pursue and what steps to take to achieve those goals , negotiate roles , resources , etc . To underscore its collaborative aspect , this type of joint activity has been called Collaborative Problem Solving ( CPS ) . Modeling the type of dialogue agents engage in during CPS must , therefore , take into account the nature of the joint activity itself . In the early 2000s , Allen and colleagues described a preliminary plan - based CPS model of dialogue based on an analysis of an agent 's collaborative behavior at various levels : \uf0b7 An individual problem - solving level , where each agent manages its own problemsolving state , plans and executes individual actions , etc . ; \uf0b7 A collaborative problem - solving level , which models and manages the joint or collaborative problem - solving state ( shared goals , resources , situations ) ; \uf0b7 An interaction level , where individual agents negotiate changes in the joint problem - solving state ; and , finally , \uf0b7 A communication level , where speech acts realize the interaction level acts . This model was refined in a series of publications , and several prototype systems were developed for illustration ( Allen et al , 2002 ; Blaylock and Allen , 2005 ; , all based on the TRIPS system ( Allen et al , 2000 ) . One of the main benefits of this model is that linguistic interpretation and high - level intention recognition could be performed independently of the individual problem - solving level , whose contribution to interpretation would be to specialize the higher - level intentions into concrete problemsolving actions and verify that such actions make sense . The corollary is that in this model the back - end problem solvers would be insulated from the need to worry about linguistic issues . On this basis , it should be possible to create a generic dialogue system shell with only domainindependent components . Other developers , not necessarily specialists in NLU or dialogue systems , could use this shell to build , relatively quickly , intelligent dialogue systems for collaborative tasks in various domains . The various prototypes of TRIPS CPS - based systems referenced above did not fulfill this promise . In each , the CPS level was integrated fairly tightly with the individual problem - solving level for the application domain , and they were all developed by the same team . Thus , even though each such prototype implemented ( a version of ) the CPS model and used the same platform for NLU , the ultimate goal of creating a domain - independent dialogue shell that others could customize to develop independently dialogue systems has so far remained elusive . Similarly , the CPS - based dialogue manager in SAMMIE ( Becker et al , 2006 ) also aimed for domain independence but never quite realized it ( Blaylock , 2007 ) . In the rest of the paper we will report on our attempt to develop a generic dialogue shell based on the CPS model . We start with a description of the general architecture of a dialogue system based on the CPS model . Then , we will describe our dialogue manager , with a focus on its interface with the domain - specific problem solving agent . Finally , we give some details on six prototype dialogue systems developed using our dialogue shell , five of which were developed by independent teams of researchers .", "entities": [[107, 108, "DatasetName", "agent"], [125, 126, "DatasetName", "agent"], [581, 582, "DatasetName", "agent"]]}
{"text": "A collaborative conversational agent must understand a user 's utterances , that is , obtain a representation of the meaning of the utterance , recognize its intention , and then reason with this intention to decide what to do and/or say next . Finally , the system must convert its own intentions into language and communicate them to the user . Figure 1 shows a conceptual diagram of the dialogue system we envision . This follows the common separation of a conversational agent 's functionality into interpretation , behavior and generation , but where the separation lines are is critical for realizing the idea of isolating domainindependent from domain - specific processing . We take the output of NL Understanding ( assumed here to have broad lexical , syntactic and semantic coverage ) to be a domain - independent semantic representation of the user 's utterance ( a communicative act ) , expressed in terms of a domainindependent ontology . Intention recognition is performed by the CPS agent , which takes into account the discourse context and converts communicative acts into abstract communicative intentions . These communicative intentions need to be further evaluated with respect to the actual problem - solving state , so they are not fully interpreted until they reach the problem solving agent . This agent is responsible for the domain - specific behaviorhereafter we will refer to it as the Behavioral Agent ( BA ) and for operationalizing the communicative intentions into actions ( which may involve planning , acting on the world , updating its knowledge of the situation , etc . ) . An autonomous BA should be able to plan and act on its own , but neither the BA nor the user can singlehandedly decide on the status of collaborative goals without a commitment from the other party . The BA expresses its attitude towards shared goals by sending to the CPS agent its own communicative intentions , which the CPS agent will use to update the collaborative state and generate communicative acts for NL generation ( such as accepting or rejecting a goal , or proposing a new one ) . Customization : Figure 1 includes , on the left side , a number of resources needed by our ideal dialogue system : ( 1 ) a broad lexicon for NL understanding ; ( 2 ) a general - purpose ( upper - level ) ontology ; and , optionally , ( 3 ) a domain ontology . Even a state - of - the - art broad coverage parser , with an extensive domain - independent high - level ontology and lexicon , will not contain all the word senses and concepts needed for every application domain . Additionally , the general ontology concepts need to be mapped onto the domain ontology used by the back - end problem solvers . Lastly , NL generation from semantic representations of communicative acts is a difficult problem , with no general solutions . Many taskoriented dialogue systems employ template - based techniques , which can lead to satisfactory , if somewhat repetitive text realizations . Such templates are tailored for the application domain . It may appear that customizing a generic dialogue shell to specific applications involves a considerable amount of work . Nevertheless , we believe these customization tasks are easier to accomplish and require less linguistic expertise than building a dialogue manager for every application , let al ne building domain - specific natural language understanding components .", "entities": [[3, 4, "DatasetName", "agent"], [82, 83, "DatasetName", "agent"], [158, 159, "MethodName", "ontology"], [167, 168, "DatasetName", "agent"], [215, 216, "DatasetName", "agent"], [218, 219, "DatasetName", "agent"], [235, 236, "DatasetName", "Agent"], [237, 238, "DatasetName", "BA"], [271, 272, "DatasetName", "BA"], [286, 287, "DatasetName", "BA"], [308, 309, "DatasetName", "BA"], [320, 321, "DatasetName", "agent"], [329, 330, "DatasetName", "agent"], [404, 405, "MethodName", "ontology"], [415, 416, "MethodName", "ontology"], [439, 440, "MethodName", "ontology"], [462, 463, "MethodName", "ontology"], [471, 472, "MethodName", "ontology"], [583, 586, "TaskName", "natural language understanding"]]}
{"text": "Let us now turn to the details of our new instantiation of the CPS model . Unlike prior work on CPSbased dialogue management , we focus on the interface between the CPS agent ( CPSA ) and the BA . This allows us to directly address the issue of domain - independence that posed difficulties in other approaches ( e.g. , Blaylock , 2007 ) . The CPSA computes communicative intentions based on the communicative acts resulting from the NLU component . These communicative intentions are realized in our model as CPS Acts , represented as a pair < ACI , CONTEXT > , where ACI represents the abstract communicative intention and CONTEXT represents the semantic content of the act in a knowledge representation language . Where there is no ambiguity we will omit CONTEXT and denote CPS acts by their ACI only . In the following subsections we will describe the set of CPS acts we have devised so far , grouped by the manner in which they affect the collaborative state .", "entities": [[21, 23, "TaskName", "dialogue management"], [32, 33, "DatasetName", "agent"], [38, 39, "DatasetName", "BA"]]}
{"text": "The CPS Model defines an objective as an intention that is driving the agent 's current behavior ( Allen et al , 2002 ) . An objective can be proposed by either agent , provided they are ready to commit to it . We represent the intention to commit to an objective via the CPS act ADOPT . For example , if the user starts a conversation with \" Let 's build a tower \" , this results in the following CPS act : ( ADOPT : i d O1 : what C1 : as ( GOAL ) ) Here , O1 represents a unique , persistent identifier for the shared objective proposed via this act ( all objectives are assigned an identifier ) . C1 is an identifier indexed into the CONTEXT of this CPS act ( i.e. , it refers to an event of building a tower ) . Additionally , the act also indicates the relation between this objective and any pre - existing objectives . In this example , the relation was identified as GOAL , indicating that this is a top - level objective ( we will discuss later other types of relations between objectives available in our model ) . Once an objective has been jointly committed to , either agent can propose to drop their commitment to it , via a CPS act called ABANDON . Or , they might propose to shift focus from the active objective ( the one currently driving the agents ' behavior ) , by an act called DEFER , which will result in the objective becoming inactive . A proposal to bring an inactive objective back into an active state an agent results in a SELECT act . Finally , an agent can propose that an objective should be considered completed , via a RELEASE act . All these four acts only take as an argument the objective 's unique identifier , for example : ( ABANDON : i d O1 ) . Note that all of these four acts can be proposed , indicating the agent 's intentional stance towards their commitment to that objective . The user performs a proposal via a speech act . The same intention may be expressed by different surface speech acts . Going back to our example , the objective of building a tower together can be expressed via a direct proposal ( \" Let 's build a tower \" ) ; a question ( \" Can we build a tower ? \" ) ; or an indirect speech act ( \" I think we should build a tower \" ) . The CPSA recognizes the user intent in all these variants , using the surface speech act and other linguistic cues present in the communicative act it receives from NLU ) . Thus , they all result in the same ADOPT act as above . If , on the other hand , the BA wants to propose that an objective be jointly pursued , say that it wants to start working on O1 by a subgoal O2 of placing a block on the table , it can do so via a PROPOSE act , whose content is the intention to commit to that objective : where C2 is indexed into the CONTEXT of the act for a representation of the event of placing a block on the table . Upon receiving this act , the CPSA will update the collaborative state to reflect the BA 's intention to commit to O2 , and formulate a communicative act for NLG to realize the proposal in a system utterance . For a proposal to result in a shared objective , the two agents must agree to commit to it . The CPSA is responsible for gathering the agreements of both the user and the BA . When the CPSA recognizes that the user is proposing an objective , it will first send an EVALUATE act to the BA , whose content is the proposed objective , e.g. , : ( EVALUATE : content ( ADOPT : i d O1 : what C1 : as ( GOAL ) ) This act creates an obligation on the part of the BA to evaluate whether it is able to commit to it in the current situation , and , if so , respond by signaling agreement ( ACCEPTABLE ) , rejection ( REJECTED ) , or , when it can not even interpret what the objective is , a failure ( FAILURE ) . For example , the BA 's agreement , that is , its intention to commit to the objective proposed by the user , would be communicated via : ( ACCEPTABLE : content ( ADOPT : i d O1 : what C1 : as ( GOAL ) ) Since the user has already signaled their intention to commit to the objective by proposing it , on receiving from the BA that the objective is ACCEPTABLE , the CPSA knows that there is mutual agreement , decides that that the objective is now adopted , and sends back to the BA the following CPS act : to signal that now there is a joint commitment to O1 . This creates an obligation on the part of the BA to pursue O1 in whatever manner it deems appropriate . When we have a system - proposed objective , such as O2 above , if the user expresses their acceptance ( \" Yes \" , \" Sure \" , \" I can handle that \" , etc . ) , the CPSA will recognize this as completing the agreement , and then it would adopt the objective and send the COMMIT act to the BA . Having described in some detail how objectives are created , and how the CPSA decides that there is joint commitment to them , let us turn briefly to some of the details that we brushed over . Relations between objectives : We mentioned above two relations between the objective currently under consideration and the prior objectives ( either previously adopted ones , or ones that have been discussed but are still being negotiated ) , namely GOAL and SUBGOAL . Currently the CPSA can infer two more . One is MODIFICATION , used when one of the agents is expressing an intention of changing in some manner a prior objective ( for example , if one of the agents had suggested placing a blue block on the table , the other agent might suggest placing a red block instead ) . The second one we call ELABORATION , and is used by the CPSA to signal that it has insufficient knowledge to decide whether the objective under discussion is really a subgoal or a modification of another one , or , perhaps a new top - level goal . It is possible , however , that the BA may be able to use its more detailed knowledge of the situation to make that determination . Thus , upon receiving an objective marked as an elaboration of another one , if the BA deems it acceptable , it has the obligation to clarify the relation as well . Rejections and failures : If a user proposes an objective , presumably they have an expectation that the objective is achievable . If the BA rejects it , the user will likely not be satisfied with a simple \" No \" . Similarly , if the BA fails to understand the objective ( or if it encounters any other type of failure , e.g. , while trying to perform some action ) , the system should be able to explain what happened . Thus , the REJECTED and FAILURE CPS acts have features for optionally specifying a reason and a possible way of repairing the situation . The reason for rejection / failure is one of a relatively small set of predefined ones ( e.g. , UNKNOWN - OBJECT , FAILED - ACTION ) , and it is expected that the NLG component will make use of it to generate more helpful utterances . As for how to repair the situation , this can be an alternative objective , that the BA is ready to commit to , which could be either a modification of the reject - ed one , or , perhaps , an objective which , if realized , would make the rejected objective acceptable . For example , if the user wanted to build an all - blue 5 - block tower , but the BA has only 4 blue blocks , it would reject the goal ( INSUFFICIENT - RESOURCES ) , but it could suggest as an alternative that a 4 - block blue tower would be an achievable alternative . This might be realized as \" Sorry , I do n't have enough blocks for that , but we can build a 4 - block blue tower . \" . If the user accepts ( \" OK \" ) , the CPSA will immediately commit to the suggested objective .", "entities": [[13, 14, "DatasetName", "agent"], [32, 33, "DatasetName", "agent"], [216, 217, "DatasetName", "agent"], [284, 285, "DatasetName", "agent"], [294, 295, "DatasetName", "agent"], [349, 350, "DatasetName", "agent"], [494, 495, "DatasetName", "BA"], [585, 586, "DatasetName", "BA"], [643, 644, "DatasetName", "BA"], [666, 667, "DatasetName", "BA"], [707, 708, "DatasetName", "BA"], [764, 765, "DatasetName", "BA"], [828, 829, "DatasetName", "BA"], [858, 859, "DatasetName", "BA"], [885, 886, "DatasetName", "BA"], [960, 961, "DatasetName", "BA"], [1093, 1094, "DatasetName", "agent"], [1159, 1160, "DatasetName", "BA"], [1193, 1194, "DatasetName", "BA"], [1233, 1234, "DatasetName", "BA"], [1255, 1256, "DatasetName", "BA"], [1380, 1381, "DatasetName", "BA"], [1438, 1439, "DatasetName", "BA"]]}
{"text": "Collaborative problem solving requires not only joint commitments to certain objectives , but also a set of shared beliefs about the situation . These shared beliefs occasionally need to be updated . One agent may inform the other of a fact that they believe the other should know . This may come about unprompted or as a result of being asked . The CPS Model offers little guidance on how such acts fit in , even though they are very common in conversation . The examples given seem to suggest an interpretation of questions and simple assertions based on plan recognition ( Allen , 1979 ) , which is a tall order , particularly for a domainindependent dialogue manager . When agent A informs agent B of a fact P , this indicates A 's immediate intention that B knows P. Similarly , if A asks B whether P is true ( an ask - if speech act ) or what object satisfies P ( an ask - wh speech act ) , A 's immediate intention is that B informs A of those particular facts ( Allen and Perrault , 1980 ) . Getting at the intentions behind these immediate intentions requires fairly sophisticated , often domainspecific reasoning ( in our implementation the CPSA can do that to some extent via abstract task models , but , due to space limitations , we will not discuss it here ) . Therefore , we created a small set of CPS acts for representing the intentions to impart and request knowledge about situations . In our model , an assertion of a fact results in the following CPS act : where C3 is an identifier pointing to a representation of the content of the assertion in the CONTEXT of the CPS act . The relation between an ASSERTION act and an existing objective ( or NIL if no such objective exists ) is an underspeci - fied one , of contributing somehow to it . The BA needs to decide , if it accepts A3 , how this addition will change its understanding of the situation and affect O1 or any other ( adopted ) objective . For ask - if questions the CPSA will produce the following act : ( ASK - IF : i d A4 : query Q4 : as ( QUERY - IN - CONTEXT : goal O1 ) ) Here Q4 is an identifier pointing to a representation ( in the CONTEXT of the CPS act ) of a statement to be evaluated for its truth value . For ask - wh questions the CPSA produces acts in the following format : This expresses the intention of knowing the value of an entity ( W5 ) , possibly restricted to a set of choices ( S5 ) , that makes a proposition ( Q5 ) true . As before , all these identifiers should be given appropriate descriptions in the CONTEXT . This act can thus represent the intention expressed by a question such as \" What color should we use for the first block , blue or red ? \" . Finally , an answer to a question takes the following form : It is important to note that we treat these intentions as special types of objectives , that can become adopted , active , etc . , just like other objectives . For example , if one of these CPS acts is initiated by the user , the act must be evaluated by the BA . If it deems the act ACCEPTABLE , the CPSA will commit to working on it ( updating the system 's beliefs , or answering the question ) . If originating from the BA , the act must be proposed first , and realized through a communicative act . ( Side effects : We noted above that updating the system 's beliefs about the situation may affect the status of existing objectives . Insofar as the BA is capable of foreseeing these effects , it ought to inform the CPSA so the collaborative state can be updated . Any such changes would result in an obligation to inform the user . In our model we use an additional feature for the ACCEPTABLE act ( see previous section ) , for describing the effect . Its value is an objective to be proposed . For example , if , in the context of the shared objective of building a tower , the system asks \" Who is going to move the blocks ? \" , and the user says \" I will \" , this answer has the side effect of modifying the existing objective ( in this case specializing it to include the identity of the builder ) . The system 's acceptance of the answer will necessarily imply the acceptance of the modification as well , and the CPSA will update the collaborative state accordingly .", "entities": [[33, 34, "DatasetName", "agent"], [121, 122, "DatasetName", "agent"], [124, 125, "DatasetName", "agent"], [280, 281, "DatasetName", "C3"], [335, 336, "DatasetName", "BA"], [591, 592, "DatasetName", "BA"], [625, 626, "DatasetName", "BA"], [668, 669, "DatasetName", "BA"]]}
{"text": "Another important role of the CPSA in managing the dialogue is to negotiate initiative . To facilitate an orderly conversation , it restricts both the timing and the magnitude of the BA 's ability to affect the collaborative state . It does so via a special CPS act , called WHAT - NEXT , which takes a single argument : the identifier of an adopted shared objective ( usually the one that is active ) . This act can be sent to the BA whenever there are no pending updates to the collaborative state , and no outstanding communicative acts to process or to wait on . In effect , by sending this act , the CPSA transfers the task initiative to the BA , which gives it the chance to , ultimately , influence discourse initiative as well . The BA has the obligation to respond with a single update to the collaborative state , presumably the one with the highest priority . This restriction is critical , because it frees the CPSA from the need to consider too many options about what to do and say next , a decision that , in many situations , would require domain - specific knowledge . The BA 's reply to a WHAT - NEXT depends on its own private problem - solving state . It may be that it has done some planning and , as a result , it wants to propose a way of making progress towards accomplishing the active objective . It may be that it does not have sufficient information to make progress , in which case it may formulate an intention to ask the user to provide the information . Or , if the active objective is a question , it may have come up with an answer ; that update would prob - ably get very high priority . All these possibilities are handled by acts we have already discussed . One other possibility is that the BA is currently not doing any reasoning , but simply acting on the active objective , or has accomplished it . Updates to the status of an objective are communicated via a special CPS act , which takes the following form : ( EXECUTION - STATUS : goal A1 : status GS ) Here GS is an expression that indicates the status of the goal . Currently it can be one of three indicators : 1 . DONE , which signifies that A1 was accomplished . CPSA will create a communicative act to inform the user , and , if the user agrees , releases the objective . 2 . WORKING - ON - IT , which indicates that the BA is actively pursuing A1 , but it will take more time . The CPSA may decide to inform the user , and creates a trigger for itself to check back later . 3 . WAITING - FOR - USER , which indicates that the BA can not make progress on A1 because it is waiting for the user to act on it ( or another objective that A1 depends on ) . As a result , the CPSA will construct a communicative act to prompt the user . This CPS act also allows the BA to communicate partial execution status ( that it has executed some actions , though it has not accomplished the objective yet ) , but we leave those details out of this discussion .", "entities": [[31, 32, "DatasetName", "BA"], [83, 84, "DatasetName", "BA"], [123, 124, "DatasetName", "BA"], [141, 142, "DatasetName", "BA"], [206, 207, "DatasetName", "BA"], [333, 334, "DatasetName", "BA"], [453, 454, "DatasetName", "BA"], [498, 499, "DatasetName", "BA"], [548, 549, "DatasetName", "BA"]]}
{"text": "We implemented our CPS model as a component in the TRIPS system ( Allen et al , 2000 ) , which has recently been released in the public domain under a GNU GPL License . The TRIPS system comes with a broad coverage parser with an extensive grammar and an effective 100 , 000 + word semantic vocabulary defined in terms of a 4000 concept domain - independent ontology . It operates in concert with a suite of statistical preprocessing components , performing tasks such as part - ofspeech tagging , named entity recognition , and identification of likely constituent boundaries . These preprocessed inputs are provided to the core TRIPS parser as advice . The parser con - structs from the input a logical form , which is a semantic representation that captures an unscoped modal logic ( Manshadi et al , 2008 ) . The logical form includes the surface speech act , semantic types , semantic roles for predicate arguments , and dependency relations . TRIPS also includes an interpretation manager that converts the logical forms into communicative acts , performing language - based intention recognition and normalizing different surface forms . We packaged the TRIPS NLU components ( including the lexicon and ontology ) with our CPS agent , thereby creating a dialogue system shell , which we call Cogent . This system does not include a BA or an NLG component ( Cogent 's components are surrounded with a dashed line in Figure 1 ) . Thus , it is a true domain - independent shell , not a system that can be adapted to other domains . It can carry out very minimal conversations because social conversational acts such as greetings are handled in a domain - independent manner in the CPSA . But , ultimately , the purpose of the shell is to be used to create domain applications . The success of the task we set to accomplish is whether this shell can be and is used by independent developers to develop operational dialogue systems in domains of their choice . As discussed in the previous section , the CPS acts and the obligations they engender establish a protocol that developers of behavioral agents must implement . Other than that , we believe the CPSA offers functionality to develop different styles of conversational agents ( user - driven , system - driven or fully mixed - initiative ) . The developers also must implement their own NL Generation component , for reasons that we touched upon earlier . Of note , by default all CPS acts have their contents expressed in the TRIPS ontology . We are also providing a tool for mapping concepts in the TRIPS ontology to domain ontologies . We have adapted the TRIPS interpretation manager to use these mappings to produce content in the domain ontology , to make it easier for the Behavioral Agents to interpret the CONTEXT associated with each CPS act . The details of the ontology mapping tool and the mappings it creates are , however , beyond the scope of this paper .", "entities": [[68, 69, "MethodName", "ontology"], [91, 94, "TaskName", "named entity recognition"], [206, 207, "MethodName", "ontology"], [211, 212, "DatasetName", "agent"], [231, 232, "DatasetName", "BA"], [441, 442, "MethodName", "ontology"], [455, 456, "MethodName", "ontology"], [477, 478, "MethodName", "ontology"], [501, 502, "MethodName", "ontology"]]}
{"text": "We describe briefly six system prototypes that have been built using Cogent as the base frame - work ; thus , they all use the same CPS agent described above . In all cases , the developers of these prototypes used the protocol described above to create behavioral agents that , in turn , act as integrators of other problem solvers . The descriptions of these systems are going to be necessarily brief ; the interested reader is encouraged to follow the references to get a better understanding of their capabilities and the kinds of dialogues they support ( unfortunately , not all systems have been published yet ) . All these systems have been developed as part of DARPA 's Communicating with Computers ( CwC ) program 3 . Cabot : This is a mixed - initiative system for planning and execution in the blocks world , the tasks being of jointly building structures ( Perera et al , 2017 ) . Both the user and the system can come up with their own goals , and , if necessary , they will negotiate constraints on those structures ( size , colors , etc . ) so all the goals can be completed . They also negotiate their roles in building these structures ( \" architect \" or \" builder \" ) . This system uses a 2D simulated version of the blocks world . The examples used in this paper are from interactions with this system . Cabot - L : This system learns names and structural properties of complex objects in a physically situated blocks world scenario ( Perera et al , 2017 ; Perera et al , 2018 ) . The user teaches the system by providing examples of structures together with descriptions in language . The system has capabilities to perceive the world and detect changes to it , and can ask the user questions about various features of the structures , to learn a general model . To validate the inferred model , the user can then show additional examples and ask the system to classify them and explain its reasoning . The user and the system can interact via either written or spoken language . BoB : This system acts as an assistant biologist . It has fairly extensive knowledge of molecular biology and can assist the user by responding to inquiries about properties of genes , proteins , molecular mechanisms , their relationship to cellular processes and disease , building and visualizing complex causal models , running simulations on these models to detect their dynamic properties , etc . To manage this wide range of problemsolving behaviors , BoB 's BA integrates a variety of agents with specific expertise . ing domain - specific named entity recognizers ) and some additional ontology concepts and mappings ; we provided those customizations . The version of the TRIPS Parser we started with proved to be fairly robust , but we did have to adapt it in response to failures reported by the dialogue system developers . Nevertheless , these enhancements were not domain - specificthat is , the same parser , with the same grammar , is used for all systems . In all systems , developers used custom template - based NLG .", "entities": [[27, 28, "DatasetName", "agent"], [119, 120, "DatasetName", "DARPA"], [448, 449, "DatasetName", "BA"], [469, 470, "MethodName", "ontology"]]}
{"text": "In this paper we reported on the development of a new domain - independent dialogue manager based on the collaborative problem solving model . We packaged this dialogue manager with a suite of broad coverage natural language understanding components ( from the TRIPS system ) and created a new , domain - independent CPS - based dialogue system shell . This shell has been used by several independent teams of researchers to develop dialogue systems in a variety of application domains , with different conversational styles . We believe this to be the first successful implementation of a domain - independent dialogue system shell based on the CPS model ( or any other model of equivalent complexity ) . We do not claim the CPSA to be complete , however . For example , it can sometimes detect an ambiguity in the user 's intention and generate a clarification question , but its abilities in this regard are fairly limited . BoB has demonstrated some limited handling of hypotheticals ( in what - if questions ) at the problem - solving level , but the CPSA itself does not yet track hypothetical situations . We expect that , with wider adoption , we will inevitably be confronted with the need to improve both our model and its implementation . As noted above in reference to BoB and Musica , for domains requiring adaptation of the NLU components , language specialists are still needed . We have not yet endeavored to create tools that would make it easier for dialogue system developers to adapt / improve themselves the NLU components . Our current focus is on evaluating the robustness of the intention recognition functionality of the CPSA .", "entities": [[35, 38, "TaskName", "natural language understanding"]]}
{"text": "This research was supported by the DARPA Communicating with Computers program , under ARO contract W911NF - 15 - 1 - 0542 .", "entities": [[6, 7, "DatasetName", "DARPA"]]}
{"text": "On Negative Interference in Multilingual Models : Findings and A Meta - Learning Treatment", "entities": [[10, 13, "TaskName", "Meta - Learning"]]}
{"text": "Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each ( positive transfer ) , with the most pronounced benefits accruing to low - resource languages . However , recent work has shown that this approach can degrade performance on high - resource languages , a phenomenon known as negative interference . In this paper , we present the first systematic study of negative interference . We show that , contrary to previous belief , negative interference also impacts low - resource languages . While parameters are maximally shared to learn language - universal structures , we demonstrate that language - specific parameters do exist in multilingual models and they are a potential cause of negative interference . Motivated by these observations , we also present a meta - learning algorithm that obtains better cross - lingual transferability and alleviates negative interference , by adding languagespecific layers as meta - parameters and training them in a manner that explicitly improves shared layers ' generalization on all languages . Overall , our results show that negative interference is more common than previously known , suggesting new directions for improving multilingual representations . 1", "entities": [[135, 138, "TaskName", "meta - learning"]]}
{"text": "Advances in pretraining language models ( Devlin et al , 2018 ; Yang et al , 2019 ) as general - purpose representations have pushed the state of the art on a variety of natural language tasks . However , not all languages enjoy large public datasets for pretraining and/or downstream tasks . Multilingual language models such as mBERT ( Devlin et al , 2018 ) and XLM ( Lample and Conneau , 2019 ) have been proven effective for cross - lingual transfer learning by pretraining a single shared Transformer model ( Vaswani et al , 2017 ) jointly on multiple languages . The goals of multilingual modeling are not limited to improving language modeling in low - resource languages ( Lample and Conneau , 2019 ) , but also include zero - shot crosslingual transfer on downstream tasks - it has been shown that multilingual models can generalize to target languages even when labeled training data is only available in the source language ( typically English ) on a wide range of tasks ( Pires et al , 2019 ; Wu and Dredze , 2019 ; Hu et al , 2020 ) . However , multilingual models are not equally beneficial for all languages . demonstrated that including more languages in a single model can improve performance for lowresource languages but hurt performance for highresource languages . Similarly , recent work ( Johnson et al , 2017 ; Tan et al , 2019 ; Aharoni et al , 2019 ; in multilingual neural machine translation ( NMT ) also observed performance degradation on high - resource language pairs . In multi - task learning ( Ruder , 2017 ) , this phenomenon is known as negative interference or negative transfer ( Wang et al , 2019 ) , where training multiple tasks jointly hinders the performance on individual tasks . Despite these empirical observations , little prior work analyzed or showed how to mitigate negative interference in multilingual language models . Particularly , it is natural to ask : ( 1 ) Can negative interference occur for low - resource languages also ? ( 2 ) What factors play an important role in causing it ? ( 3 ) Can we mitigate negative interference to improve the model 's cross - lingual transferability ? In this paper , we take a step towards addressing these questions . We pretrain a set of monolingual and bilingual models and evaluate them on a range of downstream tasks to analyze negative interference . We seek to individually characterize the un - derlying factors of negative interference through a set of ablation studies and glean insights on its causes . Specifically , we examine if training corpus size and language similarity affect negative interference , and also measure gradient and parameter similarities between languages . Our results show that negative interference can occur in both high - resource and low - resource languages . In particular , we observe that neither subsampling the training corpus nor adding typologically similar languages substantially impacts negative interference . On the other hand , we show that gradient conflicts and language - specific parameters do exist in multilingual models , suggesting that languages are fighting for model capacity , which potentially causes negative interference . We further test whether explicitly assigning language - specific modules to each language can alleviate negative interference , and find that the resulting model performs better within each individual language but worse on zero - shot cross - lingual tasks . Motivated by these observations , we further propose to meta - learn these language - specific parameters to explicitly improve generalization of shared parameters on all languages . Empirically , our method improves not only within - language performance on monolingual tasks but also cross - lingual transferability on zero - shot transfer benchmarks . To the best of our knowledge , this is the first work to systematically study and remedy negative interference in multilingual language models .", "entities": [[58, 59, "MethodName", "mBERT"], [67, 68, "MethodName", "XLM"], [80, 84, "TaskName", "cross - lingual transfer"], [90, 91, "MethodName", "Transformer"], [255, 257, "TaskName", "machine translation"], [272, 276, "TaskName", "multi - task learning"]]}
{"text": "Multilingual transfer learning aims at utilizing knowledge transfer across languages to boost performance on low - resource languages . State - of - theart multilingual language models are trained on multiple languages jointly to enable cross - lingual transfer through parameter sharing . However , languages are heterogeneous , with different vocabularies , morphosyntactic rules , and different pragmatics across cultures . It is therefore natural to ask , is knowledge transfer beneficial for all languages in a multilingual model ? To analyze the effect of knowledge transfer from other languages on a specific language lg , we can compare multilingual models with the monolingual model trained on lg . For example , in Figure 1 , we compare the performance on a named entity recognition ( NER ) task of monolingually - trained models vs. bilingual models ( trained on lg and English ) vs. state - of - the - art XLM . We can see that monolingual models outperform multilingual models on four out of six languages ( See 3.3 for details ) . This shows that language conflicts may induce negative impacts on certain languages , which we refer to as negative interference . Here , we investigate the causes of negative interference ( 3.3 ) and methods to overcome it ( 4 ) . 3 Investigating the Sources of Negative Interference in Multilingual Models", "entities": [[1, 3, "TaskName", "transfer learning"], [35, 39, "TaskName", "cross - lingual transfer"], [123, 126, "TaskName", "named entity recognition"], [127, 128, "TaskName", "NER"], [153, 154, "MethodName", "XLM"]]}
{"text": "Recent work ( Yu et al , 2020 ) shows that gradient conflict between dissimilar tasks , defined as a negative cosine similarity between gradients , is predictive of negative interference in multi - task learning . Therefore , we study whether gradient conflicts exist between languages in multilingual models . In particular , we sample one batch for each language in the model and compute the corresponding gradients ' cosine similarity for every 10 steps during pretraining .", "entities": [[32, 36, "TaskName", "multi - task learning"]]}
{"text": "Unsupervised multilingual language models such as mBERT ( Devlin et al , 2018 ) and XLM ( Lample and Conneau , 2019 ; work surprisingly well on many NLP tasks without parallel training signals ( Pires et al , 2019 ; Wu and Dredze , 2019 ) . A line of follow - up work Artetxe et al , 2019 ; Karthikeyan et al , 2020 ) study what contributes to the cross - lingual ability of these models . They show that vocabulary overlap is not required for multilingual models , and suggest that abstractions shared across languages emerge automatically during pretraining . Another line of research investigate how to further improve these shared knowledge , such as applying post - hoc alignment ( Wang et al , 2020b ; Cao et al , 2020 ) and utilizing better calibrated training signal ( Mulcaire et al , 2019 ; Huang et al , 2019 ) . While prior work emphasize how to share to improve transferability , we study multilingual models from a different perspective of how to unshare to resolve language conflicts . Our work is also related to transfer learning ( Pan and Yang , 2010 ) and multi - task learning ( Ruder , 2017 ) . In particular , prior work have observed ( Rosenstein et al , 2005 ) and studied ( Wang et al , 2019 ) negative transfer , such that transferring knowledge from source tasks can degrade the performance in the target task . Others show it is important to remedy negative transfer in multi - source settings ( Ge et al , 2014 ; Wang and Carbonell , 2018 ) . In this work , we study negative transfer in multilingual models , where languages contain heavily unbalanced training data and exhibit complex intertask relatedness . In addition , our work is related to methods that measure similarity between cross - lingual representations . For example , existing methods utilize statistical metrics to examine cross - lingual embeddings such as singular vector canonical correlation analysis ( Raghu et al , 2017 ; Kudugunta et al , 2019 ) , eigenvector similarity ( S\u00f8gaard et al , 2018 ) , and centered kernel alignment ( Kornblith et al , 2019 ; . While these methods focus on testing latent representations , we directly compare similarity of neural network structures through network pruning . Finally , our work is related to meta learning , which sets a meta task to learn model initialization for fast adaptation ( Finn et al , 2017 ; Gu et al , 2018 ; Flennerhag et al , 2019 ) , data selection ( Wang et al , 2020a ) , andhyperparameters ( Baydin et al , 2018 ) . In our case , the meta task is to mitigate negative interference .", "entities": [[6, 7, "MethodName", "mBERT"], [15, 16, "MethodName", "XLM"], [191, 193, "TaskName", "transfer learning"], [201, 205, "TaskName", "multi - task learning"], [399, 401, "TaskName", "network pruning"]]}
{"text": "We show the full results on the TyDiQA - GoldP dataset in Table 7 .", "entities": [[7, 10, "DatasetName", "TyDiQA - GoldP"]]}
{"text": "UoB - UK at SemEval - 2016 Task 1 : A Flexible and Extendable System for Semantic Text Similarity using Types , Surprise and Phrase Linking", "entities": [[17, 19, "TaskName", "Text Similarity"]]}
{"text": "We present in this paper a system for measuring Semantic Text Similarity ( STS ) in English . We introduce three novel techniques : the use of Types , methods of linking phrases , and the use of a Surprise Factor to generate 8 , 370 similarity measures , which we then combine using Support Vector and Kernel Ridge Regression . Our system out performs the State of the Art in SemEval 2015 , and our best performing run achieved a score of .7094 on the 2016 test set as a whole , and over 0.8 on the majority of the datasets . Additionally , the use of Surprise , Types and phrase linking is not limited to STS and can be used across various Natural Language Processing tasks , while our method of combining scores provides a flexible way of combining variously generated Similarity Scores .", "entities": [[10, 12, "TaskName", "Text Similarity"], [13, 14, "TaskName", "STS"], [118, 119, "TaskName", "STS"]]}
{"text": "Word embeddings provide a method of mapping words or phrases to vectors , whose cosine distance represents semantic similarity . They have proved to be powerful in many NLP tasks , and have been used by top ranking systems at SemEval STS ( Sultan et al , 2015 ; H\u00e4nig et al , 2015 ) . We use word2vec 1 , with the model trained by Google on the Google News dataset , through its Python interface Gensim 2 . We make use of word2vec in two distinct ways . The first is by extracting the mean of the vector representation of each word in a Type and finding its cosine similarity between the two sentences . The second is by adding the word2vec similarity scores of words not aligned within the same Type . We also", "entities": [[0, 2, "TaskName", "Word embeddings"], [17, 19, "TaskName", "semantic similarity"], [41, 42, "TaskName", "STS"], [66, 67, "DatasetName", "Google"], [69, 70, "DatasetName", "Google"]]}
{"text": "Given a sentence pair , we calculate their similarity based only on how similar corresponding Parts - of - Speech ( POS ) are , a method previous systems have made use of , either implicitly ( Kashyap et al , 2014 ; Sultan et al , 2015 ) or explicitly ( H\u00e4nig et al , 2015 ) . We extend this idea by defining what we call word Types , which further subdivide each POS . A Type represents an abstract concept that several words can share . Consider the sentence pair \" A man is sitting on a stool \" , \" A boy is sitting on a chair \" . Although the words \" man \" , \" boy \" , \" stool \" and \" chair \" are all nouns , an effective strategy for comparing these sentences would be to compare the first two and the last two words independently , before then adding up their similarity . To achieve this we categorise words into different Types , which are then compared across sentences . In this case , such a categorisation might place the first two into the Type \" Person \" and the others into the category \" Artifact \" . This problem could very easily extend to the problem of Word Sense Disambiguation , which we avoid by use of a heuristic . We calculate the Type of a noun by the use of WordNet ( Miller , 1995 ) hypernyms : W 1 is consid - ered a hypernym of W 2 if \u2200e W 2 , e is an instance of W 1 . We recursively find hypernyms until we reach a manually selected set of concepts ( such as food.n.02 ) . We manually combine sets of such concepts to define a Type . As a concrete example , we combine the WordNet concepts \" communication.n.02 \" , \" food.n.02 \" and other similar concepts into the Type \" thing r1 \" . As a single word can be part of several Types , based on the particular sense of the word , we pick the most frequently occurring Type for each word . 3", "entities": [[218, 221, "TaskName", "Word Sense Disambiguation"]]}
{"text": "Consider sentences with the the phrases \" Prime Minister \" and \" Prime Number \" . Although the word \" Prime \" is present in both sentences , the context in which it is being used makes this irrelevant . In this particular case , the semantic similarity of the sentences is dependent on the head of the phrase that the word \" Prime \" is contained in ( i.e. \" Minister \" and \" Number \" ) . This is also the case with phrases that contain adjectives and adverbs . We address this by finding phrases that consist of adjectives , adverbs and nouns , and varying the importance of the semantic similarity between words that are not the head of that phrase . The similarity of each word , that is part of such a phrase , but not the head of the phrase , is additionally weighted in three different ways : The first assigns a zero or one weight based on whether or not the head of the phrase is aligned , the second provides a weight based on the number of words , following this word , that are aligned in the phrase and the third simply ignores the phrase structure .", "entities": [[46, 48, "TaskName", "semantic similarity"], [113, 115, "TaskName", "semantic similarity"]]}
{"text": "In this paper we have described the system we used for participation in the SemEval STS Monolingual Task which made use of Types , Phrase Linking , and a method of establishing common noun importance . In the future , we intend to experiment with including features for each of the Methods during the training phase , other kinds of phrases , and different Type definitions . We also intend to use the STS data for learning the weights of different Types for use in other NLP applications . We believe that Types have significant potential and intend to explore them in greater detail . Our immediate objectives will be in better defining types , re - categorising common noun Types based on clearer instructions to manual annotators , including finer definitions of Types for proper nouns using named entity recognition , and exploring methods of defining Types for verbs , adverbs and adjectives . We also intend to explore the use of Types in Question Classification and Question Answering .", "entities": [[15, 16, "TaskName", "STS"], [73, 74, "TaskName", "STS"], [138, 141, "TaskName", "named entity recognition"], [166, 167, "TaskName", "Classification"], [168, 170, "TaskName", "Question Answering"]]}
{"text": "UnibucKernel : Geolocating Swiss German Jodels Using Ensemble Learning", "entities": [[7, 9, "TaskName", "Ensemble Learning"]]}
{"text": "In this work , we describe our approach addressing the Social Media Variety Geolocation task featured in the 2021 VarDial Evaluation Campaign . We focus on the second subtask , which is based on a data set formed of approximately 30 thousand Swiss German Jodels . The dialect identification task is about accurately predicting the latitude and longitude of test samples . We frame the task as a double regression problem , employing an XGBoost metalearner with the combined power of a variety of machine learning approaches to predict both latitude and longitude . The models included in our ensemble range from simple regression techniques , such as Support Vector Regression , to deep neural models , such as a hybrid neural network and a neural transformer . To minimize the prediction error , we approach the problem from a few different perspectives and consider various types of features , from lowlevel character n - grams to high - level BERT embeddings . The XGBoost ensemble resulted from combining the power of the aforementioned methods achieves a median distance of 23.6 km on the test data , which places us on the third place in the ranking , at a difference of 6.05 km and 2.9 km from the submissions on the first and second places , respectively .", "entities": [[47, 49, "TaskName", "dialect identification"], [160, 161, "MethodName", "BERT"]]}
{"text": "The Social Media Variety Geolocation ( SMG ) task was proposed , for the second year consecutively , in the 2021 edition of the VarDial Evaluation Campaign ( Chakravarthi et al , 2021 ) . This task is aimed at geolocation prediction based on short text messages exchanged by the users of social media platforms such as Twitter or Jodel . The location from where a short text was posted on a certain social media platform is expressed by two components : the latitude and the longitude . Naturally , the geolocation task is formulated as a double regression problem . Twitter and Jodel are the platforms used for data collection , and similar to the previous spin of SMG at VarDial 2020 ( G\u0203man et al , 2020 ) , the task is divided into three subtasks , by language area , namely : Standard German Jodels ( DE - AT ) - which targets conversations initiated in Germany and Austria in regional dialectal forms ( Hovy and Purschke , 2018 ) . Swiss German Jodels ( CH ) - containing a smaller number of Jodel conversations from the German speaking half of Switzerland ( Hovy and Purschke , 2018 ) . BCMS Tweets - from the area of Bosnia and Herzegovina , Croatia , Montenegro and Serbia where the macro - language is BCMS , with both similarities and a fair share of variation among the component languages ( Ljube\u0161i\u0107 et al , 2016 ) . The focus of our work falls only on the second subtask , SMG - CH , tackled via a variety of handcrafted and deep learning models . We propose a single ensemble model joining the power of several individual models through meta - learning based on Extreme Gradient Boosting ( XGBoost ) ( Chen and Guestrin , 2016 ) . We trained two independent ensemble models , each predicting one of the components that form the geographical coordinates ( latitude and longitude ) . The first model plugged into our meta - learner is a Support Vector Regression ( SVR ) model ( Chang and Lin , 2002 ) based on string kernels . Previous usage in dialect identification has proved the efficiency of this technique in the task of interest ( Butnaru and Ionescu , 2018b ; G\u0203man and Ionescu , 2020 ; Ionescu and Butnaru , 2017 ; . The second model included in the ensemble is a hybrid convolutional neural network ( CNN ) ( Liang et al , 2017 ) that combines , in the same architecture , character - level ( Zhang et al , 2015 ) and word - level representations . The ability of capturing morphological relationships at the character level and using them as features for CNNs is also known to give promising results in dialect identification Tudoreanu , 2019 ) . Different from works using solely character - level CNNs for dialect identification Tudoreanu , 2019 ) , we believe that the addition of words might bring the benefit of learning dialectspecific multi - word expressions that are hard to capture at the character level ( Dhingra et al , 2016 ; Ling et al , 2015 ) . Bidirectional Encoder Representations from Transformers ( BERT ) ( Devlin et al , 2019 ) is a top performing technique used in recent years for solving mainstream NLP problems . Thus , it seems fit to also include the outputs of a fine - tuned German version of BERT in our XGBoost meta - learner . We conducted experiments on the development set provided by the shared task organizers ( Hovy and Purschke , 2018 ) in order to decide which model to choose as our submission for the SMG - CH subtask . Our results indicate that the ensemble model attains the best performance . With median distances that are 5 - 6 km higher , all the other models , tested individually on the development set , provide slightly worse predictions . The remainder of this paper is organized as follows . We present related work on dialect identification and geolocation of short texts in Section 2 . Our approach is described in detail in Section 3 . We present the experiments and empirical results in Section 4 . Finally , our conclusions are drawn in Section 5 .", "entities": [[289, 292, "TaskName", "meta - learning"], [365, 367, "TaskName", "dialect identification"], [471, 473, "TaskName", "dialect identification"], [488, 490, "TaskName", "dialect identification"], [542, 543, "MethodName", "BERT"], [584, 585, "MethodName", "BERT"], [685, 687, "TaskName", "dialect identification"]]}
{"text": "Our study of the related work starts with a brief overview of geotagging based on text . Then , we look at more specific methods studying geotagging in social media and we also investigate the amount of research done , from a computational linguistics perspective , in geotagging by dialect , with focus on last year 's approaches for the same subtask , namely SMG - CH at VarDial 2020 . Text - based geotagging . The works based on text to perform geotagging can be divided into three generic categories by the approaches taken in order to predict location . The first type of approach relies mainly on gazetteers as the source of the location mappings . This tool is adopted in a number of works ( Cheng et al , 2010 ; Quercini et al , 2010 ) , from ruled - based methods ( Bilhaut et al , 2003 ) to various machine learning techniques that rely on named entity recognition ( Ding et al , 2000 ; Gelernter and Mushegian , 2011 ; Qin et al , 2010 ) . This category of methods brings the disadvantage of relying on specific mentions of locations in text , rather than inferring them in a less straightforward manner . These direct mentions of places are not a safe assumption , especially when it comes to social media platforms which represent the data source in some of these studies ( Cheng et al , 2010 ) . The other two main categories of approaches for text - based geolocation rely on either supervised ( Kinsella et al , 2011 ; Wing and Baldridge , 2011 ) or unsupervised ( Ahmed et al , 2013 ; Eisenstein et al , 2010 ; Hong et al , 2012 ) learning . The latter methods usually employ clustering techniques based on topic models . Geolocation in social media . A number of works ( Rout et al , 2013 ) look at this task from a supervised learning perspective . However , in these studies , other details ( e.g. social ties ) in the users profile are considered rather than their written content . Other works in this area are related to our current interest in studying language variation for the geolocation of social media posts ( Doyle , 2014 ; Eisenstein et al , 2010 ; Han et al , 2014 ; Rahimi et al , 2017 ; Roller et al , 2012 ) . Among these , various machine learning techniques are employed in location prediction , ranging from probabilistic graphical models ( Eisenstein et al , 2010 ) and adaptive grid search ( Roller et al , 2012 ) to Bayesian methods ( Doyle , 2014 ) and neural networks ( Rahimi et al , 2017 ) . Dialect - based geolocation . Many dialects are covered in the text - based geotagging research to date , including Dutch ( Wieling et al , 2011 ) , British ( Szmrecsanyi , 2008 ) , American ( Eisenstein et al , 2010 ; Huang et al , 2016 ) and African American Vernacular English ( Jones , 2015 ) . Out of all the languages that were subject to location detection by dialect , we are interested in German . In this direction , the study that is the most relevant to our work is that of Hovy and Purschke ( 2018 ) which targets the German language and its variations . Approximately 16.8 million online posts from the German - speaking area of Europe are employed in this study with the aim of learning document representations of cities . A small fraction of these posts are Jodels collected from the German speaking side of Switzerland , and these are also used in the SMG - CH subtask that we are addressing in this paper . The assumption here is that the methods should manage to capture enough regional variations in the written language , which can serve as the means to automatically distinguish the geographical region of social media posts . The verification performed in this direction in the original paper ( Hovy and Purschke , 2018 ) used clustering to determine larger regions covering a given dialect . However , given the shared task formulation , we take a different approach and use the provided data in a double regression setup , addressing the problem both from a shallow and a deep learning perspective . As previously mentioned , this is the second consecutive year in which SMG - CH is featured at Var - Dial ( Chakravarthi et al , 2021 ) , with a similar format , although an updated data set . The participants of the 2020 SMG - CH shared task ( G\u0203man et al , 2020 ) studied this task from a variety of angles . Some techniques are based on deep neural networks such as the popular BERT architecture ( G\u0203man and Ionescu , 2020 ; Scherrer and Ljube\u0161i\u0107 , 2020 ) , bidirectional Long Short - Term Memory ( LSTM ) networks applied on FastText embeddings ( Mishra , 2020 ) or character - level CNNs ( G\u0203man and Ionescu , 2020 ) . Other techniques are based on shallow or handcrafted features such as a gridbased prediction using an n - gram language model ( Jauhiainen et al , 2020 ) or a clustering technique that shifts the problem into a discrete space , then uses an SVM for the classification of posts into regions . Our best submission ( G\u0203man and Ionescu , 2020 ) in last year 's campaign was an ensemble based on XGBoost as meta - learner over the predictions of three different models : \u03bd - SVR with string kernels , a character - level CNN and an LSTM based on BERT embeddings . This year , we acknowledge that our deep learning models had sub - optimal results in our previous participation at SMG - CH . Consequently , for this year , we bring stronger neural networks into the XGBoost ensemble . Instead of using an LSTM with BERT embeddings , we finetune the cased version of German BERT and use it directly for double regression in a setup that is more suitable to the data set size , compared to our previous endeavour . Instead of a character - level CNN , we use a hybrid CNN which learns end - to - end rep - resentations for both words and characters . The aforementioned deep learning models are plugged into an XGBoost ensemble alongside a retrained version of the \u03bd - SVR model employed in our past participation ( G\u0203man and Ionescu , 2020 ) . Our current changes introduce a significant improvement in median distance compared to our previous results , on a similar ( and perhaps more challenging ) data set .", "entities": [[161, 164, "TaskName", "named entity recognition"], [309, 311, "TaskName", "topic models"], [826, 827, "MethodName", "BERT"], [843, 848, "MethodName", "Long Short - Term Memory"], [849, 850, "MethodName", "LSTM"], [854, 855, "MethodName", "FastText"], [918, 919, "MethodName", "SVM"], [974, 975, "MethodName", "LSTM"], [977, 978, "MethodName", "BERT"], [1024, 1025, "MethodName", "LSTM"], [1026, 1027, "MethodName", "BERT"], [1036, 1037, "MethodName", "BERT"]]}
{"text": "Transformers ( Vaswani et al , 2017 ) represent an important advance in NLP , with many benefits over the traditional sequential neural architectures . Based on an encoder - decoder architecture with attention , transformers proved to be better at modeling long - term dependencies in sequences , while being effectively trained as the sequential dependency of previous tokens is removed . Unlike other contemporary attempts at using transformers in language modeling ( Radford et al , 2018 ) , BERT ( Devlin et al , 2019 ) builds deep language representations in a self - supervised fashion and incorporates context from both directions . The masked language modeling technique enables BERT to pretrain these deep bidirectional representations , that can be further fine - tuned and adapted for a variety of downstream tasks , without significant architectural updates . We also make use of this property in the current work , employing the Hugging Face ( Wolf et al , 2020 ) version of the cased German BERT model 1 . The model was initially trained on the latest German Wikipedia dump , the OpenLe - galData dump and a collection of news articles , summing up to a total of 12 GB of text files . We fine - tune this pre - trained German BERT model for the geolocation of Swiss German short texts , in a regression setup . The choice of hyperparameters is , in part , inspired by the winning system in the last year 's SMG - CH subtask at VarDial ( Scherrer and Ljube\u0161i\u0107 , 2020 ) .", "entities": [[81, 82, "MethodName", "BERT"], [107, 110, "TaskName", "masked language modeling"], [112, 113, "MethodName", "BERT"], [169, 170, "MethodName", "BERT"], [218, 219, "MethodName", "BERT"]]}
{"text": "We train three different models which rely on different learning methods and types of features to perform the required double regression . Thus , we have the hybrid CNN relying on both words and characters as features , the shallow \u03bd - SVR based on string kernels and three fine - tuned German BERT models looking at higher - level features and understanding dependencies in a bidirectional manner . Table 1 shows the preliminary results obtained on the development set by each individual model as well as the results of the submitted XGBoost ensemble . The individual models provide quite similar results in terms of the median distance between the predicted and ground - truth locations . These results stay around a value of 30 km for the median distance and 35 km for the mean distance . Among the independent models , the hybrid CNN obtains slightly better results in terms of the median distance ( 30.05 km ) , whereas the second attempt at fine - tuning BERT gives the worst distances , namely 33.86 km for the median distance and 38.85 km for the mean distance . \u03bd - SVR surpasses all the other models , by a small margin , in terms of the mean distance ( 34.82 km ) . The results of the submitted XGBoost ensemble model stand proof that our intuition was correct , namely that all these individual models have the potential to complement each other if put together in an ensemble . Indeed , the submitted system clearly surpasses the best individual model by approximately 5 km in terms of both the median and the mean distance metrics .", "entities": [[53, 54, "MethodName", "BERT"], [169, 170, "MethodName", "BERT"]]}
{"text": "In this paper , we proposed an ensemble learning model for the geolocation of Swiss German social media posts . The ensemble is based on an XGBoost meta - learner applied on top of three individual models : a hybrid CNN , an approach based on string kernels and a fine - tuned German BERT model . Given the final results obtained in the SMG - CH subtask , we conclude that predicting the location of Swiss German social media posts is a challenging task , the median distance being higher than 20 km . Using external data sources to build a language model seems to be a more promising path towards success , as shown by the final standings of the VarDial 2020 ( G\u0203man et al , 2020 ) and 2021 ( Chakravarthi et al , 2021 SMG shared tasks . In future work , we aim to study the applicability of our ensemble on other geolocation tasks , perhaps taking into consideration future VarDial challenges .", "entities": [[7, 9, "TaskName", "ensemble learning"], [54, 55, "MethodName", "BERT"]]}
{"text": "BERT Prescriptions to Avoid Unwanted Headaches : A Comparison of Transformer Architectures for Adverse Drug Event Detection", "entities": [[0, 1, "MethodName", "BERT"], [10, 11, "MethodName", "Transformer"], [15, 17, "TaskName", "Event Detection"]]}
{"text": "Pretrained transformer - based models , such as BERT and its variants , have become a common choice to obtain state - of - the - art performances in NLP tasks . In the identification of Adverse Drug Events ( ADE ) from social media texts , for example , BERT architectures rank first in the leaderboard . However , a systematic comparison between these models has not yet been done . In this paper , we aim at shedding light on the differences between their performance analyzing the results of 12 models , tested on two standard benchmarks . SpanBERT and PubMedBERT emerged as the best models in our evaluation : this result clearly shows that span - based pretraining gives a decisive advantage in the precise recognition of ADEs , and that in - domain language pretraining is particularly useful when the transformer model is trained just on biomedical text from scratch .", "entities": [[8, 9, "MethodName", "BERT"], [50, 51, "MethodName", "BERT"]]}
{"text": "The identification of Adverse Drug Events ( ADEs ) from text recently attracted a lot of attention in the NLP community . On the one hand , it represents a challenge even for the most advanced NLP technologies , since mentions of ADEs can be found in different varieties of online text and present unconventional linguistic features ( they may involve specialized language , or consist of discontinuous spans of tokens etc . ) ( Dai , 2018 ) . On the other hand , the task has an industrial application of primary importance in the field of digital pharmacovigilance Karimi et al , 2015b ) . This raising interest is attested , for example , by the ACL workshop series on Social Media Health Mining ( SMM4H ) , in which shared tasks on ADE detection have been regularly organized since 2016 ( Paul et al , 2016 ; Sarker and Gonzalez - Hernandez , 2017 ; Weissenbacher et al , 2018Weissenbacher et al , , 2019 . With the recent introduction of Transformers architectures and their impressive achievements in NLP ( Vaswani et al , 2017 ; Devlin et al , 2019 ) , it is not surprising that these tools have become a common choice for the researchers working in the area . The contribution of this paper is a comparison between different Transformers on ADE detection , in order to understand which one is the most appropriate for tackling the task . Shared tasks are not the best scenario for addressing this question , since the wide range of differences in the architectures ( which could include , for example , ensembles of Transformers and other types of networks ) does not allow a comparison on the same grounds . In our view , two key questions deserve a particular attention in this evaluation . First , whether there is an advantage in using a model with some form of in - domain language pretraining , given the wide availability of Transformers for the biomedical domain ( Lee et al , 2020 ; Gu et al , 2020 ) . Second , whether a model trained to predict coherent spans of text instead of single words can achieve a better performance ( Joshi et al , 2019 ) , since our goal is to identify the groups of tokens corresponding to ADEs as precisely as possible . Two models that we introduce for the first time in this task , SpanBERT and PubMedBERT , achieved the top performance . The former takes advantage of a span - based pretraining objective , while the latter shows that in - domain language data are better used for training the model from scratch , without any general - domain pretraining .", "entities": [[127, 128, "DatasetName", "SMM4H"]]}
{"text": "Automatic extraction of ADE in social media started receiving more attention in the last few years , given the increasing number of users that discuss their drug - related experiences on Twitter and similar platforms . Studies like ; ; Daniulaityte et al ( 2016 ) were among the first to propose machine learning systems for the detection of ADE in social media texts , using traditional feature engineering and word embeddings - based approaches . With the introduction of the SMM4H shared task , methods based on neural networks became a more and more common choice for tackling the task ( Wu et al , 2018 ; Nikhil and Mundra , 2018 ) , and finally , it was the turn of Transformer - based models such as BERT ( Devlin et al , 2019 ) and BioBERT ( Lee et al , 2020 ) , which are the building blocks of most of the top performing systems in the recent competitions Mahata et al , 2019 ; Miftahutdinov et al , 2019 ) . At the same time , the task has been independently tackled also by researchers in Named Entity Recognition , since ADE detection represents a classical case of a challenging task where the entities can be composed by discontinuous spans of text ( Stanovsky et al , 2017 ; Dai et al , 2020 ; Wunnava et al , 2020 ) .", "entities": [[67, 69, "TaskName", "feature engineering"], [70, 72, "TaskName", "word embeddings"], [81, 82, "DatasetName", "SMM4H"], [123, 124, "MethodName", "Transformer"], [129, 130, "MethodName", "BERT"], [191, 194, "TaskName", "Named Entity Recognition"]]}
{"text": "There is little doubt that Transformers ( Vaswani et al , 2017 ) have been the dominant class of NLP systems in the last few years . The \" golden child \" of this revolution is BERT ( Devlin et al , 2019 ) , which was the first system to apply the bidirectional training of a Transformer to a language modeling task . More specifically , BERT is trained with a Masked Language Modeling objective : random words in the input sentences are replaced by a [ MASK ] token and the model attempts to predict the masked token based on the surrounding context . Following BERT 's success , several similar architectures have been introduced in biomedical NLP , proposing different forms of in - domain training or using different corpora ( Beltagy et al , 2019 ; Alsentzer et al , 2019 ; Lee et al , 2020 ; Gu et al , 2020 ) . Some of them already proved to be efficient for ADE detection : for example , the top system of the SMM4H shared task 2019 is based on an ensemble of BioBERTs ( Weissenbacher et al , 2019 ) . Another potentially interesting addition to the library of BERTs for ADE detection is SpanBERT ( Joshi et al , 2019 ) . During the training of Span - BERT , random contiguous spans of tokens are masked , rather than individual words , forcing the model to predict the full span from the tokens at its boundaries . We decided to introduce SpanBERT in our experiments because longer spans and relations between multiple spans of text are a key factor in ADE detection , and thus encoding such information is potentially an advantage . 3 Experimental Settings", "entities": [[36, 37, "MethodName", "BERT"], [57, 58, "MethodName", "Transformer"], [67, 68, "MethodName", "BERT"], [72, 75, "TaskName", "Masked Language Modeling"], [107, 108, "MethodName", "BERT"], [179, 180, "DatasetName", "SMM4H"], [226, 227, "MethodName", "BERT"]]}
{"text": "The datasets chosen for the experiments are two widely used benchmarks . They are annotated for the presence of ADEs at character level : each document is accompanied by list of start and end indices for the ADEs contained in it . We convert these annotations using the IOB annotation scheme for the tokens : B marks the start of a mention , I and O the tokens inside and outside a mention respectively . CADEC ( Karimi et al , 2015a ) contains 1250 posts from the health - related forum \" AskaPatient \" , annotated for the presence of ADEs . We use the splits made publicly available by Dai et al ( 2020 ) . SMM4H is the training dataset for Task 2 of the SMM4H shared task 2019 ( Weissenbacher et al , 2019 ) . It contains 2276 tweets which mention at least one drug name , 1300 of which are positive for the presence of ADEs while the other 976 are negative samples . The competition includes a blind test set , but in order to perform a deeper analysis on the results , we use the training set only . As far as we know there is no official split for the training set al ne , so we partitioned it into training , validation and test sets ( 60:20:20 ) , maintaining the proportions of positive and negative samples . This split and the code for all the experiments are available at https://github.com/AilabUdineGit/ADE . The datasets correspond to different text genres : the tweets of SMM4H are mostly short messages , containing informal language , while the texts of CADEC are longer and structured descriptions . To verify this point , we used the TEXTSTAT Python package to extract some statistics from the texts of the two datasets ( see Appendix A ) .", "entities": [[118, 119, "DatasetName", "SMM4H"], [128, 129, "DatasetName", "SMM4H"], [263, 264, "DatasetName", "SMM4H"]]}
{"text": "For all of the BERT variants , we take into account two versions . The first one simply uses the model to generate a sequence of embeddings ( one for each sub - word token ) , which are then passed to a Linear Layer + Softmax to project them to the output space ( one value for each output label ) and turn them into a probability distribution over the labels . The second version combines the Transformerbased model with a Conditional Random Field ( CRF ) classifier ( Lafferty et al , 2001 ; Papay et al , 2020 ) . The outputs generated by the first version become the input of a CRF module , producing another sequence of subword - level IOB labels . This step aims at denoising the output labels produced by the previous components . The output labels are calculated for sub - word tokens , then we aggregate each set of sub - word labels { i } into a word label L using the first rule that applies : ( i ) if i = O for all i , then L = O ; ( ii ) if i = B for any i , then L = B ; ( iii ) if i = I for any i , then L = I. The aggregated output is a sequence of word - level IOB labels .", "entities": [[4, 5, "MethodName", "BERT"], [43, 45, "MethodName", "Linear Layer"], [46, 47, "MethodName", "Softmax"], [82, 85, "MethodName", "Conditional Random Field"], [86, 87, "MethodName", "CRF"], [115, 116, "MethodName", "CRF"], [132, 133, "TaskName", "denoising"]]}
{"text": "Table 5 is a summary of the information about the version of all Transformer - based models used and their pretraining methods . D Detailed metrics of all the models ( Weissenbacher et al , 2019 ) and take into account \" partial\"matches , in which it is sufficient for a system prediction to partially overlap with the gold annotation to be considered as a true match .", "entities": [[13, 14, "MethodName", "Transformer"]]}
{"text": "IUCL at SemEval - 2016 Task 6 : An Ensemble Model for Stance Detection in Twitter", "entities": [[12, 14, "TaskName", "Stance Detection"]]}
{"text": "We present the IUCL system , based on supervised learning , for the shared task on stance detection . Our official submission , the random forest model , reaches a score of 63.60 , and is ranked 6th out of 19 teams . We also use gradient boosting decision trees and SVM and merge all classifiers into an ensemble method . Our analysis shows that random forest is good at retrieving minority classes and gradient boosting majority classes . The strengths of different classifiers wrt . precision and recall complement each other in the ensemble .", "entities": [[16, 18, "TaskName", "stance detection"], [51, 52, "MethodName", "SVM"]]}
{"text": "Stance detection is a difficult task since it often requires reasoning in order to determine whether an utterance is in favor of or against a specific issue . In the shared task ( see Mohammad et al ( 2016 ) for details about the shared task ) , we interpret it as a variant of sentiment analysis and adopt an approach that combines shallow lexical features with an ensemble of different supervised machine learning classifiers . Previous work has shown that using \" arguing \" features based on an arguing lexicon along with modal verbs and targets identified via syntactic rules ( Somasundaran and Wiebe , 2010 ) ; finding polarized relations between aspects and topics ( Somasundaran and Wiebe , 2009 ) ; adding semantic frames ( Hasan and Ng , 2013 ) and contextual features ( Anand et al , 2011 ) generally improve results . Since some of these features do not generalize across targets ( Anand et al , 2011 ) , and since we have an additional challenge in processing Twitter data , we rely on unigram features and word vectors . This means that our approach is incapable of handling sarcasm or humor . Instead , it provides a robust basis on which we can later add more informative features . Our approach consists of classifiers with a bag of words ( unigrams ) or with word vectors as features . We use three separate classifiers ( SVMs , random forest , gradient boosting decision trees ) and an ensemble classifier ( TiMBL ) . Our official submission is the random forest classifier with word unigrams .", "entities": [[0, 2, "TaskName", "Stance detection"], [55, 57, "TaskName", "sentiment analysis"]]}
{"text": "Preprocessing mostly consists of tokenization . During tokenization , we normalize capitalization , and all punctuation signs are separated except for @ and # , as these symbols indicate hashtags and handles . We extract frequency counts of each token in the entire corpus and in each stance ( Favor , Against , None ) per target for use in the feature selection process . We experimented with TWEEBOPARSER ( Kong et al , 2014 ) , a dependency parser specifically designed for Twitter data , to extract dependency relations among words . We extract POS tags , multiword expressions , and dependency triples from the parses . However , due to the feature sparsity , none of them improved over unigrams . Thus , they are not used in the final systems .", "entities": [[61, 63, "MethodName", "feature selection"]]}
{"text": "One of the major decisions in developing a machine learning system for stance detection lies in the choice of features and of feature representations . Detecting stance in political tweets can be regarded as a form of sentiment analysis for short text , and we assume that different stances of tweets are partially expressed by the choice of words . For example , not mentioning any words that express a polarized attitude indicates that a tweet is most likely a None stance . Tweets are relatively short documents , we use bag of words ( unigrams ) since in this case bigrams and trigrams are likely to be too sparse to be informative . Another possibility would be to follow approaches in sentiment analysis and use sentiment lexicons . However , such lexicons are normally general purpose resources , and domain specific information is not included . In contrast , we need such domain specific knowledge , for example to capture the fact that \" dear lord \" is an indication of a negative stance towards the target Atheism while it may have a different meaning when it occurs for the target Hillary Clinton . Since unigrams include a high number of irrelevant features and also constitute a rather impoverished representation , we use feature selection as well as word vectors in our experiments . Table 1 summarizes the features used for each of our models . We use information gain ( IG ) for feature selection on unigrams . Global refers to global features ( see section 2.2.3 ) . The three classifiers are GBDT , random forest , and SVM ; the ensemble uses their output ( predicted label and its probability ) .", "entities": [[12, 14, "TaskName", "stance detection"], [37, 39, "TaskName", "sentiment analysis"], [122, 124, "TaskName", "sentiment analysis"], [214, 216, "MethodName", "feature selection"], [245, 247, "MethodName", "feature selection"], [271, 272, "MethodName", "SVM"]]}
{"text": "There are issues resulting from the large number of bag - of - words features : 1 ) Not all words are good indicators for stance ; some words occur evenly across the data set . 2 ) Rare words , which are less likely to occur in the test data , do not contribute much . To alleviate these problems , we perform feature selection using information gain ( IG ) . IG esti - mates the amount of information a word gives for the decision on the stance . We choose IG because it has been shown to be robust across different sentiment analysis data sets and across different skewing ratios , compared to other feature selection methods ( Liu et al , 2014 ) . Note that different from its use in decision trees , we use IG as an external filter to select a subset of features , before and independent of any classifiers .", "entities": [[64, 66, "MethodName", "feature selection"], [104, 106, "TaskName", "sentiment analysis"], [117, 119, "MethodName", "feature selection"]]}
{"text": "One limitation of bag - of - words features is that they are very sparse , and they can not handle out - ofvocabulary words properly . Since tweets are relatively short , and the amount of official training data is small , it is likely that the out - of - vocabulary rate is high . Thus we also build models using word vectors , which represent each word with a vector of continuous values . Word vectors have been shown to capture the similarity among words and thus alleviate data sparseness ( Collobert et al , 2011 ) . We have experimented with two different word vector models , word2vec ( Mikolov et al , 2013 ) and GloVe ( Pennington et al , 2014 ) . We have used the pre - trained word2vec obtained from the Google News dataset , which contains a 300 - dimensional vector representation for 3 million words and phrases 1 , and the pre - trained GloVe , which is obtained from 2 billion tweets and has a 250dimensional vector representation for 1.2 million words and phrases 2 . To construct a representation for a tweet , we look up a word in the word vectors model , then average all vectors for words to produce a vector representation for the tweet . For example , to represent a 15 word tweet using word2vec , we first obtain a 300dimensional vector for each word , then average all 15 vectors . This means that the word order is lost and the representation constitutes a \" bag of vectors \" .", "entities": [[120, 121, "MethodName", "GloVe"], [140, 141, "DatasetName", "Google"], [165, 166, "MethodName", "GloVe"]]}
{"text": "We have performed a comparison of both word vector variants in a 5 - fold cross validation experiment on the training data . Table 2 summarizes the results . We can see that GloVe performs consistently better than word2vec except for Feminist where word2vec is 0.6 % better than GloVe . We assume that this performance gap is mainly caused by the domain difference from which the word vectors are obtained : We used GloVe pretrained on tweets and word2vec pre - trained on news . This leads to a higher number of out - of - vocabulary words for the word2vec model . In other words , GloVe provides a broader coverage for this data set .", "entities": [[33, 34, "MethodName", "GloVe"], [49, 50, "MethodName", "GloVe"], [74, 75, "MethodName", "GloVe"], [108, 109, "MethodName", "GloVe"]]}
{"text": "The bag - of - words features used in the classifiers ( see section 3 ) assume that the words are considered independently . However , in many situations , it is the distributions of positively and negatively oriented words that determine the final stance of a tweet . A low coverage of words from these two distributions is a strong indicator for None stance as well . This is especially important for the ensemble classifier . For this reason , we have developed two additional features for the ensemble , which capture information from these two distributions : one feature for positive orientation and one for negative orientation . The feature is a numeric score , representing the association of a tweet with positive or negative stance respectively . The positive orientation is calculated based on the following equation : score pos T = 1 | T | w\u2282T f req ( w ) in POS w \u2282V f req ( w ) in POS where T is a tweet , | T | is the tweet length excluding stop words . V is the entire vocabulary . F req ( w ) is the frequency count of w in the following set . P OS is the set of all positive tweets . This score measures for each word ( its lemma ) the association with positive stance , sums up all words in the tweet , and normalizes the score by the tweet length . The score for the negative orientation is calculated accordingly . The None orientation is not calculated since it is already represented by the absence of positively or negatively oriented words . I.e. , we assume that if a tweet has low positive and negative orientations , it indicates a None stance .", "entities": [[223, 224, "DatasetName", "lemma"]]}
{"text": "Table 4 shows the results of the three individual classifiers as well as of the two ensemble model variants , one combining only the individual classifiers ' outputs ( EnsembleNG ) , the other one ( EnsembleG ) including also the global features ( see section 2.2.3 ) . These results show that the GBDT approach using GloVe reaches the highest result ( 64.64 ) among the individual classifiers . The random forest classifier , which constitutes our official submission is about 1 percentage point lower ( 63.60 ) , and the SVM classifier is about 1.5 percentage points below that ( 61.93 ) . A closer look at the ensemble variants shows that using the global features has a detrimental effect across all targets , most likely because this information is too coarse . The other ensemble classifier improves over GBDT by 1.5 percentage points ( 66.14 ) . This shows that we can benefit from important information from all individual classifiers .", "entities": [[57, 58, "MethodName", "GloVe"], [92, 93, "MethodName", "SVM"]]}
{"text": "In this shared task , we regard stance detection as a special case of sentiment analysis , using supervised classifiers and bag of unigrams and word vectors as features . Our submitted system is based on a random forest classifier because of its capability to handle overfitting and to generalize over the test data . Since the amount of available training data is small , random forest 's ability to sample data points and fea - ture subspaces reduces data sparsity . The submitted system has an official score of 63.60 and ranked 6th out of 19 teams . We also experimented with other single models ( SVM and GBDT ) and with an ensemble model built on a memory - based classifier . The GBDT model using GloVe word vectors reaches a higher score of 64.64 , which may be a result of the word vectors ' capability to capture similarities among words , which helps in dealing with out - of - vocabulary words . The ensemble model that aggregates information from the three individual classifiers reaches the highest performance of 66.14 . Our hypothesis is that different strengths ( e.g. , good performance for minority / majority classes ) from individual models complement each other in the ensemble . However a closer look at the performance of all classifiers and ensembles across individual targets shows that no system reaches consistently good results across all targets . The best performing ensemble ( EnsembleNG ) outperforms individual classifiers only for Abortion and Feminist ; for the other targets , random forest or GBDT reach higher accuracies . Some of the variation in system performance can be explained by the class imbalance present in the data sets for the different targets , but further work is required to identify other factors . Finally , it is worth pointing out that our approach to stance detection utilizes very surface oriented features . To boost performance , we may need to develop methods that incorporate inference , entailment , and world knowledge , for example , to handle cases such as \" keep H. out of the white house \" .", "entities": [[7, 9, "TaskName", "stance detection"], [14, 16, "TaskName", "sentiment analysis"], [107, 108, "MethodName", "SVM"], [128, 129, "MethodName", "GloVe"], [313, 315, "TaskName", "stance detection"]]}
{"text": "AStarTwice at SemEval - 2021 Task 5 : Toxic Span Detection using RoBERTa - CRF , Domain Specific Pre - Training and Self - Training", "entities": [[12, 13, "MethodName", "RoBERTa"], [14, 15, "MethodName", "CRF"]]}
{"text": "In recent years there has been an exponential increase in the use of social network platforms . With rising abusive language and hate on such platforms , it is more important than ever to maintain online conversations constructive and inclusive . This problem can be tackled by filtering toxic comments / posts . The massive volume of data generated at a fast pace makes manually filtering each comment complicated and time - consuming . This process can be automated by modelling it as a supervised classification problem . A similar task was proposed in SemEval - 2019 Task 6 : Identifying and Categorizing Offensive Language in Social Media ( OffensEval ) ( Zampieri et al , 2019 ) . Most of the top - ranked teams in this task used transformer language models ( Liu et al , 2019a ; Zhu et al , 2019 ; Pelicon et al , 2019 ; Wu et al , 2019 ) or an ensemble of CNN and RNN ( Mahata et al , 2019 ; Mitrovi\u0107 et al , 2019 ) to classify the sentences . The problem with the above approach is that it does n't give moderators much knowledge about the reason for a sentence 's toxicity . Highlighting toxic spans can help human moderators who frequently deal with long comments and prefer attribution rather than just an unexplained toxicity score . SemEval 2021 Task 5 : Toxic Span Detection ( Pavlopoulos et al , 2021 ) gave a chance to propose NLP systems to solve this problem . The task is concerned with developing systems that can recognise spans that contribute to the text 's toxicity . This task had a few challenges . Since the samples were from an online commenting platform , they were grammatically incorrect and consisted of many out of vocabulary words . The noisy and ambiguous structure of comments significantly hampers the performance of general NLP models . The training dataset had a little less than 8000 samples . Thus , there was a need to select systems that can produce meaningful results , even with a limited number of training samples . Undoubtedly , the hardest part is to identify spans that can account for the toxicity of the sample . The span could be as small as a single token and as large as the sample itself . The linguistic variations in the usage of words and phrases make such attribution even more difficult . We formulated the task as a sequence tagging problem and used RoBERTa ( Liu et al , 2019b ) , a pre - trained Transformer - based ( Vaswani et al , 2017 ) language model as our base model . We further pre - trained RoBERTa on the Civil Comments Dataset as a masked language model ( Devlin et al , 2018 ) to create a domain - specific model . We employed a Conditional Random Field ( CRF ) layer ( Lafferty et al , 2001 ) for predicting the most probabilistic sequence of labels for each input sequence . We also applied a few pre - processing steps , which lead to significant performance improvements . Lastly , we leveraged the semi - supervised learning technique of self - training ( Yarowsky , 1995 ; Liao and Veeramachaneni , 2009 ; Jurkiewicz et al , 2020 ) by training our model on the manually annotated dataset and using it to further extend the training set by generating toxic spans for other unannotated datasets . We have made our system 's implementation available through GitHub 1 . The rest of the paper is organised as follows . Section 2 explains our model implementation in detail . Section 3 and 4 presents our experimental setup and achieved results , respectively . In section 4 , we perform error analysis , followed by conclusions in the last section .", "entities": [[19, 21, "TaskName", "abusive language"], [423, 424, "MethodName", "RoBERTa"], [436, 437, "MethodName", "Transformer"], [458, 459, "MethodName", "RoBERTa"], [487, 490, "MethodName", "Conditional Random Field"], [491, 492, "MethodName", "CRF"]]}
{"text": "We formulated the task as a token level sequence tagging problem where we classify each token as Begin , Inside or Outside ( BIO scheme ) . Having begin and end tags helps formulate the notion of spans better and creates dependencies between various tokens of a toxic span ( Singh et al , 2020 ) , allowing it to perform better than other alternatives such as IO ( Inside Outside ) . Pre - Processing : We applied a few preprocessing steps before fine - tuning RoBERTa on the input text samples . First , we converted all the text samples to lowercase . We observed that punctuation marks did not add any significant information to the semantics of a sentence . Therefore , as a part of the data cleaning , punctuation marks such as commas and dashes were removed . We also collapsed multiple space characters into a single space . Model : We provided the text samples as input to our pre - trained RoBERTa ( p ) model to get 768dimensional contextual embeddings for each token . These contextual embeddings were passed through two dense layers of 512 and 128 dimensions , followed by a Conditional Random Fields ( CRF ) ( Lafferty et al , 2001 ) layer with three labels ( B - Begin , I - Inside or O - Outside ) . The CRF layer models the correlation between the labels predicted for the individual tokens . It receives the logits for each input token and predicts the most probabilistic sequence of labels for each input sequence . Figure 1 shows our model architecture .", "entities": [[87, 88, "MethodName", "RoBERTa"], [168, 169, "MethodName", "RoBERTa"], [204, 205, "MethodName", "CRF"], [232, 233, "MethodName", "CRF"]]}
{"text": "There has been a growing interest in the compression of pre - trained language models . We consider three varieties of methods : distillation , pruning , and structured pruning . Knowledge distillation , introduced by Hinton et al ( 2015 ) , is a popular compression technique . Researchers have applied this method to a variety of NLP models ( Tang et al , 2019 ; Sun et al , 2019 ; Turc et al , 2019 ) . Distillation has been used to obtain significantly smaller BERT models achieving competitive performances . Sanh et al ( 2019 ) distills BERT into shallower students during the pre - training stage and optionally during the finetuning stage . MobileBERT ( Sun et al , 2020 ) and TinyBERT ( Jiao et al , 2019 ) are obtained thanks to a layer - wise distillation strategy . While the distillation of former is task - agnostic , the one used to obtain the latter is task - specific . Other previous work has focused on unstructured pruning ( LeCun et al , 1989 ; Han et al , 2015 ; Frankle and Carbin , 2018 ) . When targeting transformer models , it is typical to select the weights to prune based on their magnitude ( Gordon et al , 2020 ) , or by computing an importance score using a firstorder method ( Sanh et al , 2020 ) . While these methods allow for a significant reduction in model size , specialized hardware is required to make use of the resulting unstructured sparse matrices in order to speed up inference . In contrast , structured pruning removes coherent groups of weights ( Murray and Chiang , 2015 ; See et al , 2016 ; Joulin et al , 2016 ; Fan et al , 2020 ; Sajjad et al , 2020 ) . Recent works ( Michel et al , 2019 ; Voita et al , 2019 ) show that some heads can be removed without significant degradation in performance , leading to the conclusion that most heads provide redundant information . Other authors have worked on combining matrix factorization and weight pruning . While Mao et al ( 2020 ) combine SVD - based matrix factorization with unstructured pruning , use structured pruning in order to reduce the rank . Related to our approach , Kim and Awadalla ( 2020 ) and McCarley ( 2019 ) both apply structured pruning on the heads of the multi - head attention ( MHA ) and on the inner - layer nodes of the feed - forward network ( FFN ) . The former uses predefined pruning ratios , shared across all layers , in order to select the modules to prune after sorting them given an importance score . McCarley ( 2019 ) compares dif - ferent methods to compute the prunable module masks and find L0 regularization to perform the best .", "entities": [[31, 33, "MethodName", "Knowledge distillation"], [88, 89, "MethodName", "BERT"], [101, 102, "MethodName", "BERT"], [118, 119, "MethodName", "MobileBERT"], [373, 374, "DatasetName", "SVD"], [417, 421, "MethodName", "multi - head attention"]]}
{"text": "In this work , we extend movement pruning to work on blocks of local parameters . Specifically , each matrix in the transformer is partitioned into fixedsized blocks . This setting goes beyond the arbitrary pruning of unstructured methods , with the goal of encouraging the data locality closer to what would be needed for efficiency . 2 Our approach is extremely simple . For each parameter matrix W R M \u00d7N , we assume a fixedsized block structure ( M , N ) . Each of these blocks acts as an individual group in the regularization with a shared score parameter derived from the corresponding score matrix S R M / M \u00d7N / N . Computing the masked weight is done by expanding the thresholded values , i.e. W i , j = W i , j * M ( S ) i / M , j / N As in past work , this model is trained with distillation to match the performance of a teacher model . Unlike other distillation approaches that require fully specifying the new model structure , our method only requires the size and shapes of the blocks , i.e. the set of ( M , N ) for each parameter matrix in the model . If blocks are too large , then they are difficult to prune , but if they are too small they do not support efficient inference . To reduce the search space , we will limit ourselves to test ( M , N ) att and ( M , N ) ff : the same block size will be used for all layers for attention weights W q , W k , W v and W o on one hand , and for the feed - forward weights W 1 and W 2 on the other hand . We split the movement pruning regularization term into : \u03bb att \u03c3 ( S att ) + \u03bb ffn \u03c3 ( S ffn ) This allows us to take into account the difference in terms of gradient received by the score parameters . To reduce further the search space , we will test on two kinds of blocks : ( 32 , 32 ) : square blocks ( Block ) ( 1 , d model ) and ( d model , 1 ) : dimension pruning on paired FFN rows and columns ( Dim ) These block sizes allow for efficient models : blocks of size at least ( 16 , 16 ) are efficient to compute with appropriate GPU kernels , whereas full rows , columns or heads can be entirely removed from the matrix : the remaining matrix is then dense . We also include two additional baseline block types used to verify the approach : ( 2 n , 2 n ) , n [ 2 , 5 ] : smaller power of two square block sizes to study the impact of size on performance ( Block ) ( d model n heads , d model ) : for attention heads ( Heads ) The first considers small blocks , and the second considers very large functional blocks .", "entities": [[6, 8, "MethodName", "movement pruning"], [313, 315, "MethodName", "movement pruning"]]}
{"text": "We are using a minimal set of hyperparameters . The ratio of \u03bb att and \u03bb ffn is fixed by the relative sizes . We performed a few experiments with differ - ent values fixed manually for these parameters , but their influence is minor . The main hyperparameter is the number of training epochs . For SQuAD v1.1 , we are using 20 epochs instead of typically 2 for BERT models . This means a fine - tuning is taking about 12h with our method instead of 45mn with a standard finetuning setup . This number has to be large enough to let pruning happen slowly enough for a given task . A warming up phase and a post - pruning cooldown phase are helpful , but their exact length has not a large impact on final performance . We believe the training time is less important than the inference time for energy consideration , as inference is performed repeatedly . Our method is optimizing inference by a large factor : the training energy is potentially recouped by a large margin with inference savings . Finally , the checkpoints created during the experiments are available on an AWS S3 bucket , with their metadata and training parameters , totaling 3 TB of data , to facilitate reproduction of our results and to make it possible to study further the behavior of those models . Code for experiments , analysis , and tools to prepare the present paper are available on GitHub ( see Appendix A ) .", "entities": [[57, 58, "DatasetName", "SQuAD"], [70, 71, "MethodName", "BERT"]]}
{"text": "We report experimental results with the addition of a teacher distillation step as previous work showed this boosts movement pruning at little cost . In this section , we conduct an ablation study to evaluate the impact of distillation using a BERT - base teacher .", "entities": [[18, 20, "MethodName", "movement pruning"], [41, 42, "MethodName", "BERT"]]}
{"text": "We have shown that we can extract small pruned models that are at an equivalent or better than distilled networks . This approach can be done during fine - tuning and not pre - training . The method does not resort to techniques such as data augmentation or architecture search , and it works on a diverse set of tasks and base models . As better and larger models are published at an increasing pace , we can rely on a simple and robust method to accelerate them on specific tasks without sacrificing accuracy and distribute these models easily while keeping most of the original model accuracy .", "entities": [[45, 47, "TaskName", "data augmentation"], [93, 94, "MetricName", "accuracy"], [106, 107, "MetricName", "accuracy"]]}
{"text": "The hyperparameters of the experiments are available as JSON files ( one file per task ) in the same repository : each entry contains all the information to fine - tune and prune the model , its evaluation results , and detailed statistics about its final sparsity . For example , the SQuAD V1 checkpoints referenced in this paper are listed with the hyperparameters and related information .", "entities": [[52, 53, "DatasetName", "SQuAD"]]}
{"text": "Some of the models we produced during this research can be used directly from the Hugging Face model hub . The other models and the checkpoints , including the intermediary ones that were saved during training , are available on Amazon S3 .", "entities": [[16, 18, "TaskName", "Face model"]]}
{"text": "We introduce the first dataset for human edits of machine - generated visual stories and explore how these collected edits may be used for the visual story post - editing task . The dataset , VIST - Edit 1 , includes 14 , 905 humanedited versions of 2 , 981 machine - generated visual stories . The stories were generated by two state - of - the - art visual storytelling models , each aligned to 5 human - edited versions . We establish baselines for the task , showing how a relatively small set of human edits can be leveraged to boost the performance of large visual storytelling models . We also discuss the weak correlation between automatic evaluation scores and human ratings , motivating the need for new automatic metrics .", "entities": [[35, 38, "DatasetName", "VIST - Edit"], [69, 71, "TaskName", "visual storytelling"], [107, 109, "TaskName", "visual storytelling"]]}
{"text": "Professional writers emphasize the importance of editing . Stephen King once put it this way : \" to write is human , to edit is divine . \" ( King , 2000 ) Mark Twain had another quote : \" Writing is easy . All you have to do is cross out the wrong words . \" ( Twain , 1876 ) Given that professionals revise and rewrite their drafts intensively , machines that generate stories may also benefit from a good editor . Per the evaluation of the first Visual Storytelling Challenge ( Mitchell et al , 2018 ) , the ability of an algorithm to tell a sound story is still far from that of a human . Users will inevitably need to edit generated stories before putting them to real uses , such as sharing on social media . We introduce the first dataset for human edits of machine - generated visual stories , VIST - Edit , and explore how these collected edits may be used for the task of visual story post - editing ( see Figure 1 ) . The original visual storytelling ( VIST ) task , as introduced by Huang et al ( 2016 ) , takes a sequence of five photos as input and generates a short story describing the photo sequence . Huang et al also released the VIST dataset , containing 20 , 211 photo sequences , aligned to human - written stories . On the other hand , the automatic postediting task revises the story generated from visual storytelling models , given both a machinegenerated story and a photo sequence . Automatic post - editing treats the VIST system as a black box that is fixed and not modifiable . Its goal is to correct systematic errors of the VIST system and leverage the user edit data to improve story quality . In this paper , we ( i ) collect human edits for machine - generated stories from two different state - of - the - art models , ( ii ) analyze what people edited , and ( iii ) advance the task of visual story post - editing . In addition , we establish baselines for the task , and discuss the weak correlation between automatic evaluation scores and human ratings , motivating the need for new metrics .", "entities": [[90, 92, "TaskName", "Visual Storytelling"], [157, 160, "DatasetName", "VIST - Edit"], [187, 189, "TaskName", "visual storytelling"], [190, 191, "DatasetName", "VIST"], [228, 229, "DatasetName", "VIST"], [259, 261, "TaskName", "visual storytelling"], [273, 277, "TaskName", "Automatic post - editing"], [279, 280, "DatasetName", "VIST"], [301, 302, "DatasetName", "VIST"]]}
{"text": "The visual story post - editing task is related to ( i ) automatic post - editing and ( ii ) stylized visual captioning . Automatic post - editing ( APE ) revises the text generated typically from a machine translation ( MT ) system , given both the source sentences and translated sentences . Like the proposed VIST post - editing task , APE aims to correct the systematic errors of MT , reducing translator workloads and increasing productivity ( Astudillo et al , 2018 ) . Recently , neural models have been applied to APE in a sentence - to - sentence manner ( Libovick\u1ef3 et al , 2016 ; Junczys - Dowmunt and Grundkiewicz , 2016 ) , differing from previous phrase - based models that translate and reorder phrase segments for each sentence , such as ( Simard et al , 2007 ; B\u00e9chara et al , 2011 ) . More sophisticated sequence - to - sequence models with the attention mechanism were also introduced ( Junczys - Dowmunt and Grundkiewicz , 2017 ; Libovick\u1ef3 and Helcl , 2017 ) . While this line of work is relevant and encouraging , it has not explored much in a creative writing context . It is noteworthy that Roemmele et al previously developed an online system , Creative Help , for collecting human edits for computer - generated narrative text ( Roemmele and Gordon , 2018b ) . The collected data could be useful for story APE tasks . Visual story post - editing could also be considered relevant to style transfer on image captions . Both tasks take images and source text ( i.e. , machine - generated stories or descriptive captions ) as inputs and generate modified text ( i.e. , postedited stories or stylized captions ) . End - to - end neural models have been applied to the transfer styles of image captions . For example , StyleNet , an encoder - decoder - based model trained on paired images and factual captions together with an unlabeled stylized text corpus , can transfer descriptive image captions to creative captions , e.g. , humorous or romantic ( Gan et al , 2017 ) . Its advanced version with an attention mechanism , SemStyle , was also introduced ( Mathews et al , 2018 ) . In this paper , we adopt the APE approach to treat preand post - edited stories as parallel data instead of the style transfer approach that omits this parallel relationship during model training .", "entities": [[13, 17, "TaskName", "automatic post - editing"], [25, 29, "TaskName", "Automatic post - editing"], [30, 31, "DatasetName", "APE"], [39, 41, "TaskName", "machine translation"], [58, 59, "DatasetName", "VIST"], [64, 65, "DatasetName", "APE"], [96, 97, "DatasetName", "APE"], [248, 249, "DatasetName", "APE"], [262, 264, "TaskName", "style transfer"], [397, 398, "DatasetName", "APE"], [412, 414, "TaskName", "style transfer"]]}
{"text": "Obtaining Machine - Generated Visual Stories This VIST - Edit dataset contains visual stories gen - erated by two state - of - the - art models , GLAC and AREL . GLAC ( Global - Local Attention Cascading Networks ) ( Kim et al , 2018 ) achieved the highest human evaluation score in the first VIST Challenge ( Mitchell et al , 2018 ) . We obtain the pre - trained GLAC model provided by the authors via Github and run it on the entire VIST test set and obtain 2 , 019 stories . AREL ( Adversarial REward Learning ) was the earliest available implementation online , and achieved the highest METEOR score on public test set in the VIST Challenge . We also acquire a small set of human edits for 962 AREL 's stories generated using VIST test set , collected by Hsu et al ( 2019 ) . Crowdsourcing Edits For each machinegenerated visual story , we recruit five crowd workers from Amazon Mechanical Turk ( MTurk ) to revise it ( at $ 0.12 / HIT , ) respectively . We instruct workers to edit the story \" as if these were your photos , and you would like using this story to share your experience with your friends . \" We also ask workers to stick with the photos of the original story so that workers would not ignore the machine - generated story and write a new one from scratch . Figure 2 shows the interface . For GLAC , we collect 2 , 019 \u00d7 5 = 10 , 095 edited stories in total ; and for AREL , 962 \u00d7 5 = 4 , 810 edited stories have been collected by Hsu et al ( 2019 ) . Data Post - processing We tokenize all stories using CoreNLP ( Manning et al , 2014 ) and replace all people names with generic [ male / female ] tokens . Each of GLAC and AREL set is released as training , validation , and test following an 80 % , 10 % , 10 % split , respectively .", "entities": [[7, 10, "DatasetName", "VIST - Edit"], [34, 38, "MethodName", "Global - Local Attention"], [57, 58, "DatasetName", "VIST"], [87, 88, "DatasetName", "VIST"], [114, 115, "DatasetName", "METEOR"], [122, 123, "DatasetName", "VIST"], [141, 142, "DatasetName", "VIST"]]}
{"text": "We analyze human edits for GLAC and AREL . First , crowd workers systematically increase lexical diversity . We use type - token ratio ( TTR ) , the ratio between the number of word types and the number of tokens , to estimate the lexical diversity of a story ( Hardie and McEnery , 2006 ) . Figure 3 shows significant ( p<.001 , paired t - test ) positive shifts of TTR for both AREL and GLAC , which confirms the findings in Hsu et al ( 2019 ) . Figure 3 also indicates that GLAC generates stories with higher lexical diversity than that of AREL . Second , people shorten AREL 's stories but lengthen GLAC 's stories . We calculate the average number of Part - Of - Speech ( POS ) tags for tokens in each story using the python NLTK ( Bird et al , 2009 ) package , as shown in Table 1 . We also find that the average number of tokens in an AREL story ( 43.0 , SD=5.0 ) decreases ( 41.9 , SD=5.6 ) after human editing , while that of GLAC ( 35.0 , SD=4.5 ) increases ( 36.7 , SD=5.9 ) . Hsu has observed that people often replace \" determiner / article + noun \" phrases ( e.g. , \" a boy \" ) with pronouns ( e.g. , \" he \" ) in AREL stories ( 2019 ) . However , this observation can not explain the story lengthening in GLAC , where each story on average has an increased 0.9 nouns after editing . Given the average per - story edit distances ( Levenshtein , 1966 ; Damerau , 1964 ) for AREL ( 16.84 , SD=5.64 ) and GLAC ( 17.99 , SD=5.56 ) are similar , this difference is unlikely to be caused by deviation in editing amount . Deleting extra words requires much less time than other editing operations ( Popovic et al , 2014 ) . Per Figure 3 , AREL 's stories are much more repetitive . We further analyze the type - token ratio for nouns ( T T R noun ) and find AREL generates duplicate nouns . The average T T R noun of an AREL 's story is 0.76 while that of GLAC is 0.90 . For reference , the average T T R noun of a human - written story ( the entire VIST dataset ) is 0.86 . Thus , we hypothesize workers prioritized their efforts in deleting repetitive words for AREL , resulting in the reduction of story length .", "entities": [[128, 131, "DatasetName", "Part - Of"], [409, 410, "DatasetName", "VIST"]]}
{"text": "We report baseline experiments on the visual story post - editing task in Table 2 . AREL 's post - editing models are trained on the augmented AREL training set and evaluated on the AREL test set of VIST - Edit , and GLAC 's models are tested using GLAC sets , too . Figure 4 shows examples of the output . Human evaluations ( Table 2 ) indicate that the post - editing model improves visual story quality .", "entities": [[38, 41, "DatasetName", "VIST - Edit"]]}
{"text": "Two neural approaches , Long short - term memory ( LSTM ) and Transformer , are used as baselines , where we experiment using ( i ) text only ( T ) and ( ii ) both text and images ( T+I ) as inputs . LSTM An LSTM seq2seq model is used ( Sutskever et al , 2014 ) . For the text - only setting , the original stories and the human - edited stories are treated as source - target pairs . For the text - image setting , we first extract the image features using the pre - trained ResNet - 152 model and represent each image as a 2048 - dimensional vector . We then apply a dense layer on image features in order to both fit its dimension to the word embedding and learn the adjusting transformation . By placing the image features in front of the sequence of text embedding , the input sequence becomes a matrix R ( 5+len ) \u00d7dim , where len is the text sequence length , 5 means 5 photos , and dim is the dimension of the word embedding . The input sequence with both image information and text information is then encoded by LSTM , identical as in the text - only setting .", "entities": [[4, 9, "MethodName", "Long short - term memory"], [10, 11, "MethodName", "LSTM"], [13, 14, "MethodName", "Transformer"], [46, 47, "MethodName", "LSTM"], [48, 49, "MethodName", "LSTM"], [49, 50, "MethodName", "seq2seq"], [103, 104, "MethodName", "ResNet"], [113, 114, "DatasetName", "2048"], [207, 208, "MethodName", "LSTM"]]}
{"text": "We also use the Transformer architecture ( Vaswani et al , 2017 ) ) , Written - by - a - Human ( \" This story sounds like it was written by a human . \" ) , Visually - Grounded , and Detailed . We take the average of the five judgments as the final score for each story . LSTM ( T ) improves all aspects for stories by AREL , and improves \" Focus \" and \" Human - like \" aspects for stories by GLAC . enriched embedding . It is noteworthy that the position encoding is only applied on text embedding . The input matrix R ( len+5 ) \u00d7dim is then passed into the Transformer as in the text - only setting .", "entities": [[4, 5, "MethodName", "Transformer"], [61, 62, "MethodName", "LSTM"], [120, 121, "MethodName", "Transformer"]]}
{"text": "Data Augmentation In order to obtain sufficient training samples for neural models , we pair lessedited stories with more - edited stories of the same photo sequence to augment the data . In VIST - Edit , five human - edited stories are collected for each photo sequence . We use the human - edited stories that are less edited - measured by its Normalized Damerau - Levenshtein distance ( Levenshtein , 1966 ; Damerau , 1964 ) to the original story - as the source and pair them with the stories that are more edited ( as the target . ) This data augmentation strategy gives us in total fifteen ( 5 2 +5 = 15 ) training samples given five human - edited stories . Human Evaluation Following the evaluation procedure of the first VIST Challenge ( Mitchell et al , 2018 ) , for each visual story , we recruit five human judges on MTurk to rate it on six aspects ( at $ 0.1 / HIT . ) We take the average of the five judgments as the final scores for the story . Table 2 shows the results . The LSTM using text - only input outperforms all other baselines . It improves all six aspects for stories by AREL , and improves \" Focus \" and \" Human - like \" aspects for stories by GLAC . These results demonstrate that a relatively small set of human edits can be used to boost the story quality of an existing large VIST model . Table 2 also suggests that the quality of a post - edited story is heavily decided by its pre - edited version . Even after editing by human editors , AREL 's stories still do not achieve the quality of pre - edited stories by GLAC . The inefficacy of image features and Transformer model might be caused by the small size of VIST - Edit . It also requires further research to develop a post - editing model in a multimodal context .", "entities": [[0, 2, "TaskName", "Data Augmentation"], [33, 36, "DatasetName", "VIST - Edit"], [103, 105, "TaskName", "data augmentation"], [136, 137, "DatasetName", "VIST"], [195, 196, "MethodName", "LSTM"], [256, 257, "DatasetName", "VIST"], [312, 313, "MethodName", "Transformer"], [322, 325, "DatasetName", "VIST - Edit"]]}
{"text": "VIST - Edit , the first dataset for human edits of machine - generated visual stories , is introduced . We argue that human editing on machinegenerated stories is unavoidable , and such edited data can be leveraged to enable automatic postediting . We have established baselines for the task of visual story post - editing , and have motivated the need for a new automatic evaluation metric .", "entities": [[0, 3, "DatasetName", "VIST - Edit"]]}
{"text": "Evaluation of acoustic word embeddings", "entities": [[3, 5, "TaskName", "word embeddings"]]}
{"text": "Recently , researchers in speech recognition have started to reconsider using whole words as the basic modeling unit , instead of phonetic units . These systems rely on a function that embeds an arbitrary or fixed dimensional speech segments to a vector in a fixed - dimensional space , named acoustic word embedding . Thus , speech segments of words that sound similarly will be projected in a close area in a continuous space . This paper focuses on the evaluation of acoustic word embeddings . We propose two approaches to evaluate the intrinsic performances of acoustic word embeddings in comparison to orthographic representations in order to evaluate whether they capture discriminative phonetic information . Since French language is targeted in experiments , a particular focus is made on homophone words .", "entities": [[4, 6, "TaskName", "speech recognition"], [83, 85, "TaskName", "word embeddings"], [97, 99, "TaskName", "word embeddings"]]}
{"text": "Recent studies have started to reconsider the use of whole words as the basic modeling unit in speech recognition and query applications , instead of phonetic units . These systems are based on the use of acoustic word embedding , which are projection of arbitrary or fixed dimensional speech segments into a continuous space , in a manner that preserve acoustic similarity between words . Thus , speech segments of words that sound similarly will have similar embeddings . Acoustic word embedding were successfully used in a queryby - example search system ( Kamper et al , 2015 ; Levin et al , 2013 ) and in a ASR lattice re - scoring system ( Bengio and Heigold , 2014 ) . The authors in ( Bengio and Heigold , 2014 ) proposed an approach to build acoustic word em - beddings from an orthographic representation of the word . This paper focuses on the evaluation of these acoustic word embeddings . We propose two approaches to evaluate the intrinsic performances of acoustic word embeddings in comparison to orthographic representations . In particular we want to evaluate whether they capture discriminative information about their pronunciation , approximated by their phonetic representation . In our experiments , we focus on French language whose particularity is to be rich of homophone words . This aspect is also studied in this work .", "entities": [[17, 19, "TaskName", "speech recognition"], [159, 161, "TaskName", "word embeddings"], [173, 175, "TaskName", "word embeddings"]]}
{"text": "Table 1 : Example of the content of the three lists . In the case of the orthographic and phonetic similarity tasks , the evaluation of the acoustic embeddings is performed by ranking the pairs according to their cosine similarities and measuring the Spearman 's rank correlation coefficient ( Spearman 's \u03c1 ) . This approach is used in ( Gao et al , 2014 ; Ji et al , 2015 ; Levy et al , 2015 ; Ghannay et al , 2016 ) to evaluate the linguistic word embeddings on similarity tasks , in which the similarity scores are attributed by human annotators . For the homophone detection task , the evaluation is performed in terms of precision . For each word w in the Homophones list , let L H ( w ) be the list of k homophones of the word w , and L H neighbour ( w ) be the list of k nearest neighbours extracted based on the cosine similarity and L H f ound ( w ) be the intersection between L H ( w ) and L H neighbour ( w ) , that corresponds to the list of homophones found of the word w. The precision P w of the word w is defined as : P w = | L H f ound ( w ) | | L H ( w ) | ( 3 ) where | . | refers to the size of a list . We define the overall homophone detection precision on the Homophones list as the average of the P w : P = N i=1 P w i N ( 4 ) where N is the number of candidate words which have a none - empty Homophones list . 3 Experiments on acoustic word embeddings", "entities": [[88, 90, "TaskName", "word embeddings"], [300, 302, "TaskName", "word embeddings"]]}
{"text": "The embeddings we evaluate are built from two different vocabularies : the one used to train the neural network models ( CNN and DNN ) , composed of 52k words present in the manual transcriptions of the 488 hours of audio ; and another one composed of 160k words . The words present in the 52k vocabulary are nearly all present in the 160k vocabulary . The evaluation sets described in section 2.2 are generated from these two vocabularies : in the 52k vocabulary , all the acoustic word embeddings w + are related to words which have been observed during the training of the CNN . This means that at least two acoustic signal embeddings have been computed from the audio for each one of these words ; in the 160k vocabulary , about 110k acoustic word embeddings were computed for words never observed in the audio data .", "entities": [[88, 90, "TaskName", "word embeddings"], [137, 139, "TaskName", "word embeddings"]]}
{"text": "The quantitative evaluation of the acoustic word embeddings w + is performed on orthographic similarity , phonetic similarity , and homophones detection tasks . Results are summarized in table 2 They show that the acoustic word embeddings w + are more relevant for the phonetic similarity task , while o + are obviously the best ones on the orthographic similarity task . These results show that the projection of the orthographic embeddings o + into the acoustic embeddings space s changes their properties , since they have captured more information about word pronunciation while they have lost information about spelling . So , in addition to making possible a measure of similarity distance between the acoustic signal ( represented by s ) and a word ( represented by w + ) , acoustic word embeddings are better than orthographic ones to measure the phonetic proximity between two words . For the homophone detection task , the Homophones list is computed from the 160k vocabulary : that results to 53869 homophone pairs in total . The 52k vocabulary contains 13561 homophone pairs which are included in the pairs present in the 160k vocabulary . As we can see , the w + acoustic embeddings outperform the orthographic ones on this task on the two data sets . This confirms that acoustic word embeddings have captured additional information about word pronunciation than the one carried by orthographic word embeddings . For this task we can not compare the results between the two vocabularies , since the precision measure is dependent to the number of events . For the Spearman 's correlation , a comparison is roughly possible and results show that the way to compute w + is effective to generalize this computation to word not observed in the audio training data .", "entities": [[6, 8, "TaskName", "word embeddings"], [35, 37, "TaskName", "word embeddings"], [133, 135, "TaskName", "word embeddings"], [220, 222, "TaskName", "word embeddings"], [235, 237, "TaskName", "word embeddings"]]}
{"text": "To give more insight into the difference of the quality of the orthographic word embeddings o + and the acoustic ones w + , we propose an empirical comparison by showing the nearest neighbours of a given set of words . Table 3 shows examples of such neighbour . It can be seen that , as expected , neighbour of any given word share the same spelling with it when they are induced by the orthographic embeddings and arguably sound like it when they are induced by the acoustic word ones .", "entities": [[13, 15, "TaskName", "word embeddings"]]}
{"text": "In this paper , we have investigated the intrinsic evaluation of acoustic word embeddings . These latter offer the opportunity of an a priori acoustic representation of words that can be compared , in terms of similarity , to an embedded representation of the audio signal . We have proposed two approaches to evaluate the performances of these acoustic word embeddings and compare them to their orthographic embeddings : orthographic and phonetic performance by ranking pairs and measuring the Spearman 's rank correlation coefficient ( Spearman 's \u03c1 ) , and by measuring the precision in a homophone detection task . Experiments show that the acoustic word embeddings are better than orthographic ones to measure the phonetic proximity between two words . More , they are better too on homophone detection task . This confirms that acoustic word embeddings have captured additional information about word pronunciation .", "entities": [[12, 14, "TaskName", "word embeddings"], [59, 61, "TaskName", "word embeddings"], [106, 108, "TaskName", "word embeddings"], [137, 139, "TaskName", "word embeddings"]]}
{"text": "Hyperbolic Capsule Networks for Multi - Label Classification", "entities": [[4, 8, "TaskName", "Multi - Label Classification"]]}
{"text": "Although deep neural networks are effective at extracting high - level features , classification methods usually encode an input into a vector representation via simple feature aggregation operations ( e.g. pooling ) . Such operations limit the performance . For instance , a multi - label document may contain several concepts . In this case , one vector can not sufficiently capture its salient and discriminative content . Thus , we propose Hyperbolic Capsule Networks ( HYPERCAPS ) for Multi - Label Classification ( MLC ) , which have two merits . First , hyperbolic capsules are designed to capture fine - grained document information for each label , which has the ability to characterize complicated structures among labels and documents . Second , Hyperbolic Dynamic Routing ( HDR ) is introduced to aggregate hyperbolic capsules in a label - aware manner , so that the label - level discriminative information can be preserved along the depth of neural networks . To efficiently handle large - scale MLC datasets , we additionally present a new routing method to adaptively adjust the capsule number during routing . Extensive experiments are conducted on four benchmark datasets . Compared with the state - of - the - art methods , HY - PERCAPS significantly improves the performance of MLC especially on tail labels .", "entities": [[79, 83, "TaskName", "Multi - Label Classification"]]}
{"text": "The main difference between Multi - Class Classification ( MCC ) and Multi - Label Classification ( MLC ) is that datasets in MCC have only serval mutually exclusive classes , while datasets in MLC contain much more correlated labels . MLC allows label co - occurrence in one document , which indicates that the labels are not disjointed . In addition , a large fraction of the labels are the infrequently occurring tail labels ( Bhatia et al , 2015 ) , which is also referred as the power - law label distribution . Figure 1 illustrates the label distribution of EUR - LEX57 K ( Chalkidis et al , 2019 ) . A multi - label document usually has serval head and tail labels , and hence contain several concepts about both its head and tail labels simultaneously . Recent works for text classification , such as CNN - KIM ( Kim , 2014 ) and FASTTEXT ( Joulin et al , 2017 ) , focus on encoding a document into a fixed - length vector as the distributed document representation ( Le and Mikolov , 2014 ) . These encoding based deep learning methods use simple operations ( e.g. pooling ) to aggregate features extracted by neural networks and construct the document vector representation . A Fully - Connected ( FC ) layer is usually applied upon the document vector to predict the probability of each label . And each row in its weight matrix can be interpreted as a label vector representation ( Du et al , 2019b ) . In this way , the label probability can be predicted by computing the dot product between label and document vectors , which is proportional to the scalar projection of the label vector onto the document vector as shown in Figure 2 . For example , label \" movie \" should have the largest scalar projection onto a document about \" movie \" . However , even the learned label representation of \" music \" can be distinguished from \" movie \" , it may also have a large scalar projection onto the document . Moreover , multi - label documents always contain several concepts about multiple labels , such as a document about \" sport movie \" . Whereas the document vector representation is identical to all the labels , and training instances for tail labels are inadequate compared to head labels . The imbalance between head and tail labels makes it hard for the FC layer to make prediction , especially on tail labels . In this case , one vector can not sufficiently capture its salient and discriminative content . Therefore , the performance of constructing the document vector representation via simple aggregation operations is limited for MLC . Capsule networks ( Sabour et al , 2017 ; Yang et al , 2018a ) has recently proposed to use dynamic routing in place of pooling and achieved better performance for classification tasks . In fact , capsules are fine - grained features compared to the distributed document representation , and dynamic routing is a label - aware feature aggregation procedure . ( Zhao et al , 2019 ) improves the scalability of capsule networks for MLC . However , they only use CNN to construct capsules , which capture local contextual information ( Wang et al , 2016 ) . Effectively learning the document information about multiple labels is crucial for MLC . Thus we propose to connect CNN and RNN in parallel to capture both local and global contextual information , which would be complementary to each other . Nevertheless , Euclidean capsules necessitate designing a non - linear squashing function . Inspired by the hyperbolic representation learning methods which demonstrate that the hyper - bolic space has more representation capacity than the Euclidean space ( Nickel and Kiela , 2017 ; Ganea et al , 2018a ) , Hyperbolic Capsule Networks ( HYPERCAPS ) is proposed . Capsules are constrained in the hyperbolic space which does not require the squashing function . Hyperbolic Dynamic Routing ( HDR ) is introduced to aggregate hyperbolic capsules in a label - aware manner . Moreover , in order to fit the large label set of MLC and improve the scalability of HYPERCAPS , adaptive routing is presented to adjust the number of capsules participated in the routing procedure . The main contributions of our work are therefore summarized as follows : We propose to connect CNN and RNN in parallel to simultaneously extract local and global contextual information , which would be complementary to each other . HYPERCAPS with HDR are formulated to aggregate features in a label - aware manner , and hyperbolic capsules benefits from the representation capacity of the hyperbolic space . Adaptive routing is furthermore presented to improve the scalability of HYPERCAPS and fit the large label set of MLC . Extensive experiments on four benchmark MLC datasets demonstrate the effectiveness of HYPER - CAPS , especially on tail labels .", "entities": [[4, 8, "TaskName", "Multi - Class Classification"], [12, 16, "TaskName", "Multi - Label Classification"], [144, 146, "TaskName", "text classification"], [158, 159, "MethodName", "FASTTEXT"], [619, 620, "DatasetName", "Inspired"], [623, 625, "TaskName", "representation learning"]]}
{"text": "In order to make neural networks work in the hyperbolic space , formalism of the M\u00f6bius gyrovector space is adopted ( Ganea et al , 2018b ) . An n - dimensional Poincar\u00e9 ball B n is a Riemannian manifold defined as B n = { x R n | x < 1 } , with its tangent space around p B n denoted as T p B n and the conformal factor as \u03bb p : = 2 1\u2212 p 2 . The exponential map exp p : T p B n B n for w T p B n \\ { 0 } is consequently defined as exp p ( w ) = p ( tanh ( \u03bb p 2 w ) w w ) . ( 1 ) To work with hyperbolic capsules , M\u00f6bius operations in the Poincar\u00e9 ball also need to be formulated . M\u00f6bius addition for u , v B n is defined as u v = ( 1 + 2 u , v + v 2 ) u+ ( 1\u2212 u 2 ) v 1 + 2 u , v + u 2 v 2 , ( 2 ) where , denotes the Euclidean inner product . Thus M\u00f6bius summation can be formulated as n M i = m p i = p m p n , p i B n . ( 3 ) M\u00f6bius scalar multiplication for k R and p B n \\ { 0 } is defined as k \u2297 p = tanh ( k tanh \u22121 ( p ) ) p p . ( 4 ) And k \u2297 p = 0 when p = 0 B n . The definition of M\u00f6bius matrix - vector multiplication for M R m\u00d7n and p B n when M p = 0 is as follows M \u2297 p = tanh ( M p p tanh \u22121 ( p ) ) M p M p . ( 5 ) And M \u2297 p = 0 when M p = 0 . HDR is developed based on these operations .", "entities": [[103, 104, "DatasetName", "0"], [244, 245, "DatasetName", "0"], [273, 274, "DatasetName", "0"], [277, 278, "DatasetName", "0"], [301, 302, "DatasetName", "0"], [333, 334, "DatasetName", "0"], [338, 339, "DatasetName", "0"]]}
{"text": "Neural networks are generally used as effective feature extractors for text classification . Kernels of CNN can be used to capture local n - gram contextual information at different positions of a text sequence , while hidden states of RNN can represent global long - term dependencies of the text ( Wang et al , 2016 ) . Hence , we propose to obtain the combination of local and global hyperbolic capsules by connecting CNN and RNN in parallel , which would be complementary to each other . Given a text sequence of a document with T word tokens x = [ x 1 , . . . , x T ] , pre - trained wdimensional word embeddings ( e.g. GLOVE ( Pennington et al , 2014 ) ) are used to compose word vector representations E = [ e 1 , . . . , e T ] R T \u00d7w , upon which CNN and RNN connected in parallel are used to construct local and global hyperbolic capsules in the Poincar\u00e9 ball . Figure 3 illustrates the framework for HYPERCAPS .", "entities": [[10, 12, "TaskName", "text classification"], [117, 119, "TaskName", "word embeddings"]]}
{"text": "N - gram kernels K R k\u00d7w with different window size k are applied on the local region of the word representations E t : t+k\u22121 R k\u00d7w to construct the local features as l t = \u03d5 ( K E t : t+k\u22121 ) , ( 6 ) where denotes the element - wise multiplication and \u03d5 is a non - linearity ( e.g. ReLU ) . For simplicity , the bias term is omitted . With totally d channels , the local hyperbolic capsules at position t can be constructed as l t = exp 0 ( [ l ( 1 ) t , . . . , l ( d ) t ] ) B d . ( 7 ) Therefore , a k - gram kernel with 1 stride can construct T \u2212k+1 local hyperbolic capsules . The local hyperbolic capsule set is denoted as { u 1 , . . . , u L } .", "entities": [[65, 66, "MethodName", "ReLU"], [97, 98, "DatasetName", "0"]]}
{"text": "Bidirectional GRU ( Chung et al , 2014 ) is adopted to incorporate forward and backward global contextual information and construct the global hyperbolic capsules . Forward and backward hidden states at time - step t are obtained by \u2212 h t = GRU ( \u2212\u2212 h t\u22121 , e t ) , \u2212 h t = GRU ( \u2212\u2212 h t+1 , e t ) . ( 8 ) Each of the total 2 T hidden states can be taken as a global hyperbolic capsule using the exponential map , i.e. \u2212 g t = exp 0 ( \u2212 h t ) , and equally for the backward capsules . The global hyperbolic capsule set is denoted as { u 1 , . . . , u G } .", "entities": [[0, 2, "MethodName", "Bidirectional GRU"], [43, 44, "MethodName", "GRU"], [57, 58, "MethodName", "GRU"], [97, 98, "DatasetName", "0"]]}
{"text": "Datasets Experiments are carried out on four publicly available MLC datasets , including the small - scale AAPD ( Yang et al , 2018b ) and RCV1 ( Lewis et al , 2004 ) , the large - scale ZHIHU 1 and EUR - LEX57 K ( Chalkidis et al , 2019 ) . Labels are divided into head and tail sets according to their number of training instances , i.e. labels have less than average number of training instances are divided into the tail label set . Their statistics can be found in Table 1 .", "entities": [[26, 27, "DatasetName", "RCV1"]]}
{"text": "We use the rank - based evaluation metrics which have been widely adopted for MLC tasks ( Bhatia et al , 2015 ; Liu et al , 2017 ) , i.e. Precision@k ( P@k for short ) and nDCG@k , which are respectively defined as P@k = 1 k j rank k ( a ) y j , ( 19 ) nDCG@k = j rank k ( a ) y j /log ( j + 1 ) min ( k , y 0 ) j=1 1 / log ( j + 1 ) , ( 20 ) where y j { 0 , 1 } denotes the the ground truth about label j , rank k ( a ) denotes the indices of the candidate label - aware hyperbolic capsules with k largest activations in descending order , and y 0 is the true label number for the document instance . The final results are averaged over all the test instances . Baselines To demonstrate the effectiveness of HYPERCAPS on the benchmark datasets , six comparative text classification methods are chosen as the baselines . FASTTEXT ( Joulin et al , 2017 ) is a representative encoding - based method which use average pooling to construct document representations and MLP to make the predictions . SLEEC ( Bhatia et al , 2015 ) is a typical label - embedding method for MLC , which uses k - nearest neighbors search to predict the labels . XML - CNN ( Liu et al , 2017 ) employs CNN as local n - gram feature extractors and a dynamic pooling technique as aggregation method . SGM ( Yang et al , 2018b ) applies the seq2seq model with attention mechanism , which takes the global contextual information . REGGNN ( Xu et al , 2019 ) uses a combination of CNN and LSTM with a dynamic gate that controls the information from these two parts . NLP - CAP ( Zhao et al , 2019 ) is a capsule - based approach for MLC , which reformulates the routing algorithm . NLP - CAP use only CNN to construct capsules , and it applies the squashing function onto capsules . Implementation Details All the words are converted to lower case and padding is used to handle the various lengths of the text sequences . Maximum length of AAPD , RCV1 and EUR - LEX57 K is set to 500 , while maximum length of ZHIHU is 50 . To compose the word vector representations , pre - trained 300 - dimensional GLOVE ( Pennington et al , 2014 ) word embeddings are used for AAPD , RCV1 and EUR - LEX57 K , while ZHIHU uses its specified 256 - dimensional word embeddings . The dimension of the Poincar\u00e9 ball is set to 32 with a radius 1 \u2212 ( = 10 \u22125 ) to avoid numerical errors . Multiple one - dimensional convolutional kernels ( with window sizes of 2 , 4 , 8 ) are applied in the local hyperbolic capsule layer . The number of compressed local and global hyperbolic capsules is 128 . Adaptive routing layer is not applied on the small - scale datasets AAPD and RCV1 . The maximum candidate label number is set to 200 for the large - scale datasets ZHIHU and EUR - LEX57K. For the baselines , hyperparameters recommended by their authors are adopted .", "entities": [[82, 83, "DatasetName", "0"], [101, 102, "DatasetName", "0"], [140, 141, "DatasetName", "0"], [170, 172, "DatasetName", "the benchmark"], [176, 178, "TaskName", "text classification"], [185, 186, "MethodName", "FASTTEXT"], [202, 204, "MethodName", "average pooling"], [209, 210, "DatasetName", "MLP"], [235, 239, "MethodName", "k - nearest neighbors"], [283, 284, "MethodName", "seq2seq"], [310, 311, "MethodName", "LSTM"], [326, 327, "DatasetName", "CAP"], [351, 352, "DatasetName", "CAP"], [397, 398, "DatasetName", "RCV1"], [437, 439, "TaskName", "word embeddings"], [444, 445, "DatasetName", "RCV1"], [459, 461, "TaskName", "word embeddings"], [539, 540, "DatasetName", "RCV1"]]}
{"text": "Recent research on representation learning ( Nickel and Kiela , 2017 ) indicates that hyperbolic space is superior to Euclidean space in terms of representation capacity , especially in low dimension . ( Ganea et al , 2018b ) generalizes operations for neural networks in the Poincar\u00e9 ball using formalism of M\u00f6bius gyrovector space . Some works lately demonstrate the superiority of the hyperbolic space for serval natural language processing tasks , such as textual entailment ( Ganea et al , 2018a ) , machine translation ( Gulcehre et al , 2019 ) and word embedding ( Tifrea et al , 2019 ) . Our work presents the Hyperbolic Capsule Networks ( HYPERCAPS ) for MLC .", "entities": [[3, 5, "TaskName", "representation learning"], [84, 86, "TaskName", "machine translation"]]}
{"text": "We present the Hyperbolic Capsule Networks ( HYPERCAPS ) with Hyperbolic Dynamic Routing ( HDR ) and adaptive routing for Multi - Label Classification ( MLC ) . The proposed HYPERCAPS takes advantage of the parallel combination of finegrained local and global contextual information and label - aware feature aggregation method HDR to dynamically construct label - aware hyperbolic capsules for tail and head labels . Adaptive routing is additionally applied to improve the scalability of HYPERCAPS by controlling the number of capsules during the routing procedure . Extensive experiments are carried out on four benchmark datasets . Results compared with the state - of - the - art methods demonstrate the superiority of HYPERCAPS , especially on tail labels . As recent works explore the superiority of hyperbolic space to Euclidean space for serval natural language processing tasks , we intend to couple with the hyperbolic neural networks ( Ganea et al , 2018b ) and the hyperbolic word embedding method such as POINCAR\u00c9GLOVE ( Tifrea et al , 2019 ) in the future .", "entities": [[20, 24, "TaskName", "Multi - Label Classification"]]}
{"text": "In its fifth iteration , # SMM4H 2020 continues to serve as a venue for bringing together researchers interested in addressing the significant opportunities and challenges of utilizing the vast amount of data on social media for health informatics . For # SMM4H 2020 , we accepted 5 workshop papers ( acceptance rate of 56 % ) and 26 shared task system description papers . Each submission was peer - reviewed by two reviewers . The accepted workshop papers span a range of social media data - Twitter , Facebook , Reddit , and online health forums - and health domains , including diabetes , depression , COVID - 19 , medical misinformation , and adverse drug reactions . Cornelius et al present an online platform that aggregates and visualizes methods for extracting information related to COVID - 19 on Twitter . Dirkson et al explore modeling conversational features of posts , in addition to the posts themselves , for detecting adverse drug reactions on Facebook , and medical misinformation . Romberg et al present an annotated , Germanlanguage corpus for extracting information needs expressed online by patients with diabetes . Mo\u00dfburger et al use various text mining techniques to compare features of depression forums on Reddit and a curated , moderated site . Finally , Owen et al present an annotated , English - language corpus for detecting depression and anxiety on Twitter . The # SMM4H 2020 shared tasks sought to advance the use of Twitter data ( tweets ) for pharmacovigilance , toxicovigilance , and epidemiology of birth defects . In addition to re - reruns of three tasks , # SMM4H 2020 included new tasks for detecting adverse drug reactions in French and Russian tweets , characterizing chatter related to prescription medication abuse , and detecting self reports of birth defect pregnancy outcomes . The five tasks required methods for binary classification , multiclass classification , and named entity recognition ( NER ) . With 29 teams and a total of 130 system submissions , participation in the # SMM4H shared tasks continues to grow . Among the 26 shared task system description papers that were accepted , 6 teams were invited to present their system orally . The organizing committee of # SMM4H 2020 would like to thank the program committee , the additional reviewers of system description papers , the organizers of COLING 2020 ( especially the workshop cochairs ) , the annotators of the shared task data , and , of course , everyone who submitted a paper or participated in the shared tasks . # SMM4H 2020 would not have been possible without all of them .", "entities": [[6, 7, "DatasetName", "SMM4H"], [42, 43, "DatasetName", "SMM4H"], [91, 92, "DatasetName", "Reddit"], [206, 207, "DatasetName", "Reddit"], [237, 238, "DatasetName", "SMM4H"], [274, 275, "DatasetName", "SMM4H"], [321, 324, "TaskName", "named entity recognition"], [325, 326, "TaskName", "NER"], [343, 344, "DatasetName", "SMM4H"], [377, 378, "DatasetName", "SMM4H"], [433, 434, "DatasetName", "SMM4H"]]}
{"text": "Reinforcement learning ( RL ) is an attractive solution for task - oriented dialog systems . However , extending RL - based systems to handle new intents and slots requires a system redesign . The high maintenance cost makes it difficult to apply RL methods to practical systems on a large scale . To address this issue , we propose a practical teacherstudent framework to extend RL - based dialog systems without retraining from scratch . Specifically , the \" student \" is an extended dialog manager based on a new ontology , and the \" teacher \" is existing resources used for guiding the learning process of the \" student \" . By specifying constraints held in the new dialog manager , we transfer knowledge of the \" teacher \" to the \" student \" without additional resources . Experiments show that the performance of the extended system is comparable to the system trained from scratch . More importantly , the proposed framework makes no assumption about the unsupported intents and slots , which makes it possible to improve RL - based systems incrementally .", "entities": [[91, 92, "MethodName", "ontology"]]}
{"text": "With the flourish development of virtual personal assistants ( e.g. , Amazon Alexa and Google Assistant ) , task - oriented dialog systems , which can help users accomplish tasks naturally , have been a focal point in both academic and industry research . In the early work , the task - oriented dialog system is merely a set of hand - crafted mapping rules defined by experts . This is referred to as a rule - based system . Although rule - based systems often have acceptable performance , they are inconvenient and difficult to be optimized . Recently , reinforcement learning approaches have been applied to optimize dialog systems through interaction with a user simulator or employed real users online ( Ga\u0161i\u0107 et al , 2011 ; Su et al , 2016a ; Li et al , 2016 , Figure 1 : An example of a task - oriented dialog after the system comes online . The user is confused because the \" confirm \" intent has not been considered in the deployed system . Dialog rules should be embedded in a new system to handle such situations . 2017b ) . It has been proven that RL - based dialog systems can abandon hand - crafted dialog manager and achieve more robust performance than rulebased systems ( Young et al , 2013 ) . Typically , the first step of building RL - based dialog systems is defining a user model 1 and necessary system actions to complete a specific task ( e.g. , seek restaurants information or book hotels ) . Based on such ontology , developers can extract dialog features and train the dialog manager model in an interaction environment . Such systems work well if real users are consistent with the predefined user model . However , as shown in Fig . 1 , the unanticipated actions 2 of real users will lead to a poor user experience . In this situation , the original system should be extended to support new user actions based on user feedback . However , adding new intents or slots will change the predefined ontology . As a consequence , developers need to extract additional dialog features based on new ontology . Besides , new system actions may be added to deal with new user actions . The network architecture of the new system and the original one will be different . The new system can not inherit the parameters from the old one directly . It will make the original dialog manager model invalid . Therefore , developers have to retrain the new system by interacting with users from scratch . Though there are many methods to train a RL - based dialog manager efficiently ( Su et al , 2016a ( Su et al , , 2017Lipton et al , 2017 ; Chen et al , 2017 ) , the unmaintainable RL - based dialog systems will still be put on the shelf in real - world applications ( Paek and Pieraccini , 2008 ; Paek , 2006 ) . To alleviate this problem , we propose a teacherstudent framework to maintain the RL - based dialog manager without training from scratch . The idea is to transfer the knowledge of existing resources to a new dialog manager . Specifically , after the system is deployed , if developers find some intents and slots missing before , they can define a few simple dialog rules to handle such situations . For example , under the condition shown in Fig . 1 , a reasonable strategy is to inform the user of the location of this restaurant . Then we encode information of such hand - crafted logic rules into the new dialog manager model . Meanwhile , user logs and dialog policy of the original system can guide the new system to complete tasks like the original one . Under the guidance of the \" teacher \" ( logic rules , user logs , and original policy ) , we can reforge an extended dialog manager ( the \" student \" ) without a new interaction environment . We conduct a series of experiments with simulated and real users on restaurant domain . The extensive experiments demonstrate that our method can overcome the problem brought by the unpredictable user behavior after deployment . Owing to reuse of existing resources , our framework saves time in designing new interaction environments and retraining RL - based systems from scratch . More importantly , our method does not make any assumptions about the unsupported intents and slots . So the system can be incrementally extended once developers find new intents and slots that are not taken into account before . As far as we know , we are the first to discuss the maintainability of deep reinforcement learning based dialog systems systematically .", "entities": [[14, 15, "DatasetName", "Google"], [268, 269, "MethodName", "ontology"], [356, 357, "MethodName", "ontology"], [372, 373, "MethodName", "ontology"]]}
{"text": "Dialog Manager The dialog manager of taskoriented dialog systems , which consists of a state tracker and a dialog policy module , controls the dialog flow . Recently , deep reinforcement learning ( Mnih et al , 2013 ( Mnih et al , , 2015 has been applied to optimize the dialog manager in an \" endto - end \" way , including deep Q - Network ( Lipton et al , 2017 ; Li et al , 2017b ; Peng et al , 2017 ; Zhao and Eskenazi , 2016 ) and policy gradient methods ( Williams et al , 2017 ; Su et al , 2016b ; Dhingra et al , 2017 ) . RL methods have shown great potential in building a robust dialog system automatically . However , RL - based approaches are rarely used in real - world applications because of the maintainability problem ( Paek and Pieraccini , 2008 ; Paek , 2006 ) . To extend the domain of dialog systems , Ga\u0161ic et al ( 2014 ) explicitly defined kernel functions between the belief states that come from different domains . However , defining an appropriate kernel function is nontrivial when the ontology has changed drastically . Shah et al ( 2016 ) proposed to integrate turnlevel feedback with a task - level reward signal to learn how to handle new user intents . This approach alleviates the problem that arises from the difference between training and deployment phases . But it still fails when the developers have not considered all user actions in advance . Lipton et al ( 2017 ) proposed to use BBQ - Networks to extend the domain . However , similar to Shah et al ( 2016 ) , the BBQ - Networks have reserved a few bits in the feature vector for new intents and slots . And system actions for handling new user actions have been considered in the original system design . This assumption is not practical enough . Compared to the existing domain extension methods , our work addresses a more practical problem : new intents and slots are unknown to the original system . If we need to extend the dialog system , we should design a new network architecture to represent new user actions and take new system actions into account . Knowledge Distillation Our proposed framework is inspired by recent work in knowledge distillation ( Bucilu et al , 2006 ; Ba and Caruana , 2014 ; Li et al , 2014 ) . Knowledge distillation means training a compact model to mimic a larger teacher model by approximating the function learned by the teacher . Hinton et al ( 2015 ) introduced knowledge distillation to transfer knowledge from", "entities": [[63, 67, "MethodName", "deep Q - Network"], [93, 96, "TaskName", "policy gradient methods"], [200, 201, "MethodName", "ontology"], [273, 274, "DatasetName", "BBQ"], [293, 294, "DatasetName", "BBQ"], [391, 393, "MethodName", "Knowledge Distillation"], [402, 404, "MethodName", "knowledge distillation"], [424, 426, "MethodName", "Knowledge distillation"], [453, 455, "MethodName", "knowledge distillation"]]}
{"text": "Let A u and A s denote the supported user and system action sets in the original system design respectively . u t denotes the user input in the t - th turn . The LU module converts u t into a domain specific intent and associated slots to form a user action a u t A u . The system will return an action a s t A s according to the dialog manager \u03c0 ( \u03b8 ) . Note that not all user actions are taken into account at the beginning of system design . After deployment , the developers can find that some user actions A u new can not be handled by the original system based on the human - machine interaction logs D. Generally speaking , A u new consists of new intents and slots . Our goal is to extend the original system to support the new user action set A u = A u \u222aA u new . The extended dialog manager and new system action set are denoted as \u03c0 ( \u03b8 ) and A s respectively . To handle new user actions , more system actions may be added to the new system . It means that A s is a subset of A s .", "entities": [[77, 78, "HyperparameterName", "\u03b8"], [179, 180, "HyperparameterName", "\u03b8"]]}
{"text": "Sim2 LU Error Rate Succ . Turn Reward Satis . Succ . Turn Reward Satis . Although \" Satis . \" is obtained based on our handcrafted dialog rules , it approximately measures the subjective experience of real users after system deployment .", "entities": [[2, 3, "MetricName", "Error"]]}
{"text": "To evaluate our approach , we design another user simulator , which we denote as Sim2 , to simulate the unpredictable real customers . The user action set of Sim2 is denoted as A u . The difference between A u and A u is reflected on the domain specific intents 7 . Specifically , in addition to the intents of Sim1 , A u includes the \" confirm \" intent . The difference in user action sets will result in different interaction strategies between Sim1 and Sim2 . To verify whether a recommended restaurant meets his ( her ) constraints , Sim1 can only request what the value of a specific slot is , but Sim2 can request or confirm . After obtaining the original system S 1 , we deploy it to interact with Sim1 and Sim2 respectively , under different LU error rates ( Li et al , 2017a ) . In each condition , we simulate 3200 episodes to obtain the performance . details of the test performance . Table 2 shows the statistics of turns when S 1 interacts with Sim2 . As shown in Table 1 , S 1 achieves higher dialog success rate and rewards when testing with Sim1 . When interacting with Sim2 , nearly half of the responses to unsupported user actions are not reasonable . Notice even though Sim2 contains new user actions , some of the new actions might be appropriately handled by S 1 . It may be due to the robustness of our RL - based system . But it 's far from being desired . The unpredictable real user behavior in the deployment stage will lead to a poor user experience in real - world applications . It proves the importance of a maintainable system . To maintain the original system , we define a few simple logic rules to handle unsupported user actions : if users confirm the value of a slot in current turn , the system should inform users of that value . These rules 8 are intuitive and reasonable to handle queries such as \" Is this restaurant located in Zhongguancun ? \" . There are four slots 9 that can be used for confirmation , so we define four logic rules in all . Due to the change in ontology , we add a new status in dialog features to represent the \" confirm \" intent of users . It leads to a change in the model architecture of extended dialog manager . Then we distill knowledge of the S 1 and logic rules into the extended system . No additional data is used to obtain the extended system . For comparison , we retrain another new system ( contrast system ) from scratch by interacting 8 In the practical dialog system , we can inject more complex logic rules and take dialog history into account . These rules are not limited to question / answer mapping . 9 They are \" name \" , \" area \" , \" price range \" and \" cuisine \" . with Sim2 . After about 2600 interactions with Sim2 , the performance of contrast system starts to converge . Note that in order to build the contrast system , the developers need to redesign a new user simulator or hire real users . It 's expensive and impractical in industrial applications . Then we simulate 3200 interactions with Sim2 to obtain its performance . Fig . 4 illustrates the performance of different systems . As can be seen , the extended system performs better than the original system in terms of dialog success rate and \" Satis . \" . This is to a large degree attributed to the consideration of new user actions . Fig . 4 ( a ) shows that the contrast system achieves higher dialog success rate than the extended system . But the gap is negligible . However , the contrast system is trained from scratch under a new interaction environment and the extended system is trained by transferring knowledge of the original system and logic rules . To train the contrast system , about 2600 episodes are sampled by interacting with a new interaction environment . But no additional data is used to train the extended system . In Fig . 4 ( b ) , the \" Satis . \" of the extended system is slightly higher than the contrast system . This is due to the fact that the extended system learns how to deal with new user actions from logic rules but the contrast system obtains dialog policy by exploring the environment . As a result , the contrast system learns a more flexible dialog policy than the extended system 10 . However , the \" Satis . \" has a bias to the suboptimal rules , Left column shows the dialog context condition ; Right column shows the corresponding system action . We define 14 rules in all to handle newfound intents and slots shown in Table 3 . rather than the optimal policy gained from the environment . It suggests the extended system can be further refined by reinforcement learning once a new interaction environment is available .", "entities": [[388, 389, "MethodName", "ontology"]]}
{"text": "While classic NLG systems typically made use of hierarchically structured content plans that included discourse relations as central components , more recent neural approaches have mostly mapped simple , flat inputs to texts without representing discourse relations explicitly . In this paper , we investigate whether it is beneficial to include discourse relations in the input to neural data - to - text generators for texts where discourse relations play an important role . To do so , we reimplement the sentence planning and realization components of a classic NLG system , Methodius , using LSTM sequence - to - sequence ( seq2seq ) models . We find that although seq2seq models can learn to generate fluent and grammatical texts remarkably well with sufficiently representative Methodius training data , they can not learn to correctly express Methodius 's SIMILARITY and CONTRAST comparisons unless the corresponding RST relations are included in the inputs . Additionally , we experiment with using self - training and reverse model reranking to better handle train / test data mismatches , and find that while these methods help reduce content errors , it remains essential to include discourse relations in the input to obtain optimal performance .", "entities": [[95, 96, "MethodName", "LSTM"], [102, 103, "MethodName", "seq2seq"], [110, 111, "MethodName", "seq2seq"]]}
{"text": "Traditional approaches to the task of natural language generation ( NLG ) have employed a pipeline of modules , moving from an initial abstract meaning representation ( MR ) to human - readable natural language ( Reiter and Dale , 2000 ) . In the last decade , the success of neural methods in other domains of natural language processing ( NLP ) has led to the development of neural ' end - to - end ' ( e2e ) * The first two authors are listed in random order ( equal contribution ) , then the other authors are listed in alphabetical order by last name . E - mail : stevensguille.1@buckeyemail.osu.edu architectures in NLG ( Du\u0161ek et al , 2020 ) , where a direct mapping from MRs to text is learned . Since target texts for training neural models are typically crowd - sourced , the neural approach promises to make it easier to scale up the development of NLG systems in comparison to classic approaches , which generally require domain - or applicationspecific rules to be developed , even if the modules themselves are reusable . Accompanying the increase in crowd - sourced corpora has been a comparative simplification of both MRs and tasks . In particular , classic NLG systems typically made use of hierarchically structured content plans that included discourse relations as central components , where the discourse relations - often based on Rhetorical Structure Theory ( RST ) ( Mann and Thompson , 1988 ; Taboada and Mann , 2006 ) - group together and connect elementary propositions or messages ( Hovy , 1993 ; Stede and Umbach , 1998 ; Isard , 2016 ) . By contrast , more recent neural approaches - in particular , those developed for the E2E and WebNLG shared task challenges - have mostly mapped simple , flat inputs to texts without representing discourse relations explicitly . The absence of discourse relations in work on neural NLG to date is somewhat understandable given that neural systems have primarily tackled texts that merely describe entities , rather than comparing them , situating them in time , discussing causal or other contingency relations among them , or constructing persuasive arguments about them , where discourse relations are crucial for coherence ( Prasad et al , 2008 ) . Recently , Balakrishnan et al ( 2019a ) have argued that discourse relations should be reintroduced into neural generation in order to enable the correct expression of these relations to be more reliably controlled . However , they do note that only 6 % of the crowd - sourced E2E Challenge texts contain discourse connectives ex - pressing CONTRAST , and though they introduce a conversational weather dataset that uses both CONTRAST and JUSTIFY relations with greater frequency , it is fair to say that the use of hierarchical MRs that incorporate discourse relations remains far from common practice . In this paper , we investigate whether it is beneficial to include discourse relations in the input to neural data - to - text generators for texts where discourse relations play an important role . To do so , we reimplement the sentence planning and realization components of a classic NLG system , Methodius ( Isard , 2016 ) , using LSTM sequenceto - sequence ( seq2seq ) models , since Methodius makes similarity or contrast comparisons in most of its outputs . Specifically , rather than crowd - source output texts for Methodius 's content plans , we run the existing system to obtain target texts for training seq2seq models , and experiment with input MRs ( derived from the content plans ) that contain discourse relations as well as ones that leave them out . 1 In our experiments , we observe that the seq2seq models learn to generate fluent and grammatical texts remarkably well . As such , we focus our evaluation on the correct and coherent expression of discourse relations . Since the Methodius texts are somewhat formulaic following delexicalization and entity anonymization , it is possible to write accurate automatic correctness checks for these relations . Using these automatic checks , we find that even with sufficiently representative Methodius training data , LSTM seq2seq models can not learn to correctly express Methodius 's similarity and contrast comparisons unless the corresponding RST relations are included in the inputs . This is an at least somewhat surprising result , since these relations are easily inferred from the input facts being compared . The major conclusion of our experiments is that explicitly encoding discourse information using RST relations boosts coherence by enabling rhetorical structure to be reliably lexicalized . Several techniques for improving the models are also considered , especially for situations where the training data exhibits mismatches with the test data ( as can happen in practice ) . One technique involves outputting a beam of possible text outputs and reranking them by checking the correspondence between the input meaning representation and the meaning representation produced by using a reversed model to map texts to meaning representations . The other technique is self - training ( Li and White , 2020 ) , i.e. , using an initial model to generate additional training data . This method drastically increases the amount of training data available for what is otherwise quite a small corpus . The upshot of these techniques is moderate improvement in the performance of both models with respect to the evaluation metrics just mentioned . But the conclusion remains that the model trained on explicit RST information continues to outperform the model without explicit RST structure in the input .", "entities": [[27, 28, "DatasetName", "MR"], [78, 79, "DatasetName", "e2e"], [298, 299, "DatasetName", "E2E"], [300, 301, "DatasetName", "WebNLG"], [438, 439, "DatasetName", "E2E"], [550, 551, "MethodName", "LSTM"], [555, 556, "MethodName", "seq2seq"], [598, 599, "MethodName", "seq2seq"], [635, 636, "MethodName", "seq2seq"], [706, 707, "MethodName", "LSTM"], [707, 708, "MethodName", "seq2seq"]]}
{"text": "The Methodius system ( Isard , 2016 ) was developed for multilingual text generation , based on the M - PIRO project ( Isard et al , 2003 ; Isard , 2007 ) which focused on museum exhibit descriptions . Methodius consists of several components . The content module selects content from a database and creates a content plan , which is a tree where the nodes are labeled with rhetorical relations or facts , following the structures proposed in RST . Fig . 1 shows a content plan . The content plan is rewritten into a sequence of logical forms , one per sentence , by the sentence planner . The logical forms are then realized as a text by means of a Combinatory Categorial Grammar ( CCG ) using OpenCCG ( White , 2006 ) . The Methodius system is designed to respond to the behaviour of the its intended users . Sequences of exhibits , dubbed ' chains ' , are constructed while the user moves through the museum . The chains control dependencies between exhibit descriptions , limit redundancy , and provide discourse continuity . While RST defines a number of rhetorical relations , Methodius incorporates only four of them : ELABORATION , JOINT , SIMILARITY and CON - TRAST . ELABORATION connects the main fact about a focal entity with other , peripheral facts about that entity . JOINT connects two facts of equal status . SIMILARITY and CONTRAST each connect two facts of equal status , but they do opposite jobs : SIMILARITY is used to express the similarity of two entities in terms of a commonly shared feature , while CONTRAST is used to show that the values of a shared feature of the given entities differ . For instance , unlike the previous coins you saw , which are located in the Athens Numismatic Museum , this In the experiments discussed below we focus on SIMILARITY and CONTRAST because the Methodius corpus lexicalizes them . Due to the dynamic generation of the exhibit descriptions , SIMILAR - ITY and CONTRAST link information in the current exhibit to previously mentioned exhibits and their properties - as such , correctly generating such expressions is vital to maintaining the coherence of the exhibit chain . 3 Data Preprocessing", "entities": [[12, 14, "TaskName", "text generation"]]}
{"text": "We ran self - training experiments with two sets of unlabeled data . One of them consists of the content plans generated by Methodius . The other one , dubbed ' heuristic , ' is developed from the existing labeled data . The heuristic data is produced by the following method : for every content plan produced by Methodius , extract the set of subtrees of the content plan which respect some soft constraints on structure . We avoid extracting trees that start with an optional type . The subtrees are randomly selected but their distribution is required to closely follow the distribution of distinct RST types in the training data . Since the size of the Methodius data set is limited , the heuristic data set provides useful cheap supplementary content for training ( compared to the cost of eliciting text corresponding to content plans through e.g. Turkers ) . We are thus interested whether having genuine Methodius content plans , which are not straightforward to generate in large amounts , could be completed by a heuristic data set generated from the labeled training data set . The FACT models were trained on the FACT versions of the data set , which is obtained by simply deleting the RST structure from the RST data set . 4 We refer to the models ( for sake of clarity ) by the names in Table 3 . There are only 947 content plans for selftraining , while the training set size is 4304 . The limited number of content plans for self - trainining is due to the homogeneity of the Methodius output , the intention to sync the length of training and test sets , and the finite number of exhibits in the Methodius data base . These content plans , which are harvested from Methodius , are on average just half the length of the content plans in the training set . Their shortness ensures the system is exposed to items of multiple lengths . Because of their reduced length and their production by the Methodius system , variation in the content of the short sequences is limited . The unique unlabelled data size differs between RST and FACT data sets , because the data for FACT is produced by pruning the RST data , the deletion of structure reduces the heterogeneity of data , resulting in fewer unique sequences for the FACT - LG input . We trained the following models : LBL : A standard LSTM seq2seq model with attention on the labeled data , which is also the base model for the other methods . ST - VAN : A model trained with vanilla selftraining . ST - RMR : A model self - trained with reverse model reranking for pseudo - labeling . Models were trained over several iterations , though for exposition the results reported below concern just the best model iterations . 5 BLEU4 is calculated on both the standard and challenge test sets . BLEU4 , though limited in the conclusions it supports , seems informative enough to allow one to distinguish between RST and FACT models ; we report it in Appendix D. BLEU4 is on average 5 or more points higher for RST models than FACT models across the test sets .", "entities": [[418, 419, "MethodName", "LSTM"], [419, 420, "MethodName", "seq2seq"]]}
{"text": "We count the sum of repetitions , hallucinations and omissions per test set and report the average per item , simply dividing the sum by the number of test set samples . Fig . 2 and Fig . 3 show the results , chiefly the uniform improvement of the self - training and reranking models over the baseline LSTM models . RST - SM with self - training is the best model . RST - SM with both self - training and reverse model reranking produced some of the best results too . RST - SM and RST - LG show similar performance when it comes to repetitions , hallucinations , and omissions on the standard test set . RST - SM outperforms RST - LG on the challenge set . RST models uniformly outperform FACT models . We observed the models sometimes produced stuttering , i.e. multiple repetition . Even one of the best models with respect to the standard test set - RST - SM - ST - VAN ( see Fig . 2 ) - produced two examples of stuttering ( out of 799 ) with 57 and 59 repetitions respectively . Just these two outputs nearly doubled the average error rate of RST - SM - ST - VAN . The other models reported here did not produce such extreme stuttering . But despite stuttering , RST - SM - ST - VAN is still the best model with respect to the metrics considered here . In Appendix C , model performance is reported by simply counting the total number of test examples in which a model generates neither repetitions , nor omissions , nor hallucinations . The following error from FACT - LG - ST - RMR shows multiple hallucination of the exhibit item 's creation time . T this is an imperial portrait and it portrays roman - emperor0 . like the coin you recently saw , this imperial portrait was created during historical - period0 . H this is an imperial portrait and it portrays roman - emperor0 . like the coin , this imperial portrait was created during historical - period0 . it was created in entity0 - creation - time and it was created in entity0 - creation - time . Further errors are shown in Appendix E.", "entities": [[58, 59, "MethodName", "LSTM"]]}
{"text": "The best performances are shown by RST - SM and RST - LG . Even RST - LBL produces only 12 mistakes out of 799 test items . Production of rhetorical connectives corresponding to CONTRAST and SIM - ILARITY is uniformly correct . After fine tuning and reranking , the errors reduced to 0 and 2 respectively . With respect to the FACT models , LBL makes mistakes , but improves upon self - training and reranking . Nonetheless RST models outperform the FACT models . While the best FACT model performs well with respect to producing the correct discourse connective / structure , this model produces serious content errors that render some outputs ( discussed in Section 8.1 ) incoherent .", "entities": [[53, 54, "DatasetName", "0"]]}
{"text": "While traditional natural language generation systems , e.g. Methodius , often employ knowledge graphs , the use of such structure in neural NLG is underdeveloped . An exception in this respect is WebNLG ( Gardent et al , 2017 ) , which is a multilingual corpus for natural language generation . An For future work , there are number of direction we intend to explore , including the following : Study whether large - scale pretrained models likewise fail to generalize well without dis - , where towards errors counts if either there is an incorrectly generated discourse cue word , or there has been a cue word generated while the target has none , or no cue word is generated but the reference contains one . The dotted line links two models if there is a significant difference between their performance in terms of Fisher 's Exact Test statistics ( we take the significance threshold 5 % ) . course relations in the input . Experiment with more diverse outputs for Methodius , e.g. crowd - sourcing further outputs to express the content plans . Study whether constrained decoding could be used to reduce discourse structure errors .", "entities": [[12, 14, "TaskName", "knowledge graphs"], [32, 33, "DatasetName", "WebNLG"]]}
{"text": "SunBear at WNUT - 2020 Task 2 : Improving RoBERTa - Based Noisy Text Classification with Knowledge of the Data domain", "entities": [[9, 10, "MethodName", "RoBERTa"], [13, 15, "TaskName", "Text Classification"]]}
{"text": "Taking advantage of RoBERTa as a backbone , we propose a customized network with appreciably modifications . Figure 2 illustrates our proposed architecture . The \" base \" version of RoBERTa is used . It has 12 Transformer blocks , each block outputs a 768 - D vector for each token . Since the output of different Transformer blocks represent different semantic levels for the inputs , in our experiments we combine outputs of those Transformer blocks by concatenation . This combination is fed to a classification head . We propose two types of the head : MLP Head : A simple feed forward network with one hidden layer . This head takes the last token embedding as its input . BiLSTM Head : A recurrent neural network with one Bidirectional LSTM layer . This network takes embeddings of all tokens . The hyperparameters are shown in Section 4 . 3.2 Fine - tuning Masked Language Model ( MLM )", "entities": [[3, 4, "MethodName", "RoBERTa"], [30, 31, "MethodName", "RoBERTa"], [37, 38, "MethodName", "Transformer"], [57, 58, "MethodName", "Transformer"], [75, 76, "MethodName", "Transformer"], [97, 98, "DatasetName", "MLP"], [121, 122, "MethodName", "BiLSTM"], [130, 132, "MethodName", "Bidirectional LSTM"], [158, 159, "DatasetName", "MLM"]]}
{"text": "We assume fine - tuning only on the dataset might cause overfitting on the chosen dataset only . Hence , we propose a hierarchical fine - tuning strategy for RoBERTa : the first phase we train with custom domain COVID Tweets dataset for domain adaptation , then the second phase is a fine - tuning process with WNUT Task 2 dataset for task adaptation . Our custom COVID Tweets dataset is gathered from Twitter platform , including unlabeled 1 million posts in general COVID domain , which has the hashtag of # Covid , # Covid19 , and # Coronavirus . We expect this model to generalize better on different distributed dataset in the same field of COVID Tweets . Figure 2 : The architecture of the proposed model . The input is tokenized into a sequence of BPE tokens . RoBERTa , the \" base \" version , takes this sequence and propagates it through 12 Transformer layers . By concatenating outputs from these 12 layers , we form a long sentence representation for the follow - up classification head , which is a simple Multi - layer Perceptron / Long Short - Term Memory network .", "entities": [[29, 30, "MethodName", "RoBERTa"], [43, 45, "TaskName", "domain adaptation"], [138, 139, "MethodName", "BPE"], [141, 142, "MethodName", "RoBERTa"], [157, 158, "MethodName", "Transformer"], [191, 196, "MethodName", "Long Short - Term Memory"]]}
{"text": "Recently research ( Xie et al , 2019 ; Edunov et al , 2018 ) have shown that back - translating monolingual data can be used as a potential form of data augmentation in Text Classification . The idea behind back translation is to translate a sentence from the original language ( English ) to another selected language and then translate back to the original language . This utilizes the power of current welldeveloped translation engines . In our experiment , 25 % of the data samples is back - translated into Vietnamese , the same amount goes for Italian and French , and the rest 25 % is kept unchanged . This assures the languages contribute equally to the overall dataset . Totally , the dataset size is increased by 75 % .", "entities": [[31, 33, "TaskName", "data augmentation"], [34, 36, "TaskName", "Text Classification"]]}
{"text": "In this paper , we have explored and proposed our pipeline to solve the Identification of Informative COVID - 19 English Tweet task by using a pretrained universal language model . By conducting a lot of experiments , we have demonstrated that the use of RoBERTa and our fine - tuning strategy is highly effective in text classification tasks . With our proposed methods , we have achieved prominent results on the WNUT Task 2 . For future work , we will design more complex classification head architectures to improve model 's performance as well as solving problems indicated in Section 4.4 . Furthermore , we would like to employ our model and pipeline in different languages such as Vietnamese to see how they adapt to new languages .", "entities": [[45, 46, "MethodName", "RoBERTa"], [56, 58, "TaskName", "text classification"]]}
{"text": "Relations between comprehensibility and adequacy errors in machine translation output", "entities": [[7, 9, "TaskName", "machine translation"]]}
{"text": "This work presents a detailed analysis of translation errors perceived by readers as comprehensibility and/or adequacy issues . The main finding is that good comprehensibility , similarly to good fluency , can mask a number of adequacy errors . Of all major adequacy errors , 30 % were fully comprehensible , thus fully misleading the reader to accept the incorrect information . Another 25 % of major adequacy errors were perceived as almost comprehensible , thus being potentially misleading . Also , a vast majority of omissions ( about 70 % ) is hidden by comprehensibility . Further analysis of misleading translations revealed that the most frequent error types are ambiguity , mistranslation , noun phrase error , word - by - word translation , untranslated word , subject - verb agreement , and spelling error in the source text . However , none of these error types appears exclusively in misleading translations , but are also frequent in fully incorrect ( incomprehensible inadequate ) and discarded correct ( incomprehensible adequate ) translations . Deeper analysis is needed to potentially detect underlying phenomena specifically related to misleading translations .", "entities": [[122, 124, "TaskName", "word translation"]]}
{"text": "Our analysis has been carried out on written usergenerated content , namely user reviews . Two types of publicly available user reviews written in English have been analysed : IMDb movie reviews 1 ( Maas et al , 2011 ) and Amazon product reviews 2 ( McAuley et al , 2015 ) . A set of those user reviews was translated into Croatian and Serbian , two closely related mid - size less - resourced morphologically rich European languages . The reviews were translated 3 by three on - line systems : Google Translate 4 , Bing 5 and Amazon translate 6 . The analysed text consists of a mixture of MT outputs from the three systems including 222 translated reviews consisting of about 1500 sentences ( segments ) and 19837 untokenised words in total . This text was then given to the annotators to mark comprehensibility and adequacy issues , and the process is described in details in the next section . The annotated text is publicly available under the Creative Commons CC - BY licence . 7", "entities": [[29, 32, "DatasetName", "IMDb movie reviews"], [92, 93, "DatasetName", "Google"]]}
{"text": "A word in the source language is simply copied to the translated text . word - by - word translation A sequence of source words is translated as single words - the translation choice of each word looks random , both lexically and morphologically , without taking into account any context . Table 3 shows these error types and their percentages for misleading translations . These error types are the certainly \" dangerous \" because they can easily mislead the reader to accept incorrect information . However , the very same error types are often perceived as fully incorrect ( incomprehensible inadequate ) , too . Furthermore , they ( except of untranslated words ) even often lead to discarding correct information ( incomprehensible adequate ) . Further in - depth analysis is needed to determine whether there are some underlying phenomena related exclusively to the misleading translations . Five examples of different perceptions of ambiguity errors , noun phrase errors and word - by - word translations are presented in Table 4 . All sentences except 3 ) have misleading parts ( fully misleading marked as red and potentially misleading as green ) . In the sentences 1 ) and 2 ) there is only one misleading ambiguous word . The incorrectly chosen variants of these words are fully comprehensible so that without the source text , the reader was not able to figure out that the information is not correct . On the other hand , the ambiguous word in the sentence 3 ) , together with the noun phrase , is perceived as both incomprehensible and inadequate ( marked as violet ) . Sentences 4 ) and 5 ) illustrate how different parts of a phrase translated word - by - word are perceived in different ways : violet denotes fully incorrect , red denotes misleading , and cyan denotes discarding almost correct translation . It might be worth noting that all sentences are perfectly fluent except the sentence 3 ) which is very disfluent . Propagation effect Table 3 also shows that there is a strong effect of propagation for comprehensibility - many correct words are perceived as incomprehensible because of errors in surrounding words . In many cases , the reader finds the whole sentence incomprehensible . An example of propagation can be seen in Table 5 . All words in bold are correct , but all were perceived as major comprehensibility issues due to different types of errors in surrounding words : a red misleading omission , a fully incorrect violet word , and an incomprehensible group of almost correct cyan words . It should be mentioned that for some adequacy errors , annotators also marked one or two neighbouring words which were not really incorrect , but that happened very rarely . Omissions Since several studies reported that the omissions are generally problematic to spot without access to the source text , we compared the frequencies of omissions percieved only as comprehensibility issue , only as adequacy issue , and as both ( regardless of the severity grade ) . Table 6 confirms the previous findings : a vast majority of omissions ( 71.5 % ) was perceived only as adequacy error . Only 9 % of actual omissions were also perceived as comprehensibility issues . Apart from this , 19 % of omissions were perceived as exclusively comprehensibility issues and are not related to anything actually omitted from the source text . The most probable reason is the influence of other surrounding errors , but further analysis is needed to better understand this effect .", "entities": [[18, 20, "TaskName", "word translation"]]}
{"text": "This research is being conducted with the financial support of the European Association for Machine Translation ( EAMT ) under its programme \" 2019 Sponsorship of Activities \" at the ADAPT Research Centre at Dublin City University . The ADAPT SFI Centre for Digital Media Technology is funded by Science Foundation Ireland through the SFI Research Centres Programme and is co - funded under the European Regional Development Fund ( ERDF ) through Grant 13 / RC/2106 . We would like to thank all the evaluators for providing us with annotations and feedback .", "entities": [[14, 16, "TaskName", "Machine Translation"]]}
{"text": "Bilingual Character Representation for Efficiently Addressing Out - of - Vocabulary Words in Code - Switching Named Entity Recognition", "entities": [[16, 19, "TaskName", "Named Entity Recognition"]]}
{"text": "Named Entity Recognition ( NER ) predicts which word tokens refer to location , people , organization , time , and other entities from a word sequence . Deep neural network models have successfully achieved the state - of - the - art performance in NER tasks ( Cohen ; Chiu and Nichols , 2016 ; Lample et al , 2016 ; Shen et al , 2017 ) using monolingual corpus . However , learning from code - switching tweets data is very challenging due to several reasons : ( 1 ) words may have different semantics in different context and language , for instance , the word \" cola \" can be associated with product or \" queue \" in Spanish ( 2 ) data from social media are noisy , with many inconsistencies such as spelling mistakes , repetitions , and informalities which eventually points to Out - of - Vocabulary ( OOV ) words issue ( 3 ) entities may appear in different language other than the matrix language . For example \" todos los Domingos en Westland Mall \" where \" Westland Mall \" is an English named entity . Our contributions are two - fold : ( 1 ) bilingual character bidirectional RNN is used to capture character - level information and tackle OOV words issue ( 2 ) we apply transfer learning from monolingual pre - trained word vectors to adapt the model with different domains in a bilingual setting . In our model , we use LSTM to capture long - range dependencies of the word sequence and character sequence in bilingual character RNN . In our experiments , we show the efficiency of our model in handling OOV words and bilingual word context .", "entities": [[0, 3, "TaskName", "Named Entity Recognition"], [4, 5, "TaskName", "NER"], [45, 46, "TaskName", "NER"], [181, 182, "DatasetName", "Mall"], [186, 187, "DatasetName", "Mall"], [226, 228, "TaskName", "transfer learning"], [253, 254, "MethodName", "LSTM"]]}
{"text": "Convolutional Neural Network ( CNN ) was used in NER task as word decoder by Collobert et al ( 2011 ) and a few years later , Huang et al ( 2015 ) introduced Bidirectional Long - Short Term Memory ( BiLSTM ) ( Sundermeyer et al , 2012 ) . Character - level features were explored by using neural architecture and replaced hand - crafted features Lample et al , 2016 ; Chiu and Nichols , 2016 ; Limsopatham and Collier , 2016 ) . Lample et al ( 2016 ) also showed Conditional Random Field ( CRF ) ( Lafferty et al , 2001 ) decoders to improve the results and used Stack memory - based LSTMs for their work in sequence chunking . Aguilar et al ( 2017 ) proposed multi - task learning by combining Part - of - Speech tagging task with NER and using gazetteers to provide language - specific knowledge . Characterlevel embeddings were used to handle the OOV words problem in NLP tasks such as NER ( Lample et al , 2016 ) , POS tagging , and language modeling .", "entities": [[9, 10, "TaskName", "NER"], [41, 42, "MethodName", "BiLSTM"], [94, 97, "MethodName", "Conditional Random Field"], [98, 99, "MethodName", "CRF"], [124, 125, "TaskName", "chunking"], [133, 137, "TaskName", "multi - task learning"], [139, 145, "TaskName", "Part - of - Speech tagging"], [147, 148, "TaskName", "NER"], [173, 174, "TaskName", "NER"]]}
{"text": "For our experiment , we use English - Spanish ( ENG - SPA ) Tweets data from Twitter provided by 62.62 % 16.76 % 19.12 % 3.91 % 54.59 % + FastText ( spa ) 49.76 % 12.38 % 11.98 % 3.91 % 39.45 % + token replacement 12.43 % 12.35 % 7.18 % 3.91 % 9.60 % + token normalization 7.94 % 8.38 % 5.01 % 1.67 % 6.08 % Aguilar et al ( 2018 ) . There are nine different named - entity labels . The labels use IOB format ( Inside , Outside , Beginning ) where every token is labeled as B - label in the beginning and follows with I - label if it is inside a named entity , or O otherwise . For example \" Kendrick Lamar \" is represented as B - PER I - PER . Table 2 and Table 3 show the statistics of the dataset . \" Person \" , \" Location \" , and \" Product \" are the most frequent entities in the dataset , and the least common ones are \" Time \" , \" Event \" , and \" Other \" categories . ' Other \" category is the least trivial among all because it is not well clustered like others .", "entities": [[31, 32, "MethodName", "FastText"]]}
{"text": "In this section , we describe word - level and character - level features used in our model . Word Representation : Words are encoded into continuous representation . The vocabulary is built from training data . The Twitter data are very noisy , there are many spelling mistakes , irregular ways to use a word and repeating characters . We apply several strategies to overcome the issue . We use 300 - dimensional English and Spanish FastText pre - trained word vectors which comprise two million words vocabulary each and they are trained using Common Crawl and Wikipedia . To create the shared vocabulary , we concatenate English and Spanish word vectors . For preprocessing , we propose the following steps : 1 . Token replacement : Replace user hashtags ( # user ) and mentions ( @user ) with \" USR \" , and URL ( https://domain.com ) with \" URL \" . 2 . Token normalization : Concatenate Spanish and English FastText word vector vocabulary . Normalize OOV words by using one out of these heuristics and check if the word exists in the vocabulary sequentially Then , the effectiveness of the preprocessing and transfer learning in handling OOV words are analyzed . The statistics is showed in Table 1 . It is clear that using FastText word vectors reduce the OOV words rate especially when we concatenate the vocabulary of both languages . Furthermore , the preprocessing strategies dramatically decrease the number of unknown words . Character Representation : We concatenate all possible characters for English and Spanish , including numbers and special characters . English and Spanish have most of the characters in common , but , with some additional unique Spanish characters . All cases are kept as they are .", "entities": [[77, 78, "MethodName", "FastText"], [95, 97, "DatasetName", "Common Crawl"], [164, 165, "MethodName", "FastText"], [197, 199, "TaskName", "transfer learning"], [219, 220, "MethodName", "FastText"]]}
{"text": "Table 4 shows the results for ENG - SPA tweets . Adding pre - trained word vectors and characterlevel features improved the performance . Interestingly , our initial attempts at adding character - level features did not improve the overall performance , until we apply dropout to the Char - RNN . The performance of the model improves significantly after transfer learning with FastText word vectors while it also reduces the number of OOV words in the development and test set . The margin between ours and first place model is small , approximately 1 % . We try to use sub - words representation from Spanish FastText , however , it does not improve the result since the OOV words consist of many special characters , for example , \" /IAtrevido / Provocativo \" , \" Twets / wek \" , and possibly create noisy vectors and most of them are not entity words .", "entities": [[60, 62, "TaskName", "transfer learning"], [63, 64, "MethodName", "FastText"], [107, 108, "MethodName", "FastText"]]}
{"text": "This paper presents a bidirectional LSTM - based model with hierarchical architecture using bilingual character RNN to address the OOV words issue . Moreover , token replacement , token normalization , and transfer learning reduce OOV words rate even further and significantly improves the performance . The model achieved 62.76 % F1score for English - Spanish language pair without using any gazetteer and knowledge - based information .", "entities": [[4, 6, "MethodName", "bidirectional LSTM"], [32, 34, "TaskName", "transfer learning"]]}
{"text": "This work is partially funded by ITS/319/16FP of the Innovation Technology Commission , HKUST 16214415 & 16248016 of Hong Kong Research Grants Council , and RDC 1718050 - 0 of EMOS.AI .", "entities": [[28, 29, "DatasetName", "0"]]}
{"text": "We propose an approach to automatically test for originality in generation tasks where no standard automatic measures exist . Our proposal addresses original uses of language , not necessarily original ideas . We provide an algorithm for our approach and a run - time analysis . The algorithm , which finds all of the original fragments in a ground - truth corpus and can reveal whether a generated fragment copies an original without attribution , has a run - time complexity of \u03b8 ( n log n ) where n is the number of sentences in the ground truth .", "entities": [[82, 83, "HyperparameterName", "\u03b8"]]}
{"text": "This research addresses an ethical consideration for Natural Language Generation , namely , plagiarism . The Oxford English Dictionary defines original ( adjective ) as \" present or existing from the beginning ; first or earliest \" and \" created directly and personally by a particular artist ; not a copy or imitation \" . But , if we apply the definitions of \" original \" to language , then there are two ways in which a piece of generated text may be original . For one , the text may express an \" original idea \" , such as Einstein did in 1905 with \" E = mc 2 \" . On the other hand , a non - original idea may be expressed in an original way , via , for example , figurative language . Our proposed approach addresses original uses of language . It does not necessarily address original ideas . How do we protect intellectual property when it comes to language generators that are trained on a world - wide - web of data ? Our language generators have to be held accountable . They should also be protected . What if a language generator generates an original analogy ? What if it writes a poem that is so great that it ends up in the history books ? Multiple language generators may be trained on the same ground truth ( e.g. , Wikipedia ) with the same embedding vectors ( e.g. , BERT ( Devlin et al , 2018 ) and GPT ( Vaswani et al , 2017 ; Radford et al , 2018 ) ) and the same technologies ( deep neural networks , LSTM cells ( Hochreiter and Schmidhuber , 1997 ) , transformers ( Vaswani et al , 2017 ) ) . It will become a question of \" Whose generator said it first ? \" With automatic language generation , we need a way to automatically measure , store , and reference original ideas and language . We propose one possible solution to these originality - related problems . For the purposes of our analyses , we define ground truth as the set of sentences that are compared with the generated sentences . The ground truth may be larger than the training set , but should include the training set . The gound truth would also , ideally , grow . For example , the ground truth could start out as the training set , but as new sentences are generated with a trained model , then the new sentences may be added to the ground truth . We also claim that generated sentences should only be added to the ground truth if they are original or include citations where appropriate .", "entities": [[248, 249, "MethodName", "BERT"], [257, 258, "MethodName", "GPT"], [281, 282, "MethodName", "LSTM"]]}
{"text": "Ground Truth Fragment Ground - truth fragments that appear once and only once in the ground truth are considered original . 1 Likewise , fragments that appear more than once in the ground truth are considered \" not original \" . For example , \" lengthened shadow \" appeared twice in our ground truth and so it is not considered an original phrase in the ground truth . Combining non - original fragments to generate a new idea or analogy , however , could be considered an original use of language . For example , \" the writer is the lengthened shadow of a man \" contains the fragments \" the writer is \" and \" the lengthened shadow \" and \" of a man \" which are not original fragments in our ground truth . However , the way in which they are combined in this example creates an original use of language - in this case , a metaphor . ( Examples of fragments that appeared many times in our training set are \" it is \" and \" human life \" . ) Original C = 1 Not Original C \u2265 2 Generated Fragment Original C = 0 Not Original , Citation Needed C = 1 Not Original , No Citation Needed C \u2265 2 Here is one possible use of GOT . If a generated sentence contains a fragment that appears once and only once in the ground truth ( after duplicate sentences are removed from the ground truth ) , then the generated sentence may be discarded because it contains a fragment from the ground truth that is a candidate for protection as intellectual property . In other words , the sentence may be in violation of a copyright law . Otherwise , the sentence could include a citation of the source for the original fragment . The definition of ground - truth original fragments actually calls for more nuance , which we will elaborate and explain how to compute next . We maintain a count per fragment that is incremented each time the fragment appears in a new sentence in a new document or by a different author ( if the author can be determined in both instances ) in the ground truth . In other words , if a fragment in the ground truth is repeated in the same document , or by the same author across documents , then the count for that fragment is incremented only once . ( Therefore , an author , if known , should also be stored for each fragment , at least until the count for that fragment is greater than 1 . When the count for a fragment is greater than 1 , then it has already been determined that the fragment was seen a second time in a different document by a different known , or unknown , author . ) The count for a fragment will be 1 if it occurs just once in the ground truth , or if all of its occurrences are in the same document or by the same author ; otherwise , the count will be greater than 1 . Now , a ground - truth fragment is said to be original if and only if its count is 1 . See Algorithm 1 for psuedo - code to test for originality and find all original fragements in a dataset . To examine fragments , we use a window length of wl varying between 2 and the sentence length , where wl is the number of words in the fragment . If the first or last word in the window is a determinant ( e.g. , ' a ' or ' the ' ) , any use of the verbs to be and to have ( ' is ' , ' are ' , ' am ' , ' was ' , ' were ' , ' has ' , ' had ' , ' have ' ) , punctuation mark , or preposition / subordinating conjunction ( e.g. , ' to ' , ' of ' , or ' from ' ) , the window is moved one step to the right . ( Shortening the window to get rid of the determinant , special verb , special character , or preposition would result in a window size already covered in the previous step . ) All words and characters are allowed in the other positions of the window , so , for example , a comma or preposition may appear in the middle of a window of size 3 or more .", "entities": [[200, 201, "DatasetName", "0"]]}
{"text": "To see how GOT performed on a generation task , we applied it to a metaphor generator that we built , based on an RNN ( Elman , 1990 ) architecture with LSTM cells ( Hochreiter and Schmidhuber , 1997 ) for training a language model on the language of metaphors , using only metaphors and their topics as input . ( A topic was inserted at the beginning of each input sentence . ) The model was trained to predict the next word in the sentences from our ground truth - a set of 22 , 113 quotes , where each quote contains at least one metaphor and is labeled with a topic . There are 1 , 684 unique topics ( e.g. , \" animals \" , \" fear \" , \" fishing \" , \" grandparents \" , \" happiness \" , \" motives \" , \" politics \" , and more examples listed in Table 2 ) and the dataset is currently available to the public online as part of \" Dr. Mardy 's Dictionary of Metaphorical Quotations \" ( Grothe , 2008 ) . To the trained language model , we apply an inference engine that uses weighted random choice with a \" constraining factor \" to encourage language coherence and originality in the output , and pat - terns of metaphors to encourage the generation of grammatically correct metaphors ( Brooks and Youssef , 2020 ) . The constraining factor , c ( for c \u2265 1 ) , causes the inference engine to select - with a probability of 1 c - the most likely word to appear next . Otherwise , and with a probability of 1 \u2212 1 c , the inference engine will make a weighted random selection . Selecting the most likely next word encourages language coherencey in the output , while weighted random selection encourages originality . ( We found that a constraining factor of 3 or 4 worked best with our model . ) A generated sentence failed the GOT if a fragment of at least 2 words appeared as an \" original \" fragment in the training set ; that is , if the fragment appeared just once in the ground truth . Using our metaphor generator , we generated 500 metaphors from randomly chosen topics . Applying GOT on each of the 500 generated metaphors , we found that only 32 repeated an \" original \" fragment from the training set . From this experiment , we conclude that out of the 500 generated metaphors , 468 of them , or just over 93 % , can be considered original . ( Table 2 provides examples from our metaphor generator on randomly generated topics . )", "entities": [[32, 33, "MethodName", "LSTM"]]}
{"text": "Classical approaches to question calibration are either subjective or require newly created questions to be deployed before being calibrated . Recent works explored the possibility of estimating question difficulty from text , but did not experiment with the most recent NLP models , in particular Transformers . In this paper , we compare the performance of previous literature with Transformer models experimenting on a public and a private dataset . Our experimental results show that Transformers are capable of outperforming previously proposed models . Moreover , if an additional corpus of related documents is available , Transformers can leverage that information to further improve calibration accuracy . We characterize the dependence of the model performance on some properties of the questions , showing that it performs best on questions ending with a question mark and Multiple - Choice Questions ( MCQs ) with one correct choice .", "entities": [[59, 60, "MethodName", "Transformer"], [105, 106, "MetricName", "accuracy"]]}
{"text": "This section describes how we build the different models which are compared with the current state of the art of QDE from text . These models are built upon the two pre - trained language models , finetuning them with two different approaches . The first approach consists in directly fine - tuning the pre - trained model for the task of QDE from text . The second approach is made of two steps : we i ) further pre - train the pre - trained model on the task of Masked Language Modeling ( MLM ) to improve domain knowledge , and subsequently ii ) fine - tune it on the task of QDE from text . This is all done separately for the two datasets : we do not perform any experiments across the two datasets .", "entities": [[91, 94, "TaskName", "Masked Language Modeling"], [95, 96, "DatasetName", "MLM"]]}
{"text": "This is the simplest of the two approaches ; the architecture used for fine - tuning is shown in Figure 1 . Given the pre - trained language model , we stack an additional fully connected layer on top of the network , in order to use that as the new output . Following the fine - tuning guidelines in ( Devlin et al , 2019 ) , we use only the first output of the pre - trained language model . This works since the first output correspond to the special token [ CLS ] which is added at the beginning of the input text and is the only one used for regression and classification . Since question calibration is a regression task , the additional output layer has one neuron , and the weights of the connections with the previous layer are randomly initialized . During fine - tuning , both the weights of the additional layer and the internal weights of the pre - trained language model are updated . For the input , we use the same tokenization and the same encoding as the original models . That is , all the input samples start with the special token [ CLS ] and contain two sentences , separated by the [ SEP ] token ( another special token ) : the first sentence is the ( tokenized ) question while the second one contains the ( tokenized ) choices ' text 1 . Specifically , we experiment with three different encodings for the second sentence : i ) Q only , we leave it empty , thus considering only the text of the question , ii ) Q+correct , we use the text of the correct choice ( s ) ( as in Figure 1 ) , iii ) Q+all , we use the text of all the possible choices , concatenating them in a single sentence . For the ASSISTments dataset , only the first encoding is possible as the text of the choices ( correct answer and distractors ) is not available . We also experimented a fourth approach , considering all the possible choices using several [ SEP ] tokens between each choice ; however , this approach performed largely worse that the others , thus we do not report it here . We believe that the model , in that case , does not have enough training questions to learn the meaning of the additional separators ( BERT and DistilBERT are pre - trained to use only one [ SEP ] token ) .", "entities": [[415, 416, "MethodName", "BERT"], [417, 418, "MethodName", "DistilBERT"]]}
{"text": "Masked Language Modeling ( MLM ) is a fill - in - theblank task , where a word of the input text is substituted by a [ MASK ] token and the model is trained to use the surrounding words to predict the word that was masked . We leverage MLM to perform an additional pre - training of the pre - trained language models before the fine - tuning on QDE from text . Our goal is to let the model learn the questions ' topics more accurately than how it would do with the fine - tuning on QDE only . In order for MLM to be effective , though , we need an additional dataset of documents about the same topics that are assessed by the questions : this is available only for the CloudAcademy dataset , which contains the transcript of some of the video - lectures on the e - learning platform . In practice , we perform pretraining with MLM as follows . We randomly mask 15 % of the words of the available lectures , then train the language model to predict the masked words sentence by sentence . The actual prediction is performed by stacking a fully connected layer and a softmax layer on top of the original pre - trained model : for each masked sentence , this additional layer consumes as input the contextual embedding corresponding to the [ MASK ] token , and tries to predict the word that should be inserted in its place . After pre - training the model on the task of MLM , the additional dense and softmax layers are removed from the network , thus leaving us with and the [ SEP ] token is still used only to indicate the end of the question . We use this naming ( \" Sentence 1 \" and \" Sentence 2 \" ) since it is the one used in the original paper . a pre - trained model which has the same architecture as the original one , with the only difference that all the internal weights were updated during the additional MLM pre - training . The architecture for the final fine - tuning for QDE from text is the same as the one shown in Figure 1 .", "entities": [[0, 3, "TaskName", "Masked Language Modeling"], [4, 5, "DatasetName", "MLM"], [50, 51, "DatasetName", "MLM"], [106, 107, "DatasetName", "MLM"], [165, 166, "DatasetName", "MLM"], [209, 210, "MethodName", "softmax"], [267, 268, "DatasetName", "MLM"], [273, 274, "MethodName", "softmax"], [358, 359, "DatasetName", "MLM"]]}
{"text": "In this work we use the publicly available data collection provided by ASSISTments and the private CloudAcademy data collection . Both data collections are made of two datasets : i ) the Answers dataset ( referred to as A ) and ii ) the Questions dataset ( Q ) . It is important to remark that A and Q are abstract names : we have one A dataset for CloudAcademy and one A dataset for ASSISTments ( similarly for Q ) . A contains the students ' answers : for each one , it stores the user ID , the question ID , the correctness of the answer and a timestamp . Importantly , A contains only \" first timers \" , meaning that we consider only the first interaction between a student and a question . Q contains the textual information about the items : question ID , question text and , in the case of CloudAcademy , the text of the possible choices . For the experiments on CloudAcademy data , we also have access to an additional dataset - referred to as Lectures ( L ) which contains the transcripts of some online lectures available on the platform and is used for the additional MLM pre - training .", "entities": [[207, 208, "DatasetName", "MLM"]]}
{"text": "As displayed in Figure 2 , training is performed in two steps , repeated for the two datasets : i ) the IRT model is trained in order to calibrate the questions and obtain the ground truth difficulties , then ii ) these ground truth latent traits are used as target values to train the model on QDE from text . The first step consists in using A to estimate with IRT the target difficulty of all the questions . Specifically , we use pyirt 6 for the estimation and consider [ \u22125 ; 5 ] as the possible range of difficulties . Difficulties estimated at this stage will later be used as ground truth and therefore are inserted as target values in Q. Then , Q is split into a train dataset ( Q TRAIN ) , used to train our model on QDE from text , and a test dataset ( Q TEST ) , which is used for the final evaluation of the model ; we keep 80 % of the questions for training and 20 % for testing . At training time , we keep a portion of Q TRAIN ( 10 % ) as development set , for hyperparameter tuning . When L is used for pre - training the model , the setup is very similar . The only difference is that the regression model , before being fine - tuned on the task of QDE from text using Q TRAIN , is pre - trained on the task of MLM on L. Being an unsupervised task , we use the whole L for this . Transformers are implemented with the transformers 7 library from HuggingFace ; fine - tuning and pre - training are performed with TensorFlow 8 . Hyperparameters are shown in Appendix C.", "entities": [[256, 257, "DatasetName", "MLM"]]}
{"text": "In this paper we have performed a study of how Transformer models perform in the task of QDE from text , and have proposed a model which outperforms previous approaches . Specifically , the proposed model is built upon a pre - trained BERT language model , which is fine - tuned for the task of QDE from text . Previous approaches either require an additional dataset of documents about the same topics assessed by the questions or can not leverage such information ; differently from them the proposed model is capable of outperforming state of the art approaches being trained only on the text of the questions , and can be further improved if such additional dataset is available . As an outcome from our analysis , we can say that : i ) if an additional dataset is available , BERT with MLM pre - training seems to be the best performing model ; ii ) if the only available data is the text of the questions , DistilBERT might be a better option , as it has basically the same performance as BERT but at a fraction of the computational cost . Furthermore , we studied the effect of some questions characteristics on BERT and R2DE , comparing the two models . We have observed that the magnitude of the error naturally increases with the magnitude of the difficulty ( especially for R2DE ) , but there is not a clear correlation between the input length and the accuracy of the estimation . We have also observed that both models are less accurate in estimating the difficulty of cloze questions , compared to questions that end with a question mark , and that the decrease in accuracy is lower for BERT . We believe that this happens because underscores are not frequent in natural language and thus the model has a chance of learning them only during the fine - tuning on QDE , not during MLM pre - training . This is probably not enough data for learning ( from scratch ) the meaning of underscores in exam questions . Lastly , BERT performs better on questions with only one correct choice than on questions with multiple correct choices ( for the latter , it is also outperformed by R2DE ) . This might be due to the encoding we used for multiple correct choices , and it is worth exploring in future research . Future works could continue to dig deeper into the analysis of the model , as we believe that the accuracy of the estimation could be further improved by using an ensemble model in which different sub - models are used depending on some characteristic of the question under calibration . Also , future works will try to explore the attention layers of the proposed model , as it might provide useful information about the reasons why the model works better on some questions .", "entities": [[10, 11, "MethodName", "Transformer"], [43, 44, "MethodName", "BERT"], [142, 143, "MethodName", "BERT"], [144, 145, "DatasetName", "MLM"], [170, 171, "MethodName", "DistilBERT"], [185, 186, "MethodName", "BERT"], [206, 207, "MethodName", "BERT"], [251, 252, "MetricName", "accuracy"], [289, 290, "MetricName", "accuracy"], [293, 294, "MethodName", "BERT"], [329, 330, "DatasetName", "MLM"], [356, 357, "MethodName", "BERT"], [428, 429, "MetricName", "accuracy"], [468, 470, "HyperparameterName", "attention layers"]]}
{"text": "Copy the running instance using the \" Instance Copy \" command to the EU region . skill levels of the students and the difficulty of the questions via likelihood maximization , by selecting the configuration ( i.e.the \u03b8s and bs ) that maximizes the probability of the observed results . Also , it is possible to assess the knowledge level\u03b8 i of a student i from the correctness of its answers to a set of calibrated assessment items Q = q 1 , q 2 , ... , q Nq . This is done by maximizing the results of the multiplication between the i.r.f . of the questions that were answered correctly and the complementary ( i.e. 1 \u2212 P C ) of the i.r.f . of the questions that were answered erroneously .", "entities": [[89, 90, "DatasetName", "Nq"]]}
{"text": "Interactive topic models are powerful tools for understanding large collections of text . However , existing sampling - based interactive topic modeling approaches scale poorly to large data sets . Anchor methods , which use a single word to uniquely identify a topic , offer the speed needed for interactive work but lack both a mechanism to inject prior knowledge and lack the intuitive semantics needed for userfacing applications . We propose combinations of words as anchors , going beyond existing single word anchor algorithmsan approach we call \" Tandem Anchors \" . We begin with a synthetic investigation of this approach then apply the approach to interactive topic modeling in a user study and compare it to interactive and noninteractive approaches . Tandem anchors are faster and more intuitive than existing interactive approaches . Topic models distill large collections of text into topics , giving a high - level summary of the thematic structure of the data without manual annotation . In addition to facilitating discovery of topical trends ( Gardner et al , 2010 ) , topic modeling is used for a wide variety of problems including document classification ( Rubin et al , 2012 ) , information retrieval ( Wei and Croft , 2006 ) , author identification ( Rosen - Zvi et al , 2004 ) , and sentiment analysis ( Titov and McDonald , 2008 ) . However , the most compelling use of topic models is to help users understand large datasets ( Chuang et al , 2012 ) . Interactive topic modeling allows non - experts to refine automatically generated topics , making topic models less of a \" take it or leave it \" proposition . Including humans input during training improves the quality of the model and allows users to guide topics in a specific way , custom tailoring the model for a specific downstream task or analysis . The downside is that interactive topic modeling is slow - algorithms typically scale with the size of the corpus - and requires non - intuitive information from the user in the form of must - link and can not - link constraints ( Andrzejewski et al , 2009 ) . We address these shortcomings of interactive topic modeling by using an interactive version of the anchor words algorithm for topic models . The anchor algorithm ( Arora et al , 2013 ) is an alternative topic modeling algorithm which scales with the number of unique word types in the data rather than the number of documents or tokens ( Section 1 ) . This makes the anchor algorithm fast enough for interactive use , even in web - scale document collections . A drawback of the anchor method is that anchor words - words that have high probability of being in a single topic - are not intuitive . We extend the anchor algorithm to use multiple anchor words in tandem ( Section 2 ) . Tandem anchors not only improve interactive refinement , but also make the underlying anchor - based method more intuitive . For interactive topic modeling , tandem anchors produce higher quality topics than single word anchors ( Section 3 ) . Tandem anchors provide a framework for fast interactive topic modeling : users improve and refine an existing model through multiword anchors ( Section 4 ) . Compared to existing methods such as Interactive Topic Models , our method is much faster .", "entities": [[1, 3, "TaskName", "topic models"], [135, 137, "TaskName", "Topic models"], [189, 191, "TaskName", "document classification"], [199, 201, "TaskName", "information retrieval"], [222, 224, "TaskName", "sentiment analysis"], [239, 241, "TaskName", "topic models"], [270, 272, "TaskName", "topic models"], [387, 389, "TaskName", "topic models"], [567, 569, "TaskName", "Topic Models"]]}
{"text": "We now describe more concretely how to combine an anchor facets to describe the cooccurrence pattern of our new pseudoword anchor . In tandem anchors , we create vector representations that combine the information from anchor facets . Our anchor facets are G 1 . . . G K , where G k is a set of anchor facets which will form the kth pseudoword anchor . The pseudowords are g 1 . . . g K , where g k is the pseudoword from G k . These pseudowords form the new rows of S. We give several candidates for combining anchors facets into a single multiword anchor ; we compare their performance in Section 3 . Vector Average An obvious function for computing the central tendency is the vector average . For each anchor facet , S g k , j = i G k S i , j | G k | , ( 2 ) where | G k | is the cardinality of G k . Vector average makes the pseudoword S g k , j more central , which is intuitive but inconsistent with the interpretation from Arora et al ( 2013 ) that anchors should be extreme points whose linear combinations explain more central words . Or - operator An alternative approach is to consider a cooccurrence with any anchor facet in G k . For word j , we use De Morgan 's laws to set S g k , j = 1 \u2212 i G k ( 1 \u2212 S i , j ) . ( 3 ) Unlike the average , which pulls the pseudoword inward , this or - operator pushes the word outward , increasing each of the dimensions . Increasing the volume of the simplex spanned by the anchors explains more words . Element - wise Min Vector average and oroperator are both sensitive to outliers and can not account for polysemous anchor facets . Returning to our previous example , both \" camera \" and \" bag \" are bad anchors for camera bags because they appear in documents discussing other products . However , if both \" camera \" and \" bag \" are anchor facets , we can look at an intersection of their contexts : words that appear with both . Using the intersection , the cooccurrence pattern of our anchor facet will only include terms relevant to camera bags . Mathematically , this is an element - wise min operator , S g k , j = min i G k S i , j . ( 4 ) This construction , while perhaps not as simple as the previous two , is robust to words which have cooccurrences which are not unique to a single topic . Harmonic Mean Leveraging the intuition that we should use a combination function which is both centralizing ( like vector average ) and ignores large outliers ( like element - wise min ) , the final combination function is the element - wise harmonic mean . Thus , for each anchor facet S g k , j = i G k S \u22121 i , j | G k | \u22121 . ( 5 ) Since the harmonic mean tends towards the lowest values in the set , it is not sensitive to large outliers , giving us robustness to polysemous words .", "entities": [[63, 64, "DatasetName", "kth"]]}
{"text": "We use the well - known 20 Newsgroups dataset ( 20NEWS ) used in previous interactive topic modeling work : 18 , 846 Usenet postings from 20 different newgroups in the early 1990s . 1 We remove the newsgroup headers from each message , which contain the newsgroup names , but otherwise left messages intact with any footers or quotes . We then remove stopwords and words which appear in fewer than 100 documents or more than 1 , 500 documents . To seed the tandem anchors , we use the titles of newsgroups . To build each multiword anchor facet , we split the title on word boundaries and expand any abbreviations or acronyms . For example , the newsgroup title ' comp.os.mswindows.misc ' becomes { \" computer \" , \" operating \" , \" system \" , \" microsoft \" , \" windows \" , \" miscellaneous \" } . We do not fully specify the topic ; the title gives some intuition , but the topic modeling algorithm must still recover the complete topic - word distributions . This is akin to knowing the names of the categories used but nothing else . Critically , the topic modeling algorithm has no knowledge of document - label relationships .", "entities": [[6, 8, "DatasetName", "20 Newsgroups"]]}
{"text": "Tandem anchors will enable users to direct topic inference to improve topic quality . However , for the algorithm to be interactive we must also consider runtime . Cook and Thomas ( 2005 ) argue that for interactive applications with user - initiated actions like ours the response time should be less than ten seconds . Longer waits can increase the cognitive load on the user and harm the user interaction . 6 Significant at p < 0.01/4 when using two - tailed t - tests with a Bonferroni correction . For each of our evaluations , we verify the normality of our data ( D'Agostino and Pearson , 1973 ) and use two - tailed t - tests with Bonferroni correction to determine whether the differences between the different methods are significant . Fortunately , the runtime of tandem anchors is amenable to interactive topic modeling . On 20NEWS , interactive updates take a median time of 2.13 seconds . This result was obtained using a single core of an AMD Phemon II X6 1090 T processor . Furthermore , larger datasets typically have a sublinear increase in distinct word types , so we can expect to see similar run times , even on much larger datasets . Compared to other interactive topic modeling algorithms , tandem anchors has a very attractive run time . For example , using an optimized version of the sampler for the Interactive Topic Model described by Hu and Boyd - Graber ( 2012 ) , and the recommended 30 iterations of sampling , the Interactive Topic Model updates with a median time of 24.8 seconds ( Hu and Boyd - Graber , 2012 ) , which is well beyond our desired update time for interactive use and an order of magnitude slower than tandem anchors . Another promising interactive topic modeling approach is Utopian ( Choo et al , 2013 ) , which uses non - negative factorization , albeit without the benefit of anchor words . Utopian is much slower than tandem anchors . Even on the small InfoVis - VAST dataset which contains only 515 documents , Utopian takes 48 seconds to converge . While the times are not strictly comparable due to differing datasets , Utopian scales linearly with the size of the data , we can intuit that even for moderately sized datasets such as 20NEWS , Utopian is infeasible for interactive topic modeling due to run time . While each of these interactive topic modeling algorithms do achieve reasonable topics , only our algorithm fits the run time requirements for inter - Figure 2 : Interface for user study with multiword anchors applied to interactive topic modeling . activity . Furthermore , since tandem anchors scales with the size of the vocabulary rather than the size of the data , this trend will only become more pronounced as we increase the amount of data .", "entities": [[348, 349, "DatasetName", "VAST"]]}
{"text": "Given high quality anchor facets , the tandem anchor algorithm can produce high quality topic models ( particularly when the harmonic mean combiner is used ) . Moreover , the tandem anchor algorithm is fast enough to be interactive ( as opposed to model - based approaches such as the Interactive Topic Model ) . We now turn our attention to our main experiment : tandem anchors applied to the problem of interactive topic modeling . We compare both single word and tandem anchors in our study . We do not include the Interactive Topic Model or Utopian , as their run times are too slow for our users .", "entities": [[14, 16, "TaskName", "topic models"]]}
{"text": "To show that interactive tandem anchor words are fast , effective , and intuitive , we ask users to understand a dataset using the anchor word algorithm . For this user study , we recruit twenty participants drawn from a university student body . The student median age is twenty - two . Seven are female , and thirteen are male . None of the students had any prior familiarity with topic modeling or the 20NEWS dataset . Each participant sees a simple user interface ( Figure 2 ) with topic given as a row with two columns . The left column allows users to view and edit topics ' anchor words ; the right column lists the most probable words in each topic . 7 The user can remove an anchor word or drag words from 7 While we use topics generated using harmonic mean for our final analysis , users were shown topics generated using the min combiner . However , this does not change our result . the topic word lists ( right column ) to become an anchor word . Users can also add additional topics by clicking the \" Add Anchor \" to create additional anchors . If the user wants to add a word to a tandem anchor set that does not appear in the interface , they manually type the word ( restricted to the model 's vocabulary ) . When the user wants to see the updated topics for their newly refined anchors , they click \" Update Topics \" . We give each a participant a high level overview of topic modeling . We also describe common problems with topic models including intruding topic words , duplicate topics , and ambiguous topics . Users are instructed to use their best judgement to determine if topics are useful . The task is to edit the anchor words to improve the topics . We asked that users spend at least twenty minutes , but no more than thirty minutes . We repeat the task twice : once with tandem anchors , and once with single word anchors . 8", "entities": [[278, 280, "TaskName", "topic models"]]}
{"text": "Tandem anchors extend the anchor words algorithm to allow multiple words to be combined into anchor facets . For interactive topic modeling , using anchor facets in place of single word anchors produces higher quality topic models and are more intuitive to use . Furthermore , our approach scales much better than existing interactive topic modeling techniques , allowing interactivity on large datasets for which interactivity was previous impossible .", "entities": [[35, 37, "TaskName", "topic models"]]}
{"text": "Separating Retention from Extraction in the Evaluation of End - to - end Relation Extraction", "entities": [[13, 15, "TaskName", "Relation Extraction"]]}
{"text": "State - of - the - art NLP models can adopt shallow heuristics that limit their generalization capability ( McCoy et al , 2019 ) . Such heuristics include lexical overlap with the training set in Named - Entity Recognition ( Taill\u00e9 et al , 2020a ) and Event or Type heuristics in Relation Extraction ( Rosenman et al , 2020 ) . In the more realistic end - to - end RE setting , we can expect yet another heuristic : the mere retention of training relation triples . In this paper we propose several experiments confirming that retention of known facts is a key factor of performance on standard benchmarks . Furthermore , one experiment suggests that a pipeline model able to use intermediate type representations is less prone to over - rely on retention .", "entities": [[53, 55, "TaskName", "Relation Extraction"]]}
{"text": "Information Extraction ( IE ) aims at converting the information expressed in a text into a predefined structured format of knowledge . This global goal has been divided into subtasks easier to perform automatically and evaluate . Hence , Named Entity Recognition ( NER ) and Relation Extraction ( RE ) are two key IE tasks among others such as Coreference Resolution ( CR ) , Entity Linking or Event Extraction . Traditionally performed as a pipeline ( Bach and Badaskar , 2007 ) , these two tasks can be tackled jointly in order to model their interdependency , alleviate error propagation and obtain a more realistic evaluation setting ( Roth and Yih , 2002 ; Li and Ji , 2014 ) . Following the general trend in Natural Language Processing ( NLP ) , the recent quantitative improvements reported on Entity and Relation Extraction benchmarks are at least partly explained by the use of larger and larger pretrained Language Models ( LMs ) such as BERT ( Devlin et al , 2019 ) to obtain contextual word representations . Concurrently , Code for reproducing our evaluation settings is available at github.com/btaille/retex there is a realization that new evaluation protocols are necessary to better understand the strengths and shortcomings of the obtained neural network models , beyond a single holistic metric on an hold - out test set ( Ribeiro et al , 2020 ) . In particular , generalisation to unseen data is a key factor in the evaluation of deep neural networks . It is all the more important in IE tasks that revolve around the extraction of mentions : small spans of words that are likely to occur in both the evaluation and training datasets . This lexical overlap has been shown to be correlated to neural networks performance in NER ( Augenstein et al , 2017 ; Taill\u00e9 et al , 2020a ) . For pipeline RE , Rosenman et al ( 2020 ) and Peng et al ( 2020 ) expose shallow heuristics in neural models : relying too much on the type of the candidate arguments or on the presence of specific triggers in their contexts . In end - to - end Relation Extraction , we can expect that these NER and RE heuristics are combined . In this work , we argue that current evaluation benchmarks measure both the desired ability to extract information contained in a text but also the capacity of the model to simply retain labeled ( head , predicate , tail ) triples during training . And when the model is evaluated on a sentence expressing a relation seen during training , it is hard to disentangle which of these two behaviours is predominant . However , we can hypothesize that the model can simply retrieve previously seen information acting like a mere compressed form of knowledge base probed with a relevant query . Thus , testing on too much examples with seen triples can lead to overestimate the generalizability of a model . Even without labeled data , LMs are able to learn some relations between words that can be probed with cloze sentences where an argument is masked ( Petroni et al , 2019 ) . This raises the additional question of lexical overlap with the orders of magnitude larger unlabeled LM pretraining corpora that will remain out of scope of this paper .", "entities": [[39, 42, "TaskName", "Named Entity Recognition"], [43, 44, "TaskName", "NER"], [46, 48, "TaskName", "Relation Extraction"], [60, 62, "TaskName", "Coreference Resolution"], [66, 68, "TaskName", "Entity Linking"], [69, 71, "TaskName", "Event Extraction"], [143, 145, "TaskName", "Relation Extraction"], [158, 161, "TaskName", "pretrained Language Models"], [166, 167, "MethodName", "BERT"], [303, 304, "TaskName", "NER"], [369, 371, "TaskName", "Relation Extraction"], [377, 378, "TaskName", "NER"]]}
{"text": "We study three recent end - to - end RE models on CoNLL04 ( Roth and Yih , 2004 ) , ACE05 ( Walker et al , 2006 ) and SciERC ( Luan et al , 2018 ) . They rely on various pretrained LMs and for a fairer comparison , we use BERT ( Devlin et al , 2019 ) on ACE05 and CoNLL04 and SciBERT ( Beltagy et al , 2019 ) on SciERC 1 . PURE ( Zhong and Chen , 2021 ) follows the pipeline approach . The NER model is a classical span - based model ( Sohrab and Miwa , 2018 ) . Special tokens corresponding to each predicted entity span are added and used as representation for Relation Classification . For a fairer comparison with other models , we study the approximation model that only requires one pass in each encoder and limits to sentence - level prediction . However , it still requires finetuning and storing two pretrained LMs instead of a single one for the following models . SpERT ( Eberts and Ulges , 2020 ) uses a similar span - based NER module . RE is performed based on the filtered representations of candidate arguments as well as a max - pooled representation of their middle context . While Entity Filtering is close to the pipeline approach , the NER and RE modules share a common entity representation and are trained jointly . We also study the ablation of the max - pooled context representation that we denote Ent - SpERT . Two are better than one ( TABTO ) ( Wang and Lu , 2020 ) intertwines a sequence encoder and a table encoder in a Table Filling approach ( Miwa and Sasaki , 2014 ) . Contrary to previous models the pretrained LM is frozen and both the final hidden states and attention weights are used by the encoders . The prediction is finally performed by a Multi - Dimensional RNN ( MD - RNN ) . Because it is not based on span - level predictions , this model can not detect nested entities , e.g. on SciERC .", "entities": [[30, 31, "DatasetName", "SciERC"], [53, 54, "MethodName", "BERT"], [75, 76, "DatasetName", "SciERC"], [92, 93, "TaskName", "NER"], [124, 126, "TaskName", "Relation Classification"], [191, 192, "TaskName", "NER"], [229, 230, "TaskName", "NER"], [360, 361, "DatasetName", "SciERC"]]}
{"text": "We first observe very different statistics of Mention and Relation Lexical Overlap in the three datasets , which can be explained by the singularities of their entities and relations . In CoNLL04 , mentions are mainly Named Entities denoted with proper names while in ACE05 the surface forms are very often common names or even pronouns , which explains the occurrence of training entity mentions such as \" it \" , \" which \" , \" people \" in test examples . This also leads to a weaker entity label consistency ( Fu et al , 2020a ) : \" it \" is labeled with every possible entity type and appears mostly unlabeled whereas a mention such as \" President Kennedy \" is always labeled as a person in CoNLL04 . Similarly , mentions in SciERC are common names which can be tagged with different labels and they can also be nested . Both the poor label consistency as well as the nested nature of entities hurt the performance of the retention heuristic . For RE , while SciERC has almost no exact overlap between test and train relations , ACE05 and CoNLL04 have similar levels of exact match . The larger proportion of partial match in ACE05 is explained by the pronouns that are more likely to co - occur in several instances . The difference in performance of the heuristic is also explained by a poor relation label consistency .", "entities": [[135, 136, "DatasetName", "SciERC"], [178, 179, "DatasetName", "SciERC"], [197, 199, "MetricName", "exact match"]]}
{"text": "While we can not evaluate TABTO on SciERC because it is unfit for extraction of nested entities , we can notice different hierarchies of models on every dataset suggesting that there is no one - size - fits - all best model , at least in current evaluation settings . The most obvious comparison is between SpERT and Ent - SpERT where the explicit representation of context is ablated . This results in a loss of performance on the RE part and especially on partially matching or new relations for which the entity representations pairs have not been seen . Ent - SpERT is particularly effective on Exact Matches on CoNLL04 , suggesting its retention capability . Other comparisons are more difficult , given the numerous variations between the very structure of each model as well as training procedures . However , the PURE pipeline setting seems to only be more effective on ACE05 where its NER performance is significantly better , probably because learning a separate NER and RE encoder enables to learn and capture more specific information for each distinctive task . Even then , TABTO yields better Boundaries performance only penalized on the Strict setting by entity types confusions . On the contrary , on CoNLL04 , TABTO significantly outperforms its counterparts , especially on unseen relations . This indicates that it proposes a more effective incorporation of contextual information in this case where relation and argument types are mapped bijectively . On SciERC , performance of all models is already compromised at the NER level before the RE step , which makes further distinction between model performance even more difficult .", "entities": [[7, 8, "DatasetName", "SciERC"], [74, 75, "MetricName", "loss"], [156, 157, "TaskName", "NER"], [167, 168, "TaskName", "NER"], [246, 247, "DatasetName", "SciERC"], [257, 258, "TaskName", "NER"]]}
{"text": "Ground Truth Relation Original John Wilkes Booth , who assassinated President Lincoln , was an actor . ( John Wilkes Booth , Kill , President Lincoln ) Swapped President Lincoln , who assassinated John Wilkes Booth , was an actor . ( President Lincoln , Kill , John Wilkes Booth ) similarly to what is proposed in ( Ribeiro et al , 2020 ) . We propose a very focused experiment that consists in selecting asymmetric relations that occur between entities of same type and swap the head with the tail in the input . If the model predicts the original triple , then it over relies on the retention heuristic , whereas finding the swapped triple is an evidence of broader context incorporation . We show an example in Table 2 . Because of the requirements of this experiment , we have to limit to two relations in CoNLL04 : \" Kill \" between people and \" Located in \" between locations . Indeed , CoNLL04 is the only dataset with a bijective mapping between the type of a relation and the types of its arguments and the consistent proper nouns mentions makes the swaps mostly grammatically correct . For each relation type , we only consider sentences with exactly one instance of corresponding relation and swap its arguments . We only consider this relation in the RE scores reported in Table 3 . We use the strict RE score as well as revRE which measures the extraction of the reverse relation , not expressed in the sentence . For each relation , the hierarchy of models corresponds to the overall CoNLL04 . Swapping arguments has a limited effect on NER , mostly for the \" Located in \" relation . However , it leads to a drop in RE for every model and the revRE score indicates that SpERT and TABTO predict the reverse relation more often than the newly expressed one . This is another proof of the retention heuristic of end - to - end models , although it might also be attributed to the language model to the language model . In particular for the \" Located in \" relation , swapped heads and tails are not exactly equivalent since the former are mainly cities and the latter countries . On the contrary , the PURE model is less prone to information retention , as shown by its revRE scores significantly smaller than the standard RE scores on swapped sentences . Hence , it outperforms SpERT and TABTO on swapped sentences despite being the least effective on the original dataset . The important discrepancy in results can be explained by the different types of representations used by these models . The pipeline approach allows the use of argument type representations in the Relation Classifier whereas most end - to - end models use lexical features in a shared entity representation used for both NER and RE . These conclusions from quantitative results are validated qualitatively . We can observe that the four predominant patterns are intuitive behaviours on sentences with swapped relations : retention of the incorrect original triple , prediction of the correct swapped triple and prediction of none or both triples . We report some examples in Table 9 and Table 10 in the Appendix .", "entities": [[281, 282, "TaskName", "NER"], [488, 489, "TaskName", "NER"]]}
{"text": "In this paper , we study three state - of - the - art endto - end Relation Extraction models in order to highlight their tendency to retain seen relations . We confirm that retention of seen mentions and relations play an important role in overall RE performance and can explain the relatively higher scores on CoNLL04 and ACE05 compared to SciERC . Furthermore , our experiment on swapping relation heads and tails tends to show that the intermediate manipulation of type representations instead of lexical features enabled in the pipeline PURE model makes it less prone to over - rely on retention . While the limited extend of our swapping experiment is an obvious limitation of this work , it shows limitations of both current benchmarks and models . It is an encouragement to propose new benchmarks that might be easily modified by design to probe such lexical overlap heuristics . Contextual information could for example be contained in templates of that would be filled with different ( head , tail ) pairs either seen or unseen during training . Furthermore , pretrained Language Models can already capture relational information between phrases ( Petroni et al , 2019 ) and further experiments could help distinguish their role in the retention behaviour of RE models .", "entities": [[17, 19, "TaskName", "Relation Extraction"], [61, 62, "DatasetName", "SciERC"], [183, 186, "TaskName", "pretrained Language Models"]]}
{"text": "HacRED : A Large - Scale Relation Extraction Dataset Toward Hard Cases in Practical Applications", "entities": [[6, 8, "TaskName", "Relation Extraction"]]}
{"text": "A series of datasets have been built for RE as of late , which have extraordinarily advanced the improvement of RE systems . RE datasets such as SemEval - 2010 Task 8 ( Hendrickx et al , 2009 ) and ACE05 are constructed through human annotation with relatively limited relation types and size . A large - scale dataset TACRED ( Zhang et al , 2017 ) is obtained via crowdsourcing to satisfy the training of data - hungry models . As RE applications differ much in various scenarios , constructing datasets aimed at specific targets is a popular trend in RE . DocRED ( Yao et al , 2019 ) is constructed to accelerate the research on document - level RE . To meet the challenges of fewshot RE , FewRel ( Han et al , 2018 ) as well as FewRel 2.0 ( Gao et al , 2019 ) have been presented . RELX ( Koksal and Ozgur , 2020 ) is a benchmark for cross - lingual RE . Jia et al ( 2020 ) propose the task of interpersonal RE in dyadic dialogues and further construct a corresponding dataset called DDRel . Compared with previous RE datasets , HacRED is derived from the analysis of the performance gap between popular datasets and practical applications . It targets towards promoting the RE models to extract information from the complex contexts .", "entities": [[59, 60, "DatasetName", "TACRED"], [103, 104, "DatasetName", "DocRED"], [131, 132, "DatasetName", "FewRel"], [142, 144, "DatasetName", "FewRel 2.0"], [155, 156, "DatasetName", "RELX"], [194, 195, "DatasetName", "DDRel"]]}
{"text": "To analyze where models struggle in practical instances and distinguish the hard cases , we conduct a manual exploratory analysis on the errorprone instances of SOTA models ( CGCN , CasRel , DGCNN - BERT ) on NYT , DuIE and industry data . Then we formulate the potential causes of the errors with nine indicators illustrated as follows : Text Length . We notice that models tend to fail on instances with longer text . The experiments of Alt et al ( 2020 ) also reflect that RE models get a relatively higher error rate with the length of sentence greater than 30 in TACRED . Argument Distance . We observe that the performance of the models declines when the arguments ( i.e. , head and tail entity mentions ) are far away , especially in inter - sentence RE . Distractors . Extracting triples in contexts with linguistic distractors is tough for current models . For example , drop out will contribute to wrong relation graduate_from between entity mentions with PERSON and SCHOOL type . Reasoning . Reasoning is needed to extract the relation mentioned implicitly in the text . Recent work suggests that future researchers consider incorporating common sense knowledge or improved causal modules in RE tasks ( Han et al , 2018 ) . Homogeneous Entities . The context contains multiple homogeneous entity mentions with iden - Similar Relations . Models struggle to identify the correct relation among those semantically similar ones concurrently mentioned in context . A sharp decrease is also found in few - shot RE when selecting N similar relations on N - way K - shot settings ( Han et al , 2020 ) . Long - tail Relations . Only a handful instances are available for long - tail relations in common datasets . Current data - hungry models struggle to learn the semantic patterns on these relations . Multiple Triples . Models always get a poor performance on the instances with numerous triples . Overlapping Triples . Different triples involve the identical entity mentions . Many existing models can not well handle the EntityPairOverlap and SingleEntityOverlap ( Zeng et al , 2018 ) instances . Table 1 provides various examples from NYT and corresponding hard case indicators . In Table 2 , the proportion growing on the error instances reflects the gap between existing datasets and practical data , which also proves the effectiveness of these indicators .", "entities": [[32, 33, "MethodName", "DGCNN"], [34, 35, "MethodName", "BERT"], [105, 106, "DatasetName", "TACRED"]]}
{"text": "The overall architecture of the proposed caseoriented construction framework is illustrated in Figure 2 . Different from previous works ( Zhang et al , 2017 , Zaporojets et al , 2020 which start crowdsourcing annotation straight after the data collection stage , we introduce additional stages of hard case feature engineering and target instance prediction . Moreover , we design a novel three - stage annotation method and employ CrowdTruth2.0 .", "entities": [[49, 51, "TaskName", "feature engineering"]]}
{"text": "To avoid data bias to high - frequency entities and relations , we first obtain about 5 million plain texts and 800 thousand triples from CN - DBpedia . The abundant texts and triples contribute to a more reasonable distribution . We use fine - grained named entity recognition ( NER ) toolkit TexSmart ( Zhang et al , 2020 ) and entity linking ( Chen et al , 2018 ) to align mentioned entities in texts to those in triples . Finally , we construct a distantly supervised dataset D ds with 1.6 million instances , where we select challenging instances in the following steps .", "entities": [[27, 28, "DatasetName", "DBpedia"], [46, 49, "TaskName", "named entity recognition"], [50, 51, "TaskName", "NER"], [62, 64, "TaskName", "entity linking"]]}
{"text": "It is impossible to manually select all instances to construct a large - scale dataset . So we utilize a classifier to recall more hard cases similar to the seed samples selected by experts . The classifiers consist of three categories : ( 1 ) Decision tree ( Quinlan , 1986 ) ; ( 2 ) Deep classifiers by positive negative ( PN ) learning ( Rakhlin , 2016 ) ; ( 3 ) Deep classifiers by positive unlabeled ( PU ) learning ( Kiryo et al , 2017 ; du Plessis et al , 2015 ) . First of all , we adopt the decision tree to make the classifier aware of the indicators explicitly . Then , we form the representation vector as recommended in Baldini Soares et al ( 2019 et al , 1998 ) and BiLSTM ( Hochreiter and Schmidhuber , 1997 ) , to capture the context information . More training details can be found in Appendix B. We ensemble multiple classifiers by weighted average and distinguish hard cases with high confidence in the original massive unlabeled dataset . Besides , we directly select instances by implicit semantic patterns to explore more hard cases fitting the indicator of Reasoning which is not well quantified by the auxiliary features . Finally , we obtain the dataset D hc ready for annotation .", "entities": [[139, 140, "MethodName", "BiLSTM"]]}
{"text": "To make instances in D hc fully and accurately labeled , we develop a novel three - stage RE annotation platform taking the following two aspects into consideration : ( 1 ) Heavy workload of annotating all information at once results in growing negative feedback as the task goes on ; ( 2 ) Aggregated method , such as majority vote ( Dumitrache et al , 2018 ) , is insufficient for complicated and openended tasks . To relieve the pressure of workers , we divide the whole task into three partitions consisting of Relation Annotation , Entity Annotation , and Triple Annotation . Moreover , we utilize patterns and toolkits to provide high - quality recommendations in each stage for higher recall . To capture the label disagreement more thoroughly among workers , we employ CrowdTruth2.0 ( Dumitrache et al , 2018 ) , which models the quality of workers , documents , and annotations . In short , in the Relation Annotation , workers select the missed relations or delete wrong recom - mended ones . When all relations are annotated , NER toolkit recommends multiple entity mentions with the corresponding type based on schema information . Workers also need to append new entity mentions or delete incorrect ones in the Entity Annotation . As for Triple Annotation , workers verify the correctness of a candidate triples automatically generated by permutation of entity arguments and relations based on schema . Note that every input data in the three stage is assigned to three different annotators and aggregated by CrowdTruth2.0 . Detailed annotation process is in Appendix D.", "entities": [[184, 185, "TaskName", "NER"]]}
{"text": "We randomly select 200 contexts from test set and ask three volunteers to extract relational facts in an end - to - end manner . Schema information like entity type set as well as relation set is provided but no entity mentions . As for relation classification task , three volunteers select the relation , including NA regarded as negative , of the given entity pair . As demonstrated in Table 9 , humans fulfill excellent results which indicate the possible ceiling performance on HacRED .", "entities": [[45, 47, "TaskName", "relation classification"]]}
{"text": "We illustrate the three - stage annotation method . Given the context in Figure 5 , director , cast_member , and adapted_by is appended to the annotation of Stage 1 by relational pattern . Crowdsourcing workers select the missing relation such as author . When all relation mentions are annotated , NER toolkit recommend multiple entity mentions with the corresponding type . Workers need to select the highlighted words that are not covered by entity recommendation in the Stage 2 . After stage 2 , all mentions in context with specific type are obtained . As the example shown in Figure 5 , given the target entity type of PERSON , platform recommends the candidates including PERSON - 1 to PERSON - 4 . Workers select highlighted words PERSON - 5 which is missed . In the final stage , we generate the candidate triples automatically by permutation of arguments and relations based on triple schema . Due to the relation director connects arguments with entity type PERSON and FILM , we generate the triple ( PERSON - 2 , director , FILM ) and ask annotator to verify the correctness . Note that we employ the powerful quality control method crowdtruth2.0 in every stages to prevent error propagation . As a result , all triples marked as valid are saved . E Calculation of the UQS , AQS , and WQS Metrics in CrowdTruth2.0 We give the details of the calculation in data quality evaluation . We calculate the three metric unit quality score ( UQS ) , annotation quality score ( AQS ) , and worker quality score ( WQS ) by CrowdTruth2.0 ( Dumitrache et al , 2018 ) on the whole 9 , 231 instances in HacRED proposed as follows , where W 1 , W 2 is the weight of the iteration method and is initialized as one , u is the unit for annotation , a is one annotation given a unit , i , j denotes the different workers . We straightforward report the average of these metrics in Section 5.1 . U QS ( u ) = \u2211 i , j W1 ( i , j , u ) W QS ( i ) W QS ( j ) \u2211 i , j W QS ( i ) W QS ( j ) ( 2 ) AQS ( a ) = \u2211 i , j W QS ( i ) W QS ( j ) Pa ( i | j ) \u2211 i , j W QS ( i ) W QS ( j ) ( 3 ) W1 ( i , j , u ) W QS ( j ) U QS ( u ) \u2211 j , u W QS ( j ) U QS ( u ) W QS ( i ) = W U A ( i ) W W A ( i ) W U A ( i ) = \u2211 u W2 ( u , i ) U QS ( u ) \u2211 u U QS ( u ) W W A ( i ) = \u2211 j , u ( 4 )", "entities": [[51, 52, "TaskName", "NER"]]}
{"text": "Task - oriented dialog systems are becoming pervasive , and many companies heavily rely on them to complement human agents for customer service in call centers . With globalization , the need for providing cross - lingual customer support becomes more urgent than ever . However , cross - lingual support poses great challenges - it requires a large amount of additional annotated data from native speakers . In order to bypass the expensive human annotation and achieve the first step towards the ultimate goal of building a universal dialog system , we set out to build a cross - lingual state tracking framework . Specifically , we assume that there exists a source language with dialog belief tracking annotations while the target languages have no annotated dialog data of any form . Then , we pre - train a state tracker for the source language as a teacher , which is able to exploit easy - to - access parallel data . We then distill and transfer its own knowledge to the student state tracker in target languages . We specifically discuss two types of common parallel resources : bilingual corpus and bilingual dictionary , and design different transfer learning strategies accordingly . Experimentally , we successfully use English state tracker as the teacher to transfer its knowledge to both Italian and German trackers and achieve promising results .", "entities": [[199, 201, "TaskName", "transfer learning"]]}
{"text": "Over the past few years , we have witnessed the burgeoning of real - world applications of dialog systems , with many academic , industrial , and startup efforts racing to lead the widely - believed next - generation human - machine interfaces . As a result , numerous task - oriented dialog systems such as virtual assistants and customer conversation services were developed Rojas - Barahona et al , 2017 ; Bordes and Weston , 2017 ; Williams et al , 2017 ; Li et al , 2017 ) , with Google Duplexbeing the most recent example . With the rapid process of globalization , more countries have observed growing populations of immigrants , and more companies have moved forward to develop their overseas business sectors . To provide better customer service and bring down the cost of labor at call centers , the development of universal dialog systems has become a practical issue . A straightforward strategy is to separately collect training data and train dialog systems for each language . However , it is not only tedious but also expensive . Two settings naturally arise for more efficient usage of the training data : ( 1 ) Multi - lingual setting : we annotate data for multiple languages and train a single model , with possible innovations on joint training . ( 2 ) Crosslingual setting : we annotate data and train a model for only one ( popular ) language , and transfer the learned knowledge to other languages . Here we are interested in the second case , and the important research question we ask is : How can we build cross - lingual dialog systems that can support less popular , low - or even zero - resource languages ? As an initial step towards cross - lingual dialog systems , we focus on the cornerstone of dialog systems - dialog state tracking ( DST ) , or belief tracking , a key component for understanding user inputs and updating belief state , i.e. , a system 's internal representation of the state of conversation ( Young et al , 2010 ) . Based on the perceived belief state , the dialog manager can decide which action to take , and what verbal response to generate ( Precup and Teh , 2017 ; Bordes and Weston , 2017 ) . DST models require a considerable amount of annotated data for training ( Henderson et al , 2014b ; . For a common dialog shown in Figure 1 , a typical data acquisition process ( Rojas - Barahona et al , 2017 ) not only requires two human users to converse for multiple turns but also requires annotators to identify user 's intention in each turn . Such two - step annotation is very expensive , especially for rare languages . We study the novel problem of cross - lingual DST , where one leverages the annotated data of a source language to train DST for a target language with zero annotated data ( Figure 1 ) ; no conversation dialog or dialog state annotation is available for the target language . In order to deal with this zero - resource challenging scenario , we first decouple the state - of - the - art neural belief tracker framework into sub - modules , namely utterance encoder , context gate , and slotvalue decoder . By introducing a teacher - student framework , we are able to transfer knowledge across languages module by module , following the divide - and - conquer philosophy . Requiring no target - side dialog data , our method relies on other easy - to - access parallel resources to understand the connection between languages . Depending on the popularity and availability of target language resources , we study two kinds of parallel data : bilingual corpus and bilingual dictionary , and we respectively design two transfer learning strategies . We use the popular Wizard - of - Oz ( Rojas - Barahona et al , 2017 ) dataset as our DST benchmark to evaluate the effectiveness of our crosslingual transfer learning . We specify English as the source ( primary ) language and two different European languages ( German and Italian ) as our zero - annotation target languages . Compared with an array of alternative transfer learning strategies , our cross - lingual DST models consistently achieve promising results in both scenarios for both zero - annotation languages . To ensure reproducibility , we release our code , training data and parallel resources in the github 1 . Our main contributions are three - fold : Towards building cross - lingual dialog systems , we are the first to study the crosslingual dialog state tracking problem . We systematically study different scenarios for this problem based on the availability of parallel data and propose novel transfer learning methods to tackle the problem . We empirically demonstrate the efficacy of the proposed methods , showing that our methods can accurately track dialog states for 1 https://github.com/wenhuchen/ Cross - Lingual - NBT languages with zero annotated data .", "entities": [[92, 93, "DatasetName", "Google"], [445, 446, "DatasetName", "converse"], [656, 658, "TaskName", "transfer learning"], [664, 669, "DatasetName", "Wizard - of - Oz"], [690, 692, "TaskName", "transfer learning"], [727, 729, "TaskName", "transfer learning"], [817, 819, "TaskName", "transfer learning"]]}
{"text": "Cross - lingual transfer learning has been a very popular topic during the years , which can be seen as a transductive process . In such process , the input domains of the source and target are different ( Pan and Yang , 2010 ) since each language has its own distinct lexicon . By discovering the underlying connections between the source and target domain , we could design transfer algorithms for different tasks . Recently , algorithms have been successfully designed for POS tagging ( Zhang et al , 2016 ; , NER ( Pan et al , 2017 ; Ni et al , 2017 ) as well as image captioning ( Miyazaki and Shimizu , 2016 ) . These methods first aim at discovering the relatedness between two languages and separate languagecommon modules from language - specific modules , then resort to external resources to transfer the knowledge across the language boundary . Our method addresses the transfer learning using a teacher - student framework and proposes to use the teacher to gradually guide the student to make more proper decisions . The dialog states are defined as a set of search constraints ( i.e. informable slots or goals ) that the user specified through the dialog and a set of attribute questions regarding the search results ( i.e. requestable slots or requests ) . The objective of dialog state tracking ( DST ) is to predict and track the user intention ( i.e. , the values of the aforementioned slots ) at each time step based on the current user utterance and the entire dialog history . As shown in Figure 2 , for each slot , the DST computes an output distribution of the candidate values using three inputs : ( i ) system response a t , which is the sentence generated by the system , ( ii ) utterance u t , which is the sentence from the user , and ( iii ) previous state , which denotes the selected slot - value pairs . We define the ontology of the dialog system to be the set of all the possible words the dialog slot and value can take . In this paper , we are interested in learning a cross - lingual DST . Specifically , we assume that the DST for the source language has access to a human - annotated training dataset D while the DSTs for the target languages do not have access to annotated data in other languages except for testing data . We here mainly consider two different types of parallel resources to assist the transfer learning : ( 1 ) Bilingual Corpus , where abundant bilingual corpora exist between the source and the target languages . This is often the case for common language pairs like German , Italian , and French , etc . ( 2 ) Bilingual Dictionary , where public bilingual dictionaries exist between the source and the target languages , but large - scaled parallel corpus are harder to obtain . This can be the case for rarer languages like Finnish , Bulgarian , etc . Furthermore , we assume that all the languages share a common multi - lingual database , whose column / row names and entry values are stored via multiple languages ( see the database in Figure 1 ) . That is , the ontology of dialog among different languages is known with a one - to - one mapping between them ( e.g. , greek = griechisch = greco , food = essen = cibo ) . Based on that , we could construct a mapping function M to associate the ontology terms from different languages with predesigned language - agnostic concepts : for exam - ple , M ( f oods ) = M ( Essen ) = M ( Cibo ) = FOOD . We illustrate our problem definition in Fig - ure 2 .", "entities": [[0, 4, "TaskName", "Cross - lingual transfer"], [93, 94, "TaskName", "NER"], [110, 112, "TaskName", "image captioning"], [159, 161, "TaskName", "transfer learning"], [345, 346, "MethodName", "ontology"], [438, 440, "TaskName", "transfer learning"], [566, 567, "MethodName", "ontology"], [614, 615, "MethodName", "ontology"]]}
{"text": "The second part is the context gate , which takes the system acts a t = ( t q , t s , t v ) 2 tq represents the system request , ts , tv represents the system confirmation . If the system wants to request some information from the user by asking \" what 's your favorite area ? \" , then NBT sets tq=\"AREA \" . If the system wants to confirm some information from a user by asking \" should I try Persian restaurants in the north ? \" then NBT sets ts , tv=\"area , north \" . and the candidate slot - value pair ( c s , c v ) as its inputs and filter out the desired information from the encoded utterance . The context gate g is a sum of three separate gates : g ( c s , c v , a t ) = g 1 + g 2 + g 3 ( 1 ) where the individual gates are defined as : g 1 = \u03c3 ( W s c ( c s + c v ) + b s c ) g 2 = ( c s W q t t q ) [ 1 , , 1 ] H g 3 = ( c s W s t t s ) ( c v W v t t v ) [ 1 , , 1 ] H ( 2 ) where W s c , W q t , W s t , W v t R H\u00d7H are the weight matrices , and and denote the Hadamard product and the inner product , respectively . The three gates g 1 R H , g 2 R H , g 3 R H model the relevance between the candidate slot and value , the system request and the system confirms , respectively . The transformation matrices W q t , W s t , W v t are added to the original NBT to increase the model flexibility of the gates .", "entities": [[33, 34, "MethodName", "ts"], [96, 97, "MethodName", "ts"]]}
{"text": "In this section , we develop a cross - lingual Neural Belief Tracker ( XL - NBT ) that distills knowledge from one NBT to another using a teacherstudent framework . We assume the ontology mapping M is known a priori ( see Figure 3 ) . XL - NBT uses language - specific utterance encoder and context gate for each input language while sharing a common ( language - agnostic ) slot - value decoder across different languages ( see Figure 3 ) . The key idea is to optimize the language - specific components of the student network ( NBT of the target language ) so that their outputs are languageagnostic . This is achieved by making these outputs close to that of the teacher network ( NBT of the source language ) , as we detail below .", "entities": [[34, 35, "MethodName", "ontology"]]}
{"text": "The Wizard of Oz ( WOZ ) ( Rojas - Barahona et al , 2017 ) dataset is used for training and evaluation , which consists of user conversations with taskoriented dialog systems designed to help users find suitable restaurants around Cambridge , UK . The corpus contains three informable ( i.e. goaltracking ) slots : FOOD , AREA , and PRICE . The users can specify values for these slots in order to find which best meet their criteria . Once the system suggests a restaurant , the users can ask about the values of up to eight requestable slots ( PHONE NUMBER , ADDRESS , etc . ) . Multilingual WOZ 2.0 has expanded this dataset to include more dialogs and more languages . The train , valid and test datasets for three different languages ( English , German , Italian ) are available online 3 . We use the English as source language where 600 dialogs are used for training , 200 for validation and 400 for testing . We use the German and Italian as the target language to transfer our knowledge from English DST system . In the experiments , we do not have access to any training or validation dataset for German and Italian , and we only have access to their testing dataset which is composed of 400 dialogs . For external resource , we use the IWSLT2014 Ted Talk parallel corpus ( Mauro et al , 2012 ) from the official website 4 for bilingual corpus scenario . In the IWSLT2014 parallel corpus , we only keep the sentences between 4 and 40 words and decrease the sentence pairs to around 150K. We use Panlex ( Kamholz et al , 2014 ) as our data source and crawl translations for all the words appearing in the dialog datasets to build our bilingual dictionary . We specifically investigate two kinds of pretrained embedding , and we use Glove ( Pennington et al , 2014 ) as the monolingual embedding and MUSE ( Conneau et al , 2017 ) as the bilingual embedding to see their impacts on the DST performance . We split the raw DST corpus into turn - level examples . During training , we use the ground truth previous state V t\u22121 as inputs . At test time , we use the model searched states as the previous state to continue tracking intention until the end of the dialog . When the dialog terminates , we use two evaluation metrics introduced in Henderson et al ( 2014a ) to evaluate the DST performance : ( 1 ) Goals : the proportion of dialog turns where all the users search goal constraints were correctly identified . ( 2 ) Requests : similarly , the proportion of dialog turns where users requests for information were identified correctly . Our implementation is based on the NBT 5 , the details of our system setting are described in the appendix .", "entities": [[41, 42, "DatasetName", "Cambridge"], [204, 206, "DatasetName", "validation dataset"], [282, 283, "DatasetName", "Panlex"], [337, 338, "DatasetName", "MUSE"]]}
{"text": "In our paper , we propose a novel teacher - student framework to perform cross - lingual transfer learning for DST . The key idea of our model is to decouple the current DST neural network into two separate modules and transfer them separately . We believe our method can be further extended into a general purpose multi - lingual transfer framework to resolve other NLP matching or classification problems .", "entities": [[14, 18, "TaskName", "cross - lingual transfer"]]}
{"text": "We are gratefully supported by a Tencent AI Lab Rhino - Bird Gift Fund . We are also very thankful for the public belief tracking code and multilingual state - tracking datasets released by Nikola Mrksic from the University of Cambridge .", "entities": [[40, 41, "DatasetName", "Cambridge"]]}
{"text": "A Multi - Type Multi - Span Network for Reading Comprehension that Requires Discrete Reasoning", "entities": [[9, 11, "TaskName", "Reading Comprehension"]]}
{"text": "In the reading comprehension task that requires discrete reasoning , a passage and a question are given . The goal is to predict an answer to the question by reading and understanding the passage . Unlike previous dataset such as SQuAD ( Rajpurkar et al , 2016 ) where the answer is limited to be a single span of text , DROP loosens the constraint so that the answer involves various types such as number , date , or span of text ( Figure 1 ) . Moreover , the answer can be multiple text strings instead of single continuous span ( A 2 ) . To suc - Passage : As of the census of 2000 , there were 218 , 590 people , 79 , 667 households , ... 22.5 % were of German people , 13.1 % Irish people , 9.8 % Italian people , ... Q1 : Which group from the census is larger : German or Irish ? A1 : German Q2 : Which ancestral groups are at least 10 % ? A2 : German , Irish Q3 : How many more people are there than households ? A3 : 138 , 923 Q4 : How many percent were not German ? A4 : 77.5 Figure 1 : Question - answer pairs along with a passage from the DROP dataset . cessfully find the answer , some discrete reasoning abilities , such as sorting ( A 1 ) , subtraction ( A 3 ) , and negation ( A 4 ) , are required .", "entities": [[2, 4, "TaskName", "reading comprehension"], [40, 41, "DatasetName", "SQuAD"], [61, 62, "DatasetName", "DROP"], [223, 224, "DatasetName", "DROP"]]}
{"text": "Figure 2 gives an overview of our model that aims to combine neural reading comprehension with numerical reasoning . Our model uses BERT ( Devlin et al , 2019 ) as encoder : we map word embeddings into contextualized representations using pre - trained Transformer blocks ( Vaswani et al , 2017 ) ( 3.1 ) . Based on the representations , we employ a multi - type answer predictor that is able to produce four answer types : ( 1 ) span from the text ; ( 2 ) arithmetic expression ; ( 3 ) count number ; ( 4 ) negation on numbers ( 3.2 ) . Following Dua et al ( 2019 ) , we first predict the answer type of a given passage - question pair , and then adopt individual prediction strategies . To support multispan extraction ( 3.3 ) , the model explicitly predicts the number of answer spans . It then outputs non - overlapped spans until the specific amount is reached . Moreover , we do not directly use the arithmetic expression that possesses the maximum probability , but instead re - rank several expression candidates that are decoded by beam search to further confirm the prediction ( 3.4 ) . Finally , the model is trained under weakly - supervised signals to maximize the marginal likelihood over all possible annotations ( 3.5 ) .", "entities": [[13, 15, "TaskName", "reading comprehension"], [22, 23, "MethodName", "BERT"], [35, 37, "TaskName", "word embeddings"], [44, 45, "MethodName", "Transformer"]]}
{"text": "To obtain a universal representation for both the question and the passage , we utilize BERT ( Devlin et al , 2019 ) , a pre - trained deep bidirectional Transformer model that achieves state - of - the - art performance across various tasks , as the encoder . Specifically , we first tokenize the question and The multi - type answer predictor supports four kinds of answer types including span , addition / subtraction , count , and negation . A multi - span extraction method is proposed to dynamically produce one or several spans . The arithmetic expression reranking mechanism aims to rank expression candidates that are decoded by beam search for further validating the prediction . the passage using the WordPiece vocabulary ( Wu et al , 2016 ) , and then generate the input sequence by concatenating a [ CLS ] token , the tokenized question , a [ SEP ] token , the tokenized passage , and a final [ SEP ] token . For each token in the sequence , its input representation is the elementwise addition of WordPiece embeddings , positional embeddings , and segment embeddings ( Devlin et al , 2019 ) . As a result , a list of input embeddings H 0 2 R T \u21e5 D can be obtained , where D is the hidden size and T is the sequence length . A series of L pre - trained Transformer blocks are then used to project the input embeddings into contextualized representations H i as : H i = TransformerBlock ( H i 1 ) , 8i 2 [ 1 , L ] Here , we omit a detailed introduction of the block architecture and refer readers to Vaswani et al ( 2017 ) for more details .", "entities": [[15, 16, "MethodName", "BERT"], [30, 31, "MethodName", "Transformer"], [124, 125, "MethodName", "WordPiece"], [185, 186, "MethodName", "WordPiece"], [212, 213, "DatasetName", "0"], [242, 243, "MethodName", "Transformer"]]}
{"text": "Rather than restricting the answer to always be a span of text , the discrete - reasoning reading comprehension task involves different answer types ( e.g. , number , date , span of text ) . Following Dua et al ( 2019 ) , we design a multi - type answer predictor to selectively produce different kinds of answers such as span , count number , and arithmetic expression . To further increase answer coverage , we propose adding a new answer type to support logical negation . Moreover , unlike prior work that separately predicts passage spans and question spans , our approach directly extracts spans from the input sequence . Answer type prediction Inspired by the Augmented QANet model ( Dua et al , 2019 ) , we use the contextualized token representations from the last four blocks ( H L 3 , ... , H L ) as the inputs to our answer predictor , which are denoted as M 0 , M 1 , M 2 , M 3 , respectively . To predict the answer type , we first split the representation M 2 into a question representation Q 2 and a passage representation P 2 according to the index of intermediate [ SEP ] token . Then the model computes two vectors h Q 2 and h P 2 that summarize the question and passage information respectively : \u21b5 Q = softmax ( W Q Q 2 ) , h Q 2 = \u21b5 Q Q 2 where h P 2 is computed in a similar way over P 2 . Next , we calculate a probability distribution to represent the choices of different answer types as : p type = softmax ( FFN ( [ h Q 2 ; h P 2 ; h CLS ] ) ) Here , h CLS is the first vector in the final contextualized representation M 3 , and FFN denotes a feed - forward network consisting of two linear projections with a GeLU activation ( Hendrycks and Gimpel , 2016 ) followed by a layer normalization ( Lei Ba et al , 2016 )", "entities": [[17, 19, "TaskName", "reading comprehension"], [113, 115, "TaskName", "type prediction"], [115, 116, "DatasetName", "Inspired"], [163, 164, "DatasetName", "0"], [237, 238, "MethodName", "softmax"], [287, 288, "MethodName", "softmax"], [336, 337, "MethodName", "GeLU"], [348, 350, "MethodName", "layer normalization"]]}
{"text": "Span To extract the answer either from the passage or from the question , we combine the gating mechanism of Wang et al ( 2017 ) with the standard decoding strategy of Seo et al ( 2017 ) to predict the starting and ending positions across the entire sequence . Specifically , we first compute three vectors , namely g Q 0 , g Q 1 , g Q 2 , that summarize the question information among different levels of question representations : Q = softmax ( FFN ( Q 2 ) , g Q 2 = Q Q 2 where g Q 0 and g Q 1 are computed over Q 0 and Q 1 respectively , in a similar way as described above . Then we compute the probabilities of the starting and ending indices of the answer span from the input sequence as : M start = [ M 2 ; M 0 ; g Q 2 \u2326 M 2 ; g Q 0 \u2326 M 0 ] , M end = [ M 2 ; M 1 ; g Q 2 \u2326 M 2 ; g Q 1 \u2326 M 1 ] , p start = softmax ( W SMstart ) , p end = softmax ( W EMend ) where \u2326 denotes the outer product between the vector g and each token representation in M.", "entities": [[61, 62, "DatasetName", "0"], [85, 86, "MethodName", "softmax"], [103, 104, "DatasetName", "0"], [112, 113, "DatasetName", "0"], [155, 156, "DatasetName", "0"], [166, 167, "DatasetName", "0"], [169, 170, "DatasetName", "0"], [200, 201, "MethodName", "softmax"], [209, 210, "MethodName", "softmax"]]}
{"text": "In order to model the process of performing addition or subtraction among multiple numbers mentioned in the passage , we assign a three - way categorical variable ( plus , minus , or zero ) for each number to indicate its sign , similar to Dua et al ( 2019 ) . As a result , an arithmetic expression that has a number as the final answer can be obtained and easily evaluated . Specifically , for each number mentioned in the passage , we gather its corresponding representation from the concatenation of M 2 and M 3 , eventually yielding U = ( u 1 , ... , u N ) 2 R N \u21e5 2 \u21e4 D where N numbers exist . Then the probabilities of the i - th number being assigned a plus , minus or zero is computed as : p sign i = softmax ( FFN ( [ u i ; h Q 2 ; h P 2 ; h CLS ] ) ) Count We consider the ability of counting entities and model it as a multi - class classification problem . To achieve this , the model first produces a vector h U that summarizes the important information among all mentioned numbers , and then computes a counting probability distribution as : \u21b5 U = softmax ( W U U ) , h U = \u21b5 U U , p count = softmax ( FFN ( [ h U ; h Q 2 ; h P 2 ; h CLS ] ) ) Negation One obvious but important linguistic phenomenon that prior work fails to capture is negation . We find there are many cases in DROP that require to perform logical negation on numbers . The question ( Q 4 ) in Figure 1 gives a qualitative example of this phenomenon . To model this phenomenon , we assign a two - way categorical variable for each number to indicate whether a negation operation should be performed . Then we compute the probabilities of logical negation on the i - th number as : p negation i = softmax ( FFN ( [ u i ; h Q 2 ; h P 2 ; h CLS ] ) )", "entities": [[149, 150, "MethodName", "softmax"], [183, 187, "TaskName", "multi - class classification"], [223, 224, "MethodName", "softmax"], [240, 241, "MethodName", "softmax"], [284, 285, "DatasetName", "DROP"], [357, 358, "MethodName", "softmax"]]}
{"text": "As discussed in 3.2 , we model the phenomenon of discrete reasoning on numbers by learning to predict a plus , minus , or zero for each number in the passage . In this way , an arithmetic expression composed of signed numbers can be obtained , where the final answer can be deduced by performing simple arithmetic computation . However , since the sign of each number is only determined by the number representation and some coarsegrained global representations , the context information of the expression itself has not been considered . As a result , the model may predict some Algorithm 1 Multi - span extraction Input : p start ; p end ; p span 1 : Generate the set S by extracting top - K spans 2 : Sort S in descending order of span scores 3 : t = arg max p span + 1 4 : InitializeS = { } 5 : while S 6 = { } and | S | < t do 6 : for si in S do 7 : Add span si toS 8 : Remove span si from S 9 : for sj in S do 10 : if f1 ( si , sj ) > 0 then 11 : Remove span sj from S 12 : returnS obviously wrong expressions ( e.g. , the signs that have maximum probabilities are either minus or zero , resulting in a large negative value ) . Therefore , in order to further validate the prediction , it is necessary to rank several highly confident expression candidates using the representation summarized from the expression 's context . Specifically , we use beam search to produce top - ranked arithmetic expressions , which are sent back to the network for reranking . Since each expression consists of several signed numbers , we construct an expression representation by taking both the numbers and the signs into account . For each number in the expression , we gather its corresponding vector from the representation U. As for the signs , we initialize an embedding matrix E 2 R 3 \u21e5 2 \u21e4 D , and find the sign embeddings for each signed number . In this way , given the i - th expression that contains M signed numbers at most , we can obtain number vectors V i 2 R M \u21e5 2 \u21e4 D as well as sign embeddings C i 2 R M \u21e5 2 \u21e4 D . Then the expression representation along with the reranking probability can be calculated as : \u21b5 V i = softmax ( W V ( V i + C i ) ) , h V i = \u21b5 V i ( V i + C i ) , p arith i = softmax ( FFN ( [ h V i ; h Q 2 ; h P 2 ; h CLS ] ) ) 3 .", "entities": [[208, 209, "DatasetName", "0"], [435, 436, "MethodName", "softmax"], [467, 468, "MethodName", "softmax"]]}
{"text": "Reading comprehension benchmarks Promising advancements have been made for reading comprehension due to the creation of many large datasets . While early research used cloze - style tests ( Hermann et al , 2015 ; Hill et al , 2016 ) , most of recent works ( Rajpurkar et al , 2016 ; Joshi et al , 2017 ) are designed to extract answers from the passage . Despite their success , these datasets only require shallow pattern matching and simple logical reasoning , thus being well solved Devlin et al , 2019 ) . Recently , Dua et al ( 2019 ) released a new benchmark named DROP that demands discrete reasoning as well as deeper paragraph understanding to find the answers . Saxton et al ( 2019 ) introduced a dataset consisting of different types of mathematics problems to focuses on mathematical computation . We choose to work on DROP to test both the numerical reasoning and linguistic comprehension abilities . Neural reading models Previous neural reading models , such as BiDAF ( Seo et al , 2017 ) , R - Net ( Wang et al , 2017 ) , QANet ( Yu et al , 2018 ) , Reinforced Mreader ( Hu et al , 2018 ) , are usually designed to extract a continuous span of text as the answer . Dua et al ( 2019 ) enhanced prior single - type prediction to support various answer types such as span , count number , and addition / subtraction . Different from these approaches , our model additionally supports a new negation type to increase answer coverage , and learns to dynamically extract one or multiple spans . Morevoer , answer reranking has been well studied in several prior works ( Cui et al , 2016 ; Wang et al , 2018a , b , c ; Hu et al , 2019 ) . We follow this line of work , but propose ranking arithmetic expressions instead of candidate answers . End - to - end symbolic reasoning Combining neural methods with symbolic reasoning was considered by Graves et al ( 2014 ) ; Sukhbaatar et al ( 2015 ) , where neural networks augmented with external memory are trained to execute simple programs . Later works on program induction ( Reed and De Freitas , 2016 ; Neelakantan et al , 2016 ; Liang et al , 2017 ) extended this idea by using several built - in logic operations along with a key - value memory to learn different types of compositional programs such as addition or sorting . In contrast to these works , MTMSN does not model various types of reasoning with a universal memory mechanism but instead deals each type with individual predicting strategies . Visual question answering In computer vision community , the most similar work to our approach is Neural Module Networks ( Andreas et al , 2016b ) , where a dependency parser is used to lay out a neural network composed of several pre - defined modules . Later , Andreas et al ( 2016a ) proposed dynamically choosing an optimal layout structure from a list of layout candidates that are produced by off - the - shelf parsers . Hu et al ( 2017 ) introduced an end - to - end module network that learns to predict instance - specific network layouts without the aid of a parser . Compared to these approaches , MTMSN has a static network layout that can not be changed during training and evaluation , where pre - defined \" modules \" are used to handle different types of answers .", "entities": [[0, 2, "TaskName", "Reading comprehension"], [9, 11, "TaskName", "reading comprehension"], [108, 109, "DatasetName", "DROP"], [151, 152, "DatasetName", "DROP"], [236, 238, "TaskName", "type prediction"], [383, 385, "TaskName", "program induction"], [465, 468, "DatasetName", "Visual question answering"]]}
{"text": "Abstract Text Summarization : A Low Resource Challenge", "entities": [[1, 3, "TaskName", "Text Summarization"]]}
{"text": "Automatic text summarization is considered as a challenging task because while summarizing a piece of text , we read it entirely to develop our understanding to prepare highlighting its main points . Due to the lack of human knowledge and language processing abilities in computers , automatic text summarization is a major non - trivial task ( Allahyari et al , 2017 ) . Two major approaches for automatic summarization are : extractive and abstractive . The extractive summarization approach produces summaries by choosing a subset of sentences in the original text . The abstract text summarization approach aims to shorten the long text into a humanreadable form that contains the most important fact from the original text ( Allahyari et al , 2017 ; Kry\u015bci\u0144ski et al , 2018 ) . The deep learning - based neural attention model when applying to abstract text summarization performs well compared to standard learning - based approaches ( Rush et al , 2015 ) . Abstract text summarization using the attentional encoder - decoder recurrent neural network approach shows a stateof - the - art performance and sets a baseline model ( Nallapati et al , 2016 ) . Further improvements are introduced to the baseline model by using the pointer generator network and coverage mechanism using reinforcement learning based training procedure ( See et al , 2017 ; Paulus et al , 2017 ) . There is an inherent limitation to natural language processing tasks such as text summarization for resource - poor and morphological complex languages owing to a shortage of quality linguistic data available ( Kurniawan and Louvan , 2018 ) . The use of synthetic data along with the real data is one of the popular approaches followed in machine translation domain for the low resource conditions to improve the translation quality ( Bojar and Tamchyna , 2011 ; Hoang et al , 2018 ; Chinea - R\u0131os et al , 2017 ) . The iterative back - translation ( e.g. training back - translation systems multiple times ) were also found effective in machine translation ( Hoang et al , 2018 ) . We explore similar approaches in our experiments for the text summarization task . The organizations of this paper is as follows : Section 1 describes related work on abstract text summarization . Section 2 explains the techniques followed in our work . Section 3 describes the dataset used in our experiment . Section 4 explains the experimental settings : models and their parameters . Section 5 provides evaluation results with analysis and discussion . Section 6 provides conclusion to the paper .", "entities": [[1, 3, "TaskName", "text summarization"], [47, 49, "TaskName", "text summarization"], [69, 70, "TaskName", "summarization"], [77, 79, "TaskName", "extractive summarization"], [95, 97, "TaskName", "text summarization"], [144, 146, "TaskName", "text summarization"], [164, 166, "TaskName", "text summarization"], [246, 248, "TaskName", "text summarization"], [291, 293, "TaskName", "machine translation"], [346, 348, "TaskName", "machine translation"], [365, 367, "TaskName", "text summarization"], [385, 387, "TaskName", "text summarization"]]}
{"text": "Across all experiments performed in this paper , we have used the Transformer model as implemented in OpenNMT - py 1 ( Vaswani et al , 2018 ; See et al , 2017 ) . The Transformer model is based on encoder / decoder architecture . In context to summarize , it takes text as input and provides its summary . We use synthetic data as shown in Figure 1 to increase the size of the training data . Figure 1 : Generation of synthetic data using a reverse system . To generate synthetic data , first , a system in the reverse direction ( i.e. source as summary and target as text ) is trained and then used to generate text for the given summary . Then both the real and synthetic data acts as input to the final system .", "entities": [[12, 13, "MethodName", "Transformer"], [36, 37, "MethodName", "Transformer"]]}
{"text": "We use German wiki data ( spread across different domain ) collected from the SwissText 2019 2 ( real data ) and Common Crawl 3 data ( synthetic data ) in our experiment . The statistics of all the datasets are shown in Table 1 .", "entities": [[22, 24, "DatasetName", "Common Crawl"]]}
{"text": "The data crawled from the Internet ( Common Crawl ) used to prepare synthetic data to boost the training . The steps followed to create the synthetic dataset as follows : Step 1 : Build vocab : We create vocabulary using SwissText based on the occurrence of the most frequent ( top N ) German words . Step 2 : Sentence selection : The sentences from the Common Crawl data are selected with respect to the vocabulary based on the threshold we provide ( e.g. a sentence has 10 words and the threshold is 10 % ( 0.1 ) ) . For a sentence to be selected , at least 1 out of 10 words should be in the vocabulary . Step 3 : Filtering : Select random sentences ( e.g. 100 K ) from the selected Common Crawl data in the previous step . Step 4 : Generate summary : The 100 K data obtained from the previous step are used as a summary and required to generate corresponding text . We use the reverse trained model where we provide the summary as source and target as text . This results in the text as well as the corresponding summary as additional data to be utilized along with real data ( SwissText ) . Eventually , the 190 K dataset is created ( denote as Train RealSynth ) as a combination of 90 K SwissText train data ( real ) and 100 K synthetic data . This dataset is used in the experimental setup S2 ( described in details in Section 4.3 ) .", "entities": [[7, 9, "DatasetName", "Common Crawl"], [67, 69, "DatasetName", "Common Crawl"], [137, 139, "DatasetName", "Common Crawl"]]}
{"text": "This section describes our experiments conducted for the text summarization task .", "entities": [[8, 10, "TaskName", "text summarization"]]}
{"text": "We use 3 settings : ( i ) real data ( we set this as the baseline in our experiment ) , ( ii ) real data and synthetic data , and ( iii ) real and regenerated synthetic data for the summarization task , described as follows : 1 . S1 : Transformer model using Train Real data In this setup , we use the \" Train Real \" data for training the Transformer model .", "entities": [[42, 43, "TaskName", "summarization"], [53, 54, "MethodName", "Transformer"], [74, 75, "MethodName", "Transformer"]]}
{"text": "In this setup , we use the \" Train RealSynth \" data for training the Transformer model . As the balance between real and synthetic data is an important factor , we maintain a 1:1 ratio ( e.g. 1 ( real ) :1 ( synthetic ) ) for our experiment ( Sennrich et al , 2016 ) .", "entities": [[15, 16, "MethodName", "Transformer"]]}
{"text": "Transformer Model using Train RealSynthRegen data We propose an iterative approach to improve the quality of synthetic summaries . In this setup , after training a system with ( real+synthetic ) data , it is used to regenerate synthetic data for the final system . As a result , the input data to the final system is a combination of real and regenerated synthetic data as shown in Figure 2 .", "entities": [[0, 1, "MethodName", "Transformer"]]}
{"text": "The copying mechanism is applied during training . It allows the summarizer to fall back and copy the source text when encounters < unk > tokens by referencing to the softmax of the multiplication between attention scores of the output with the attention scores of the source ( See et al , 2017 ) . The systems are trained for 300 K iterations .", "entities": [[30, 31, "MethodName", "softmax"]]}
{"text": "In this paper , we highlighted the implementation of synthetic data for the abstract text summariza - Ref Summary : \" Das Feuerschiff Relandersgrund war ein finnisches Feuerschiff , das von 1888 bis 1914 i m Schrenmeer bei Rauma positioniert war . Heute dient es als Restaurantschiff in Helsinki . \" Gloss : The lightship Relandersgrund was a Finnish lightship , which was built from 1888 to 1914 Schrenmeer was positioned at Rauma . Today serves it as a restaurant ship in Helsin S1 Summary : : \" Die \" Rauma \" . ist ein 1886 - 1888 Feuerschiff der norwegischen Reederei \" Libauskij \" , Das Schiff wurde in den 1930er Jahren gebaut und in den 2000er Jahren als Museumsschiff als \" Gloss:\"The \" Rauma \" . is a 1886 - 1888 Lightship of the Norwegian shipping company \" Libauskij \" , The ship was built in the 1930s and in the 2000s as a museum ship as S2 Summary : : \" Das Feuerschiff Relandersgrund war ein Feuerschiff des das von 1888 bis 1914 i m Einsatz war . Heute dient es als Restaurantschiff in Kotka , \" Gloss : The lightship Relandersgrund was on Lightship of the 1888 to 1914 was in use . Today it serves as a restaurant ship in Kotka S3 Summary : : \" Das Kotka . \" ist ein finnischer Museumsschiff der i m Zweiten Weltkrieg von der russischen Marine als Restaurantschiff 1 \" eingesetzt wurde . I m Mittelalter war das Schiff unter dem Namen \" Vuolle \" 1 \" fr die finnische Marine 1 \" Gloss : The Kotka . \" Is a Finnish one Museum ship of the World War II Russian Navy used as a restaurant ship 1 \" has been . In the Middle Ages , the ship was under the name \" Vuolle \" 1 \" for the Finnish Navy 1 tion task under low resource condition , which helps improving the text summarization system in terms of automatic evaluation metrics . As the next step , we plan to investigate : i ) synthetic summarization data , and ii ) applying transfer learning on text summarization for the multilingual low resource data set with little or no ground truth summaries ( Keneshloo et al , 2018 ) .", "entities": [[326, 328, "TaskName", "text summarization"], [349, 350, "TaskName", "summarization"], [356, 358, "TaskName", "transfer learning"], [359, 361, "TaskName", "text summarization"]]}
{"text": "The work is supported by an innovation project ( under an InnoSuisse grant ) oriented to improve the automatic speech recognition and natural language understanding technologies for German . Title : \" SM2 : Extracting Semantic Meaning from Spoken Material \" funding application no . 29814.1 IP - ICT .", "entities": [[18, 21, "TaskName", "automatic speech recognition"], [22, 25, "TaskName", "natural language understanding"]]}
{"text": "A Negative Case Analysis of Visual Grounding Methods for VQA", "entities": [[5, 7, "TaskName", "Visual Grounding"], [9, 10, "TaskName", "VQA"]]}
{"text": "Existing Visual Question Answering ( VQA ) methods tend to exploit dataset biases and spurious statistical correlations , instead of producing right answers for the right reasons . To address this issue , recent bias mitigation methods for VQA propose to incorporate visual cues ( e.g. , human attention maps ) to better ground the VQA models , showcasing impressive gains . However , we show that the performance improvements are not a result of improved visual grounding , but a regularization effect which prevents over - fitting to linguistic priors . For instance , we find that it is not actually necessary to provide proper , humanbased cues ; random , insensible cues also result in similar improvements . Based on this observation , we propose a simpler regularization scheme that does not require any external annotations and yet achieves near state - of - theart performance on VQA - CPv2 1 .", "entities": [[1, 4, "DatasetName", "Visual Question Answering"], [5, 6, "TaskName", "VQA"], [38, 39, "TaskName", "VQA"], [55, 56, "TaskName", "VQA"], [76, 78, "TaskName", "visual grounding"], [149, 150, "TaskName", "VQA"]]}
{"text": "As expected of any real world dataset , VQA datasets also contain dataset biases ( Goyal et al , 2017 ) . The VQA - CP dataset was introduced to study the robustness of VQA methods against linguistic biases . Since it contains different answer distributions in the train and test sets , VQA - CP makes it nearly impossible for the models that rely upon linguistic correlations to perform well on the test set Shrestha et al , 2019 ) .", "entities": [[8, 9, "TaskName", "VQA"], [23, 26, "DatasetName", "VQA - CP"], [34, 35, "TaskName", "VQA"], [53, 56, "DatasetName", "VQA - CP"]]}
{"text": "VQA algorithms without explicit bias mitigation mechanisms fail on VQA - CP , so recent works have focused on the following solutions :", "entities": [[0, 1, "TaskName", "VQA"], [9, 12, "DatasetName", "VQA - CP"]]}
{"text": "Given a question Q and an image I , e.g. , represented by bottom - up region proposals : v ( Anderson et al , 2018 ) , a VQA model is tasked with predicting the answer a : P ( a | Q , I ) = f V QA ( v , Q ) . ( 1 )", "entities": [[29, 30, "TaskName", "VQA"]]}
{"text": "Without additional regularization , existing VQA models such as the baseline model used in this work : UpDn ( Anderson et al , 2018 ) , tend to rely on the linguistic priors : P ( a | Q ) to answer questions . Such models fail on VQA - CP , because the priors in the test set differ from the train set .", "entities": [[5, 6, "TaskName", "VQA"], [48, 51, "DatasetName", "VQA - CP"]]}
{"text": "We compare the baseline UpDn model with HINT and SCR - variants trained on VQAv2 or VQA - CPv2 to study the causes behind the improvements . We report mean accuracies across 5 runs , where a pretrained UpDn model is fine - tuned on subsets with human attention maps and textual explanations for HINT and SCR respectively . Further training details are provided in the Appendix .", "entities": [[16, 17, "TaskName", "VQA"]]}
{"text": "In our next experiment we studied how random visual cues performed with HINT and SCR . We assign random importance scores to the visual regions : S rand \u223c uniform ( 0 , 1 ) . We test two variants of randomness : Fixed random regions , where 1 , both of these variants obtain similar results as the model trained with human - based importance scores . The performance improves even when the importance scores are changed every epoch , indicating that it is not even necessary to look at the same visual regions .", "entities": [[31, 32, "DatasetName", "0"]]}
{"text": "As observed by Selvaraju et al ( 2019 ) and as shown in Fig . 2 , we observe small improvements on VQAv2 when the models are fine - tuned on the entire train set . However , if we were to compare against the improvements in VQA - CPv2 in a fair manner , i.e. , only use the instances with visual cues while fine - tuning , then , the performance on VQAv2 drops continuously during the course of the training . This indicates that HINT and SCR help forget linguistic priors , which is beneficial for VQA - CPv2 but not for VQAv2 .", "entities": [[47, 48, "TaskName", "VQA"], [99, 100, "TaskName", "VQA"]]}
{"text": "In order to quantitatively assess visual grounding , we propose a new metric called : Correctly Predicted but Improperly Grounded ( CPIG ) : % CP IG = N correct ans , improper grounding N correct ans \u00d7 100 % , which is the number instances for which the most sensitive visual region used to correctly predict the answer is not within top - 3 most relevant ground truth regions , normalized by the total number of correct predictions . HINT and SCR trained on relevant regions obtained lower CPIG values that other variants ( 70.24 % and 80.22 % respectively ) , indicating they are better than other variants at finding relevant regions . However , these numbers are still high , and show that only 29.76 % and 19.78 % of the correct predictions for HINT and SCR were properly grounded . Further analysis is presented in the Appendix .", "entities": [[5, 7, "TaskName", "visual grounding"]]}
{"text": "Here , we showed that existing visual grounding based bias mitigation methods for VQA are not working as intended . We found that the accuracy improvements stem from a regularization effect rather than proper visual grounding . We proposed a simple regularization scheme which , despite not requiring additional annotations , rivals state - of - theart accuracy . Future visual grounding methods should be tested with a more comprehensive experimental setup and datasets for proper evaluation .", "entities": [[6, 8, "TaskName", "visual grounding"], [13, 14, "TaskName", "VQA"], [24, 25, "MetricName", "accuracy"], [34, 36, "TaskName", "visual grounding"], [57, 58, "MetricName", "accuracy"], [60, 62, "TaskName", "visual grounding"]]}
{"text": "Following ( Selvaraju et al , 2019 ) , we report Spearman 's rank correlation between network 's sensitivity scores and human - based scores in Table A3 . For HINT and our zero - out regularizer , we use human - based attention maps . For SCR , we use textual explanation - based scores . We find that HINT trained on human attention maps has the highest correlation coefficients for both datasets . However , compared to baseline , HINT variants trained on random visual cues also show improved correlations . For SCR , we obtain surprising results , with the model trained on irrelevant cues obtaining higher correlation than that trained on relevant visual cues . As expected , applying our regularizer does not improve rank correlation . Since HINT trained on relevant cues obtains the highest correlation values , it does indicate improvement in visual grounding . However , as we have seen , the improvements in performance can not necessarily be attributed to better overlap with ground truth localizations .", "entities": [[148, 150, "TaskName", "visual grounding"]]}
{"text": "Presentation of qualitative examples in visual grounding models for VQA suffers from confirmation bias i.e. , while it is possible to find qualitative samples that look at relevant regions to answer questions properly , it is also possible to find samples that produce correct answers without looking at relevant regions . We present examples for such cases in Fig . A3 . We next present a quantitative assessment of visual grounding , which does not suffer from the confirmation bias .", "entities": [[5, 7, "TaskName", "visual grounding"], [9, 10, "TaskName", "VQA"], [69, 71, "TaskName", "visual grounding"]]}
{"text": "Table A4 shows VQA accuracy for each answer type on VQACPv2 's test set . HINT / SCR and our regularizer show large gains in ' Yes / No ' questions . We hypothesize that the methods help forget linguistic priors , which improves test accuracy of such questions . In the train set of VQACPv2 , the answer ' no ' is more frequent than the answer ' yes ' , tempting the baseline model to answer ' yes / no ' questions with ' no ' . However , in the test set , answer ' yes ' is more frequent . Regularization effects caused by HINT / SCR and our method cause the models to weaken this prior i.e. , reduce the tendency to just predict ' no ' , which would increase accuracy at test because ' yes ' is more frequent in the test set . Next , all of the methods perform poorly on ' Number ( Num ) ' answer type , showing that methods find it difficult to answer questions that are most reliant on correct visual grounding such as : localizing and counting objects . Finally , we do not observe large improvements in ' Other ' question type , most likely due to the large number of answers present under this answer type .", "entities": [[3, 4, "TaskName", "VQA"], [4, 5, "MetricName", "accuracy"], [45, 46, "MetricName", "accuracy"], [136, 137, "MetricName", "accuracy"], [184, 186, "TaskName", "visual grounding"]]}
{"text": "The Hebrew Universal Dependency Treebank : Past , Present and Future", "entities": [[2, 5, "DatasetName", "Universal Dependency Treebank"]]}
{"text": "The Hebrew treebank ( HTB ) , consisting of 6221 morpho - syntactically annotated newspaper sentences , has been the only resource for training and validating statistical parsers and taggers for Hebrew , for almost two decades now . During these decades , the HTB has gone through a trajectory of automatic and semi - automatic conversions , until arriving at its UDv2 form . In this work we manually validate the UDv2 version of the HTB , and , according to our findings , we apply scheme changes that bring the UD HTB to the same theoretical grounds as the rest of UD . Our experimental parsing results with UDv2New confirm that improving the coherence and internal consistency of the UD HTB indeed leads to improved parsing performance . At the same time , our analysis demonstrates that there is more to be done at the point of intersection of UD with other linguistic processing layers , in particular , at the points where UD interfaces external morphological and lexical resources .", "entities": [[92, 93, "DatasetName", "UD"], [103, 104, "DatasetName", "UD"], [121, 122, "DatasetName", "UD"], [151, 152, "DatasetName", "UD"], [165, 166, "DatasetName", "UD"]]}
{"text": "The Hebrew Treebank ( HTB ) , initially introduced by Sima'an et al ( 2001 ) , is the first , and so far only , gold standard for morphologically and syntactically annotated sentences in Modern Hebrew . It was created with the main goal in mind to enable the development of statistical models for morphological and syntactic parsing for Hebrew , but also to facilitate linguistic investigations into the structure and distribution of linguistic Semitic phenomena . The pilot version of Sima'an et al ( 2001 ) has been minimal - it consisted of 500 sentences , morphologically and syntactically annotated by hand . This modest start , however , defined linguistic conventions and annotation principles that would continue to affect many treebank versions derived from the HTB for many years , including the universal dependencies ( UD ) HTB version . During these two decades , the HTB has expanded from 500 to 6221 sentences and changed several forms . The different versions of the treebank reflect different theories and formal representation types , that in turn reflect different , and sometimes contradictory , linguistic annotation principles . The reasons for these differences were sometimes practical , e.g. , a new version was derived to answer an emerging technological need , and sometimes socio - academic , e.g. , because different teams adopted different linguistic theories as their underlying annotation principles . The HTB thus enabled the development of many statistical morphological and syntactic processing models ( Adler , 2007 ; Bar - haim et al , 2008 ; Shacham and Wintner , 2007 ; Tsarfaty , 2006 ; Goldberg and Tsarfaty , 2008 ; Goldberg and Elhadad , 2009 ; Tsarfaty , 2010 ; Goldberg andElhadad , 2010 , 2011 ; More and Tsarfaty , 2016 ; More et al , In Press ) , but these models were trained on vastly different versions of the treebank , obeying different theories and annotation schemes , which then rendered the reported results mostly non - comparable . Hebrew dependency parsing presents an acute version of this syndrome . Studies such as Goldberg and Elhadad ( 2011 ) , Tsarfaty et al ( 2012 ) , More et al ( In Press ) , as well as the SPMRL shared tasks ( Seddah et al , 2013 ( Seddah et al , , 2014 , all present attachment scores on Hebrew dependency parsing . But for reporting these scores they use HTB versions that reflect distinct schemes , sometime reporting different metrics , which makes the numerical comparison between the respective results meaningless ( Tsarfaty et al , 2011 ) . This is why the UD initiative comes as a blessing , not only for the cross - linguistic parsing community but also for the Hebrew NLP community - by presenting a unique opportunity to standardize the resources and metrics used for Hebrew parsing . Ideally , the current UDv2 version would make for such a standard Hebrew resource . Unfortunately though , many of the conversion processes since Sima ' an et al ( 2001 ) to the present UDv2 have been automatic or semi - automatic , with no point of systematic qualitative validation . This resulted in odd , and sometime plain wrong , dependency structures , with respect to the UD scheme . In this work we take the opportunity to validate the UDv2 HTB , by manually going through the published trees , identifying systematic errors or annotation inconsistencies , and locating cases where the annotated structures contradict the UD guidelines ( or spirit ) . We identified and corrected three main points of failure in the UD HTB : ( i ) the classification of argument types , deriving from the classification in the original HTB ( ii ) a mix - up of morphological and syntactic properties , where morphological features serve as syntactic sub - relations and vice versa , and ( iii ) a mix up of language - specific versus universal phenomena , where label sub - typing is exploited to indicate a supposedly language - specific phenomenon , which in fact has a designated universal label elsewhere . Based on these corrections , we present a revised version of the HTB that we call UDv2New . We use UDv2 and UDv2New to train a morphosyntactic parser ( More et al , In Press ) and provide baseline results on Hebrew UD parsing , in both ideal and realistic scenarios . Comparing our Hebrew parsing results on UDv2 and UDv2New , we verify that the improvement of linguistic coherence and annotation consistency has also led to improved parsing performance . Lessons learned from our empirical analysis concern the systematic organization of natural language grammar in UD , and in particular ( i ) the need to standardize the interface of UD treebanks to external morphological and lexical resources , and ( ii ) the need to organize the form - function mapping in a language - specific vs. family - specific vs. strictly - universal relations taxonomy , within and across treebanks . The remainder of this paper is organized as follows . In Section 2 we describe the trajectory of the HTB from its inception to UDv2 . In Section 3 we present our validation process and the scheme changes we applied . In Section 4 we present raw - to - dependencies Hebrew parsing results and in Section 5 we share our future plans and lessons learned . Finally , in Section 6 we conclude .", "entities": [[135, 137, "DatasetName", "universal dependencies"], [138, 139, "DatasetName", "UD"], [340, 342, "TaskName", "dependency parsing"], [402, 404, "TaskName", "dependency parsing"], [446, 447, "DatasetName", "UD"], [555, 556, "DatasetName", "UD"], [595, 596, "DatasetName", "UD"], [613, 614, "DatasetName", "UD"], [742, 743, "DatasetName", "UD"], [796, 797, "DatasetName", "UD"], [811, 812, "DatasetName", "UD"]]}
{"text": "The RR version of the Unified - SD HTB provided the basis for automatically converting the Hebrew trees into UDv1 trees . The UD HTB assumes the same segmentation principles as the first edition of the HTB , segmenting off prefixes and suffixes , with the addition of splitting off genitive pronominal clitics from nouns . Goldberg and Tsarfaty ( 2014 ) devised an automatic process that chooses a lexical head in each relational network of each constituent in the RR treebank . They also mapped the fine - grained POS categories to the coarse - grained UPOS categories in UD , and remaining POS distinctions in HebLex ( HebBinyan , construct - states , etc . ) are stored in FEATS . The label set of U - SD was automatically mapped to UD , and relations from U - SD outside of UD were kept as relation : subtype . The conversion of UDv1 to UDv2 was also done automatically , by augmenting the script of Goldberg and . Points of failure of the UDv1 version of the HTB to comply with UDv2 were identified by aiming to locate skewed distributions of tags or labels , and they were corrected in the conversion script on a case by case basis . This process has stopped when the treebank complied with the UDv2 validation script . The converted HTB is documented on the UD webpage . 7", "entities": [[1, 2, "DatasetName", "RR"], [23, 24, "DatasetName", "UD"], [80, 81, "DatasetName", "RR"], [100, 101, "DatasetName", "UD"], [134, 135, "DatasetName", "UD"], [144, 145, "DatasetName", "UD"], [234, 235, "DatasetName", "UD"]]}
{"text": "Open Clausal Complements . In the UDv2 HTB , predicative complements were labeled advmod when adjectival . Following the UDv2 guidelines , we label them xcomp , as they are subordinated predicates , after all , even if not verbal . Argument iobj vs. obl . Some UD definitions stand in clear contrast with the canonical syntactic analysis of Hebrew . Perhaps the most salient case is of core arguments . The canonical view of Hebrew core arguments ( Coffin and Bolozky ( 2005 ) p. 290 ) is of a direct object , marked by an accusative case when definite , and an indirect object , marked by an oblique case marker when a pronoun , and preceded by a preposition when common or proper noun . UDv2 dedicates an iobj ( indirect object ) relation to secondary core arguments which are not preceded by prepositions , and arguments which do follow a preposition are labeled obl , whether core or non - core . We revised the labels accordingly . Predicate types : the case of auxiliaries As part of the shift towards a lexically - driven analysis , structural changes were made to sentences containing auxiliary elements and copulas . There are three main sets of these : ( i ) Auxiliary elements marking modality , ( ii ) Auxiliary verbs which mostly mark habituality , but occasionally participate in negation or tense inflection when the predicate has no past / future form , and ( iii ) Positive or negative copulars . Modals do not constitute any uniform syntactic class in Hebrew , and there is an ongoing debate as to the POS of each modal expression ( cf . Netzer et al ( 2007 ) ) . In line with Netzer et al s conclusion , these are tagged as AUX in the UD HTB . In UDv2 , the modal served as the head of the clause , while the following predicate was labeled xcomp , as it is consistently realized in Hebrew in infinitive form . As of UDv2New , those modals which are tagged as AUX are also labeled aux , and the subsequent predicate receives the label which was attributed to the modal . See Table 1 . In the opposite direction , auxiliary verbs , such as the ones in sets ii and iii were tagged as VERB . As the UDv2 scheme dedicates an AUX tag to function words in auxiliary functions even when they are verbs , we changed them to AUX as well in UDv2New . Finally , consistency across sets ii and iii was achieved by unifying the labeling of copular verbs as cop regardless of their inflection , whereas previous versions labeled past and future inflections of copular verbs as aux .", "entities": [[47, 48, "DatasetName", "UD"], [308, 309, "DatasetName", "UD"]]}
{"text": "As UD aspires to present a set of tags which are relevant to as many languages as possible , natu - 11 All analyses are visualized in the supp . materials . rally many language - specific phenomena are left unanswered . To allow representation of these , the UD scheme allows for sub - relations in the form of relation : subtype , as exemplified above . However , although originally aiming toward coverage of language - specific phenomena , this structure can be frequently seen as a subtype of relation which is present in many languages ( e.g. nsubj : pass , which is in use for subjects of passive sentences - not unique to any one language or even a family of languages ) . In our revision to adhere to UDv2 guidelines , we tried as much as possible to narrow the use of relation : subtype to Hebrew - specific phenomena , eliminating any hierarchical structure of dependency relations . As a result , the following subtypes were reduced to their parent relation : ( i ) det : quant , originally marking an arbitrary subset of existential quantifiers , was reduced to simply det , and ( ii ) advmod : phrase , originally marking multi - word adverbials , were re - structured as advmod+fixed , in line with the UD guidelines for multi - word - expressions . From conj : discourse to parataxis An interesting case is with labels not used at all in the older versions of the UD HTB , while language - specific labels stand to mark their function . The UD label parataxis , for instance , describes a relation between two ( or more ) sentences which are syntactically independent ( i.e. do not stand in subordination or conjunction relation to one another ) , but are thematically connected , and consequently punctuated as the same sentence . Previously , this relation was labeled in the HTB as conj : discourse , simply classifying conjunctions that are not explicitly marked as of type discourse . In our revised version , we comply with UD guidelines and label this relation ' parataxis ' . From PART to ADP The accusative and possessive case markers in Hebrew , AT and FL respectively , are realised as separate tokens , as opposed to some other case markers , which prefix the following nouns . Furthermore , a possessive case marker may also morphologically suffix the noun , whether instead of or in addition to the above - mentioned particle . In older versions of HTB , while preposition ( whether standalone or not ) were tagged IN , the accusative case marker was tagged AT and the possessive case marker was tagged POSS . As a result , automatic conversions led to converting IN to ADP across the board , while AT and FL were converted into PART . As there is no real difference between AT and FL and prepositions according to the UDv2 scheme , and as they are in no way particles , we converted them into ADP .", "entities": [[1, 2, "DatasetName", "UD"], [49, 50, "DatasetName", "UD"], [182, 183, "DatasetName", "det"], [199, 200, "DatasetName", "det"], [227, 228, "DatasetName", "UD"], [258, 259, "DatasetName", "UD"], [273, 274, "DatasetName", "UD"], [357, 358, "DatasetName", "UD"]]}
{"text": "Goal : We wish to examine the empirical impact of our effort to correct the treebank and retain linguistic ( as well as cross - treebank ) coherence in its annotation scheme . Indeed , ease of parsing should not be the indication for selecting one scheme over another , but the hypothesis is that , within one and the same set of guidelines , a version that presents better coherence and consistency will also be more suitable for statistical training and will yield better results . Settings : To gauge the effect of our revision we conducted two sets of experiments : one with the HTB UDv2 version used in the recent shared task , and another our revised UDv2New . We use the syntactic evaluation script provided by the CoNLL shared task 2018 . We train on the portion defined as train set and report results on the dev set . For training and parsing we used yap , 13 a transitionbased morphosyntactic parser written in go , which includes a morphological analyzer , a morphological disambiguator , and syntactic parser . In previous work yap was shown to obtain state of the art results on Hebrew parsing using the SPMRL version of the treebank ( More et al , In Press ) . Here we report its performance on the UD HTB . Scenarios : Because of its rich morphology and orthographic convention to attach or fuse adpositions and pronominals onto open - class categories , there is severe ambiguity in the morphological analysis of the Hebrew input tokens . This is further magnified by the lack of diacritics in Hebrew written texts . Hence , it is unknown upfront how many morphemes ( in the HTB terminology ) or syntactic words ( in the UD terminology ) are in the space - delimited tokens . We examine two kinds of scenarios : ideal : assuming gold morphological analysis and disambiguation given by an oracle . realistic : assuming automatically predicted morphological analysis and disambiguation . We use yap for predicting morphological analysis ( MA ) and morphological disambiguation ( More , 2016 ) , and we contrast the use of a data - driven lexicon baselinelex with an external broad - coverage lexicon HebLex . To gauge the effect of the lexical coverage of the morphological resource , we contrast each variant with an infused scenario , where the correct analysis is injected into the lattice . Note that the input in the infused cases is still high as there are many MA alternatives . However , the correct morphological disambiguation is guaranteed to be one of the morphological MA provided to the system as input . Results : Table 2 shows the parsing results in an ideal scenario , assuming gold morphology . Here we see that there is a consistent improvement for all metrics . This supports our conjecture that a more consistent and coherent annotation of the treebank will benefit parsing , and it corroborates a wider conjecture , that , when it comes to supervised learning , the quality of the annotated data is as important as the learning algorithm ( and maybe more important ) . Table 3 shows the parsing results in realistic scenarios , where we assume automatically predicted morphological analysis and disambiguation . As expected , the results substantially drop relative to the ideal scenario . Also expected is the result that assuming an external broad - coverage lexicon substantially improves the results relative to a data - driven lexicon learned from the treebank . The result that seems less expected here is that , as opposed to the ideal scenario , we see no improvement in the results of UDv2New relative to UDv2 . For some of the metrics the results slightly drop . This drop could be either due to parser errors , or due to the lack of lexical coverage of the lexicon with respect to our revised UDv2New scheme . To test this , we execute an infused scenario where the morphological analysis lattices are guaranteed to also include the correct analysis . Here we see a substantial improvement for both types of lexica , on all the different metrics , for the UDv2New version . This result suggests that the drop has indeed been due to the insufficient lexical coverage of the resources , or due to mismatches between the lexicon and the new scheme . As far as the statistical components for morphological and syntactic analysis and disambiguation go , the revised version helps the parser obtain better disambiguation , in line of our results in the gold experiments .", "entities": [[223, 224, "DatasetName", "UD"], [255, 257, "TaskName", "morphological analysis"], [298, 299, "DatasetName", "UD"], [320, 322, "TaskName", "morphological analysis"], [334, 336, "TaskName", "morphological analysis"], [344, 346, "TaskName", "morphological analysis"], [350, 352, "TaskName", "morphological disambiguation"], [433, 435, "TaskName", "morphological disambiguation"], [550, 552, "TaskName", "morphological analysis"], [677, 679, "TaskName", "morphological analysis"]]}
{"text": "The original HTB ( Sima'an et al , 2001 ; Guthmann et al , 2008 ) has seen many revisions all of which executed automatically , or semi - automatically . Our endeavor here has been to manually verify the current version of the UD HTB resulting analyses , and to correct lingering errors . Apart from being linguistically justified , this process has proven to be also empirically valuable , as indeed this revision has led to a improvement in parsing results . Much work is still needed in order to bring the level of performance to be adequate for downstream applications , in particular in realistic scenarios . We conjecture that in order to obtain decent performance , the work on the treebank should be complemented by adapting language - specific lexica to the set of guidelines for word segmentation and for representing morphology , as defined by UD . Even when external lexica assumes the same labeling scheme as UD , gaps between the theories underlying the development of these resources could lead to lack of coverage that substantially harms parsing performance . Additional lessons learned from our manual verification process have to do with the organization of morphological features and syntactic subtypes within the HTB and in the UD treebanks collection in general . In the HTB UDv2 , there appeared to be a mix between the linguistic notions expressed using these two mechanisms . For example , subtypes were sometimes used to indicate morphological features ( see the case for acl : inf ) while the features column is exploited to express syntactic properties . We argue that clearer guidelines are needed in the general UD scheme , instructing directly what kind of linguistic information should go where , by which formal means . Furthermore , it seems to us that the languagespecific mechanisms are exploited for expressing phenomena that could potentially be crosslinguistic , or at least shared by a language family . An example to this is the feature HebBinyan in the UD HTB , which stores the value of the morphological template of the verb . The phenomenon of Binyan ( a root - template construction ) is clearly not Hebrew specific - in fact all Semitic languages have Binyanim ( morphological constructions ) in their grammar , so we see no good reason for not unifying this feature across the Semitic sub - family . Same goes with marking construct state nouns , a phenomenon that extends beyond Semitic languages , and is currently marked differently in each language ( Hebrew , Arabic , Persian , etc . ) . We propose that the next major revision of the UD treebank scheme could ideally focus on the universal organization of the grammar , and will center around these themes : subtypes : A universal inventory and management of the sub - label system which will define what linguistic phenomena can count as subtype of a label , and will maintain crosslinguistic consistency in its use for shared phenomena . features : A universal inventory and management of features which will define what can count as a feature , and will foster crosslinguistic reuse . lexical resources : For languages that have external lexica , especially in the case of morphologically rich and resource scarce languages , an effort is needed to verify that the labeling scheme theoretical guidelines underlying lexica are harmonized with the UD guidelines . Such lexica can be made available via the CoNLL - UL format ( More et al , 2018 ) to benefit the entire UD community . semantic applications : in addition to aligning lexical resources , it is important to advance the usability of UD in down - stream application scenarios , by making available the additional layer of enhanced dependencies .", "entities": [[44, 45, "DatasetName", "UD"], [150, 151, "DatasetName", "UD"], [162, 163, "DatasetName", "UD"], [212, 213, "DatasetName", "UD"], [280, 281, "DatasetName", "UD"], [339, 340, "DatasetName", "UD"], [448, 449, "DatasetName", "UD"], [573, 574, "DatasetName", "UD"], [599, 600, "DatasetName", "UD"], [620, 621, "DatasetName", "UD"]]}
{"text": "In this paper we describe the long and multiphased process of coming - into - existence of the Hebrew version of the HTB . Most of the process has consisted of automatic conversions between different schemes . In this work we manually verified the recent UD HTB version and corrected lingering errors . The revised version is more linguistically and cross - linguistically consistent and obtains better parsing results in scenarios that are not dependent on the coverage of external lexica . Our future plans include a comprehensive revision of the lexical and morphological resources associated with the UD scheme , to improve the empirical parsing results in realistic scenarios , and the addition of enhanced dependencies , which would be more adequate for downstream semantic tasks .", "entities": [[45, 46, "DatasetName", "UD"], [98, 99, "DatasetName", "UD"]]}
{"text": "Language models are trained only on text despite the fact that humans learn their first language in a highly interactive and multimodal environment where the first set of learned words are largely concrete , denoting physical entities and embodied states . To enrich language models with some of this missing experience , we leverage two sources of information : ( 1 ) the Lancaster Sensorimotor norms , which provide ratings ( means and standard deviations ) for over 40 , 000 English words along several dimensions of embodiment , and which capture the extent to which something is experienced across 11 different sensory modalities , and ( 2 ) vectors from coefficients of binary classifiers trained on images for the BERT vocabulary . We pre - trained the ELECTRA model and fine - tuned the RoBERTa model with these two sources of information then evaluate using the established GLUE benchmark and the Visual Dialog benchmark . We find that enriching language models with the Lancaster norms and image vectors improves results in both tasks , with some implications for robust language models that capture holistic linguistic meaning in a language learning context .", "entities": [[120, 121, "MethodName", "BERT"], [128, 129, "MethodName", "ELECTRA"], [135, 136, "MethodName", "RoBERTa"], [148, 149, "DatasetName", "GLUE"], [152, 154, "TaskName", "Visual Dialog"]]}
{"text": "Children learn their first spoken language in a highly interactive setting where generally the first words children learn are concrete words that denote physical objects , which is an important developmental step in child first language acquisition ( Kuperman et al , 2012a ; McCune , 2008 ; Clark , 2013 ) . This is partly because handling the Symbol Grounding Problem - the ablity to connect symbolic knowledge of language with representations of the physical world ( Harnad , 1990 ) - must take place before children learn more abstract concepts later in their cognitive development ( Borghi et al , 2019 ; Ponari et al , 2018 ) . Importantly , the physical world is not just the visual world ; children learn that words ground into proprioperceptive states ( e.g. , a hand grasp around an object has specific muscle activations tied to the word grab ) , interoceptive states ( i.e. , affect and valence ) , as well as all other sensory modalities ( e.g. , the word stinky grounds into olfactory , the word loud grounds into auditory ) . These claims are evidenced in a large body of child development and cognitive science literature . Smith and Gasser ( 2005 ) , for example , identified that babies ' experience of the world is profoundly multimodal : babies live in a physical world full of rich regularities that organize perception , action and thought ; babies learn in a social world to learn a shared linguistic communicative system that is symbolic . Furthermore , a growing body of literature from linguistics and computational linguistics makes a strong case that the process of language learning ( indeed , general human cognition ) is embodied , interactive , and enacted ; i.e. , movement in the world is required ( Pulverm\u00fcller , 1999 ; Lakoff and Johnson , 1999 ; Barsalou , 2008 ; Johnson , 2008 ; Smith and Samuelson , 2009 ; Di Paolo et al , 2018 ; Bisk et al , 2020 ) ; see also the prior work in developmental robotics research ; e.g. , Cangelosi and Schlesinger ( 2015 ) , Chapter 7 . 1 Taken together , it is clear that aspects of the physical world are necessary for holistic knowledge of semantic meaning , which has implications for how language is modeled computationally . In particular , what does this mean for language models that are trained purely on text ( likely largely written by adults ) , such as BERT ( Devlin et al , 2018 ) or GPT - 3 ? These models have clearly led to important advances for natural language processing tasks and applications , but it is also clear that language models trained only on text are missing critical semantic information ( Bender and Koller , 2020 ) . In this paper , we contribute to a growing body of recent work that attempts to addresses these limitations by ( 1 ) leveraging multimodal and sensorimotor knowledge of the Lancaster Sensorimotor Norms ( Lynott et al , 2019 ) and ( 2 ) using vectorized representations of images by treating both ( 1 ) and ( 2 ) as embeddings of language models for GLUE and Visual Dialog benchmarks . In the following section , we explain related work - a growing body of literature that is adding multimodal information to language models , then we explain our two embeddings that we will use . We explore how these embeddings can be used to enrich the ELEC - TRA language model 's pre - training and fine - tuning , and evaluate on the GLUE benchmark ( Experiment 1 , Section 4 ) , and how they can be used to replace input embeddings for a pre - trained RoBERTa model for the Visual Dialog task ( Experiment 2 , Section 5 ) . Our experiments shed light on how useful multimodal information can be in a task that is text - only ( Experiment 1 ) and a task that is multimodal ( Experiment 2 ) . Our results show that our parsimonious method to unifying vision ( and sensorimotor knowledge ) in existing language models shows improvements in multimodal benchmarks with accessible hardware ( i.e. , a single GPU ) as a step towards models that can be trained in settings similar to that of child language learners .", "entities": [[35, 37, "TaskName", "language acquisition"], [423, 424, "MethodName", "BERT"], [432, 433, "MethodName", "GPT"], [542, 543, "DatasetName", "GLUE"], [544, 546, "TaskName", "Visual Dialog"], [612, 613, "DatasetName", "GLUE"], [637, 638, "MethodName", "RoBERTa"], [641, 643, "TaskName", "Visual Dialog"]]}
{"text": "Language models are trained on text . G\u00fcnther et al ( 2018 ) took up the question do words inherit sensorimotor activation from purely linguistic context ? and showed that experience is necessary for reactivating experiential traces , but this reactivation is not a necessary condition for understanding the corresponding aspects of word meaning . We take this to mean that humans are very adept at learning new concepts from language exposure alone ( i.e. , abstract concepts ) ; e.g. , someone who has never seen a zebra before , but hears them described as \" horses with vertical black and white stripes \" can compose a connotation of what zebra denotes without direct visual exposure . However , this only works if an agent that has learned the language has the knowledge of horses , black , white , stripes , and vertical concepts - i.e. , via direct experience , not just through linguistic exposure or encyclopedic definitions . These claims are further backed up by neuroscience research that showed that neural assemblies encode concrete content words ( i.e. , words that denote visual objects ) and verbs ( i.e. , words that denote actions ) are learned and represented in different brain regions ( Pulverm\u00fcller , 1999 ; Borghesani et al , 2019 ) . Rogers et al ( 2020 ) provides a recent primer and overview of research that has attempted to uncover strengths and weaknesses of BERT and related language models ( so - called BERTology ) . While our work does fit into that growing body of literature , our criticisms on current language models specifically lies in the fact that they are only trained on easy - to - obtain text . This criticism is born out in Forbes et al ( 2019 ) which showed that BERT can guess affordances and properties of objects because that information can be found in text ( e.g. , a typical chair has the affordance of being sittable , and a property of having legs ) , but has no notion of how objects are related semantically to each other , and Da and Kasai ( 2019 ) further showed that real - world perceptual properties are likely to be assumed instead of inferred . Furthermore , Bender and Koller ( 2020 ) make a strong case that BERT learns form instead of meaning , and while the fact that BERT performs so well on many tasks is difficult to dispute , models trained on text are missing semantic information crucial for holistic language understanding . Since before BERT which has proven powerful in many language processing tasks , efforts have been made to encode multimodal ( i.e. , more than just text as a learning modality ) information into embeddings and language models ( Takano and Utsumi , 2016 ; Kiros et al , 2014 ; Zellers et al , 2021 ) and recent , continued efforts towards bridging grounded visual representations to distributional representations of word meanings give credence to the claim that text - only models like BERT are missing crucial semantic information because enriching BERT with visual information improves performance in several known tasks ( Kim et al , 2019 ; Lu et al , 2019 ; Li et al , 2019 ) . These models usually treat language and vision as separate pipelines ; our method directly endows the language model with visual and sensorimotor knowledge .", "entities": [[125, 126, "DatasetName", "agent"], [242, 243, "MethodName", "BERT"], [305, 306, "MethodName", "BERT"], [393, 394, "MethodName", "BERT"], [405, 406, "MethodName", "BERT"], [433, 434, "MethodName", "BERT"], [515, 516, "MethodName", "BERT"], [523, 524, "MethodName", "BERT"]]}
{"text": "In this section , we motivate and introduce of multimodal information we will use in our experiments . The Lancaster Sensorimotor Norms The Lancaster Sensorimotor norms ( Lynott et al , 2019 ) provide ratings ( means and standard deviations ) for 40 , 000 English words along dimensions of embodiment which capture the extent to which a concept is experienced across 11 different sensory modalities , and measures derived from those categories , listed below ( each has an example word that rates highly for that modalitiy ) : Auditory - sound ; ping Gustatory - having to do with eating ; cream Haptic - muscle movement ; handshake Interoceptive - having to do with affect or emotion ; headache Olfactory - smell ; incense Visual - visual ; barcode Foot - leg - haptics for foot / leg ; run Hand - arm - haptics for hand / arm ; pointing Head - having to do with the head ; eye Mouth - haptics for mouth ; kiss Torso - haptics for torso ; breath Max - strength.perceptual - the highest rating across the 11 sensorimotor dimensions Minkowski3.perceptual - treating the 11 modalities as a vector , this represents the distance of the vector from the origin with influence of weaker dimensions attenuated Exclusivity.perceptual - the extent to which a concept ( out of the 11 ) which is experienced through a single perceptual modalitiy The last three can be seen as aggregates from the 11 modalities ; they also have .action values representing the extent to which a concept is experienced as an action ( as opposed to .perceptual ) , and .sensorimotor values representing the extent a concept is experience as sensorimotor . As these norms were derived from surveys given to adults , these norms represent the degree to which the survey participants assigned those words to those categories . Though this does not represent a neurophysiological grounding of words to those modalities learned through interaction and embodiment , this serves as a useful approximation . The final set is a vocabulary of 39 , 707 words ( after removing rows which had null values ) , each represented as a vector of length 39 ( i.e. , 11 mean , 11 stdev columns ; Max - strength , Minkowski , and Exclusivity columns for different ways of aggregating the modalities ) . We normalize each value in the vector independently to a value between 0 - 1 by dividing each value over its max value . We call this the Lancaster vectors . We performed t - SNE on the Lancaster vectors ( mapping to 2 dimensions ) to determine if clus - ters would reveal any intuitions about the kinds of semantic relatedness that the words might have with each other . Some clusters emerged such as foods ( presumably because they have similar gustatory ratings ) , leg - movement verbs ( e.g. , walk , jump , sit ) , colors with eye - related words ( e.g. , purple , green , blue , dark , see , eyes ) , soft things ( e.g. , hug , tummy , pillow , clothes ) , audio - related words ( e.g. , talk , story , sound , music , lie , say ) , among others .", "entities": [[118, 119, "DatasetName", "emotion"], [410, 411, "DatasetName", "0"]]}
{"text": "The main contribution of this paper is to explore using the Lancaster Sensorimotor Norms and the Words - as - Classifiers model as vectorized knowledge from the physical world on the GLUE and Visual Dialog tasks . Lancaster norms performed well on their own in one GLUE task compared to other word embeddings like GloVe , and coupled with the WAC vectors as the embedding in an ELECTRA model , they performed respectably on the GLUE task . The WAC vectors , when used as embeddings in the RoBERTa model performed well on the Visual Dialog task , particularly when the vocabulary was more restricted to the Age of Acquisition vocabulary . Crucially , this work differs from other visually grounded models because the grounded knowledge is part of the language model itself ( i.e. , the embeddings ) rather than computed in parallel and added for a task - specific purpose . Moreover , standard language models can not actually identify denotations when they are present ; i.e. , ELECTRA and RoBERTa are not actually capable of determining if an object is red or soft from observing that object - a basic ability for a language learning child - simply because those models can not observe the world outside of text , though the purpose of the WAC ( and models like VilBERT ) model is to do just that : identify denotations ; by coupling WAC with ELECTRA and RoBERTa , both models can make use of that capability . This work is critical in our ongoing efforts towards a model that learns language in a co - located setting in an embodied platform . In particular , our knowledge from this paper informs us that the ELECTRA model with embeddings tied to WAC classifier weights is a good candidate for live interaction of a robot that is learning words from a human collaborator because the ELECTRA - WAC model can function with small amounts of data and the embedding layer can successfully be tied to weights of the WAC classifiers . We leave implementation and evaluation of this model on a robotic platform for future work .", "entities": [[31, 32, "DatasetName", "GLUE"], [33, 35, "TaskName", "Visual Dialog"], [46, 47, "DatasetName", "GLUE"], [51, 53, "TaskName", "word embeddings"], [54, 55, "MethodName", "GloVe"], [67, 68, "MethodName", "ELECTRA"], [75, 76, "DatasetName", "GLUE"], [88, 89, "MethodName", "RoBERTa"], [94, 96, "TaskName", "Visual Dialog"], [170, 171, "MethodName", "ELECTRA"], [172, 173, "MethodName", "RoBERTa"], [223, 224, "MethodName", "VilBERT"], [239, 240, "MethodName", "ELECTRA"], [241, 242, "MethodName", "RoBERTa"], [289, 290, "MethodName", "ELECTRA"], [318, 319, "MethodName", "ELECTRA"]]}
{"text": "Verbal prediction has been shown to be critical during online comprehension of Subject - Object - Verb ( SOV ) languages . In this work we present three computational models to predict clause final verbs in Hindi given its prior arguments . The models differ in their use of prior context during the prediction processthe context is either noisy or noise - free . Model predictions are compared with the sentence completion data obtained from Hindi native speakers . Results show that models that assume noisy context outperform the noise - free model . In particular , a lossy context model that assumes prior context to be affected by predictability and recency captures the distribution of the predicted verb class and error sources best . The success of the predictabilityrecency lossy context model is consistent with the noisy channel hypothesis for sentence comprehension and supports the idea that the reconstruction of the context during prediction is driven by prior linguistic exposure . These results also shed light on the nature of the noise that affects the reconstruction process . Overall the results pose a challenge to the adaptability hypothesis that assumes use of noise - free preverbal context for robust verbal prediction .", "entities": [[70, 72, "TaskName", "sentence completion"]]}
{"text": "In spite of the proposed central role of verb prediction during online processing of Hindi ( e.g. , Vasishth and Lewis , 2006 ; Agrawal et al , 2017 ; Husain et al , 2014 ) , there is a surprising lack of any modeling attempt to understand the processes that subserve verbal predictions in the language . While there are computational metrics that model reading time data ( e.g. , Hale , 2001 ; Shain et al , 2016 ; Futrell et al , 2020 ) , a computational model that makes precise verbal prediction in SOV languages has not been investigated thoroughly ( but see , Grissom II et al , 2016 , for an initial attempt ) . Understanding the mechanisms that subserve verbal prediction in SOV languages is critical to understanding how these languages are processed ( cf . Konieczny , 2000 ; Vasishth et al , 2010 ; Husain et al , 2014 ; Levy and Keller , 2013 ; Kuperberg and Jaeger , 2016 ) . Our work fills this gap in the literature . In this section we summarize the key results of a recent study by Apurva and Husain ( 2020 ) who investigated the nature of verbal prediction in Hindi using a series sentence completion studies . Later , in sections 4 , 5 we present three computational models to account for these results .", "entities": [[212, 214, "TaskName", "sentence completion"]]}
{"text": "Apurva and Husain ( 2020 ) used the sentence completion paradigm ( Taylor , 1953 ) to probe the nature of clause final verbal prediction when differing the number of preverbal nouns that precede the tobe - completed target verb . The number of nouns ranged from 1 to 3 and appeared in different casemarker order . All preverbal nouns were proper nouns . Example 1 shows some of the conditions where 3 preverbal nouns preceded the target verb . In the example , ne is the Ergative case - marker , ko is the Accusative case - marker and se is the Ablative case - marker . In all , there were 6 conditions in this experiment ( ne - ko - se , ne - se - ko , ko - ne - se , ko - se - ne , se - ko - ne , se - ne - ko ) . 36 native speakers participated in the 3 - NP condition experiments . Similar to the 3 - NP conditions , the 1 - NP and 2 - NP items had proper nouns and the nouns occurred in various case - marker order . 25 native speakers participated in the 1 - NP and 2 - NP condition experiments . ( The key result from these completion studies was that the number of ungrammatical verbal completions increased as the number of preverbal nominals increased . For the 1 - NP conditions the percentage ungrammatical completions was 4 % , for the 2 - NP conditions this was 8 % , while for the 3 - NP conditions the ungrammatical completions increased to 15 % . In addition , the completion data was also analyzed for the nature of grammatical and ungrammatical verbal completions . Completions were analyzed based on the verb classes rather than lexical identity ( cf . Luke and Christianson , 2016 ) . The data contains a distribution over a total of 18 verbs classes for the 2 - NP and 3 - NP conditions . In majority of the grammatical completions , Hindi native speakers posit simple syntactic structures ( in terms of the number of clausal embeddings and the number of core argument structure ) . For the 2 - NP conditions , the topmost verb classes were T ( Transitive verb ) , IN ( Intransitive verb ) , and DT ( Ditransitive verb ) . For the 3 - NP conditions , CAUS ( Causative verb ) and T DT ( Transitive non - finite verb followed by a ditransitive matrix verb ) were consistently the most frequent , covering at least 50 % of completions between them for all conditions . Some of the other classes observed were DT , N T DT , and DT DT . Interestingly , while the 3 - NP conditions can be grammatically completed using a double embedded structure ( e.g. , IN DT DT ) , such cases were not found in the completion data . Among the ungrammatical verb completions across various conditions , N DT , IN DT and CAUS were consistently the most frequent verb classes predicted . Similar to the trend in the grammatical completions discussed above , the parser posits simple structures even when making mistakes . Additionally , a closer analysis of the ungrammatical completions showed the formation of locally coherent parses ( Tabor et al , 2004 ) for the various 3 - NP conditions where the first noun was ignored and only the 2nd and the 3rd nouns were used to make the prediction ( we call these N2 - N3 errors ) . Other errors were made when either N2 or N3 were ignored to make the prediction ( we call these N1 - N3 , N1 - N2 errors respectively ) . The errors also show a subject primacy effect ( H\u00e4ussler and Bader , 2015 ; Knoedler et al , 1999 ) where the presence of an Ergative case marker on N1 is not forgotten . This leads to lack of passive predictions in such cases . 2 To sum up , the key results of the completion studies were , ( a ) verb prediction was good in 1 - NP and 2 - NP conditions , ( b ) predictions deteriorated in 3 - NP conditions , ( c ) grammatical verbal completions are syntactically simple rather than complex ( e.g. , clausal embeddings are avoided ) , ( d ) error types for the 3 - NP conditions show use of two preverbal NPs to make predictions , as well as being sensitive to subject primacy . Table 1 provides the details on the number of grammatical and ungrammatical completions over all conditions . Also see Table 3 for verb class numbers for the 2 - NP conditions . Table 2 shows examples of various error types in the 3 - NP conditions .", "entities": [[8, 10, "TaskName", "sentence completion"]]}
{"text": "We use the monolingual Hindi corpus developed by IIT Bombay ( Kunchukuttan et al , 2017 ) . It is a collection of raw sentences of Hindi taken from various sources ( HindMonoCorp ( Bojar et al , 2014 ) , BBC , Wikipedia etc . ) . For training our models , we use the first 5 million sentences of this data . For the sentence simplification step ( described in the Section 3.2 ) , we use the ISC dependency parser for Hindi . 3 Moreover , as the sentence completion experiment included only animate nouns in various items ( see Section 2 ) , we use an additional animacy annotation ( Jena et al , 2013 ) to label the nouns accordingly .", "entities": [[91, 93, "TaskName", "sentence completion"]]}
{"text": "A key aim of the behavioral experiments discussed in Section 2 was to investigate the role of preverbal arguments on clause final verbal prediction . Consequently , our models had to be trained on sentences with various features ( e.g. , case - marker , animacy ) of the preverbal arguments . Since the raw data may contain other intervening material ( nominal modifiers , verbal adjuncts , etc . ) , 4 the task necessitated removal of such material from the training corpus to render it more tractable to the appropriate computational model . Thus , we simplify each sentence in the training data by removing these intervening materials while ensuring that the grammaticality of the sentence remains intact . 5 This , of course , implies that the model only uses the local argument structure to make the necessary verbal prediction . The sentence simplification process preserves verbal and nominal arguments , such as direct / oblique objects , case - markers , and auxiliaries , but removes adjective phrases , relative clauses , and adjuncts . It treats conjunct structures as separate components . It identifies intra - sentential noun ellipsis and truncates a sequence that displays such a structure , while processing its other verbs . For example : We also flatten all the nouns in the data to \" noun tokens \" by merging the noun and its corresponding case - marker . Since we are interested in capturing the variations of the completions for different order of case - markers in the prompt , we can abstract away from the lexicality of the nouns . Thus , we replace the nominal lexical item with its corresponding label depending on whether it is animate ( A ) or not ( N ) . Such an abstraction is well motivated considering that humans are known to be sensitive to both syntactic part - of - speech tags as well as lexical semantics during sentence processing ( e.g. , Demberg and Keller , 2008 ; Trueswell et al , 1994 ) .", "entities": [[315, 318, "DatasetName", "part - of"]]}
{"text": "All the models are evaluated by comparing the model output with the sentence completion data obtained from the native speakers ; specifically , model output is evaluated in terms of the nature of the predicted verb class . We let VC denote the set of all verb - classes , h ( x ) denotes the probability distribution of verb - class predictions made by humans , and m ( x ) denotes the corresponding distribution of the model . We measure KL - divergence between these two distributions , replacing zero probabilities with a fixed value 9 ( = 10 \u22125 ) ; this is shown in ( 1 ) KLp ( h | | m ) = KL ( h | | m ) ( 1 ) where KL denotes the KL - divergence and m is a distribution such that m ( x ) = max ( m ( x ) , 10 \u22125 ) for each x VC . Apart from this primary measure , we use two other metrics F and D to quantify the span and quality of model predictions with respect to the predicted verb classes , respectively , in order to better understand these characteristics of each model ( see Section 6.1 ) . Further , to ascertain a qualitative understanding of the model performance , we also evaluate each model on the basis of the following characteristics that are displayed in the completion data discussed in Section 2 : Deterioration in the number of grammatical completions on the 3 - NP conditions compared to the 2 - NP conditions Within the grammatical completions , a preference for simpler structures as opposed to complex or embedded constructions Exhibition of similar types of errors as humans ; for example , in 3 - NP conditions , N1 - N2 errors , as well as a sensitivity to subject primacy with the Ergative case . For the 3 - NP conditions , we classify errors into types based on their compatibility with a 2 - NP sub - context ( N1 - N2 , N1 - N3 , N2 - N3 ) . For example , an error type of N1 - N2 would mean that the corresponding ungrammatical prediction is compatible only with first two NPs and not the full 3 - NP context . This scheme follows the error types found in the completion data discussed in Section 2 . Additionally , see Section 2 of the supplementary material for examples of various errors .", "entities": [[12, 14, "TaskName", "sentence completion"], [415, 417, "DatasetName", "supplementary material"]]}
{"text": "In this section , we discuss two models to test the noisy channel hypothesis . As stated in Section 1 , the underlying assumption is that human communication is noisy ( Gibson et al , 2013 ; Kurumada and Jaeger , 2015 ) and the comprehender has to reinterpret the input to make prediction about upcoming linguistic material . In order to evaluate this hypothesis , we implement different versions of the lossy - context surprisal metric ( Futrell et al , 2020 ) . Lossy - context surprisal holds that processing difficulty at a word in a context is proportional to the surprisal of a word given a lossy memory representation of the context . The two models discussed in sections 5.1 and 5.2 differ in their noise functions that affect the interpretation of the preverbal context . For the current investigation , lossy - context surprisal is extended to model the sentencecompletion task . The word with the highest probability in a given context is assumed to be most likely to complete the sentence ( cf . Levy , 2008 ; Smith and Levy , 2013 ) . As noted by Futrell et al ( 2020 ) , the lossy surprisal model is not representation - agnostic . Its predictions are dependent on a noise distribution ( M ) . One can then obtain : p ( w | r ) \u221d c p M ( r | c ) p ( c ) p L ( w | c ) , ( 2 ) where w is the predicted word and r is the result of adding noise to the context c. Here , we consider L to be a 4 - gram model , same as the one discussed in Section 4 . Moreover , for c = w 1 w 2 w n we calculate p ( c ) also using L p ( c ) = n i=1 p L ( w i | w i\u22123 w i\u22122 w i\u22121 ) In addition , if | c | = n \u2264 2 , we do n't add any noise to the context and simply use the ngram model L for prediction . In other words , if c = w 1 w 2 or c = w 1 , then we consider p ( w | r ) = p L ( w | c ) . Since we only consider erasure - based noise distributions , this is done to ensure that the whole context is not lost during prediction . In order to get an average behavior of the model , we run the model 10 times and then take the top 50 predictions based on the total probability of each prediction . In other words , suppose a phrase s is predicted to follow a given preverbal arguments in a condition . Then , the total probability of s to be predicted in the given condition by the average model is equal to 1 10 10 i=1 p i ( s ) , where p i ( s ) denotes the probability of prediction s in the ith run . Note that if s is not predicted in the ith run , then p i ( s ) = 0 . In the next subsections , we present two models with different noise distribution .", "entities": [[548, 549, "DatasetName", "0"]]}
{"text": "In this section we assess the span and quality of the predictions made by the models when compared to the human data . The span of verb prediction made by the model can be computed by the proportion of human distribution that the model misses on . Formally , F ( h | | m ) \u221d x VC m ( x ) = 0 h ( x ) ( 5 ) Since the model will not be able to predict all verb classes that humans produce , we formulate a metric to evaluate the quality of the predictions that the model makes . For this , we restrict the verb classes to only those that are predicted by the model and find the KL - divergence ( Kullback and Leibler , 1951 ) on those verb classes between the model and the human ; this is shown in ( 6 ) D ( h | | m ) = x VC m ( x ) = 0 h ( x ) log h ( x ) m ( x ) ( 6 ) where h ( x ) is normalized from h ( x ) after removing x where m ( x ) = 0 . Note that higher the F , lower is the model 's span ; and similarly , higher the D , lower is its quality of predictions ( as compared to humans ) . Table 5 shows that for both F and D , the LC - Surp Pred - Rec model consistently outperforms the LC - Surp Pred - Bias and the 4 - gram surprisal model . This suggests that when compared to the human data , the LC - Surp Pred - Rec is better in predicting the valid verb class both in terms of span and the quality of the predictions .", "entities": [[64, 65, "DatasetName", "0"], [167, 168, "DatasetName", "0"], [205, 206, "DatasetName", "0"]]}
{"text": "In order to interpret the metrics mentioned in Table 5 , we did a detailed analysis of the model output in terms of the nature of verb class and the type of prediction errors . This is summarized in Table 6 . One can note that Grammaticality in all models drops in 3 - NP conditions as compared to 2 - NP conditions , in line with the human data ( cf . Section 2 ) 13 . The models prefer simple outcomes , and largely predict DT , CAU S ( grammatical ) and T , N DT ( ungrammatical ) . Investigating the reason for the better span of the Pred - Rec model , we find that it is primarily due to the important T DT verb class . This embedded 13 See Section 5 of the supplement for actual percentages . structure is often used by humans , and neither of the 4 - gram or the Pred - Bias model managed to predict it ; thus , we can link the better span numbers of the Pred - Rec model to an observable improvement in the nature of verbal predictions . We also study the error types made by the models and compare them to human errors . The 4 - gram model by its nature is only capable of making the locally coherent N2 - N3 errors , whereas both the Pred - Bias and Pred - Rec models produce N1 - N3 and N1 - N2 errors as well . However , while the human data was sensitive to the subject primacy effect - presence of Ergative case - marker never lead to passive verb completion ; none of the models is able to fully replicate this pattern . However , the 4 - gram model produces the least percentage of passives , followed by the Pred - Rec model . See Section 6 of the supplementary material for more details about error types .", "entities": [[323, 325, "DatasetName", "supplementary material"]]}
{"text": "Results show that the Lossy context surprisal model with Predictability Recency Bias noise performs best in terms of the distribution of predicted verbs and the error types vis - \u00e0 - vis the completion data . This provides support for the noisy channel hypothesis and poses a challenge to the adaptability hypothesis . In addition , the comparison of the two lossy surprisal models sheds light on the nature of the noise during the reconstruction process . Results show that qualitatively all the models capture the completion data to a certain extent ( see , Section 6.2 ) . At the same time , overall the noisy context models performed better than the ngram model in two clear ways . First , the models were able to capture the differential nature of casemarker combination in a limited context . This leads to better coverage of error sources ( both in terms of errors made and not made ) . Second , the models were therefore also better at making better verb predictions compared to the n - gram model . In particular , the overall success of the Pred - Rec model showed that reconstruction of the noisy context in influenced by both past exposure of preverbal subcontext and the recency of the context ( cf . Futrell et al , 2020 ) . Put differently , the reconstruction of the context is driven by sub - strings that are more frequent ( e.g. , ne - ko ) and that are closer to the verb . Critically , this shows that the reconstruction process is not random . 14 While the performance of the predictability recency model is good , it suffers from three issues ( a ) it overestimates the number of errors made by humans , ( b ) its overall coverage for various verb class is low , and ( c ) it is insensitive to subject primacy . The model is able to successfully predict verb phrase involving no clausal embedding , and to a limited extent , those with embeddings . While certain complex structures such as N DT DT , predicted rarely by humans , are dropped entirely by the model , its prediction for the T DT structure which is frequent in the completion data is not that high . An investigation into the data also shows a scarcity of training examples that exhibit an animate 3 - NP context followed by such T DT continuations . 15 One reason for this could be the size of the training data , currently 5 million sentences ; future work can train on a larger data set . Another possibility is that certain patterns in the human data are not captured in the written corpus used for training and requires a dialogue corpus . Unfortunately , such a corpus currently does not exist for Hindi and attempts to modeling using such a data will have to wait its availability . Relatedly , argue that prediction based on corpus frequency of syntactic information may not be able to fully capture the notion of preactivation during the completion task . Hence , future work will need to incorporate other sources of information . Finally , the results show that the 4 - gram model is more sensitive to subject primacy . This is because , the 4 - gram model ( unlike noisy context models ) has access to the N1 features when making predictions . It can thus correctly use the N1 case feature to avoid predicting passive verbs . This suggests that a noise function relying only on local information will be limited in accounting for the current data . tion 5 , we also investigated a purely random noise function . Due to space constraint , details of this model have been mentioned as supplementary material ( Section 7 ) . 15 See Section 3 of the supplementary material for more details on training data . The current work provided the first set of detailed results towards modeling clause final verb prediction in an SOV language . The work demonstrated the effectiveness of lossy surprisal models and probed the nature of the noise function during the reconstruction process . In addition to the quantitative analyses demonstrating the success of the Predictability Recency lossy surprisal model , a key contribution of the work was that it highlighted the nature of model 's closeness to the human data , both in terms of verb class prediction and the error type . Overall , the results support the proposals that highlight the detrimental effect of increased complexity of the preverbal linguistic material in SOV languages ( e.g. , Gibson et al , 2013 ; Ueno and Polinsky , 2009 ; Ros et al , 2015 ; Yadav et al , 2020 ) . Future models need to explore other noise functions to investigate the interaction of context predictability with recency as well as primacy of non - local information ( e.g. , subject ) . Further , these models need to be tested to investigate the effect of distance ( e.g. , Vasishth and Lewis , 2006 ) and structural complexity ( Vasishth et al , 2010 ) on verbal prediction in SOV languages .", "entities": [[641, 643, "DatasetName", "supplementary material"], [654, 656, "DatasetName", "supplementary material"]]}
{"text": "Interpretability methods for neural networks are difficult to evaluate because we do not understand the black - box models typically used to test them . This paper proposes a framework in which interpretability methods are evaluated using manually constructed networks , which we call white - box networks , whose behavior is understood a priori . We evaluate five methods for producing attribution heatmaps by applying them to white - box LSTM classifiers for tasks based on formal languages . Although our white - box classifiers solve their tasks perfectly and transparently , we find that all five attribution methods fail to produce the expected model explanations .", "entities": [[71, 72, "MethodName", "LSTM"]]}
{"text": "Attribution methods are a family of interpretability techniques for individual neural network predictions that attempt to measure the importance of input features for determining the model 's output . Given an input , an attribution method produces a vector of attribution or relevance scores , which is typically visualized as a heatmap that highlights portions of the input that contribute to model behavior . In the context of NLP , attribution scores are usually computed at the token level , so that each score represents the importance of a token within an input sequence . These heatmaps can be used to identify keywords upon which networks base their decisions ( Li et al , 2016 ; Sundararajan et al , 2017 ; Arras et al , 2017a , b ; Murdoch et al , 2018 , inter alia ) . One of the main challenges facing the evaluation of attribution methods is that it is difficult to assess the quality of a heatmap when the network in question is not understood in the first place . If a word is deemed relevant by an attribution method , we do not know whether the model actually considers that word relevant , or whether the attribu - tion method has erroneously estimated its importance . Indeed , previous studies have argued that attribution methods are sensitive to features unrelated to model behavior in some cases ( e.g. , Kindermans et al , 2019 ) , and altogether insensitive to model behavior in others ( Adebayo et al , 2018 ) . To tease the evaluation of attribution methods apart from the interpretation of models , this paper proposes an evaluation framework for attribution methods in NLP that uses only models that are fully understood a priori . Instead of testing attribution methods on black - box models obtained through training , we construct white - box models for testing by directly setting network parameters by hand . Our focus is on white - box LSTMs that implement intuitive strategies for solving simple classification tasks based on formal languages with deterministic solutions . We apply our framework to five attribution methods : occlusion ( Zeiler and Fergus , 2014 ) , saliency ( Simonyan et al , 2014 ; Li et al , 2016 ) , gradient \u00d7 input , ( G \u00d7 I , Shrikumar et al , 2017 ) , integrated gradients ( IG , Sundararajan et al , 2017 ) , and layer - wise relevance propagation ( LRP , Bach et al , 2015 ) . In doing so , we make the following contributions . We construct four white - box LSTMs that can be used to test attribution methods . We provide a complete description of our model weights in Appendix A. 1 Beyond the five methods considered here , our white - box networks can be used to test any attribution method compatible with LSTMs . Empirically , we show that all five attribution methods produce erroneous heatmaps for our white - box networks , despite the models ' transparent behavior . As a preview of our re - sults , Table 1 shows sample heatmaps computed for two models designed to identify the non - contiguous subsequence ab in the input aacb . Even though both models ' outputs are determined by the presence of the two as and the b , all four methods either incorrectly highlight the c or fail to highlight at least one of the as in at least one case . We identify two general ways in which four of the five methods do not behave as intended . Firstly , while saliency , G \u00d7 I and IG are theoretically invariant to differences in model implementation ( Sundararajan et al , 2017 ) , in practice we find that these methods can still produce qualitatively different heatmaps for nearly identical models . Secondly , we find that LRP is susceptible to numerical issues , which cause heatmaps to be zeroed out when values are rounded to zero .", "entities": [[51, 52, "MethodName", "heatmap"], [162, 163, "MethodName", "heatmap"]]}
{"text": "Several approaches have been taken in the literature for understanding how to evaluate attribution methods . On a theoretical level , axiomatic approaches propose formal desiderata that attribution methods should satisfy , such as implementation invariance ( Sundararajan et al , 2017 ) , input translation invariance ( Kindermans et al , 2019 ) , continuity with respect to inputs ( Montavon et al , 2018 ; Ghorbani et al , 2019 ) , or the existence of relationships between attribution scores and logit or softmax scores ( Sundararajan et al , 2017 ; Ancona et al , 2018 ; Montavon , 2019 ) . The degree to which attribution methods fulfill these criteria can be determined either mathematically or empirically . Other approaches , which are more experimental in nature , attempt to directly assess the relationship between attribution scores and model behav - ior . A common test , due to Bach et al ( 2015 ) and Samek et al ( 2017 ) and applied to sequence modeling by Arras et al ( 2017a ) , involves ablating or perturbing parts of the input , from those with the highest attribution scores to those with the lowest , and counting the number of features that need to be ablated in order to change the model 's prediction . Another test , proposed by Adebayo et al ( 2018 ) , tracks how heatmaps change as layers of a network are incrementally randomized . A third kind of approach evaluates the extent to which heatmaps identify salient input features . For example , Zhang et al ( 2018 ) propose the pointing game task , in which the highest - relevance pixel for an image classifier input must belong to the object described by the target output class . Within this framework , ) , Poerner et al ( 2018 , Arras et al ( 2019 ) , and Yang and Kim ( 2019 ) construct datasets in which input features exhibit experimentally controlled notions of importance , yielding \" ground truth \" attributions against which heatmaps can be evaluated . Our paper incorporates elements of the groundtruth approaches , since it is straightforward to determine which input features are important for our formal language tasks . We enhance these approaches by using white - box models that are guaranteed to be sensitive to those features .", "entities": [[85, 86, "MethodName", "softmax"]]}
{"text": "Counter languages ( Fischer , 1966 ; Fischer et al , 1968 ) are languages recognized by automata equipped with counters . Weiss et al ( 2018 ) demonstrate using an acceptance task for the languages a n b n and a n b n c n that LSTMs naturally learn to use cell state units as counters . Merrill 's ( 2019 ) asymptotic analysis shows that LSTM acceptors accept only counter languages when their weights are fully saturated . Thus , counter languages may be viewed as a characterization of the expressive power of LSTMs . We define the counting task based on a simple example of a counting language . Task 1 ( Counting Task ) . Given a string in x { a , b } * , determine whether or not x has strictly more as than bs . Example 2 . The counting task classifies aaab as True , ab as False , and bbbba as False . A counter automaton can solve the counting task by incrementing its counter whenever an a is encountered and decrementing it whenever a b is encountered . It outputs True if and only if its counter is at least 1 . We expect attribution scores for all input symbols to have roughly the same magnitude , but that scores assigned to a will have the opposite sign to those assigned to b.", "entities": [[68, 69, "MethodName", "LSTM"]]}
{"text": "The Dyck language is the language D generated by the following context - free grammar , where \u03b5 is the empty string .", "entities": [[17, 18, "HyperparameterName", "\u03b5"]]}
{"text": "We use two approaches to construct white - box networks for our tasks . In the counter - based approach , the cell state contains a set of counters , which are incremented or decremented throughout the computation . The network 's final output is based on the values of the counters . In the automaton - based approach , we use the LSTM to simulate an automaton , with the cell state containing a representation of the automaton 's state . We use a counter - based network to solve the counter task and an automaton - based network to solve the bracket prediction task . We use both kinds of networks to solve the SP task . All networks perfectly solve the tasks they were designed for . This section describes our white - box networks at a high level ; a detailed description is given in Appendix A. In the rest of this paper , we identify the alphabet symbols a , b , c , and d with the one - hot vectors for indices 1 , 2 , 3 , and 4 , respectively . The vectors f ( t ) , i ( t ) , and o ( t ) represent the forget , input , and output gates , respectively . g ( t ) is the value added to the cell state at each time step , and \u03c3 represents the sigmoid function . We assume that the hidden state h ( t ) and cell state c ( t ) are updated as follows . c ( t ) = f ( t ) c ( t\u22121 ) + i ( t ) g ( t ) h ( t ) = o ( t ) tanh ( c ( t ) )", "entities": [[63, 64, "MethodName", "LSTM"]]}
{"text": "In the counter - based approach , each position of the cell state contains the value of a counter . To adjust the counter in position j by some value v ( \u22121 , 1 ) , we set g ( t ) j = v , and we saturate the gates by setting them to \u03c3 ( m ) \u2248 1 , where m \u226b 0 is a large constant . For example , our network for the counting task uses a single hidden unit , with the gates always saturated and with g ( t ) given by g ( t ) = tanh ( u [ 1 \u22121 ] x ( t ) ) , where u > 0 is a hyperparameter that scales the counter by a factor of v = tanh ( u ) . 2 When x ( t ) = a , we have g ( t ) = v , so the counter is incremented by v. When x ( t ) = b , we compute g ( t ) = \u2212v , so the counter is decremented by v. For the SP task , we use seven counters . The first four counters record how many occurrences of each symbol have been observed at time step t. The next three counters record the number of bs , cs , and ds that form one of the four distinguished subsequences with an earlier symbol . For example , after seeing the input aaabbc , the counterbased network for the SP task satisfies c ( 6 ) = v [ 3 2 1 0 2 1 0 ] \u22a4 . The first four counters represent the fact that the input has 3 as , 2 bs , 1 c , and no ds . Counter # 5 is 2v because the two bs form a subsequence with the as , and counter # 6 is v because the c forms a subsequence with the bs . The logit scores of our counter - based networks are computed by a linear decoder using the tanh of the counter values . For the counting task , the score of the True class is h ( t ) , while the score of the False class is fixed to tanh ( v ) /2 . This means that the network outputs True if and only if the final counter value is at least v. For the SP task , the score of the True class is h ( t ) 5 + h ( t ) 6 + h ( t ) 7 , while the score of the False class is again tanh ( v ) /2 .", "entities": [[66, 67, "DatasetName", "0"], [121, 122, "DatasetName", "0"], [271, 272, "DatasetName", "0"], [274, 275, "DatasetName", "0"]]}
{"text": "Let X be a matrix of input vectors , such that the input at time t is the row vector X t , : = ( x ( t ) ) \u22a4 . Given X , an LSTM classifier produces a vector y of logit scores . Based on X , \u0177 , and possibly a baseline input X , an attribution method assigns an attribution score R ( c ) t , i ( X ) to input feature X t , i for each output class c. These feature - level scores are then aggregated to produce token - level scores : R ( c ) t ( X ) = \u2211 i R ( c ) t , i ( X ) . Broadly speaking , our five attribution methods are grouped into three types : one perturbation - based , three gradient - based , and one decompositionbased . The following subsections describe how each method computes R ( c ) t , i ( X ) .", "entities": [[37, 38, "MethodName", "LSTM"]]}
{"text": "Perturbation - based methods are premised on the idea that if X t , i is an important input feature , then changing the value of X t , i would cause\u0177 to change . The one perturbation method we consider is occlusion . In this method , R ( c ) t , i ( X ) is the change in\u0177 c observed when X t , : is replaced by 0 . Gradient - based methods rely on the same intuition as perturbation - based methods , but use automatic differentiation to simulate infinitesimal perturbations . The definitions of our three gradientbased methods are given in Table 2 . The most basic of these is saliency , which simply measures relevance by the derivative of the logit score with respect to each input feature . G \u00d7 I attempts to improve upon saliency by using the first - order terms in a Taylor - series approximation of the model instead of the gradients on their own . IG is designed to address the issue of small gradients found in saturated units by integrating G \u00d7 I along the line connecting X to a baseline input X , here taken to be the zero matrix .", "entities": [[72, 73, "DatasetName", "0"]]}
{"text": "Occlusion , G \u00d7 I , and IG are well - behaved for the counting task . As expected , these methods assign a a positive value and b a negative value when the output class for attribution is c = True . When the number of as is different from the number of bs , occlusion assigns a lower - magnitude score to the symbol with fewer instances . When c = False , all relevance scores are 0 . This is because\u0177 False is fixed to a constant value supplied by a bias term , so input features can not affect its value . Saliency and LRP both fail to produce nonzero scores , at least in some cases . Saliency scores satisfy R ( True ) t , 1 ( X ) = \u2212R ( True ) t , 2 ( X ) , resulting in token - level scores of 0 for all inputs . Heatmaps # 3 and # 4 show that LRP assigns scores of 0 to prefixes containing equal numbers of as and bs . We will see in Subsection 7.1 that this phenomenon appears to be related to the fact that the LSTM gates are saturated .", "entities": [[79, 80, "DatasetName", "0"], [154, 155, "DatasetName", "0"], [171, 172, "DatasetName", "0"], [200, 201, "MethodName", "LSTM"]]}
{"text": "We obtain radically different heatmaps for the two SP task networks , despite the fact that they produce the same classifications for all inputs . For the counter - based network , all methods except for saliency assign positive scores for c = True to symbols constituting one of the four subsequences , and scores of zero elsewhere . The saliency heatmaps do not adhere to this pattern , and instead generally assign higher scores to tokens occurring near the end of the input . Heatmaps # 7 - 10 show that LRP fails to assign positive scores to the first symbol of each subsequence , while the other methods generally do not . 4 The LRP behavior reflects the fact that the initial a does not increment the subsequence counters , which determine the final logit score . In contrast , the behavior of occlusion , G \u00d7 I , and IG is explained by the fact that removing either the a or the b destroys the subsequence . Note that the as in heatmap # 9 receive scores of 0 from occlusion and G \u00d7 I , since removing only one of the two as does not destroy the subsequence . For the FSA - based network , saliency , G \u00d7 I , and LRP assign only the last symbol a nonzero score when the relevance output class c matches the network 's predicted class . IG appears to produce erratic heatmaps , exhibiting no immediately obvious pattern . Although occlusion appears to be erratic at first glance , its behavior can be explained by the fact that changing x ( t ) to 0 causes h ( t ) to be 0 , which the LSTM interprets as the initial state of the FSA ; thus , R ( c ) t ( X ) \u0338 = 0 precisely when X t+1 : , : is classified differently from X. In all cases , the heatmaps for the FSA - based network diverge significantly from the expected heatmaps .", "entities": [[175, 176, "MethodName", "heatmap"], [181, 182, "DatasetName", "0"], [277, 278, "DatasetName", "0"], [285, 286, "DatasetName", "0"], [289, 290, "MethodName", "LSTM"], [311, 312, "DatasetName", "0"]]}
{"text": "Topic information as a crucial auxiliary for text understanding has drawn great attention in recent decades ( Wu et al , 2019 ; Sahlgren , 2020 ) . In the literature , previous studies on topic modeling usually extract topics by introducing latent variables for tokens for topic assigning ( Hofmann , 1999 ; Blei et al , 2003 ; Yishu et al , 2017 ) . Similarly , researches on text - tilling achieve topic segments through lexical cohesion modeling ( Hearst , 1997 ; Purver et al , 2006 ) . Instead of lexical cohesion measuring , Rahimi et al ( 2015 ) put their attention on evaluating the organization and cohesion of pieces of evidence and build topic chains on related text units . Besides , recent studies on argument mining explore to build links or clusters for topic - dependent arguments ( Wachsmuth et al , 2018 ; Shnarch et al , 2018 ; Reimers et al , 2019 ) . Obviously , more and more researches show that there are certain structures among topic segments that deserve deeper exploration . In this work , we aim to explore the cohesion of topic - related text segments . Different from Rahimi et al ( 2015 ) , we show great interest in uncovering how fine - grain topics emerge , evolve , and disappear in an article , which is referred to as discourselevel topic chain ( DTC ) parsing . Since the DTC structure can provide relatively rich and low - noise information about certain topic aspects of articles , it is meaningful for various NLP tasks like summarization ( Perez - Beltrachini et al , 2019 ) , document similarity measuring ( Gong et al , 2018 ) , and response generation ( Dziri et al , 2019 ) . In the literature , topic detection and tracking ( TDT ) ( Allan , 2002 ) is a research area most similar to DTC parsing which aims at identifying new events and tracking how they change over time . However , the events in the TDT task refer to happenings at certain places and times which only compose a small subset of general topics . Recently , Xi and Zhou ( 2017 ) manually annotate the first Chinese DTC corpus based on the theme - rheme theory ( Halliday and Matthiessen , 2004 ) . By contrast , due to the lack of corpus , previous study on English DTC parsing usually uses unsupervised methods ( Kim and Oh , 2011 ) to explore the structure and trends of important topics hidden within news articles . Obviously , one intractable problem facing DTC parsing is the lack of data . This research is primarily motivated by ( Polanyi and Scha , 1984 ; Kim and Oh , 2011 ) on the topic chain concept , ( Xi and Zhou , 2017 ) on DTC corpus construction , and ( Reimers et al , 2019 ) on topic - dependent argument linking . And our contributions mainly include two aspects : ( i ) building an English corpus of discourse - level topic chain ( EDTC ) through a two - step annotation method and ( ii ) lunching a simple but robust Bert - based baseline system for automatic DTC parsing . Moreover , as implied in recent researches on discourse rhetorical structure ( DRS ) parsing ( Zhang et al , 2020 ; Kobayashi et al , 2021 ; Zhang et al , 2021 ) DTC structures for the 385 Wall Street Journal ( WSJ ) articles in the RST - DT corpus aiming to build a bridge between discourse rhetorical structure and DTC structure for discourse researchers to utilize .", "entities": [[132, 134, "TaskName", "argument mining"], [273, 274, "TaskName", "summarization"], [296, 298, "TaskName", "response generation"], [606, 609, "DatasetName", "RST - DT"]]}
{"text": "A Chinese saying about Shakespeare is that \" There are a thousand Hamlets in a thousand people 's eyes \" . From the above annotation process we find that one intractable problem of DTC annotation is the high subjective differences between annotators . More precisely , judging whether the temporary TE evolves from the previous one is really a very subjective problem , and it is hard to make a strict regulation for the annotators . In this case , we tackle the issue from two aspects : ( i ) using a well pretrained topic model to assist manual annotation in a two - step fashion and ( ii ) calculating the confidence scores of the annotations for data filtrating . Two - Step Annotation : The two - step method consists of two phases : first automatically building topic links between topic - related DTUs 2 and then manually refining the automatic annotations for DTC structures . As depicted in Figure 2 , each DTU is preceded by an index pair ( i , j ) according to which u - i and u - j are connected through a topic link . And u - i is an ending unit when j equals - 1 . The solid arcs in the example refer to the topic links generated in the first stage . On this basis , we bring in an auxiliary marker to refine the chain structures where \" \u00d7 \" means that the initial topic arcs ( either machine - labeled or manually labeled links ) are unreasonable and should be deleted directly , and \" = \" means that the original arcs should be replaced with more proper topic links predicted by the human annotators , e.g. , the dashed arcs in the example . In this way , we can dynamically optimize the DTC structures during the human annotation process thus determining the most relevant DTUs for annotation . Our statistics show that around 37.4 % of the automatic annotations are retained in the corpus and 62.6 % of them are invalid and re - annotated by our annotators . According to this , although there is a great dissimilarity between automatic and manually annotated structures , the topic links of the pre - trained model do provide a good 2 Recently , Reimers et al ( 2019 ) use superior contextualized language models for argument linking , which has proven to have great capabilities in aggregating arguments for unseen topics ( https://github.com/UKPLab ) . To improve the reliability of the initial chains , we only keep the topic links with topic similarity higher than 0.9 in the first stage . reference for better annotation consistency . Annotation Confidence : As stated before , considering the problem of subjective difference , it 's really challenging to build a topic link between two DTUs because we 're not sure if they 're the most relevant . Although it is hard to strictly regulate the annotators ' subjectivities , it is feasible to calculate the reliability of each annotation item . Therefore , we aim to ensure the quality of the corpus by filtering out the annotations with low confidence scores . Specifically , given the annotation results of the pre - trained topic model , ( \u03c4 , \u03b9 ) , and that of three annotators , ( \u03c4 , \u03bd ) , ( \u03c4 , \u03b9 ) , and ( \u03c4 , \u03bd ) , on the DTU \u03c4 , we set the confidence of the pre - trained topic model to 0.5 and that of human annotators to 1 , then the confidence score of each annotation on \u03c4 can be calculated as : ( \u03c4 , \u03b9 ) ( 0.5 + 1 ) /3.5 , ( \u03c4 , \u03bd ) 2/3.5 . Based on the results , the annotation ( \u03c4 , \u03bd ) with the highest confidence score of 0.57 is determined as the result . Following this way , we can greatly alleviate the \" subjectivity \" problem by retaining annotations with high confidence . According to our statistics , the averaged confidence score of each DTU annotation is around 0.73 . Data Details . The annotated corpus contains 385 news articles ( 7962 DTUs ) from RST - DT . We annotate 4122 topic links corresponding to 1757 topic chains in the corpus , and the chain length distribution is presented in Table 1 . Obviously , the distribution of chain langths is uneven and most chains have less than 5 topic arcs . For supervised learning , we have divided the dataset into three parts ( the test corpus is consist with that of RST - DT ) , as shown in Table 2 . Based on the test corpus , we calculate the annotation consistency with an averaged Cohen 's kappa value of 0.72 . Concretely , we compare three groups of manual annotations on DTUs with each other for kappa value calculation and report the average score . The data and codes are published at https://github . com / NLP - Discourse - SoochowU / DTCP .", "entities": [[166, 167, "DatasetName", "DTU"], [584, 585, "DatasetName", "DTU"], [697, 698, "DatasetName", "DTU"], [718, 721, "DatasetName", "RST - DT"], [787, 790, "DatasetName", "RST - DT"]]}
{"text": "In this research , we explored how fine - grain topics emerge , evolve , and disappear within an article . To address the lack of data , we built an English DTC corpus through a two - step annotation method , and filtered out the annotations with low confidence scores to ensure the high reliability of the corpus . During annotation , we found that each annotated topic chain does provide relatively low - noise information about a certain aspect of the article and the complete DTC structure can well describe the overall vein of topics in an article . With this in mind , we introduced a simple and robust baseline system , and the parsing model we trained can be straightforwardly harnessed in downstream topic - sensitive NLP tasks to boost performance . It is worth mentioning that we annotated the WSJ articles in the RST - DT corpus also aim to allow the discourse researchers to explore the potential correlation between RST - and DTC - style discourse analysis in future work . Inc. said it downgraded its rating to B - 2 from Ba - 3 on less than $ 20 million of this thrift 's senior subordinated notes . [ u7 ] The rating concern said Franklin 's \" troubled diversification record in the securities business \" was one reason for the downgrade , citing the troubles at its L.F. Rothschild subsidiary and the possible sale of other subsidiaries . \" They perhaps had concern that we were getting out of all these , \" said Franklin President Duane H. Hall . \" I think it was a little premature on their part . \" wsj_2375 u1 u2 u3 u4 u5 u6 u7 u1 u2 u3 u4 u5 u6 u7 u8 u9 u10 u11 u12 u13 [ u7 ] MedChem said the court 's ruling was issued as part of a \" firstphase trial \" in the patent - infringement proceedings and concerns only one of its defenses in the case . [ u8 ] It said it is considering \" all of its options in light of the decision , including a possible appeal . \" The medical - products company added that it plans to \" assert its other defenses \" against Pharmacia 's lawsuit , including the claim that it has n't infringed on Pharmacia 's patent . [ u9 ] MedChem said that the court scheduled a conference for next Mondayto set a date for proceedings on Pharmacia 's motion for a preliminary injunction . wsj_2336", "entities": [[148, 151, "DatasetName", "RST - DT"]]}
{"text": "XMU Neural Machine Translation Systems for WMT 17", "entities": [[2, 4, "TaskName", "Machine Translation"]]}
{"text": "This paper describes the Neural Machine Translation systems of Xiamen University for the translation tasks of WMT 17 . Our systems are based on the Encoder - Decoder framework with attention . We participated in three directions of shared news translation tasks : English German and Chinese\u2194English . We experimented with deep architectures , different segmentation models , synthetic training data and targetbidirectional translation models . Experiments show that all methods can give substantial improvements .", "entities": [[5, 7, "TaskName", "Machine Translation"]]}
{"text": "Neural Machine Translation ( NMT ) ( Cho et al , 2014 ; Bahdanau et al , 2015 ) has achieved great success in recent years and obtained state - of - the - art results on various language pairs ( Zhou et al , 2016 ; Sennrich et al , 2016a ; Wu et al , 2016 ) . This paper describes the NMT systems of Xiamen University ( XMU ) for the WMT 17 . We participated in three directions of shared news translation tasks : English German and Chinese\u2194English . We use two different NMTs for shared news translation tasks : MININMT : A deep NMT system ( Zhou et al , 2016 ; Wu et al , 2016 ; Wang et al , 2017 ) with a simple architecture . The decoder is a stacked Long Short - Term Memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) with 8 layers . The encoder has two variants . For English - German translation , we use an interleaved bidirectional encoder with 2 columns . Each column consists of 4 LSTMs . For Chinese - English translation , we use a stacked bidirectional encoder with 8 layers . DL4MT : Our reimplementation of dl4mttutorial 1 with minor changes . We also use a modified version of AmuNMT C++ decoder 2 for decoding . This system is used in the English - Chinese translation task . We use both Byte Pair Encoding ( BPE ) ( Sennrich et al , 2016c ) and mixed word / character segmentation ( Wu et al , 2016 ) to achieve open - vocabulary translation . Back - translation method ( Sennrich et al , 2016b ) is applied to make use of monolingual data . We also use target - bidiretional translation models to alleviate the label bias problem ( Lafferty et al , 2001 ) . The remainder of this paper is organized as follows : Section 2 describes the architecture of MIN - INMT . Section 3 describes all experimental features used in WMT 17 shared translation tasks . Section 4 shows the results of our experiments . Section 5 shows the results of shared translation task . Finally , we conclude in section 6 .", "entities": [[1, 3, "TaskName", "Machine Translation"], [139, 144, "MethodName", "Long Short - Term Memory"], [145, 146, "MethodName", "LSTM"], [243, 246, "MethodName", "Byte Pair Encoding"], [247, 248, "MethodName", "BPE"]]}
{"text": "Deep architectures have recently shown promising results on various language pairs ( Zhou et al , 2016 ; Wu et al , 2016 ; Wang et al , 2017 ) . We also experimented with a deep architecture as depicted in Figure 1 . We use LSTM as the main recurrent unit and residual connections ( He et al , 2016 ) to help training . Given a source sentence x = { x 1 , . . . , x S } and a target sentence y = { y 1 , . . . , y T } , the encoder maps the source sentence x into a sequence of annotation vectors { x i } . ( Zhou et al , 2016 ) and GNMT ( Wu et al , 2016 ) . Both the encoder and decoder adopt LSTM as its main recurrent unit . We also use residual connections ( He et al , 2016 ) to help training , but here we omit it for clarity . We use black lines to denote input connections while use blue lines to denote recurrent connections . translation y t given the source annotation vectors { x i } and target history y < t .", "entities": [[46, 47, "MethodName", "LSTM"], [142, 143, "MethodName", "LSTM"]]}
{"text": "The interleaved bidirectional encoder was introduced by ( Zhou et al , 2016 ) , which is also used in ( Wang et al , 2017 ) . Like ( Zhou et al , 2016 ) , our interleaved bidirectional encoder consists of two columns . In interleaved bidirectional encoder , the LSTMs in adjacent layers run in opposite directions : \u2212 x i t = LSTM f i ( \u2212 x i\u22121 t , \u2212 s i t+ ( \u22121 ) i ) ( 1 ) \u2212 x i t = LSTM b i ( \u2212 x i\u22121 t , \u2212 s i t+ ( \u22121 ) i+1 ) ( 2 ) Here x 0 t R e is the word embedding of word x t , x i t R h is the output of LSTM unit and s i t = ( c i t , m i t ) denotes the memory and hidden state of LSTM . We set both e and h to 512 in all our experiments . The annotation vectors x i R 2h are obtained by concatenating the final output \u2212 x Lenc and \u2212 x Lenc of two encoder columns . In our experiments , we set L enc = 4 .", "entities": [[66, 67, "MethodName", "LSTM"], [92, 93, "MethodName", "LSTM"], [115, 116, "DatasetName", "0"], [137, 138, "MethodName", "LSTM"], [160, 161, "MethodName", "LSTM"]]}
{"text": "To better exploit source representation , we adopt a stacked bidirectional encoder . As shown in Figure 1 , all layers in the encoder are bidirectional . The calculation is described as follows : \u2212 x i = LSTM f i ( x i\u22121 t , \u2212 s i t\u22121 ) ( 3 ) \u2212 x i = LSTM b i ( x i\u22121 t , \u2212 s i t+1 ) ( 4 ) x i = [ \u2212 x i T ; \u2212 x i T ] T ( 5 ) To reduce parameters , we reduce the dimension of hidden units from h to h/2 so that x i R h . The annotation vectors are taken from the output x Lenc of top LSTM layer . In our experiments , L enc is set to 8 .", "entities": [[38, 39, "MethodName", "LSTM"], [58, 59, "MethodName", "LSTM"], [126, 127, "MethodName", "LSTM"]]}
{"text": "To enable open - vocabulary , we use two approaches : BPE and mixed word / character segmentation . In most of our experiments , we use BPE 3 ( Sennrich et al , 2016c ) with 50 K operations . In our preliminary experiments , we found that BPE works better than UNK replacement techniques . For English - Chinese translation task , we apply mixed word / character model ( Wu et al , 2016 ) to Chinese sentences . We keep the most frequent 50 K words and split other words into characters . Unlike ( Wu et al , 2016 ) , we do not add any prefixes or suffixes to the segmented Chinese characters . In post - processing step , we simply remove all the spaces .", "entities": [[11, 12, "MethodName", "BPE"], [27, 28, "MethodName", "BPE"], [49, 50, "MethodName", "BPE"]]}
{"text": "We describe XMU 's neural machine translation systems for the WMT 17 shared news translation tasks . All our models perform quite well on all tasks we participated . Experiments also show the effectiveness of all features we used .", "entities": [[5, 7, "TaskName", "machine translation"]]}
{"text": "Rigid Formats Controlled Text Generation", "entities": [[3, 5, "TaskName", "Text Generation"]]}
{"text": "Neural text generation has made tremendous progress in various tasks . One common characteristic of most of the tasks is that the texts are not restricted to some rigid formats when generating . However , we may confront some special text paradigms such as Lyrics ( assume the music score is given ) , Sonnet , SongCi ( classical Chinese poetry of the Song dynasty ) , etc . The typical characteristics of these texts are in three folds : ( 1 ) They must comply fully with the rigid predefined formats . ( 2 ) They must obey some rhyming schemes . ( 3 ) Although they are restricted to some formats , the sentence integrity must be guaranteed . To the best of our knowledge , text generation based on the predefined rigid formats has not been well investigated . Therefore , we propose a simple and elegant framework named SongNet to tackle this problem . The backbone of the framework is a Transformer - based auto - regressive language model . Sets of symbols are tailor - designed to improve the modeling performance especially on format , rhyme , and sentence integrity . We improve the attention mechanism to impel the model to capture some future information on the format . A pre - training and fine - tuning framework is designed to further improve the generation quality . Extensive experiments conducted on two collected corpora demonstrate that our proposed framework generates significantly better results in terms of both automatic metrics and the human evaluation . 1", "entities": [[1, 3, "TaskName", "text generation"], [128, 130, "TaskName", "text generation"], [152, 153, "MethodName", "SongNet"], [165, 166, "MethodName", "Transformer"]]}
{"text": "Recent years have seen the tremendous progress in the area of natural language generation especially benefiting by the neural network models such as Recurrent Neural Networks ( RNN ) or Convolutional Neural Networks ( CNN ) based sequence - tosequence ( seq2seq ) frameworks ( Bahdanau et al , 1 Code : http://github.com/lipiji/SongNet Let me not to the marriage of true minds Admit impediments , love is not love", "entities": [[41, 42, "MethodName", "seq2seq"]]}
{"text": "We conduct all the experiments on two collected corpus with different literary genres : SongCi and Sonnet , in Chinese and English respectively . The statistic number are shown in Table 3 . We can see that Sonnet is in small size since we only utilize the samples from the Shakespeare 's Sonnets ( Shakespeare , 2000 ) . Since SongCi and Sonnet are in different languages , thus we conduct the pre - training procedure on two large scale corpus in the corresponding languages respectively . For Chinese , we collect Chinese Wikipedia ( 1700 M Characters ) and a merged Chinese News ( 9200 M Characters ) corpus from the Internet . We did not conduct the word segmenting operations on the Chinese datasets , which means that we just use the characters to build the vocabulary , and the size is 27681 . For English , same as BERT , we employ English Wikipedia ( 2400 M words ) and BooksCorpus ( 980 M words ) ( Zhu et al , 2015 ) to conduct the pre - training . We did not use BPE operation ( Sennrich et al , 2015 ) on this corpus considering the format controlling purpose . We keep the most frequent 50 , 000 words to build the vocabulary .", "entities": [[151, 152, "MethodName", "BERT"], [187, 188, "MethodName", "BPE"]]}
{"text": "Sequence - to - sequence framework with attention mechanism . We regard the format and rhyme symbols C as the input sequence , and the target as the output sequence . GPT2 We fine - tune the GPT2 models ( the pretraining versions are used for sentence integrity evaluation ) on SongCi and Sonnet respectively . SongNet Out proposed framework with both the per - training and fine - tuning stages . We also conduct ablation analysis to verify the performance of the defined symbols as well as the variants of model structures . SongNet ( only pre - tuning ) Without the finetuning stage . SongNet ( only fine - tuning ) Without the pretraining stage . SongNet - GRU Employ GRU to replace Transformer as the core structure .", "entities": [[56, 57, "MethodName", "SongNet"], [94, 95, "MethodName", "SongNet"], [106, 107, "MethodName", "SongNet"], [118, 119, "MethodName", "SongNet"], [120, 121, "MethodName", "GRU"], [122, 123, "MethodName", "GRU"], [125, 126, "MethodName", "Transformer"]]}
{"text": "SongNet - SongCi CiPai : Zhe Gu Tian , Format : 7 . 7 . 7 , 7 . 3 , 3 . 7 . what lies , for when you are not that , \\ no one in this and that can see me lies ! Table 5 : Cases of the generated results for SongCi and Sonnet respectively . For SongCi , the number in Format ( e.g. , 3 , 5 , 7 ) denotes the number of tokens in one sentence . The rhyming words are labeled in red color and italic font following is the Pinyin . ( Since cases are provided to confirm the format consistency , thus we did not conduct translation for the Chinese samples . Translation for Chinese poetry is also a challenging task . )", "entities": [[0, 1, "MethodName", "SongNet"], [124, 125, "TaskName", "Translation"]]}
{"text": "SongNet - SongCi CiPai : Bu Suan Zi , Format : 5 , 5 . 7 , 5 . 5 , 5 . 7 , 5 . though all thy love with thy hearts , thou still are lacking of my dead ; if thy love love is lost to your love and parts , and yet mine own heart can be buried . so many are ill or in tear , hath not this time that we will make their eye , for that which lies not well hath now appear , no longer nor the world that holds thee lie ! for if it would be buried in my live , or by the earth of mine was gone , then my own parts as my body and mine give , may not be so far beyond thine alone : so far as thee and this world view find thee , then mine life be far enough from all thee and no me . Format C _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ( 1 ) ( 2 ) Format C _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ ( 1 ) ( 2 ) SongNet - Sonnet _ _ _ _ Table 6 : Cases of the generated results given the formats with partial pre - defined content . Format token \" \" needs to be translated to real word token .", "entities": [[0, 1, "MethodName", "SongNet"], [255, 256, "MethodName", "SongNet"]]}
{"text": "For human evaluation , we just conduct the judging on the results generated by our final model SongNet . From the result we can observe that the results on corpus SongCi is much better than the ones on corpus Sonnet , which is because the corpus scale is different . And the the small scale also lead to dramatically dropping on all the metrics .", "entities": [[17, 18, "MethodName", "SongNet"]]}
{"text": "Table 5 depicts several generated cases for SongCi and Sonnet respectively . For SongCi , the formats ( CiPai ) are all cold - start samples which are not in the training set or even newly defined . Our model can still generate high quality results on the aspects of format , rhyme as well as integrity . However , for corpus Sonnet , even though the model can generate 14 lines text , the quality is not as good as SongCi due to the insufficient training - set ( only 100 samples ) . We will address this interesting and challenging few - shot issue in the future . In addition , we mentioned that our model has the ability of refining and polishing given the format C which contains some fixed text information . The examples of the generated results under this setting are shown in Table 6 , which show that our model SongNet can generate satisfying results especially on SongCi .", "entities": [[156, 157, "MethodName", "SongNet"]]}
{"text": "We propose to tackle a challenging task called rigid formats controlled text generation . A pre - training and fine - tuning framework SongNet is designed to address the problem . Sets of symbols are tailordesigned to improve the modeling performance for format , rhyme , and sentence integrity . Extensive experiments conducted on two collected corpora demonstrate that our framework generates significantly better results in terms of both automatic metrics and human evaluations given arbitrary cold start formats .", "entities": [[11, 13, "TaskName", "text generation"], [23, 24, "MethodName", "SongNet"]]}
{"text": "In this paper , we report on the shared task on metaphor identification on VU Amsterdam Metaphor Corpus and on a subset of the TOEFL Native Language Identification Corpus . The shared task was conducted as apart of the ACL 2020 Workshop on Processing Figurative Language .", "entities": [[25, 28, "TaskName", "Native Language Identification"]]}
{"text": "Over the last decade , automated detection of metaphor has become an popular topic , which manifests itself in both a variety of approaches and in an increasing variety of data to which the methods are applied . In terms of methods , approaches based on feature - engineering in a supervised machine learning paradigm explored features based on concreteness and imageability , semantic classification using WordNet , FrameNet , VerbNet , SUMO ontology , property norms , and distributional semantic models , syntactic dependency patterns , sensorial and vision - based features K\u00f6per and i m Walde , 2017 ; Tekiroglu et al , 2015 ; Tsvetkov et al , 2014 ; Beigman Klebanov et al , 2014 ; Dunn , 2013 ; Neuman et al , 2013 ; Mohler et al , 2013 ; Hovy et al , 2013 ; Tsvetkov et al , 2013 ; Turney et al , 2011 ; Shutova et al , 2010 ; Gedigian et al , 2006 ) ; see and Veale et al ( 2016 ) for reviews of supervised as well as semi - supervised and unsupervised approaches . Recently , deep learning methods have been explored for token - level metaphor detection ( Mao et al , 2019 ; Dankers et al , 2019 ; Gao et al , 2018 ; Wu et al , 2018 ; Rei et al , 2017 ; Gutierrez et al , 2017 ; Do Dinh and Gurevych , 2016 ) . In terms of data , researchers used specially constructed or selected sets , such as adjective noun pairs Tsvetkov et al , 2014 ) , WordNet synsets and glosses ( Mohammad et al , 2016 ) , annotated lexical items ( from a range of word classes ) in sentences sampled from corpora ( \u00d6zbal et al , 2016 ; Jang et al , 2015 ; Hovy et al , 2013 ; Birke and Sarkar , 2006 ) , all the way to annotation of all words in running text for metaphoricity Steen et al , 2010 ) ; Veale et al ( 2016 ) review various annotated datasets . The goal of this shared task is to detect , at the word level , all content word metaphors in a given text . We are using two datasets - VUA and TOEFL , to be described shortly . There are two tracks for each dataset , for a total of four tracks : VUA All POS , VUA Verbs , TOEFL All POS , and TOEFL Verbs . The AllPOS track is concerned with the detection of all content words , i.e. , nouns , verbs , adverbs and adjectives that are labeled as metaphorical while the Verbs track is concerned only with verbs that are metaphorical . We excluded all forms of be , do , and have for both tracks . For each dataset , each participating individual or team can elect to compete in the All POS track , Verbs track , or both . The competition is organized into two phases : training and testing .", "entities": [[68, 69, "DatasetName", "FrameNet"], [73, 74, "MethodName", "ontology"]]}
{"text": "We make available to shared task participants a number of features from prior published work on metaphor detection , including unigram features , features based on WordNet , VerbNet , and those derived from a distributional semantic model , POS - based , concreteness and difference in concreteness , as well as topic models . We adopted three informed baselines from prior work . As Baseline 1 : UL + WordNet + CCDB , we use the best system from Beigman . The features are : lemmatized unigrams , generalized WordNet semantic classes , and difference in concreteness ratings between verbs / adjectives and nouns ( UL + WN + CCDB ) . 6 Baseline 2 : bot.zen is one of the top - ranked systems in the first metaphor shared task in 2018 by Stemle and Onysko ( 2018 ) that uses a bi - directional recursive neural network architecture with long - term short - term memory ( LSTM BiRNN ) and implements a flat sequenceto - sequence neural network with one hidden layer using TensorFlow and Keras in Python . The system uses fastText word embeddings from different corpora , including learner corpus and BNC data . Finally , Baseline 3 : BERT is constructed by finetuning the BERT model ( Devlin et al , 2018 ) in a standard token classification task : After obtaining the contextualized embeddings of a sentence , we apply a linear layer followed by softmax on each token to predict whether it is metaphorical or not . gives more details about the architecture of this baseline . For Verbs tracks , we tune the system on All POS data and test on Verbs , as this produced better results during preliminary experimentation than training on Verbs only . Zenith : Character embeddings + Similarity Networks + Bi - LSTM + Transformer Kumar and Sharma ( 2020 ) added lexical and orthographic information via character embeddings in addition to GloVe and ELMo embeddings for an enriched input representation . The authors also constructed a similarity metric between the literal and contextual representations of a word as another input component . A Bi - LSTM network and Transformer network are trained independently and combined in an ensemble . Eventually , adding both character - based information and similarity network are the most helpful , as evidenced by results obtained using cross - validation on the training datasets . rowanhm : Static and contextual embeddings + concreteness + Multi - layer Perceptron Maudslay et al ( 2020 ) created a system that combines the concreteness of a word , its static embedding and its contextual embedding before providing them as inputs into a deep Multi - layer Perceptron network which predicts word metaphoricity . Specifically , the concreteness value of a word is formulated as a linear interpolation between two reference vectors ( concrete and abstract ) which were randomly initialized and learned from data . iiegn : LSTM BiRNN + metadata ; combine TOEFL and VUA data Stemle and Onysko ( 2020 ) used an LSTM BiRNN classifier to study the relationship between the metadata in the TOEFL corpus ( proficiency , L1 of the author , and the prompt to which the essay is responding ) and classifier performance . The system is an extension of the authors ' system for the 2018 shared task ( Stemle and Onysko , 2018 ) that served as one of the baseline in the current shared task ( see section 4.1 ) . Analyzing the training data , the authors observed that essays written by more proficient users had significantly more metaphors , and that essays responding to some of the prompts had significantly more metaphors than other prompts ; however , using proficiency and prompt metadata explicitly in the classifier did not improve performance . The authors also experimented with combining VUA and TOEFL data . Duke Data Science : BERT , XNET language models + POS tags as features for a Bi - LSTM classifier Liu et al ( 2020 ) use pre - trained BERT and XLNet language models to create contextualized embeddings , which are combined with POS tags to generate features for a Bi - LSTM for token - level metaphor classification . For the testing phase , the authros used an ensemble strategy , training four copies of the Bi - LSTM with different initializations and averaging their predictions . To increase the likelihood of prediction of a metaphor label , a token is declared a metaphor if : ( 1 ) its predicted probability is higher than the threshold , or ( 2 ) if its probability is three orders of magnitude higher than the median predicted probability for that word in the evaluation set . chasingkangaroos : RNN + BiLSTM + Attention + Ensemble Brooks and Youssef ( 2020 ) use an ensemble of RNN models with Bi - LSTMs and bidirectional attention mechanisms . Each word was represented by an 11 - gram and appeared at the center of the 11 - gram ; each word in the 11 - gram was represented by a 1 , 324 dimensional word embedding ( concatenation of ELMo and GloVe embeddings ) . The authors experimented with ensembles of models that implement somewhat different architecture ( in terms of attention ) and models trained on all POS and on a specific POS . baseline system ( also one of the shared task baselines , see section 4.1 ) uses BERT - after obtaining the contextualized embeddings of a sentence , a linear layer is applied followed by softmax on each token to predict whether it is metaphorical or not . The authors spell - correct the TOEFL data , which improves performance . present two multi - task settings : In the first , metaphor detection on out - of - domain data is treated as an auxiliary task ; in the second , idiom detection on in - domain data is the auxiliary task . Performance on TOEFL is helped by the first multi - task setting ; performance on VUA is helped by the second .", "entities": [[52, 54, "TaskName", "topic models"], [160, 161, "MethodName", "LSTM"], [186, 187, "MethodName", "fastText"], [187, 189, "TaskName", "word embeddings"], [205, 206, "MethodName", "BERT"], [211, 212, "MethodName", "BERT"], [223, 225, "TaskName", "token classification"], [239, 241, "MethodName", "linear layer"], [243, 244, "MethodName", "softmax"], [307, 308, "MethodName", "LSTM"], [309, 310, "MethodName", "Transformer"], [310, 311, "DatasetName", "Kumar"], [327, 328, "MethodName", "GloVe"], [329, 330, "MethodName", "ELMo"], [361, 362, "MethodName", "LSTM"], [364, 365, "MethodName", "Transformer"], [494, 495, "MethodName", "LSTM"], [512, 513, "MethodName", "LSTM"], [656, 657, "MethodName", "BERT"], [670, 671, "MethodName", "LSTM"], [682, 683, "MethodName", "BERT"], [684, 685, "MethodName", "XLNet"], [705, 706, "MethodName", "LSTM"], [732, 733, "MethodName", "LSTM"], [802, 803, "MethodName", "BiLSTM"], [868, 869, "MethodName", "ELMo"], [870, 872, "MethodName", "GloVe embeddings"], [920, 921, "MethodName", "BERT"], [932, 934, "MethodName", "linear layer"], [938, 939, "MethodName", "softmax"]]}
{"text": "UoB team : Bi - LSTM + GloVe embeddings + concreteness Alnafesah et al ( 2020 ) explore ways of using concreteness information in a neural metaphor detection context . GloVe embeddings are used as features to an SVM classifier to learn concreteness values , training it using human labels of concreteness . Then , for metaphor detection , every input word is represented as a 304 - dimensional vector - 300 dimensions are GloVe pre - trained embeddings , plus probabilities for the four concreteness classes . These representations of words are given as input to a Bi - LSTM which outputs a sequence of labels . Results suggest that explicit concreteness information helps improve metaphor detection , relative to a baseline that uses GloVe embeddings only . zhengchang : ALBERT + BiLSTM Li et al ( 2020 ) use a sequence labeling model based on ALBERT - LSTM - Softmax . Embeddings produced by BERT serve as input to BiLSTM , as well as to the final softmax layer . The authors report on experiments with inputs to BERT ( single - sentence vs pairs ; variants using BERT tokenization ) , spellcorrection of the TOEFL data , and CRF vs softmax at the classification layer . PolyU - LLT : Sensorimotor and embodiment features + embeddings + n - grams + logistic regression classifier Wan et al ( 2020 ) use sensorimotor and embodiment features . They use the Lancaster Sensorimotor norms ( Lynott et al , 2019 ) that include measures of sensorimotor strength for about 40 K English words across six perceptual modalities ( e.g. , touch , hearing , smell ) , and five action effectors ( mouth / throat , hand / arm , etc ) , and embodiment norms from Sidhu et al ( 2014 ) . The authors also use word , lemma , and POS n - grams ; word2vec and GloVe word embeddings , as well as cosine distance measurements using the embeddings . The different features are combined using logistic regression and other classifiers .", "entities": [[5, 6, "MethodName", "LSTM"], [7, 9, "MethodName", "GloVe embeddings"], [30, 32, "MethodName", "GloVe embeddings"], [38, 39, "MethodName", "SVM"], [74, 75, "MethodName", "GloVe"], [100, 101, "MethodName", "LSTM"], [125, 127, "MethodName", "GloVe embeddings"], [131, 132, "MethodName", "ALBERT"], [133, 134, "MethodName", "BiLSTM"], [147, 148, "MethodName", "ALBERT"], [149, 150, "MethodName", "LSTM"], [151, 152, "MethodName", "Softmax"], [156, 157, "MethodName", "BERT"], [161, 162, "MethodName", "BiLSTM"], [169, 170, "MethodName", "softmax"], [180, 181, "MethodName", "BERT"], [190, 191, "MethodName", "BERT"], [201, 202, "MethodName", "CRF"], [203, 204, "MethodName", "softmax"], [224, 226, "MethodName", "logistic regression"], [311, 312, "DatasetName", "lemma"], [321, 322, "MethodName", "GloVe"], [322, 324, "TaskName", "word embeddings"], [341, 343, "MethodName", "logistic regression"]]}
{"text": "Interactive Query - Assisted Summarization via Deep Reinforcement Learning", "entities": [[4, 5, "TaskName", "Summarization"]]}
{"text": "Interactive summarization is a task that facilitates user - guided exploration of information within a document set . While one would like to employ state of the art neural models to improve the quality of interactive summarization , many such technologies can not ingest the full document set or can not operate at sufficient speed for interactivity . To that end , we propose two novel deep reinforcement learning models for the task that address , respectively , the subtask of summarizing salient information that adheres to user queries , and the subtask of listing suggested queries to assist users throughout their exploration . 1 In particular , our models allow encoding the interactive session state and history to refrain from redundancy . Together , these models compose a state of the art solution that addresses all of the task requirements . We compare our solution to a recent interactive summarization system , and show through an experimental study involving real users that our models are able to improve informativeness while preserving positive user experience .", "entities": [[1, 2, "TaskName", "summarization"], [36, 37, "TaskName", "summarization"], [150, 151, "TaskName", "summarization"]]}
{"text": "Integrating human interaction into NLP tasks has been gaining the interest of the NLP community . Human - machine cooperation can improve the general quality of results , as well as provide a higher sense of control for the targeted consumer . We focus on the task of interactive summarization ( INTSUMM : Shapira et al , 2021b ) which enables information exploration within a document set on a topic , by means of user - guided summarization . As illustrated in Figure 1 , a user can incrementally expand on a summary by submitting requests to the system , in order to expose the information of interest within the topic . A proper exploration session demands access to all information within the document set , and fast reaction time for smooth human Figure 1 : An INTSUMM system , ingesting a large document set . A user interactively submits queries in order to expand on the information . The system is required to process the full document set for comprehensive exploration , respond quickly , and expose nonredundant salient information that also complies to the input queries . See real example in Figure 5 . engagement ( Anderson , 2020 ; Attig et al , 2017 ) . In addition , presented information must consider the session history to refrain from repetitiveness . While it is worthwhile to apply recent NLP advances that excel at extracting salient and querybiased information , those advances usually come at a cost of rather small input size limits or heavy computation time . Indeed , all previous interactive summarization systems we know of either apply traditional methods or are inadequate for real - time processing due to high latency ( 2 ) . Our goal is to overcome these obstacles , and leverage advanced methods to improve information exposure while keeping latency acceptable for interaction . As depicted in Figure 1 , an INTSUMM system provides an initial generic summary as an overview of the topic , after which a user can iteratively issue queries to the system for summary expansions on subtopics of interest . To support querying , the system offers a list of suggested queries , hinting at information concealed within the document set . We address the INTSUMM task components through two subtasks : ( 1 ) generating the initial summary and query responses , and ( 2 ) generating lists of suggested queries . For each of the subtasks we propose a deep reinforcement learning ( RL ) algorithm that addresses the respective sub - task requirements . To enable comprehensive topic exploration , our models speedily process the full document set , as inspired by . Additionally , they are able to peek at session history to comply to the current state of the interaction . The model for the query - assisted summarization subtask , M Summ , incorporates the query sequence by ( 1 ) encoding a query into the contextual sentence representations , ( 2 ) attending the representations using a new query - biased variant of the maximal marginal relevance ( MMR : Carbonell and Goldstein , 1998 ) function , and ( 3 ) a dual reward mechanism for policy optimization ( Pasunuru and Bansal , 2018 ) which we adapt to consider both reference summaries and the query ( 3 ) . The model for the suggested queries list generation subtask , M Sugg , works at the phrase level , as opposed to the sentence level , to enable extraction of important phrases that serve as suggested queries . Similarly to M Summ , the model learns importance with consideration to session history , but without an input query - as its role is to suggest such a query ( 4 ) . The models are trained on the DUC 2 2007 multidocument summarization ( MDS ) news - domain dataset , with adaptions for our task setting . For testing , we follow the INTSUMM evaluation framework of Shapira et al ( 2021b ) to run simulations , collect real user sessions , and assess the results , using DUC 2006 . In principle , summary informativeness , i.e. general salience , could potentially come at the expense of query responsiveness , but importantly , our results show that our RL - based solution is able to significantly improve information exposure over the baseline of Shapira et al ( 2021b ) , without compromising user experience ( 5 ) .", "entities": [[49, 50, "TaskName", "summarization"], [77, 78, "TaskName", "summarization"], [265, 266, "TaskName", "summarization"], [476, 477, "TaskName", "summarization"], [643, 644, "TaskName", "summarization"], [690, 692, "DatasetName", "DUC 2006"]]}
{"text": "Interactive summarization facilitates user - guided information navigation within document sets . The task suffered from a lack of a methodological evaluation , until Shapira et al ( 2021b ) formalized the INTSUMM task with a framework consisting of a benchmark , evaluation metrics , a session collection process and baseline systems . This framework , that we leverage , enables comparison and analysis of systems , allowing principled research on the task and accelerated development of algorithms . To the best of our knowledge , all previous works on INTSUMM have either applied more traditional text - processing methods or require costly prepro - cessing of inputs to facilitate seamless interaction . Leuski et al ( 2003 ) used surface - form features for processing content , and Baumel et al ( 2014 ) adapted classic MDS algorithms like LexRank ( Erkan and Radev , 2004 ) and KLSum ( Haghighi and Vanderwende , 2009 ) . Christensen et al ( 2014 ) optimized discourse graphs and Shapira et al ( 2017 ) relied on a knowledge representation , both expensively pre - generating hierarchical summaries that limit expansions to pre - prepared information selections . Hirsch et al ( 2021 ) applied advanced coreference resolution algorithms that take several hours for preprocessing a document set . The two INTSUMM baseline systems of Shapira et al ( 2021b ) use sentence clustering or TextRank ( Mihalcea and Tarau , 2004 ) for summarization , sentence similarity heuristics for query - responses , and n - gram frequency or TextRank for suggested query extraction . Moreover , their query - response generators strictly consider a given query , ignoring history or global informativeness . Our proposed algorithms significantly improve information exposure over the latter baselines , using advanced deep RL methods , working in real time . We next review some recent techniques in MDS , query - focused summarization and multi - document keyphrase extraction , all of which relate to the INTSUMM task and our choice of algorithms . The subtask of query - assisted summarization . Non - interactive MDS has been researched extensively , with few recent neural - based methods that can handle relatively large inputs . For example , Wang et al ( 2020 ) use graph neural networks to globally score sentence salience , Xiao et al ( 2021 ) summarize using Longformers ( Beltagy et al , 2020 ) , and combine a Longformer with BART ( Lewis et al , 2020 ) and incorporate graphical representation of information . apply deep RL for autoregressive sentence selection , and , in contrast to most other neural methods , can ingest the full document set . In the query - focused summarization ( QFS ) task summaries are biased on a query . To accommodate a query , use conditional selfattention to enforce dependency of the query on source words . Pasunuru et al ( 2021a ) and Kulkarni et al ( 2021 ) hierarchically encode a query with the documents . These and other QFS methods require large training sets , and limit the allowed input size ( Baumel et al , 2018 ; Laskar et al , 2020 ) . Relatedly , incremental update summarization ( Mc - Creadie et al , 2014 ; Lin et al , 2017 ) marks queryrelevant information as reported texts stream in , avoiding repeating information marked earlier . Interactivity is not a constraining factor here , yielding solutions with relatively high computation time . With respect to the above related work , we develop a model inspired by , which is closest to our requirements . To facilitate an interactive setting , our model ( 1 ) enables query+history injection , ( 2 ) supports full input processing , necessary for complete information availability during exploration , ( 3 ) has low latency at inference time , and ( 4 ) requires a relatively small training set . The subtask of suggested - queries list generation . Extracting suggested queries on a document set most resembles the multi - document keyphrase extraction ( MDKE ) task since it aims to identify salient keyphrases ( Shapira et al , 2021a ) . MDKE was mostly addressed using traditional heuristics or graph - centrality algorithms applied over the documents ( e.g. Mihalcea and Tarau , 2004 ; Florescu and Caragea , 2017 ) . In contrast to MDKE , the suggested queries extraction subtask is a new paradigm that updates \" keyphrases \" with respect to session history . While previous methods for keyphrase extraction could potentially be adapted for our dynamic setting , we choose to focus in this work on a deep RL architecture for suggested queries that resonates our model for query - assisted summarization and allows sharing insights between the models .", "entities": [[1, 2, "TaskName", "summarization"], [205, 207, "TaskName", "coreference resolution"], [243, 244, "TaskName", "summarization"], [319, 320, "TaskName", "summarization"], [324, 326, "TaskName", "keyphrase extraction"], [347, 348, "TaskName", "summarization"], [411, 412, "MethodName", "Longformer"], [413, 414, "MethodName", "BART"], [458, 459, "TaskName", "summarization"], [543, 544, "TaskName", "summarization"], [686, 688, "TaskName", "keyphrase extraction"], [767, 769, "TaskName", "keyphrase extraction"], [801, 802, "TaskName", "summarization"]]}
{"text": "The subtask of query - assisted summarization covers two main components of the INTSUMM task : the generators of an initial summary and of queryresponses . The initial summary concisely specifies some central issues from the input topic ( not biased on a query ) to initiate the user 's understanding of the topic and to motivate further exploration . Then , for each user submitted query , the query - response generator non - redundantly expands on the previously presented information with topically salient responses that are also biased around the query . We next formally define the subtask and then describe our RL model for it .", "entities": [[6, 7, "TaskName", "summarization"]]}
{"text": "The input to the query - assisted summarization subtask is tuple ( D , q , E in , m ) , such that : D is a document set on a topic where the j - th sentence in the concatenation of D 's documents is denoted s j ; q is a query , and can be empty ( denoted _ ) for an unbiased generic summary ; E in = { e in 1 , ... , e in k } is a sequence of sentences from D termed the history , containing texts previously output in the session ; and m is the number of sentences to output . The output is sentence sequence E out = { e out 1 , ... , e out m } from D ( extractive summarization ) . When inputting ( D , _ , { } , m ) , the output is a generic summary of m sentences , that can serve as the initial summary ; and when q and E in are not empty , the output is an expansion on E in in response to q , containing new salient information biased on q. D is paired with a set of generic reference summaries R , which is used for training or as a part of the evaluation effort .", "entities": [[7, 8, "TaskName", "summarization"], [135, 137, "TaskName", "extractive summarization"]]}
{"text": "Pre - training . To provide a warm start for training M Summ , a reduced version of M Summ is first pre - trained for generic extractive single - document summarization using the large - scale CNN / Daily Mail corpus ( Hermann et al , 2015 ) , as proposed by Chen and Bansal ( 2018 ) For each topic , we generate an \" oracle \" extractive summary by greedily aggregating 10 sentences from D , that maximizes the ROUGE \u2206 - 1 recall against R. Then for each sentence , we extract a bi - or trigram that is most lexically - unique to the sentence , in comparison to all other sentences in D. This yields a sequence of 10 \" queries \" that could easily render the corresponding oracle summary . The intuition for this approach is that it would teach M Summ that it is worthwhile to consider a given query when selecting a sentence that is informative with respect to the reference summaries . This further assists in fulfilling the dual requirements of selecting a globally informative sentence that also adheres to the query . 4 Appendix B.3 discusses usage of different query types for training . Validation metric . As the interactive session progresses , a recall curve emerges , that maps the ROUGE recall score ( here ROUGE - 1 ) versus the expanding summary token - length . Once the session halts , the area under the curve indicates the efficacy of the session for information exposure . A higher value implies faster unveiling of salient information . Normalizing by the final summary length allows approximate comparability between different length sessions . We hence use the average ( over topics ) length - normalized area under the recall curve for validating the training progress .", "entities": [[30, 32, "TaskName", "document summarization"], [37, 41, "DatasetName", "CNN / Daily Mail"]]}
{"text": "The INTSUMM task involves human users by definition . Nevertheless , running on simulated query lists and session histories is pertinent for efficient system evaluation and comparison of methods . To simulate the query - assisted summarization algorithms , we utilize the real sessions recorded by Shapira et al ( 2021b ) : 3 - 4 user sessions on 20 topics from DUC 2006 collected with S 2 . In our simulation , each summary - so - far from a recorded session is fed as input to the system together with the following recorded user query . We then measure R recall 1\u2206 ( difference of ROUGE - 1 recall incurred by the query response compared with the input summary - so - far ) . Additionally , we use R F 1 1 ( ROUGE - 1 F 1 ) for initial summary informativeness . Both are measured w.r.t . the reference summaries , normalized by the output length , and averaged per session recording , and then over all sessions and topics , to get an overall system infor - mativeness score . We also measure system queryresponsiveness using the QSIM metric . Table 1 presents a representative partial ablation of the M Summ model . All variants were configured to output sentences of up to 30 tokens , initial summaries are 75 tokens , and query responses are 2 sentences . Configurations i - iv use the query in training , while v and vi do not . Each configuration is measured for informativeness ( columns marked with \u2020 ) , and for query - responsiveness ( QSIM column ) . Out of configurations i - iv , config . i , where we employ all mechanisms for query inclusion , yields the best overall scores in both informativeness and query - responsiveness , despite the inherent tradeoff between the two . In the second set of configurations ( v - vi ) , we observe that ignoring the query at train time substantially degrades queryresponsiveness , and this is expectedly further exacerbated when also ignoring the query at inference time . However , disregarding the query gives more informative expansions with respect to reference summaries , since the model was trained only to optimize content informativeness , and is less likely to sidetrack to the query - related information . Compared to S 2 ( last row ) , our model significantly improves informativeness . Queryresponsiveness is better in the S 2 baseline since its query - response generator simply invokes a function similar to QSIM , but for the price of lower informativeness . Still , this does not lead to inferior overall user experience , see 5.3 .", "entities": [[36, 37, "TaskName", "summarization"], [62, 64, "DatasetName", "DUC 2006"]]}
{"text": "Interactive summarization for information exploration is a task that requires compliance to user requests and session history , while comprehensively handling a large input document set . These requirements pose a challenge for advanced text processing methods due to the need for fast reaction time . We present novel deep reinforcement learning based algorithms that answer to the task requirements , improving salient information exposure while satisfying user queries and keeping user experience positive . We note that while M Summ is designed for the INTSUMM task , it may potentially be serviceable for standard MDS , QFS , update summarization and combinations thereof . This can be accommodated by a proper choice of input , e.g. , QFS can be addressed by giving M Summ as input a query , an empty history and target summary length . In future work , we may study the performance of our solutions for such tasks , as well as strive to further improve their performance on both ends of the INTSUMM task - selecting topically salient information and responding to user queries .", "entities": [[1, 2, "TaskName", "summarization"], [100, 101, "TaskName", "summarization"]]}
{"text": "We extracted all noun - phrases from the document set by first mapping all tokens to their part - of - speech tags , and then applying a regularexpression chunker with regex : { ( < JJ > * < NN . * > + < IN > ) ? < JJ > * < NN . * > + } . These steps were accomplished with NLTK . Phrase length . There is no limit set on the phrase length . We tried training and inferring with a phrase length constraint of 4 words , but found that this gave worse results overall . History sentences to phrases . M Sugg works on the phrase level . Meanwhile , in our extractive interactive setting , the history is a set of sentences already presented to the reader . Therefore , when extracting phrases from D , we also link each phrase to its source sentence , and obtain E in by compiling the phrases linked from the history sentences .", "entities": [[17, 20, "DatasetName", "part - of"]]}
{"text": "While DUC 2006 ( our test set ) and 2007 ( our train / validation set ) were originally designed for the query - focused summarization task , they contain excessive topic concentration due to their long and descriptive topic queries ( Baumel et al , 2016 ) . Hence , their reference summaries can practically be considered generic . 9 https://spacy.io/", "entities": [[1, 3, "DatasetName", "DUC 2006"], [25, 26, "TaskName", "summarization"]]}
{"text": "Modern Natural Language Processing ( NLP ) models are known to be sensitive to input perturbations and their performance can decrease when applied to real - world , noisy data . However , it is still unclear why models are less robust to some perturbations than others . In this work , we test the hypothesis that the extent to which a model is affected by an unseen textual perturbation ( robustness ) can be explained by the learnability of the perturbation ( defined as how well the model learns to identify the perturbation with a small amount of evidence ) . We further give a causal justification for the learnability metric . We conduct extensive experiments with four prominent NLP models - TextRNN , BERT , RoBERTa and XLNetover eight types of textual perturbations on three datasets . We show that a model which is better at identifying a perturbation ( higher learnability ) becomes worse at ignoring such a perturbation at test time ( lower robustness ) , providing empirical support for our hypothesis .", "entities": [[125, 126, "MethodName", "BERT"], [127, 128, "MethodName", "RoBERTa"]]}
{"text": "Despite the success of deep neural models on many Natural Language Processing ( NLP ) tasks ( Liu et al , 2016 ; Devlin et al , 2019 ; Liu et al , 2019b ) , recent work has discovered that these models are not robust to noisy input from the real world and thus their performance will decrease ( Prabhakaran et al , 2019 ; Niu et al , 2020 ; Ribeiro et al , 2020 ; Moradi and Samwald , 2021 ) . A reliable NLP system should not be easily fooled by slight noise in the text . Although a wide range of evaluation approaches for robust NLP models have been proposed ( Ribeiro et al , 2020 ; Morris et al , 2020 ; Goel et al , 2021 ; , few attempts have been made to understand these benchmark results . Given the difference of robustness between models and perturbations , it is a natural question why models are more sensitive to some perturbations than others . It is crucial to avoid over - sensitivity to input perturbations , and understanding why it happens is useful for revealing the weaknesses of current models and designing more robust training methods . To the best of our knowledge , a quantitative measure to interpret the robustness of NLP models to textual perturbations has yet to be proposed . To improve the robustness under perturbation , it is common practice to leverage data augmentation ( Li and Specia , 2019 ; Min et al , 2020 ; Tan and Joty , 2021 ) . Similarly , how much data augmentation through the perturbation improves model robustness varies between models and perturbations . In this work , we aim to investigate two Research Questions ( RQ ) : RQ1 : Why are NLP models less robust to some perturbations than others ? RQ2 : Why does data augmentation work better at improving the model robustness to some perturbations than others ? We test a hypothesis for RQ1 that the extent to which a model is affected by an unseen textual perturbation ( robustness ) can be explained by the learnability of the perturbation ( defined as how well the model learns to identify the perturbation with a small amount of evidence ) . We also validate another hypothesis for RQ2 that the learnability metric is predictive of the improvement on robust performance brought by data augmentation along a perturbation . Our proposed learnability is inspired by the concepts of Randomized Controlled Trial ( RCT ) and Average Treatment Effect ( ATE ) from Causal Inference ( Rubin , 1974 ; Holland , 1986 ) . Estimation of perturbation learnability for a model consists of three steps : \u2460 randomly labelling a dataset , \u2461 perturbing examples of a particular pseudo class with probabilities , and \u2462 using ATE to measure the ease with which the model learns the perturbation . The core intuition for our method is to frame an RCT as a perturbation identification task and formalize the notion of learnability Exp No . Measurement", "entities": [[244, 246, "TaskName", "data augmentation"], [270, 272, "TaskName", "data augmentation"], [317, 319, "TaskName", "data augmentation"], [405, 407, "TaskName", "data augmentation"], [434, 436, "MethodName", "Causal Inference"]]}
{"text": "Perturbation Training Examples Test Examples 0 Standard original l ( x i , 0 ) , ( x j , 1 ) ( x i , 0 ) , ( x j , 1 ) 1 Robustness original l { 0 , 1 } ( x i , 0 ) , ( x j , 1 ) ( x * i , 0 ) , ( x * j , 1 ) 2 Data Augmentation original l { 0 , 1 } ( x i , 0 ) , ( x j , 1 ) ( x * i , 0 ) , ( x * j , 1 ) ( x * i , 0 ) , ( x * j , 1 ) 3 Learnability random l \u2032 { 1 \u2032 } ( x j , 0 \u2032 ) , ( x * i , 1 \u2032 ) ( x * i , 1 \u2032 ) 4 random l \u2032 { 1 \u2032 } ( x j , 0 \u2032 ) , ( x * i , 1 \u2032 ) ( x i , 1 \u2032 ) Table 1 : Example experiment settings for measuring learnability , robustness and improvement by data augmentation . We perturb an example if its label falls in the set of label ( s ) in \" Perturbation \" column . means no perturbation at all . Training / test examples are the expected input data , assuming we have only one negative ( x i , 0 ) and positive ( x j , 1 ) example in our original training / test set . l \u2032 is a random label and x * is a perturbed example . as a causal estimand based on ATE . We conduct extensive experiments on four neural NLP models with eight different perturbations across three datasets and find strong evidence for our two hypotheses . Combining these two findings , we further show that data augmentation is only more effective at improving robustness against perturbations that a model is more sensitive to , contributing to the interpretation of robustness and data augmentation . Learnability provides a clean setup for analysis of the model behaviour under perturbation , which contributes better model interpretation as well . Contribution . This work provides an empirical explanation for why NLP models are less robust to some perturbations than others . The key to this question is perturbation learnability , which is grounded in the causality framework . We show a statistically significant inverse correlation between learnability and robustness .", "entities": [[5, 6, "DatasetName", "0"], [13, 14, "DatasetName", "0"], [26, 27, "DatasetName", "0"], [40, 41, "DatasetName", "0"], [48, 49, "DatasetName", "0"], [62, 63, "DatasetName", "0"], [73, 75, "TaskName", "Data Augmentation"], [78, 79, "DatasetName", "0"], [86, 87, "DatasetName", "0"], [100, 101, "DatasetName", "0"], [115, 116, "DatasetName", "0"], [138, 139, "DatasetName", "0"], [170, 171, "DatasetName", "0"], [203, 205, "TaskName", "data augmentation"], [254, 255, "DatasetName", "0"], [329, 331, "TaskName", "data augmentation"], [355, 357, "TaskName", "data augmentation"]]}
{"text": "With the above - defined terminologies , we propose hypotheses for RQ1 and RQ2 in Section 1 , respectively . Hypothesis 1 ( H1 ) : A model for which a perturbation is more learnable is less robust against the same perturbation at the test time . This is not obvious because the model encounters this perturbation during training in learnability estimation while they do not in robustness measurement . Hypothesis 2 ( H2 ) : A model for which a perturbation is more learnable experiences bigger robustness gains with data augmentation along such a perturbation . We validate both Hypotheses 1 and 2 with experiments on several perturbations and models described in Section 4.1 and 4.2 .", "entities": [[90, 92, "TaskName", "data augmentation"]]}
{"text": "In Section 2.1 , we introduce the term \" learnability \" in an intuitive way . Now we map it to a formal , quantitative measure in standard statistical frameworks . Learnability is actually motivated by concepts from the causality literature . We provide a brief introduction to basic concepts of causal inference in Appendix B. In fact , learnability is the causal effect of perturbation on models , which is often difficult to measure due to the confounding latent features . In the language of causality , this is \" correlation is not causation \" . Causality provides insight on how to fully decouple the effect of perturbation and other latent features . We introduce the causal motivations for step 2.1 and 2.1 of learnability estimation in the following Section 3.1 and 3.2 , respectively .", "entities": [[51, 53, "MethodName", "causal inference"]]}
{"text": "To test the learnability , robustness and improvement by data augmentation with different NLP models and perturbations , we experiment with four modern and representative neural NLP models : TextRNN ( Liu et al , 2016 ) , BERT ( Devlin et al , 2019 ) , RoBERTa ( Liu et al , 2019b ) and XLNet ( Yang et al , 2019 ) . For TextRNN , we use the implementation by an open - source text classification toolkit NeuralClassifier ( Liu et al , 2019a ) . For the other three pretrained models , we use the bert - base - cased , roberta - base , xlnet - base - cased versions from Hugging Face ( Wolf et al , 2020 ) , respectively . These two platforms support most of the common NLP models , thus facilitating extension studies of more models in future . We use three common binary text classification datasets - IMDB movie reviews ( IMDB ) ( Pang and Lee , 2005 ) , Yelp polarity reviews ( YELP ) ( Zhang et al , 2015 ) , Quora Question Pair ( QQP ) ( Iyer et al , 2017 ) - as our testbeds . IMDB and YELP datasets present the task of sentiment analysis , where each sentence is labelled as positive or negative sentiment . QQP is a paraphrase detection task , where each pair of sentences is marked as semantically equivalent or not . To control the effect of dataset size and imbalanced classes , all datasets are randomly subsampled to the same size as IMDB ( 50k ) with balanced classes . The training steps for all experiments are the same as well . We implement perturbations g ( \u22c5 ) with two self - designed ones and six selected ones from the NL - Augmenter library ( Dhole et al , 2021 ) . For perturbation probabilities , we choose 0.001 , 0.005 , 0.01 , 0.02 , 0.05 , 0.10 , 0.50 , 1.00 . We run all experiments across three random seeds and report the average results .", "entities": [[9, 11, "TaskName", "data augmentation"], [38, 39, "MethodName", "BERT"], [47, 48, "MethodName", "RoBERTa"], [56, 57, "MethodName", "XLNet"], [77, 79, "TaskName", "text classification"], [109, 110, "MethodName", "xlnet"], [154, 156, "TaskName", "text classification"], [158, 161, "DatasetName", "IMDB movie reviews"], [162, 163, "DatasetName", "IMDB"], [190, 191, "DatasetName", "QQP"], [204, 205, "DatasetName", "IMDB"], [212, 214, "TaskName", "sentiment analysis"], [226, 227, "DatasetName", "QQP"], [267, 268, "DatasetName", "IMDB"], [347, 348, "DatasetName", "seeds"]]}
{"text": "Figure 3 shows learnability as a function of perturbation probability . Learnability @ p generally increases as we increase the perturbation probability , and when we perturb all the examples ( i.e. , p = 1.0 ) , every model can easily identify it well , resulting in the maximum learnability of 1.0 . This shows that neural NLP models master these perturbations eventually . At lower perturbation probabilities , some models still learn that perturbation alone predicts the label . In fact , the major difference between different p \u2212 learnability curves is the area of lower perturbation probabilities and this provides motivation for using log AU C instead of AU C as the summarization of learnability at different p ( Section 2.1 ) . Table 2 shows the average learnability over all perturbation probabilities of each modelperturbation pair on IMDB dataset in Figure 3 . 4 It reveals the most learnable perturbation for each model . For example , the learnability of \" vi - sual_attack_letters \" and \" leet_letters \" are very high for all four models , likely due to their strong effects on the tokenization process ( Salesky et al , 2021 ) . Perturbations like \" white_space_perturbation \" and \" duplicate_punctuations \" are less learnable for pretrained models , probably because they have weaker effects on the subword level tokenization , or they may have encountered similar noise in the pretraining corpora . We observe that \" dupli - cate_punctuations \" already exists in the original text of YELP dataset ( e.g. , \" The burgers are awesome ! ! \" ) , thus violating our assumptions for perturbations in Section 4.1 . As a result , the curve for 4 Please refer to Appendix E for benchmark results on YELP ( Table 5 ) and QQP ( this perturbation substantially deviates from others in Figure 3 . We do not count this perturbation on YELP dataset in the following analysis . The perturbation learnability experiments provide a clean setup for NLP practitioners to analyze the effect of textual perturbations on models .", "entities": [[115, 116, "TaskName", "summarization"], [141, 142, "DatasetName", "IMDB"], [302, 303, "DatasetName", "QQP"]]}
{"text": "We observe a negative correlation between learnability ( Equation 4 ) and robustness ( Equation 1 ) across all three datasets in Table 2 , validating Hypothesis 1 . Table 2 also quantifies the trend that data augmentation with a perturbation the model is less robust to has more improvement on robustness ( Hypothesis 2 ) . We plot the correlations on IMDB dataset in Figure 4a and 4b . 5 Both the correlations between 1 ) learnability vs. robustness and 2 ) learnability vs. improvement by data augmentation are strong ( Spearman | \u03c1 | > 0.6 ) and highly significant ( p - value < 0.001 ) , which firmly supports our hypotheses . Our findings provide insight about when the model is less robust and when data augmentation works better for improving robustness . Figure 4c shows that the more learnable a perturbation is for a model , the greater the likelihood that its robustness can be improved through data augmentation along this perturbation . We argue that this is not simply because there is more room for improvement by data augmentation . From a causal perspective , learnability acts as a common cause ( confounder ) for both robustness and improvement by data augmentation . This indicates a potential limitation of using data augmentation for improving robustness to perturbations : data augmentation is only more effective at improving robustness against perturbations more learnable for a model .", "entities": [[36, 38, "TaskName", "data augmentation"], [62, 63, "DatasetName", "IMDB"], [87, 89, "TaskName", "data augmentation"], [129, 131, "TaskName", "data augmentation"], [162, 164, "TaskName", "data augmentation"], [183, 185, "TaskName", "data augmentation"], [206, 208, "TaskName", "data augmentation"], [216, 218, "TaskName", "data augmentation"], [224, 226, "TaskName", "data augmentation"]]}
{"text": "Robustness of NLP Models to Perturbations . The performance of NLP models can decrease when encountering noisy data in the real world . Recent works ( Prabhakaran et al , 2019 ; Ribeiro et al , 2020 ; Niu et al , 2020 ; Moradi and Samwald , 2021 ) present comprehensive evaluations of the robustness of NLP models to different types of perturbations , including typos , changed entities , negation , etc . Their results reveal the phenomenon that NLP models can handle some specific types of perturbation more effectively than others . However , they do not go into a deeper analysis of the reason behind the difference of robustness between models and perturbations . Interpretation of Data Augmentation . Although data augmentation has been widely used in CV ( Sato et al , 2015 ; DeVries and Taylor , 2017 ; Dwibedi et al , 2017 ) and NLP ( Wang and Yang , 2015 ; Kobayashi , 2018 ; Wei and Zou , 2019 ) , the underlying mechanism of its effectiveness remains under - researched . Recent studies aim to quantify intuitions of how data augmentation improves model generalization . Gontijo - Lopes et al ( 2020 ) introduce affinity and diversity , and find a correlation between the two metrics and augmentation performance in image classification . In NLP , Kashefi and Hwa ( 2020 ) propose a KL - divergence - based metric to predict augmentation performance . Our proposed learnability metric implies when data augmentation works better and thus acts as a complement to this line of research .", "entities": [[120, 122, "TaskName", "Data Augmentation"], [124, 126, "TaskName", "data augmentation"], [190, 192, "TaskName", "data augmentation"], [221, 223, "TaskName", "image classification"], [252, 254, "TaskName", "data augmentation"]]}
{"text": "The aim of causal inference is to investigate how a treatment T affects the outcome Y . Confounder X refers to a variable that influences both treatment T and outcome Y . For example , sleeping with shoes on ( T ) is strongly associated with waking up with a headache ( Y ) , but they both have a common cause : drinking the night before ( X ) ( Neal , 2020 ) . In our work , we aim to study how a perturbation ( treatment ) affects the model 's prediction ( outcome ) . However , the latent features and other noise usually act as confounders . Causality offers solutions for two questions : 1 ) how to eliminate the spurious association and isolate the treatment 's causal effect ; and 2 ) how varying T affects Y , given both variables are causallyrelated . We leverage both of these properties in our proposed method . Let us now introduce Randomized Controlled Trial and Average Treatment Effect as key concepts in answering the above two questions , respectively . Randomized Controlled Trial ( RCT ) . In an RCT , each participant is randomly assigned to either the treatment group or the non - treatment group . In this way , the only difference between the two groups is the treatment they receive . Randomized experiments ideally guarantee that there is no confounding factor , and thus any observed association is actually causal . We operationalize RCT as a perturbation classification task in Section 3.1 . Average Treatment Effect ( ATE ) . In Section 3.2 , we apply ATE ( Holland , 1986 ) as a measure of learnability . ATE is based on Individual Treatment Effect ( ITE , Equation 9 ) , which is the difference of the outcome with and without treatment . IT E i = Y i ( 1 ) \u2212 Y i ( 0 ) . ( 9 ) Here , Y i ( 1 ) is the outcome Y of individual i that receives treatment ( T = 1 ) , while Y i ( 0 ) is the opposite . In the above example , waking up with a headache ( Y = 1 ) with shoes on ( T = 1 ) means Y i ( 1 ) = 1 . We calculate the Average Treatment Effect ( ATE ) by taking an average over ITEs : AT E = E [ Y ( 1 ) ] \u2212 E [ Y ( 0 ) ] . ( 10 ) ATE quantifies how the outcome Y is expected to change if we modify the treatment T from 0 to 1 . We provide specific definitions of ITE and ATE in Section 3.2 .", "entities": [[3, 5, "MethodName", "causal inference"], [325, 326, "DatasetName", "0"], [358, 359, "DatasetName", "0"], [427, 428, "DatasetName", "0"], [451, 452, "DatasetName", "0"]]}
{"text": "Explanation Graph Generation via Pre - trained Language Models : An Empirical Study with Contrastive Learning", "entities": [[1, 3, "TaskName", "Graph Generation"], [14, 16, "MethodName", "Contrastive Learning"]]}
{"text": "Graph Generation from Language Models . Representative works on graph generation from language models include knowledge graph completion models like Comet Hwang et al , 2021 ) that fine - tune GPT ( Radford et al , 2019 ; Brown et al , 2020 ) and BART ( Lewis et al , 2020 ) , generation of event influence graphs ( Tandon et al , 2019 ; Madaan et al , 2020 ) , partially ordered scripts ( Sakaguchi et al , 2021 ) , temporal graphs ( Madaan and Yang , 2021 ) , entailment trees , proof graphs ( Saha et al , 2020 ; Saha et al , 2021a ) and commonsense explanation graphs ( Saha et al , 2021b ) . Linguistic tasks like syntactic parsing Mohammadshahi and Henderson , 2021 ; Kondratyuk and Straka , 2019 ) and semantic parsing ( Chen et al , 2020b ; Shin et al , 2021 ) have also made use of language models . There is also a large body of work on building generative models for learning unconditional graph distributions ( You et al , 2018 ; Simonovsky and Komodakis , 2018 ; Grover et al , 2019 ; Liao et al , 2019 ; Shi * et al , 2020 ) without any semantics attached to the graphs . Our novelty lies in presenting the first systematic analysis of structure and semantics of graph generation for two downstream NLP tasks using pre - trained language models and improving them via constrastive learning . Data Augmentation and Contrastive Learning . Data Augmentation for NLP ( Hedderich et al , 2020 ; has been a powerful tool in low - data settings , ranging from its early usages with synonym replacement ( Kolomiyets et al , 2011 ; Wang and Yang , 2015 ) to more recent methods of perturbing hidden representations ( Miyato et al , 2016 ; . Contrastive learning , beyond its historical use in learning robust image representations ( Chopra et al , 2005 ; Hadsell et al , 2006 ; Gutmann and Hyv\u00e4rinen , 2010 ; Hoffer and Ailon , 2015 ; Hjelm et al , 2018 ; Chen et al , 2020a ; He et al , 2020 ) has been explored in supervised scenarios ( Khosla et al , 2020 ; Gunel et al , 2020 ) and for NLP , in training self - supervised language models ( Fang et al , 2020 ) , learning sentence representations ( Gao et al , 2021 ) , document clustering , summarization Cao and Wang , 2021 ) and generic text generation . It has also been used in unconditional graph representation learning ( You et al , 2020 ; Hassani and Khasahmadi , 2020 ; . We follow this rich line of work to explore their applicability in supervised graph generation tasks from pretrained language models in low - resource settings . Generative Commonsense Reasoning . While traditional commonsense reasoning tasks are discriminative in nature ( Zellers et al , 2018 ; Talmor et al , 2019 ; Bisk et al , 2020 ; Sakaguchi et al , 2020 ; Talmor et al , 2021 ) , recent focus on generative evaluation have led to the development of tasks and benchmarks that explore unstructured commonsense sentence generation ( Lin et al , 2020 ) , event influence graph generation ( Madaan et al , 2020 ) , commonsense explanation graph generation ( Saha et al , 2021b ) , etc . We experiment with two graph generation tasks , primarily focusing on ExplaGraphs ( Saha et al , 2021b ) because of the clear distinction in the underlying structural constraints and the semantic aspect dealing with commonsense .", "entities": [[0, 2, "TaskName", "Graph Generation"], [9, 11, "TaskName", "graph generation"], [15, 18, "TaskName", "knowledge graph completion"], [31, 32, "MethodName", "GPT"], [46, 47, "MethodName", "BART"], [143, 145, "TaskName", "semantic parsing"], [237, 239, "TaskName", "graph generation"], [257, 259, "TaskName", "Data Augmentation"], [260, 262, "MethodName", "Contrastive Learning"], [263, 265, "TaskName", "Data Augmentation"], [322, 324, "MethodName", "Contrastive learning"], [429, 430, "TaskName", "summarization"], [438, 440, "TaskName", "text generation"], [448, 451, "TaskName", "graph representation learning"], [478, 480, "TaskName", "graph generation"], [482, 485, "TaskName", "pretrained language models"], [566, 568, "TaskName", "graph generation"], [578, 580, "TaskName", "graph generation"], [594, 596, "TaskName", "graph generation"]]}
{"text": "Most prior works that collect human - annotated graphs for a downstream NLP task have found such collection processes to be quite expensive and tedious ( Tandon et al , 2019 ; Saha et al , 2021b ) . For instance , Saha et al ( 2021b ) obtained high - quality data only after multiple rounds of refinement and employ trained expert annotators for entailment tree construction . The corresponding datasets are also relatively small in size ( 2 - 3k ) , thus limiting the prospect of large - scale training . Hence , our approach towards improving explanation graph generation is through data augmentation techniques that perturb human - curated graphs to construct positive and negative graphs . As noted earlier , we wish to construct graphs that enable better learning of structural graph constraints and their semantics .", "entities": [[101, 103, "TaskName", "graph generation"], [105, 107, "TaskName", "data augmentation"]]}
{"text": "Next we propose different methods of leveraging these positive and negative graphs for explanation graph generation . Our models either use only positive graphs as simple data augmentation , only negative graphs in a max - margin model , or both in a Generate & Refine model and a Contrastive model .", "entities": [[14, 16, "TaskName", "graph generation"], [26, 28, "TaskName", "data augmentation"]]}
{"text": "In this first simple approach , we augment the training data with the synthetically created positive graphs and retrain the baseline T5 model .", "entities": [[21, 22, "MethodName", "T5"]]}
{"text": "ExplaGraphs was constructed using a \" Refinement \" phase wherein the initially constructed graphs that are marked incorrect by human verifiers are further refined by another set of annotators . Here we emulate the graph refinement phase with the help of a model . Specifically , our approach is a 2 - stage pipeline - first , an initial graph is generated by the baseline T5 model and second , an Explanation Graph Refinement model conditions on the initial graph , along with the belief , argument and the stance to refine the graph . The refiner is also a T5 model fine - tuned with the prefix \" Refine the Explanation Graph for \" on all positive and negative graphs described in Sec . 4 . Note that our approach differs from the actual data collection process in two aspects . Unlike the human - annotated graphs , which are refined only for semantic correctness , the model - generated graphs can be both structurally and semantically incorrect . Second , our approach does not involve a graph verification stage and thus , the refiner model acts on all ( correct and incorrect ) graphs generated in stage 1 and is thus trained with both correct and incorrect graphs .", "entities": [[65, 66, "MethodName", "T5"], [100, 101, "MethodName", "T5"]]}
{"text": "Automatically evaluating graphs for semantic correctness is challenging . We conduct human evaluation to further validate our findings . We compare the graphs generated by T5 and our Max - Margin model on Amazon Mechanical Turk where three annotators choose which graph is better or if they are mostly similar ( instructions in Appendix F ) . For fair comparison , we evaluate only those samples where both models predict the correct stance and the graphs are also structurally correct . In fact , this lets us evaluate the semantic aspect in isolation when both graphs are structurally correct . With majority voting on 150 samples , we observe that our Max - Margin model 's graphs are preferred 13 % more times compared to those of the T5 model ( 43 % vs 30 % and statistically significant with p < 0.05 ) while in 22 % cases , the graphs are marked similar ( remaining have no majority ) .", "entities": [[25, 26, "MethodName", "T5"], [128, 129, "MethodName", "T5"]]}
{"text": "In ness and diversity in these graphs and hence are the best candidates for contrastive learning .", "entities": [[14, 16, "MethodName", "contrastive learning"]]}
{"text": "We presented an empirical study of graph structure and semantics for end - to - end explanation graph generation from pre - trained language models and showed that the generated graphs often violate structural constraints or are semantically incorrect . We significantly improve both the structural and semantic accuracy of graph generation by proposing contrastive learning models that leverage simple yet efficient methods of graph perturbations and also generalize to similar graph generation tasks .", "entities": [[17, 19, "TaskName", "graph generation"], [48, 49, "MetricName", "accuracy"], [50, 52, "TaskName", "graph generation"], [54, 56, "MethodName", "contrastive learning"], [71, 73, "TaskName", "graph generation"]]}
{"text": "The task of temporal graph generation requires constructing a temporal graph from a document ( see Fig . 4 ) . The nodes in the graph are events from the 7 : Train , validation and test split sizes of the two datasets . For Temporal Graph Generation , we randomly sample 1.3 % of the overall corpus ( Madaan and Yang , 2021 ) . document ( e.g. , \" Markovic jailed \" or \" Covering up attempted murder \" ) and the edges are temporal relations between the events ( e.g. , \" Markovic jailed ; before ; Covering up attempted murder \" ) . The authors consider five temporal relations ( \" before \" , \" after \" , \" simultaneous \" , \" is included \" and \" includes \" ) and build an automatically constructed large - scale dataset for the task . Following our overall goal of improving graph generation in limited data settings , we randomly sample 1.3 % of the overall corpus ( \u223c 9.5k samples ) as the training corpus such that all graphs are connected DAGs . 5 Following Madaan and Yang ( 2021 ) , we represent graphs in DOT format ( Koutsofios and North , 1996 ) as shown in Fig . 4 . We find that the specifics of the graph representations do not matter much , as long as all the edges are concatenated in one particular ordering ( either DFS , BFS or Topological order ) . We construct semantic negative graphs by randomly sampling a fraction of the edges and performing the following operations . If an edge relation is one of \" before \" , \" after \" or \" simulatenous \" , we replace it with any other relation from this set and if the relation is one of \" is included \" or \" includes \" we replace it with the other relation . Note that these perturbations will always lead to incorrect graphs because \" A before B \" implies that \" A after B \" or \" A simultaneous B \" do not hold . Finally , we construct positive graphs by randomly sampling a fraction of edges and replacing them using the following rules : ( 1 ) \" A before B \" with \" B after A \" and viseversa , ( 2 ) \" A simultaneous B \" with \" B simultaneous A \" , ( 3 ) \" A includes B \" with \" B is included A \" . Note that all these operations preserve the temporal meaning of the graph and are done in a way such that the perturbed graph continues to be a connected DAG .", "entities": [[4, 6, "TaskName", "graph generation"], [46, 48, "TaskName", "Graph Generation"], [154, 156, "TaskName", "graph generation"]]}
{"text": "Table 8 shows the results of all models on the Ex - plaGraphs ( Saha et al , 2021b ) ( Madaan et al , 2020 ) showing the source document , the target temporal graph and the corresponding DOT representation . Figure 5 : Interface for human evaluation of commonsense explanation graphs . T5 model on the facts based on ConceptNet relations from ATOMIC - 2020 ( Hwang et al , 2021 , a large - scale commonsense knowledge base . The fine - tuning objective is to predict the target concept given the source concept and the relation . Next , we fine - tune this model further on the end - task of graph generation which leads to small improvements in both StCA and SeCA . This suggests that better methods of inducing commonsense knowledge in these models can potentially lead to bigger gains with more semantically coherent graphs .", "entities": [[54, 55, "MethodName", "T5"], [61, 62, "DatasetName", "ConceptNet"], [64, 65, "DatasetName", "ATOMIC"], [116, 118, "TaskName", "graph generation"]]}
{"text": "In Fig . 6 , 7 , 8 and 9 , we show various examples of explanation graphs generated by our models . In Fig . 6 and 7 , our proposed models improve upon Contrastive Graph T5 - generated Graph Semantically Incorrect Figure 7 : Example of explanation graphs generated by different models . The baseline T5 - generated graph is semantically incorrect ( incoherent relations marked in dashed red ) while our proposed models generate both structurally and semantically correct graphs .", "entities": [[37, 38, "MethodName", "T5"], [57, 58, "MethodName", "T5"]]}
{"text": "We thank the reviewers for their helpful feedback and the annotators for their time and effort . This work was supported by DARPA MCS Grant N66001 - 19 - 2 - 4031 , NSF - CAREER Award 1846185 , DARPA YFA17 - D17AP00022 , ONR Grant N00014 - 18 - 1 - 2871 , Microsoft Investigator Fellowship , and Munroe & Rebecca Cobey Fellowship . The views in this article are those of the authors and not the funding agency .", "entities": [[22, 23, "DatasetName", "DARPA"], [39, 40, "DatasetName", "DARPA"]]}
{"text": "the incorrect semantic relations from the T5 baseline graphs . Fig . 8 shows an example where all generated graphs , while different , are correct . Finally , Fig 9 shows an example where although our proposed models improve the semantic aspect compared to the baseline graph , the generated graphs are disconnected and hence structurally incorrect . Overall , our quantitative results and human evaluation suggest that there is significant room for improvement on the task of commonsense explanation graph generation .", "entities": [[6, 7, "MethodName", "T5"], [81, 83, "TaskName", "graph generation"]]}
{"text": "Structurally Incorrect T5 - generated Graph Semantically Incorrect Figure 9 : Example of explanation graphs generated by different models . T5 generates a semantically incorrect graph . Our models generate graphs , which while contain meaningful edges , are disconnected and hence are structurally incorrect .", "entities": [[2, 3, "MethodName", "T5"], [20, 21, "MethodName", "T5"]]}
{"text": "Syntactic analyses and named entity recognition for PubMed and PubMed Central - up - to - the - minute", "entities": [[3, 6, "TaskName", "named entity recognition"]]}
{"text": "Although advanced text mining methods specifically adapted to the biomedical domain are continuously being developed , their applications on large scale have been scarce . One of the main reasons for this is the lack of computational resources and workforce required for processing large text corpora . In this paper we present a publicly available resource distributing preprocessed biomedical literature including sentence splitting , tokenization , part - of - speech tagging , syntactic parses and named entity recognition . The aim of this work is to support the future development of largescale text mining resources by eliminating the time consuming but necessary preprocessing steps . This resource covers the whole of PubMed and PubMed Central Open Access section , currently containing 26 M abstracts and 1.4 M full articles , constituting over 388 M analyzed sentences . The resource is based on a fully automated pipeline , guaranteeing that the distributed data is always up - to - date . The resource is available at https://turkunlp . github.io/pubmed_parses/.", "entities": [[66, 72, "TaskName", "part - of - speech tagging"], [76, 79, "TaskName", "named entity recognition"]]}
{"text": "Due to the rapid growth of biomedical literature , the maintenance of manually curated databases , usually updated following new discoveries published in articles , has become unfeasible . This has led to a significant interest in developing automated text mining methods specifically for the biomedical domain . Various community efforts , mainly in the form of shared tasks , have resulted in steady improvement in biomedical text mining methods ( Kim et al , 2009 ; Segura Bedmar et al , 2013 ) . For instance the GENIA shared tasks focusing on extracting biological events , such as gene regulations , have consistently gathered wide interest and have led to the development of several text mining tools ( Miwa et al , 2012 ; Bj\u00f6rne and Salakoski , 2013 ) . These methods have been also succesfully applied on a large scale and several biomedical text mining databases are publicly available ( Van Landeghem et al , 2013a ; Franceschini et al , 2013 ; M\u00fcller et al , 2004 ) . Although these resources exist , their number does not reflect the vast amount of fundamental research invested in the underlying methods , mainly due to the nontrivial amount of manual labor and computational resources required to process large quantities of textual data . Another issue arising from the challenging text preprocessing is the lack of maintenance of the existing databases which in effect nullifies the purpose of text mining as these resources tend to be almost as much out - of - date as their manually curated counterparts . According to MEDLINE statistics 1 806 , 326 new articles were indexed during 2015 and thus a text mining resource will miss on average 67 thousand articles each month it has n't been updated . In this paper we present a resource aiming to support the development and maintenance of large - scale biomedical text mining . The resource includes all PubMed abstracts as well as full articles from the open access section of PubMed Central ( PMCOA ) , with the fundamental language technology building blocks , such as part - ofspeech ( POS ) tagging and syntactic parses , readily available . In addition , recognition of several bio - logically relevant named entities , such as proteins and chemicals is included . Hence we hope that this resource eliminates the need of the tedious preprocessing involved in utilizing the PubMed data and allows swifter development of new information extraction databases . The resource is constructed with an automated pipeline which provides weekly updates with the latest articles indexed in PubMed and PubMed Central , ensuring the timeliness of the distributed data . All the data is downloadable in an easily handleable XML format , also used by the widely adapted event extraction system TEES ( Bj\u00f6rne and Salakoski , 2015 ) . A detailed description of this format is available on the website .", "entities": [[88, 89, "DatasetName", "GENIA"], [465, 467, "TaskName", "event extraction"]]}
{"text": "In this section , we discuss our processing pipeline as shown in Figure 1 . Firstly , both PubMed and PMCOA documents are downloaded from NCBI FTP services . For the periodical updates of our resource this is done weekly - the same interval the official PMCOA dataset is updated . From the PubMed incremental updates we only include newly added documents and ignore other updates . As the PMCOA does not provide incremental updates , we use the index file and compare it to the previous file list to select new articles for processing . Even though the PubMed and PMCOA documents are provided in slightly different XML formats , they can be processed in similar fashion . As a result , the rest of the pipeline discussed in this section is applied to both document types . Both PubMed XML articles and PMCOA NXML full texts are preprocessed using publicly available tools 2 ( Pyysalo et al , 2013 ) . These tools convert XML documents to plain text and change character encoding from UTF - 8 to ASCII as many of the legacy language processing tools are incapable of handling non - ASCII characters . Additionally , all excess meta data is removed , leaving titles , abstracts and full - text contents for further processing . These documents are subsequently split into sentences using GENIA sentence splitter ( Saetre et al , 2007 ) as most linguistic analyses are done on the sentence level . GENIA sentence splitter is trained on biomedical text ( GENIA corpus ) and has state - of - the - art performance on this domain . The whole data is parsed with the BLLIP constituent parser ( Charniak and Johnson , 2005 ) , using a model adapted for the biomedical domain ( McClosky , 2010 ) , as provided in the TEES processing pipeline . The distributed tokenization and POS tagging are also produced with the parser pipeline . We chose to use this tool as the performance of the TEES software has been previously evaluated on a large - scale together with this parsing pipeline ( Van Landeghem et al , 2013b ) and it should be a reliable choice for biomedical relation extraction . Since dependency parsing has become the prevalent approach in modeling syntactic relations , we also provide conversions to the collapsed Stanford dependency scheme ( De Marneffe et al , 2006 ) . The pipeline is run in parallel on a cluster computer with the input data divided into smaller batches . The size of these batches is altered along the pipeline to adapt to the varying computational requirements of the different tools .", "entities": [[228, 229, "DatasetName", "GENIA"], [249, 250, "DatasetName", "GENIA"], [258, 259, "DatasetName", "GENIA"], [373, 375, "TaskName", "relation extraction"], [377, 379, "TaskName", "dependency parsing"]]}
{"text": "Named entity recognition ( NER ) is one of the fundamental tasks in BioNLP as most of the cru - ( Okazaki , 2007 ) . Having a single tool for this processing step instead of using the various state - of - the - art tools is critical for the maintainability of the processing pipeline . NERsuite was selected as several biological models are readily available for this software ( Kaewphan et al , 2016 ; Pyysalo and Ananiadou , 2014 ) and as it supports label weighting ( Minkov et al , 2006 ) unlike many other NER tools . For cell line names we use a publicly available state - of - the - art model ( Kaewphan et al , 2016 ) , whereas for the other entity types we train our own models with manually annotated data from GENETAG ( Tanabe et al , 2005 ) , CHEMDNER ( Krallinger et al , 2015 ) , SPECIES ( Pafilis et al , 2013 ) and NCBI disease ( Do\u01e7an et al , 2014 ) corpora for GGPs , chemicals , organisms and diseases , respectively . All these corpora are comprised of biomedical articles and should thus reflect well the text types seen in PubMed . All used corpora provide the data divided to training , development and test sets in advance , the 3 http://nersuite.nlplab.org/ SPECIES corpus being an exception . For this corpus we do our own data division with random sampling on document level , for each taxonomy category separately . For each entity type , the C2 value , as well as the label weights are selected to optimize the F - score on the development set . For the training of the final models used in the resource , we use the whole corpora , i.e. the combination of training , development and test sets . Detailed performance evaluations for all entity types are shown in Table 1 . We evaluate NERsuite in terms of precision , recall and F - score against the test data using \" strict matching \" criteria , i.e. only consider the tagged entities correct if they are perfectly matched with the gold standard data . These results may not be directly comparable to the results reported in other studies as relaxed evaluation methods are sometimes used . However , we can conclude that our system is on par with the methods published elsewhere and the limitation of using a single tool does not have a significant negative impact on the overall performance .", "entities": [[0, 3, "TaskName", "Named entity recognition"], [4, 5, "TaskName", "NER"], [99, 100, "TaskName", "NER"], [170, 172, "DatasetName", "NCBI disease"]]}
{"text": "Our future efforts will focus on expanding the coverage of supported entity types to mutations and anatomical entities ( Wei et al , 2013 ; Pyysalo and Ananiadou , 2014 ) , deepening the captured information of biological processes and bringing text mining one step closer to extracting a realistic view of biological knowledge . As many of the NER training corpora include only abstracts and are limited to specific domains , the generalizability of the trained NER models to full articles and to the wide spectrum of topics covered in PubMed is not clear . Thus we wish to assess how well these models perform on largescale datasets and analyze how their performance could be improved on out - of - domain documents . We plan to also include entity normalization for all supported types , but as we wish to minimize the number of individual tools in the processing pipeline , we are developing a generic approach suitable for most entity types .", "entities": [[59, 60, "TaskName", "NER"], [77, 78, "TaskName", "NER"]]}
{"text": "BOUN - ISIK Participation : An Unsupervised Approach for the Named Entity Normalization and Relation Extraction of Bacteria Biotope\u1e61", "entities": [[14, 16, "TaskName", "Relation Extraction"]]}
{"text": "This paper presents our participation at the Bacteria Biotope Task of the BioNLP Shared Task 2019 . Our participation includes two systems for the two subtasks of the Bacteria Biotope Task : the normalization of entities ( BB - norm ) and the identification of the relations between the entities given a biomedical text ( BB - rel ) . For the normalization of entities , we utilized word embeddings and syntactic re - ranking . For the relation extraction task , pre - defined rules are used . Although both approaches are unsupervised , in the sense that they do not need any labeled data , they achieved promising results . Especially , for the BB - norm task , the results have shown that the proposed method performs as good as deep learning based methods , which require labeled data .", "entities": [[68, 70, "TaskName", "word embeddings"], [78, 80, "TaskName", "relation extraction"]]}
{"text": "The amount of electronic resources in the biomedical domain and its rapid growth are major challenges for the scientists who make research in this domain . Text mining methods which aim to automatically extract useful information from the text of these electronic resources provide convenience to the researchers . A number of shared tasks , including the BioNLP Shared Tasks , have been conducted with the goal of developing biomedical text mining methods . In 2011 , the Bacteria Biotope Task has been conducted for the first time as a part of the BioNLP Shared Task targeting the extraction of useful information regarding bacteria and their habitats ( Bossy et al , 2011 ) . Since then , the participant teams of the following shared task series developed various solutions for the problem of bacteria biotopes ( Bossy et al , 2015 ; Deleger et al , 2016 ) . The Bacteria Biotope Task of the BioNLP Shared Task 2019 ( Bossy et al , 2019 ) is the final version of the tasks that have been conducted until now readdressing the problem of extraction of the information regarding the bacteria biotopes . This year 's task has presented the opportunity to the participants to develop solutions for three subproblems : normalization ( BB - norm ) , relation extraction ( BB - rel ) , and knowledge base extraction ( BB - kb ) . For the BB - norm task of the Bacteria Biotope Task of the BioNLP Shared Task 2019 , the participants are expected to develop systems to link the named entities ( Microorganism , Habitat , and Phenotype ) in a given text through a given ontology , when the entities are given with their boundaries . For instance , the sample sentence \" Atypical mycobacteria causing non - pulmonary disease in Queensland . \" consists of the following mentions : \" mycobacteria \" microorganism mention , \" causing non - pulmonary disease \" phenotype mention , and \" pulmonary \" habitat mention , which should be normalized to the \" Mycobacteria \" term in the NCBI taxonomy , and \" human pathogen \" and \" lung \" terms in the Onto - Biotope ontology , respectively . For the BB - rel task of the Bacteria Biotopes Task of the BioNLP Shared Task 2019 , the participants are required to extract the relations between the entities when the entities are given . There are two types of relations : Lives in relation , which indicates a localization relation between a Microorganism entity and a Habitat / Geographical entity , and Exhibits relation , which indicates a property relation between a Phenotype entity and a Microorganism entity . For instance , the sample sentence above indicates two relations : a Lives in relation between the \" Mycobacteria \" Microorganism entity and the \" Queensland \" Geographical entity , and an Exhibits relation between the \" Mycobacteria \" Microorganism entity and the \" causing nonpulmonary disease \" Phenotype entity . We participated at the Bacteria Biotope Task in the BioNLP Shared Task 2019 with our system ( named as the BOUN - ISIK system ) and ob - tained promising results in the official evaluation . This paper presents our participating system for two sub - tasks : one for the BB - norm ( Entity Normalization ) sub - task and one for the BB - rel ( Relation Extraction ) sub - task . For the entity normalization sub - task , we utilized word embeddings and syntactic re - ranking to normalize the entities . On the other hand , for the relation extraction sub - task , we proposed a rule - based method . Although both systems are unsupervised , they achieved promising results . For the BB - norm sub - task , the official results of our system achieved state - of - the - art results on the BioNLP Shared Task 2019 Bacteria Biotope task test data set . The results have shown that our unsupervised approach , which does not require labeled data , performs as good as the deep learning based methods , which require labeled data .", "entities": [[218, 220, "TaskName", "relation extraction"], [281, 282, "MethodName", "ontology"], [369, 370, "MethodName", "ontology"], [573, 575, "TaskName", "Relation Extraction"], [590, 592, "TaskName", "word embeddings"], [609, 611, "TaskName", "relation extraction"]]}
{"text": "Several approaches , which consider the extraction of relations between various biomedical entities such as protein / protein ( Giuliano et al , 2006 ; Airola et al , 2008 ; Choi , 2018 ) , drug / drug ( Segura - Bedmar et al , 2011 ; Kim et al , 2015 ) , and gene / disease ( Bravo et al , 2015 ) from biomedical text , have been presented in the literature . Relation extraction in the bacteria biotopes domain has also attracted considerable attention owing to the BioNLP Bacteria Biotope Shared Tasks . Previous work in the bacteria biotopes domain consists of the extraction of relations between bacteria entities and habitat entities ( Localization Relation Extraction ) and of relations between two habitat entities ( Part Of Relation Extraction ) . The participants of the BioNLP Shared Task 2011 , which is the first shared task that addressed the relation extraction task of bacteria biotopes , utilized both machine learning and rule - based approaches for detecting the Localization and Part - of relations among bacteria and habitats ( Bossy et al , 2011 ) . Sub - task 2 of the Bacteria Biotope ( BB ) Task in the BioNLP Shared Task 2013 also gave another opportunity to scientists to address the task of extracting the Localization and Part Of relations in the bacteria biotopes domain . For this subtask , the best F - score ( 42 % ) was obtained by the TEES 2.1 system ( Bj\u00f6rne and Salakoski , 2013 ) , which used support vector machine classification . After the shared task , a new sentence - level cooccurrence approach with an anaphora resolution component in order to handle relations that span multiple sentences has been developed in ( Karadeniz and\u00d6zg\u00fcr , 2015 ) , which resulted in an improved F - score performance of 53 % on Sub - task 2 . In the BioNLP Shared Task 2016 , the VERSE team ( Lever and Jones , 2016 ) achieved the best F - score , which is 56 % , on the relation extraction sub - task of Bacteria Biotopes by utilizing support vector machines . ( NCBI , 2018 ) were provided , while in the test phase , only the entity boundaries and the entity types were given by the task organizers . For the training and development phases of BBrel , document texts with manually annotated Microorganism , Habitat , Phenotype and Geographical entities , as well as the Lives in and Exhibits relations were provided , while in the test phase , document texts annotated only for Microorganism , Habitat , Phenotype and Geographical entities were given . Since our system for the named entity normalization and relation extraction of bacteria biotopes is based on unsupervised approaches and does not require any labeled training data , the errors of the developed system are analyzed on the provided training and the development sets . The test set is used for the evaluation of the performance of the system .", "entities": [[77, 79, "TaskName", "Relation extraction"], [119, 121, "TaskName", "Relation Extraction"], [132, 134, "TaskName", "Relation Extraction"], [154, 156, "TaskName", "relation extraction"], [175, 178, "DatasetName", "Part - of"], [263, 266, "MethodName", "support vector machine"], [331, 332, "MethodName", "VERSE"], [354, 356, "TaskName", "relation extraction"], [463, 465, "TaskName", "relation extraction"]]}
{"text": "In this section of the paper , the utilized methods for the BB - norm task are explained in detail . The BB - norm task includes the normalization of Habitat entities and Phenotype entities in a given set of documents through the Onto - Biotope ontology and the normalization of Microorganism entities through the NCBI Taxonomy . The methods developed for the normalization of the named entities can be categorized into two according to the type of the entities : Habitat and Phenotype Normalization and Microorganism Normalization .", "entities": [[46, 47, "MethodName", "ontology"]]}
{"text": "For the normalization of semantically meaningful entities such as Habitat and Phenotype entities , a two - step approach that we have previously proposed in ( Karadeniz and\u00d6zg\u00fcr , 2019 ) is adapted to this new data set . According to this approach , for the normalization of an entity mention , the top k semantically most similar ontology concepts are found at the first step using the word embedding representations of the entity mention and the ontology concepts . At the second step , these top k semantically most similar concepts are re - ranked according to a similarity metric that utilizes the constituency parses of the entity mention and ontology concept phrases . The resulting most similar ontology concept is assigned as the normalized concept for the corresponding mention . The details of this approach are explained in the following subsections .", "entities": [[58, 59, "MethodName", "ontology"], [77, 78, "MethodName", "ontology"], [111, 112, "MethodName", "ontology"], [119, 120, "MethodName", "ontology"]]}
{"text": "In the pre - processing step , the named entity mentions and the ontology concept names are tokenized , and the stop - words are removed from the mentions and the ontology concept names . The intuition behind the adapted method is that semantically similar words have similar word vectors . Following this intuition , the semantic similarity between named entity mentions and ontology concept terms would be higher for the similar pairs , and lower for the dissimilar pairs , if the words can be converted into a machine processable format such as real - valued vectors . After pre - processing , to convert each word into a real - valued vector , we utilized a pre - trained word embedding model ( Chiu et al , 2016 ) , which has been trained on PubMed by using the Word2Vec tool ( Mikolov et al , 2013 ) . The corresponding word vectors are obtained for each word by using this previously trained model . For the multiword named entity mentions and ontology concept terms , the vector representations are obtained by averaging the real - valued vectors of their composing words .", "entities": [[13, 14, "MethodName", "ontology"], [31, 32, "MethodName", "ontology"], [56, 58, "TaskName", "semantic similarity"], [63, 64, "MethodName", "ontology"], [174, 175, "MethodName", "ontology"]]}
{"text": "After the vector representations are obtained for each entity mention and for each ontology concept term , the semantic similarity between each pair is computed by using the cosine similarity . For each entity mention , the top k most similar ontology concepts are retained as candidates for further processing , i.e. , for syntactic weighting based re - ranking . k is chosen as 5 based on the results obtained in our previous study ( Karadeniz and\u00d6zg\u00fcr , 2019 ) .", "entities": [[13, 14, "MethodName", "ontology"], [18, 20, "TaskName", "semantic similarity"], [41, 42, "MethodName", "ontology"]]}
{"text": "For our re - ranking approach , the assumption is that the entity mentions are noun phrases and the most informative words in the mentions are the heads of the noun phrases . We used the Stanford Parser ( version 3.8.0 ) ( Klein and Manning , 2003 ) to obtain the corresponding head words of the entity mentions by providing the entity mentions as input and extracting the syntactic parses of the mentions as output . Next , the top level rightmost \" noun \" is searched in the tree structured syntactic parse and assigned as the head of the mention phrase . The semantic similarities are recomputed using the mathematical formulation shown in Equation ( 1 ) , which considers also the similarity between the head words of the entity mention and ontology concept pair . In Equation ( 1 ) , S RR ( m , c ) is the final computed similarity between mention m and the candidate concept c , and S S is the semantic similarity , in which m head is the head word of the mention m and c head is the head word of the concept c , S S ( m , c ) is the similarity between mention m and concept c computed as described in Section 3.1.1 , and w is a weighting parameter which can take values between 0 and 1 . w is chosen as 0.25 based on the results reported in our previous study ( Karadeniz and\u00d6zg\u00fcr , 2019 ) .", "entities": [[134, 135, "MethodName", "ontology"], [145, 146, "DatasetName", "RR"], [170, 172, "TaskName", "semantic similarity"], [231, 232, "DatasetName", "0"]]}
{"text": "Our system for the relation extraction sub - task is based on the naive assumption that the related entities for most of the relations appear within the same sentence . Therefore , firstly , the input texts are split into sentences using the NLTK library . For the extraction of Lives in relations , all the sentences in the related document are searched to determine whether there exists a Microorganism entity and a Habitat entity or a Microorganism entity and a Geographical entity in the corresponding sentence . If there exists such a pair , this will be a sign of a Lives in relation . For any given sentence , there can be more than one Habitat entity and Microorganism entity . For this kind of sentences , two different approaches , which are called smart matching and distributed matching , are applied . In smart matching , each Habitat entity is paired with the closest Microorganism entity . In other words , the locations of each type of entities in the sentences are checked , and then the pairing process of the Microorganism and the Habitat entities are done based on the proximity criteria . In distributed matching , on the other hand , each Habitat entity is paired with every Microorganism entity in the sentence . Distributed matching can be seen as a type of N x N matching , while smart matching 1 x 1 matching . The performance of each approach is tested on the development data set . While there is slight increase in the precision , the recall is observed to decrease considerably for the smart matching method ( see Table 1 ) . As a result , the distributed matching approach is used in the final submission . For the overlapping entities in which one entity contains another , some relations can be ignored . For instance , for the sample sentence \" An example of this fact is the presence of Psychrobacter DNA on the surface of Formaggio di Fossa cheeses \" , the Habitat entity \" surface of Formaggio di Fossa cheeses \" , Habitat entity \" Formaggio di Fossa cheeses \" , and Habitat entity \" cheeses \" are overlapping entities . In this case , it would not be appropriate to build three relations such as \" Psychrobacter \" - \" surface of Formaggio di Fossa cheeses \" , \" Psychrobacter \" - \" Formaggio di Fossa cheeses \" , and \" Psychrobacter \" - \" cheeses \" . Instead of extracting multiple relations , \" cheeses \" can be ignored and two relations between \" Psychrobacter \" - \" surface of Formaggio di Fossa cheeses \" and \" Psychrobacter \" - \" Formaggio di Fossa cheeses \" are extracted . This strategy , where the shortest overlapping entity is ignored , is called as the soft filter operation . On the other hand , the strategy when only the longest overlapping entity is retained and the remaining ones are ignored , is named as the hard filter operation . In hard filtering , \" Psychrobacter \" - \" Formaggio di Fossa cheeses \" and \" Psychrobacter \" - \" cheeses \" are ignored and only one relation between \" Psychrobacter \" - \" surface of Formaggio di Fossa cheeses \" is extracted . The performance of each approach is tested on the development data set ( see Table 2 ) . Since our rule - based system for relation extraction is based on the assumption that most of the relations appear within the same sentences , our system is not able to catch the relations that cross sentence boundaries . To overcome this problem , a new rule , which is called remote matching , is integrated into the system . According to this rule , if there exists only one entity type ( Microorganism ) in a sentence , and within a context window of three sentences there exists only one entity ( Habitat or Geographical ) , then there is a relation between these two entities . The performance of the remote matching rule is tested on the development data set . The results show that the number of the predicted relations increased , which also led to an increase in recall . The obtained precision and recall values are 51.4 % and 78.5 % , respectively .", "entities": [[4, 6, "TaskName", "relation extraction"], [581, 583, "TaskName", "relation extraction"]]}
{"text": "In this study , we presented two systems that are implemented in the scope of the BioNLP Shared Task 2019 - Bacteria Biotope Task . The aim of the first system is the normalization of the entity mentions in a biomedical text through the corresponding ontology , whereas the goal of the second system is the extraction of localization and property relations between the related entities when the entities are given . Both systems are unsupervised in the sense that they do not require domainspecific labeled data , while the normalization system makes use of word embeddings and syntactic re - ranking . According to the official evaluation , both of our systems achieved promising results , which have shown that the proposed methods are comparable to or better than the labeled data driven deep learning based approaches used in the shared task .", "entities": [[45, 46, "MethodName", "ontology"], [95, 97, "TaskName", "word embeddings"]]}
{"text": "Second Language Acquisition Modeling", "entities": [[1, 3, "TaskName", "Language Acquisition"]]}
{"text": "We present the task of second language acquisition ( SLA ) modeling . Given a history of errors made by learners of a second language , the task is to predict errors that they are likely to make at arbitrary points in the future . We describe a large corpus of more than 7 M words produced by more than 6k learners of English , Spanish , and French using Duolingo , a popular online language - learning app . Then we report on the results of a shared task challenge aimed studying the SLA task via this corpus , which attracted 15 teams and synthesized work from various fields including cognitive science , linguistics , and machine learning .", "entities": [[6, 8, "TaskName", "language acquisition"]]}
{"text": "As computer - based educational apps increase in popularity , they generate vast amounts of student learning data which can be harnessed to drive personalized instruction . While there have been some recent advances for educational software in domains like mathematics , learning a language is more nuanced , involving the interaction of lexical knowledge , morpho - syntactic processing , and several other skills . Furthermore , most work that has applied natural language processing to language learner data has focused on intermediate - toadvanced students of English , particularly in assessment settings . Much less work has been devoted to beginners , learners of languages other than English , or ongoing study over time . We propose second language acquisition ( SLA ) modeling as a new computational task to help broaden our understanding in this area . First , we describe a new corpus of language learner data , containing more than 7.1 M words , annotated for production errors that were made by more than 6.4k learners of English , Spanish , and French , during their first 30 days of learning with Duolingo ( a popular online language - learning app ) . Then we report on the results of a \" shared task \" challenge organized by the authors using this SLA modeling corpus , which brought together 15 research teams . Our goal for this work is threefold : ( 1 ) to synthesize years of research in cognitive science , linguistics , and machine learning , ( 2 ) to facilitate cross - dialog among these disciplines through a common large - scale empirical task , and in so doing ( 3 ) to shed light on the most effective approaches to SLA modeling .", "entities": [[120, 122, "TaskName", "language acquisition"]]}
{"text": "Sample data from the resulting corpus can be found in Figure 3 . Each token from the reference answer is labeled according to the alignment with the learner 's response ( the final column : 0 for correct and 1 for incorrect ) . Tokens are grouped together by exercise , including user - , exercise - , and session - level meta - data in the previous line ( marked by the # character ) . We included all exercises done by the users sampled from the 30 - day data collection window . The overall format is inspired by the Universal Dependencies ( UD ) format 2 . Column 1 is a unique B64 - encoded token ID , column 2 is a token ( word ) , and columns 3 - 6 are morpho - syntactic features from the UD tag set ( part of speech , morphology features , and dependency parse labels and edges ) . These were generated by processing the aligned reference answers with Google SyntaxNet ( Andor et al , 2016 ) . Because UD tags are meant to be language - agnostic , it was our goal to help make cross - lingual SLA modeling more straightforward by providing these features . Exercise meta - data includes the following : user : 8 - character unique anonymous user ID for each learner ( B64 - encoded ) countries : 2 - character ISO country codes from which this learner has done exercises days : number of days since the learner started learning this language on Duolingo client : session device platform session : session type ( e.g. , lesson or practice ) format : exercise format ( see Figure 1 ) time : the time ( in seconds ) it took the learner to submit a response for this exercise . Lesson sessions ( about 77 % of the data set ) are where new words or concepts are introduced , although lessons also include previously - learned material ( e.g. , each exercise attempts to introduce only one new word or inflection , so all other tokens should have been seen by the student be - . fore ) . Practice sessions ( 22 % ) should contain only previously - seen words and concepts . Test sessions ( 1 % ) are mini - quizzes that allow a student to skip out of a single skill in the curriculum ( i.e. , the student may have never seen this content before in the Duolingo app , but may well have had prior knowledge before starting the course ) . It is worth mentioning that for the shared task , we did not provide actual learner responses , only the closest reference answers . Releasing such data ( at least in the TEST set ) would by definition give away the labels and might undermine the task . However , we plan to release a future version of the corpus that is enhanced with additional meta - data , including the actual learner responses .", "entities": [[35, 36, "DatasetName", "0"], [102, 104, "DatasetName", "Universal Dependencies"], [105, 106, "DatasetName", "UD"], [142, 143, "DatasetName", "UD"], [171, 172, "DatasetName", "Google"], [182, 183, "DatasetName", "UD"]]}
{"text": "SLA modeling is a rich problem , and presents a opportunity to synthesize work from various subfields in cognitive science , linguistics , and machine learning . This section highlights a few key concepts from these fields , and how they relate to the approaches taken by shared task participants . Item response theory ( IRT ) is a common psychometric modeling approach used in educational software ( e.g. , Chen et al , 2005 ) . In its simplest form ( Rasch , 1980 ) , an IRT model is a logistic regression with two weights : one representing the learner 's ability ( i.e. , user ID ) , and the other representing the difficulty of the exercise or test item ( i.e. , token ID ) . An extension of this idea is the additive factor model ( Cen et al , 2008 ) which adds additional \" knowledge components \" ( e.g. , lexical , morphological , or syntactic features ) . Teams that employed linear models ( including our baseline ) are essentially all additive factor IRT models . For decades , tutoring systems have also employed sequence models like HMMs to perform knowledge tracing ( Corbett and Anderson , 1995 ) , a way of estimating a learner 's mastery of knowledge over time . RNN - based approaches that encode user performance over time ( i.e. , that span across exercises ) are therefore variants of deep knowledge tracing ( Piech et al , 2015 ) . Relatedly , the spacing effect ( Dempster , 1989 ) is the observation that people will not only learn but also forget over time , and they remember more effectively through scheduled practices that are spaced out . Settles and Meeder ( 2016 ) and Ridgeway et al ( 2017 ) recently proposed non - linear regressions that explicitly encode the rate of forgetting as part of a decision surface , however none of the current teams chose to do this . Instead , forgetting was either modeled through engineered features ( e.g. , user / token histories ) , or opaquely handled by sequential RNN architectures . SLA modeling also bears some similarity to research in grammatical error detection ( Leacock et al , 2010 ) and correction ( Ng et al , 2013 ) . For these tasks , a model is given a ( possibly ill - formed ) sequence of words produced by a learner , and the task is to identify which are mistakes . SLA modeling is in some sense the opposite : given a well - formed sequence of words that a learner should be able to produce , identify where they are likely to make mistakes . Given these similarities , a few teams adapted state - of - the - art GEC / GED approaches to create their SLA modeling systems . Finally , multitask learning ( e.g. , Caruana , 1997 ) is the idea that machine learning systems can do better at multiple related tasks by trying to solve them simultaneously . For example , recent work in machine translation has demonstrated gains through learning to translate multiple languages with a unified model ( Dong et al , 2015 ) . Similarly , the three language tracks in this work presented an opportunity to explore a unified multitask framework , which a few teams did with positive results .", "entities": [[92, 94, "MethodName", "logistic regression"], [198, 200, "TaskName", "knowledge tracing"], [244, 246, "TaskName", "knowledge tracing"], [371, 374, "TaskName", "grammatical error detection"], [476, 477, "DatasetName", "GED"], [523, 525, "TaskName", "machine translation"]]}
{"text": "In this section , we analyze the various modeling choices explored by the different teams in order to shed light on what kinds of algorithmic and feature engineering decisions appear to be useful for the SLA modeling task .", "entities": [[26, 28, "TaskName", "feature engineering"]]}
{"text": "Popularity Effect . ture the same phenomena ) . We also include each feature 's popularity and an effect estimate 5 . Broadly speaking , results suggest that feature engineering had a much smaller impact on system performance than the choice of learning algorithm . Only \" response time \" and \" days in course \" showed even marginally significant trends . Of particular interest is the observation that morpho - syntactic features ( described in 2.4 ) actually seem to have weakly negative effects . This echoes singsound 's finding that their linguistic encoder contributed the least to system performance , and Cambridge determined through ablation studies that these features in fact hurt their system . One reasonable explanation is that these automaticallygenerated features contain too many systematic parsing errors to provide value . ( Note that NYU artificially introduced punctuation to the exercises and re - parsed the data in their work . ) As for newly - engineered features , word information such as frequency , semantic embeddings , and stemming were popular . It may be that these features showed such little return because our corpus was too biased toward beginners - thus representing a very narrow sample of language - for these features to be meaningful . Cognate features were an interesting idea used by a few teams , and may have been more useful if the data included users from a wider variety of different L1 language backgrounds . Spaced repetition features also exhibited marginal ( but statistically insignificant ) gains . We posit that the 30 - day window we used for data collection was simply not long enough for these features to capture more longterm learning ( and forgetting ) trends .", "entities": [[28, 30, "TaskName", "feature engineering"], [103, 104, "DatasetName", "Cambridge"]]}
{"text": "In this work , we presented the task of second language acquisition ( SLA ) modeling , described a large data set for studying this task , and reported on the results of a shared task challenge that explored this new domain . The task attracted strong participation from 15 teams , who represented a wide variety of fields including cognitive science , linguistics , and machine learning . Among our key findings is the observation that , for this particular formulation of the task , the choice of learning algorithm appears to be more important than clever feature engineering . In particular , the most effective teams employed sequence models ( e.g. , RNNs ) that can capture user performance over time , and tree ensembles ( e.g. , GBDTs ) that can capture non - linear relationships among features . Furthermore , using a multitask framework - in this case , a unified model that leverages data from all three language tracks - can provide further improvements . Still , many teams opted for a simpler algorithm ( e.g. , logistic regression ) and concentrated instead on more psychologically - motivated features . While these teams did not always perform as well , several demonstrated through ablation studies that these features can be useful within the limitations of the algorithm . It is possible that the constraints of the SLA modeling data set ( beginner language , homogeneous L1 language background , short 30 - day time frame , etc . ) prevented these features from being more useful across different", "entities": [[10, 12, "TaskName", "language acquisition"], [98, 100, "TaskName", "feature engineering"], [182, 184, "MethodName", "logistic regression"]]}
{"text": "Gaze fixation patterns have been shown to strongly reflect the online cognitive processing demands of 1 More precisely , our measure of dissimilarity between experimental conditions is analogous to ground distance and dissimilarity between RDMs to earth mover 's distance . human readers ( Raney et al , 2014 ; Ashby et al , 2005 ) and to be dependent upon a number of linguistic factors ( Van Gompel , 2007 ) . Specifically , it has been demonstrated that word frequency , syntactic complexity , and lexical ambiguity play a strong part in determining which sentences are difficult for humans to process ( Rayner and Duffy , 1986 ; Duffy et al , 1988 ; Levy , 2008 ) . Using the RSA framework , we aim to explore how gaze fixation patterns and the linguistic factors associated with sentence processing difficulty relate to the representational spaces of popular language encoders . Namely , we hypothesize that , for a given sentence , disagreement between hidden layers corresponds to processing difficulty . Because layer disagreement for a sentence measures the extent to which two layers ( e.g. within BERT ) disagree with each other about the pairwise similarity of the sentence ( with other sentences in the corpus ) , a sentence with high layer disagreement will have unstable similarity relationships to other sentences in the corpus . This indicates that it has a degraded encoder representation . Going further , we also hypothesize that models ' representations of said sentences may be confounded , in part , by factors that are known to influence humans . Eye - tracking data For our experiments , we make use of the Dundee eye - tracking corpus ( Kennedy et al , 2003 ) , the English part of which consists of eye - movement data recorded as 10 native participants read 2 , 368 sentences from 20 newspaper articles . We consider the following fixation features : TOTAL FIXATION DURATION and FIRST PASS DURATION . For each of the features , we first take the average of the measurements recorded for all 10 participants per word , then ob - tain sentence - level annotations by summing the measurements of all words in a sentence and dividing by its length . The result of this is two vectors V totf ix and V f irstpass of length 2 , 368 , where each cell in the vector corresponds to a sentence 's average total fixation and average first pass duration , respectively . Syntactic complexity , word frequency , and lexical ambiguity We also consider the three following linguistic features which affect processing difficulty . For each of the following the result is also a vector of length 2 , 368 where each cell corresponds to a sentence : a. the average word log frequency per sentence extracted from the British National Corpus ( Leech , 1992 ) , V logF req . . b. the average number of senses per word per sentence extracted from WordNet ( Miller , 1995 ) , V wordSense . c. Yngve scores , a standard measure of syntactic complexity based on cognitive load ( Yngve , 1960 ) , V Y ngve . Pretrained encoders We conduct our analysis on pretrained BERT - large ( Devlin et al , 2018 ) and ELMo ( Peters et al , 2018 ) , two widely employed contextual sentence encoders . To obtain a representation of a sentence from a given layer L , we perform mean - pooling over the time - steps which correspond to the words of a sentence , obtaining a vector representation of the sentence . Meanpooling is a common approach for obtaining vector representations of sentences for downstream tasks ( Peters et al , 2018 ; Conneau et al , 2017b ) . We refer to ELMo 's lowest layer as E1 , BERT 's 11th layer as B11 , etc . RDMs We construct an RDM ( see 2 ) for each contextual encoder 's layers . Each RDM is a 2 , 368 \u00d7 2 , 368 matrix which represents the dissimilarity structure of the layer , ( i.e. , each row vector in the matrix contains the dissimilarity of a given sentence to every other sentence ) . We then compute the correlations between the two different RDMs . For our evaluation of how well the representational geometry of a layer correlates to another , we employ Kendall 's \u03c4 A as suggested in Nili et al ( 2014 ) , computing the pairwise correlation for each two corresponding rows in two RDMs . This second - order analysis gives us a pairwise relational similarity vector V Corr L i \u2212L j of length 2 , 368 , which has the correlations between two layers L i and L j 's RDMs for each of the sentences . Third - order analysis The final part of our analysis involves computing correlations ( Spearman 's \u03c1 ) of { V Corr L i \u2212L j , V logF req , V Y ngve , V wordSense } with each of V totf ix and V f irstpass . The results from this are shown in Table 1 . The top section of the table shows correlations when L i and L j are the three final adjacent layers in BERT and ELMo . The middle section shows the results for top three BERT layer pairs L i and L j which maximize the correlation scores . The final section shows correlation with the linguistic features . Finally , Figure 2 shows Spearman 's \u03c1 correlations between V Corr L i \u2212L j and each of V totf ix , and V Y ngve for all combinations of the 24 BERT layers .", "entities": [[189, 190, "MethodName", "BERT"], [332, 333, "DatasetName", "PASS"], [548, 549, "MethodName", "BERT"], [559, 560, "MethodName", "ELMo"], [646, 647, "MethodName", "ELMo"], [653, 654, "MethodName", "BERT"], [901, 902, "MethodName", "BERT"], [903, 904, "MethodName", "ELMo"], [914, 915, "MethodName", "BERT"], [971, 972, "MethodName", "BERT"]]}
{"text": "Our results show highly significant negative correlations between V Corr L i \u2212L j and sentence gaze fixation times . These findings confirm the hypothesis that the sentences that are most challenging for humans to process , are the sentences ( a ) the layers of BERT disagree most on among themselves ; and ( b ) that ELMo and BERT disagree most on , indicating that there may be common factors which affect human processing difficulty and result in disagreement between layers . By Layer disagreement we refer to the expression 1 \u2212 V Corr L i \u2212L j . It is important to note that these encoders are trained with a language modelling objective , unlike models where reading behaviour is explicitly modelled ( Hahn and Keller , 2016 ) or predicted ( Matthies and S\u00f8gaard , 2013 ) . Indeed , the similarities here emerge naturally as a function of the task being performed . This can be seen as analogous to the case of similarities observed between neural networks trained to perform object recognition and spatio - temporal cortical dynamics ( Cichy et al , 2016 ) . Syntactic complexity Figure 2 shows that , for all combinations of BERT layers , total fixation time and Yngve scores have strong negative and positive correlations ( respectively ) with layer disagreement . Furthermore , we observe that disagreement between middle layers seems to show the strongest correlation with Yngve scores . To confirm this , we split the correlations into four groups : \" low \" ( i , j [ 1 , 8 ] ) , \" middle \" ( i , j 1 : Spearman 's \u03c1 between V Corr L i \u2212L j , V logF req . , VwordSense , V Y ngve and each of V totf ix and V f irstpass . All correlations significant with p < 0.0001 after Bonferroni correction unless marked with * . [ 9 , 16 ] ) , \" high \" ( i , j [ 17 , 24 ] ) , and \" out \" ( | i \u2212 j | > 7 ) , with the latter representing out - ofgroup correlations ( e.g. Corr L 1 \u2212L 24 ) . To account for correlations between disagreeing adjacent layers ( e.g. | i \u2212 j | = 1 ) and Yngve scores being higher ( as a possible confounding factor ) , we also distinguish layers as either \" adjacent \" or \" non - adjacent \" . Considering these two factors as three - and two - leveled independent variables respectively , we conduct a two - way analysis of variance . The analysis reveals that the effect of group is significant at F ( 3 , 275 ) = 78.47 , p < 0.0001 , with \" low \" ( \u00b5 = 0.65 , \u03c3 = 0.08 ) , \" middle \" ( \u00b5 = 0.84 , \u03c3 = 0.03 ) , \" high \" ( \u00b5 = 0.80 , \u03c3 = 0.05 ) , and \" out \" ( \u00b5 = 0.80 , \u03c3 = 0.05 ) . Neither the effect of adjacency nor its interaction with group proved to be significant . This can be seen as ( modest ) support for the findings of previous work ( Blevins et al , 2018 ; Tenney et al , 2019 ) : namely , that the intermediate layers of neural language models encode the most syntax , and are therefore possibly more sensitive towards syntactic complexity . A very similar pattern is observed for total fixation time . When considered together with the correlation between V Y ngve and fixation times , this indicates a tripartite affinity between layer disagreement , syntactic complexity , and fixation . Lexical Ambiguity and Word Frequency Finally , we observe that V logF req . has a moderate correlation with both fixation time and layer disagreement and that V wordSense is nearly uncorrelated to both . Detailed plots of the latter can be found in Appendix A.", "entities": [[46, 47, "MethodName", "BERT"], [58, 59, "MethodName", "ELMo"], [60, 61, "MethodName", "BERT"], [113, 115, "TaskName", "language modelling"], [176, 178, "TaskName", "object recognition"], [203, 204, "MethodName", "BERT"]]}
{"text": "We would like to thank Vinit Ravishankar , Matt Lamm , and the anonymous reviewers for their helpful comments . Mostafa Abdou and Anders S\u00f8gaard are supported by a Google Focused Research Award and a Facebook Research Award .", "entities": [[29, 30, "DatasetName", "Google"]]}
{"text": "Investigating the Generative Approach for Question Answering in E - Commerce", "entities": [[5, 7, "TaskName", "Question Answering"]]}
{"text": "Many e - commerce websites provide Productrelated Question Answering ( PQA ) platform where potential customers can ask questions related to a product , and other consumers can post an answer to that question based on their experience . Recently , there has been a growing interest in providing automated responses to product questions . In this paper , we investigate the suitability of the generative approach for PQA . We use state - of - the - art generative models proposed by Deng et al ( 2020 ) and Lu et al ( 2020 ) for this purpose . On closer examination , we find several drawbacks in this approach : ( 1 ) input reviews are not always utilized significantly for answer generation , ( 2 ) the performance of the models is abysmal while answering the numerical questions , ( 3 ) many of the generated answers contain phrases like \" I do not know \" which are taken from the reference answer in training data , and these answers do not convey any information to the customer . Although these approaches achieve a high ROUGE score , it does not reflect upon these shortcomings of the generated answers . We hope that our analysis will lead to more rigorous PQA approaches , and future research will focus on addressing these shortcomings in PQA .", "entities": [[7, 9, "TaskName", "Question Answering"], [83, 86, "DatasetName", "Deng et al"], [123, 125, "TaskName", "answer generation"]]}
{"text": "With the increase in e - commerce shopping , customer - generated product queries are also growing . Manually answering the questions in real - time is infeasible , and also some questions go unanswered for an extended period . It is necessary to answer the user queries in the e - commerce business automatically . The user reviews are a vast source of information with diverse opinions , and they can be used to answer user queries . Earlier works on product question answering ( PQA ) focus on retrieval - based approaches and binary answer prediction tasks . McAuley and Yang ( 2016 ) ; Fan et al ( 2019 ) ; aim to predict the answer as \" yes / no \" based on the relevant reviews , customer ratings , aspects in the reviews , etc . Retrieval - based approaches try to find the most relevant review snippet as the answer ( Chen et al , 2019a ) and use a ranked list of review snippets as the response for a given question . With the success of machine translation ( Sutskever et al , 2014 ) and summarization ( See et al , 2017 ) , the PQA approaches are shifting towards natural answer generation from relevant product reviews ( Gao et al , 2019 ; Chen et al , 2019b ; Deng et al , 2020 ; Lu et al , 2020 ; Gao et al , 2021 ) . In this work , we analyse the answer generated from state - of - the - art generative models OAAG ( Deng et al , 2020 ) and CHIME ( Lu et al , 2020 ) in detail beyond their traditional scores on popular metrics such as ROUGE ( Lin , 2004 ) . We find that despite achieving a good score on these metrics , generated answers have several drawbacks that can lead to user dissatisfaction . ( Ni et al , 2019 ; He and McAuley , 2016 ) includes users ' reviews along with a rating of the product given by the same user . The Product ID is used to align the question with its reviews .", "entities": [[83, 85, "TaskName", "question answering"], [183, 185, "TaskName", "machine translation"], [193, 194, "TaskName", "summarization"], [209, 211, "TaskName", "answer generation"], [228, 231, "DatasetName", "Deng et al"], [268, 271, "DatasetName", "Deng et al"]]}
{"text": "We use Opinion - aware Answer Generation ( OAAG ) model ( Deng et al , 2020 ) and Crosspassage Hierarchical Memory Network ( CHIME ) model ( Lu et al , 2020 ) for our analysis . Following the generative approach , these two models achieve state - of - the - art performance on the Amazon Question Answering dataset . There are thousands of products in each category in the Amazon Product Review dataset , and each product has thousands of reviews . All the reviews may not be relevant for a particular query , and therefore , to answer a product - related question , models need to filter out the irrelevant reviews first . OAAG and CHIME use the BM25 algorithm to retrieve and rank all the review snippets of a product , and the top k relevant snippets ( we use top 10 reviews snippets ) for that question are taken as the premise of the answer .", "entities": [[5, 7, "TaskName", "Answer Generation"], [12, 15, "DatasetName", "Deng et al"], [21, 23, "MethodName", "Memory Network"], [58, 60, "TaskName", "Question Answering"]]}
{"text": "Upon retrieving the relevant reviews , OAAG uses an encoder - decoder model for answer generation . OAAG encodes the question and each review corresponding to that question using a Bi - LSTM ( Hochreiter and Schmidhuber , 1997 ) network . They apply a co - attention mechanism over these encodings to get the question and review representations . They utilize the ratings of the retrieved reviews to mine the general opinion about the question using the attention mechanism . Finally , they employ a multi - view pointer - generator network that copies words from the question as well as from the reviews and fuses the opinion by re - weighting the attention scores of the words in reviews to generate an opinionated answer . They report ROUGE - based scores to compare the model performance against the previous approaches ( Chen et al , 2019b ; Gao et al , 2019 ) .", "entities": [[14, 16, "TaskName", "answer generation"], [32, 33, "MethodName", "LSTM"]]}
{"text": "CHIME uses a transformer - based encoder - decoder model to generate the response . It extends pretrained XLNet ( Yang et al , 2019 ) with an auxiliary memory module that consists of two components : the context memory , and the answer memory . Given a question with K review passages , it creates K training instances , each consisting of the question , a review passage , and the reference answer . Each training instance is fed into an XLNet encoder to get the hidden representations that are used to update the two memories . The context memory mechanism sequentially reads the review passages and gathers the cross - passage evidences to identify the most prominent opinion in reviews . The answer memory works as a buffer to gradually refine the generated answers after reading each ( question , review passage ) pair . After reading the last review , the answer memory is fed to the decoder to get a final response .", "entities": [[18, 19, "MethodName", "XLNet"], [82, 83, "MethodName", "XLNet"]]}
{"text": "Different types of questions are asked on the Amazon product page like numerical , \" yes / no \" , descriptive . The generative model may not be suitable for answering all kinds of questions . So , we categorize the questions as template - based and descriptive . For template - based questions , the answer can be yes or no without any explanation . We filter the questions where the answer starts with ' yes ' , ' yeah ' , ' no ' , ' nope ' and mark these as templatebased questions . Both categories contain \u223c75 % descriptive questions . Table 2 summarizes the result of the template - based and generative questions . Both models ' performance in descriptive questions is better than the template - based questions . Furthermore , we categorized the questions into numerical and non - numerical questions . We consider a question to be numerical if there are numbers in the question or in the reference answer . The test datasets of both the categories have \u223c19 % numerical questions . The OAAG model performs better in answering non - numerical questions , while CHIME performs better in answering numerical questions . Although the ROUGE scores are close in numerical and non - numerical questions for both the models , on analyzing the numerical answers , we find that the words in generated and reference answers might match , but the numbers generally do not match . 3 We present some examples of numerical questions with their answers in Table A.4 of Appendix .", "entities": [[9, 11, "DatasetName", "product page"]]}
{"text": "This paper describes the Air Force Research Laboratory ( AFRL ) machine translation systems and the improvements that were developed during the WMT19 evaluation campaign . This year , we refine our approach to training popular neural machine translation toolkits , experiment with a new domain adaptation technique and again measure improvements in performance on the Russian - English language pair .", "entities": [[11, 13, "TaskName", "machine translation"], [37, 39, "TaskName", "machine translation"], [45, 47, "TaskName", "domain adaptation"]]}
{"text": "As part of the 2019 Conference on Machine Translation ( Bojar et al , 2019 ) news - translation shared task , the AFRL Human Language Technology team participated in the Russian - English portion of the competition . We build on our strategies from last year ( Gwinnup et al , 2018 ) , adding additional language ID based data processing and optimizing subword segmentation strategies . For Russian - English we again submitted an entry comprising our best systems trained with Marian ( Junczys - Dowmunt et al , 2018 ) , Sockeye ( Hieber et al , 2017 ) with Elastic Weight Consolidation ( EWC ) ( Thompson et al , 2019 ) , OpenNMT ( Klein et al , 2018 ) , and Moses ( Koehn et al , 2007 ) combined using the Jane system combination method ( Freitag et al , 2014 ) .", "entities": [[7, 9, "TaskName", "Machine Translation"], [103, 106, "MethodName", "Elastic Weight Consolidation"], [107, 108, "MethodName", "EWC"]]}
{"text": "We used and preprocess data as outlined in Gwinnup et al ( 2018 ) . For all systems trained , we applied either byte - pair encoding ( BPE ) ( Sennrich et al , 2016 ) or SentencePiece ( Kudo and Richardson , 2018 ) subword strategies to address the vocabulary - size problem . For this year , we also employed a language ID filtering step for the BPE - based systems . Using the pre - built language ID model developed by the authors of fastText ( Joulin et al , 2016a , b ) , we developed a utility that examined the source and target sentence pairs and discarded that pair if either side fell below 0.8 1 probability of the desired language . We applied this filtering to all provided parallel corpora , removing 33.7 % of lines . This process was particularly effective when used to filter the Paracrawl corpus where 57.1 % of lines were removed . Pre and post - filtering line counts for various corpora are shown in Table 1 .", "entities": [[28, 29, "MethodName", "BPE"], [38, 39, "MethodName", "SentencePiece"], [70, 71, "MethodName", "BPE"], [88, 89, "MethodName", "fastText"], [154, 155, "DatasetName", "Paracrawl"]]}
{"text": "One of the problems faced when addressing the closed - vocabulary problem is the granularity of the subword units either produced by SentencePiece or BPE . To that end , we examined varying the number of BPE merge operations in order to determine an optimal setting to maximize performance for the Russian - English language pair . For the OpenNMT - based systems , a vocabulary size of 32k entries was employed during training of a SentencePiece segmentation model 2 . This vocabulary size was determined empirically from the training data . Alternatively , for the BPE - based systems , we systematically examined varying sizes of BPE merge operations and vocabulary sizes in 10k increments from 30k to 80k . Results in Table 3 show that 40k BPE merge operations perform best across all test sets decoded for this language pair . All subsequent Marian experiments in this work utilize this 40k BPE training corpus .", "entities": [[22, 23, "MethodName", "SentencePiece"], [24, 25, "MethodName", "BPE"], [36, 37, "MethodName", "BPE"], [76, 77, "MethodName", "SentencePiece"], [96, 97, "MethodName", "BPE"], [107, 108, "MethodName", "BPE"], [128, 129, "MethodName", "BPE"], [153, 154, "MethodName", "BPE"]]}
{"text": "This year , we focused system - building efforts on the Marian , Sockeye , OpenNMT , and Moses toolkits , having explored a variety of parameters , data , and conditions . While most of our experimentation builds off of previous years ' efforts , we did examine domain adaptation via continued training , including Elastic Weight Consolidation ( EWC ) ( Thompson et al , 2019 ) .", "entities": [[49, 51, "TaskName", "domain adaptation"], [56, 59, "MethodName", "Elastic Weight Consolidation"], [60, 61, "MethodName", "EWC"]]}
{"text": "For our Sockeye ( Hieber et al , 2017 ) systems , we experimented with continued training ( Luong and Manning , 2015 ; Sennrich et al , 2015 ) - a means to specialize a model in a new domain after a period of training on a general domain . One downside of utilizing continued training is the model adapts \" too - well \" to the new domain at the expense of performance in the original domain ( Freitag and Al - Onaizan , 2016 ) . One method to mitigate this performance drop is to prevent certain parameters of the network from changing with Elastic Weight Consolidation ( EWC ) ( Kirkpatrick et al , 2017 ) . Thompson et al ( 2019 ) ference in style and content . Here , we created a news subdomain corpus from the newstest2014 through newstest2017 test sets . The intuition is that more current events will be discussed in these test sets than the remainder of the provided training corpora , allowing better adaptation of new events in the newest test sets ( newstest2018 and newstest2019 . ) We first trained a baseline transformer system using the best - performing BPE parameters from Section 2.2 , 512 - dimension word embeddings , 6 layer encoder and decoder , 8 attention heads , label smoothing and transformer attention dropout of 0.1 . We then continue - train a model on the adaptation set described above . We also followed the Sockeye EWC training procedure , producing a model more resilient to overfitting due to continued training . Results for these systems are shown in Table 5 . We see that the baseline Sockeye transformer model performs similarly to the baseline singlemodel Marian transformer system shown in Table 4 . The continued - training system ( con't train ) system predictably overfit on newstest2014 as expected , since that test set is a part of the adaptation set . Likewise , performance on the out - ofdomain newstest2018 also dropped as a result of overfitting . The best - performing EWC system 5 5 EWC applied with weight - decay of 0.001 and learning - actually improved performance on 2018 with lesspronounced overfitting on 2014 . For system combination outlined later in Section 4 , we decoded test sets with an ensemble of the four highest - scoring model checkpoints from the best EWC training run .", "entities": [[107, 110, "MethodName", "Elastic Weight Consolidation"], [111, 112, "MethodName", "EWC"], [201, 202, "MethodName", "BPE"], [210, 212, "TaskName", "word embeddings"], [223, 225, "MethodName", "label smoothing"], [227, 229, "MethodName", "attention dropout"], [251, 252, "MethodName", "EWC"], [349, 350, "MethodName", "EWC"], [353, 354, "MethodName", "EWC"], [402, 403, "MethodName", "EWC"]]}
{"text": "Our first Open - NMT system was trained using the Transformer architecture with the default \" Trans - formerBig \" settings as described in Vaswani et al ( 2017 ) : 6 layers of 1024 units , 16 attention heads . Dropout rates of 0.3 for layers and 0.1 for attention heads and relu 's . Training data for this system utilized the training corpus from our WMT17 Russian - English system ( Gwinnup et al , 2017 ) consisting of provided parallel and backtranslated rate of 0.00001 data . This data was then processed with a joint 32k word vocabulary SentencePiece model .", "entities": [[10, 11, "MethodName", "Transformer"], [41, 42, "MethodName", "Dropout"], [53, 54, "MethodName", "relu"], [101, 102, "MethodName", "SentencePiece"]]}
{"text": "For our second OpenNMT system , we first trained language - specific , 32k word vocabularies using SentencePiece . WMT news test data from all years except 2014 and 2017 were used to train Senten - cePiece . These data , with the addition of the language ID filtered ParaCrawl corpus outlined in Section 2.1 , were used for training the system . WMT news test data from 2014 was used for validation . OpenNMT - tf was used to create the system , using the stock \" Transformer \" model .", "entities": [[17, 18, "MethodName", "SentencePiece"], [49, 50, "DatasetName", "ParaCrawl"], [88, 89, "MethodName", "Transformer"]]}
{"text": "We submitted the final 5 - system combination outlined in Section 4 and the four - checkpoint EWC ensemble detailed in Section 3.2 to the Russian - English portion of the WMT19 news task evaluation . Selected newstest2019 automatic scores from the WMT Evaluation Matrix 6 are shown in Table 7 .", "entities": [[17, 18, "MethodName", "EWC"]]}
{"text": "We presented a series of improvements to our Russian - English systems , including improved preprocessing and domain adaptation . Clever remixing of older techniques from the phrasebased MT era enabled improvements in ensembled neural decoding . Lastly , we performed system combination to leverage benefits from these new techniques and favorite approaches from previous years .", "entities": [[17, 19, "TaskName", "domain adaptation"]]}
{"text": "Analysis of online user discussion continues to be a critical area of interdisciplinary research . Increasing rates of internet access and the development of a diverse range of online forums has allowed for conversation between individuals across the globe on an extraordinary range of topics . However , this has been accompanied by a surge in abuse and other negative behaviours online , the impacts of which have been well - documented in academic research . It has been found that targeted negative comments and harassment online can seriously impact individual well - being ( Weingartner and Stahel , 2019 ; Bauman , 2013 ) , force users to leave a community or reduce online participation ( Wulczyn et al , 2017 ; Blackburn and Kwak , 2014 ) , and potentially lead to offline hate - crimes ( Mulki et al , 2019 ; Hassan et al , 2018 ) . While these forms of comments may be explicit or overtly harmful , they are also often difficult to detect or ambiguous . Where there are insufficient moderation resources to scale with a forum 's user - base , this can lead to unchecked negative discourse , or cause website administrators to restrict user comment functions . This means that research which aims to enable automated moderation , provide a review triage service for human moderation teams , or design systems to nudge users towards healthier conversation , has significant potential for contributing to both the availability and quality of online discourse . A persistent challenge for researchers and site administrators in this area is the need to : ( a ) establish a typology of comments which are undesirable in online discussions ; ( b ) apply this typology in a consistent and reliable manner ; and ( c ) account for adversarial user behaviour in response to moderation . This is complicated by the fact that there is no single objective set of categories for speech which ought to be excluded in all contexts , with perceptions of undesirable speech differing across individuals , cultures , geographies , and online communities ( Vidgen et al , 2019 ) . Prior research on toxic comments online has found that classifiers trained on crowdsourced data can be effective at detecting the most overt forms of toxic comments . However , there remain difficulties in detecting subtler forms of toxicity which may be implicit , require idiosyncratic knowledge , familiarity with the conversation context , or familiarity with particular cultural tropes ( Kohli et al , 2018 ; van Aken et al , 2018 ; Parekh and Patel , 2017 ) . One of the key ingredients to progress on this front will be high quality , large , annotated datasets addressing these more subtle harmful attributes , from which machine learning models will be able to learn . Unfortunately , for most subtler toxic attributes there are few available datasets ( or none , particularly in many languages other than English ) , which is a bottleneck preventing further research ( Fortuna et al , 2019 ) . We aim to contribute to research in this area through the release of the Unhealthy Comment Corpus ( UCC ) of approximately 44 , 000 comments and corresponding crowdsourced labels and confidence scores . The labelling typology for the dataset identifies for each comment a higher - level classification of whether that comment ' has a place in a healthy online conversation ' , accompanied for each comment by binary labels for whether it is : ( 1 ) hostile , ( 2 ) antagonistic , insulting , provocative or trolling ( together , ' antagonistic ' ) , ( 3 ) dismissive , ( 4 ) condescending or patronising ( together , ' condescending ' ) , ( 5 ) sarcastic , and/or ( 6 ) an unfair generalisation . For each label there is also an associated confidence score ( between 0.5 and 1 ) . The UCC is open source and available on Github . 1 The UCC contributes further high quality data on attributes like sarcasm , hostility , and condescension , adding to existing datasets on these and related attributes ( Wang and Potts , 2019 ; Davidson et al , 2017 ; Wulczyn et al , 2017 ; Chen et al , 2017 ) , and provides ( to the best of our knowledge ) the first dataset of this scale with labels for dismissiveness , unfair generalisations , antagonistic behavior , and overall assessments of whether those comments fall within ' healthy ' conversation . We also make use of and illustrate the benefits of annotator trustworthiness scores when crowdsourcing labels on subjective data of this sort . 1 github.com/conversationai/unhealthy - conversations This paper is structured as follows . Section 2 outlines the motivation and background to the UCC attribute typology . Section 3 details the data collection and quality control processes . In Section 4 we present some summary statistics , benefits , and limitations of the data , and in Section 5 we present a baseline classification model for this dataset , and evaluate its performance . Section 6 highlights potential sources of bias in this dataset , and the need to be cognisant of these when conducting further research in this area .", "entities": [[537, 538, "DatasetName", "UCC"], [668, 669, "DatasetName", "UCC"], [679, 680, "DatasetName", "UCC"], [814, 815, "DatasetName", "UCC"]]}
{"text": "Also included in the UCC dataset are the individual annotations for each comment by all ' trusted ' annotators . Users of the data may therefore apply any alternative trustworthiness threshold , or use a preferred aggregation method to derive labels .", "entities": [[4, 5, "DatasetName", "UCC"]]}
{"text": "Human sarcasm is often communicated by stating something which the author presumes to be so obviously untrue that it will be read as sarcastic . These presumptions reflect the author 's biases - or in the cases of comment annotation , labelling comments as sarcastic reflects the annotators beliefs of what is obviously untrue . With the comment corpus being in English , and given the subtlety of the attributes , higher quality annotations were likely to be achieved by annotators with first - language proficiency in English . The best proxy for this available on the Figure Eight platform was to restrict the country of origin of our annotators to a limited subset of countries with a large English - speaking population ( as either an official language or primary second language ) , in particular : the United States , the United Kingdom , South Africa , Sweden , New Zealand , Norway , Netherlands , Denmark , Canada , and Australia . Although our early iterations of this annotation job indicated a significant reduction in annotators failing test comments once this was enforced , this introduces a clear cultural and geographic bias . For example , the comment ' Iran and Turkey are the BEST places to be a woman ! ' , was scored as sarcastic with 72 % confidence by the annotators . Finding this comment sarcastic relies on an assumption by the annotators ( of which the pool excludes residents of Iran and Turkey ) that Iran and Turkey are clearly not the best places to be women . Our annotators were not selected as broadly representative across language , geography , culture , or other attributes and this assumption is not universal . While important research has begun to explore the composition of the global crowd workforce , it remains difficult to select for annotators representative of specific characteristics on crowd work platforms ( Posch et al , 2018 ) . In the current version of the Appen platform , unless annotators are asked standalone questions on demographics , the only available de - tails are the annotators ' country and/or city ( and even then , only for some annotators ) . Research and modelling based on this dataset , and similar datasets , requires the exercise of great care in mitigating biases produced by the underlying data collection . This potential selection bias is likely to be evident across the broader healthy / unhealthy categorisation along with each of the attributes . Prior research has found substantial disagreement on subtle attributes of speech both among individuals and across geographies ( Salminen et al , 2018a ) . Finally , the source of the comments and their manner of presentation could introduce bias into the dataset . The source data is solely from a Canadian online newspaper comment section and comments were presented in isolation to annotators , without the surrounding context of the news article and other comments . Annotators were also provided with the standard questionnaire ( Appendix A ) , which includes high level descriptions of the attributes that may not generalise across cultures . There is a substantial body of research demonstrating the potential impact of introducing biased datasets , and Vidgen et al ( Vidgen et al , 2019 ) note that public datasets in this area are prone to systematic bias and mislabelling , with interannotator agreement typically low for complex multi - class tasks of this kind . These challenges are to be expected in a relatively new field which aims to improve on human baseline moderation for highly subjective characteristics of online discussion . At this early stage of research , we must be mindful of addressing these biases and cognisant that the manner in which this data is collected can have critical impacts on users in a production environment . It is important to note at this stage of the field in general , and with our understanding of this dataset in particular , that the UCC dataset is not designed to train models which are immediately available for automated moderation without human intervention in a live online setting . As the field develops further , initial use - cases may include less interventionist ' nudges ' or reminders of how a comment could be perceived by a reader to assist participants in discussions online .", "entities": [[400, 402, "TaskName", "selection bias"], [673, 674, "DatasetName", "UCC"]]}
{"text": "We introduced a new corpus of labelled comments and a typology for some of the more subtle aspects of unhealthy online conversation . Our typology provides 6 sub - attributes of typically unhealthy con - tributions , and confidence scores for the labels . We described the process and challenges in creating such a dataset , and provided statistics to convey the scale of data . In particular , we note that although there is a substantial body of research on more extreme forms of negative contributions , such as toxicity , the subtler forms of unhealthy comments in our typology are often similarly prevalent online . Our analysis also shows that the sub - attributes are largely independent from overt toxicity , and mostly correlated with unhealthy contributions . We also provide results from a modern baseline ML model ( fine tuning BERT ) and note that performance exceeds that of a crowd - worker . This suggests that further work could also be done to collect a larger corpus of annotations to improve the capacity to measure models in this domain . While this dataset provides a new contribution in gathering the 6 attributes under the umbrella of an ' unhealthy ' conversation , there also remains an open question as to how exhaustive this typology of unhealthy contributions is . Future research and annotation work could further refine the typology , amend the standard questionnaire , or apply it to forums which differ in cultural and geographic context . Further work also includes exploring the unintended biases in the model and data . This dataset is well - placed to further explore early signs of conversations going awry ( Zhang et al , 2018 ) , while models based on the data could be explored to provide assistance to moderating online conversations . A Annotator Questionnaire", "entities": [[143, 144, "MethodName", "BERT"]]}
{"text": "In this paper , we describe two systems we developed for the three tracks we have participated in the BEA - 2019 GEC Shared Task . We investigate competitive classification models with bi - directional recurrent neural networks ( Bi - RNN ) and neural machine translation ( NMT ) models . For different tracks , we use ensemble systems to selectively combine the NMT models , the classification models , and some rules , and demonstrate that an ensemble solution can effectively improve GEC performance over single systems . Our GEC systems ranked the first in the Unrestricted Track , and the third in both the Restricted Track and the Low Resource Track .", "entities": [[45, 47, "TaskName", "machine translation"], [98, 99, "DatasetName", "Unrestricted"], [107, 108, "DatasetName", "Restricted"]]}
{"text": "Grammatical error correction ( GEC ) is the task of automatically correcting grammatical errors in text . With the increasing number of language learners , GEC has gained more and more attention from educationists and researchers in the past decade . The following is a GEC example : I [ fall fell ] asleep at 11 p.m. last [ nigh night ] . Here fall needs to be corrected to its past tense form and nigh is a spelling mistake . GEC is considered as a mapping task from incorrect sentences to correct sentences . Incorrect sentences can be seen as being produced by adding noises to correct sentences . The added noise does not happen randomly , but occurs when people learn or use the language according to a certain error distribution and language usage bias . Initially , people used rule - based approaches to solve GEC problems ( Naber and Mi\u0142kowski , 2005 ) . Rules are relatively easy to make but with poor generalization . Later researchers began to treat GEC as a classification task . According to the grammatical information around the target word , classifiers can be constructed to predict the true grammatical role of the target word . One drawback of the classification methods for GEC is that training different classifiers for different error types may be resource - intensive and inefficient since there are many grammatical error types . Recently , translation methods have become the focus of research , and there is a clear trend that state - of - the - art GEC systems are being shifted from traditional NLP methods to NMT based methods . In recent years , GEC performance has seen significant improvement in some public GEC test sets ( Ge et al , 2018 ) . In CoNLL - 2013 ( Ng et al , 2013 and CoNLL - 2014 ( Ng et al , 2014 ) GEC Shared Task , machine learning based GEC methods emerged with relatively good performance . Classification methods achieved the best result in CoNLL - 2013 ( Rozovskaya et al , 2013 ) . After that , statistical machine translation ( SMT ) methods began to show better performance in CoNLL - 2014 ( Felice et al , 2014 ) . ( Chollampatt et al , 2016 ) was the first study to obtain the state - ofthe - art result with neural networks . Then after ( Junczys - Dowmunt and Grundkiewicz , 2016 ) , machine translation methods became the mainstream in GEC solutions . In addition , an RNNbased context model achieved better results than previous traditional classification models ( Wang et al , 2017 ) . Using a CNN - based sequenceto - sequence architecture ( Gehring et al , 2017 ) , ( Chollampatt and Ng , 2018 ) proposed the first end - to - end NMT model and reported the state - ofthe - art result . As Transformer ( Vaswani et al , 2017 ) plays an increasingly important role in sequence modeling , Transformer - based end - to - end NMT models began to lead the current GEC research Ge et al , 2018 ; Zhao et al , 2019 ) . It is worth mentioning that ( Lichtarge et al , 2019 ) used Wikipedia ed - its history corpus , which is huge but noisy , and gained a result very close to the state - of - the - art result . Learning a GEC translation model from noisy data is a worthy future direction as the GEC parallel corpus is expensive to obtain . This paper describes our two systems for the three tracks in the BEA - 2019 GEC Shared Task ( Bryant et al , 2019 ) . We use two popular NMT models and two improved versions of neural classification models to train the basic models . Ensemble strategies are then used to combine outcomes from different models . Our two systems for the three tracks are described in next section . In Section 3 , we evaluate the systems on the development data and show the final results on the test data . Section 4 concludes the paper and summarizes the future work .", "entities": [[0, 3, "TaskName", "Grammatical error correction"], [336, 337, "TaskName", "Classification"], [358, 360, "TaskName", "machine translation"], [417, 419, "TaskName", "machine translation"], [495, 496, "MethodName", "Transformer"], [512, 513, "MethodName", "Transformer"]]}
{"text": "We submitted the same system output for the Restricted and Unrestricted tasks . The system uses several ensemble methods to combine the CNNbased and Transformer - based translation models , described in details below .", "entities": [[8, 9, "DatasetName", "Restricted"], [10, 11, "DatasetName", "Unrestricted"], [24, 25, "MethodName", "Transformer"]]}
{"text": "Transformer is currently considered to be one of the most powerful models for sequence modeling . For GEC , some of the best recent results reported on CoNLL - 2014 test set are obtained by Transformer - based translation models . We trained eight Transformer - based translation models in a low resource translation paradigm . We tuned parameters for domain and error adaptation . We also compared the results using 2 GPUs and 4 GPUs as the authors reported the difference in their Github repository 1 .", "entities": [[0, 1, "MethodName", "Transformer"], [35, 36, "MethodName", "Transformer"], [44, 45, "MethodName", "Transformer"]]}
{"text": "For the Low Resource Track we developed different individual systems and used an ensemble method to combine them . For the translation model , we did not obtain very strong performance because the training data is limited . We also explored the noisy Wikipedia edit history corpus for the Transformer - based translation model . However , we noticed that , for some error types with clear definitions , the classifiers trained on a large amount of native corpus have good performance . In addition , we made some grammatical rules to correct errors and adopted an off - the - shelf spelling checker ( Kelly , 2006 ) . Finally , we leverage a sim - ple ensemble method to combine all of the classifiers , rules , spelling checker and translation models . Note that for the Restricted and Unrestricted tracks , we did not observe any gain from the classification models or the rule - based methods , therefore only the translation systems were used for those tracks .", "entities": [[49, 50, "MethodName", "Transformer"], [139, 140, "DatasetName", "Restricted"], [141, 142, "DatasetName", "Unrestricted"]]}
{"text": "After an analysis of the development sets , we decided to build classifiers for eight common error types . Based on ( Wang et al , 2017 ) , we developed two classification model structures for the eight error types . ( A ) Bi - GRU context model Figure 2 shows the bi - directional GRU context model we use to determine the right grammatical category for a target word . The concatenated left and right source states of the target word form the contextual semantic vector representation . This is used as a query to calculate the attention weight a t . An attention vector C t is then computed as the weighted average , according to a t , over all the source states . C t is then fed through a fully connected layer and softmax layer to produce the predictive distribution . We use this to train models for the following error types : Subject - verb agreement , Article , Plural or singular noun , Verb form , Preposition substitution , Missing comma and Period comma substitution . Labels for each task were extracted automatically from the native corpus through part - ofspeech tagging tools . ( B ) Pointer context model The classifiers above use the same classification labels for different target words . We also need a classification model to deal with the problem as in the Word form task , where each word has a different set of predictive labels ( as shown for word ' gone ' in Figure 3 ) . Inspired by the Pointer network model ( Vinyals et al , 2015 ) , we proposed the pointer context model . Figure 3 shows the pointer context model that takes the target word 's confusion set as the label candidates . The computation path is the same as the Bi - GRU model structure . We concatenate the target word 's char - based embedding and C t to obtain C 1 t , and then use it as the query to compute dot product a 1 t with each of the word embeddings in the confusion set . a 1 t is then fed through a softmax layer to produce the predictive distribution . This model is very effective at dealing with varying number of candidates as seen in the Word form task .", "entities": [[46, 47, "MethodName", "GRU"], [56, 57, "MethodName", "GRU"], [139, 140, "MethodName", "softmax"], [262, 263, "DatasetName", "Inspired"], [265, 267, "MethodName", "Pointer network"], [313, 314, "MethodName", "GRU"], [354, 356, "TaskName", "word embeddings"], [369, 370, "MethodName", "softmax"]]}
{"text": "We use the same Transformer - based translation model mentioned in Subsection 3.2.2 . Due to the limitation of the corpus , we leverage the Wiked ( Grundkiewicz and Junczys - Dowmunt , 2014 ) as our training corpus for the NMT model .", "entities": [[4, 5, "MethodName", "Transformer"]]}
{"text": "We use the conflict solver described above to do the ensemble for all of the outputs of the classifiers , rules , spell checker and NMT model . ( Mizumoto et al , 2012 ) , NUCLE ( Ng et al , 2014 , W&I+LOCNESS ( Bryant et al , 2019 ) and Common Crawl . We use Common Crawl to pretrain the decoder parameters for the Transformer - based translation model . FCE , Lang - 8 , NUCLE and W&I are used to train all of the translation models . It is worth noting that we did data augmentation for W&I to train all of the translation models . The data sets used in Low Resource Track include Wiked , Wikipedia Dumps and Common Crawl . All of the classifiers are trained on Wikipedia Dumps and the translation model is trained on Wiked corpus . For Wiked corpus , we did some data cleaning work . We discarded some noisy sentences that include error types such as U : OTHER , R : OTHER , R : NOUN , etc . The development set from W&I+LOCNESS are used in all the tracks . Following the data pre - processing pipeline used to generate the data provided by the shared task , we tokenize all of the data using spaCy 3 .", "entities": [[53, 55, "DatasetName", "Common Crawl"], [58, 60, "DatasetName", "Common Crawl"], [67, 68, "MethodName", "Transformer"], [73, 74, "DatasetName", "FCE"], [99, 101, "TaskName", "data augmentation"], [125, 127, "DatasetName", "Common Crawl"]]}
{"text": "We added the W&I corpus eight times to the training corpus for domain adaptation . Table 2 shows the performance of the single CNN - based translation models . All the parameters in Table 2 are tuned over the W&I+LOCNESS development set . Table 3 shows the results of the four CNN - based ensemble systems . We use ensembles in the same way as ( Chollampatt and Ng , 2018 ) . The above results prove that the ensemble method has yielded a very large improvement in this task .", "entities": [[12, 14, "TaskName", "domain adaptation"]]}
{"text": "Table 6 summarizes some results on the development set and gives the official test result . We can see that the individual CNN or Transformer - based translation models perform reasonably well , and the ensemble methods consistently outperform the individual systems . The second pass correction further improves the performance , and the last post - processing step boosts both recall and F 0.5 . Step", "entities": [[24, 25, "MethodName", "Transformer"]]}
{"text": "A Transformer - based translation model is trained on the filtered Wiked corpus . The model architecture follows that in . Although the performance of the NMT model is not strong , it provides good performance equivalent to the classifiers for some error types .", "entities": [[1, 2, "MethodName", "Transformer"]]}
{"text": "We have presented two different systems for the three GEC tracks . When there is a sufficient parallel learner corpus , such as in Restricted Track and Unrestricted Track , the NMT ensemble model is the best choice to implement a GEC system . We have evaluated two kinds of NMT models : CNN - based and Transformer - based translation models . We have also explored different ensemble strategies from multiple base mod - els to maximize the overall system performance . Finally we reached the result of F 0.5 = 0.6678 on the official test set in Restricted Track and Unrestricted Track , ranking the third in the Restricted track 4 . It is worth noting that there is a huge gap between the results on the development set and the test set , which suggests that there might be an unneglectable mismatch between the development set and the test set . Indeed , the development set is annotated by one annotator , while the test set is annotated by five , as announced officially . For Low Resource Track , there is a lack of parallel learner corpus , and thus we rely less on the translation models . We have built eight classifiers trained on Wikipedia dumps according to different error types and an NMT model trained on the Wikipedia edits history corpus . By a simple ensemble method , we reached F 0.5 = 0.5181 , placing our system in the third place in Low Resource Track . Although GEC has reached the human level performance on some GEC test sets , there is still room for improvement . In a low resource setup , how to deal with the huge but noisy data is worth exploring . ( Lichtarge et al , 2019 ) gave a good solution on this topic , but more work needs to be done . Second , we will investigate methods such as the reinforcement learning based method ( Wu et al , 2018 ) to address the mismatch between the training objectives and evaluation methods in GEC .", "entities": [[24, 25, "DatasetName", "Restricted"], [27, 28, "DatasetName", "Unrestricted"], [57, 58, "MethodName", "Transformer"], [99, 100, "DatasetName", "Restricted"], [102, 103, "DatasetName", "Unrestricted"], [110, 111, "DatasetName", "Restricted"]]}
{"text": "In this paper we present a new unsupervised approach , \" Attraction to Topics \" - A2 T , for the detection of argumentative units , a sub - task of argument mining . Motivated by the importance of topic identification in manual annotation , we examine whether topic modeling can be used for performing unsupervised detection of argumentative sentences , and to what extend topic modeling can be used to classify sentences as claims and premises . Preliminary evaluation results suggest that topic information can be successfully used for the detection of argumentative sentences , at least for corpora used in the evaluation . Our approach has been evaluated on two English corpora , the first of which contains 90 persuasive essays , while the second is a collection of 340 documents from user generated content .", "entities": [[31, 33, "TaskName", "argument mining"]]}
{"text": "Argument mining involves the automatic discovery of argument components ( i.e. claims , premises ) and the argumentative relations ( i.e. supports , attacks ) among these components in texts . Primarily aiming to extract arguments from texts in order to provide structured data for computational models of argument and reasoning engines ( Lippi and Torroni , 2015a ) , argument mining has additionally the potential to support applications in various research fields , such as opinion mining ( Goudas et al , 2015 ) , stance detection ( Hasan and Ng , 2014 ) , policy modelling ( Florou et al , 2013 ; Goudas et al , 2014 ) , legal information systems ( Palau and Moens , 2009 ) , etc . Argument mining is usually addressed as a pipeline of several sub - tasks . Typically the first sub - task is the separation between argumentative and non - argumentative text units , which can be performed at various granularity levels , from clauses to several sentences , usually depending on corpora characteristics . Detection of argumentative units ( AU ) 1 , as discussed in Section 2 , is typically modeled as a fully - supervised classification task , either a binary one , where units are separated in argumentative and non - argumentative ones with argumentative ones to be subsequently classified in claims and premises as a second step , or as a multi - class one , where identification of argumentative units and classification into claims and premises are performed as a single step . According to a recent survey ( Lippi and Torroni , 2015a ) , the performance of proposed approaches depends on highly engineered and sophisticated , manually constructed , features . However , fully - supervised approaches rely on manually annotated datasets , the construction of which is a laborious , costly , and error - prone process , requiring significant effort from human experts . At the same time , reliance on sophisticated features may hinder the generalisation of an approach to new corpora types and domains ( Lippi and Torroni , 2015a ) . The removal of manual supervision through exploitation of unsupervised approaches is a possible solution to both of the aforementioned problems .", "entities": [[0, 2, "TaskName", "Argument mining"], [60, 62, "TaskName", "argument mining"], [76, 78, "TaskName", "opinion mining"], [86, 88, "TaskName", "stance detection"], [125, 127, "TaskName", "Argument mining"]]}
{"text": "Topics seem to be related to the task of argument mining , at least for some types of corpora , as topic identification frequently appears as a step in the process of manual annotation of arguments in texts ( Stab and Gurevych , 2014a ) . However , despite its apparent importance in manual annotation , only a small number of studies have examined the inclusion of topic information in sub - tasks of argument mining . Habernal and Gurevych ( 2015 ) have included sentiment and topic information as features for classifying sentences as claims , premises , backing and non - argumentative units . A less direct exploitation of topic information has been presented in ( Nguyen and Litman , 2015 ) , where topics have been used to extract lexicons of argument and domain words , which can provide evidence regarding the existence of argument components . In this paper we propose \" Attraction to Topics \" - A2 T , an unsupervised approach based on topic modeling techniques for detecting argumentative discourse units at sentence - level granularity ( a sub - task known as \" argumentative sentence detection \" ) . The goals of A2 T are twofold . On the one side , A2 T enforces identification of sentences that contain argument components , by also distinguishing them from the non - argumentative sentences that do not contain argument components . On the other side , A2 T classifies the discovered argumentative sentences according to their role , as major claims , claims , and premises . The rest of the paper is organized as follows : Section 2 presents an overview of approaches related to argument mining focusing on the detection of argumentative units , while Section 3 presents our approach on applying topic modeling for identifying sentences that contain argument components . Section 4 presents our experimental setting and evaluation results , with Section 5 concluding this paper and proposing some directions for further research .", "entities": [[9, 11, "TaskName", "argument mining"], [74, 76, "TaskName", "argument mining"], [282, 284, "TaskName", "argument mining"]]}
{"text": "UDapter : Language Adaptation for Truly Universal Dependency Parsing", "entities": [[7, 9, "TaskName", "Dependency Parsing"]]}
{"text": "Recent advances in multilingual dependency parsing have brought the idea of a truly universal parser closer to reality . However , crosslanguage interference and restrained model capacity remain major obstacles . To address this , we propose a novel multilingual task adaptation approach based on contextual parameter generation and adapter modules . This approach enables to learn adapters via language embeddings while sharing model parameters across languages . It also allows for an easy but effective integration of existing linguistic typology features into the parsing network . The resulting parser , UDapter , outperforms strong monolingual and multilingual baselines on the majority of both high - resource and lowresource ( zero - shot ) languages , showing the success of the proposed adaptation approach . Our in - depth analyses show that soft parameter sharing via typological features is key to this success . 1", "entities": [[4, 6, "TaskName", "dependency parsing"]]}
{"text": "In this section , we present our truly universal dependency parser , UDapter . UDapter consists of a biaffine attention layer stacked on top of the pretrained Transformer encoder ( mBERT ) . This is similar to ( Wu and Dredze , 2019 ; Kondratyuk and Straka , 2019 ) , except that our mBERT layers are interleaved with special adapter layers inspired by Houlsby et al ( 2019 ) . While mBERT weights are frozen , biaffine attention and adapter layer weights are generated by a contextual parameter generator ( Platanios et al , 2018 ) that takes a language embedding as input and is updated while training on the treebanks . Note that the proposed adaptation approach is not restricted to dependency parsing and is in principle applicable to a range of multilingual NLP tasks . We will now describe the components of our model .", "entities": [[27, 28, "MethodName", "Transformer"], [30, 31, "MethodName", "mBERT"], [54, 55, "MethodName", "mBERT"], [72, 73, "MethodName", "mBERT"], [123, 125, "TaskName", "dependency parsing"], [134, 136, "TaskName", "multilingual NLP"]]}
{"text": "The top layer of UDapter is a graph - based biaffine attention parser proposed by Dozat and Manning ( 2017 ) . In this model , an encoder generates an internal representation r i for each word ; the decoder takes r i and passes it through separate feedforward layers ( MLP ) , and finally uses deep biaffine attention to score arcs connecting a head and a tail : h ( head ) i = MLP ( head ) ( r i ) ( 1 ) h ( tail ) i = MLP ( tail ) ( r i ) ( 2 ) s ( arc ) = Biaffine ( H ( head ) , H ( tail ) ) ( 3 ) Similarly , label scores are calculated by using a biaffine classifier over two separate feedforward layers . Finally , the Chu - Liu / Edmonds algorithm ( Chu , 1965 ; Edmonds , 1967 ) is used to find the highest scoring valid dependency tree .", "entities": [[51, 52, "DatasetName", "MLP"], [76, 77, "DatasetName", "MLP"], [93, 94, "DatasetName", "MLP"]]}
{"text": "Overall , UDapter outperforms the monolingual and multilingual baselines on both high - resource and zero - shot languages . Below , we elaborate on the detailed results . High - resource Languages Labelled Attachement Scores ( LAS ) on the high - resource set are given in Table 1 . UDapter consistently outperforms both our monolingual and multilingual baselines in all languages , and beats the previous work , setting a new state of the art , in 9 out of 13 languages . Statistical significance testing 8 applied between UDapter and multi / mono - udify confirms that UDapter 's performance is significantly better than the baselines in 11 out of 13 languages ( all except en and it ) . Among directly comparable baselines , multiudify gives the worst performance in the typologically diverse high - resource setting . This multilingual model is clearly worse than its monolingually trained counterparts mono - udify : 83.0 vs 86.0 . This result resounds with previous findings in multilingual NMT ( Arivazhagan et al , 2019 ) and highlights the importance of language adaptation even when using high - quality sentence representations like those produced by mBERT . To understand the relevance of adapters , we also evaluate a model which has almost the same architecture as multi - udify except for the adapter modules and the tuning choice ( frozen mBERT weights ) . Interestingly , this adapter - only model considerably outperforms multi - udify ( 85.0 vs 83.0 ) , indicating that adapter modules are also effective in multilingual scenarios . Finally , UDapter achieves the overall best results , with consistent gains over both multi - udify and adapter - only , showing the importance of linguistically informed adaptation even for in - training languages . Low - Resource Languages Average LAS on the 30 low - resource languages are shown in column lr - avg of Table 1 . Overall , UDapter slightly outperforms the multi - udify baseline ( 36.5 vs 36.3 ) , which shows the benefits of our approach on both in - training and zero - shot languages . For a closer look , Table 2 provides individual results for the 18 representative languages in our low - resource set . Here we find a mixed picture : UDapter outperforms multi - udify on 13 out of 18 languages 9 . Achieving improvements in the zero - shot parsing 9 LAS scores for all 30 languages are given in Appendix A.2 . By significance testing , UDapter is significantly better than multi - udify on 16/30 low - resource languages , which is shown in setup is very difficult , thus we believe this result is an important step towards overcoming the problem of positive / negative transfer trade - off . Indeed , UDapter - proxy results show that choosing a proxy language embedding from the same language family underperforms UDapter , apart from not being available for many languages . This indicates the importance of typological features in our approach ( see 5.2 for further analysis ) .", "entities": [[196, 197, "MethodName", "mBERT"], [231, 232, "MethodName", "mBERT"]]}
{"text": "Figure 2 presents the LAS gain of UDapter over the multi - udify baseline for each high - resource language along with the respective treebank training size . To summarize , the gains are higher for languages with less training data . This suggests that in UDapter , useful knowledge is shared among intraining languages , which benefits low resource languages without hurting high resource ones . For zero - shot languages , the difference between the two models is small compared to high - resource languages ( +1.2 LAS ) . While it is harder to find a trend here , we notice that UDapter is typically beneficial for the languages not present in the mBERT training corpus : it outperforms multi - udify in 13 out of 22 ( non - mBERT ) languages . This suggests that typological feature - based adaptation leads to improved sentence representations when the pretrained encoder has not been exposed to a language .", "entities": [[116, 117, "MethodName", "mBERT"], [133, 134, "MethodName", "mBERT"]]}
{"text": "Table 4 shows LAS scores on all 30 low - resouce languages for UDapter , original UDify ( Kondratyuk and Straka , 2019 ) , and re - trained ' multiudify ' . Languages with ' * ' are not included in mBERT training data . Note that original UDify is trained on all available UD treebanks from 75 languages . For the zero - shot languages , we obtained original UDify scores by running the pre - trained model .", "entities": [[42, 43, "MethodName", "mBERT"], [55, 56, "DatasetName", "UD"]]}
{"text": "CompLx@SMM4H'22 : In - domain pretrained language models for detection of adverse drug reaction mentions in English tweets", "entities": [[5, 8, "TaskName", "pretrained language models"]]}
{"text": "The paper describes the system that team CompLx developed for sub - task 1a of the Social Media Mining for Health 2022 ( # SMM4H ) Shared Task . We finetune a RoBERTa model , a pretrained , transformer - based language model , on a provided dataset to classify English tweets for mentions of Adverse Drug Reactions ( ADRs ) , i.e. negative side effects related to medication intake . With only a simple finetuning , our approach achieves competitive results , significantly outperforming the average score across submitted systems . We make the model checkpoints 1 and code 2 publicly available . We also create a web application 3 to provide a userfriendly , readily accessible interface for anyone interested in exploring the model 's capabilities .", "entities": [[24, 25, "DatasetName", "SMM4H"], [32, 33, "MethodName", "RoBERTa"]]}
{"text": "The Shared Task ( Weissenbacher et al , 2022 ) of the 2022 Social Media Mining for Health Applications ( # SMM4H ) workshop proposed ten sub - tasks in the domain of social media mining for health monitoring and surveillance . From the perspective of Natural Language Processing ( NLP ) , these tasks present a considerable challenge since the nature of social media posts requires dealing with both a significant level of language variation ( informal and colloquial expressions , ambiguity , multilingual posts ) and data sparsity , as well as a widespread presence of noise such as misspellings of clinical concepts and syntactic errors . In the 2022 instantiation of the # SMM4H Shared Task , our team participated in : ( i ) sub - task 1a , the classification of English tweets containing mentions of Adverse Drug Reactions ( ADRs ) ( Magge et al , 2021 ) , ( ii ) sub - task 3 , the classification of English tweets ( 3a ) and WebMD reviews ( 3b ) contain - 1 https://huggingface.co/orestxherija/roberta - base - adr - smm4h2022 2 https://github.com/orestxherija/CompLx - SMM4H2022 3 https://huggingface.co/spaces/orestxherija/adr - mentionclassifier ing mentions of changes in medication treatments , and ( iii ) sub - task 8 , the classification of English tweets self - reporting chronic stress . In this paper we primarily describe our approach for task 1a , as that constituted the major focus of our efforts . To address these challenges , we finetune a variant of a RoBERTa ( Liu et al , 2019 ) model , a transformer - based ( Vaswani et al , 2017 ) language model pretrained on approximately 128 million tweets ( Loureiro et al , 2022 ) on each sub - task 's provided dataset . Without any domain adaptation efforts ( apart from standard finetuning on the downstream task ) or hyperparameter optimizations , the model outperforms the average of all submissions for sub - task 1a by a 9 % absolute difference in F1score . In the following sections , we introduce the subtasks ' datasets , describe the model architecture and training setup , report our results , and conclude with a discussion of related research and potential avenues for future work .", "entities": [[21, 22, "DatasetName", "SMM4H"], [116, 117, "DatasetName", "SMM4H"], [256, 257, "MethodName", "RoBERTa"], [303, 305, "TaskName", "domain adaptation"]]}
{"text": "Label vyvanse make me so hyper and creative and i think of so many tweets ADR feed an ocd vyvanse and cover him in crayons No ADR trazodone has screwed up my sleep schedule . its helping tho . ADR on medication - related keywords for label assignment is going to be problematic : both the first and the second example contain the medication term \" vyvanse \" but they have been assigned different labels , \" ADR \" and \" No ADR \" respectively . This motivates the use of a modeling approach that leverages the overall semantic content of the sentence , rather than keyword matching with individual constituents . 3 Modeling Approach", "entities": [[18, 19, "DatasetName", "ocd"]]}
{"text": "The establishment of language modeling as the pretraining step in the transfer learning pipeline revolutionized modern NLP with models such as ULMFiT ( Howard and Ruder , 2018 ) , ELMo ( Peters et al , 2018 ) and , most notably , transformerbased language models such as GPT ( Radford et al , 2018 ) and BERT ( Devlin et al , 2019 ) . In recent years , there have been intensive efforts in the research community to produce ever - larger transformer - based pretrained language models that are trained using a variety of datasets , transformermodel architectures , training objectives and optimization techniques . This should come as no surprise , since such language models have dominated virtually all NLP leaderboards , most notably GLUE ( Wang et al , 2018 ) and SuperGLUE ( Wang et al , 2019 ) . Considering this overwhelming success , we opt for a RoBERTa ( Liu et al , 2019 ) model 4 that has been trained on approximately 128 million tweets ( Loureiro et al , 2022 ) . Our exact modeling approach is depicted in Figure [ 1 ] . We opt for a model that has been trained on an in - domain corpus , namely tweets , as transfer learning has been shown to yield improved results when there is indomain pretraining ( Gururangan et al , 2020 ) . We do not use any text normalization steps . 4 https://huggingface.co/cardiffnlp/twitter - roberta - base - mar2022", "entities": [[11, 13, "TaskName", "transfer learning"], [21, 22, "MethodName", "ULMFiT"], [30, 31, "MethodName", "ELMo"], [48, 49, "MethodName", "GPT"], [57, 58, "MethodName", "BERT"], [87, 90, "TaskName", "pretrained language models"], [128, 129, "DatasetName", "GLUE"], [137, 138, "DatasetName", "SuperGLUE"], [155, 156, "MethodName", "RoBERTa"], [214, 216, "TaskName", "transfer learning"]]}
{"text": "Clause - Wise and Recursive Decoding for Complex and Cross - Domain Text - to - SQL Generation", "entities": [[12, 17, "TaskName", "Text - to - SQL"]]}
{"text": "Our work is related to the grammar - based constrained decoding approaches for semantic parsing ( Yin and Neubig , 2017 ; Rabinovich et al , 2017 ; Iyer et al , 2018 ) . While their approaches are focused on general purpose code generation , we instead focus on SQL - specific grammar to address the text - to - SQL task . Our task differs from code generation in two aspects . First , it takes a database schema as an input in addition to natural language . To predict SQL correctly , a model should fully understand the relationship between the question and the schema . Second , as SQL is a non - procedural language , predictions of SQL clauses do not need to be done sequentially . For text - to - SQL generation , several SQLspecific approaches have been proposed ( Zhong et al , 2017 ; Xu et al , 2017 ; Huang et al , 2018 ; Yu et al , 2018a ; Dong and Lapata , 2018 ; Yavuz et al , 2018 ) based on WikiSQL dataset ( Zhong et al , 2017 ) . However , all of them are limited to the specific WikiSQL SQL sketch , which only supports very simple queries . It includes only the SELECT and WHERE clauses , only a single expression in the SELECT clause , and works only for a single table . To predict more complex SQL queries , sequence - to - sequence ( Iyer et al , 2017 ; Finegan - Dollak et al , 2018 ) and template - based ( Finegan - Dollak et al , 2018 ; Lee et al , 2019 ) approaches have been proposed . However , they focused only on specific databases such as ATIS ( Price , 1990 ) and GeoQuery ( Zelle and Mooney , 1996 ) . Because they only considered question and SQL pairs without requiring an understanding of database schema , their approaches can not generalize to unseen databases . SyntaxSQLNet ( Yu et al , 2018b ) is the first and state - of - the - art model for the Spider ( Yu et al , 2018c ) , a complex and cross - domain text - to - SQL task . They proposed an SQL specific syntax tree - based decoder with SQL generation history . Our approach differs from their model in the following aspects . First , taking into account that SQL corresponds to non - procedural language , we develop a clause - specific decoder for each SQL clause , where SyntaxSQLNet predicts SQL tokens sequentially . For example , in SyntaxSQL - Net , a single column prediction module works both in the SELECT and WHERE clauses , depending on the SQL decoding history . In contrast , we define and train decoding modules separately for each SQL clause to fully utilize clausedependent context . Second , we apply sequenceto - sequence architecture to predict columns instead of using the sequence - to - set framework from SyntaxSQLNet , because correct ordering is essential for the GROUP BY and ORDER BY clauses . Finally , we introduce a self - attention mechanism ( Lin et al , 2017 ) to efficiently encode database schema , which includes multiple tables .", "entities": [[13, 15, "TaskName", "semantic parsing"], [43, 45, "TaskName", "code generation"], [57, 62, "TaskName", "text - to - SQL"], [68, 70, "TaskName", "code generation"], [133, 138, "TaskName", "text - to - SQL"], [185, 186, "DatasetName", "WikiSQL"], [205, 206, "DatasetName", "WikiSQL"], [303, 304, "DatasetName", "ATIS"], [381, 386, "TaskName", "text - to - SQL"]]}
{"text": "In this paper , we propose a recursive and SQL clause - wise decoding neural architecture to address the complex and cross - domain text - to - SQL task . We evaluate our model with the Spider dataset , and the experimental result shows that our model significantly outperforms previous work for generating not only simple queries , but also complex and nested queries . ( Yu et al , 2018b ) , and modified SQLNet ( Xu et al , 2017 ) by Yu et al ( 2018c ) , respectively .", "entities": [[24, 29, "TaskName", "text - to - SQL"]]}
{"text": "Previous work on readability has classified or ranked texts based on document - level measures such as word length , sentence length , number of different phrasal categories & parse tree depth ( Petersen , 2007 ) , and discourse coherence ( Graesser et al , 2004 ) , inter alia . However , not all applications that need readability ratings deal with long documents . For many applications in text simplification , computer - aided language learning ( CALL ) systems , authorship tools , translation , and information retrieval , sentence - level readability metrics are direly needed . For instance , an automatic text simplification system must begin by asking which portions of a text need to be simplified . To this end , a measure that can assign ratings on a sentence - by - sentence level can help target simplification only to those sentences which need it most , and such measures also serve to confirm that the resulting ' simplified ' sentence is in fact simpler than the original sentence . Similarly , CALL and other pedagogical systems will benefit if it is possible to predict which portions of a text will be harder for students . Authorship tools can offer more specific editorial advice when they know why individual sentences can cause difficulties for readers . Translation tools can aim to preserve not just meaning but also the approximate difficulty of the sentences they are translating or use a sentence - level difficulty metric to target output that is easier to understand . Furthermore , information retrieval systems also benefit when they can return not merely relevant texts , but also texts appropriate to the reading level of the user . Recently there has been an increased interest in sentential models of text difficulty in the automatic text simplification and summarization communities in particular ( Vajjala and Meurers , 2014 ; Macdonald and Siddharthan , 2016 ) . One area that has produced a lot of research on sentence level processing difficulty is psycholinguistics . Over the past three decades , a number of theories of human sentence processing ( i.e. reading ) have been proposed and validated in a large variety of experimental studies . The most important sentence processing theories have furthermore been implemented based on broad - coverage tools , so that estimates for arbitrary sentences can be generated automatically . For example , eyetracking studies of reading times on a large corpus of newspaper text have found that measures such as integration cost and surprisal provide partial explanations for subjects ' reading behavior ( Demberg and Keller , 2008 ) . This paper leverages these implemented measures based on psycholinguistic theories of sen - tence processing in order to test whether they can help to more accurately score individual sentences with respect to their difficulty . In the process , we evaluate the contributions of the individual features to our models , testing their utility in examining fine - grained distinctions in sentence difficulty . Section 2 reviews the literature on readability in general before we shift to psycholinguistic theories of sentence processing in Section 4 . In Section 5 we discuss our methods , including the corpora used , how features were extracted , and the set up for our averaged perceptron models . Section 6 presents our findings which we connect to related work on sentence - level readability models in 3 . Finally we offer our conclusions and suggestions for future work in Section 7 . 2 Readability Chall 's ( 1958 ) comprehensive review of readability research in the first half of the 20 th century divides the early work in readability into \" survey and experimental studies \" and \" quantitative associational studies \" . Studies of the former category took place during the 1930s and 1940s and included surveys of expert and reader opinion as well as experimental studies which manipulated texts according to one variable at a time in order to determine the effects of those variables on readers . The results of these studies suggest that , once you have managed to control for reader interest in the content of a text , the most important factor with respect to its readability is its ' style ' , e.g. its \" scope of vocabulary and ... kinds of sentences \" ( Gray andLeary , 1935 , as quoted in ( Chall , 1958 ) ) . Our study belongs to the second class , relating the features of a text to its ordering relative to some other texts . The earliest work in this direction was by L. A. Sherman , who proposed a quantitative analysis of text difficulty based on the number of clauses per sentence , among other features ( Sherman , 1893 ) . Where Sherman 's pedagogical focus was on literature , Lively & Pressey ( 1923 ) focused on vocabulary as a bottleneck in science education . Work in this vein led to the development of a number of readability formulae in the mid - 20 th century 1 , including the familiar Flesch - Kincaid Grade - Level score ( Kincaid et al , 1975 ) . These formulae typically use a linear combination of average word length and average sentence length , though some also incorporate a vocabulary - diversity term . The simple , twofeature versions of these models are still widely used , and inspired our BASELINE model . More recently , Petersen ( 2007 ) sought to apply familiar natural language processing techniques to the problem of identifying text difficulty for nonnative readers . In particular , she used a number of parse - based features which captured , for example , the average number of noun and verb phrases per sentence and the height of the parse tree . Petersen trained SVM classifiers to classify texts as belonging to one of four primary school grade levels based on the Weekly Reader educational newspaper 2 . These document - level models achieved Fscores in the range of 0.5 to 0.7 , compared to the F - scores between 0.25 and 0.45 achieved by the Flesch - Kincaid Reading Ease score for the same texts . Recent work has also looked at features related to discourse and working memory constraints . Feng et al ( 2009 ) worked on a model of readability for adults with intellectual disabilities . Considering working memory constraints , they extracted features related to the number of entities mentioned in a document and the ' lexical chains ' ( Galley and McKeown , 2003 ) that connected them . They found that their features resulted in a better correlation ( Pearson 's r = \u22120.352 ) compared to both Flesch - Kincaid score ( r = \u22120.270 ) and a number of ' basic ' linguistic features based on those used by Petersen & Ostendorf ( 2009 ) ( r = \u22120.283 ) . 3 Coh - Metrix ( Graesser et al , 2004 ) also includes a number of measures related to discourse coherence , for example . Such features are not suited to the problem of determining the difficulty of sentences in isolation , but they have also been shown to better predict readability for second - language learners compared to ' traditional ' readability measures like those described above ( Crossley et al , 2011 ) .", "entities": [[70, 72, "TaskName", "text simplification"], [89, 91, "TaskName", "information retrieval"], [106, 108, "TaskName", "text simplification"], [223, 224, "TaskName", "Translation"], [262, 264, "TaskName", "information retrieval"], [304, 306, "TaskName", "text simplification"], [307, 308, "TaskName", "summarization"], [981, 982, "MethodName", "SVM"], [1170, 1171, "MethodName", "Metrix"]]}
{"text": "For our purposes , we focus on readability as reading ease and on linguistic constraints in particular , rather than constraints of medium ( relating to e.g. legibility ) , reader interest , or comprehensibility . Without directly modeling comprehensibility , we assume that making material easier to read will also make it easier to comprehend . Here we focus on four psycholinguistic theories of human sentence processing : idea density , surprisal , integration cost , and embedding depth . Kintsch ( 1972 ) defined propositional idea density as the ratio of propositions or ideas to words in the sentences . 4 Keenan & Kintsch conducted two different experiments in order to examine free reading behavior as well as subjects ' performance in speeded reading conditions . They found that \" the number of propositions [ in a text ] had a large effect upon reading times , [ but ] it could only account for 21 % of their variance \" when subjects were allowed to read freely . Subjects ' overall recall was worse for more dense texts in the speeded reading condition . In addition to effects of idea density , they found that propositions which were presented as surface - form modifiers ( as opposed to , e.g. , main verbs ) were \" very poorly recalled \" and that propositions playing a subordinate role relative to another proposition were also less - well recalled . Finally , propositions involving a proper name were generally recalled better than similar propositions involving , e.g. , a common noun . While Kintsch & Keenan ( 1973 ) looked at the influence of propositional idea density on reading times and recall for both individual sentences as well as short paragraphs , work since the 1970s has been limited to the level of multiple sentences and used primarily as an indicator of cognitive deficits ( Ferguson et al , 2014 ; Bryant et al , 2013 ; Farias et al , 2012 ; Riley et al , 2005 ) . This paper returns to the examination of idea density 's applicability for individual sentences . Surprisal , on the other hand , has been widely examined in theories of language comprehension at a variety of levels , including the word - and sentence - levels . Surprisal is another word for Shannon ( 1948 ) information , operationalized in linguistics as the probability of the current word conditioned on the preceding sequence of words : surprisal ( w n ) = \u2212log ( P ( w n | w 1 . . . w n\u22121 ) ) ( 1 ) where w i is the i th word in the sentence and P ( w 1 . . . w i ) denotes the probability of the sequence of i words w 1 . . . w i . One reason psycholinguists consider surprisal as a factor in sentence processing difficulty is that it makes sense in a model of language users as rational learners . Levy ( 2008 ) argues the rational reader 's attention must be spread across all possible analyses for the sentence being observed . Based on prior experience , the reader expects some analyses to be more probable than others and therefore allocates more resources to those analyses . In this analysis , surprisal is derived as a measure of the cost paid when the reader misallocates resources : when a new word invalidates a highly probable analysis , the reader has effectively ' wasted ' whatever resources were allocated to that analysis . The notion of surprisal is also used in theories of language production , see the Uniform Information Density hypothesis ( Jaeger , 2006 ; Levy and Jaeger , 2007 ; Jaeger , 2010 , UID ) . While surprisal focuses on predictability effects in sentence processing , Gibson 's ( 1998 ; 2000 ) Dependency Locality Theory ( DLT ) focuses on the memory cost of recalling referents and integrating new ones into a mental representation . DLT proposes that the the distance between syntactic heads and dependents , measured by the number of intervening discourse referents , approximates the difficulty that the listener or reader will have integrating the two units . This model maintains that the act of creating a new discourse referent and holding it in memory makes it more difficult to recall a previous discourse referent and connect that discourse referent to the current one . 5 In addition to integration cost , DLT proposes a storage cost associated with the number of open dependencies that must be maintained in memory . The notion of connected components in van Schijndel et al 's ( 2012 ; incremental parsing model picks up this idea . Related models were also suggested earlier by Yngve ( 1960 ) and Miller 's ( 1956a ; 1956b ) whose work was based on results showing that human working memory is limited to 7 \u00b1 2 items . Yngve 's mechanistic , incremental model of language production considered the evaluation of phrase structure grammars ( PSGs ) in a system with finite memory , exploring the structure speakers must keep track of during production and how grammars might be structured to avoid overtaxing working memory . Van Schijndel et al develop this idea further in the context of a hierarchical sequence model of parsing . In this incremental model of parsing , at each stage the reader has an active state ( e.g. S for sentence ) and an awaited state ( e.g. VP for verb phrase ) . 6 At each new word , the parser must decide between continuing to analyze the current connected component or hypothesizing the start of a new one . 7 These measures provide an idealized representation of the number of different states a human parser must keep track of at any point in time . We refer to this number of states as the embedding depth of a sentence at a particular word , and the ModelBlocks parser of van Schijndel et al ( 2012 ) calculates this number of states averaged over the beam of currently plausible parses . Also of interest is the embedding difference , which is the embedding depth at the present word relative to the previous word , elaborated upon in the following example . Consider the state described above ( i.e. that of being in the active state S and awaiting state VP ) might be reached after a reader has observed a noun phrase , resulting in the state S / VP . This means that the word sequence observed so far will be consistent with a sentence if the reader now observes a verb phrase . If , however , the next word in the input is inconsistent with the start of a verb phrase ( e.g. the relative clause marker that ) , then this parse will be ruled out and another must be considered . At this point the parser must hypothesize the beginning of a new connected component , i.e. a new syntactic substructure that must be completed before continuing to parse the top - level of the sentence . Therefore , the parser must now keep track of two states : ( 1 ) the fact that we are still looking for a VP to complete the overall sentence ; and ( 2 ) the fact that we now have a relative clause to parse before we can complete the current NP . In this example , we are at embedding depth 1 or 0 up until we encounter the word that , which increases the embedding depth by 1 , resulting in a nonzero embedding difference score .", "entities": [[1271, 1272, "DatasetName", "0"]]}
{"text": "We used two corpora in this work . The English and Simple English Wikipedia corpus of Hwang et al ( 2015 , ESEW ) is a new corpus of more than 150k sentence pairs designed to address the flaws of the Parallel Wikipedia Corpus of Zhu et al ( 2010 , PWKP ) , which was previously dominant in work on text simplification , by using a more sophisticated method of aligning pairs of English and Simple English sentences . We used the section labeled as having ' good ' alignments for our work and assumed that , in every sentence pair , the Simple English sentence should be ranked as easier than the English sentence ( rank=1 < rank=2 in Table 1 ) . This provides a large corpus with noisy labels , as there are likely to be instances where the English and Simple English sentences are not substantially different or the English sentence is the easier one . 8 For a more controlled corpus , we use Vajjala 's ( 2015 ) One Stop English ( OSE ) corpus . This corpus consists of 1577 sentence triples , drawn from news stories edited to three difficulty levels : elementary , intermediate , and advanced . Vajjala used T F * IDF and cosine similarity scores to align sentences from stories drawn from onestopenglish.com . While One Stop English does not publish an explanation of their methods for creating these texts , they are at least created by human editors for pedagogical purposes , so the labels should be more consistent and reliable than those associated with the ESEW corpus . The three levels of OSE make it possible to compare system performance on sentence pairs which are close to one another in difficulty ( e.g. ' advanced ' versus ' intermediate ' sentences ) with performance on pairs which are further apart , as with ' advanced ' sentences paired with their ' elementary ' counterparts . In this paper we will refer to the pairs of advanced and elementary sentences as OSE f ar , the remaining pairs as OSE near , and the full OSE dataset as OSE all . An example triple of sentences from the corpus is given in Table 2 .", "entities": [[61, 63, "TaskName", "text simplification"]]}
{"text": "We used two parsers to extract 22 features from the corpora . The ModelBlocks parser provided features based on surprisal and embedding depth while the Stanford parser 9 provided the dependency parses used to calculate integration cost and idea density features . Both parsers are trained and perform near the state of the art on the standard sections of the Wall Street Journal section of the Penn Treebank . From ModelBlocks ' complexity feature extraction mode , we took the lexical and syntactic surprisal features . We used the average lexical surprisal and average syntactic surprisal as idealized measures of the channel capacity required to read a sentence . While this underestimates the channel capacity required to process a sentence , it is at least internally consistent , insofar as a sentence with higher average surprisal overall is likely to require a higher channel capacity as well . We also used the maximum of each form of surprisal as a measure of the maximum demand on cognitive resources . These features comprise the SURPRISAL model . We also calculated average and maximum values for the embedding depth and embedding difference output from ModelBlocks . The average provides an estimate of the typical memory load throughout a sentence , while the ( absolute ) embedding difference is a measure of how many times a reader needs to push or pop a connected component to or from their memory store . These features comprise the EMBEDDING model . To extract the remaining features , we first ran the Stanford dependency parser on both corpora . The program icy - parses uses part - of - speech tags and head - dependent relations to determine the total , average , and maximum integration cost across a sentence . Here average integration cost functions as another kind of memory load estimate while the maximum value models the most - Rank Sentence 2 Gingerbread was brought to Europe in 992 by the Armenian monk Gregory of Nicopolis - LRB - Gregory Makar - RRB - - LRB - Gr\u00e9goire de Nicopolis - RRB - . 1 Armenian monk Gregory of Nicopolis - LRB - Gregory Makar - RRB - - LRB - Grgoire de Nicopolis - RRB - brought ginger bread to Europe in 992 . Table 1 : Example sentences from English ( 2 ) and Simple ( 1 ) English Wikipedia .", "entities": [[66, 68, "DatasetName", "Penn Treebank"], [269, 272, "DatasetName", "part - of"]]}
{"text": "In order to rank sentences , we need some way of generating a complexity score for each sentence . Using a perceptron model allows us to train a simple linear scoring model by converting the ranking task into a classification task . Suppose we have two sentences s 1 and s 2 with feature vectors s 1 and s 2 such that s 1 is more complex than s 2 . Then we want to train a perceptron model such that score ( s 1 ) > score ( s 2 ) ( 2 ) W s 1 > W s 2 ( 3 ) W ( s 1 \u2212 s 2 ) > 0 ( 4 ) We refer to the vector s 1 \u2212 s 2 as a vector of difference features . In order to train the model , we take all pairs of sentences present in a given corpus and create a difference vector as above . In half of the cases , we flip the sign of the difference vector , creating a binary classification task with balanced classes . The learning problem is now to classify each difference vector based on whether the first term in the difference was the ' easier ' or the ' harder ' sentence Note that the benefit to this approach is that the resulting weight vector W learned via the classification task can be used directly to score individual sentences as well , with the expectation that higher scores will correspond to more difficult sentences . We use an averaged perceptron model ( Collins , 2002 ) implemented in Python as our classifier .", "entities": [[114, 115, "DatasetName", "0"]]}
{"text": "Narrated instructional videos provide rich visual , acoustic and language information for people to easily understand how to complete a task by procedures . An increasing amount of people resort to narrated instructional videos to learn skills and solve problems . For example , people would like to watch videos to repair a water damaged plasterboard / drywall ceiling 1 or cook Cottage Pie 2 . This motivates us to investigate whether machines can understand narrated instructional videos like In this task , the video frames and the transcript are given to ( 1 ) extract procedures in the video , ( 2 ) generate a descriptive and informative sentence as the caption of each procedure . humans . Besides , watching a long video is timeconsuming , captions of videos provide a quick overview of video content for people to learn the main steps rapidly . Inspired by this , our task is to generate procedure captions from narrated instructional videos which are a sequence of step - wise clips with a description as shown in Figure 1 . Previous works on video understanding tend to recognize actions in video clips by detecting pose ( Wang et al , 2013a ; Packer et al , 2012 ) and motion ( Wang et al , 2013b ; Yang et al , 2013 ) or both ( Wang et al , 2014 ) and fine - grained features . These works take low - level vision features into account and can only detect human actions , instead of complicated events that occur in the scene . To deeply understand the video content , Video Dense Captioning ( Krishna et al , 2017 ) is proposed to generate semantic captions for a video . The goal of this task is to identify all events inside a video and our target is the video dense captioning on narrated instructional videos which we call dense procedure captioning . Different from videos in the open domain , instructional videos contain an explicit sequential structure of procedures accompanied by a series of shots and descriptive transcripts . Moreover , they contain fine - grained information including actions , entities , and their interactions . According to our analysis , many fine - grained entities and actions also present in captions which are ignored by previous works like ( Krishna et al , 2017 ; Zhou et al , 2018b ) . The procedure caption should be detailed and informative . Previous works ( Krishna et al , 2017 ; Xu et al , 2016 ) for video captioning usually consist of two stages : ( 1 ) temporal event proposition ; and ( 2 ) event captioning . However , there are two challenges for narrated instructional videos : one of the challenges is that video content fails to provide semantic information so as to extract procedures semantically ; the other challenge is that it is hard to recognize fine - grained entities from the video content only , and thus tends to generate coarse captions . Previous models for dense video captioning only use video signals without considering transcripts . We argue that transcripts in narrated instructional videos can enhance video representation by providing fine - grained complimentary and semantic textual information . As shown in Figure 1 , the task takes a video with a transcript as input and extracts the main procedures as well as these captions . The whole video is divided into four proposal procedure spans in sequential order including : ( 1 ) grate some pecorino cheese and beat the eggs during time span [ 0:00:12 - 0:00:46 ] , ( 2 ) then stir cheese into the eggs during [ 0:00:52 - 0:01:10 ] , and so on . Besides video content , transcripts can provide semantic information . Our model embeds transcript using a pre - trained context - aware model to provide rich semantic information . Furthermore , with the transcript , our model can directly \" copy \" many fine - grained entities , e.g. pecorino cheese for procedure captioning . In this paper , we propose utilizing multi - modal content of videos including frame features and transcripts to conduct procedure extraction and captioning . First , we use the transcript of instructional videos as a global text feature and fuse it with video signals to construct context - aware features . Then we use temporal convolution to encode these features and generate procedure proposals . Next , the fused features of video and transcript tokens within the proposed time span are used to generate the final caption via a recurrent model . Experiments on the YouCookII dataset ( Zhou et al , 2018a ) ( a cooking - domain instructional video corpus ) are conducted to show that our model can achieve state - of - the - art results and the ablation studies demonstrate that the transcript can not only improve procedure proposition performance but also be very effective for procedure captioning . The contributions of this paper are as follows : 1 . We propose a model fusing transcript of narrated instructional video during procedure extraction and captioning . 2 . We employ the pre - trained BERT ( Devlin et al , 2018 ) and self - attention ( Vaswani et al , 2017 ) layer to embed transcript , and then integrate them to visual encoding during procedure extraction . 3 . We adopt the sequence - to - sequence model to generate captions by merging tokens of the transcript with the aligned video frames .", "entities": [[147, 148, "DatasetName", "Inspired"], [183, 185, "TaskName", "video understanding"], [430, 432, "TaskName", "video captioning"], [514, 517, "TaskName", "dense video captioning"], [741, 742, "MethodName", "convolution"], [875, 876, "MethodName", "BERT"]]}
{"text": "Narrated Instructional Video Understanding Previous works aim to ground the description to the video . ( Malmaud et al , 2015 ) adopted an HMM model to align the recipe steps to the narration . ( Naim et al , 2015 ) utilize latent - variable based discriminative models ( CRF , Structured Perceptron ) for unsupervised alignment . Besides the alignment of transcripts with video , ( Alayrac et al , 2016 ( Alayrac et al , , 2018 propose to learn the main steps from a set of narrated instructional videos for five different tasks and formulate the problem into two clustering problems . Graph - based clustering is also adopted to learn the semantic storyline of instructional videos in ( Sener et al , 2015 ) . These works assume that \" one task \" has the same procedures . Different from previous works , we focus on learning more complicated procedures for Temporal action proposal is designed to divide a long video into contiguous segments as a sequence of actions , which is similar to the first stage of our model . ( Shou et al , 2016 ) adopt 3D convolutional neural networks to generate multi - scale proposals . DAPs in ( Escorcia et al , 2016 ) apply a sliding window and a Long Short - Term Memory ( LSTM ) network for video content encoding and predicting proposals covered by the window . SST in ( Buch et al , 2017 ) effectively generates proposals in a single pass . However , previous methods do not consider context information to produce nonoverlapped procedures . ( Zhou et al , 2018a ) is the most similar work to ours , which is designed to detect long complicated event proposals rather than actions . We adopt this framework and inject the textual transcript of narrated instructional videos as our first step . Dense video caption aims to generate descriptive sentences for all events in the video . Different from video captioning and paragraph generation , dense video caption requires segmenting of each video into a sequence of temporal propos - als with corresponding captions . ( Krishna et al , 2017 ) resorts to the DAP method ( Escorcia et al , 2016 ) for event detection and apply the contextaware S2VT model ( Venugopalan et al , 2015 ) . ( Yu et al , 2018 ) propose to generate long and detailed description for sport videos . ( Li et al , 2018 ) train jointly on unifying the temporal proposal localization and sentence generation for dense video captioning . ( Xiong et al , 2018 ) assembles temporally localized description to produce a descriptive paragraph . ( Duan et al , 2018 ) propose weakly supervised dense event captioning , which does not require temporal segment annotations , and decomposes the problem into a pair of dual tasks . ( Wang et al , 2018a ) exploit both past and future context for predicting accurate event proposals . ( Zhou et al , 2018b ) adopt a transformer for action proposing and captioning simultaneously . Besides , there are also some works try to incorporate multi - modal information ( e.g. audio stream ) for dense video captioning task ( Ramanishka et al , 2016 ; Xu et al , 2017 ; Wang et al , 2018b ) . The major difference is that our work adopts a different model structure and fuses transcripts to further enhance semantic representation . Experiments show that transcripts can improve both procedure ex - traction and captioning .", "entities": [[2, 4, "TaskName", "Video Understanding"], [50, 51, "MethodName", "CRF"], [220, 225, "MethodName", "Long Short - Term Memory"], [226, 227, "MethodName", "LSTM"], [241, 242, "DatasetName", "SST"], [335, 337, "TaskName", "video captioning"], [381, 383, "TaskName", "event detection"], [434, 437, "TaskName", "dense video captioning"], [544, 547, "TaskName", "dense video captioning"]]}
{"text": "We first encode transcripts and video frames separately and then extract cross - modal features by feeding both embeddings into a context - aware model . To embed transcripts , we first split all tokens in the transcript by a sliding window and input them into a uncased BERT - large ( Devlin et al , 2018 ) model . Next , we encode these sentences by a Transformer ( Vaswani et al , 2017 ) and take the first output as the context - aware transcript embedding e R e . To embed the videos , we uniformly sample T frames and encode each frame v t in V = { v 1 , , v T } to an embedding representation by an ImageNet - pre - trained ResNet - 32 ( He et al , 2016 ) network . Then we adopt another Transformer model to further encode the context information , and output X = { x 1 , , x T } R T \u00d7d . Finally , we combine each of the frame features in X with transcript feature e to get the fused feature ( Hochreiter and Schmidhuber , 1997 ) in order to encode past and future contextual information of video frames : C = { c 1 , , c t , , c T | c t = { x t e } } and feed it into a Bi - directional LSTM F = Bi - LSTM ( C ) where F = { f 1 f T } R T \u00d7f , and f is the hidden size of the LSTM layers .", "entities": [[48, 49, "MethodName", "BERT"], [68, 69, "MethodName", "Transformer"], [125, 126, "DatasetName", "ImageNet"], [130, 131, "MethodName", "ResNet"], [146, 147, "MethodName", "Transformer"], [242, 243, "MethodName", "LSTM"], [247, 248, "MethodName", "LSTM"], [272, 273, "MethodName", "LSTM"]]}
{"text": "We design an LSTM based sequence - to - sequence model ( Sutskever et al , 2014 ) to generate captions for each extracted procedure . For the ( k , t ) - th extracted procedure , we calculate the starting time t s and ending time t e separately and retrieve all tokens within the time span [ t s , t e ] : E ( t s , t e ) = { e ts , , e te } \u2282 { e 1 , , e Q } where Q is the total word count of a video 's transcript . On each step , we concatenate the embedding representation of each token q E ( t s , t e ) , i.e. q , with the nearest video frame feature fq into the input vector e q = { q fq } of the encoder . We employ the hidden state of the last step after encoding all tokens in E ( t s , t e ) and decode the caption of this extracted procedure as W = { w 1 , , w Z } where Z is the word count of the decoded procedure caption .", "entities": [[3, 4, "MethodName", "LSTM"], [78, 79, "MethodName", "ts"]]}
{"text": "In this paper , we use the YouCookII 3 ( Zhou et al , 2018a ) dataset to conduct experiments . It contains 2000 videos dumped from YouTube which are all instructional cooking recipe videos . For each video , human annotators were asked to first label the starting and ending time of procedure segments , and then write captions for each procedure . This dataset contains pre - processed frame features ( T = 500 frames for each video , each frame feature is a 512 - d vector , extracted by ResNet - 32 ) which were used in ( Zhou et al , 2018a ) . In this paper , we also use these pre - computed video features for our task . Besides the video content , our proposed model also relies on transcripts to provide multi - modality information . Since the YouCookII dataset does not have transcripts , we crawl all transcripts automatically generated by YouTube 's ASR engine . YouCookII provides a partition on these 2000 videos : 1333 for training , 457 for validation and 210 for testing . However , the labels of 210 testing videos are unpublished , we can only adopt the training and validation dataset for our experiment . We also remove several videos which are unavailable on YouTube . In all , we use 1387 videos from the YouCookII dataset . We split these videos into 967 for training , 210 for validation and 210 for testing . As shown in", "entities": [[93, 94, "MethodName", "ResNet"], [205, 207, "DatasetName", "validation dataset"]]}
{"text": "As shown in Table 1 , we first show the results reported in ( Zhou et al , 2018a ) which use the full dataset with 2000 videos . In order to ensure a fair comparison , we first run the ProcNets on the validation dataset of YouCookII and get a comparable result . In further experiments , we directly use the subset ( the our partition in the table ) described in the previous section . Moreover , we conduct two experiments to demonstrate the effectiveness of incorporating transcripts in this task . The Ours ( Full Model ) is the final model we propose , which achieves state - of - the - art results . The Ours ( Video Only ) model considers video content without transcripts in the procedure extraction module . Compared with ProcNets , our video only model adds a captioning module , which helps the procedure extraction module to get a better result .", "entities": [[44, 46, "DatasetName", "validation dataset"]]}
{"text": "Domain Adaptation for Sentiment Analysis using Keywords in the Target Domain as the Learning Weight", "entities": [[0, 2, "TaskName", "Domain Adaptation"], [3, 5, "TaskName", "Sentiment Analysis"]]}
{"text": "This paper proposes a new method of instance - based domain adaptation for sentiment analysis . First , our method defines the likelihood of keywords , through the value of inverse document frequency ( IDF ) , for each word in documents in the target domain . Next , the keyword content rate of a document is calculated using the likelihood of keywords and the domain adaptation is performed by giving the keyword content rate to each document in the source domain as the weight . The experiment used an Amazon dataset to demonstrate the effectiveness of our proposed method . Although the instance - based method has not shown great efficiency , the advantages combining instance - based method and feature - based method are shown in this paper .", "entities": [[10, 12, "TaskName", "domain adaptation"], [13, 15, "TaskName", "sentiment analysis"], [65, 67, "TaskName", "domain adaptation"]]}
{"text": "Domain adaptation is roughly divided into two types : the supervised approach using labeled data in the target domain and the unsupervised approach that does not use them . For supervised approach , Daum\u00e9 's method ( Daum\u00e9 III , 2007 ) has become a standard method because of its simplicity and high ability . The method in the current research is an unsupervised approach . Unsupervised approaches can further be divided into two types : feature - based and instance - based ( Pan and Yang , 2010 ) . They are both weighted learning methods ; feature - based methods give weights to features and instance - based methods give weights to instances . Among featurebased methods , the most representative method is structural correspondence learning ( SCL ) ( Blitzer et al , 2006 ) . In addition , CORAL has attracted much attention for its simplicity and high ability in recent years . Moreover , the feature - based methods with deep learning ( Glorot et al , 2011 ) , the expanded CORAL and adversarial networks ( Ganin and Lempitsky , 2015 ) ( Tzeng et al , 2017 ) are also considered as the state of the art . On the other hand , instance - based methods have not been studied as much as feature - based methods . The instance - based method assumes a covariate shift . A covariate shift assumes P S ( c | x ) = P T ( c | x ) and P S ( x ) = P T ( x ) . Under a covariate shift , P T ( c | x ) can be obtained by the weighted learning that uses the probability density ratio r = P T ( x ) /P S ( x ) as the weight of the document of the source data x. There are a variety of methods for calculating the probability density ratio . The simplest way to calculate the ratio is directly estimate P S ( x ) and P T ( x ) , but in the case of complex models , the problem will be more complicated . Thus , the method that directly models the probability density ratio was studied . Among these methods , uLSIF ( Yamada et al , 2011 ) is widely used because the time complexity of the method is relatively small . However , P ( x ) of bag - of - words can be modeled by Naive Bayes model if the problem is limited to natural language processing . Therefore , ( Shinnou and Sasaki , 2014 ) defined P r ( x ) , the prior of x , as follows : P R ( x ) = \u220f n i=1 P R ( f i ) , where x denotes a data in the domain R and x has a set of features , that is , x = { f 1 , f 2 , , f n } . They also obtain P R ( f i ) using the following equation : P R ( f ) = n ( R , f ) +1 N ( R ) +2 . Here , n ( R ; f ) is the frequency of feature f in the domain R , and n ( R ) is the number of data in the domain R. Therefore , the probability density ratio is obtained as follows : r = P T ( x ) P S ( x ) = n ( T , f ) + 1 N ( T ) + 2 N ( S ) + 2 n ( S , f ) + 1 ( 1 ) 3 Proposed Method", "entities": [[0, 2, "TaskName", "Domain adaptation"]]}
{"text": "Set the weight w x of the instance x in the source domain . The words ( file ) x is { w i } K i=1 , and the frequency within x for word w i is f i . Using these , w x is given by the following equation : w x = 1 \u2211 k i=1 f i K \u2211 i=1 f i l w i 4 Experiment The Amazon dataset ( Blitzer et al , 2007 ) used in the experiment is specifically developed using the processed_acl.tar.gz file on the following website . https://www.cs.jhu.edu/\u02dcmdredze/ datasets / sentiment/. The data include books ( B ) , dvd ( D ) , electronics ( E ) , and kitchen ( K ) . The number of files contained in each domain is shown in The learning algorithm is an SVM with scikitlearn . The core is linear , the value of the c parameter is fixed at 0.1 , and the scikit - learn SVM supports Weighted - Learning 1 , so the scikit - learn SVM is used here . domain adaptations are : B D , B E , B K , D B , D E , D K , E B , E D , E K , K B , K D , K E. See Table 1 for the results of the two methods uLSIF ( Yamada et al , 2011 ) and using equation ( 1 ) of Naive Bayes for determining the rate density ratio of each domain and the proposed method . NONE in Table 1 means that the domain adaptation method was not used but simply applies the classifier formed from the training data of the source domain to the result of the test data in the target domain was applied . In addition , IDEAL is a result using the training data in the target domain to learn through the classifier and apply it to the test data in the target domain . Using the case as a weighted method , a compari - son of uLSIF , NB , the our method shows that the six highest correct answer rates in the 12 domain adaptations are obtained by our method , and the remaining six highest positive answer rates are obtained by NB . When we take 12 averages , the solution rate of our method is more than that of NB , and our method is weighted with example and , which is excellent .", "entities": [[142, 143, "MethodName", "SVM"], [167, 168, "MethodName", "SVM"], [179, 180, "MethodName", "SVM"], [271, 273, "TaskName", "domain adaptation"]]}
{"text": "This paper proposed a method for instance - based domain adaptation of sentiment analysis . For outline , from the target domain , using IDF to set the likelihood of keywords , and the data in the source domain , the content rate in the target domain keyword , and the keyword content rate as the weight . In the experiment , we compared our proposed method with two typical instance - based methods : uLSI using the probability density ratio and the method defining weight through Naive Bayes model . However , using an instance - based alone to perform domain adaptation has a very small effect , the combining instance - based method and feature - based method is assured as shown in this paper . Further , the combination is easy to implement in the neural network model . Thus , we will investigate this approach in future .", "entities": [[9, 11, "TaskName", "domain adaptation"], [12, 14, "TaskName", "sentiment analysis"], [101, 103, "TaskName", "domain adaptation"]]}
{"text": "Joint Learning of POS and Dependencies for Multilingual Universal Dependency Parsing", "entities": [[9, 11, "TaskName", "Dependency Parsing"]]}
{"text": "In this section , we describe our joint model 2 for POS tagging and dependency parsing in the CoNLL 2018 Shared Task , which is built on the STACKPTR parser introduced by ( Ma et al , 2018 ) . Our model is mainly composed of three components , the representation ( Section 2.1 ) , POS tagger ( Section 2.2 ) and dependency parser ( Section 2.3 ) . Figure 1 illustrates the overall model .", "entities": [[14, 16, "TaskName", "dependency parsing"]]}
{"text": "Representation is a key component in various NLP models , and good representations should ideally model both complex characteristics and linguistic contexts . In our system , we follow the bidirectional LSTM - CNN architecture ( BiLSTM - CNNs ) ( Chiu and Nichols , 2016 ; Ma and Hovy , 2016 ) , where CNNs encode word information into character - level representation and BiLSTM models context information of each word . Character Level Representation Though word embedding is popular in many existing parsers , they are not ideal for languages with high out - ofvocabulary ( OOV ) ratios . Hence , our system introduces the character - level ( Li et al , 2018a ) representation to address the challenge . Formally , given a word w = { BOW , c 1 , c 2 , ... , c n , EOW } , where two special BOW ( begin - of - word ) and EOW ( end - of - word ) tags indicate the begin and end positions respectively , we use the CNN to extract character - level representation as follows : e c = M axP ool ( Conv ( w ) ) where the CNN is similar to the one in ( Chiu and Nichols , 2016 ) , but we use only characters as the inputs to CNN , without character type features . Word Level Representation Word embedding is a standard component of most state - of - the - art NLP architectures . Due to their ability to capture syntactic and semantic information of words from large scale unlabeled texts , we pre - train the word embeddings from the given training dataset by word2vec ( Mikolov et al , 2013 ) toolkit . For low - resource languages without available training data , we sample the training dataset from similar languages to generate a mixed dataset .", "entities": [[30, 32, "MethodName", "bidirectional LSTM"], [36, 37, "MethodName", "BiLSTM"], [65, 66, "MethodName", "BiLSTM"], [279, 281, "TaskName", "word embeddings"]]}
{"text": "To enrich morphological information , we also incorporate UPOS tag embeddings into the representation . Therefore , we jointly predict the UPOS tag in our system . The architecture for the POS tagger in our model is almost identical to that of the parser . The tagger uses a BiLSTM over the concatenation of word embeddings and character embeddings : s pos i = BiLST M pos ( e w i e c i ) Then we calculate the probability of tag for each type using affine classifiers as follows : h pos i = M LP pos ( s pos i ) r pos i = W pos h pos i + b pos y pos i = arg max ( r i ) The tag classifier is trained jointly using crossentropy losses that are summed together with the dependency parser loss during optimization .", "entities": [[49, 50, "MethodName", "BiLSTM"], [54, 56, "TaskName", "word embeddings"], [142, 143, "MetricName", "loss"]]}
{"text": "In order to integrate contextual information , we concatenate the character embedding e c , pre - trained word embedding e w and UPOS tag embedding e pos , then feed them into the BiLSTM . We take the bidirectional vectors at the final layer as the contextsensitive representation : \u2212 s i = LST M f orward ( e w i e c i e pos i ) \u2212 s i = LST M backward ( e w i e c i e pos i ) s i = \u2212 s i \u2212 s i Notably , we use the UPOS tag from the output of our POS tagging model .", "entities": [[34, 35, "MethodName", "BiLSTM"]]}
{"text": "The training objective of pur system is to learn the probability of UPOS tags P \u03b8 pos ( y pos | x ) and the dependency trees P \u03b8 dep ( y dep | x , y pos ) . Given a sentence x , the probabilities are factorized as : P \u03b8 pos ( y pos | x ) = k i=1 P \u03b8 pos ( p i | x ) y pos = arg max ypos Ypos ( P \u03b8 pos ( y pos | x ) ) P \u03b8 dep ( y dep | x , y pos ) = k i=1 P \u03b8 dep ( p i | p < i , x , y pos ) = k i=1 l i j=1 P \u03b8 dep ( c i , j | c i , < j , p < i , x , y pos ) where \u03b8 pos and \u03b8 dep represent the model parameters respectively . p < i denotes the preceding dependency paths that have already been generated . c i , j represents the j th word in p i and c i , j denotes all the proceeding words on the path p i . Therefore , the whole loss is the sum of three objectives : Loss = Loss pos + Loss arc + Loss label where the Loss pos , Loss arc and Loss label are the conditional likehood of their corresponding target , using the cross - entropy loss . Specifically , we train a dependency label classifier following , which takes the dependency head - child pair as input features .", "entities": [[15, 16, "HyperparameterName", "\u03b8"], [28, 29, "HyperparameterName", "\u03b8"], [52, 53, "HyperparameterName", "\u03b8"], [64, 65, "HyperparameterName", "\u03b8"], [81, 82, "HyperparameterName", "\u03b8"], [91, 92, "HyperparameterName", "\u03b8"], [106, 107, "HyperparameterName", "\u03b8"], [128, 129, "HyperparameterName", "\u03b8"], [152, 153, "HyperparameterName", "\u03b8"], [155, 156, "HyperparameterName", "\u03b8"], [209, 210, "MetricName", "loss"], [251, 252, "MetricName", "loss"]]}
{"text": "In this paper , we describe our system in the CoNLL 2018 shared task on UD parsing . Our system uses a transition - based neural network architecture for dependency parsing , which predicts the UPOS tag and dependencies jointly . Combining pointer networks with an internal stack to track the status of the top - down , depth - first search in the parsing decoding procedure , the STACKPTR parser is able to capture information from the whole sentence and all the previously derived subtrees , removing the left - to - right restriction in classical transition - based parsers , while maintaining", "entities": [[15, 16, "DatasetName", "UD"], [29, 31, "TaskName", "dependency parsing"]]}
{"text": "Get To The Point : Summarization with Pointer - Generator Networks", "entities": [[5, 6, "TaskName", "Summarization"]]}
{"text": "Neural sequence - to - sequence models have provided a viable new approach for abstractive text summarization ( meaning they are not restricted to simply selecting and rearranging passages from the original text ) . However , these models have two shortcomings : they are liable to reproduce factual details inaccurately , and they tend to repeat themselves . In this work we propose a novel architecture that augments the standard sequence - to - sequence attentional model in two orthogonal ways . First , we use a hybrid pointer - generator network that can copy words from the source text via pointing , which aids accurate reproduction of information , while retaining the ability to produce novel words through the generator . Second , we use coverage to keep track of what has been summarized , which discourages repetition . We apply our model to the CNN / Daily Mail summarization task , outperforming the current abstractive state - of - the - art by at least 2 ROUGE points .", "entities": [[14, 17, "TaskName", "abstractive text summarization"], [147, 151, "DatasetName", "CNN / Daily Mail"], [151, 152, "TaskName", "summarization"]]}
{"text": "We use the CNN / Daily Mail dataset ( Hermann et al , 2015 ; , which contains online news articles ( 781 tokens on average ) paired with multi - sentence summaries ( 3.75 sentences or 56 tokens on average ) . We used scripts supplied by to obtain the same version of the the data , which has 287 , 226 training pairs , 13 , 368 validation pairs and 11 , 490 test pairs . Both the dataset 's published results ( Nallapati et al , , 2017 use the anonymized version of the data , which has been pre - processed to replace each named entity , e.g. , The United Nations , with its own unique identifier for the example pair , e.g. , @entity5 . By contrast , we operate directly on the original text ( or non - anonymized version of the data ) , 2 which we believe is the favorable problem to solve because it requires no pre - processing .", "entities": [[3, 7, "DatasetName", "CNN / Daily Mail"], [115, 116, "DatasetName", "Nations"]]}
{"text": "We find that both our baseline models perform poorly with respect to ROUGE and METEOR , and in fact the larger vocabulary size ( 150k ) does not seem to help . Even the better - performing baseline ( with 50k vocabulary ) produces summaries with several common problems . Factual details are frequently reproduced incorrectly , often replacing an uncommon ( but in - vocabulary ) word with a morecommon alternative . For example in Figure 1 , the baseline model appears to struggle with the rare word thwart , producing destabilize instead , which leads to the fabricated phrase destabilize nigeria 's economy . Even more catastrophically , the summaries sometimes devolve into repetitive nonsense , such as the third sentence produced by the baseline model in Figure 1 . In addition , the baseline model ca n't reproduce out - of - vocabulary words ( such as muhammadu buhari in Figure 1 ) . Further examples of all these problems are provided in the supplementary material . Our pointer - generator model achieves much better ROUGE and METEOR scores than the baseline , despite many fewer training epochs . The difference in the summaries is also marked : outof - vocabulary words are handled easily , factual details are almost always copied correctly , and there are no fabrications ( see Figure 1 ) . However , repetition is still very common . Our pointer - generator model with coverage improves the ROUGE and METEOR scores further , convincingly surpassing the best abstractive model Article : smugglers lure arab and african migrants by offering discounts to get onto overcrowded ships if people bring more potential passengers , a cnn investigation has revealed . ( ... ) Summary : cnn investigation uncovers the business inside a human smuggling ring . Article : eyewitness video showing white north charleston police officer michael slager shooting to death an unarmed black man has exposed discrepancies in the reports of the first officers on the scene . ( ... ) Summary : more questions than answers emerge in controversial s.c . police shooting . of by several ROUGE points . Despite the brevity of the coverage training phase ( about 1 % of the total training time ) , the repetition problem is almost completely eliminated , which can be seen both qualitatively ( Figure 1 ) and quantitatively ( Figure 4 ) . However , our best model does not quite surpass the ROUGE scores of the lead - 3 baseline , nor the current best extractive model ( Nallapati et al , 2017 ) . We discuss this issue in section 7.1 .", "entities": [[14, 15, "DatasetName", "METEOR"], [167, 169, "DatasetName", "supplementary material"], [180, 181, "DatasetName", "METEOR"], [247, 248, "DatasetName", "METEOR"]]}
{"text": "It is clear from Table 1 that extractive systems tend to achieve higher ROUGE scores than abstractive , and that the extractive lead - 3 baseline is extremely strong ( even the best extractive system beats it by only a small margin ) . We offer two possible explanations for these observations . Firstly , news articles tend to be structured with the most important information at the start ; this partially explains the strength of the lead - 3 baseline . Indeed , we found that using only the first 400 tokens ( about 20 sentences ) of the article yielded significantly higher ROUGE scores than using the first 800 tokens . Secondly , the nature of the task and the ROUGE metric make extractive approaches and the lead - 3 baseline difficult to beat . The choice of content for the reference summaries is quite subjective - sometimes the sentences form a self - contained summary ; other times they simply showcase a few interesting details from the article . Given that the articles contain 39 sentences on average , there are many equally valid ways to choose 3 or 4 highlights in this style . Abstraction introduces even more options ( choice of phrasing ) , further decreas - ing the likelihood of matching the reference summary . For example , smugglers profit from desperate migrants is a valid alternative abstractive summary for the first example in Figure 5 , but it scores 0 ROUGE with respect to the reference summary . This inflexibility of ROUGE is exacerbated by only having one reference summary , which has been shown to lower ROUGE 's reliability compared to multiple reference summaries ( Lin , 2004a ) . Due to the subjectivity of the task and thus the diversity of valid summaries , it seems that ROUGE rewards safe strategies such as selecting the first - appearing content , or preserving original phrasing . While the reference summaries do sometimes deviate from these techniques , those deviations are unpredictable enough that the safer strategy obtains higher ROUGE scores on average . This may explain why extractive systems tend to obtain higher ROUGE scores than abstractive , and even extractive systems do not significantly exceed the lead - 3 baseline . To explore this issue further , we evaluated our systems with the METEOR metric , which rewards not only exact word matches , but also matching stems , synonyms and paraphrases ( from a predefined list ) . We observe that all our models receive over 1 METEOR point boost by the inclusion of stem , synonym and paraphrase matching , indicating that they may be performing some abstraction . However , we again observe that the lead - 3 baseline is not surpassed by our models . It may be that news article style makes the lead - 3 baseline very strong with respect to any metric . We believe that investigating this issue further is an important direction for future work .", "entities": [[246, 247, "DatasetName", "0"], [392, 393, "DatasetName", "METEOR"], [427, 428, "DatasetName", "METEOR"]]}
{"text": "We thank the ACL reviewers for their helpful comments . This work was begun while the first author was an intern at Google Brain and continued at Stanford . Stanford University gratefully acknowledges the support of the DARPA DEFT Program AFRL contract no . FA8750 - 13 - 2 - 0040 . Any opinions in this material are those of the authors alone .", "entities": [[22, 23, "DatasetName", "Google"], [37, 38, "DatasetName", "DARPA"]]}
{"text": "Re - evaluating Evaluation in Text Summarization", "entities": [[5, 7, "TaskName", "Text Summarization"]]}
{"text": "Automated evaluation metrics as a stand - in for manual evaluation are an essential part of the development of text - generation tasks such as text summarization . However , while the field has progressed , our standard metrics have not - for nearly 20 years ROUGE has been the standard evaluation in most summarization papers . In this paper , we make an attempt to re - evaluate the evaluation method for text summarization : assessing the reliability of automatic metrics using top - scoring system outputs , both abstractive and extractive , on recently popular datasets for both systemlevel and summary - level evaluation settings . We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems . We release a dataset of human judgments that are collected from 25 top - scoring neural summarization systems ( 14 abstractive and 11 extractive ) :", "entities": [[25, 27, "TaskName", "text summarization"], [54, 55, "TaskName", "summarization"], [73, 75, "TaskName", "text summarization"], [144, 145, "TaskName", "summarization"]]}
{"text": "In this section we describe the datasets , systems , metrics , and meta evaluation methods used below . - 2008 , 2009 ( Dang and Owczarzak , 2008 , 2009 are multi - document , multi - reference summarization datasets . Human judgments are available on for the system summaries submitted during the TAC - 2008 , TAC - 2009 shared tasks . CNN / DailyMail ( CNNDM ) ( Hermann et al , 2015 ; Nallapati et al , 2016 ) is a commonly used summarization dataset that contains news articles and associated highlights as summaries . We use the version without entities anonymized .", "entities": [[39, 40, "TaskName", "summarization"], [87, 88, "TaskName", "summarization"]]}
{"text": "We use the following representative top - scoring systems that either achieve state - of - the - art ( SOTA ) results or competitive performance , for which we could gather the outputs on the CNNDM dataset . Extractive summarization systems . We use CNN - LSTM - BiClassifier ( CLSTM - SL ; Kedzie et al ( 2018 ) ) , Latent ( Zhang et al , 2018 ) , Ban - ditSum ( Dong et al , 2018 ) , REFRESH ( Narayan et al , 2018 ) , NeuSum , HIBERT ( Zhang et al , 2019b ) , Bert - Sum - Ext ( Liu and Lapata , 2019a ) , CNN - Transformer - BiClassifier ( CTrans - SL ; Zhong et al ( 2019 ) ) , CNN - Transformer - Pointer ( CTrans - PN ; Zhong et al ( 2019 ) ) , HeterGraph ( Wang et al , 2020 ) and MatchSum ( Zhong et al , 2020 ) as representatives of extractive systems , totaling 11 extractive system outputs for each document in the CNNDM test set . Abstractive summarization systems . We use pointer - generator+coverage ( See et al , 2017 ) , fastAbsRL ( Chen and Bansal , 2018 ) , fastAbsRLrank ( Chen and Bansal , 2018 ) , Bottom - up ( Gehrmann et al , 2018 ) , T5 ( Raffel et al , 2019 ) , Unilm - v1 ( Dong et al , 2019 ) , Unilm - v2 ( Dong et al , 2019 ) , twoStageRL ( Zhang et al , 2019a ) , pre - SummAbs ( Liu and Lapata , 2019b ) , preSummAbsext ( Liu and Lapata , 2019b ) BART ( Lewis et al , 2019 ) and Semsim ( Yoon et al , 2020 ) as abstractive systems . In total , we use 14 abstractive system outputs for each document in the CNNDM test set .", "entities": [[39, 41, "TaskName", "Extractive summarization"], [47, 48, "MethodName", "LSTM"], [118, 119, "MethodName", "Transformer"], [136, 137, "MethodName", "Transformer"], [190, 191, "TaskName", "summarization"], [235, 236, "MethodName", "T5"], [294, 295, "MethodName", "BART"]]}
{"text": "We follow a 3 - step process to collect human judgments : ( 1 ) we collect system - generated summaries on the most - commonly used summarization dataset , CNNDM ; ( 2 ) we select representative test samples from CNNDM and ( 3 ) we manually evaluate system - generated summaries of the aboveselected test samples .", "entities": [[27, 28, "TaskName", "summarization"]]}
{"text": "Motivated by the central research question : \" does the rapid progress of model development in summarization models require us to re - evaluate the evaluation process used for text summarization ? \" We use the collected human judgments to meta - evaluate current metrics from four diverse viewpoints , measuring the ability of metrics to : ( 1 ) evaluate all systems ; ( 2 ) evaluate top - k strongest systems ; ( 3 ) compare two systems ; ( 4 ) evaluate individual summaries . We find that many previously attested properties of metrics observed on TAC exhibit different trends on the new CNNDM dataset .", "entities": [[16, 17, "TaskName", "summarization"], [29, 31, "TaskName", "text summarization"]]}
{"text": "This work is connected to the following threads of topics in text summarization . Human Judgment Collection Despite many approaches to the acquisition of human judgment ( Chaganty et al , 2018 ; Nenkova and Passonneau , 2004 ; Shapira et al , 2019 ; Fan et al , 2018 ) , Pyramid ( Nenkova and Passonneau , 2004 ) has been a mainstream method to meta - evaluate various automatic metrics . Specifically , Pyramid provides a robust technique for evaluating content selection by exhaustively obtaining a set of Semantic Content Units ( SCUs ) from a set of references , and then scoring system summaries on how many SCUs can be inferred from them . Recently , Shapira et al ( 2019 ) proposed a lightweight and crowdsourceable version of the original Pyramid , and demonstrated it on the DUC 2005 ( Dang , 2005 ) and 2006 ( Dang , 2006 ) multi - document summarization datasets . In this paper , our human evaluation methodology is based on the Pyramid ( Nenkova and Passonneau , 2004 ) and LitePyramids ( Shapira et al , 2019 ) techniques . Chaganty et al ( 2018 ) also obtain human evaluations on system summaries on the CNNDM dataset , but with a focus on language quality of summaries . In comparison , our work is focused on evaluating content selection . Our work also covers more systems than their study ( 11 extractive + 14 abstractive vs. 4 abstractive ) .", "entities": [[11, 13, "TaskName", "text summarization"], [141, 143, "DatasetName", "DUC 2005"], [155, 159, "TaskName", "multi - document summarization"]]}
{"text": "Our work not only diagnoses the limitations of current metrics but also highlights the importance of upgrading the existing meta - evaluation testbed , keeping it up - to - date with the rapid development of systems and datasets . In closing , we highlight some potential future directions : ( 1 ) The choice of metrics depends not only on different tasks ( e.g , summarization , translation ) but also on different datasets ( e.g. , TAC , CNNDM ) and application scenarios ( e.g , system - level , summary - level ) . Future works on meta - evaluation should investigate the effect of these settings on the performance of metrics . ( 2 ) Metrics easily overfit on limited datasets . Multidataset meta - evaluation can help us better understand each metric 's peculiarity , therefore achieving a better choice of metrics under diverse scenarios . ( 3 ) Our collected human judgments can be used as supervision to instantiate the most recentlyproposed pretrain - then - finetune framework ( originally for machine translation ) ( Sellam et al , 2020 ) , learning a robust metric for text summarization .", "entities": [[66, 67, "TaskName", "summarization"], [177, 179, "TaskName", "machine translation"], [193, 195, "TaskName", "text summarization"]]}
{"text": "Building a De - identification System for Real Swedish Clinical Text Using Pseudonymised Clinical Text", "entities": [[2, 5, "TaskName", "De - identification"]]}
{"text": "This article presents experiments with pseudonymised Swedish clinical text used as training data to de - identify real clinical text with the future aim to transfer non - sensitive training data to other hospitals . Conditional Random Fields ( CFR ) and Long Short - Term Memory ( LSTM ) machine learning algorithms were used to train deidentification models . The two models were trained on pseudonymised data and evaluated on real data . For benchmarking , models were also trained on real data , and evaluated on real data as well as trained on pseudonymised data and evaluated on pseudonymised data . CRF showed better performance for some PHI information like Date Part , First Name and Last Name ; consistent with some reports in the literature . In contrast , poor performances on Location and Health Care Unit information were noted , partially due to the constrained vocabulary in the pseudonymised training data . It is concluded that it is possible to train transferable models based on pseudonymised Swedish clinical data , but even small narrative and distributional variation could negatively impact performance .", "entities": [[42, 47, "MethodName", "Long Short - Term Memory"], [48, 49, "MethodName", "LSTM"], [103, 104, "MethodName", "CRF"]]}
{"text": "Electronic health records ( EHR ) are produced in a steady stream , with the potential of advancing future medical care . Research on EHR data holds the potential to improve our understanding of patient care , care processes , and disease characteristics and progression . However , much of the data \u21e4 Hercules Dalianis is also guest professor at the Norwegian Centre for E - health Research is sensitive , containing Protected Health Information ( PHI ) such as personal names , addresses , phone numbers , that can identify particular individuals and thus can not be available to the public for general scientific inquiry . Although good progress has been made in the general sub - field of de - identifying clinical text , the problem is still not fully resolved ( Meystre et al , 2010 ; Yogarajan et al , 2018 ) . This study examines the use of pseudonymised health records as training data for de - identification tasks . Several ethical and scientific issues arise regarding the balance between maintaining patient confidentiality and the need for wider application of trained models . How will a de - identification system be constructed and used in a cross hospital setting without risking the privacy of patients ? Is it possible to obscuring the training data by pseudonymising it and then use it for the training of a machine learning system ? De - identification and pseudonymisation are two related concepts . In this paper de - identification is used as a more general term to describe the process of finding personal health information to be able to conceal identifying information . A pseudonymised text is a text where the personal health information has been identified either manually or automatically and then replaced with realistic surrogates . The research question in this study is whether it is possible to use de - identified and pseudonymised clinical text in Swedish as training data for deidentifying real clinical text , and hence make it possible to transfer the system cross hospital . We highlight whether learning from the exist - ing , non - sensitive , pseudonymised Swedish clinical text can be useful in a new and different context ; considering the normal variations in the distribution and nature of PHI information , and potential effects of scrubbing ( Berman , 2003 ) , that is , removing and modifying PHIs that was carried out to patient records during the de - identification process .", "entities": [[160, 163, "TaskName", "de - identification"], [191, 194, "TaskName", "de - identification"], [235, 238, "TaskName", "De - identification"], [248, 251, "TaskName", "de - identification"], [411, 414, "TaskName", "de - identification"]]}
{"text": "In this study , machine learning approaches are used since the best de - identification systems appear to be machine learning - based ( Kushida et al , 2012 ) . While rule - based methods such as using dictionaries and pattern - matching were previously more prevalent than machine learning methods for solving text - based de - identification problems ( Meystre et al , 2010 ) , today it is more typical to have both approaches used , since rule - based methods still yield better results for some PHI information ( Neamatullah et al , 2008b ) . Dictionaries and patterns were therefore used as features within one of the models .", "entities": [[12, 15, "TaskName", "de - identification"], [57, 60, "TaskName", "de - identification"]]}
{"text": "Two different data sets for de - identification were used : Stockholm EPR PHI Psuedo Corpus ( Pseudo ) as well as the Stockholm EPR PHI Cor - The Stockholm EPR PHI Pseudo Corpus was produced from the Stockholm EPR PHI Corpus by automatically pseudonymising all PHIs . This process is described by Dalianis ( 2019 ) . The Stockholm EPR PHI Corpus is described by Dalianis and Velupillai ( 2010 ) . An example is shown in Figure 1 ( Dalianis et al , 2015 ) . The number of entities and types of entities in both the Stockholm EPR PHI Psuedo Corpus and the Stockholm EPR PHI Corpus is shown in Table 2 . From Table 2 , it can be observed that the distribution of PHI instances between the two data sets is somewhat similar , but there is a significant difference when it comes to unique instances between the two data sets . In total , the Real data set contains proportionally more unique instances than the Pseudo data set . The entities in the Real data set al o tend to have more tokens .", "entities": [[5, 8, "TaskName", "de - identification"]]}
{"text": "Using the de - identified and pseudonymised data set , two models were trained based on two machine learning algorithms ; CRF and the deep learning algorithm LSTM . The two algorithms were chosen since both have been shown to produce state of the art performance , and applying the two on Swedish clinical data sets makes for an informative comparison . The two models were evaluated on both the real data set that is annotated for PHI , but not pseudonymised , ' Pseudo - Real ' , as well as on the pseudonymised data set , ' Pseudo - Pseudo ' . For additional comparison basis models trained on the real data set were evaluated on test sets from the same data set , ' Real - Real ' .", "entities": [[21, 22, "MethodName", "CRF"], [27, 28, "MethodName", "LSTM"]]}
{"text": "In this study , the CRF algorithm implemented in CRFSuite ( Okazaki , 2007 ) is used with the sklearn - crfsuite wrapper 2 and the LSTM architecture described by Lample et al ( 2016 ) , based on an open - source implementation with Tensorflow 3 is used . The linear - chain Conditional Random Fields model , implemented with sklearn - CRFSuite 4 , Figure 1 : Example of a pseudonymised record . The original Swedish pseudonymised record is to the right and the translated version is to the left . The underlined words are the surrogates , where real data has been replaced with pseudonyms . uses lexical , orthographic , syntactic and dictionary features . The CRF is based on trial - and - error experiments with feature sets described by Berg and Dalianis ( 2019 ) , and uses the same features except for section features .", "entities": [[5, 6, "MethodName", "CRF"], [26, 27, "MethodName", "LSTM"], [120, 121, "MethodName", "CRF"]]}
{"text": "The long short - term memory ( LSTM ) needs word embeddings as features for the training . Word2vec 5 was used to produce word embeddings using shallow neural networks , based on two corpora ; a clinical corpus and medical journals . For the training using real clinical data , word embeddings were produced using a clinical corpus of 200 million tokens that produced 300 , 824 vectors with a dimension of 300 . For the training with pseudo clinical data , word embeddings were produced using L\u00e4kartidningen corpus ( The Swedish scientific medical journals from 1996 to 2005 ) containing 21 million tokens that produced 118 , 662 vectors with a dimension of 300 . The reason for using L\u00e4kartidningen is that the corpus does not contain sensitive data and hence is also more easily usable for transferable cross hospital training .", "entities": [[1, 6, "MethodName", "long short - term memory"], [7, 8, "MethodName", "LSTM"], [10, 12, "TaskName", "word embeddings"], [24, 26, "TaskName", "word embeddings"], [51, 53, "TaskName", "word embeddings"], [83, 85, "TaskName", "word embeddings"]]}
{"text": "The training set used in this study has a substantially constrained vocabulary compared to the evaluation set , which may partially explain the overall performance achieved when evaluating on real data ( Pseudo - Real version of the data has less PHI tokens and the entities are more often single tokens . The Full Date structure yyyyddmm - yyyyddmm is commonly occurring in the pseudo data , and the dash between the dates , \" - \" , is often incorrectly identified . For example , using the CRF algorithm on real - data training and pseudo - data testing ( Real - Pseudo ) , of the 159 instances not identified as full dates tokens , sixty contain ' - ' . The pseudo data uses the structure yyyyddmm while the real data uses yyddmm , which leads to errors . For these kinds of errors on standard data formats such as dates , it is easy to see how rule - based approaches using regular expressions could significantly improve the overall performance of the system . The weakest performance area was for location information . There is a large variety of locations in the pseudo - data . These are also fairly specific and unlikely to occur in the real data , for example , locations with very few inhabitants . These uncommon rural places have names similar to residential homes ( \u00e4ldreboenden ) . There are multiple instances of the suffix ' g\u00e5rden ' ( yard ) in the location pseudo - PHI , whereas , in the real data , the same suffix is common for care units . In the pseudo - data , the care units are more general than in the real data , often too general to be annotated in the real data set . Infirmaries are fairly common in the real data but non - existent in the pseudo data . This lack of variation in the pseudo is partially responsible for the drop in performance . There are at least two ways to think about mitigating this poor performance . First , location and care unit could be combined as one entity type since they are conceptually very similar , and sometimes have interchangeable entity names . Secondly , using more detailed municipality street and location mapping databases as dictionaries could be considered .", "entities": [[88, 89, "MethodName", "CRF"]]}
{"text": "There is one similar study to ours but for English by Yeniterzi et al ( 2010 ) , where the authors train their de - identification system with all combinations of pseudonymised textual data ( or what they call resynthesized records ) and real data and their results are in line with ours . However , there are some studies on cross - domain adaptation . In cross - domain adaption there is , however , a substantial domain change between the training and testing data , unlike in this study . Martinez et al ( 2014 ) used models trained in one hospital on pathology reports in another hospital . Their system only required minor feature normalisation , and the reported results were comparable across the hospitals . Although this demonstrates feasibility , it is important to note that the pathology reports were from the same medical sub - speciality with only some narrative differences . In this study , in addition to narrative differences between the training data and the target evaluation data , the number of care units and locations involved , as well as personal names , are widely varied . With large amounts of out of vocabulary variation , training on limited data will likely yield poor results . In practice , these data types exist in other non - sensitive sources such as city and rural location and street mapping data . Except for location and care unit , evaluation on pseudo - data ( Pseudo - Pseudo ) produced better outcomes compared to performance on real - data ( Pseudo - Real ) , which can be expected . What was a bit unexpected was the lower performance of the LSTM algorithm . The algorithm 's results would potentially have been improved by larger vector data or more labelled data ( Dernoncourt et al , 2017 ) . While clinical notes have unique linguistic structures and grammatical peculiarities , nonclinical data sources could still provide important contextual information for constructing a useful vector space . Additional sources using nonsensitive data , such as public corpora in the general domain , hold a potential to improve performance on the de - identification task , therefore this line of inquiry will be followed up on in future work . In the same vein , factoring in part of speech tags from other sources of clinical data could be useful in this case . For instance , there are deidentification databases of clinical text , such as MIMIC ( Neamatullah et al , 2008a ; Goldberger et al , 2000 ) , which could be used as additional information for training purposes , and using only the part of speech tags reduces security risks . Current results are calculated as exact matches , and the partial match is not factored in , which may affect the result . As mentioned in the analysis the CRF algorithm rarely classifies the ' - ' in between dates as a part of the dates , and these are therefore not counted as matches despite the most identifying parts of the entity being identified . To improve the general performance , a combination of both the LSTM and CRF algorithms could be performed instead of testing them independently . Combining high - performance algorithms and the use of ensemble methods seem to produce the best results as reported in the literature ( Dernoncourt et al , 2017 ; Liu et al , 2017 ) , and these techniques will be investigated in future work on the data sets .", "entities": [[23, 26, "TaskName", "de - identification"], [63, 65, "TaskName", "domain adaptation"], [287, 288, "MethodName", "LSTM"], [365, 368, "TaskName", "de - identification"], [488, 489, "MethodName", "CRF"], [536, 537, "MethodName", "LSTM"], [538, 539, "MethodName", "CRF"]]}
{"text": "The results of this study suggest that although it is possible to train models on pseudonymised data for use in different contexts , there is severe deterioration in performance for some PHI information . Even small narrative and distributional variation could negatively impact performance . Transferring a system from one set of clinical text to a different set could result in the performance of the system deteriorating ; in this study the Pseudo - Real case . This problem , what we call The cross pseudo - real text adaptation problem , is an issue that could happen due to the pseudonymisation / de - identification processes on the training data due to the narrative and distributional variation as well as other differences in the nature of the PHI between the training data and the target . In the future , we will try to improve the pseudonymisation module described in Dalianis ( 2019 ) to produce a larger variation in the vocabulary as the lack of variation may affect the current result negatively . We will also apply the learned models to other Nordic languages such as Norwegian clinical text and use the system as a pre - annotation system to assist the manual annotators in their work to create a Norwegian gold standard .", "entities": [[103, 106, "TaskName", "de - identification"]]}
{"text": "IIITT at CASE 2021 Task 1 : Leveraging Pretrained Language Models for Multilingual Protest Detection", "entities": [[8, 11, "TaskName", "Pretrained Language Models"]]}
{"text": "The recent surge in social media users has led many people to express their opinions on various global issues . These opinions travel far and wide within a matter of seconds ( Hossny et al , 2018 ) . This can influence many people and may engage public movements ( Won et al , 2017a ) . Therefore , there is a definite need to detect these protests and analyse them to know the significant areas of disinterest . Being a free and easy to use platform , social media has become a part of our day to day life . It incorporates people of different ages , gender , location , religions , background , and so on . The enormous number of rich and diversified users results in an enormous amount of information being generated , which is helpful in many ways ( Kapoor et al , 2018 ) . Some of this even contains private information about the users , which others could misuse . Cases were also found where certain users were being targeted and harassed by people using this platform , a common scenario in cyberbullying ( Abaido , 2020 ) . Social media plays a crucial role in amplifying these protests and movements ( Won et al , 2017b ) . It enables political groups and protesters to organise protest movements and share information . It acts as a platform for the people who are underrepresented by giving a voice to them . It also offers new opportunities for people to engage in activism , political resistance , and protest outside the political groups and civic institutions . Thus , it has a social impact on everyone ( Pulido et al , 2018 ) . It is to be noted that social media , similar to news media , plays a vital role in its social and political events worldwide ( Holt et al , 2013 ) . For the above reasons , we can state that social media plays a crucial role in most worldwide events . The English language is widely regarded as the first Lingua Franca . Statistically , it is one of the most widely spoken languages globally , having official status in over 53 countries ( Crystal , 2008 ) . Over 400 million people speak English as their primary language and widely spoken in the United States and the United Kingdom . BlackLivesMatter ( Dave et al , 2020 ) , EarthDay ( Rome , 2010 ) are some of the major protests that have occurred in these countries . Espa\u00f1ol commonly referred to as Spanish , is spoken by over 360 million people worldwide , with most of its speakers residing in Mexico , Argentina , Spain . 15 - M Movement ( Casero - Ripoll\u00e9s andFeenstra , 2012 ) andYoSoy132 ( Garc\u00eda andTrer\u00e9 , 2014 ) are some of the recent protests where people have been vocal about in the Spanish language . Portuguese has over 220 million native speakers . Brazil , Portugal , Angola are some of the major countries where this language is spoken . Protests like Racism Kills , May 68 ( Ross , 2008 ) are the recent ones that occurred in the Portuguese language . The recent upheavals of protests are due to so - Sentence Language Label Fabius ran against Royal for the presidential nomination in 2007 . English Event He planned to start a race war . English Event Metro police intervened and the fire was put out . English Not - event Pero no es\u00e9se el mayor problema . Spanish Event La Argentina retroceder\u00eda un paso todos los d\u00edas . Spanish Event Carri\u00f3 no objet\u00f3 que se trat\u00f3 de un secuestro . Spanish Not - event Os servidores do Piau\u00ed est\u00e3o em greve h\u00e1 17 dias . Portuguese Not - event E uma nova experi\u00eancia mobilizat\u00f3ria . Portuguese Event E decidiram ir\u00e0s aulas e passar o dia de saia . Portuguese Not - event cial media , youth , exaggeration of certain events . ( Basile and Caselli , 2020 ) . Any early detection of mass protest detection through social media platforms such as Facebook , Twitter , and Instagram to help minimizing the aftermath of the protests ( Wilson , 2017 ) . This has motivated Natural Language Processing ( NLP ) researchers to develop NLP systems to generalize on data coming from diverse sources to leverage the NLP systems to more realistic environments ( B\u00fcy\u00fck\u00f6z et al , 2020 ) . Hence , there is a need to develop NLP systems that could be generalized to any protest / events ( Peng et al , 2013 ) , which has motivated us to participate in the shared task for multilingual protest detection ( H\u00fcrriyetoglu et al , 2019a The objective of the task is to identify if any sentence talks about any mentions of protests or events in three languages , namely , English , Spanish , and Portuguese . Hence , we treat this as a sequence classification task . The rest of the paper is organized as follows , Section 2 presents previous work on protest detection and analysis . Section 3 entails a comprehensive analysis of the dataset used for our cause . Next , section 4 gives a detailed description of the models used for the multilingual event detection . Finally , section 5 analyses the results obtained , and Section 6 concludes our work while discussing the potential directions for future work .", "entities": [[897, 899, "TaskName", "event detection"]]}
{"text": "We used pretrained transformer - based models for identifying if a sentence talks about an event or not . The models that were used are BERT ( Devlin et al , 2019 ) , RoBERTa ( Liu et al , 2019 ) and DistilBERT ( Sanh et al , 2019 ) . Even though there are 3 different languages , we used a single model for all three due to memory constraints and reduced training time . We fine - tuned these models for sequence classification . Soft Voting is done on all these models to produce the respective final outputs for the languages . In soft voting , each classifier predicts that a specific data point belongs to the particular target class . A weighted sum of the predictions is done based on the importance of the classifier ( all models have equal weights ) . The overall prediction is chosen as the target with the greatest sum of the weighted probability , thus winning the vote ( Beyeler , 2017 ; Hande et al , 2021 ) .", "entities": [[25, 26, "MethodName", "BERT"], [34, 35, "MethodName", "RoBERTa"], [43, 44, "MethodName", "DistilBERT"]]}
{"text": "Robustly Optimized BERT ( RoBERTa ) ( Liu et al , 2019 ) follows the same architecture of BERT while differing in the pretraining strategy . It is pretrained with MLM as its objective where the model tries to predict the masked words . RoBERTa model is trained on the vast English Wikipedia and CC - News datasets . The NSP is not employed as a pretraining strategy , and the tokens are dynamically masked , making the model slightly different to BERT . During tokenization , RoBERTa follows byte - pair encoding ( BPE ) ( Gall\u00e9 , 2019 ) as opposed to WordPiece employed in BERT . We use robertabase , a pretrained language model consisting of 12 layers , 768 hidden , 12 attention heads , and 125 million parameters .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "RoBERTa"], [18, 19, "MethodName", "BERT"], [30, 31, "DatasetName", "MLM"], [44, 45, "MethodName", "RoBERTa"], [54, 57, "DatasetName", "CC - News"], [82, 83, "MethodName", "BERT"], [87, 88, "MethodName", "RoBERTa"], [94, 95, "MethodName", "BPE"], [104, 105, "MethodName", "WordPiece"], [107, 108, "MethodName", "BERT"]]}
{"text": "According to the Oxford English Dictionary , emotion is defined as \" [ a ] strong feeling deriving from one 's circumstances , mood , or relationships with others . \" 1 This \" standard \" definition identifies emotions as constructs involving something innate that is often invoked in social interactions and that aids in communicating with others ( Hwang and Matsumoto , 2016 ) . It is no exaggeration that humans are emotional beings : Emotions are an integral part of human life , and affect our decision making as well as our mental and physical health . As such , developing emotion detection models is important ; they have a wide array of applications , ranging from building nuanced virtual assistants that cater for the emotions of their users to detecting the emotions of social media users in order to understand their mental and/or physical health . However , emotion detection has remained a challenging task , partly due to the limited availability of labeled data and partly due the controversial nature of what emotions themselves are ( Aaron C. Weidman and Tracy , 2017 ) . Recent advances in machine learning for natural language processing ( NLP ) suggest that , given enough labeled data , there should be an opportunity to build better emotion detection models . Manual labeling of data , however , is costly and so it is desirable to develop labeled emotion data without annotators . While the proliferation of social media has made it possible for us to acquire large datasets with implicit labels in the form of hashtags ( Mohammad and Kiritchenko , 2015 ) , such labels are noisy and reliable . In this work , we seek to enable deep learning by creating a large dataset of fine - grained emotions using Twitter data . More specifically , we harness cues in Twitter data in the form of emotion hashtags as a way to build a labeled emotion dataset that we then exploit using distant supervision ( Mintz et al , 2009 ) ( the use of hashtags as a surrogate for annotator - generated emotion labels ) to build emotion models grounded in psychology . We construct such a dataset and exploit it using powerful deep learning methods to build accurate , high coverage models for emotion prediction . Overall , we make the following contributions : 1 ) Grounded in psychological theory of emotions , we build a large - scale , high quality dataset of tweets labeled with emotions . Key to this are methods to ensure data quality , 2 ) we validate the data collection method using human annotations , 3 ) we develop powerful deep learning models using a gated recurrent network to exploit the data , yielding new state - of - the - art on 24 fine - grained types of emotions , and 4 ) we extend the task beyond these emotion types to model Plutick 's 8 primary emotion dimensions . Our emotion modeling relies on distant supervision ( Read , 2005 ; Mintz et al , 2009 ) , the approach of using cues in data ( e.g. , hashtags or emoticons ) as a proxy for \" ground truth \" labels as we explained above . Distant supervision has been investigated by a number of researchers for emotion detection ( Tanaka et al , 2005 ; Mohammad , 2012 ; Purver and Battersby , 2012 ; Wang et al , 2012 ; Pak and Paroubek , 2010 ; Yang et al , 2007 ) and for other semantic tasks such as sentiment analysis ( Read , 2005 ; Go et al , 2009 ) and sarcasm detection ( Gonz\u00e1lez - Ib\u00e1nez et al , 2011 ) . In these works , authors successfully use emoticons and/or hashtags as marks to label data after performing varying degrees of data quality assurance . We take a similar approach , using a larger collection of tweets , richer emotion definitions , and stronger filtering for tweet quality . The remainder of the paper is organized as follows : We first overview related literature in Section 2 , describe our data collection in Section 3.1 , and the annotation study we performed to validate our distant supervision method in Section 4 . We then describe our methods in Section 5 , provide results in Section 6 , and conclude in Section 8 .", "entities": [[7, 8, "DatasetName", "emotion"], [88, 90, "TaskName", "decision making"], [103, 104, "DatasetName", "emotion"], [151, 152, "DatasetName", "emotion"], [217, 218, "DatasetName", "emotion"], [238, 239, "DatasetName", "emotion"], [319, 320, "DatasetName", "emotion"], [328, 329, "DatasetName", "emotion"], [356, 357, "DatasetName", "emotion"], [361, 362, "DatasetName", "emotion"], [388, 389, "DatasetName", "emotion"], [491, 492, "DatasetName", "emotion"], [499, 500, "DatasetName", "emotion"], [503, 504, "DatasetName", "emotion"], [560, 561, "DatasetName", "emotion"], [604, 606, "TaskName", "sentiment analysis"], [618, 620, "TaskName", "sarcasm detection"], [668, 669, "DatasetName", "emotion"]]}
{"text": "A number of studies have also been performed to analyze and/or model mood in social media data . ( De Choudhury et al , 2012 ) identify more than 200 moods frequent on Twitter as extracted from psychological literature and filtered by AMT workers . They then collect tweets which have one of the moods in their mood lexicon in the form of a hashtag . To verify the quality of the mood data , the authors run AMT studies where they ask workers whether a tweet displayed the respective mood hashtag or not and find that in 83 % of the cases hashtagged moods at the end of posts did capture users ' moods , whereas for posts with mood hashtags anywhere in the tweet , only 58 % of the cases capture the mood of users . Although they did not build models for mood detection , the annotation studies ( De Choudhury et al , 2012 ) perform further support our specific use of hashtags to label emotions . ( Mishne and De Rijke , 2006 ) collect user - labeled mood from blog post text on LiveJournal and exploit them for predicting the intensity of moods over a time span rather than at the post level . Similarly , ( Nguyen , 2010 ) builds models to infer patterns of moods in a large collection of LiveJournal posts . Some of the moods in these LiveJournal studies ( e.g. , hungry , cold ) , as ( De Choudhury et al , 2012 ) explain , would not fit any psychological theory . Our work is different in that it is situated in psychological theory of emotion .", "entities": [[280, 281, "DatasetName", "emotion"]]}
{"text": "In spite of the effectiveness of feature engineering for NLP , it is a labor intensive task that also needs domain expertise . More importantly , feature engineering falls short of extracting and organizing all the discriminative information from data Goodfellow et al , 2016 ) . Neural networks ( Goodfellow et al , 2016 ) have emerged as a successful class of methods that has the power of automatically discovering the representations needed for detection or classification and has been successfully applied to multiple NLP tasks . A line of studies in the literature ( e.g. , ( Labutov and Lip - son , 2013 ; Maas et al , 2011 ; Tang et al , 2014b , a ) aim to learn sentiment - specific word embeddings ( Bengio et al , 2003 ; from neighboring text . Another thread of research focuses on learning semantic composition ( Mitchell and Lapata , 2010 ) , including extensions to phrases and sentences with recursive neural networks ( a class of syntax - tree models ) ( Socher et al , 2013 ; Irsoy and Cardie , 2014 ; Li et al , 2015 ) and to documents with distributed representations of sentences and paragraphs ( Le and Mikolov , 2014 ; Tang et al , 2015 ) for modeling sentiment . Long - short term memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) and Gated Recurrent Neural Nets ( GRNNs ) ( Cho et al , 2014 ; Chung et al , 2015 ) , variations of recurrent neural networks ( RNNs ) , a type of networks suitable for handling time - series data like speech ( Graves et al , 2013 ) or handwriting recognition ( Graves , 2012 ; Graves and Schmidhuber , 2009 ) , have also been used successfully for sentiment analysis ( Ren et al , 2016 ; Tai et al , 2015 ; Tang et al , 2015 ; . Convolutional neural networks ( CNNs ) have also been quite successful in NLP , and have been applied to a range of sentence classification tasks , including sentiment analysis ( Blunsom et al , 2014 ; Kim , 2014 ; Zhang et al , 2015 ) . Other architectures have also been recently proposed ( e.g. , ( Bradbury et al , 2016 ) ) . A review of neural network methods for NLP can be found in ( Goldberg , 2016 ) .", "entities": [[6, 8, "TaskName", "feature engineering"], [26, 28, "TaskName", "feature engineering"], [127, 129, "TaskName", "word embeddings"], [147, 149, "TaskName", "semantic composition"], [228, 229, "MethodName", "LSTM"], [289, 291, "TaskName", "handwriting recognition"], [309, 311, "TaskName", "sentiment analysis"], [353, 355, "TaskName", "sentence classification"], [358, 360, "TaskName", "sentiment analysis"]]}
{"text": "To be able to use deep learning for modeling emotion , we needed a large dataset of labeled tweets . Since there is no such human - labeled dataset publicly available , we follow ( Mohammad , 2012 ; Mintz et al , 2009 ; Purver and Battersby , 2012 ; Gonz\u00e1lez - Ib\u00e1nez et al , 2011 ; Wang et al , 2012 ) in adopting distant supervision : We collect tweets with emotion - carrying hashtags as a surrogate for emotion labels . To be able to collect enough tweets to serve our need , we developed a list of hashtags representing each of the 24 emotions proposed by Robert Plutchick ( Plutchik , 1980 ( Plutchik , , 1985 ( Plutchik , , 1994 . Plutchik ( Plutchik , 2001 ) organizes emotions in a three - dimensional circumplex model analogous to the colors on a color wheel . The cone 's vertical dimension represents intensity , and the 3 circle represent degrees of similarity among the various emotion types . The eight sectors are meant to capture that there are eight primary emotion dimensions arranged as four pairs of opposites . Emotions in the blank spaces are the primary emotion dyads ( i.e. , emotions that are mixtures of two of the primary emotions ) . For this work , we exclude the dyads in the exploded model from our treatment . For simplicity , we refer to the circles as plutchik - 1 : with the emotions { admiration , amazement , ecstasy , grief , loathing , rage , terror , vigilance } , plutchik - 2 : with the emotions { joy , trust , fear , surprise , sadness , disgust , anger , anticipation } , and plutchik - 3 : with the emotions { acceptance , annoyance , apprehension , boredom , distraction , interest , pensiveness , serenity } . The wheel is shown in Figure 1 . For each emotion type , we prepared a seed set of hashtags representing the emotion . We used Google synonyms and other online dictionaries and thesauri ( e.g. , www.thesaurus . com ) to expand the initial seed set of each emotion . We acquire a total of 665 emotion hashtags across the 24 emotion types . For example , for the joy emotion , a subset of the seeds in our expanded set is { \" happy \" , \" happiness \" , \" joy \" , \" joyful \" , \" joyfully \" , \" delighted \" , \" feelingsunny \" , \" blithe \" , \" beatific \" , \" exhilarated \" , \" blissful \" , \" walkingonair \" , \" jubilant \" } . We then used the expanded set to extract tweets with hashtags from the set from a number of massive - scale in - house Twitter datasets . We also used Twitter API to crawl Twitter with hashtags from the expanded set . Using this method , we were able to acquire a dataset of about 1/4 billion tweets covering an extended time span from July 2009 till January 2017 .", "entities": [[9, 10, "DatasetName", "emotion"], [74, 75, "DatasetName", "emotion"], [82, 83, "DatasetName", "emotion"], [171, 172, "DatasetName", "emotion"], [186, 187, "DatasetName", "emotion"], [203, 204, "DatasetName", "emotion"], [331, 332, "DatasetName", "emotion"], [343, 344, "DatasetName", "emotion"], [347, 348, "DatasetName", "Google"], [370, 371, "DatasetName", "emotion"], [378, 379, "DatasetName", "emotion"], [383, 384, "DatasetName", "emotion"], [392, 393, "DatasetName", "emotion"], [398, 399, "DatasetName", "seeds"]]}
{"text": "Twitter data are very noisy , not only because of use of non - standard typography ( which is less of a problem here ) but due to the many duplicate tweets and the fact that tweets often have multiple emotion hashtags . Since these reduce our ability to build accurate models , we need to clean the data and remove duplicates . Starting with > 1/4 billion tweets , we employ a rigorous and strict pipeline . This results in a vastly smaller set of about 1.6 million dependable labeled tweets . Since our goal is to create non - overlapping categories at the level of a tweet , we first removed all tweets with hashtags belonging to more than one emotion of the 24 emotion categories . Since it was observed ( e.g. , ( Mohammad , 2012 ; Wang et al , 2012 ) ) and also confirmed by our annotation study as described in Section 4 , that hashtags in tweets with URLs are less likely to correlate with a true emotion label , we remove all tweets with URLs from our data . We filter out duplicates using a two - step procedure : 1 ) we remove all retweets ( based on existence of the token \" RT \" regardless of case ) and 2 ) we use the Python library pandas http://pandas . pydata.org/ \" drop duplicates \" method to compare the tweet texts of all the tweets after normalizing character repetitions [ all consecutive characters of > 2 to 2 ] and user mentions ( as detected by a string starting with an \" @ \" sign ) . We then performed a manual inspection of a random sample of 1 , 000 tweets from the data and found no evidence of any remaining tweet duplicates . Next , even though the emotion hashtags themselves are exclusively in English , we observe the data do have tweets in languages other than English . This is due to code - switching , but also to the fact that our data dates back to 2009 and Twitter did not allow use of hashtags for several non - English languages until 2012 . To filter out non - English , we use the langid ( Lui and Baldwin , 2012 ) ( https://github.com/ saffsd / langid.py ) library to assign language tags to the tweets . Since the common wisdom in the literature ( e.g. , ( Mohammad , 2012 ; Wang et al , 2012 ) ) is to restrict data to hashtags occurring in final position of a tweet , we investigate correlations between a tweet 's relevance and emotion hashtag location in Section 4 and test models exclusively on data with hashtags occurring in final position . We also only use tweets con - taining at least 5 words . Table 2 shows statistics of the data after applying our cleaning , filtering , language identification , and deduplication pipeline . Since our focus is on English , we only show statistics for tweets tagged with an \" en \" ( for \" English \" ) label by langid . Table 2 provides three types of relevant statistics : 1 ) counts of all tweets , 2 ) counts of tweets with at least 5 words and the emotion hashtags occurring in the last quarter of the tweet text ( based on character count ) , and 3 ) counts of tweets with at least 5 words and the emotion hashtags occurring as the final word in the tweet text . As the last column in Table 2 shows , employing our most strict criterion where an emotion hashtag must occur finally in a tweet of a minimal length 5 words , we acquire a total of 1 , 608 , 233 tweets : 205 , 125 tweets for plutchik - 1 , 790 , 059 for plutchik - 2 , and 613 , 049 for plutchik - 3 .", "entities": [[40, 41, "DatasetName", "emotion"], [122, 123, "DatasetName", "emotion"], [126, 127, "DatasetName", "emotion"], [175, 176, "DatasetName", "emotion"], [310, 311, "DatasetName", "emotion"], [446, 447, "DatasetName", "emotion"], [492, 494, "TaskName", "language identification"], [556, 557, "DatasetName", "emotion"], [587, 588, "DatasetName", "emotion"], [615, 616, "DatasetName", "emotion"]]}
{"text": "For our core modeling , we use Gated Recurrent Neural Networks ( GRNNs ) , a modern variation of recurrent neural networks ( RNNs ) , which we now turn to introduce . For notation , we denote scalars with italic lowercase ( e.g. , x ) , vectors with bold lowercase ( e.g. , x ) , and matrices with bold uppercase ( e.g. , W ) . Recurrent Neural Network A recurrent neural network ( RNN ) is one type of neural network architecture that is particularly suited for modeling sequential information . At each time step t , an RNN takes an input vector x t IR n and a hidden state vector h t\u22121 IR m and produces the next hidden state h t by applying the recursive operation : h t = f ( Wx t + Uh t\u22121 + b ) ( 1 ) Where the input to hidden matrix W IR mxn , the hidden to hidden matrix U IR mxm , and the bias vector b IR m are parameters of an affine transformation and f is an element - wise nonlinearity . While an RNN can in theory summarize all historical information up to time step h t , in practice it runs into the problem of vanishing / exploding gradients ( Bengio et al , 1994 ; Pascanu et al , 2013 ) while attempting to learn longrange dependencies . LSTM Long short - term memory ( LSTM ) networks ( Hochreiter and Schmidhuber , 1997 ) addresses this exact problem of learning long - term dependencies by augmenting an RNN with a memory cell c t IR n at each time step . As such , in addition to the input vector x t , the hiddent vector h t\u22121 , an LSTM takes a cell state vector c t\u22121 and produces h t and c t via the following calculations : i t = \u03c3 W i x t + U i h t\u22121 + b i f t = \u03c3 W f x t + U f h t\u22121 + b f o t = \u03c3 ( W o x t + U o h t\u22121 + b o ) g t = tanh ( W g x t + U g h t\u22121 + b g ) c t = f t c t\u22121 + i t g t h t = o t tanh ( c t ) ( 2 ) Where \u03c3 ( ) and tanh ( ) are the element - wise sigmoid and hyperbolic tangent functions , the element - wise multiplication operator , and i t , f t , o t are the input , forget , and output gates . The g t is a new memory cell vector with candidates that could be added to the state . The LSTM parameters W j , U j , and b j are for j { i , f , o , g } . GRNNs ( Cho et al , 2014 ; Chung et al , 2015 ) propose a variation of LSTM with a reset gate r t , an update state z t , and a new simpler hidden unit h t , as follows : r t = \u03c3 ( W r x t + U r h t\u22121 + b r ) z t = \u03c3 ( W z x t + U z h t\u22121 + b z ) h t = tanh Wx t + r t * Uhh t\u22121 + bh h t = z t * h t\u22121 + ( 1 \u2212 z t ) * h t ( 3 ) The GRNN parameters W j , U j , and b j are for j { r , z , h } . In this set up , the hidden state is forced to ignore a previous hidden state when the reset gate is close to 0 , thus enabling the network to forget or drop irrelevant information . Additionally , the update gate controls how much information carries over from a previous hidden state to the current hidden state ( similar to an LSTM memory cell ) . We use GRNNs as they are simpler and faster than LSTM . For GRNNs , we use Theano ( Theano Development Team , 2016 ) . Online Classifiers We compare the performance of the GRNNs to four online classifiers that are capable of handling the data size : Stochastic Gradient Descent ( SGD ) , Multinomial Naive Bayes ( MNB ) , Perceptron , and the Passive Agressive Classifier ( PAC ) . These classifiers learn online from mini - batches of data . We use minibatches of 10 , 000 instances with all the four classifiers . We use the scikit - learn implementation of these classifiers ( http://scikit - learn . org ) . Settings We aim to model Plutchik 's 24 finegrained emotions as well as his 8 primary emotion dimensions where each 3 related types of emotion ( perceived as varying in intensity ) are combined in one dimension . We now turn to describing our experiments experiments .", "entities": [[240, 241, "MethodName", "LSTM"], [241, 246, "MethodName", "Long short - term memory"], [247, 248, "MethodName", "LSTM"], [303, 304, "MethodName", "LSTM"], [481, 482, "MethodName", "LSTM"], [523, 524, "MethodName", "LSTM"], [666, 667, "DatasetName", "0"], [704, 705, "MethodName", "LSTM"], [719, 720, "MethodName", "LSTM"], [757, 760, "MethodName", "Stochastic Gradient Descent"], [761, 762, "MethodName", "SGD"], [841, 842, "DatasetName", "emotion"], [849, 850, "DatasetName", "emotion"]]}
{"text": "In this paper , we built a large , automatically curated dataset for emotion detection using distant supervision and then used GRNNs to model finegrained emotion , achieving a new state - of - the - art performance . We also extended the classification to 8 primary emotion dimensions situated in psychological theory of emotion .", "entities": [[13, 14, "DatasetName", "emotion"], [25, 26, "DatasetName", "emotion"], [47, 48, "DatasetName", "emotion"], [54, 55, "DatasetName", "emotion"]]}
{"text": "Emotion lexicons describe the affective meaning of words and thus constitute a centerpiece for advanced sentiment and emotion analysis . Yet , manually curated lexicons are only available for a handful of languages , leaving most languages of the world without such a precious resource for downstream applications . Even worse , their coverage is often limited both in terms of the lexical units they contain and the emotional variables they feature . In order to break this bottleneck , we here introduce a methodology for creating almost arbitrarily large emotion lexicons for any target language . Our approach requires nothing but a source language emotion lexicon , a bilingual word translation model , and a target language embedding model . Fulfilling these requirements for 91 languages , we are able to generate representationally rich high - coverage lexicons comprising eight emotional variables with more than 100k lexical entries each . We evaluated the automatically generated lexicons against human judgment from 26 datasets , spanning 12 typologically diverse languages , and found that our approach produces results in line with state - of - the - art monolingual approaches to lexicon creation and even surpasses human reliability for some languages and variables . Code and data are available at github.com/JULIELab/MEmoLon archived under", "entities": [[17, 18, "DatasetName", "emotion"], [90, 91, "DatasetName", "emotion"], [105, 106, "DatasetName", "emotion"], [110, 112, "TaskName", "word translation"]]}
{"text": "An emotion lexicon is a lexical repository which encodes the affective meaning of individual words ( lexical entries ) . Most simply , affective meaning can be encoded in terms of polarity , i.e. , the distinction whether an item is considered as positive , negative , or neutral . This is the case for many well - known resources such as WORDNET - AFFECT ( Strapparava and Valitutti , 2004 ) , SENTIWORD - NET ( Baccianella et al , 2010 ) , or VADER ( Hutto and Gilbert , 2014 ) . Yet , an increasing number of researchers focus on more expressive encodings for affective states inspired by distinct lines of work in psychology Buechel and Hahn , 2017 ; Sedoc et al , 2017 ; Abdul - Mageed and Ungar , 2017 ; Bostan and Klinger , 2018 ; Mohammad , 2018 ; Troiano et al , 2019 ) . Psychologists , on the one hand , value such lexicons as a controlled set of stimuli for designing experiments , e.g. , to investigate patterns of lexical access or the structure of memory Monnier and Syssau , 2008 ) . NLP researchers , on the other hand , use them to augment the emotional loading of word embeddings ( Yu et al , 2017 ; Khosla et al , 2018 ) , as additional input to sentence - level emotion models so that the performance of even the most sophisticated neural network gets boosted ( Mohammad and Bravo - Marquez , 2017 ; De Bruyne et al , 2019 ) , or rely on them in a keyword - spotting approach when no training data is available , e.g. , for studies dealing with historical language stages . As with any kind of manually curated resource , the availability of emotion lexicons is heavily restricted to only a few languages whose exact number varies depending on the variables under scrutiny . For example , we are aware of lexicons for 15 languages that encode the emotional variables of Valence , Arousal , and Dominance ( see Section 2 ) . This number leaves the majority of the world 's ( less - resourced ) languages without such a dataset . In case such a lexicon exists for a particular language , it is often severely limited in size , sometimes only comprising some hundreds of entries ( Davidson and Innes - Ker , 2014 ) . Yet , even the largest lexicons typically cover only some ten thousands of words , still leaving out major portions of the emotion - carrying vocabulary . This is especially true for languages with complex morphology or productive compounding , such as Finnish , Turkish , Czech , or German . Finally , the diversity of emotion representation schemes adds another layer of complexity . While psychologists and NLP researchers alike find that different sets of emotional variables are complementary to each other ( Stevenson et al , 2007 ; Pinheiro et al , 2017 ; Barnes et al , 2019 ; De Bruyne et al , 2019 ) , manually creating emotion lexicons for every language and every emotion representation scheme is virtually impossible . We here propose an approach based on crosslingual distant supervision to generate almost arbitrarily large emotion lexicons for any target language and emotional variable , provided the following requirements are met : a source language emotion lexicon covering the desired variables , a bilingual word translation model , and a target language embedding model . By fulfilling these preconditions , we can automatically generate emotion lexicons for 91 languages covering ratings for eight emotional variables and hundreds of thousands of lexical entries each . Our experiments reveal that our method is on a par with state - of - the - art monolingual approaches and compares favorably with ( sometimes even outperforms ) human reliability .", "entities": [[1, 2, "DatasetName", "emotion"], [210, 212, "TaskName", "word embeddings"], [233, 234, "DatasetName", "emotion"], [304, 305, "DatasetName", "emotion"], [432, 433, "DatasetName", "emotion"], [466, 467, "DatasetName", "emotion"], [522, 523, "DatasetName", "emotion"], [529, 530, "DatasetName", "emotion"], [551, 552, "DatasetName", "emotion"], [571, 572, "DatasetName", "emotion"], [580, 582, "TaskName", "word translation"], [600, 601, "DatasetName", "emotion"]]}
{"text": "Representing Emotion . Whereas research in NLP has focused for a very long time almost exclusively on polarity , more recently , there has been a growing interest in more informative representation structures for affective states by including different groups of emotional variables ( Bostan and Klinger , 2018 ) . Borrowing from distinct schools of thought in psychology , these variables can typically be subdivided into dimensional vs. discrete approaches to emotion representation ( Calvo and Mac Kim , 2013 ) . The dimensional approach assumes that emotional states can be composed out of several foundational factors , most noticeably Valence ( corresponding to polarity ) , Arousal ( measuring calmness vs. excitement ) , and Dominance ( the perceived degree of control in a social situation ) ; VAD , for short ( Bradley and Lang , 1994 ) . Conversely , the discrete approach assumes that emotional states can be reduced to a small , evolutionary motivated set of basic emotions ( Ekman , 1992 ) . Although the exact division of the set has been subject of hot debates , recently constructed datasets ( see Section 4 ) most often cover the categories of Joy , Anger , Sadness , Fear , and Disgust ; BE5 , for short . Plutchik 's Wheel of Emotion takes a middle ground between those two positions by postulating emotional categories which are yet grouped into opposite pairs along different levels of intensity ( Plutchik , 1980 ) . Another dividing line between representational approaches is whether target variables are encoded in terms of ( strict ) class - membership or scores for numerical strength . In the first case , emotion analysis translates into a ( multi - class ) classification problem , whereas the latter turns it into a regression problem . While our proposed methodology is agnostic towards the chosen emotion format , we will focus on the VAD and BE5 formats here , using numerical ratings ( see the examples in Table 1 ) due to the widespread availability of such data . Accordingly , this paper treats word emotion prediction as a regression problem . 1.6 7.4 2.7 1.2 2.9 3.3 3.9 2.5 nuclear 4.3 7.3 4.1 1.4 2.2 1.9 3.2 1.6 ownership 5.9 4.4 7.5 2.1 1.4 1.2 1.4 1.3 Fear , and Disgust [ BE5 ] . VAD uses 1 - to - 9 scales ( \" 5 \" encodes the neutral value ) and BE5 1 - to - 5 scales ( \" 1 \" encodes the neutral value ) . Building Emotion Lexicons . Usually , the ground truth for affective word ratings ( i.e. , the assignment of emotional values to a lexical item ) is acquired in a questionnaire study design where subjects ( annotators ) receive lists of words which they rate according to different emotion variables or categories . Aggregating individual ratings of multiple annotators then results in the final emotion lexicon ( Bradley and Lang , 1999 ) . Recently , this workflow has often been enhanced by crowdsourcing ( Mohammad and Turney , 2013 ) and best - worst scaling ( Kiritchenko and Mohammad , 2016 ) . As a viable alternative to manual acquisition , such lexicons can also be created by automatic means ( Bestgen , 2008 ; K\u00f6per and Schulte i m Walde , 2016 ; Shaikh et al , 2016 ) , i.e. , by learning to predict emotion labels for unseen words . Researchers have worked on this prediction problem for quite a long time . Early work tended to focus on word statistics , often in combination with linguistic rules ( Hatzivassiloglou and McKeown , 1997 ; Turney and Littman , 2003 ) . More recent approaches focus heavily on word embeddings , either using semi - supervised graph - based approaches Hamilton et al , 2016 ; Sedoc et al , 2017 ) or fully supervised methods ( Rosenthal et al , 2015 ; Li et al , 2017 ; Rothe et al , 2016 ; Du and Zhang , 2016 ) . Most important for this work , Buechel and Hahn ( 2018b ) report on near - human performance using a combination of FASTTEXT vectors and a multi - task feed - forward network ( see Section 4 ) . While this line of work can add new words , it does not extend lexicons to other emotional variables or languages . A relatively new way of generating novel labels is emotion representation mapping ( ERM ) , an annotation projection that translates ratings from one emotion format into another , e.g. , mapping VAD labels into BE5 , or vice versa ( Hoffmann et al , 2012 ; Hahn , 2016 , 2018a ; Alarc\u00e3o and Fonseca , 2017 ; Landowska , 2018 ; Zhou et al , 2020 ; Park et al , 2019 ) . While our work uses ERM to add additional emotion variables to the source lexicon , ERM alone can neither increase the coverage of a lexicon , nor adapt it to another language . Translating Emotions . The approach we propose is strongly tied to the observation by Leveau et al ( 2012 ) and Warriner et al ( 2013 ) who found - comparing a large number of existing emotion lexicons of different languages - that translational equivalents of words show strong stability and adherence to their emotional value . Yet , their work is purely descriptive . They do not exploit their observation to create new ratings , and only consider manual rather than automatic translation . Making indirect use of this observation , Mohammad and Turney ( 2013 ) offer machine - translated versions of their NRC Emotion Lexicon . Also , many approaches in cross - lingual sentiment analysis ( on the sentence - level ) rely on translating polarity lexicons ( Abdalla and Hirst , 2017 ; Barnes et al , 2018 ) . Perhaps most similar to our work , Chen and Skiena ( 2014 ) create ( polarity - only ) lexicons for 136 languages by building a multilingual word graph and propagating sentiment labels through that graph . Yet , their method is restricted to high frequency words - their lexicons cover between 12 and 4 , 653 entries , whereas our approach exceeds this limit by more than two orders of magnitude . Our methodology also resembles previous work which models word emotion for historical language stages ( Cook and Stevenson , 2010 ; Hamilton et al , 2016 ; Hellrich et al , 2018 ; Li et al , 2019 ) . Work in this direction typically comes up with a set of seed words with assumingly temporally stable affective meaning ( our work assumes stability against translation ) and then uses distributional methods to derive emotion ratings in the target language stage . However , gold data for the target language ( stage ) is usually inaccessible , often preventing evaluation against human judgment . In contrast , we here propose several alternative evaluation set - ups as an integral part of our methodology .", "entities": [[72, 73, "DatasetName", "emotion"], [281, 282, "DatasetName", "emotion"], [313, 314, "DatasetName", "emotion"], [353, 354, "DatasetName", "emotion"], [477, 478, "DatasetName", "emotion"], [493, 494, "DatasetName", "emotion"], [577, 578, "DatasetName", "emotion"], [631, 633, "TaskName", "word embeddings"], [707, 708, "MethodName", "FASTTEXT"], [755, 756, "DatasetName", "emotion"], [770, 771, "DatasetName", "emotion"], [830, 831, "DatasetName", "emotion"], [891, 892, "DatasetName", "emotion"], [972, 974, "TaskName", "sentiment analysis"], [1082, 1083, "DatasetName", "emotion"], [1147, 1148, "DatasetName", "emotion"]]}
{"text": "Our methodology integrates ( 1 ) cross - lingual generation and expansion of emotion lexicons and ( 2 ) their evaluation against gold and silver standard data . Consequently , a key aspect of our workflow design is how data is split into train , dev , and test sets at different points of the generation process . Figure 1 gives an overview of our framework including a toy example for illustration . Lexicon Generation . We start with a lexicon ( Source ) of arbitrary size , emotion format 1 and source language which is partitioned into train , dev , and test splits denoted by Source - train , Source - dev , and Source - test , respectively . Next , we leverage a bilingual word translation model between source and desired target language to build the first target - side emotion lexicon denoted as TargetMT . Source words are translated according to the model , whereas target - side emotion labels are simply copied from the source to the target ( see Section 2 ) . Entries are assigned to train , dev , or test set according to their source - side assignment ( cf . Figure 1 ) . The choice of our translation service ( see below ) ensures that each source word receives exactly one translation . TargetMT is then used as the distant supervisor to train a model that predicts word emotions based on target - side word embeddings . TargetMT - train and TargetMT - dev are used to fit model parameters and optimize hyperparameters , respectively , whereas TargetMT - test is held out for later evaluation . Once finalized , the model is used to predict new labels for the words in TargetMT , resulting in a second target - side emotion lexicon denoted TargetPred . Our rationale for doing so is that a reasonably trained model should generalize well ( 4.3 , 7.3 ) ) test ( Terrorismus , ( 1.6 , 7.4 ) : Schematic view on the methodology for generating and evaluating an emotion lexicon for a given target language based on source language supervision . Included is a toy example starting with an English VA lexicon ( sunshine , nuclear , terrorism and the associated numerical scores for Valence and Arousal ) and resulting in an extended German lexicon which incorporates translated entries with altered VA scores and additional entries originating from the embedding model with newly learned scores . over the entire TargetMT lexicon because it has access to the target - side embedding vectors . Hence , it may mitigate some of the errors which were introduced in previous steps , either by machine translation or by assuming that sourceand target - side emotion are always identical . We validate this assumption in Section 6 . We also predict ratings for all the words in the embedding model , leading to a large number of new entries . The splits are defined as follows : let M T train , M T dev , and M T test denote the set of words in train , dev , and test split of TargetMT , respectively . Likewise , let P train , P dev , and P test denote the splits of TargetPred and let E denote the set of words in the embedding model . Then P train : = M T train P dev : = M T dev \\ M T train P test : = ( M T test \u222a E ) \\ ( M T dev \u222a M T train ) The above definitions help clarify the way we address polysemy . 2 Ambiguity on the target - side 2 In short , our work evades this problem by dealing with lexical entries exclusively on the type - rather than the senselevel . From a lexicological perspective , this may seem like a strong assumption . From a modeling perspective , however , it appears almost obvious as it aligns well with the major components of our methodology , i.e. , lexicons , embeddings , and translation . The lexicons we work with follow the design of behavioral experiments : a stimulus ( word type ) is given to may result in multiple source entries translating to the same target - side word . 3 This circumstance leads to \" partial duplicates \" in TargetMT , i.e. , groups of entries with the same word type but different emotion values ( because they were derived from distinct Source entries ) . Such overlap could do harm to the integrity of our evaluation since knowledge may \" leak \" from training to validation phase , i.e. , by testing the model on words it has already seen during training , although with distinct emotion labels . The proposed data partitioning eliminates such distortion effects . Since partial duplicates receive the same embedding vector , the prediction model assigns the same emotion value to both , thus merging them in TargetPred . Evaluation Methodology . The main advantage of the above generation method is that it allows us to create large - scale emotion lexicons for languages a subject and the response ( rating ) is recorded . The absence of sense - level annotation simplifies the mapping between lexicon and embedding entries . While sense embeddings form an active area of research ( Camacho - Collados and Pilehvar , 2018 ; Chi and Chen , 2018 ) , to the best of our knowledge , type - level embeddings yield state - of - the - art performance in downstream applications . 3 Source - side polysemy , in contrast to its target - side counterpart , is less of a problem , because we receive only a single candidate during translation . This may result in cases where the translation misaligns with the copied emotion value in TargetMT . Yet , the prediction step partly mitigates such inconsistencies ( see Section 6 ) . for which gold data is lacking . But if that is the case , how can we assess the quality of the generated lexicons ? Our solution is to propose two different evaluation scenarios - a gold evaluation which is a strict comparison against human judgment , meaning that it is limited to languages where such data ( denoted TargetGold ) is available , and a silver evaluation which substitutes human judgments by automatically derived ones ( silver standard ) which is feasible for any language in our study . The rationale is that if both , gold and silver evaluation , strongly agree with each other , we can use one as proxy for the other when no target - side gold data exists ( examined in Section 6 ) . Note that our lexicon generation approach consists of two major steps , translation and prediction . However , these two steps are not equally important for each generated entry in TargetPred . Words , such as German Sonnenschein for which a translational equivalent already exists in the Source ( \" sunshine \" ; see Figure 1 ) , mainly rely on translation , while the prediction step acts as an optional refinement procedure . In contrast , the prediction step is crucial for words , such as Erdbeben , whose translational equivalents ( \" earthquake \" ) are missing in the Source . Yet , these words also depend on the translation step for producing training data . These considerations are important for deciding which words to evaluate on . We may choose to base our evaluation on the full TargetPred lexicon , including words from the training set - after all , the word emotion model does not have access to any target - side gold data . The problem with this approach is that it merges words that mainly rely on translation , because their equivalents are in the Source , and those which largely depend on prediction , because they are taken from the embedding model . In this case , generalizability of evaluation results becomes questionable . Thus , our evaluation methodology needs to fulfill the following two requirements : ( 1 ) evaluation must not be performed on translational equivalents of the Source entries to which the model already had access during training ( e.g. , Sonnenschein and nuklear in our example from Figure 1 ) ; but , on the other hand , ( 2 ) a reasonable number of instances must be available for evaluation ( ideally , as many as possible to increase reliability ) . The intricate cross - lingual train - dev - test set assignment of our generation methodology is in place so that we meet these two requirements . In particular , for our silver evaluation , we intersect TargetMT - test with TargetPred - test and compute the correlation of these two sets individually for each emotion variable . Pearson 's r will be used as correlation measure throughout this paper . Establishing a test set at the very start of our workflow , Source - test , assures that there is a relatively large overlap between the two sets and , by extension , that our requirements for the evaluation are met . The gold evaluation is a somewhat more challenging case , because we can , in general , not guarantee that the overlap of a TargetGold lexicon with TargetPred - test will be of any particular size . For this reason , the words of the embedding model are added to TargetPred - test ( see above ) , maximizing the expected overlap with TargetGold . In practical terms , we intersect TargetGold with TargetPred - test and compute the variable - wise correlation between these sets , in parallel to the silver evaluation . A complementary strategy for maximizing overlap , by exploiting dependencies between published lexicons , is described below .", "entities": [[13, 14, "DatasetName", "emotion"], [88, 89, "DatasetName", "emotion"], [128, 130, "TaskName", "word translation"], [144, 145, "DatasetName", "emotion"], [163, 164, "DatasetName", "emotion"], [246, 248, "TaskName", "word embeddings"], [303, 304, "DatasetName", "emotion"], [348, 349, "DatasetName", "emotion"], [451, 453, "TaskName", "machine translation"], [461, 462, "DatasetName", "emotion"], [751, 752, "DatasetName", "emotion"], [805, 806, "DatasetName", "emotion"], [832, 833, "DatasetName", "emotion"], [864, 865, "DatasetName", "emotion"], [987, 988, "DatasetName", "emotion"], [1294, 1295, "DatasetName", "emotion"], [1498, 1499, "DatasetName", "emotion"]]}
{"text": "Gold Lexicons and Data Splits . We use the English emotion lexicon from Warriner et al ( 2013 ) as first part of our Source dataset . This popular resource comprises about 14k entries in VAD format collected via crowdsourcing . Since manually gathered BE5 ratings are available only for a subset of this lexicon ( Stevenson et al , 2007 ) , we add BE5 ratings from Buechel and Hahn ( 2018a ) who used emotion representation mapping ( see Section 2 ) to convert the existing VAD ratings , showing that this is about as reliable as human annotation . As apparent from the previous section , a crucial aspect for applying our methodology is the design of the train - dev - test split of the Source because it directly impacts the amount of words we can test our lexicons on during gold evaluation . In line with these considerations , we choose the lexical items which are already present in ANEW ( Bradley and Lang , 1999 ) as Source - test set . ANEW is the precursor to the version later distributed by Warriner et al ( 2013 ) ; it is widely used and has been adapted to a wide range of languages . With this choice , it is likely that a resulting TargetPred - test set has a large overlap with the respective TargetGold lexicon . As for the TargetGold lexicons , we included every VA ( D ) and BE5 lexicon we could get hold of with more than 500 entries . This resulted in 26 datasets covering 12 quite diverse languages ( see Table 2 ) . Note that we also include English lexicons in the gold evaluation . In these cases , no translation will be carried out ( Source is identical to TargetMT ) so that only the expansion step is validated . Appendix A.1 gives further details on data preparation . Translation . We used the GOOGLE CLOUD TRANSLATION API 4 to produce word - to - word translation tables . This is a commercial service , total translation costs amount to 160 EUR . API calls were performed in November 2019 . Embeddings . We use the fastText embedding models from Grave et al ( 2018 ) trained for 157 languages on the respective WIKIPEDIA and the respective part of COMMONCRAWL . These resources not only greatly facilitate our work but also increase comparability across languages . The restriction to \" only \" 91 languages comes from intersecting the ones covered by the vectors with the languages covered by the translation service . Models . Since our proposed methodology is agnostic towards the chosen word emotion model , we will re - use models from the literature . In particular , we will rely on the multi - task learning feed - forward network ( MTLFFN ) worked out by Buechel and Hahn ( 2018b ) . This network constitutes the current state of the art for monolingual emotion lexicon creation ( expanding an existing lexicon for a given language ) for many of the datasets in Table 2 . The MTLFFN has two hidden layers of 256 and 128 units , respectively , and takes pre - trained embedding vectors as input . Its distinguishing feature is that hidden layer parameters are shared between the different emotion target variables , thus constituting a mild form of multi - task learning ( MTL ) . We apply MTL to VAD and BE5 variables individually ( but not between both groups ) , thus training two distinct emotion models per language , following the outcome of a development experiment . Details are given in Appendix A.2 together with the remainder of the model specifications . Being aware of the infamous instability of neural approaches ( Reimers and Gurevych , 2017 ) , we also employ a ridge regression model , an L 2 regularized version of linear regression , as a more robust , yet al o powerful baseline ( Li et al , 2017 ) .", "entities": [[10, 11, "DatasetName", "emotion"], [76, 77, "DatasetName", "emotion"], [324, 325, "TaskName", "Translation"], [330, 331, "DatasetName", "CLOUD"], [340, 342, "TaskName", "word translation"], [371, 372, "MethodName", "fastText"], [449, 450, "DatasetName", "emotion"], [470, 474, "TaskName", "multi - task learning"], [502, 503, "DatasetName", "emotion"], [561, 562, "DatasetName", "emotion"], [571, 575, "TaskName", "multi - task learning"], [600, 601, "DatasetName", "emotion"], [659, 661, "MethodName", "linear regression"]]}
{"text": "Emotion lexicons are at the core of sentiment analysis , a rapidly flourishing field of NLP . Yet , despite large community efforts , the coverage of existing lexicons is still limited in terms of languages , size , and types of emotion variables . While there are techniques to tackle these three forms of sparsity in isolation , we introduced a methodology which allows us to cope with them simultaneously by jointly combining emotion representation mapping , machine translation , and embedding - based lexicon expansion . Our study is \" large - scale \" in many respects . We created representationally complex lexiconscomprising 8 distinct emotion variables - for 91 languages with up to 2 million entries each . The evaluation of the generated lexicons featured 26 manually annotated datasets spanning 12 diverse languages . The predicted ratings showed consistently high correlation with human judgment , compared favorably with state - of - the - art monolingual approaches to lexicon expansion and even surpassed human inter - study reliability in some cases . The sheer number of test sets we used allowed us to validate fundamental methodological assumptions underlying our approach . Firstly , the evaluation procedure , which is integrated into the generation methodology , allows us to reliably estimate the quality of resulting lexicons , even without target language gold standard . Secondly , our data suggests that embedding - based word emotion models can be used as a repair mechanism , mitigating poor target - language emotion estimates acquired by simple word - to - word translation . Future work will have to deepen the way we deal with word sense ambiguity by way of exchanging the simplifying type - level approach our current work is based on with a semantically more informed sense - level approach . A promising direction would be to combine a multilingual sense inventory such as BABELNET ( Navigli and Ponzetto , 2012 ) with sense embeddings ( Camacho - Collados and Pilehvar , 2018 ) .", "entities": [[7, 9, "TaskName", "sentiment analysis"], [42, 43, "DatasetName", "emotion"], [74, 75, "DatasetName", "emotion"], [78, 80, "TaskName", "machine translation"], [107, 108, "DatasetName", "emotion"], [236, 237, "DatasetName", "emotion"], [251, 252, "DatasetName", "emotion"], [260, 262, "TaskName", "word translation"]]}
{"text": "Paraphrase Generation : A Survey of the State of the Art", "entities": [[0, 2, "TaskName", "Paraphrase Generation"]]}
{"text": "This paper focuses on paraphrase generation , which is a widely studied natural language generation task in NLP . With the development of neural models , paraphrase generation research has exhibited a gradual shift to neural methods in the recent years . This has provided architectures for contextualized representation of an input text and generating fluent , diverse and human - like paraphrases . This paper surveys various approaches to paraphrase generation with a main focus on neural methods .", "entities": [[4, 6, "TaskName", "paraphrase generation"], [26, 28, "TaskName", "paraphrase generation"], [70, 72, "TaskName", "paraphrase generation"]]}
{"text": "Paraphrases are texts that convey the same meaning while using different words or sentence structures . The generation of paraphrases is a longstanding problem for natural language learning . For example , the question How do I improve my English could be equivalently phrased as What is the best way to learn English . Paraphrasing can be play an important role in language understanding tasks , such as question answering ( Dong et al , 2017 ; Zhu et al , 2017 ) , machine translation ( Seraj et al , 2015 ; Thompson and Post , 2020a ) , and semantic parsing ( Berant and Liang , 2014 ; . And it is also a good way for data augmentation ( Kumar et al , 2019 ; Gao et al , 2020 ) . Given a sentence , paraphrase generation aims to create its paraphrases that can have a different wording or different structure from the original sentence , while preserving the original meaning . The focus of paraphrase generation has exhibited a gradual shift from classical approaches to more advanced neural approaches in the recent years with the rapid development of various neural models . Neural models have changed the traditional way paraphrase generation is performed and also provided new directions and architectures for the NLP community . While several surveys on the traditional methods and limited neural methods for paraphrase gener -", "entities": [[68, 70, "TaskName", "question answering"], [84, 86, "TaskName", "machine translation"], [101, 103, "TaskName", "semantic parsing"], [119, 121, "TaskName", "data augmentation"], [122, 123, "DatasetName", "Kumar"], [139, 141, "TaskName", "paraphrase generation"], [169, 171, "TaskName", "paraphrase generation"], [204, 206, "TaskName", "paraphrase generation"]]}
{"text": "What is the distance between Sun and Earth if at any time in the preparation of this product the integrity of this container is compromised it should not be used . this container should not be used if the product is compromised at any time in preparation . ation have been published ( Metzler et al , 2011 ; Gupta and Krzy\u017cak , 2020 ) , there is no thorough and comprehensive survey on neural methods for paraphrase generation . To our best knowledge , this is the first survey on neural methods for paraphrase generation . Therefore , our goal in this paper is to provide a timely survey on paraphrase generation , with a main focus on neural methods . In the following section , we will first introduce the most frequently used datasets for paraphrase generation ( Section 2 ) . Then we list the traditional evaluation metrics in Section 3 . In Section 4 , we present some of the traditional approaches that were used before the neural methods . Neural models , the main focus of this paper , will be discussed in Section 5 . After introducing all the methods , we compare the performance of the different models for paraphrase generation in Section 6 . Finally , we identify some research gaps in paraphrase generation .", "entities": [[77, 79, "TaskName", "paraphrase generation"], [94, 96, "TaskName", "paraphrase generation"], [111, 113, "TaskName", "paraphrase generation"], [137, 139, "TaskName", "paraphrase generation"], [206, 208, "TaskName", "paraphrase generation"], [220, 222, "TaskName", "paraphrase generation"]]}
{"text": "In this section , we describe several datasets that have been extensively used for paraphrase generation . PPDB The paraphrase database ( Ganitkevitch et al , 2013 ) WikiAnswer This dataset ( Fader et al , 2013 ) contains approximately 18 million word - aligned question pairs that are paraphrases . The word alignments provided by this dataset al o relate the synonyms in the paraphrase sentences . However , all the sentences provided in this dataset are questions , which restricts the paraphrases to only questions . MSCOCO MSCOCO ( Lin et al , 2014 ) ParaNMT ParaNMT ) is a dataset of more than 50 million English - English sentential paraphrase pairs . The pairs were generated automatically by using back - translation to translate the non - English side of a large Czech - English parallel corpus . Owing to its recency , it has not been used widely .", "entities": [[14, 16, "TaskName", "paraphrase generation"], [88, 89, "DatasetName", "MSCOCO"], [89, 90, "DatasetName", "MSCOCO"]]}
{"text": "Rule - based paraphrase generation approaches build on hand - crafted or automatically collected paraphrase rules . In the early works , these rules were mainly hand - crafted ( McKeown , 1983 ) . Due to the significant manual efforts , some researchers have sought to collect paraphrase rules automatically ( Lin and Pantel , 2001 ; Barzilay and Lee , 2003 ) . However , the limitation of the extracting methods has led to the generation of long and complex paraphrase patterns , in turn impacting performance .", "entities": [[3, 5, "TaskName", "paraphrase generation"]]}
{"text": "This approach is based on statistical machine translation ( SMT ) and is motivated by the fact that paraphrase generation can be seen as a special case of machine translation ( i.e. , monolingual machine translation ) . A machine translation model normally finds a best translation\u00ea of a text in language f to a text in language e by utilizing a statistical translation model p ( f | e ) and a language model p ( e ) : e = arg max e e * p ( f | e ) p ( e ) Applying this idea to paraphrase generation , such a model will find a best paraphraset of a text in the source side s to a text in the target side t obtained as , t = arg max t t * p ( s | t ) p ( t ) For instance , ( Wubben et al , 2010 ) constructed a large - scale parallel corpus containing paraphrases collected from the headlines that appeared in Google News . Then they trained a Phrase - Based Machine Translation model ( PBMT ) ( Koehn et al , 2007 ) on their parallel corpus using the MOSES package . The trained PBMT is finally used to generate paraphrases .", "entities": [[6, 8, "TaskName", "machine translation"], [18, 20, "TaskName", "paraphrase generation"], [28, 30, "TaskName", "machine translation"], [34, 36, "TaskName", "machine translation"], [39, 41, "TaskName", "machine translation"], [101, 103, "TaskName", "paraphrase generation"], [174, 175, "DatasetName", "Google"], [184, 186, "TaskName", "Machine Translation"], [203, 204, "DatasetName", "MOSES"]]}
{"text": "Early works on paraphrasing mainly focused on template - based or statistical machine translation approaches . However , the matching of templates and modeling of a statistical translation model are both challenging tasks . With the recent advances of neural networks , especially the sequence - to - sequence framework , Seq2Seq models were first use for paraphrase generation by ( Prakash et al , 2016 ) . Their work inspired the wide use of neural models for paraphrase generation . Below we introduce the main approaches based on neural models that are used for paraphrase generation .", "entities": [[12, 14, "TaskName", "machine translation"], [51, 52, "MethodName", "Seq2Seq"], [57, 59, "TaskName", "paraphrase generation"], [78, 80, "TaskName", "paraphrase generation"], [95, 97, "TaskName", "paraphrase generation"]]}
{"text": "Currently , most of the existing paraphrase generation models are based on sequence - to - sequence models consisting of an encoder and a decoder . The encoder will encode the source texts into a contextualized vector representation along with a list of vector representations capturing the semantics of each word and context . Then , the decoder will generate paraphrases based on the vectors given by the encoder .", "entities": [[6, 8, "TaskName", "paraphrase generation"]]}
{"text": "The main purpose of encoding is to extract the semantic information for the decoder to generate paraphrases . With the development of various neural models , researchers also have multiple choices for the encoder . Encoder With a consistent goal of learning better abstract contextualized representation of the input text , several architectures have been explored by researchers . ( Prakash et al , 2016 ) first utilized a seq2seq model implemented as recurrent neural networks - long short term memory networks ( LSTMs ) ( Hochreiter and Schmidhuber , 1997 ) - to process long sequences . A nonvolutional neural network ( CNN ) has also been used to construct seq2seq models as a CNN has fewer parameters and thus is faster to train ( Vizcarra and Ochoa - Luna , 2020 ) . The Transformer model ( Vaswani et al , 2017 ) has shown state - of - the - art performance on multiple text generation tasks . Due to the Transformer 's improved ability to capture long - range dependencies in sentences , utilized a Transformer to construct their seq2seq model . More recently , large language models using transformer architectures have achieved state - ofthe - art results for many NLP tasks while using less supervised data than before . Therefore , some researchers also utilized large pretrained language models such as GPT - 2 ( Radford et al , 2019 ) and BART ( Lewis et al , 2020 ) as their encoder - decoder framework ( Witteveen and Andrews , 2019 ; Hegde and Patil , 2020 ; Garg et al , 2021 ) .", "entities": [[69, 70, "MethodName", "seq2seq"], [111, 112, "MethodName", "seq2seq"], [136, 137, "MethodName", "Transformer"], [157, 159, "TaskName", "text generation"], [164, 165, "MethodName", "Transformer"], [179, 180, "MethodName", "Transformer"], [183, 184, "MethodName", "seq2seq"], [222, 225, "TaskName", "pretrained language models"], [227, 228, "MethodName", "GPT"], [238, 239, "MethodName", "BART"]]}
{"text": "At the decoding side , the contextualized representation is used at each decoding step with the vector representation of previously generated words . Finally , a distribution over the vocabulary is obtained and the word with highest probability will be generated . This method is greedy decoding . Besides , a more commonly used method called beam search ( Wiseman and Rush , 2016 ) is used , which identifies the k - best paths up to current timestep during decoding . However , greedy decoding and beam search methods are both generic approaches for all text generation tasks without a specific focus on paraphrase generation . Therefore , with the goal of generating paraphrases and avoiding the words existing in the source sentences , a few blocking mechanisms have been proposed to prevent the decoder from generating the same words in the source sentences . This is also a way to guarantee the diversity of the generated paraphrases and prevent the models from directly copying the input into the output paraphrases ( Niu et al , 2020 ; Thompson and Post , 2020b ) .", "entities": [[96, 98, "TaskName", "text generation"], [104, 106, "TaskName", "paraphrase generation"]]}
{"text": "Encoder - Decoder Architecture The numerous attempts that have been made to improve the Encoder - Decoder architecture for paraphrase generation can be broadly categorized into two types based on their focus : A. Model - focused ; and B. Attribute - focused . Next we introduce them respectively with more fine - grained divisions .", "entities": [[19, 21, "TaskName", "paraphrase generation"]]}
{"text": "Although recent neural models have shown great advances , state - of - the - art results are still not satisfactory enough . Therefore , more advanced paraphrasing models still need to be explored . Below we discuss several potential directions of research that we believe are worth studying . Pretrained language models Virtually all recent work related to the application of pretrained language models on paraphrase generation is quite naive . Therefore , we could combine the large pretrained language models with other mechanisms , for example reinforcement learning , VAE and GAN .", "entities": [[50, 53, "TaskName", "Pretrained language models"], [62, 65, "TaskName", "pretrained language models"], [66, 68, "TaskName", "paraphrase generation"], [79, 82, "TaskName", "pretrained language models"], [91, 92, "MethodName", "VAE"], [93, 94, "MethodName", "GAN"]]}
{"text": "We demonstrate that it is feasible to accurately diacritize Hebrew script without any human - curated resources other than plain diacritized text . We present NAKDIMON , a two - layer character - level LSTM , that performs on par with much more complicated curationdependent systems , across a diverse array of modern Hebrew sources . The model is accompanied by a training set and a test set , collected from diverse sources .", "entities": [[34, 35, "MethodName", "LSTM"]]}
{"text": "Learning directly from plain diacritized text can go a long way , even with relatively limited resources . NAKDIMON demonstrates that a simple architecture for diacritizing Hebrew text as a sequence tagging problem can achieve performance on par with much more complex systems . We also introduce and release a corpus of dotted Hebrew text , as well as a source - balanced test set . In the future , we wish to evaluate the utility of dotting as a feature for downstream tasks such as question answering , machine translation , and speech generation , taking advantage of the fact that our simplified model can be easily integrated in an end - to - end Hebrew processing system .", "entities": [[86, 88, "TaskName", "question answering"], [89, 91, "TaskName", "machine translation"]]}
{"text": "We present results for the Dicta test set in Table 5 . In order to provide fair comparison and to preempt overfitting on this test data , we ran this test in a preliminary setup on a variant of NAKDIMON which was not tuned or otherwise unfairly trained . This system , NAKDIMON 0 , differs from our final variant in three main aspects : it is not trained on the Dicta portion of our training corpus ( 2.2 ) , it is not trained on the AUTOMATIC corpus , and it employs a residual connection between the two character Bi - LSTM layers . Testing on the Dicta test set required some minimal evaluation adaptations resulting from encoding constraints ( for example , we do not distinguish between kamatz katan and kamatz gadol ) . Thus , we copy the results reported in", "entities": [[53, 54, "DatasetName", "0"], [94, 96, "MethodName", "residual connection"], [102, 103, "MethodName", "LSTM"]]}
{"text": "Transfer learning has proven to be crucial in advancing the state of speech and natural language processing research in recent years . In speech , a model pre - trained by self - supervised learning transfers remarkably well on multiple tasks . However , the lack of a consistent evaluation methodology is limiting towards a holistic understanding of the efficacy of such models . SUPERB was a step towards introducing a common benchmark to evaluate pretrained models across various speech tasks . In this paper , we introduce SUPERB - SG , a new benchmark focused on evaluating the semantic and generative capabilities of pre - trained models by increasing task diversity and difficulty over SUPERB . We use a lightweight methodology to test the robustness of representations learned by pre - trained models under shifts in data domain and quality across different types of tasks . It entails freezing pretrained model parameters , only using simple task - specific trainable heads . The goal is to be inclusive of all researchers , and encourage efficient use of computational resources . We also show that the task diversity of SUPERB - SG coupled with limited task supervision is an effective recipe for evaluating the generalizability of model representation . Equal contribution .", "entities": [[0, 2, "TaskName", "Transfer learning"], [31, 35, "TaskName", "self - supervised learning"]]}
{"text": "Transfer learning is a paradigm in machine learning that has been very effective for natural language processing ( NLP ) ( Peters et al , 2018 ; Devlin et al , 2019 ; Lan et al , 2019 ; Dong et al , 2019 ; Raffel et al , 2020 ; Conneau et al , 2020 ) , and speech processing ( van den Oord et al , 2018 ; Rivi\u00e8re et al , 2020 ; Chung et al , 2019 ; Schneider et al , 2019 ; Baevski et al , 2020b ; Hsu et al , 2021 ; Liu et al , 2020c , b ; Ravanelli et al , 2020 ; . Self - supervised learning ( SSL ) is the main driver of this paradigm , an effective and scalable way to learn high - level representation of language that transfers to a variety of tasks . SSL entails learning from the input or some perturbation of it without the need for labelled data . This has unlocked the usage of large amounts of cheaply available unlabelled data . It lends naturally to neural network models that have been shown to possess impressive scaling characteristics such that it is often enough to increase the model and data sizes to improve downstream performance ( Hestness et al , 2017 ; Jozefowicz et al , 2016 ; Mahajan et al , 2018 ; Radford et al , 2019 ) . Speech signal consists of acoustic , linguistic , prosodic , and speaker characteristics . SSL algo - rithms in speech must be evaluated in their ability to produce representations that are useful for tasks that demand understanding of linguistic , speaker , and prosodic elements of spoken language as well as high - level semantics . Researchers have used auto - regressive , contrastive , discriminative and multi - task learning objectives to pre - train models , and have investigated their capabilities across tasks like phoneme recognition ( van den Oord et al , 2018 ; Chung et al , 2019 ) , automatic speech recognition ( ASR ) ( Liu et al , 2020b ; Schneider et al , 2019 ; Ravanelli et al , 2020 ; Hsu et al , 2021 ; Chang et al , 2021 ) , speaker verification ( Fan et al , 2020 ) , speaker identification ( Chung et al , 2019 ; Liu et al , 2020c ) , emotion recognition ( Macary et al , 2021 ) , speech translation ( Chung et al , 2019 ) , voice conversion ( Lin et al , 2020 ; Huang et al , 2021a ) , spoken language understanding ( Lai et al , 2021 ) , and text - tospeech ( \u00c1lvarez et al , 2019 ) . However , the methodologies in such studies vary in the use of datasets , fine - tuning strategies and task - specific model architectures . To bridge this gap , SUPERB ( Yang et al , 2021 ) introduced a standardized benchmark of 10 speech tasks to compare 13 pre - trained models and a Log Mel - Filterbank baseline . It studied the models ' performance in tasks focusing on linguistic ( phoneme recognition and automatic speech recognition , keyword spotting and query by example ) , shallow semantic ( intent classification and slot filling ) , speaker ( speaker identification , speaker verification and speaker diarization ) , and prosodic ( emotion recognition ) characteristics . In this paper , we introduce SUPERB - SG , a benchmark with 5 new tasks , which are speech translation , out - of - domain ASR , voice conversion , speech separation , and speech enhancement , with an emphasis on evaluating the semantic and generative capabilities of pre - trained models that require high - level representations to capture linguistic , semantic , and speaker characteristics . These tasks go beyond speech recognition by focusing on various other aspects that are essential to building intelligent speech interfaces . Further , we show that while SSL models achieve close to state - of - the - art performance on many tasks , there is n't one model that outperforms all others , and that a simple Log Mel - Filterbank can perform competitively on some tasks . We also demonstrate the robustness of our methodology with an ablation study over different task - specific model architectures and data sizes .", "entities": [[0, 2, "TaskName", "Transfer learning"], [115, 119, "TaskName", "Self - supervised learning"], [120, 121, "DatasetName", "SSL"], [151, 152, "DatasetName", "SSL"], [256, 257, "DatasetName", "SSL"], [309, 313, "TaskName", "multi - task learning"], [346, 349, "TaskName", "automatic speech recognition"], [384, 386, "TaskName", "speaker verification"], [394, 396, "TaskName", "speaker identification"], [410, 412, "TaskName", "emotion recognition"], [430, 432, "TaskName", "voice conversion"], [446, 449, "TaskName", "spoken language understanding"], [545, 548, "TaskName", "automatic speech recognition"], [549, 551, "TaskName", "keyword spotting"], [560, 562, "TaskName", "intent classification"], [563, 565, "TaskName", "slot filling"], [569, 571, "TaskName", "speaker identification"], [572, 574, "TaskName", "speaker verification"], [575, 577, "TaskName", "speaker diarization"], [582, 584, "TaskName", "emotion recognition"], [616, 618, "TaskName", "voice conversion"], [619, 621, "TaskName", "speech separation"], [623, 625, "TaskName", "speech enhancement"], [661, 663, "TaskName", "speech recognition"], [684, 685, "DatasetName", "SSL"]]}
{"text": "As more powerful SSL models are proposed with promising performance on various tasks , researchers continually try to find extensive evaluation methods to assess model performance , and wish to holistically understand the capability of the learned representations in these models . SUPERB ( Yang et al , 2021 ) is a framework to benchmark the SSL models on 10 speech tasks by learning task - specific prediction heads on top of the frozen shared SSL models . Although the tasks in SUPERB span across different domains , most of them are simple classification problems , or only require utilization of shallow semantics . In contrast , we focus on harder semantic and generative tasks . Another recently proposed benchmark is the LeBenchmark ( Evain et al , 2021 ) , investigating the performance of SSL models trained on French data with three semantic tasks . However , they only consider wav2vec 2.0 ( Baevski et al , 2020b ) with different architectures as their upstream models ( i.e. , networks pre - trained with SSL ) . Here , we evaluate a diverse set of SSL models , and offer a more comprehensive analysis . The Zero Resource Speech Benchmark 2021 ( Nguyen et al , 2020 ) introduces unsupervised speech processing tasks , particularly the spoken language modeling problem . They evaluate the SSL models via zero - shot probings at four linguistic levels . While their benchmark task is specific for certain domain , we use various tasks to evaluate different aspects of SSL models . The HEAR 2021 Challenge 2 aims to develop general - purpose audio representation by focusing on audio tasks beyond speech that include sound event detection , speech commands and pitch & chroma classification . We specifically focus on various aspects of speech processing , thus providing a wide variety of spoken language tasks .", "entities": [[3, 4, "DatasetName", "SSL"], [56, 57, "DatasetName", "SSL"], [75, 76, "DatasetName", "SSL"], [135, 136, "DatasetName", "SSL"], [175, 176, "DatasetName", "SSL"], [186, 187, "DatasetName", "SSL"], [225, 226, "DatasetName", "SSL"], [256, 257, "DatasetName", "SSL"], [281, 284, "TaskName", "sound event detection"], [285, 287, "DatasetName", "speech commands"]]}
{"text": "Speech separation ( SS ) is the task of separating target speech from background interference . We choose frequency domain method instead of a time domain based method because of the stride size constraint and computational cost .", "entities": [[0, 2, "TaskName", "Speech separation"]]}
{"text": "Speech enhancement ( SE ) is the task of removing background noise from a degraded speech signal , and it aims to improve the perceived quality and intelligibility of the signal . We include this . A 3 - layer BLSTM model similar to the speech separation task is trained to predict the spectral mask for the clean signal . The mean square error between the predicted mask and INPSM is used as the objective .", "entities": [[0, 2, "TaskName", "Speech enhancement"], [45, 47, "TaskName", "speech separation"]]}
{"text": "We evaluate the tasks on 15 upstream models , which are PASE+ ( Ravanelli et al , 2020 ) , APC ( Chung et al , 2019 ) , VQ - APC , NPC ( Liu et al , 2020a ) , Mockingjay ( Liu et al , 2020c ) , TERA ( Liu et al , 2020b ) , DeCoAR 2.0 ( Ling and Liu , 2020 ) , Modifile CPC ( Rivi\u00e8re et al , 2020 ) , wav2vec family ( Schneider et al , 2019 ) ( Baevski et al , 2020a ) ( Baevski et al , 2020b ) and HuBERT ( Hsu et al , 2021 ) . They span across different architectures , sizes and learning objectives . Some models also use vector quantization which has an added benefit of signal compression . For grounding , we use Log Mel Filterbank as our baseline . The detailed properties of upstream mod - els are shown in Table 1 .", "entities": [[11, 12, "MethodName", "PASE+"], [129, 130, "TaskName", "quantization"]]}
{"text": "The results of the upstream models evaluated on SUPERB - SG are shown in Table 2 . We only report the averaged WER for OOD - ASR . Full results can be found in Appendix A. For speech - to - text tasks ( ST and OOD - ASR ) , wav2vec 2.0 and HuBERT offer competitive results , while DeCoAR 2.0 shows some improvements . In speech generation tasks ( VC , SS , and SE ) , FBANK yields comparable or superior performance than some SSL models , especially for those metrics that take the quality of the output signal into account . For VC , the 3 reported metrics have the same trend for respective models . Here , vq - wav2vec achieves the best performance on MCD and ASV , while HuBERT performs the best on WER . For SS , Hubert - Large achieves the best performance , followed by Modified CPC . PASE+ , which is pre - trained with denoising tasks , performs better than half the SSL models , but this observation does n't transfer to the other tasks . For SE , all upstream models perform comparably . The largest gap is only 0.17 in PESQ and 1.1 in STOI . Overall , no model outperforms all others on all tasks . However , HuBERT - Large performs most competitively on all downstream tasks , especially those requiring linguistic and semantic signals .", "entities": [[87, 88, "DatasetName", "SSL"], [158, 159, "MethodName", "PASE+"], [166, 167, "TaskName", "denoising"], [174, 175, "DatasetName", "SSL"]]}
{"text": "We analyze the correlations between tasks in SUPERB - SG to understand the similarity between tasks , and verify if the experimental results agree with the common understanding of related tasks based on shared representation they require . To compute the correlation , we first change all metrics into a higher - better manner . Then , we compute the Spearman 's rank correlation coefficients ( Spearman 's \u03c1 ) between all pairs of tasks . For multiple metrics contained in a single task , such as MCD / WER / ASV in VC as well as PESQ / STOI in SE , we compute each of them separately . To make our analysis more representative and generalized to all speech domains , we bring back the six tasks from SUPERB ( Yang et al , 2021 ) that are considered representative of the following four domains : ( i ) Content recognition tasks contain - ing Phoneme Recognition ( PR ) , Automatic Speech Recognition ( ASR ) ( ii ) Speaker identity tasks including Identification ( SID ) , Automatic Speaker Verification ( ASV ) ( iii ) Semantics task which is Intent Classification ( IC ) and ( iv ) Prosodic task which is Emotion Recognition ( ER ) . Together with the 5 tasks introduced in this paper , we show the results of total 11 downstream tasks with the 14 corresponding metrics in Figure 2 . Overall , results show that all tasks except SS and SE have strong positive correlation among them . One possible explanation for SS and SE not showing strong correlation is that the low - level information closely related to audio signals is more critical as they need to reconstruct clean speech from interfering speakers and background noise by estimating the STFT masks . As a result , high - level information extracted from SSL models has little benefit for these tasks but is helpful for other tasks . As noted earlier , there is only a small gap in performance between FBANK and SSL models . If we leave SS and SE out , all correlation coefficients are greater than 0.58 , showing that the SSL model representations are useful for multiple domains . Although the Spearman 's \u03c1 are large in general in Figure 2 , differences between tasks are observable . Here , we focus on the relation between correlation and similarity of tasks . We list the most and the least two correlated tasks comparing with ST , OOD - ASR , VC , SS , and SE . SS and SE are skipped as candidates for for the least correlated tasks since they dominate the results . For VC , we average the correlation coefficients across the three metrics . The results are shown in Table 3 . ST and OOD - ASR are highly correlated with ASR since they both transform speech signals into discrete text tokens . IC is also correlated with ST since semantic information is required to perform both tasks . Moreover , ASV and VC are the least correlated tasks since they primarily focus on the speaker information with lesser regard to the semantic content . However , the absolute correlation values are still larger than 0.7 . For VC , the speaker information needs to be removed while the content has to be kept , similar to PR and ASR but different from SID . SS and SE are correlated with each other and have a much lower correlation with speaker identity and semantics tasks , supporting our assumption . Overall , we find that empirically highly - correlated tasks require similar knowledge or understanding ability . To give a broader view of our correlation results , we further cluster the downstream tasks by their correlation with each other using K - means . In this way , all the tasks are considered simultaneously , and the grouping is driven automatically by the empirical correlation results . If more than one metric are used in a downstream task , we cluster them independently . The clustering results are shown in Table 4 and a rearranged correlation map is shown in Figure 3 . The result shows that the clusters of the tasks align with our empirical knowledge . Cluster A includes tasks that require content information , while tasks in cluster B are more sensitive to speaker and prosodic features . Cluster C contains metrics MCD and ASV of VC , which are used to evaluate the signal quality and the rates of speaker transfer . It is worth noting that WER in VC belongs to cluster A , showing that it is more similar to content - related tasks . Furthermore , clusters D , E , and F each contain one of the metrics in SS and SE , aligning with our assumption that these tasks utilize different types of information compared to other tasks . With the analysis of the correlation between tasks , we empirically confirm the reliability of the results , and show that we increase the heterogeneity among speech tasks over SUPERB . We further discover shared properties between tasks with clustering , and the result is aligned with our common understanding of related tasks .", "entities": [[163, 166, "TaskName", "Automatic Speech Recognition"], [178, 179, "DatasetName", "SID"], [182, 184, "TaskName", "Speaker Verification"], [194, 196, "TaskName", "Intent Classification"], [207, 209, "TaskName", "Emotion Recognition"], [314, 315, "DatasetName", "SSL"], [344, 345, "DatasetName", "SSL"], [366, 367, "DatasetName", "SSL"], [574, 575, "DatasetName", "SID"]]}
{"text": "To study the impact of downstream model architecture and the data sizes used in SUPERB - SG we evaluate the robustness of SUPERB - SG with variations in downstream model as well as training data size , and show that our conclusions still hold true . We choose ST , OOD - ASR and SS as the downstream tasks for evaluation with an aim to cover semantic , content recognition , and generative task types . For the upstream models , FBANK , TERA , CPC , wav2vec 2.0 Base and HuBERT Base are used to cover different SSL algorithms .", "entities": [[98, 99, "DatasetName", "SSL"]]}
{"text": "To study the effect of data size , we create 3 pseudo datasets per task by sub - sampling 10 % , 5 % and 1 % from the original training set while fixing the validation and test sets . The statistics of the datasets are shown in Table 7 , and the results are in Table 8 . The ranking of the upstream models remains almost the same for 10 % of training data . When that is further reduced to 5 % , there is a change in ranking in SS due to a performance drop in Modified CPC . Excluding Modified CPC , the ranking is still fixed showing that the relative performance of the upstream models is agnostic to data size . Furthermore , when using only 1 % of training data , most of the SSL models fail on the 3 downstream tasks . This phenomenon is caused by insufficient task - specific knowledge due to limited training data size . Although SSL models learn highlevel representations from the unlabeled speech signal , acquisition of task - specific knowledge such as translingual ability in ST , text - level token mapping in OOD - ASR , and mask prediction in SS , requires non - trivial supervision . We We have open - sourced all the codes 1 and released a challenge 3 to encourage further research of SSL in speech . We welcome the community to participate and advance the research frontier together .", "entities": [[140, 141, "DatasetName", "SSL"], [167, 168, "DatasetName", "SSL"], [233, 234, "DatasetName", "SSL"]]}
{"text": "Here , we provide complete results of OOD - ASR tasks , as shown in Tables 9 , 10 , 11 . All upstream models used in this paper are trained with English speech data , but we are also interested in multilingual pre - trained models in OOD - ASR . Therefore , we evaluate the wav2vec 2.0 XLSR model on the OOD - ASR tasks , as shown in the last row of Table 9 . XLSR has identical architecture as wav2vec 2.0 Large , but is trained with 56k hours of speech including 53 different languages . The pre - training data of XLSR cover our cross - lingual tasks ' training data . As expected , using multilingual data improves OOD - ASR tasks and achieves the best performance among all upstream models .", "entities": [[59, 60, "MethodName", "XLSR"], [78, 79, "MethodName", "XLSR"], [106, 107, "MethodName", "XLSR"]]}
{"text": "Building an end - to - end conversational agent for multi - domain task - oriented dialogues has been an open challenge for two main reasons . First , tracking dialogue states of multiple domains is non - trivial as the dialogue agent must obtain complete states from all relevant domains , some of which might have shared slots among domains as well as unique slots specifically for one domain only . Second , the dialogue agent must also process various types of information across domains , including dialogue context , dialogue states , and database , to generate natural responses to users . Unlike the existing approaches that are often designed to train each module separately , we propose \" UniConv \" - a novel unified neural architecture for end - to - end conversational systems in multi - domain task - oriented dialogues , which is designed to jointly train ( i ) a Bi - level State Tracker which tracks dialogue states by learning signals at both slot and domain level independently , and ( ii ) a Joint Dialogue Act and Response Generator which incorporates information from various input components and models dialogue acts and target responses simultaneously . We conduct comprehensive experiments in dialogue state tracking , contextto - text , and end - to - end settings on the Multi - WOZ2.1 benchmark , achieving superior performance over competitive baselines .", "entities": [[8, 9, "DatasetName", "agent"], [42, 43, "DatasetName", "agent"], [76, 77, "DatasetName", "agent"], [208, 211, "TaskName", "dialogue state tracking"]]}
{"text": "A conventional approach to task - oriented dialogues is to solve four distinct tasks : ( 1 ) natural language understanding ( NLU ) which parses user utterance into a semantic frame , ( 2 ) dialogue state tracking ( DST ) which updates the slots and values from semantic frames to the latest values for knowledge base retrieval , ( 3 ) dialogue policy which determines an appropriate dialogue act for the next system response , and ( 4 ) response generation which generates a natural language sequence conditioned on the dialogue act . This traditional pipeline modular framework has achieved remarkable successes in task - oriented dialogues ( Wen et al , 2017 ; Liu and Lane , 2017 ; Williams et al , 2017 ; Zhao et al , 2017 ) . However , such kind of dialogue system is not fully optimized as the modules are loosely integrated and often not trained jointly in an end - to - end manner , and thus may suffer from increasing error propagation between the modules as the complexity of the dialogues evolves . A typical case of a complex dialogue setting is when the dialogue extends over multiple domains . A dialogue state in a multi - domain dialogue should include slots of all applicable domains up to the current turn ( See Table 1 ) . Each domain can have shared slots that are common among domains or unique slots that are not shared with any . Directly applying single - domain DST to multi - domain dialogues is not straightforward because the dialogue states extend to multiple domains . A possible approach is to process a dialogue of N D domains multiple times , each time obtaining a dialogue state of one domain . However , this approach does not allow learning co - reference in dialogues in which users can switch from one domain to another . As the number of dialogue domains increases , traditional pipeline approaches propagate errors from dialogue states to dialogue policy and subsequently , to natural language generator . Recent efforts Madotto et al , 2018 ; Wu et al , 2019b ) address this problem with an integrated sequence - to - sequence structure . These approaches often consider knowledge bases as memory tuples rather than relational entity tables . While achieving impressive performance , these approaches are not scalable to large - scale knowledgebases , e.g. thousands of entities , as the memory cost to query entity attributes increases substantially . Another limitation of these approaches is the absence of dialogue act modelling . Dialogue act Human : could you make a suggestion ? one in the centre ? Dialogue agent : fitzbillies restaurant is an expensive british restaurant in the centre . can i book that for you ? Dialogue state : { restaurant : { pricerange : expensive , area : centre } } Dialogue acts : [ inform - restaurant , request - booking ] ... ... Human : also , i need the number for kings hedges learner pool . Dialogue agent : the phone number for the pool is 01234567 , is there something else i can help you ? Dialogue state : { restaurant : { pricerange : expensive , area : centre , name = fizbillies restaurant , request= [ address ] } , attraction : { name : kings hedges learner pool , request= [ phone ] } } Dialogue acts : [ inform - phone ] Table 1 : Example of a multi - domain dialogue with two domains : restaurant and attraction . is particularly important in task - oriented dialogues as it determines the general decision towards task completion before a dialogue agent can materialize it into natural language response ( See Table 1 ) . To tackle the challenges in multi - domain taskoriented dialogues while reducing error propagation among dialogue system modules and keeping the models scalable , we propose UniConv , a unified neural network architecture for end - to - end dialogue systems . UniConv consists of a Bi - level State Tracking ( BDST ) module which embeds natural language understanding as it can directly parse dialogue context into a structured dialogue state rather than relying on the semantic frame output from an NLU module in each dialogue turn . BDST implicitly models and integrates slot representations from dialogue contextual cues to directly generate slot values in each turn and thus , remove the need for explicit slot tagging features from an NLU . This approach is more practical than the traditional pipeline models as we do not need slot tagging annotation . Furthermore , BDST tracks dialogue states in dialogue context in both slot and domain levels . The output representations from two levels are combined in a late fusion approach to learn multi - domain dialogue states . Our dialogue state tracker disentangles slot and domain representation learning while enabling deep learning of shared representations of slots common among domains . UniConv integrates BDST with a Joint Dialogue Act and Response Generator ( DARG ) that simultaneously models dialogue acts and generates system responses by learning a latent variable representing dialogue acts and semantically conditioning output response tokens on this latent variable . The multitask setting of DARG allows our models to model dialogue acts and utilize the distributed representations of dialogue acts , rather than hard discrete output values from a dialogue policy module , on output response tokens . Our response generator incorporates information from dialogue input components and intermediate representations progressively over multiple attention steps . The output representations are refined after each step to obtain high - resolution signals needed to generate appropriate dialogue acts and responses . We combine both BDST and DARG for end - to - end neural dialogue systems , from input dialogues to output system responses . We evaluate our models on the large - scale Mul - tiWOZ benchmark , and compare with the existing methods in DST , context - to - text generation , and end - to - end settings . The promising performance in all tasks validates the efficacy of our method .", "entities": [[18, 21, "TaskName", "natural language understanding"], [36, 39, "TaskName", "dialogue state tracking"], [81, 83, "TaskName", "response generation"], [452, 453, "DatasetName", "agent"], [517, 518, "DatasetName", "agent"], [625, 626, "DatasetName", "agent"], [696, 699, "TaskName", "natural language understanding"], [826, 828, "TaskName", "representation learning"], [1013, 1015, "TaskName", "text generation"]]}
{"text": "Dialogue State Tracking . Traditionally , DST models are designed to track states of singledomain dialogues such as WOZ ( Wen et al , 2017 ) and DSTC2 ( Henderson et al , 2014a ) benchmarks . There have been recent efforts that aim to tackle multi - domain DST such as ( Ramadan et al , 2018 ; Lee et al , 2019 ; Wu et al , 2019a ; . These models can be categorized into two main categories : Fixed vocabulary models ( Zhong et al , 2018 ; Ramadan et al , 2018 ; Lee et al , 2019 ) , which assume known slot ontology with a fixed candidate set for each slot . On the other hand , open - vocabulary models ( Lei et al , 2018 ; Wu et al , 2019a ; Le et al , 2020 ) derive the candidate set based on the source sequence i.e. dialogue history , itself . Our approach is more related to the open - vocabulary approach as we aim to generate unique dialogue states depending on the input dialogue . Different from previous Context - to - Text Generation . This task was traditionally solved by two separate dialogue modules : Dialogue Policy ( Peng et al , 2017 ( Peng et al , , 2018 and NLG ( Wen et al , 2016 ; Su et al , 2018 ) . Recent work attempts to combine these two modules to directly generate system responses with or without modeling dialogue acts . Zhao et al ( 2019 ) models action space of dialogue agent as latent variables . predicts dialogue acts using a hierarchical graph structure with each path representing a unique act . Pei et al ( 2019 ) ; Peng et al ( 2019 ) use multiple dialogue agents , each trained for a specific dialogue domain , and combine them through a common dialogue agent . Mehri et al ( 2019 ) models dialogue policy and NLG separately and fuses feature representations at different levels to generate responses . Our models simultaneously learn dialogue acts as a latent variable while allowing semantic conditioning on distributed representations of dialogue acts rather than hard discrete features . End - to - End Dialogue Systems . In this task , conventional approaches combine Natural Language Understanding ( NLU ) , DST , Dialogue Policy , and NLG , into a pipeline architecture ( Wen et al , 2017 ; Bordes et al , 2016 ; Liu and Lane , 2017 ; Liu and Perez , 2017 ; Williams et al , 2017 ; Zhao et al , 2017 ; Jhunjhunwala et al , 2020 ) . Another framework does not explicitly modularize these components but incorporate them through a sequence - to - sequence framework Lei et al , 2018 ; Yavuz et al , 2019 ) and a memory - based entity dataset of triplets Madotto et al , 2018 ; Gangi Reddy et al , 2019 ; Wu et al , 2019b ) . These approaches bypass dialogue state and/or act modeling and aim to generate output responses directly . They achieve impressive success in generating dialogue responses in open - domain dialogues with unstructured knowledge bases . However , in a task - oriented setting with an entity dataset , they might suffer from an explosion of memory size when the number of entities from multiple dialogue domains increases . Our work is more related to the traditional pipeline strategy but we integrate our dialogue models by unifying two major components rather than using the traditional four - module architecture , to alleviate error propagation from upstream to downstream components . Different from prior work such as ( Shu et al , 2019 ) , our model facilitates multi - domain state tracking and allows learning dialogue acts during response generation .", "entities": [[0, 3, "TaskName", "Dialogue State Tracking"], [109, 110, "MethodName", "ontology"], [194, 196, "TaskName", "Text Generation"], [270, 271, "DatasetName", "agent"], [324, 325, "DatasetName", "agent"], [390, 393, "TaskName", "Natural Language Understanding"], [649, 651, "TaskName", "response generation"]]}
{"text": "Slot - level DST . We adopt the Transformer attention ( Vaswani et al , 2017 ) , which consists of a dot - product attention with skip connection , to integrate dialogue contextual information into each slot representation . We denote Att ( Z 1 , Z 2 ) as the attention operation from Z 2 on Z 1 . We first enable models to process all slot representations together rather than separately as in previous DST models ( Ramadan et al , 2018 ; Wu et al , 2019a ) . This strategy allows our models to explicitly learn dependencies between all pairs of slots . Many pairs of slots could exhibit correlation such as time - wise relation ( \" departure_time \" and \" arrival_time \" ) . We obtain Z dst SS = Att ( Z S , Z S ) R S \u00d7d . We incorporate the dialogue information by learning dependencies between each slot representation and each token in the dialogue history . Previous approaches such as ( Budzianowski and Vuli\u0107 , 2019 ) consider all dialogue history as a single sequence but we separate them into two inputs because the information in X utt is usually more important to generate responses while X ctx includes more background information . We then obtain Z dst S , ctx = Att ( Z ctx , Z dst SS ) R S \u00d7d and Z dst S , utt = Att ( Z utt , Z dst S , ctx ) R S \u00d7d . Following ( Lei et al , 2018 ) , we incorporate dialogue state of the previous turn B t\u22121 which is a more compact representation of dialogue context . Hence , we can replace the full dialogue context to only R t\u22121 as the remaining part is represented in B t\u22121 . This approach avoids taking in all dialogue history and is scalable as the conversation grows longer . We add the attention layer to obtain Z dst S , st = Att ( Z prev st , Z dst S , ctx ) R S \u00d7d ( See Figure 1 ) . We further improve the feature representations by repeating the attention sequence over N dst S times . We denote the final output Z dst S . Domain - level DST . We adopt a similar architecture to learn domain - level representations . The representations learned in this module exhibit global information while slot - level representations contain local dependencies to decode multi - domain dialogue states . First , we enable the domain - level DST to capture dependencies between all pairs of domains . For example , some domains such as \" taxi \" are typically paired with other domains such as \" attraction \" , but usually not with the \" train \" domain . We then obtain Z dst DD = Att ( Z D , Z D ) R D \u00d7d . We then allow models to capture dependencies between each domain representation and each token in dialogue context and current user utterance . By segregating dialogue context and current utterance , our models can potentially detect changes of dialogue domains from past turns to the current turn . Especially in multi - domain dialogues , users can switch from one domain to another and the next system response should address the latest domain . We then obtain Z dst D , ctx = Att ( Z ctx , Z dst DD ) R D \u00d7d and Z dst D , utt = Att ( Z utt , Z dst D , ctx ) R D \u00d7d sequentially . Similar to the slot - level module , we refine feature representations over N dst D times and denote the final output as Z dst D . Domain - Slot DST . We combined domain and slot representations by expanding the tensors to identical dimensions i.e. D \u00d7 S \u00d7 d. We then apply Hadamard product , resulting in domain - slot joint features Z dst DS R D \u00d7 S \u00d7d . We then apply a self - attention layer to allow learning of dependencies between joint domain - slot features : Z dst = Att ( Z dst DS , Z dst DS ) R D \u00d7 S \u00d7d . In this attention , we mask the intermediate representations in positions of invalid domain - slot pairs . Compared to previous work such as ( Wu et al , 2019a ) , we adopt a late fusion method whereby domain and slot representations are integrated in deeper layers .", "entities": [[8, 9, "MethodName", "Transformer"], [22, 26, "MethodName", "dot - product attention"]]}
{"text": "We evaluate our models with the multi - domain dialogue corpus MultiWOZ 2.0 and 2.1 ( Eric et al , 2019 ) ( The latter includes corrected state labels for the DST task ) . From the dialogue state annotation of the training data , we identified all possible domains and slots . We identified D = 7 domains and S = 30 slots , including 19 inform slots and 11 request slots . We also identified A = 32 acts . The corpus includes 8 , 438 dialogues in the training set and 1 , 000 in each validation and test set . We present a summary of the dataset in Table 2 . For additional information of data pre - processing procedures , domains , slots , and entity DBs , please refer to Appendix A.", "entities": [[11, 13, "DatasetName", "MultiWOZ 2.0"]]}
{"text": "We select d = 256 , h att = 8 , N dst S = N dst D = N gen = 3 . We employed dropout ( Srivastava et al , 2014 ) of 0.3 and label smoothing ( Szegedy et al , 2016 ) on target system responses during training .", "entities": [[37, 39, "MethodName", "label smoothing"]]}
{"text": "We A Data Pre - processing First , we delexicalize each target system response sequence by replacing the matched entity attribute that appears in the sequence to the canonical tag domain_slot . For example , the original target response ' the train i d is tr8259 departing from cambridge ' is delexicalized into ' the train i d is train_id departing from train_departure ' . We use the provided entity databases ( DBs ) to match potential attributes in all target system responses . To construct dialogue history , we keep the original version of all text , including system responses of previous turns , rather than the delexicalized form . We split all sequences of dialogue history , user utterances of the current turn , dialogue states , and delexicalized target responses , into case - insensitive tokens . We share the embedding weights of all source sequences , including dialogue history , user utterance , and dialogue states , but use a separate embedding matrix to encode the target system responses . We summarize the number of dialogues in each domain in Table 2 . For each domain , a dialogue is selected as long as the whole dialogue ( i.e. singledomain dialogue ) or parts of the dialogue ( i.e. in multi - domain dialogue ) is involved with the domain . For each domain , we also build a set of possible inform and request slots using the dialogue state annotation in the training data . The details of slots and database in each domain can be seen in Table 9 . The DBs of 3 domains taxi , police , and hospital are not available as part of the benchmark . On average , each dialogue has 1.8 domains and extends over 13 turns .", "entities": [[283, 285, "DatasetName", "the benchmark"]]}
{"text": "We describe our baseline models in DST , contextto - text generation , and end - to - end dialogue tasks .", "entities": [[10, 12, "TaskName", "text generation"]]}
{"text": "FJST and HJST ( Eric et al , 2019 ) . These models adopt a fixed - vocabulary DST approach . Both models include encoder modules ( either bidirectional LSTM or hierarchical LSTM ) to encode the dialogue history . The models pass the context hidden states to separate linear transformation to obtain final vectors to predict individual slots separately . The output vector is used to measure a score of each candidate from a predefined candidate set . DST Reader . This model considers the DST task as a reading comprehension task and predicts each slot as a span over tokens within dialogue history . DST Reader utilizes attentionbased neural networks with additional modules to predict slot type and carryover probability . TSCP ( Lei et al , 2018 ) . The model adopts a sequence - to - sequence framework with a pointer network to generate dialogue states . The source sequence is a combination of the last user utterance , dialogue state of the previous turn , and user utterance . To compare with TSCP in a multi - domain task - oriented dialogue setting , we adapt the model to multi - domain dialogues by formulating the dialogue state of the previous turn similarly as our models . We reported the performance when the maximum length of the output dialogue state sequence L is set to 20 tokens ( original default parameter is 8 tokens but we expect longer dialogue state in MultiWOZ benchmark and selected 20 tokens ) . HyST . This model combines the advantage of fixed - vocabulary and openvocabulary approaches . The model uses an openvocabulary approach in which the set of candidates of each slot is constructed based on all word ngrams in the dialogue history . Both approaches are applied in all slots and depending on their performance in the validation set , the better approach is used to predict individual slots during test time . TRADE ( Wu et al , 2019a ) . The model adopts a sequence - to - sequence framework with a pointer network to generate individual slot token - by - token . The prediction is additionally supported by a slot gating component that decides whether the slot is \" none \" , \" dontcare \" , or \" generate \" . When the gate of a slot is predicted as \" generate \" , the model will generate value as a natural output sequence for that slot . NADST ( Le et al , 2020 ) . The model proposes a non - autoregressive approach for dialogue state tracking which enables learning dependencies between domain - level and slot - level representations as well as token - level representations of slot values . DSTQA ( Zhou and Small , 2019 ) . The model treats dialogue state tracking as a question answering problem in which state values can be predicted through lexical spans or unique generated values . It is enhanced with a knowledge graph where each node represent a slot and edges are based on overlaps of their value sets . SOM - DST ( Kim et al , 2020 ) . This is the current state - of - the - art model on the MultiWOZ2.1 dataset . The model exploits a selectively overwriting mechanism on a fixed - sized memory of dialogue states . i d , address , area , internet , parking , single , double , family , name , phone , postcode , pricerange ' , takesbookings , stars , type At each dialogue turn , the mechanism involve decision making on whether to update or carryover the state values from previous turns .", "entities": [[28, 30, "MethodName", "bidirectional LSTM"], [32, 33, "MethodName", "LSTM"], [90, 92, "TaskName", "reading comprehension"], [144, 146, "MethodName", "pointer network"], [246, 247, "DatasetName", "MultiWOZ"], [347, 349, "MethodName", "pointer network"], [433, 436, "TaskName", "dialogue state tracking"], [472, 475, "TaskName", "dialogue state tracking"], [477, 479, "TaskName", "question answering"], [519, 520, "MethodName", "SOM"], [603, 605, "TaskName", "decision making"]]}
{"text": "Figure 1 : Confounding bias from the data generation mechanism . u refers to the unobserved mechanism ( i.e. , plaintiffs sue when they have a high probability to be supported ) that causes the judgment in dataset D ( J ) to be imbalanced . D ( J ) I denotes that the imbalanced data D ( J ) has a causal effect on the representation of input I ( i.e. , plaintiff 's claim and fact description ) , and D ( J ) V denotes that D ( J ) has a causal effect on the representation of court 's view V . Such imbalance in D ( J ) leads to the confounding bias that the representations of I and V tend to be supportive , and blind the conventional training on P ( V | I ) . 2019 ) . Court 's view can be regarded as the interpretation for the sentence of a case . Being an important portion of verdict , court 's view is difficult to generate due to its logic reasoning in the content . Therefore court 's view generation is regarded as one of the most critical functions in a legal assistant system . Court 's view consists of two main parts , including the judgment and the rationales , where the judgment is a response to the plaintiff 's claims in civil cases or charges in criminal cases , and the rationales are summarized from the fact description to derive and explain the judgment . Recently , Ye et al ( 2018 ) investigated the problem of court 's view generation for the criminal cases , but it focused on the generation of rationales in the court 's view based on the given criminal charge and fact description of a case . Such an experimental scenario is not applicable to the practical situation since the rationales should be concluded before reaching the final judgment . Moreover , dif - Figure 2 : An example of plaintiff 's claim , fact description , and court 's view from a legal document in a civil case . The judgment is non - support since there exists a rejection on one of the plaintiff 's claims in the court 's view . ferent from the criminal cases , in civil cases , the judgment depends not only on the facts recognized but also on the claims that the plaintiff declared . In this paper , we focus on the problem of automatically generating the court 's view in civil cases by injecting the plaintiff 's claim and fact description , as shown in Fig . 2 . In such a context , the problem of the court 's view generation can be formulated as a text - to - text natural language generation ( NLG ) problem , where the input is the plaintiff 's claim and the fact description , and the output is the corresponding court 's view that contains the judgment and the rationales 1 . Although classical text generative models ( e.g. , sequence - tosequence model Sutskever et al , 2014 , attentionbased model , andpointer - generator networks See et al , 2017 ) have been applied to many text generation tasks , yet , in the task of the court 's view generation , such techniques can not be simply applied for the following reasons : ( 1 ) There exists \" no claim , no trial \" principle in civil legal systems : The judgment in the real court 's view is the response to the claims declared by the plaintiff , where its rationales summarize the corresponding facts . In other words , there exists a correspondence relationship between the input ( claims and facts ) and the generated text ( court 's view ) . For example , the plaintiff 's claims shown in Fig . 2 mentioned the principal and the interest , respectively . Hence , the court 's view of this case would and might only focus on the facts about the principal and the interest . ( 2 ) The imbalance of judgment in civil cases : The distribution of judgment results of civil cases is very imbalanced . For example , over 76 % of cases were supported in private lending , which is the most frequent category in civil cases . Such an imbalance of judgment would blind the training of the model by focusing on the supported cases while ignoring the non - supported cases , leading to incorrect judgment generation of court 's view . From the perspective of causality ( Pearl , 2009 ; Kuang et al , 2020 ) , the imbalance of judgment reveals the confounding bias induced by the data generation mechanism that plaintiffs sue when they have a high probability to be supported . Such imbalanced data would cause the learned representation of both inputs ( claims and recognized facts ) and output ( court 's view ) to be supported , leading to confounding bias between inputs and output , and blinding the training process of conventional NLG models as we demonstrated in Fig . 1 . To address these challenges , we propose an Attentional and Counterfactual based Natural Language Generation ( AC - NLG ) method by jointly optimizing a claim - aware encoder , a pair of counterfactual decoders to generate judgmentdiscriminative court 's views ( both supportive and non - supportive views ) and a synergistic judgment predictive model . Specifically , the claim - aware encoder is designed to represent the fact description which emphasizes on the declared claims . The counterfactual decoder is inspired by the backdoor adjustment in causal inference ( Pearl et al , 2016 ; Kuang et al , 2020 ) to address the confounding bias and the imbalance problem in judgment . To determine the judgment result of each case , a judgment predictive model is jointly learned with the two decoders and decides which output to be selected as the final generated court 's view . We validate the effectiveness of our AC - NLG method with extensive experiments on real legal documents . Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics . Since legal AI is a sensitive field , we make ethical discussion in the penultimate section ( Sec . 6 ) . The main contributions of this paper can be sum - marized as follows : We investigate the problem of de - biased court 's view generation in civil cases from a causal perspective , considering the issue of confounding bias from judgment imbalance . We propose a novel AC - NLG model to jointly optimize a claim - aware encoder and a pair of counterfactual decoders for generating a judgment - discriminative court 's view by incorporating with a judgment predictive model . We construct a dataset based on raw civil legal documents , where each case is objectively split into three parts : plaintiff 's claim , fact description , and court 's view with human annotation on the judgment . To motivate other scholars to investigate this novel but important problem , we make the experiment dataset publicly available 2 . We validate the superior performance of the proposed method with extensive experiments . Our method can be applied to other natural language generation tasks with confounding bias or data imbalance . 2 Related Work", "entities": [[544, 546, "TaskName", "text generation"], [958, 960, "MethodName", "causal inference"]]}
{"text": "In recent years , many researchers from both law and computer science fields have been exploring the potential and methods to perform judicial decisions and auxiliary tasks , aiming at helping lawyers and lower court judges . In recent work , judicial intelligence is also applied to various tasks of natural language processing . Since most of the legal documents appear in textual form , many NLP technologies have been brought into the legal field to improve the efficiency of legal work . Charge prediction is a common task of judgment prediction , considered as a text classification problem ( Lin et al , 2012 ; Zhong et al , 2018 ; Hu et al , 2018 ; Jiang et al , 2018 ; Chalkidis et al , 2019 ) . Besides , there are also works on legal questions classification ( Xiao et al , 2017 ) , law articles recommendation ( Chen et al , 2019 ) , controversy focus mining ( Duan et al , 2019 ) and relevant case retrieval ( Chen et al , 2013 ) . Ye et al ( 2018 ) explored the court 's view generation in criminal cases , where the input is only fact description , and the court 's view generation is conditioned on the known judgment results , which is not applicable in real cases . 2 https://github.com/wuyiquan/AC - NLG", "entities": [[96, 98, "TaskName", "text classification"]]}
{"text": "Our task aims at generating the court 's view based on the plaintiff 's claim and the fact description , which can be regarded as a NLG task . NLG has been widely studied and applied to many tasks , such as machine translation ( Wu et al , 2016 ) , question answering ( McCann et al , 2018 ; Bagchi and Wynter , 2013 ) and text summarization ( Rush et al , 2015 ) . The recent success of sequence - to - sequence models ( Sutskever et al , 2014 ) , in which recurrent neural networks ( RNNs ) reading and generating text simultaneously , has made the generation task feasible . Bahdanau et al ( 2014 )", "entities": [[42, 44, "TaskName", "machine translation"], [52, 54, "TaskName", "question answering"], [68, 70, "TaskName", "text summarization"]]}
{"text": "Causal Inference ( Pearl , 2009 ; Kuang et al , 2020 ) is a powerful statistical modeling tool for explanatory analysis by removing confounding bias in data . That bias might bring a spurious correlation or confounding effect among variables . Recently , many methods have been proposed to remove confounding bias in the literature of causal inference , including do - operation based on structure causal model ( Pearl , 2009 ) and counterfactual outcome prediction based on potential outcome framework ( Imbens and Rubin , 2015 ) . With dooperation , the backdoor adjustment ( Pearl et al , 2016 ) have been proposed for data de - bias . In this paper , we sketch the causal structure model of our problem , as shown in Fig . 1 , and adopt backdoor for confounding bias reduction .", "entities": [[0, 2, "MethodName", "Causal Inference"], [57, 59, "MethodName", "causal inference"]]}
{"text": "In this work , we focus on the problem of the court 's view generation in civil cases , where the input is the plaintiff 's claim and the fact description , and the output is the corresponding court 's view . We formulate our problem with the definition of the plaintiff 's claim , the fact description , and the court 's view , as shown in Fig . 2 . Plaintiff 's claim ( C ) is a descriptive sentence that depicts the claims from the plaintiff . In a civil case , it often appears multiple claims from the plaintiff . For example , the plaintiff 's claim demonstrated in Fig . 2 contains the principal claim and the interest claim . Here , we denote the plaintiff 's claim in a case as a sentence form c = { w c t } m t=1 , where w c t represents one word , and m is the number of words in plaintiff 's claim . Fact description ( F ) is also a descriptive sentence , which describes the identified facts ( relevant events that have happened ) in a case , as Fig . 2 shows . Here , we denote the fact description in a case as f = { w f t } n t=1 , where n is the length . Court 's view ( V ) contains two main components , judgment and rationales , where the judgment is to respond the plaintiff 's claims , and the rationales are the claim - related summarization on the fact description to determine and interpret the judgment . Here , we denote the court 's view as v = { w v t } l t=1 , where l is the length . Moreover , we use a variable j to denote the judgment in the court 's view . For simplicity , we set j = 1 to denote supported judgment ( all the claims are judged to be accepted ) , and j = 0 to denote non - supported judgment . Then , the problem of court 's view generation can be denoted as follow : Problem 1 ( Court 's View Generation ) . Given the plaintiff 's claim c = { w c t } m t=1 and the fact de - scription f = { w f t } n t=1 , our task is to generate the court 's view v = { w v t } l t=1 .", "entities": [[264, 265, "TaskName", "summarization"], [344, 345, "DatasetName", "0"]]}
{"text": "As shown in Fig . 1 , the confounding bias from the data generation mechanism would blind the conventional training on P ( V | I ) , and current sequenceto - sequence models struggle to solve this problem . Here , we see through why these models fail mathematically . For a certain case , given the input I = ( c , f ) , using Bayes rule , we would train the model to generate the court 's view V as follow : P ( V | I ) = j P ( V | I , j ) P ( j | I ) ( 1 ) If the supported cases dominate our training data , e.g. , P ( j = 1 | I ) \u2248 1 . Thus , P ( V | I ) degrades to P ( V | I , j = 1 ) , which would ignore the representation of non - supported cases , leading to the learned representations of inputs I and output V tend to be supported . Thus , the model tends to build a strong connection between inputs and the supported court 's view , even for the cases that are non - supported . In this way , the representation of input I is contaminated by the confounding bias from I D ( J ) V . Backdoor adjustment is a main de - confounding technique in causal inference ( Pearl et al , 2016 ; Pearl , 2009 ) . De - confounding seeks the exact causal effect of one variable on another , which appeals for our court 's view generation task since the court 's view should be faithful only to the content of the plaintiff 's claims and fact descriptions . The backdoor adjustment makes a do - operation on I , which promotes the posterior probability from passive observation to active intervention . The backdoor adjustment addresses the confounding bias by computing the interventional posterior P ( V | do ( I ) ) and controlling the confounder as : P ( V | do ( I ) ) = j P ( V | I , j ) P ( j ) ( 2 ) In our problem , the variable j is a binary variable ( support or non - support ) , hence , P ( V | do ( I ) ) = P ( V | I , j = 0 ) P ( j = 0 ) + P ( V | I , j = 1 ) P ( j = 1 ) The main difference between traditional posterior in Eq . 1 and interventional posterior in Eq . 2 is that P ( j | I ) is changed to P ( j ) . Since the backdooor adjustment help to cut the dependence between D ( J ) and I , we can eliminate the confounding bias from data generation mechanism and learn a interventional model for de - biased court 's view generation .", "entities": [[243, 245, "MethodName", "causal inference"], [416, 417, "DatasetName", "0"], [422, 423, "DatasetName", "0"]]}
{"text": "We implement the following baselines for comparison : S2S Sequence - to - sequence model ( Sutskever et al , 2014 ) is a classic model for NLG task . We concatenate the plaintiff claims and facts descriptions as input . PGN Pointer Generator Networks ( See et al , 2017 ) utilizes a pointer network to solve the outof - vocabulary ( OOV ) problem , which is essential for the court 's view generation since many nouns occur there . Oversampling is a common method to alleviate data imbalance . We oversample the non - supported cases so that the ratio between supported cases and non - supported cases become 1 : 1 . S2SwS Apply oversampling to S2S. PGNwS Apply oversampling to PGN . AC - NLGwS Apply oversampling to AC - NLG . We do ablation experiments as follows : AC - NLGw / oD We remove the decoder and train the remaining model ( encoder and predictor ) as a classification task for judgment prediction . AC - NLGw / oBA We remove the backdoor adjustment by replacing the pair of counterfactual decoders and predictor with a single decoder , but keep the claim - aware attention mechanism . AC - NLGw / oCA We remove the claim - aware attention , and concatenate the claims and the facts instead .", "entities": [[54, 56, "MethodName", "pointer network"]]}
{"text": "We use Gensim ( \u0158eh\u016f\u0159ek and Sojka , 2010 ) with a large - scale generic corpus to train a language model as the pre - trained model , then use it to initialize the word embeddings , which is in the dimension of 300 . 8", "entities": [[35, 37, "TaskName", "word embeddings"]]}
{"text": "Alexander M Rush , Sumit Chopra , and Jason Weston . 2015 . A neural attention model for abstractive sentence summarization . arXiv preprint arXiv:1509.00685 . Abigail See , Peter J Liu , and Christopher D Manning . 2017 . Get to the point : Summarization with pointer - generator networks . arXiv preprint arXiv:1704.04368 .", "entities": [[19, 21, "TaskName", "sentence summarization"], [22, 23, "DatasetName", "arXiv"], [45, 46, "TaskName", "Summarization"], [52, 53, "DatasetName", "arXiv"]]}
{"text": "The court concluded that : The loan relationship between the plaintiff A and the defendant B is legal and effective . After the defendant borrowed money , he should take the responsibility to return the loan and pay legal interest . If the borrower and the lender have not agreed on the interest on the loan , it shall be deemed as non - payment of interest Acceptance . However , the plaintiff has the right to claim the interest to be calculated from the benchmark interest rate of the same grade of loans issued by the People 's Bank of China at the same period when the loan occurred since the date of the prosecution Rejection .", "entities": [[84, 86, "DatasetName", "the benchmark"]]}
{"text": "Opinion target extraction and opinion term extraction are two fundamental tasks in Aspect Based Sentiment Analysis ( ABSA ) . Many recent works on ABSA focus on Targetoriented Opinion Words ( or Terms ) Extraction ( TOWE ) , which aims at extracting the corresponding opinion words for a given opinion target . TOWE can be further applied to Aspect - Opinion Pair Extraction ( AOPE ) which aims at extracting aspects ( i.e. , opinion targets ) and opinion terms in pairs . In this paper , we propose Target - Specified sequence labeling with Multi - head Self - Attention ( TSMSA ) for TOWE , in which any pre - trained language model with multi - head self - attention can be integrated conveniently . As a case study , we also develop a Multi - Task structure named MT - TSMSA for AOPE by combining our TSMSA with an aspect and opinion term extraction module . Experimental results indicate that TSMSA outperforms the benchmark methods on TOWE significantly ; meanwhile , the performance of MT - TSMSA is similar or even better than state - of - the - art AOPE baseline models .", "entities": [[5, 7, "TaskName", "term extraction"], [14, 16, "TaskName", "Sentiment Analysis"], [156, 158, "TaskName", "term extraction"], [166, 168, "DatasetName", "the benchmark"]]}
{"text": "Aspect - Based Sentiment Analysis ( ABSA ) ( Pontiki et al , 2014 ) has attracted much attention of researchers in recent years . In ABSA , aspect ( or called opinion target ) extraction and opinion term extraction are two fundamental tasks . Aspect is the word or phrase in the reviews referring to the object towards which users show attitudes , while opinion terms are those words or phrases representing users ' attitudes ( Wu et al , 2020 ) . For example , in the sentence \" The dim sum is delicious . \" , the phrase \" dim sum \" is an aspect and the word \" delicious \" is an opinion term . See the upper part of Table 1 for more examples . Plenty of works based on neural networks have been done in both aspect * The corresponding author .", "entities": [[0, 5, "TaskName", "Aspect - Based Sentiment Analysis"], [38, 40, "TaskName", "term extraction"]]}
{"text": "\" Soooo great ! The food is delicious and inexpensive , and the environment is in a nice . The only problem is that the soup and dessert are ordinary . \" Aspect - Opinion Pairs : food : [ delicious , inexpensive ] ( one - to - many ) environment : [ nice ] ( one - to - one ) soup , dessert : [ ordinary ] ( many - to - one ) Table 1 : The upper part is a restaurant review and the lower part shows the corresponding aspect - opinion pairs . Extracted aspects and opinion terms are marked in red and blue , respectively . and opinion term extraction ( Liu et al , 2015 ; Poria et al , 2016 ; Xu et al , 2018 ) ; moreover , some studies combine these two tasks into a multi - task structure to extract aspects and opinion terms simultaneously ( Wang et al , 2016 ( Wang et al , , 2017Li and Lam , 2017 ; Dai and Song , 2019 ) . However , one critical deficiency in the researches mentioned above is that they ignore the relation of aspects and opinion terms , which leads to the birth of Target - oriented Opinion Words ( or Terms ) Extraction ( TOWE ) ( Fan et al , 2019 ) for extracting the corresponding opinion terms of a given opinion target . Subsequently , Aspect - Opinion Pair Extraction ( AOPE ) ( Chen et al , 2020 ) and Pair - wise Aspect and Opinion Terms Extraction ( PAOTE ) have emerged , which both aim at extracting aspects and opinion terms in pairs . AOPE and PAOTE are exactly the same task , only named differently . In the following , we use AOPE to denote this task for simplicity . It can be considered that AOPE contains aspect and opinion word extraction and TOWE . Since aspect extraction has been fully studied and satisfactory results have been obtained , TOWE , which aims at mining the relation between aspects and opinion terms , is the key to the AOPE task . As shown in the lower part of Table 1 , the relational structure of the aspect - opinion pairs within a sentence can be complicated , including one - to - one , one - to - many , and many - to - one . The challenge of TOWE is the learning of representations of the given opinion target accurately and a few works focus on this task . For instance , Fan et al ( 2019 ) propose an Inward - Outward LSTM to pass target information to the left context and the right context of the target respectively , and then they combine the left , right , and global context to encode the sentence . Recently , SDRN ( Chen et al , 2020 ) and SpanMlt both adopt a pre - trained language model to learn contextual representations for AOPE . In SDRN , a double - channel recurrent network and a synchronization unit are applied to extract aspects , opinion terms and their relevancy . In SpanMlt , the terms are extracted under annotated span boundaries with contextual representations , and then the relations between every two span combinations are identified . However , apart from hyper - parameters in the pre - trained language model , these two methods introduce many other hyper - parameters ( e.g. , the hidden size , thresholds and recurrent steps in SDRN , and the span length , top k spans and the balanced factor of different tasks in SpanMlt ) . Some of these hyper - parameters have a significant impact on the model performance . Motivated by the previous work and to address the challenges mentioned above , we propose a Target - Specified sequence labeling method based on Multi - head Self - Attention ( Vaswani et al , 2017 ) ( TSMSA ) . The sentence is first processed in the format \" [ SEP ] Aspect [ SEP ] \" ( e.g. , \" The [ SEP ] food [ SEP ] is delicious . \" ) , which is inspired by Soares et al ( 2019 ) who utilized a special symbol \" [ SEP ] \" to label all entities and output their corresponding representations . Then we develop a sequence labeling model based on multi - head self - attention to identify the corresponding opinion terms . By using the special symbol and self - attention mechanism , TSMSA is capable of capturing the information of the specific aspect . To improve the performance of our model , we apply pre - trained language models like BERT ( Devlin et al , 2019 ) which contain a multi - head self - attention module as the encoder . As a case study , we integrate aspect and opinion term extraction , and TOWE into a Multi - Task architecture named MT - TSMSA to validate the effectiveness of our method on the AOPE task . In addition , apart from hyper - parameters in the pre - trained language model , we only need to adjust the balanced factor of different tasks in MT - TSMSA . In summary , our main contributions are as follows : We propose a target - specified sequence labeling method with multi - head self - attention mechanism to perform TOWE , which generates target - specific context representations for different targets in the same review with the special symbol and multi - head self - attention . Pre - trained language models can be conveniently applied to improve the performance . For our TSMSA and MT - TSMSA , only a small amount of hyper - parameters need to be adjusted when using pre - trained language models . Compared to the existing models for TOWE and AOPE , we alleviate the tradeoff issue between a model 's complexity and performance . Extensive experiments validate that our TSMSA can achieve the best performance on TOWE , and MT - TSMSA performs quite competitive on AOPE . The rest of this paper is organized as follows . Section 2 introduces the existing studies on TOWE and AOPE , respectively . Section 3 details the proposed TSMSA and MT - TSMSA . Section 4 presents our experimental results and discussions . Finally , we draw conclusions in Section 5 .", "entities": [[115, 117, "TaskName", "term extraction"], [330, 332, "TaskName", "aspect extraction"], [449, 450, "MethodName", "LSTM"], [802, 803, "MethodName", "BERT"], [834, 836, "TaskName", "term extraction"]]}
{"text": "Plenty of works have been carried out for aspect extraction and opinion term extraction . Early researches can be divided into unsupervised / semisupervised methods ( Hu and Liu , 2004 ; Zhuang et al , 2006 ; Qiu et al , 2011 ) and supervised methods ( Jakob and Gurevych , 2010 ; Shu et al , 2017 ) . With the development of neural networks , deep learning methods ( Liu et al , 2015 ; Yin et al , 2016 ; Poria et al , 2016 ; Xu et al , 2018 ) have made impressive progress in recent years . Several works integrate aspect extraction and opinion term extraction into a co - extraction process . Qiu et al ( 2011 ) expand the list of aspects and opinion terms in a bootstrapping method by double propagation . Some other works adopt the co - extraction structure in neural networks with multi - task learning ( Wang et al , 2016 ( Wang et al , , 2017Li and Lam , 2017 ) . However , the above methods ignore the relation between aspects and opinion terms and only a few works focus on this field . Rule - based methods ( Hu and Liu , 2004 ; Zhuang et al , 2006 ) are proposed to select corresponding opinion terms with distance rule and syntactic rule templates based on dependency parsing trees . However , the performance of these methods heavily relies on expert knowledge and these rules usually cover only a small amount of cases . Fan et al ( 2019 ) carry out TOWE by extracting the corresponding opinion terms for a given aspect , and then utilize Inward - Outward LSTM to generate implicit representations of aspects . Nevertheless , this approach is not capable of applying powerful pre - trained language models like BERT as the encoder to perform better . Our model aims to extract corresponding opinion terms of the given aspect with explicit representations , in addition to boost performance by employing BERT as the encoder .", "entities": [[8, 10, "TaskName", "aspect extraction"], [12, 14, "TaskName", "term extraction"], [107, 109, "TaskName", "aspect extraction"], [111, 113, "TaskName", "term extraction"], [155, 159, "TaskName", "multi - task learning"], [234, 236, "TaskName", "dependency parsing"], [288, 289, "MethodName", "LSTM"], [312, 313, "MethodName", "BERT"], [343, 344, "MethodName", "BERT"]]}
{"text": "Aspect - Opinion Pair Extraction ( AOPE ) ( Chen et al , 2020 ) and Pair - wise Aspect and Opinion Terms Extraction ( PAOTE ) ) both aim at extracting aspects and opinion terms in pairs . AOPE and PAOTE are essentially the same task with different names , and they can be split into aspect extraction and TOWE . Chen et al ( 2020 ) propose a Synchronous Double - channel Recurrent Network ( SDRN ) which consists of an opinion entity extraction unit , a relation detection unit , and a synchronization unit for pair extraction . develop a span - based multi - task learning framework ( SpanMlt ) where the terms are extracted under annotated span boundaries , so as to identify the relations between every two span combinations . However , SDRN contains a lot of hyperparameters and SpanMlt generates a great many of candidate spans if the value of maximal length of a span is large or the sentence is too long . The advantage of our methods is that only a small amount of hyper - parameters adjustment is required and similar or even better performance can be achieved .", "entities": [[57, 59, "TaskName", "aspect extraction"], [106, 110, "TaskName", "multi - task learning"]]}
{"text": "The structures of our Target - Specified sequence labeling method based on Multi - head Self - Attention ( TSMSA ) and the Multi - Task version ( MT - TSMSA ) are shown in Figure 1 ( c ) and ( d ) . As aforementioned , we first use a special symbol \" [ SEP ] \" to label each aspect . Next , the multi - head self - attention method is applied to capture the context representations of the specific aspect explicitly , then they are passed to a projection layer and a Conditional Random Field ( CRF ) ( Lafferty et al , 2001 ) layer for sequence labeling . Furthermore , the aspect and opinion words extraction ( task 0 ) as well as the target - oriented opinion words extraction ( task 1 ) are combined for multi - task learning . These two tasks share the parameters of encoder but differ in projection and CRF layers .", "entities": [[97, 100, "MethodName", "Conditional Random Field"], [101, 102, "MethodName", "CRF"], [125, 126, "DatasetName", "0"], [144, 148, "TaskName", "multi - task learning"], [162, 163, "MethodName", "CRF"]]}
{"text": "We describe the multi - head self - attention approach according to Vaswani et al ( 2017 ) with the details shown in Figure 1 ( a ) and ( b ) . For each attention head in the above approach , we first compute the scaled dot - product attention . Particularly , the input consists of a set of queries , keys , and values , where d k stands for the dimension of queries and keys , and d v represents the dimension of values . Then they are packed together into matrices Q , K , and V , respectively . The scaled dot - product attention is calculated as follows : Attention ( Q , K , V ) = sof tmax ( QK T \u221a d k ) V. ( 1 ) Next , given the number of attention heads h , we can get the dimension of output d model = h \u00d7 d v . Finally , the multi - head attention is described as follows : length . The parameter matrices of projections are M H ( I , h ) = Concat ( head 1 , ... , head h ) W O , ( 2 ) head i = Attention ( IW Q i , IW K i , IW V i ) , ( 3 ) where I = { i 1 , i 2 , ... , i n } ( the dimension of i is d model ) W Q i R d model \u00d7d k , W K i R d model \u00d7d k , W V i R d model \u00d7dv , and W O i R d model \u00d7d model .", "entities": [[46, 51, "MethodName", "scaled dot - product attention"], [106, 111, "MethodName", "scaled dot - product attention"], [125, 126, "DatasetName", "sof"], [166, 170, "MethodName", "multi - head attention"]]}
{"text": "Given a sequential representation H l and a sequential label Y = { y 1 , ... , y n } ( y i { B , I , O , [ SEP ] } or y i { B - ASP , I - ASP , B - OP , I - OP , O } 1 ) , we can use H l to compute p ( Y | H l ) . Greedy decoding or CRF can be adopted in the decoding process . CRF is chosen as our decoding strategy because CRF has the ability to capture the correlations between tokens and labels and the correlations between adjacent labels simultaneously . Given a new sentence , we use Viterbi algorithm ( Viterbi , 1967 ) to predict the label sequence by maximizing the conditional probability p ( Y | H l ) in the decoding process .", "entities": [[78, 79, "MethodName", "CRF"], [87, 88, "MethodName", "CRF"], [95, 96, "MethodName", "CRF"]]}
{"text": "For TOWE , a sentence with a given aspect ( i.e. , target ) is first processed into target - specified mode ( \" [ SEP ] Aspect [ SEP ] \" ) with the special symbol \" [ SEP ] \" and then passed into TSMSA , the outputs of which are the target - oriented opinion terms . For AOPE , MT - TSMSA generates aspect - opinion pairs by a two - stage inference process . Firstly , a sentence is passed into MT - TSMSA , where aspects are extracted in task 0 . Secondly , given extracted aspects , repeating the inference process of TOWE , MT - TSMSA outputs the target - oriented opinion terms from task 1 . Accordingly , the combinations of aspects from task 0 and target - orient opinion terms from task 1 are aspect - opinion pairs .", "entities": [[96, 97, "DatasetName", "0"], [133, 134, "DatasetName", "0"]]}
{"text": "Fan et al ( 2019 ) have employed various baselines in TOWE , including Distance - rule ( Hu and Liu , 2004 ) , Dependency - rule ( Zhuang et al , 2006 ) , BiLSTM + Distance - rule , and TC - BiLSTM , except for BERT - based methods . To achieve comprehensive comparative analysis , we develop baselines of BERT + Distance - rule and Target - fused BERT ( TF - BERT ) for this task . The former trains a sentence - level opinion term extraction model by BERT , and the target - oriented opinion term is the one nearest to each aspect . The latter utilizes the average pooling of target word embeddings to represent the target information . The word representation at each position is the addition of word embedding and target information , which is fed into BERT to extract target - oriented opinion terms . have applied some baselines in AOPE , including HAST ( Li et al , 2018 ) + IOG and JERE - MHS ( Bekoulis et al , 2018 ) . Besides the above methods , we also employ the following baselines : IOG ( Fan et al , 2019 ) utilizes an Inward - Outward LSTM and a Global LSTM to capture the information of aspects and global information respectively , then it combines these information for sequence labeling . SpanMlt ) is a span - based multi - task learning framework where the terms are extracted with annotated span boundaries and then the relations between combinations of every two spans are identified . SDRN ( Chen et al , 2020 ) utilizes BERT as the encoder which consists of an opinion entity extraction unit , a relation detection unit , and a synchronization unit for the AOPE task . In the case of TOWE , this model extracts the target - oriented opinion terms with given correct aspects .", "entities": [[36, 37, "MethodName", "BiLSTM"], [45, 46, "MethodName", "BiLSTM"], [49, 50, "MethodName", "BERT"], [64, 65, "MethodName", "BERT"], [73, 74, "MethodName", "BERT"], [77, 78, "MethodName", "BERT"], [91, 93, "TaskName", "term extraction"], [95, 96, "MethodName", "BERT"], [116, 118, "MethodName", "average pooling"], [120, 122, "TaskName", "word embeddings"], [148, 149, "MethodName", "BERT"], [212, 213, "MethodName", "LSTM"], [216, 217, "MethodName", "LSTM"], [244, 248, "TaskName", "multi - task learning"], [280, 281, "MethodName", "BERT"]]}
{"text": "As mentioned above , our method can be applied to AOPE by combining TOWE with aspect and opinion term extraction . We here compare the performance of our multi - task model ( i.e. , MT - TSMSA ) with the following competitive models : HAST + IOG , JERE - MHS , SpanMlt , and SDRN . The results are shown in Table 4 . Note that the overlapping ratios of pairs in 14lap , 14res , and 15res are 78.8 % , 92 % , and 99.8 % for ( Fan et al , 2019 ) , and 87.1 % , 86.2 % , and 86.4 % for ( Chen et al , 2020 ) , respectively . Thus , there is a difference ( within 2 % mostly ) between the results on these two datasets . and Chen et al ( 2020 ) . Best results are marked in bold . The performance of JERE - MHS is better than HAST + IOG , which indicates that the degree of error propagation in the separate training model might be smaller than it in the model of joint training . Moreover , SpanMlt , SDRN , and MT - TSMSA ( BERT ) use powerful pre - trained language models , which have a significant improvement in the performance on AOPE . We observe that SDRN and MT - TSMSA ( BERT ) perform better than Span - Mlt , showing that selecting top k spans from candidate spans as pairs might miss some correct pairs . Compared to SDRN , MT - TSMSA ( BERT ) performs better on three datasets and nearly the same on four datasets . Overall , MT - TSMSA achieves quite competitive performance on AOPE by simply incorporating our TSMSA into a multitask structure .", "entities": [[18, 20, "TaskName", "term extraction"], [204, 205, "MethodName", "BERT"], [234, 235, "MethodName", "BERT"], [268, 269, "MethodName", "BERT"]]}
{"text": "To evaluate the impacts of different word embeddings and training strategies on our models , we conduct ablation experiments by varying the above factors . The results shown in Table 5 indicate that a suitable word embedding is capable of improving the performance of our models . Firstly , BERT embedding shows poor performance when compared to Glove . We conjecture that BERT embedding needs to cooperate with the pre - trained encoder of BERT to perform better on TOWE . Secondly , applying the word embedding and the encoder of BERT without fine - tuning also fails to work on TOWE . The reason may be that the encoder of BERT without fine - tuning can not capture the information of the specific aspect with the symbol \" [ SEP ] \" . Furthermore , opinion terms extracted from task 0 help to identify the corresponding opinion terms in task 1 , which means that the multi - task structure is able to achieve better results than the single - task structure on TOWE . Although the improvement is not significant in average , we observe that the former structure can achieve more stable performance than the latter one .", "entities": [[6, 8, "TaskName", "word embeddings"], [49, 50, "MethodName", "BERT"], [62, 63, "MethodName", "BERT"], [74, 75, "MethodName", "BERT"], [91, 92, "MethodName", "BERT"], [111, 112, "MethodName", "BERT"], [141, 142, "DatasetName", "0"]]}
{"text": "In this part , we apply an open source tool 4 to visualize the attention scores of TSMSA ( BERT ) and describe two attention heads on the tenth layer in Figure 3 ( a ) and ( b ) , where attention scores less than 0.1 and unimportant words are not displayed . As we can see , the words \" nice \" and \" great \" are both close to the aspect \" food \" , but \" nice \" will not pay attention to this aspect . In addition , \" great \" and \" reasonable \" focus on the special symbol \" [ SEP ] \" and the specific aspect \" food \" , as shown in Figure 3 ( a ) . At the same time , \" food \" gives attention to \" great \" and \" reasonable \" on different attention heads , as described in Figure 3 ( b ) . All these instances reveal that multi - head self - attention mechanism is capable of capturing the representation of a [ SEP ] [ SEP ] [ SEP ] [ SEP ] [ SEP ] [ SEP ] [ SEP ] specific aspect .", "entities": [[19, 20, "MethodName", "BERT"]]}
{"text": "The framing of political issues can influence policy and public opinion . Even though the public plays a key role in creating and spreading frames , little is known about how ordinary people on social media frame political issues . By creating a new dataset of immigrationrelated tweets labeled for multiple framing typologies from political communication theory , we develop supervised models to detect frames . We demonstrate how users ' ideology and region impact framing choices , and how a message 's framing influences audience responses . We find that the more commonlyused issue - generic frames obscure important ideological and regional patterns that are only revealed by immigration - specific frames . Furthermore , frames oriented towards human interests , culture , and politics are associated with higher user engagement . This large - scale analysis of a complex social and linguistic phenomenon contributes to both NLP and social science research . Frame Type Frame Description Issue - Generic Economic Financial implications of an issue Policy Capacity & Resources The availability or lack of time , physical , human , or financial resources Morality & Ethics Perspectives compelled by religion or secular sense of ethics or social responsibility Fairness & Equality The ( in ) equality with which laws , punishments , rewards , resources are distributed Legality , Constitutionality & Jurisdiction Court cases and existing laws that regulate policies ; constitutional interpretation ; legal processes such as seeking asylum or obtaining citizenship ; jurisdiction", "entities": [[187, 188, "DatasetName", "Ethics"], [200, 201, "TaskName", "Fairness"]]}
{"text": "Because many people now generate and consume political content on social media , scholars have increasingly used automated techniques to study framing on social media . Large - scale research of framing on Twitter has commonly focused on unsupervised approaches . ( e.g. , Russell Neuman et al , 2014 ; Meraz and Papacharissi , 2013 ; de Saint Laurent et al , 2020 ) . Such approaches , including those focused on hashtag analysis , can reveal interesting framing patterns . For instance , Siapera et al ( 2018 ) shows that frame usage varies across events . Similarly , topic models have been used to compare \" refugee crisis \" media discourses across the European countries ( Heidenreich et al , 2019 ) , and to uncover differences in attitudes towards migrants ( Hartnett , 2019 ) . Although lexicon analysis and topic models can provide insights about immigration discourse , here , we adopt a supervised approach to ground our work in framing research and to enable robust evaluation . We draw inspiration from a growing body of NLP research that uses supervised approaches to detect issue - generic policy frames in news articles , a task popularized by the Media Frames Corpus ( Card et al , 2015 ) , which contains issuegeneric policy frame labels for articles across several issues ( Boydstun et al , 2013 ) . Using this corpus , prior work has detected frames with techniques including logistic regression ( Card et al , Table 1 : List of all issue - generic policy ( Boydstun et al , 2013 ) , immigration - specific ( Benson , 2013Hovden and Mjelde , 2019 ) , and narrative ( Iyengar , 1991 ) frames with brief descriptions . 2016 ) , recurrent neural networks ( Naderi and Hirst , 2017 ) , lexicon induction ( Field et al , 2018 ) , and fine - tuning pretrained language models ( Khanehzar et al , 2019 ; Kwak et al , 2020 ) . Roy and Goldwasser ( 2020 ) further extracted subcategories of issue - generic policy frames in newspaper coverage using a weakly - supervised approach . Finally , issue - generic frames have also been computationally studied in other media , including online fora and politicians ' tweets ( Johnson et al , 2017 ; Hartmann et al , 2019 ) . We build upon this literature by incorporating additional frame typologies that reflect important dimensions of media discourse with real - world consequences ( Iyengar , 1991 ; Gross , 2008 ; Eberl et al , 2018 ) . Beyond detecting frames , we computationally analyze frame - building and frame - setting among social media users ; though well - studied in traditional news media , little is known about how social media users frame immigration or its effects ( Eberl et al , 2018 ) . Noting that issue - generic policy frames obscure important linguistic differences , several works stud - ied issue - specific frames in news media for issues such as missile defense and gun violence ( Morstatter et al , 2018 ; Liu et al , 2019a ) . We extend issuespecific frame analyses to immigration by adopting an immigration - specific typology developed by political communication scholars ( Benson , 2013 ) . In contrast to prior NLP work focused on traditional media or political elites ( Johnson et al , 2017 ; Field et al , 2018 ) , we highlight the role that social media publics play in generating and propagating frames . Furthermore , we provide a new computational model of narrative framing ( Iyengar , 1991 ) , that together with models for issue - generic policy and issue - specific frames , provides complementary views on the framing of immigration . Finally , our large - scale analysis of frame - setting illustrates the potential for using NLP to understand how a message 's framing shapes its audience behavior .", "entities": [[101, 103, "TaskName", "topic models"], [144, 146, "TaskName", "topic models"], [245, 247, "MethodName", "logistic regression"], [323, 326, "TaskName", "pretrained language models"]]}
{"text": "We first collect a large dataset of immigrationrelated tweets , and then annotate a subset of this full dataset for multiple types of frames . Data Collection We extract all English - language tweets in 2018 and 2019 from the Twitter Decahose containing at least one of the following terms : immigration , immigrant ( s ) , emigration , emigrant ( s ) , migration , migrant ( s ) , illegal alien ( s ) , illegals , and undocumented 1 . We focus on content creation and thus exclude retweets from our dataset , though we consider retweeting rates when analyzing the social influence of different frames . We further restrict our dataset to tweets whose authors are identified as being located in the United States ( US ) , United Kingdom ( GB ) , and European Union ( EU ) by an existing location inference tool ( Compton et al , 2014 ) . To compare framing across political ideologies , we obtain ideal point estimates for nearly two - thirds of US - based users with Barber\u00e1 ( 2015 ) 's Bayesian Spatial Following model . Our full dataset contains over 2.66 million tweets , 86.2 % of which are from the United States , 10.4 % from the United Kingdom , and 3.4 % from the European Union . Data Annotation Tweets are annotated using three frame typologies : ( i ) issue - generic policy , ( ii ) immigration - specific , and ( iii ) narrative frames , where a tweet may use multiple frames simultaneously . We use Boydstun et al ( 2013 ) 's Policy Frames Codebook to formulate our initial guidelines to code for policy frames . We use Benson ( 2013 ) 's immigration - specific frames , but follow Hovden and Mjelde ( 2019 ) in including an additional category for framing immigrants as victims of war . Finally , we code for narrative frames using definitions from Iyengar ( 1991 ) . All frames and descriptions can be found in Table 1 , with a complete codebook in Supplementary Materials . Because annotation guidelines from prior work focus on elite communications , we first adjusted our codebook to address challenges posed by Twitter content . Changes were made based on feedback from four trained annotators who labeled 360 tweets from 2018 , split between the EU , GB , and US . Even for humans , identifying frames in tweets is a difficult task . Defining the boundaries of what constitutes a message is not trivial . Beyond the text , frames could be identified in hashtags , images , videos , and content from linked pages . Furthermore , tweets are often replies to other users or part of a larger thread . This additional context may influence an issue 's framing . For simplicity , we treat each tweet as a standalone message and label frames based only on the text ( including hashtags ) . Unlike news stories , where frames are clearly cued , tweets often implicitly allude to frames due to character limitations . For example , a tweet expressing desire to \" drive immigrants out \" with no additional context may suggest a criminal frame , but criminality is not explicit . To minimize errors , we avoid making assumptions about intended meaning and interpret al messages literally . Training , development , and test data were annotated using two procedures after four annotators completed four rounds of training . The dataset contains equal numbers of tweets from the EU , UK , and US . Training data was singly annotated and includes 3 , 600 tweets , while the development and test sets each contain 450 tweets ( 10 % of the full dataset ) and were consensus - coded by pairs of trained annotators . We opt for this two - tier approach due to ( i ) the inherent difficulty of the task 2 and ( ii ) the need to maximize diversity seen in training . During annotator training , pilot studies attained moderate agreement , suggesting that to attain high - reliability , consensus coding with adjudication would be needed ( Krippendorff , 2013 ) , which comes at a cost of substantially increased time . Because a large dataset of unique , singlycoded documents is preferable to a small dataset of documents coded by multiple annotators for text classification ( Barbera et al , 2021 ) , we decided to increase corpus diversity in the training data by singly - annotating , at the expense of potentially noisier annotation , and to consensus code all evaluation data . On the double annotated data , annotators attained Krippendorff 's \u03b1=0.45 . Additional details are provided in Supplementary Material ( B , Figures 6 and 7 ) . Results We observe differences across frame typologies in coverage rates within the annotated data set . While 84 % of tweets are labeled with at least one issue - generic policy frame and 85 % with at least one narrative frame , only 51 % are labeled with at least one issue - specific frame . This difference is due to immigration - specific frames being more narrowly - defined , as they require explicit judgment of immigrants as heroes , victims , or threats . Further details about frame distributions Random LogReg RoBERTa FT RoBERTa 0.193 0.296 0.611 0.657 in our annotations can be found in Supplementary Material ( A , Figure 5 ) . While the precision of issue - specific frames can reveal patterns otherwise obscured by the broader issue - generic frames , this lack of coverage presents two challenges : 1 ) automated detection is more challenging given this sparsity and 2 ) analyses of issue - specific frames do not capture a large portion of immigration - related discourse . By incorporating multiple framing strategies , we leverage both the coverage of issue - generic frames and the precision and interpretability of issue - specific frames .", "entities": [[745, 747, "TaskName", "text classification"], [803, 805, "DatasetName", "Supplementary Material"], [907, 908, "MethodName", "RoBERTa"], [909, 910, "MethodName", "RoBERTa"], [921, 923, "DatasetName", "Supplementary Material"]]}
{"text": "We detect frames for all 2.6 M immigration - related tweets using the finetuned RoBERTa model with the best - performing seed on development data . Using this labeled data , we estimate the effects of region and ideology by fitting separate mixed - effects logistic regression models to predict the presence or absence of each frame . We treat region ( US , UK , and EU ) as a categorical variable , with US as the reference level . Ideology is estimated using the method of Barber\u00e1 ( 2015 ) , which is based on users ' connections to US political elites ; as such , we restrict our analysis of ideology to only tweets from the United States . To account for exogenous events that may impact framing , we include nested random effects for year , month , and date . We further control for user characteristics ( e.g. the author 's follower count , friends count , verified status and number of prior tweets ) as well as other tweet characteristics ( e.g. tweet length , if a tweet is a reply , and whether the tweet contains hashtags , URLs , or mentions of other users ) . We apply Holm - Bonferroni corrections on p - values before significance testing to account for multiple hypothesis testing . Ideology Ideology is strongly predictive of framing strategies in all three categories , as shown in Figure 2 . Our results reveal three broad themes . First , prior work has argued that liberals and conservatives adhere to different moral foundations , with conservatives being more sensitive to in - group / loyalty and authority than liberals , who are more sensitive to care and fairness ( Graham et al , 2009 ) . Our results agree with this argument . Liberals are more likely to frame immigration as a fairness and morality issue , and immigrants as victims of discrimination and inhumane policies . More conservative authors , on the other hand , fo - cus on frames with implications for the in - group . They express concerns about 1 ) immigrants imposing a burden on taxpayers and governmental programs and 2 ) immigrants being criminals and threats to public safety . We qualitatively observe three distinct , though unsubstantiated , conservative claims contributing to the latter : ( i. ) Immigrants commit violent crimes ( Light and Miller , 2018 ) , ( ii . ) Undocumented immigrants illegally vote in US elections ( Smith , 2017 ; Udani and Kimball , 2018 ) , and ( iii . ) Immigrants are criminals simply by virtue of being immigrants ( Ewing et al , 2015 ) . Figure 2 shows a clear ideological stratification for issue - specific frames : liberals favor hero and victim frames , while conservatives favor threat frames . This finding is consistent with prior work on the role perceived threats play in shaping white American attitudes towards immigration ( Brader et al , 2008 ) , and the disposition of political conservatism to avoid potential threats ( Jost et al , 2003 ) . Second , while all frame categories show ideological bias , issue - specific frames are the most extreme . Most notably , our analysis shows that focusing solely on issue - generic policy frames would obscure important patterns . For example , the issuegeneric cultural identity frame shows a slight liberal bias ; yet , related issue - specific frames diverge : hero : cultural diversity is very liberal while threat : national cohesion is very conservative . Similarly , the issue - generic economic policy frame is slightly favored by more conservative authors , but the related issue - specific frames threat : jobs and hero : worker reveal ideological divides . This finding highlights the importance of using multiple framing typologies to provide a more nuanced analysis of immigration discourse . Third , more liberal authors tend to use episodic frames , while conservative authors tend to use thematic frames . This difference is consistent with Somaini ( 2019 ) 's finding that a local liberal newspaper featured more episodic framing in immigration coverage , but a comparable conservative newspaper featured more thematic framing . Other efforts that examine the relationship between narrative frames and cognitive and emotional responses provide some clues for the observed pattern . For instance , Aar\u00f8e ( 2011 ) shows that thematic frames are stronger when there are no or weak emotional responses ; and that the opposite is true for episodic frames . The divergence of findings could be driven by partisans ' differing emotional responses . Our findings also highlight important consequences for opinion formation . Iyengar ( 1990 ) shows that episodic framing diverts attention from societal and political party responsibility ; our results suggest that liberal Twitter users are likely to produce ( and , due to partisan self - segregation , consume ) social media content with such effects . Region Immigration framing depends heavily on one 's geopolitical entity ( US , UK , and EU ) , as shown in Figure 3 . Several notable themes emerge . First , many ideologically - extreme frames in the US , including crime & punishment , security & defense , threat : public order , and threat : fiscal are all significantly more likely to be found in US - based tweets relative to the UK and EU . This pattern suggests that region and ideology , and likely many other factors , interact in intricate ways to shape how ordinary people frame political issues . Second , cultural identity is more strongly associated with both the UK and EU than the US . Perhaps immigrants ' backgrounds are more marked in European discourse than in US discourse because the UK and EU have longer histories of cultural and ethnic homogeneity ( Thorbj\u00f8rnsrud , 2015 ) . This finding also reflects that Europeans ' attitudes towards immigration depend on where immigrants are from and parallels how European newspapers frame immigration differently depending on migrants ' countries of origin ( Eberl et al , 2018 ) . Finally , the bottom of Figure 3 shows that users from the UK are more likely to invoke labor - related frames . This prevalence of labor and economic frames has also been found in British traditional media ( Caviedes , 2015 ; Lawlor , 2015 ) , and has been attributed to differences in the labor market . Unlike migrants in the US , Italy , and France , who often work clandestinely in different economic sectors than domestic workers , UK migrants have proper authorization and are thus viewed as competition for British workers because they can work in the same industries ( Caviedes , 2015 ) .", "entities": [[14, 15, "MethodName", "RoBERTa"], [45, 47, "MethodName", "logistic regression"]]}
{"text": "Our analysis of frame - building involves inferring political ideology and regional from users with existing tools , so we aggregated this information in our analysis in order to minimize the risk of exposing potentially sensitive personal data about individuals . Our dataset includes tweet IDs along with frame labels , but no additional social information . However , there are also ethical consequences of categorizing people along these social dimensions . We acknowledge that reducing people 's social identities to region and ideology obscures the wide range of unobservable and non - quantifiable predispositions and experiences that may impact framing and attitudes towards immigration . We emphasize that our dataset is not fully representative of all immigration discourse and should not be treated as such . Twitter 's demographics are not representative of the global population ( Mislove et al , 2011 ) . Furthermore , our dataset only includes tweets with authors from particular Western countries . All tweets were automatically identified by Twitter as being written in English , thus additionally imposing standard language ideologies on the data that we include ( Milroy , 2001 ) . Furthermore , language choice itself can be a socially and politically meaningful linguistic cue that may have unique interactions with framing ( e.g. , Gal , 1978 ; Shoemark et al , 2017 ; Stewart et al , 2018 ; Ndubuisi - Obi et al , 2019 ) . Although we do not focus on abusive language , our topical content contains frequent instances of racism , Islamophobia , antisemitism , and personal insults . We caution future researchers about potentially traumatic psychological effects of working with this dataset . We aim to support immigrants , an often marginalized group , by shedding light on their representation on social media . However , there is a risk that malicious agents could exploit our framesetting findings by disseminating harmful content packaged in more popular frames .", "entities": [[245, 247, "TaskName", "abusive language"]]}
{"text": "Tables 5 - 8 and Figures 8 - 9 provide details about the fine - tuned RoBERTa models ' performance .", "entities": [[16, 17, "MethodName", "RoBERTa"]]}
{"text": "We probe the heterogeneity in levels of abusive language in different sections of the Internet , using an annotated corpus of Wikipedia page edit comments to train a binary classifier for abuse detection . Our test data come from the CrimeBB Corpus of hacking - related forum posts and we find that ( a ) forum interactions are rarely abusive , ( b ) the abusive language which does exist tends to be relatively mild compared to that found in the Wikipedia comments domain , and tends to involve aggressive posturing rather than hate speech or threats of violence . We observe that the purpose of conversations in online forums tend to be more constructive and informative than those in Wikipedia page edit comments which are geared more towards adversarial interactions , and that this may explain the lower levels of abuse found in our forum data than in Wikipedia comments . Further work remains to be done to compare these results with other inter - domain classification experiments , and to understand the impact of aggressive language in forum conversations .", "entities": [[7, 9, "TaskName", "abusive language"], [31, 33, "TaskName", "abuse detection"], [65, 67, "TaskName", "abusive language"], [93, 95, "DatasetName", "hate speech"]]}
{"text": "The automatic identification of abusive language online 1 is of growing interest and concerns have proliferated about aggressive Internet behaviours commonly known as ' trolling ' . From an applications perspective , the accurate detection of vitriolic language is one of the clearest examples of natural language processing for social good , assuming data has been collected ethically and stored legally , and that any intervention is left to the appropriate authorities ( Kennedy et al , 2017 ; Kumar et al , 2018 ) . Meanwhile from a theoretical point of view , there are many outstanding linguistic and sociological research questions surrounding Internet aggression and how it manifests itself in writing ( Pieschl et al , 2015 ; Waseem et al , 2017 ) . The question we address here is whether online abusive language is of one type or whether there is discernible variation in the level of abuse found in different subsections of the Internet . We do not claim to have the final answer to this nebulous question , but instead we have addressed one small part of the whole : is the level of abuse found in one Internet domain - namely discussions about English Wikipedia page edits - similar to that found in another domain , that of an online hacking forum ? We show that the type of abusive language occurring in the latter is more closely aligned with the milder levels of abuse of those found in Wikipedia discussions , and consider why this might be . We observe that the online hacking forum tends to contain texts aimed at helping or informing other users , whereas the Wikipedia conversations are inherently more adversarial since they relate to recent page edits and disputes arising . Where abusive language is found in the online hacking forum , it tends to involve profane namecalling , insults and heated disputes , rather than hate speech or threats of violence - those which have tended to be the more prominent causes for public concern . Note here that we make a distinction between aggressive and offensive language : the former often involves the latter , but not always so . Offensive language - identifiable word tokens such as swearwords and the like - may offend but is not always used aggressively ; sometimes it is used in a jocular fashion , for example . Aggressive language , which more often than not is built on the composition of many words , involves a hostile stance from one speaker or writer to another . It is this which might seem to be abusive and which we seek to automatically detect and better understand . We also distinguish aggressive language from hate speech - that which might be characterised as prejudicial diatribes to provoke action , perhaps violent , against a group or groups - and from cyberbullying - that which involves a sustained period of persecution against an individual or individuals . Certainly the distinctions are fuzzy at the edges , but these might be thought of as the canonical definitions of these abuse types . We are dealing with what we deem to be one - off instances of aggression in online communities , though if these were shown to be prejudicial against a group , or sustained against an individual , then the instances start to move into hate speech or cyberbullying behaviours . In both Wikipedia edits and the online hacking forum , abusive comments are infrequent in the community as a whole and the general objective of gaining reputation in the domain dis - incentivises aggressive behaviour . Nevertheless we show that aggressive language which does occur may be detected fairly well by training on the Wikipedia edits corpus - the advantage being that it has been multiply and widely annotated - and setting the threshold for a binary aggression classifier at a fairly moderate level relative to the worst types of abuse found in Wikipedia comments . Future work remains to be done to more broadly characterise intra - community behaviour in different subsections of the Internet .", "entities": [[4, 6, "TaskName", "abusive language"], [79, 80, "DatasetName", "Kumar"], [135, 137, "TaskName", "abusive language"], [226, 228, "TaskName", "abusive language"], [295, 297, "TaskName", "abusive language"], [319, 321, "DatasetName", "hate speech"], [454, 456, "DatasetName", "hate speech"], [564, 566, "DatasetName", "hate speech"]]}
{"text": "Offensive language serves many purposes in everyday discourse : from deliberate effect in humour to self - directed profanity to toxic or abusive intent . We are not concerned here with humorous uses of offensive language or with general profanity . Instead we are interested in toxic and abusive behaviour , specifically online harassment involving abusive language , aggression and personal attacks . There has been work on other forms of abusive behaviour , such as hate speech ( Warner and Hirschberg , 2012 ; Kwok and Wang , 2013 ; Ribeiro et al , 2018 ) and cyberbullying ( Xu et al , 2013 ; Pieschl et al , 2015 ) , and we put these aside for now as challenging , distinct topics ( though with the fuzzy edges described above ) . In terms of online harassment , previous work has centred around definitions , automatic detection , and dataset creation - for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpus ( Waseem and Hovy , 2016 ; Wulczyn et al , 2017 ) . Most work has been conducted on English data , with some extensions to other languages ( e.g. Arabic ( Mubarak et al , 2017 ) , Slovene ( Fi\u0161er et al , 2017 ) ) . Automated detection approaches have drawn on classic document classification methods for spam detection and sentiment analysis , and tend to use lexical and syntactic features ( Nobata et al , 2016 ; Li et al , 2017 ; Bourgonje et al , 2018 ) . Machine learning techniques range from logistic regression ( Cheng et al , 2015 ) to support vector machines ( Yin et al , 2009 ) to neural networks ( Gamb\u00e4ck and Sikdar , 2017 ) . Our aim here is not especially to push the boundaries on detection techniques - though naturally we wish our classifier to perform fairly well - but rather we are interested in how to make use of existing labelled training data when predicting personal attacks in other corpora . In case any persuasion is needed that improved understanding , detection and action on abusive language are desirable , there is evidence that experience of online harassment leads to decreased online participation and is connected with oppression , violence and suicide ( Dinakar et al , 2011 ; Sood et al , 2012 ; Wulczyn et al , 2017 ) . Of course there may be reasons to be concerned about the perpetrator 's wellbeing along with that of the victims ( Cheng et al , 2017 ) .", "entities": [[55, 57, "TaskName", "abusive language"], [76, 78, "DatasetName", "hate speech"], [151, 153, "DatasetName", "and dataset"], [158, 160, "DatasetName", "Hate Speech"], [223, 225, "TaskName", "document classification"], [227, 229, "TaskName", "spam detection"], [230, 232, "TaskName", "sentiment analysis"], [266, 268, "MethodName", "logistic regression"], [359, 361, "TaskName", "abusive language"]]}
{"text": "It is evident from our classification experiments that levels of linguistic aggression in HackForums tend to be milder than those in WikiComments , if we take the optimal value of t to lie between 2 and 3 ( Table 3 ) whereas for WikiComments it was found to be 4.25 ( Wulczyn et al , 2017 ) . A possible explanation for this finding may be the difference in purposes of the two sources for our test and training data : discussion of Wikipedia page edits often end up as arguments between contributors . The fact these arguments may become aggressive or personally offensive at times is unsurprising . In HackForums , where our test data came from , users often have the intention of educating others , learning from others , buying and selling products , and in many cases discouraging others from acting illegally online ( those with a so - called ' white hat ' hacking ethos - hackers who identify security vulnerabilities and report them rather than exploit them ) . HackForums is not an oasis of calm , positive behaviour , however - on the con - trary , users can often be off - hand in their comments , dismissive of ' noobs ' and ' skids ' ( script kiddies - a novice or tinkerer ) , sarcastic and rude . These attitudes , where they do not cross the line into aggressive behaviour , map to our negative label for author intent . Debates about hacking techniques , authorship of code , and user behaviour ( e.g. spam , posting out - of - date tutorials , offering hacking tools which do n't work as advertised ) are frequent . But on the whole , the forum exists for information and technology exchange and the white hat hackers , along with active administrators and a reputation scoring system , help to constrain user behaviour . Indeed this highly active reputation scoring system may deter aggressive online harassment and allow for users to engender trust in what could otherwise be quite untrustworthy environments ( Holt et al , 2016 ; D\u00e9cary - H\u00e9tu and Lepp\u00e4nen , 2016 ) . Furthermore , online deviant communities such as these tend to be rather homogeneous , particularly involving young males ( Hutchings and Chua , 2017 ) . Therefore the targets for any harassment may be off , rather than on , the forum . Aside from aggression , we also labelled positive texts ( which answer others ' questions , contain laughter - related word tokens or emoticons , or praise the work of others ) , neutral texts , and negative texts ( including users stating that others can not or should not do something , sarcasm and arguments ) . These intent types are the majority labels in our 4123 post subset , with 1562 positive , 2566 neutral and 788 negative occurrences ( the posts could be multiply labelled , hence these counts sum to more than 4123 ) . Minority labels are aggression ( n=100 ) , users posting to moderate discussion ( n=119 ) , and requests to continue discussion in private messaging ( n=238 ) . We further subdivide our set of 100 aggressive forum posts into seven classes : simply aggressive , personal denigration , alludes to violence , refers to disability , features misogyny , homophobia , racism . Personal denigration typically involves name - calling - dismissing someone as an idiot or moron , doubting their technical skills , and so on . The other classes indicate that the author of the post alludes to violence ( \" I 'll cut your neck \" ) , disability ( \" you 're a retard \" ) , misogyny ( \" stop bitching \" ) , homophobia ( \" that 's gay \" ) , and racism ( \" fucking jew \" ) . Note that , with the exception of ' simply aggressive ' which tends to be a fallback if the post falls into no other class , the posts may be assigned multiple labels and that a single annotator undertook labelling . Label counts are shown in Table 4 . We find that most aggressive posts are just that - simply aggressive manners of writing which would be out of place in polite discourse . For example , authors add emphasis with the f - word , including formulaic phrases in acronym form ( ' gtfo ' , ' wtf ' , ' stfu ' ) . The next most common aggression type is personal denigration : most often calling the addressee 's intelligence into question , or doubting their motives . After that , the minority labels are those which might feature in hate speech : discriminating against women , homosexuals and ethnicities . In addition , the ' refers to dis -", "entities": [[799, 801, "DatasetName", "hate speech"]]}
{"text": "The ContrastMedium Algorithm : Taxonomy Induction From Noisy Knowledge Graphs With Just a Few Links", "entities": [[8, 10, "TaskName", "Knowledge Graphs"]]}
{"text": "In this paper , we present ContrastMedium , an algorithm that transforms noisy semantic networks into full - fledged , clean taxonomies . ContrastMedium is able to identify the embedded taxonomy structure from a noisy knowledge graph without explicit human supervision such as , for instance , a set of manually selected input root and leaf concepts . This is achieved by leveraging structural information from a companion reference taxonomy , to which the input knowledge graph is linked ( either automatically or manually ) . When used in conjunction with methods for hypernym acquisition and knowledge base linking , our methodology provides a complete solution for end - to - end taxonomy induction . We conduct experiments using automatically acquired knowledge graphs , as well as a SemEval benchmark , and show that our method is able to achieve high performance on the task of taxonomy induction .", "entities": [[121, 123, "TaskName", "knowledge graphs"]]}
{"text": "Recent years have witnessed an impressive amount of work on automatic construction of wide - coverage knowledge resources . Web - scale open information extraction systems like NELL ( Carlson et al , 2010 ) or ReVerb have been successful in acquiring massive amounts of machine - readable knowledge by effectively tapping large amounts of text from Web pages . However , the output of these systems does not consist of a clean , fully - semantified output . Such output , on the other hand , could be provided by the vocabulary of large - scale ontologies like DBpedia ( Bizer et al , 2009 ) or YAGO ( Hoffart et al , 2013 ) and the integration of open and closed information extraction approaches ( Dutta et al , 2014 ) . The use of an encyclopedia - centric ( e.g. , Wikipedia - based ) dictionary of entities leads to poor coverage of domain - specific terminologies . This can be alleviated by constructing knowledge bases of ever increasing coverage and complexity from the Web ( Wu et al , 2012 ; Gupta et al , 2014 ; Dong et al , 2014 ) or by community efforts ( Bollacker et al , 2008 ) . However , the focus on large size and wide coverage at entity level has led all these resources to avoid the complementary problem of curating and maintaining a clean taxonomic backbone with as minimal supervision as possible . That is , no resource , to date , integrates structured information from existing wide - coverage knowledge graphs with empirical evidence from text for the explicit goal of building full - fledged taxonomies consisting of a clean and fully - connected directed acyclic graph ( DAG ) . This is despite the fact that taxonomies have been known for a long time to provide valid tools to represent domain - specific knowledge with dozens of scientific , industrial and social applications ( Glass and Vessey , 1995 ) . In taxonomy induction , the required domain knowledge can be acquired with many different methods for hypernym extraction , ranging from simple lexical patterns ( Hearst , 1992 ; Oakes , 2005 ; Kozareva and Hovy , 2010 ) to statistical and machine learning techniques ( Caraballo , 1999 ; Agirre et al , 2000 ; Ritter et al , 2009 ; Velardi et al , 2013 ) . Recent efforts , such as Microsoft 's Probase ( Wu et al , 2012 ) or the WebIsaDB ( Seitner et al , 2016 ) similarly focus on ' local ' extraction of single hypernym relations , and do not address the problem of how to combine these single relations into a coherent taxonomy . When taxonomies are automatically acquired , their cleaning ( also called \" pruning \" ) becomes a mandatory step ( Velardi et al , 2013 ) . The contributions of this paper are two - fold : 1 . We introduce a new algorithm , named Con - trastMedium , which , given a noisy knowledge graph and its ( possibly automatically generated ) links to a companion taxonomy , is able to output a full - fledged taxonomy . Information from the reference taxonomy is projected onto the input noisy graph to automatically acquire topological clues , which are then used to drive the cleaning process . The reference taxonomy provides us with ground - truth taxonomic relations that make our knowledge - based method not truly unsupervised sensu stricto . However , the availability of resources like , for instance , WordNet ( Fellbaum , 1998 ) or BabelNet ( Navigli and Ponzetto , 2012 ) implies that these requirements are trivially satisfied ; 2 . We combine our approach with an unsupervised framework for knowledge acquisition from text to provide a full end - to - end pipeline for taxonomy induction from scratch .", "entities": [[22, 25, "TaskName", "open information extraction"], [27, 28, "DatasetName", "NELL"], [36, 37, "DatasetName", "ReVerb"], [99, 100, "DatasetName", "DBpedia"], [108, 109, "DatasetName", "YAGO"], [264, 266, "TaskName", "knowledge graphs"]]}
{"text": "At its core , our algorithm relies on the notion of a linked noisy knowledge graph ( LNKG ) . This consists of a quintuple ( G , KB , KB root , \u03bb , M ) where : i ) G = ( V G , E G ) is a noisy knowledge graph ; ii ) KB = ( V KB , E KB ) is a companion knowledge base providing a ground - truth taxonomy ; iii ) KB root is the root node of the reference knowledge base KB ( if several top - level nodes exist , an artificial root can be created by connecting them all ) ; iv ) \u03bb is a conventional symbol to represent the \" undefined concept \" , i.e. , a place - holder for empty mappings ; v ) M : V G V KB \u222a { \u03bb } is the function , which maps nodes of V G into nodes of V KB or into the undefined concept \u03bb . The key ideas behind ContrastMedium are : Identification of important topological clues from the companion knowledge base KB in order to hierarchically sort the concepts in G. For our purposes , KB is expected to be able to provide ground - truth taxonomic relations that can be safely projected onto G to guide the cleaning process : that is , we assume it to be reasonably clean . In contrast , we do not make any assumption on how KB has been created : our approach can be used with either manually created taxonomies like WordNet or ( semi - ) automatically induced ones , provided they are of sufficient quality . Hence , our method is knowledge - based without the need of further supervision other than that contained in KB ; Projection of topological clues from KB back onto the LNKG G on the basis of the links found in the mapping M . Similarly to the case of the reference knowledge base , we do not make any assumption on how the links between G and KB have been created : while there exists different methods to automatically link ( lexical ) knowledge bases ( Navigli and Ponzetto , 2012 ; , we later show that it is also possible to achieve state - of - the - art performance with a few manually given links ; Propagation of the topological clues across the entire NKG G. That is , to cope with the partial coverage of automatic mappings , as well as the need to reduce the number of manually created KB links , we apply a signal propagation technique that solely relies on the structure of G ; To make use of the resulting topological clues to drive the taxonomy pruning process . That is , propagated topological clues from KB are additionally leveraged to ensure that the output results in a proper taxonomic structure . We rely on the metaphor of a contrast medium ( CM ) to describe our approach , which is summarized in Figure 1 . In the context of clinical analysis , a CM is injected into the human body to highlight specific complex internal body structures ( in general , the venuous system ) . In a similar fashion , we detect the topological structure of a graph by propagating a certain amount of CM that we initially inject through the node KB root of the companion knowledge base KB . The highlighted structure indicates the distance of a node with respect to the node KB root . Then the lowest values of contrast medium indicate the leaf terminological nodes . The observed quantities are then transferred to corresponding nodes of the noisy graph by the mapping M . Next , the medium is propagated by ' shaking ' the noisy graph . We let the fluid reach all the components G by alternating two phases of propagation : letting the CM to flow through both incoming ( ' shake up ' ) ; and outgoing ( ' shake down ' ) edges . At the end , we use the partial order induced by the observed node level of CM to drive the pruning phase , and ' stretch ' the original NKG G into a proper DAG . Our approach is presented in Algorithms 1 and 2 . 3 It consists of the following main steps : 1 ) CM injection Cf . Figure 1 , block 1 and Algorithm 1 , lines 1 - 2 . We initially define the function C KB : V KB [ 0.0 \u2212 1.0 ] and assign a zero contrast medium level to all the nodes of the KB graph C KB ( x ) = 0 , x V KB ( line 1 ) . Next , we call the routine ' injectContrastMedium ' which : 1 ) assigns an initial contrast level equals to 1.0 to the node KB root of the KB graph ; ii ) uses the routine \" Shake \" with the direction parameter equals to \" DOWN \" ( see Algorithm 2 and Step 3 \" Graph shaking \" for more details ) to let the CM drop through KB . In practice , the shaking routine implements a node contrast medium level propagation algorithm following the outgoing ( ' down ' ) or the incoming ( ' up ' ) edges of the graph . 2 ) CM transfer Cf . Figure 1 , block 2 and Algorithm 1 , lines 3 - 5 . In the next phase , we first extract the hypernymy subgraph T = ( V T , E T ) of G ( see Section 3.1 ) and then follow the links in the mapping M to transfer the contrast medium levels , i.e. , C T ( y ) = C KB ( x ) ( s.t . x V KB , y V T , ( y x ) M ) . 3 ) Graph shaking Cf . Figure 1 , block 3 and Algorithm 1 , lines 6 - 8 . After having transferred the CM to the target hypernym graph T of G , we shake T to let the CM flow by traversing the incoming , the outgoing , and finally the incoming edges again - see Algorithm 2 for details on the ' Shake ' routine . Note that these two kinds of propagation are needed since the CM needs to be propagated through all the nodes of the graph to highlight the topological clues we are searching for . In particular , in Algorithm 2 at each iteration t for each node x V graph , depending on the value of the parameter direction ( line 8 and line 12 ) : i ) we observe a CM level for the node x ( line 7 ) ; ii ) if direction = = DOWN ( lines 9 - 11 ) we traverse all the outgoing edges ( x , y ) of x and propagate the observed CM level of x , otherwise ( direction = = UP , lines 13 - 15 ) we traverse the incoming edges ( y , x ) and propagate the CM level to the nodes y ; iii ) the value of F lown graph ( x ) is incremented by the observed CM level ( line 16 ) ; iv ) for each node x we reset the current observed value of the CM level with the portion of the liquid which has flown from the incoming or the outgoing edges during the propagation ( lines 17 - 18 ) . Depending on the propagation direction , we have two different behaviours for the CM . When exiting a node x through out the outgoing edges ( direction = = DOWN ) we increment the level of contrast medium of the reached nodes by the observed value of x divided by number of outgoing edges of x. By converse , when we climb ( direction = = UP ) across the incoming edges of a node x we increment the CM level of the reached node by the observed CM quantity of x divided by the number of incoming edges of x. Note that the sequence UP / DOWN / UP and the specular DOWN / UP / DOWN are the only ones from the 8 possible combinations which can guarantee the contrast medium to flow on the entire graph . We simply selected the first sequence since the final rank places candidate root nodes on the top ( and candidate leaf nodes on the bottom ) . 4 ) Pruning Cf . Figure 1 , block 4 and Algorithm 1 , lines 9 . Finally , we create a clean taxonomy T by pruning the graph T on the basis of the contrast levels found in C T . CM levels in C T can be used to induce a order of the nodes that , intuitively , captures the level of conceptual abstraction for the nodes in T . We use them to produce a clean taxonomy as follows . We first sort the nodes v V T in a list S = s 0 , s 1 , . . . , s | V T | \u22121 by the decreasing resulting CM level value in C T . The nodes with a higher level of contrast medium are candidates to be at the top level while the ones at the end of the list are candidates to be leaf nodes of the output taxonomy . Next , the pruning routine starts from a graph T = ( V T = V T , E T = ) and for each node s S ( from the last node to the first ) add to E T all the edges of the kind e = ( y , s ) where a path from y to s does not exists in T and with y belonging to one of the following : i ) the set of peers { x S s.", "entities": [[799, 800, "DatasetName", "0"], [1349, 1350, "DatasetName", "converse"], [1557, 1558, "DatasetName", "0"]]}
{"text": "We perform two sets of experiments . We first evaluate our approach when applied to large , automatically induced noisy knowledge graphs ( Section 4.1 ) and then quantify the impact it can have to further improve the quality of the output of state - ofthe - art taxonomy induction systems ( Section 4.2 ) .", "entities": [[20, 22, "TaskName", "knowledge graphs"]]}
{"text": "We first apply ContrastMedium to a variety of knowledge graphs that have been automatically acquired and linked to reference KBs like Word - Net and BabelNet using unsupervised methods ( Section 3.2 ) . Our research questions ( RQs ) are : RQ2 What is the quality of the resulting taxonomies ?", "entities": [[8, 10, "TaskName", "knowledge graphs"]]}
{"text": "We apply our pruning algorithm to the automatically acquired KBs presented by . These noisy knowledge graphs have been induced from large text corpora and include both taxonomic and other ( i.e. , related , topically associative ) semantic relations ( cf . Table 1 ) , as well as automatically induced mappings to lexical knowledge bases like WordNet and BabelNet . These NKGs have been induced from a 100 million sentence news corpus ( news ) and from a 35 million sentence Wikipedia corpus ( wiki ) , using different parameter values to generate sense inventories of different granularities ( e.g. , 1.8 vs. 6.0 average senses per term for the wiki - p1.8 and wiki - p6.0 datasets , respectively ) . Table 2 : Dimensions of the four datasets adopted as linked noisy knowledge graphs . the dimensions for each of the four NKGs - number of senses , average and maximum sense polysemy , number and average hypernyms per sense , the number of linked senses to WordNet concepts ( i.e. , \" links \" ) , and the number of nodes and edges for the corresponding hypernymy graph . Since our algorithm primarily focuses on conceptual hierarchical ( taxonomic ) structures - referred to as the TBox in Knowledge Representationwe use the WordNet mappings only , since the manual inspection of the BabelNet mappings revealed that they are focused primarily on instances ( that is , ABox statements ) . In order to have a complete quintuple for each NKG , we selected , for the companion KB , the top KB root concept entity of the WordNet taxonomy ( SynsetID SID - 00001740 - N ) .", "entities": [[15, 17, "TaskName", "knowledge graphs"], [136, 138, "TaskName", "knowledge graphs"], [276, 277, "DatasetName", "SID"]]}
{"text": "We use the benchmark data from the SemEval - 15 task 17 \" Taxonomy Extraction Evaluation : TExEval \" ( Bordea et al , 2015 ) , since it provides us with gold - standard datasets and system outputs within a standard , easy - to - reproduce setting . Initially , we select from the participating systems 4 the two best performing taxonomies based on the Cumula - tive Fowlkes&Mallows ( CF&M ) measure ( Velardi et al , 2012 ) , the Equipments and Sciences taxonomies from the INRIASAC and the LT3 teams respectively . We next apply our approach to these taxonomies , in order to clean them in a postprocessing fashion . By selecting the top - systems we can see how far we can advance the state - ofthe - art overall . Besides , these two taxonomies are also the ones containing the highest number of cycles , giving the application of our cleaning algorithm a more challenging ( and meaningful ) setting . To remove the effects of automatic linking and quantify the amount of manual efforts needed by our approach , 10 random concepts from each of these resources are manually linked to Word - Net , and the taxonomies subsequently pruned using ContrastMedium and the baseline . We then evaluate performance following the task 's experimental setting and compute the CF&M measure for different levels of manually - created KB links .", "entities": [[2, 4, "DatasetName", "the benchmark"]]}
{"text": "In this paper , we presented ContrastMedium , a novel algorithm that can be applied to automatically linked noisy knowledge graphs to provide an end - to - end solution for fully unsupervised taxonomy induction from scratch , i.e. , without any human effort . Our results indicate that Con - trastMedium can be successfully applied to a wide range of automatically acquired KBs , ranging from large linked noisy knowledge graphs all the way to small - scale induced taxonomies to produce high - quality isa hierarchies that achieve state - ofthe - art results on SemEval benchmarks . As future work , we plan to improve the scalability of the algorithm , in particular its time complexity order , and apply it to Web - scale resources like the WebIsaDB ( Seitner et al , 2016 ) or state - of - the - art approaches like TAXI , as well as to publicly release the created resources .", "entities": [[19, 21, "TaskName", "knowledge graphs"], [70, 72, "TaskName", "knowledge graphs"]]}
{"text": "Recently , some researchers have tackled argumentation synthesis statistically with neural networks . For instance , Wang and Ling ( 2016 ) employed a sequence - to - sequence model to generate summaries of argumentative texts , and Hua and Wang ( 2018 ) did similar to generate counterarguments . Using neural methods in text generation , it is possible to achieve output that is on topic and grammatically ( more or less ) correct . However , when the desired text is to span multiple sentences , the generated text regularly suffers from incoherence and repetitiveness , as for instance discussed by Holtzman et al ( 2018 ) who examine texts that were produced by RNNs in various domains . While these problems may be tolerable to some extent in some applications , such as chatbots , bad text can not be accepted in an argumentative or debating scenario , where the goal is to convince or persuade a reader ( rather than to merely inform or entertain ) . Holtzman et al ( 2018 ) propose to alleviate incoherence and repetitiveness by training a set of discriminators , which aim to ensure that a text respects the Gricean maxims of quantity , quality , relation , and manner ( Grice , 1975 ) . To this end , they employ specific datasets , such as one that opposes authentic text continuation to randomly - sampled text . The discriminators learn optimal weightings for the various models and their combination , such that overall text quality is maximized . For argumentation , we hypothesize that one needs to go even further and eventually account for the author , implementing her underlying intention in the different parts of an argumentative text as well as in the relations between the parts . In the past times of rule - based text generation , argumentation synthesis was a popular task ( Zukerman et al , 2000 ) . Approaches involved much handcrafted ( linguistic and domain ) knowledge and user modeling . For example , the system of Carenini and Moore ( 2006 ) compares attributes of houses ( from a database ) to desired target attributes ( from a user model ) , to then recommend a house to the reader in a convincing text following the Gricean maxims . To this end , it selected house attributes potentially interesting to the user , arranged , and finally phrased them . The resulting texts resembled the arguments we work with here , which have been manually composed by experts ( Wachsmuth et al , 2018 ) from the claims , evidence , and objections in the arg - microtext corpus ( Peldszus and Stede , 2016 ) . To achieve a similar level of output control , today 's text - to - text generation models need to account for the various interdependencies between the text units to be combined . Most related to our approach is the system of , where a user can enter a claimlike topic along with a stance . The system then generates argumentative paragraphs on specific aspects of the topic by selecting sentences from 10 million news texts of the Gigaword corpus . Potentially relevant aspects are those that trigger evaluative judgment in the reader . The sentences are arranged so that the text starts with a claim sentence and is followed by support sentences , employing the approach of . The support sentences are ordered by maximizing the semantic connectivity between sentences . Finally , some rephrasing is done in terms of certain aspects of surface realization . In a manual evaluation , however , no text was seen as sounding natural , underlining the difficulty of the task . In contrast to , we learn directly from input data what argumentative discourse units to combine and how to arrange them . We leave surface realization aside to keep the focus on the argument composition .", "entities": [[54, 56, "TaskName", "text generation"], [309, 311, "TaskName", "text generation"], [472, 474, "TaskName", "text generation"]]}
{"text": "Thesis t1 German universities should on no account charge tuition fees t2 the universities in Germany should not under any circumstances charge tuition fees t3 tuition fees should not generally be charged by universities t4 universities should not charge tuition fees in Germany Con c1 one could argue that an increase in tuition fees would allow institutions to be better equipped c2 those who study later decide this early on , anyway c3 to oblige non - academics to finance others ' degrees through taxes is not just c4 unfortunately sponsoring can lead to disagreeable dependencies in some cases Pro p1 education and training are fundamental rights which the state , the society must provide p2 education must not be a question of money in a wealthy society such as Germany p3 fees result in longer durations of studies p4 funding - wise it ought to be considered how costs incurred by students from other ( federal ) states can be reimbursed p5 if a university lacks the funds , sponsors must be found p6 longer durations of studies are costly p7 studying and taking higher degrees must remain a basic right for everyone p8 there are other instruments to motivate tighter discipline while studying p9 this would impede or prevent access to those who are financially weaker p10 this would mean that only those people with wealthy parents or a previous education and a part - time job while studying would be able to apply for a degree programme in the first place p11 universities are for all citizens , independent of their finances p12 what is the good of a wonderfully outfitted university if it does n't actually allow the majority of clever people to broaden their horizons with all that great equipment Topic Should all universities in Germany charge tuition fees ? Stance Con Table 1 : The candidate thesis , con , and pro units for one topic - stance pair in the dataset of Wachsmuth et al ( 2018 ) . Some other approaches have been proposed that recompose existing text segments in new arguments . In particular , Bilu and Slonim ( 2016 ) generated new claims by \" recycling \" topics and predicates that were found in a database of claims . Claim selection involves preferring predicates that are generally amenable to claim units and that are relevant for the target topic . Egan et al ( 2016 ) created summaries of the main points in a debate , and synthesized complete arguments from a set of manually curated topic - stance relations based on the fine - grained argument model of Toulmin ( 1958 ) . However , we are not aware of any approach that synthesizes arguments fully automatically , let al ne that follows rhetorical considerations in the synthesis process .", "entities": [[72, 73, "DatasetName", "c3"], [88, 89, "DatasetName", "c4"], [131, 132, "DatasetName", "p3"]]}
{"text": "Logos - oriented c1 one could argue that an increase in tuition fees would allow institutions to be better equipped , t1 however German universities should on no account charge tuition fees . p1 education and training are fundamental rights which the state , the society must provide , p12 because what is the good of a wonderfully outfitted university if it does n't actually allow the majority of clever people to broaden their horizons with all that great equipment . p4 Besides , funding - wise it ought to be considered how costs incurred by students from other ( federal ) states can be reimbursed . Pathos - oriented p1 education and training are fundamental rights which the state , the society must provide . t2 This is why the universities in Germany should not under any circumstances charge tuition fees . c1 one could argue that an increase in tuition fees would allow institutions to be better equipped , p3 however fees result in longer durations of studies p6 and longer durations of studies are costly . Wachsmuth et al ( 2018 ) . The italiced connectives were added by the participants ; they are not part of the ADUs . pathos - oriented argumentative texts . We use these 260 texts to develop and evaluate our computational model for argumentation synthesis .", "entities": [[161, 162, "DatasetName", "p3"]]}
{"text": "We start from a training set of ADUs for a set of m topic - stance pairs . To generalize the language model beyond the covered topics , each ADU is represented using features that aim to capture general emotion - related and logic - related characteristics , accounting for the two given strategies . In particular , we first cluster the pool of all training ADUs based on their feature representation . As a result , each ADU is represented by a cluster label ( A - F in Figure 2 ) , where each label represents one ADU type . Now , for each of the strategies , we map each manually - generated sequence of ADUs to a sequence of cluster labels . Using these sequences of labels , we train one separated selection language model for each strategy . For clustering , we rely on topic - independent features that we expect to implicitly encode logical and emotional strategies : ( 1 ) psychological meaningfulness ( Pennebaker et al , 2015 ) , ( 2 ) eight basic emotions ( Plutchik , 1980 ; Mohammad and Turney , 2013 ) , and ( 3 ) argumentativeness ( Somasundaran et al , 2007 ) . In the following , we elaborate on the concrete features that we extract : The input is a corpus of argumentative texts for m topic - stance pairs , each decomposed into a sequence of theses , con units , and pro units . Initially , the set of all these ADUs is clustered to obtain a set topic - independent ADU types , called A - F here . ( 1 ) Selection language model : Each argument is converted from a sequence of ADUs to a sequence of ADU types , where a language model is trained on these type sequences . ( 2 ) Arrangement language model : Each argument is converted from a sequence of ADUs to a sequence of ADU roles ( thesis , pro , and con ) where a language model is trained on these ADU role sequences . ( 3 ) Phrasing regression model : A linear regression model is trained which scores each ADU sequence with respect to its semantic coherence .", "entities": [[39, 40, "DatasetName", "emotion"], [362, 364, "MethodName", "linear regression"]]}
{"text": "LIWC is a lexicon - based text analysis that counts words in psychologically meaningful categories ( Tausczik and Pennebaker , 2010 ) . We use the version by Pennebaker et al ( 2015 ) , which contains the following 15 dimensions : 1 . Language metrics , e.g. , words per sentence . 2 . Function words , e.g. , pronouns and auxiliary verbs . 3 . Other grammar , e.g. , common verbs and comparisons . 4 . Affect words , e.g. , positive emotion words . 5 . Social words , e.g. , \" family \" and \" friends \" . 6 . Cognitive processes , e.g. , \" discrepancies \" and \" certainty \" . 7 . Perceptual processes , e.g. , \" feeling \" . 8 . Biological processes , e.g. , \" health \" . 9 . Core drives and needs , e.g. , \" power \" and \" reward focused \" . 10 . Time orientation , e.g. , past - focused . 11 . Relativity , e.g. , \" time \" and \" space \" . 12 . Personal concerns , e.g. , \" work \" and \" leisure \" . 13 . Informal speech , e.g. , fillers and nonfluencies . 14 . Punctuation , e.g. , periods and commas . 15 . Summary variables , as detailed below . There are four summary variables , each of which is derived from various LIWC dimensions : ( 1 ) analytical thinking ( Pennebaker et al , 2014 ) , i.e. , the degree to which people use narrative language ( low value ) , or more logical and formal language ( high ) ; ( 2 ) clout ( Kacewicz et al , 2014 ) , i.e. , the relative social status , confidence , and leadership displayed in a text ; ( 3 ) authenticity ( Newman et al , 2003 ) , i.e. , the degree to which people reveal themselves in an authentic way ; and ( 4 ) emotional tone ( Cohn et al , 2004 ) , i.e. , negative for values lower than 50 and positive otherwise .", "entities": [[85, 86, "DatasetName", "emotion"]]}
{"text": "For each argument , we opt for a feature representation that embeds the content properties of ADUs in order to capture their content relationship . Concretely , we represent each argument by calculating the semantic similarities of each adjacent bigram in a human - generated argument . We train a linear regression model where each instance represents the features of one argument . To this end , we set a score to be the sum of the probabilities of ADU bigrams occurring in one argument . The phrasing model scores each of the filtered arguments given as output by the arrangement model . The argument with the highest score is the final generated argument .", "entities": [[50, 52, "MethodName", "linear regression"]]}
{"text": "universities should not charge tuition fees in Germany . c3 to oblige non - academics to finance others ' degrees through taxes is not just . p9 this would impede or prevent access to those who are financially weaker . p5 if a university lacks the funds , sponsors must be found . p8 there are other instruments to motivate tighter discipline while studying .", "entities": [[9, 10, "DatasetName", "c3"]]}
{"text": "p2 education must not be a question of money in a wealthy society such as Germany . c1 one could argue that an increase in tuition fees would allow institutions to be better equipped . p7 studying and taking higher degrees must remain a basic right for everyone . p6 longer durations of studies are costly . t2 the universities in Germany should not under any circumstances charge tuition fees . Table 6 : Comparison of two con arguments computationally synthesized with our model for the topic Should all universities in Germany charge tuition fees ? , each being a sequence of five ADUs . A logos - oriented argument ( t 4 , c 3 , p 9 , p 5 , p 8 ) and a pathos - oriented argument ( p 2 , c 1 , p 7 , p 6 , t 2 ) . The thesis of each argument is marked bold . and by the baseline , with and without considering the ordering of ADUs . Our models outperform the baseline for 1 - grams and 2 - grams in all cases . For sequential 3 - grams , however , it did not achieve any overlap with the human - generated arguments for either strategy . This may be explained by the fact that the employed selection and phrasing models are based on 2 - grams only . For n \u2265 2 , the synthesis generally does not work well anymore . We believe that the small data size is a main cause behind this , although it may also point to the limitation of composing ADUs based on surface features . In the non - sequential case , though , our model performs comparably well for 3 - grams , and it even manages to correctly synthesize some ADU 4 - grams . In Table 6 , we exemplify the top - scored arguments for one topic - stance pair , synthesized by our approach for logos and for pathos respectively . They indicate that our model was able to learn strategy - specific differences . 6 In particular , the logos argument starts with the thesis ( t 2 ) , as argumentation guidelines suggest . It then reasons based on consequences and alternatives . Matching intu - ition , the pathos argument appeals more to emotion , reflected in phrases such as \" wealthy society \" and \" under any circumstances \" . Particularly the thesis ( t 4 ) has a more intense tonality than t 2 , and putting it at the end creates additional emphasis .", "entities": [[395, 396, "DatasetName", "emotion"]]}
{"text": "Natural language inference ( NLI ) has been widely used as a task to train and evaluate models for language understanding . However , the ability of NLI models to perform inferences requiring understanding of figurative language such as idioms and metaphors remains understudied . We introduce the IMPLI ( Idiomatic and Metaphoric Paired Language Inference ) dataset , an English dataset consisting of paired sentences spanning idioms and metaphors . We develop novel methods to generate 24k semiautomatic pairs as well as manually creating 1.8k gold pairs . We use IMPLI to evaluate NLI models based on RoBERTa fine - tuned on the widely used MNLI dataset . We then show that while they can reliably detect entailment relationship between figurative phrases with their literal counterparts , they perform poorly on similarly structured examples where pairs are designed to be non - entailing . This suggests the limits of current NLI models with regard to understanding figurative language and this dataset serves as a benchmark for future improvements in this direction . 1 * The work was done while the second author was still affiliated with the UKP Lab at TU Darmstadt .", "entities": [[0, 3, "TaskName", "Natural language inference"], [98, 99, "MethodName", "RoBERTa"], [106, 107, "DatasetName", "MNLI"], [188, 189, "DatasetName", "UKP"]]}
{"text": "Understanding figurative language ( i.e. , that in which the intended meaning of the utterance differs from the literal compositional meaning ) is a particularly difficult area in NLP ( Shutova , 2011 ; Veale et al , 2016 ) , but is essential for proper natural language understanding . We consider here two types of figurative language : idioms and metaphors . Idioms can be viewed as non - compositional multiword expressions ( Jochim et al , 2018 ) , and have been historically difficult for NLP systems . For instance , sentiment systems struggle with multiword expressions in which individual words do not directly contribute to the sentiment ( Sag et al , 2002 ) .", "entities": [[46, 49, "TaskName", "natural language understanding"], [111, 112, "MethodName", "Sag"]]}
{"text": "Figurative language includes idioms , metaphors , metonymy , hyperbole , and more . Critically , figurative language is that in which speaker meaning ( what the speaker intends to accomplish through an utterance ) differs from the literal meaning of that utterance . This leads to problems in NLP systems if they are trained mostly on literal data , as their representations for particular words and/or phrases will not reflect their figurative intended meanings . Figurative language has a significant impact on many NLP tasks . Metaphoric understanding has been shown to be necessary for proper machine translation ( Mao et al , 2018 ; Mohammad et al , 2016 ) . Sentiment analysis also relies critically on figurative language : irony and sarcasm can reverse the polarity of a sentence , while metaphors and idioms may make more subtle changes in the speaker meaning ( Ghosh et al , 2015 ) . Political discourse tasks including bias , misinformation , and political framing detection benefit from joint learning with metaphoricity ( Huguet Cabot et al , 2020 ) . Figurative language engendered by creativity on social media also poses difficulty for many NLP tasks including identifying depression symptoms ( Yadav et al , 2020 ; Iyer et al , 2019 ) and hate speech detection ( Lemmens et al , 2021 ) . We are here focused on idioms and metaphors . There is currently a gap in diagnostic datasets for idioms , and our work fills this gap . There exist some relevant metaphoric resources ( see 2.2 ) ; metaphors are known to be extremely common and important to understanding figurative language , our resource serves to build upon this work .", "entities": [[97, 99, "TaskName", "machine translation"], [113, 115, "TaskName", "Sentiment analysis"], [214, 217, "TaskName", "hate speech detection"]]}
{"text": "To build idiomatic pairs , we use three corpora that contain sentences with idiomatic expressions ( IEs ) labelled as either figurative or literal . 2 These are the MAGPIE Corpus ( Haagsma et al , 2020 ) , the PIE Corpus ( Adewumi et al , 2021 ) , and the SemEval 2013 Task 5 ( Korkontzelos et al , 2013 ) . We collect the total set of IEs that are present in these corpora . We then extract definitions for these using freely available online idiom dictionaries . 3 These definitions are often faulty , incomplete , or improperly formatted . We employed annotators to make manual corrections . The annotators were given the original IE as well as the definition extracted from the dictionary . The annotators were asked to ensure that the dictionary definition given was ( 1 ) a correct literal interpretation and ( 2 ) fit syntactically in the same environments as the original IE . If the definition met both of these criteria , the IE can be replaced by its definition to yield an entailment pair . If either criterion was not met , annotators were asked to minimally update the definition so that it satisfied the requirements . In total this process yielded 697 IE definitions . We then used the above corpora , replacing these definitions into the original sentences ( see Figure 1 ) . We use the figurative / literal labels from the as right as rain original corpora : replacing them into figurative contexts yields entailment relations , while replacing them into contexts where the phrase is meant literally then yields non - entailments .", "entities": [[40, 41, "DatasetName", "PIE"], [52, 54, "DatasetName", "SemEval 2013"]]}
{"text": "Figure 3 : Metaphor entailment generation . Pairs are generated using annotator - defined literal translations substituted into metaphoric contexts . 2 . hard truth unpleasant truth 3 . hairy problem difficult problem These can then be replaced in a similar fashion : we start with the original figurative sentence , replace the ME with the literal replacements , and the result is an entailing pair with the metaphoric sentence entailing the literal . We apply this procedure to the dataset of Tsvetkov et al ( 2014 ) , yielding 100 metaphoric / literal NLI entailment pairs . We then take a portion of the Common Crawl dataset 4 , and identify sentences that contain these original MEs . We identify sentences that contain the words from the metaphoric phrase , and replace the metaphoric word itself with its literal counterpart . This yields 645 additional silver pairs .", "entities": [[105, 107, "DatasetName", "Common Crawl"]]}
{"text": "For all silver methods , we also employ syntactic postprocessing to overcome a number of hurdles . First , phrases used idiomatically often follow different syntactic patterns than when used literally . Original : These point out of this world , but where to is not made clear . Replaced : * These point wonderful , but where to is not made clear . This phrase in literal contexts functions syntactically as a prepositional phrase , while idiomatically it is used as an adjective . When replaced with the definition \" wonderful \" in a literal context , we get a grammatically incoherent sentence . Second , phrases in their literal usage often do not form full constituents , due to the string - matching approach of the original datasets . Many literal usages of these phrases are thus incompatible with the defined replacement . I think [ this one has to die ] for the other one to live . Turn in [ the raw edges ] of both seam allowances towards each other and match the folded edges . To avoid these issues , we ran syntactic parsing on the definition and the expression within each context , requiring that the expression in context begins with the same part of speech as the definition and that it does not end inside of another phrase . Additionally , for each replacement , we ensured that the verb conjugation matched the context . For this , we identified the conjugation in the context , and used a de - lemmatization script to conjugate the replacement verb to match the original .", "entities": [[259, 260, "TaskName", "lemmatization"]]}
{"text": "In order for these automatically created pairs to be useful for NLI - based evaluation , they need to be of sufficiently high quality . As the annotators were generating novel definitions and pairs , rather than inter - annotator agreement , we instead evaluate the quality of the resulting pairs by testing whether the automatically generated pairs contained the appropriate entailment relation . For this task , each annotator was given 100 samples for each general category of silver generations ( idiomatic entailments , idiomatic non - entailments , and metaphoric entailments ) . They were asked if the entailment relation between the two sentences was as expected . An expert than adjudicated disagreements to determine the final percentage of valid pairs . To evaluate the syntactic validity of the generated pairs , we additionally ran the Stanford PCFG dependency parser ( Klein and Manning , 2003 ) the pairs . Per previous work in NLI ( Williams et al , 2018 ) , we evaluate the proportion of sentences for which the root node is S. Table 4 shows the results . The semi - supervised examples evoked the correct entailment relation between % 88 and % 97 of the time : while there is still noise present , this indicates the effectiveness of the proposed methods . With regard to syntax , we see S node roots for between 82 % and % 90 of the sentences : within the range of the SNLI performance ( 74 % - 88 % ) , and slightly behind the MNLI ( 91 % - 98 % ) . We find that the generated hypotheses are not significantly different in quality than the premises . This indicates that the method for generation preserves the original syntax . These methods allow us to quickly generate a substantial number of high - quality pairs to evaluate NLI systems on figurative language . However , they may introduce additional bias as we employ a number of restrictions in order to ensure syntactic and semantic compatibility , and we lack full nonentailment pairs for metaphoric data . We therefore expand our dataset with manually generated pairs .", "entities": [[246, 247, "DatasetName", "SNLI"], [260, 261, "DatasetName", "MNLI"]]}
{"text": "Using the IMPLI dataset , we aim to answer a series of questions via NLI pertaining to language models ' ability to understand and represent figurative language accurately . These questions are : Our dataset provides unique advantages in addressing these research questions that cover gaps in previous work : it contains a large number of both entailments and non - entailments and is large enough to be used for training the models . Idioms Metaphors Model MNLI MNLI - MM S S l S d G G a G S G G roberta -", "entities": [[77, 78, "DatasetName", "MNLI"], [78, 81, "DatasetName", "MNLI - MM"]]}
{"text": "To evaluate incorporating idioms into training , we then split the idiom data by idiomatic phrase types , keeping a set of IEs separate as test data to assess whether the model can learn to correctly handle novel , unseen phrases . Our goal is to assess whether poor performance is due to models ' not containing these expressions in training , or because their ability to represent figurative language inherently limited . We hypothesize that the noncompositional nature of these types of figuration should lead to poor performance on unseen phrases , even if the model is trained on other idiomatic data . For each task , we split the data into 10 folds by IE and incrementally incorporate these folds into the original MNLI for training , leaving one fold out for testing . We experiment with incorporating all training data for both labels , as well as using only entailment or non - entailment samples . We then evaluate our results on the entire test set , as well as the entailment and non - entailment partitions . Figure 4 shows the results , highlighting that additional training data yields only small improvements . Pairs with non - entailment relations remain exceedingly difficult , with performance capping out at only slightly better than chance . As hypothesized , additional training data is only somewhat effective in improving language models ' idiomatic capabilities ; this is not sufficient to overcome difficulties from literal usages of idiomatic phrases and adversarial definitions , indicating that idiomatic language remains difficult for pre - trained language models to learn to represent . R3 : Syntactic Flexibility Finally , we assess models ' representation of idiomatic compositionality . Nunberg et al ( 1994 ) indicate that there are two general types of idioms : \" idiomatic phrases \" , which exhibit limited flexibility and generally occur only in a single surface form , and \" idiomatically combining expressions \" or ICEs , in which the constituent elements of the idiom carry semantic meaning which can influence their syntactic properties , allowing them to be more syntactically flexible . For example , in the idiom spill the beans , we can map the spilling activity to divulging of information , and the beans to the information . Because this expression has semantic mappings to figurative meaning for its syntactic constituents , Nunberg et al ( 1994 ) argue that it can be more syntactically flexible , allowing for expressions like the beans that were spilled by Martha to maintain idiomatic meaning . For fixed expressions such as kick the bucket , no syntactic constituents map directly to the figurative meaning ( \" die \" ) . We then expect less syntactic flexibility , and thus the bucket that was kicked by John loses its idiomatic meaning . We hypothesize that model performance will be correlated with the degree to which a given idiom type is flexible : more fixed expressions may be easier , as they are seen in regular , fixed patterns that the models can memorize , while more flexible ICEs will be more difficult , as they can appear in different patterns , cases , and word order , often even mixing in with other constituents . To test this , we define an ICE score as the percentage of times a phrase occurs in our test data in a form that does not match its original base form . Higher percentages mean the phrase occurs more frequently in a non - standard form , acting as a measure for the syntactic flexibility of the expression . We assessed the performance of the roberta - base model for each idiom type , evaluating Spearman correlations between performance and idioms ' ICE scores . We found no correlation between ICE scores and performance for entailments , nor for adversarial definition non - entailments ( r = .004/.45 , p = .921/.399 , see Appendix C ) . However , we do see a weak but significant correlation ( r = .188 , p = 0.016 ) with non - entailments from literal contexts : the model performs better when the phrases are more flexible , contrary to our initial hypothesis . One possible explanation is that the model memorizes a specific figurative meanings for each fixed expression , disregarding the possibility of these words being used literally . When the expression is used in a literal context , the model then still assumes the figurative meaning , resulting in errors on non - entailment samples . The ICEs are more fluid , and thus the model is less likely to have a concrete representation for the given phrase : it is better able to reason about the context and interacting words within the expression , making it easier to distinguish the entailing and non - entailing samples .", "entities": [[125, 126, "DatasetName", "MNLI"]]}
{"text": "In this work , we introduce the IMPLI dataset , which we then use to evaluate NLI models ' capabilities on figurative language . We show that while widely used MNLI models handle entailment admirably and metaphoric expressions are relatively easy , nonentailment idiomatic relationships are more difficult . Additionally , adding idiom - specific training data fails to alleviate poor performance for nonentailing pairs . This highlights how currently language models are inherently limited in representing some figurative phenomena and can provide a target for future model improvements . For future work , we aim to expand our data collection processes to new data sources . Our dataset creation procedure relies on annotated samples and definitions : as more idiomatic and metaphoric resources become available , this process is broadly extendable to create new figurative / literal pairs . Additionally , we only explore this data for evaluating NLI systems : this data could also be used for other parallel data tasks such as figurative language interpretation ( Shutova , 2013 ; Su et al , 2017 ) and figurative paraphrase generation . As natural language generation often relies on training or fine - tuning models with paired sentences , this data could be a valuable resource for figurative language generation systems .", "entities": [[30, 31, "DatasetName", "MNLI"], [181, 183, "TaskName", "paraphrase generation"]]}
{"text": "Previous research shows that NLI systems exploit cues based on lexical overlap , predicting entailment for overlapping sentences ( McCoy et al , 2019 ; Nie et al , 2019 ) . Our dataset consists mostly of pairs with high overlap : this could explain why the nonentailment sections are more difficult . We thus evaluate system predictions for our datasets as a function of lexical overlap . Figure 5 shows densitybased histograms of the results , comparing overlap via Levenshtein distance ( Levenshtein , 1965 ) for correctly and incorrectly classified pairs . Our data contains higher overlap than the MNLI data , with the bulk of the density falling on minimally distant pairs . We also note a distinct difference between our entailment and non - entailment pairs : non - entailments contain extremely high overlap and are frequently misclassified in these cases where the distance is small , matching previous reports for NLI tasks : lexical overlap is a key artifact for entailment , and this reliance persists when classifying idiomatic pairs .", "entities": [[101, 102, "DatasetName", "MNLI"]]}
{"text": "BioNLP - OST 2019 RDoC Tasks : Multi - grain Neural Relevance Ranking Using Topics and Attention Based Query - Document - Sentence Interactions", "entities": [[2, 3, "DatasetName", "OST"]]}
{"text": "The scientific research output of the biomedical community is becoming more sub - domain specialized and increasing at a faster pace . Most of the biomedical domain knowledge is in the form of unstructured text data . Natural Language Processing ( NLP ) techniques such as relation extraction and information retrieval have enabled us to effectively mine relevant information from a large corpus . These techniques have significantly reduced the time and effort required for knowledge min - * : Equal Contribution ing and information extraction from past scientific studies and electronic health reports ( EHR ) . Information Retrieval ( IR ) is the process of retrieving relevant information from an unstructured text corpus , which satisfies a given query / requirement , for example Google search , email search , database search etc . This is generally achieved by converting the query and the document collection into an external representation which by preserving the important semantical information can reduce the IR processing time . This external representation can be generated using either statistical approach i.e. , word counts or distributed semantical approach i.e. , word embeddings . Therefore , there is a motivation to develop such IR system which can understand the specialized sub - domain language and domainspecific jargon of biomedical domain and assist researchers and medical professionals by effectively and efficiently retrieving most relevant information given a query . RDoC Tasks aims at exploring information retrieval ( IR ) and information extraction ( IE ) tasks on selected abstracts from PubMed dataset . While Task - 1 aims to rank abstracts i.e. , coarse granularity , Task - 2 aims to rank sentences i.e. , fine granularity and hence the term multi - grain . An RDoC construct combines information from multiple sources like genomics , symptoms , behaviour etc . and therefore , is a much broader way of describing mental health disorders than symptoms based approach . Table 1 shows the association between PubMed abstracts and RDoC constructs depending on the semantic knowledge of the highlighted content words . Both of these tasks aim in the direction of ease of accessibility of PubMed abstracts labelled with diverse RDoC constructs so that this information can reach its full potential and can be of help to biomedical researchers and healthcare professionals . ( PMID ) . Highlighted words ( blue and red ) in each abstract shows content words which together provide the semantic understanding of the corresponding RDoC constructs .", "entities": [[46, 48, "TaskName", "relation extraction"], [49, 51, "TaskName", "information retrieval"], [98, 100, "TaskName", "Information Retrieval"], [126, 127, "DatasetName", "Google"], [186, 188, "TaskName", "word embeddings"], [238, 240, "TaskName", "information retrieval"]]}
{"text": "The Linguistic Annotation Workshop ( LAW ) is organized annually by the Association for Computational Linguistics ' Special Interest Group for Annotation ( ACL SIGANN ) . It provides a forum to facilitate the exchange and propagation of research results concerned with the annotation , manipulation , and exploitation of corpora ; work towards harmonisation and interoperability from the perspective of the increasingly large number of tools and frameworks for annotated language resources ; and work towards a consensus on all issues crucial to the advancement of the field of corpus annotation . These proceedings include papers that were presented at LAW XIII , held in conjunction with the annual meeting of the Association for Computational Linguistics ( ACL ) in Florence , Italy , on August 1 , 2019 . The series is now in its thirteenth year . The first workshop took place in 2007 at the ACL in Prague . Since then , the LAW has been held every year , consistently drawing substantial participation ( both in terms of paper / poster submissions and participation in the actual workshop ) providing evidence that the LAW 's overall focus continues to be an important area of interest in the field . This year 's LAW has received 52 submissions , out of which 28 papers have been accepted to be presented at the workshop , 10 as talks and 18 as posters . In addition to oral and poster paper presentations , LAW XIII also features an invited talk by Rebecca Passonneau and a discussion session . Our thanks go to SIGANN , our organizing committee , for its continuing organization of the LAW workshops , and to the ACL 2019 workshop chairs for their support . Also , we thank Jet Hoek , the LAW XIII publications chair , for her invaluable help with these proceedings . Most of all , we would like to thank all the authors for submitting their papers to the workshop , and our program committee members for their dedication and their thoughtful reviews .", "entities": [[5, 6, "DatasetName", "LAW"], [101, 102, "DatasetName", "LAW"], [121, 122, "MethodName", "Florence"], [157, 158, "DatasetName", "LAW"], [188, 189, "DatasetName", "LAW"], [207, 208, "DatasetName", "LAW"], [245, 246, "DatasetName", "LAW"], [276, 277, "DatasetName", "LAW"], [298, 299, "DatasetName", "LAW"]]}
{"text": "At a high level , the four components of a Hybrid Code Network are a recurrent neural network ; domain - specific software ; domain - specific action templates ; and a conventional entity extraction module for identifying entity mentions in text . Both the RNN and the developer code maintain state . Each action template can be a textual communicative action or an API call . The HCN model is summarized in Figure 1 . The cycle begins when the user provides an utterance , as text ( step 1 ) . The utterance is featurized in several ways . First , a bag of words vector is formed ( step 2 ) . Second , an utterance embedding is formed , using a pre - built utterance embedding model ( step 3 ) . Third , an entity extraction module identifies entity mentions ( step 4 ) - for example , identifying \" Jennifer Jones \" as a < name > entity . The text and entity mentions are then passed to \" Entity tracking \" code provided by the developer ( step 5 ) , which grounds and maintains entitiesfor example , mapping the text \" Jennifer Jones \" to a specific row in a database . This code can optionally return an \" action mask \" , indicating actions which are permitted at the current timestep , as a bit vector . For example , if a target phone number has not yet been identified , the API action to place a phone call may be masked . It can also optionally return \" context features \" which are features the developer thinks will be useful for distinguish - ing among actions , such as which entities are currently present and which are absent . The feature components from steps 1 - 5 are concatenated to form a feature vector ( step 6 ) . This vector is passed to an RNN , such as a long shortterm memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) or gated recurrent unit ( GRU ) ( Chung et al , 2014 ) . The RNN computes a hidden state ( vector ) , which is retained for the next timestep ( step 8 ) , and passed to a dense layer with a softmax activation , with output dimension equal to the number of distinct system action templates ( step 9 ) . 1 Thus the output of step 9 is a distribution over action templates . Next , the action mask is applied as an element - wise multiplication , and the result is normalized back to a probability distribution ( step 10 ) - this forces non - permitted actions to take on probability zero . From the resulting distribution ( step 11 ) , an action is selected ( step 12 ) . When RL is active , exploration is required , so in this case an action is sampled from the distribution ; when RL is not active , the best action should be chosen , and so the action with the highest probability is always selected . The selected action is next passed to \" Entity output \" developer code that can substitute in entities ( step 13 ) and produce a fully - formed action - for example , mapping the template \" < city > , right ? \" to \" Seattle , right ? \" . In step 14 , control branches depending on the type of the action : if it is an API action , the corresponding API call in the developer code is invoked ( step 15 ) - for example , to render rich content to the user . APIs can act as sensors and return features relevant to the dialog , so these can be added to the feature vector in the next timestep ( step 16 ) . If the action is text , it is rendered to the user ( step 17 ) , and cycle then repeats . The action taken is provided as a feature to the RNN in the next timestep ( step 18 ) .", "entities": [[334, 335, "MethodName", "LSTM"], [344, 347, "MethodName", "gated recurrent unit"], [348, 349, "MethodName", "GRU"], [388, 389, "MethodName", "softmax"]]}
{"text": "This paper has introduced Hybrid Code Networks for end - to - end learning of task - oriented dialog systems . HCNs support a separation of concerns where procedural knowledge and constraints can be expressed in software , and the control flow is learned . Compared to existing end - to - end approaches , HCNs afford more developer control and require less training data , at the expense of a small amount of developer effort . Results in this paper have explored three different dialog domains . On a public benchmark in the restaurants domain , HCNs exceeded performance of purely learned models . Results in two troubleshooting domains exceeded performance of a commercially deployed rule - based system . Finally , in a name - dialing domain , results from dialog simulation show that HCNs can also be optimized with a mixture of reinforcement and supervised learning . In future work , we plan to extend HCNs by incorporating lines of existing work , such as integrating the entity extraction step into the neural network ( Dhingra et al , 2017 ) , adding richer utterance embeddings ( Socher et al , 2013 ) , and supporting text generation ( Sordoni et al , 2015 ) . We will also explore using HCNs with automatic speech recognition ( ASR ) input , for example by forming features from n - grams of the ASR n - best results ( Henderson et al , 2014b ) . Of course , we also plan to deploy the model in a live dialog system . More broadly , HCNs are a general model for stateful control , and we would be interested to explore applications beyond dialog systemsfor example , in NLP medical settings or humanrobot NL interaction tasks , providing domain constraints are important for safety ; and in resourcepoor settings , providing domain knowledge can amplify limited data .", "entities": [[199, 201, "TaskName", "text generation"], [216, 219, "TaskName", "automatic speech recognition"]]}
{"text": "Discourse Self - Attention for Discourse Element Identification in Argumentative Student Essays", "entities": [[11, 12, "DatasetName", "Essays"]]}
{"text": "Discourse describes how a document is organized . This paper focuses on the task of discourse element identification ( DEI ) in argumentative student essays . Discourse elements represent the function and contribution of every discourse unit to the discourse . Burstein et al ( 2003 ) formulate discourse elements as 5 categories : introduction , thesis , main idea , supporting and conclusion , while argument components such as major claim , claim and premise are used as discourse elements in argumentation structure parsing in persuasive essays ( Stab and Gurevych , 2014 ) . DEI can benefit automated essay scoring in many aspects : modeling organization , inferring topics and opinions or used as features for scoring systems ( Attali and Burstein , 2006 ; Burstein et al , 2001 ; Persing et al , 2010 ; Song et al , 2020 ) . Despite its importance , DEI is challenging . First , the ambiguity of sentences makes learning models difficult to distinguish some discourse elements . For example , the thesis is defined as expressing the central claim of the author and the main ideas support the thesis from specific aspects . However , it is hard to distinguish them from their content and style . Second , the discourse element of a specific sentence depends on context . As a result , considering individual sentences only would have difficulties in identifying discourse elements . The relations and relatedness among multiple sentences should be explored . Third , the data imbalance problem is serious , e.g. , the number of elaboration sentences could be 10 times more than the number of thesis sentences . The minority discourse elements ( such as thesis , main ideas or major claim ) are harder to be recalled although they have important roles in many scenarios , e.g. , evaluating the organization of essays ( Attali and Burstein , 2006 ) . In this paper , we propose a method to explicitly model sentence positions and relations to improve discourse element identification in argumentative student essays . Our idea is partially motivated by the self - attention mechanism such as ( Vaswani et al , 2017 ) . Self - attention is usually applied to capture dependencies between words . We aim to apply self - attention mechanism to describe relations between sentences . On one hand , position information is important for DEI to give clues on discourse elements beyond content and style , because authors usually hold some conventions to organize content . Position is one of the most useful feature classes in feature - based DEI ( Burstein et al , 2003 ; Stab and Gurevych , 2014 ) . Previous neural network models usually cast DEI as a classification or sequence labeling task and do not explicitly model position information . Motivated by the positional encoding of words , we propose a simple structural positional encoding strategy for a sentence by considering its relative position in its essay , relative position of its paragraph in its essay , and its relative position within its paragraph . On the other hand , relatedness among sentences may also indicate properties of discourse elements . For example , thesis sentences should have close relations to the whole essay ; main ideas usually locate in similar positions and have high relatedness . Relatedness between discourse elements has shown to be an important indicator of essay coherence ( Higgins et al , 2004 ) . We compute inter - sentence attention vectors to represent either element - wise or content - wise relations to other sentences , which bring in additional information beyond individual sentences and enhance sentence representation without extra information . Experiments show that the proposed approach can get considerable improvements compared with feature - based and neural network based baselines on a Chinese dataset and obtain competitive results compared with the state - of - the - art method on an English dataset . The structural positional encodings of sentences show effectiveness to achieve obvious overall improvements . The inter - sentence attention vectors enhance sentence representation helping identify discourse elements as well .", "entities": [[99, 102, "TaskName", "automated essay scoring"]]}
{"text": "DEI could be seen as a subtask in discourse structure analysis . It aims to identify discourse elements , determine their functions and establish relationships among them in an argumentative text . Typical tasks in this line include argumentative zoning ( Teufel et al , 1999 ) , argumentation mining ( Mochales and Moens , 2011 ; Lippi and Torroni , 2016 ) and analyzing argumentative student essays ( Burstein et al , 2003 ; Stab and Gurevych , 2014 ) . Argumentative zoning identifies arguments in scientific articles ( Teufel et al , 1999 ; Guo et al , 2010 ) . Argumentation mining aims to identify argument components and relations from legal texts ( Palau and Moens , 2009 ; Mochales and Moens , 2011 ) or argumentative texts ( Stab and Gurevych , 2014 ; . The solutions to these tasks usually adopt similar machine learning methods but use domain related features . The methods could be roughly classified into the following categories . Classification based methods cast DEI as a classification problem . Various classifiers have been tested , such as SVM ( Stab and Gurevych , 2014 ) , decision trees ( Burstein et al , 2003 ( Burstein et al , , 2001 and naive Bayes , maximum entropy model ( Moens et al , 2007 ; Palau and Moens , 2009 ) . Sequence labeling based methods exploit contextual information for DEI with conditional random fields ( Hirohata et al , 2008 ; Song et al , 2015 ) or recurrent neural networks . Establishing relations between sentences is often viewed as a classification tasks as well ( Stab and Gurevych , 2014 ) . Parsing based methods are also adopted to build more complex structures with techniques like ILP or RST style parsing ( Peldszus and Stede , 2015 ) . Feature engineering . Some common features are shared across these tasks , including syntactic , lexical , semantic and discourse relations . There are also domain related features to further boost the performance . Mochales and Moens ( 2011 ) designed special features for argumentation mining in legal texts . Nguyen and Litman ( 2015 ) identified claims based on domain words . Lippi and Torroni ( 2015 ) modeled syntactic structures for content independent claim detection based on tree kernels . Our work is mostly related to DEI in argumentative student essays ( Burstein et al , 2003 ; Stab and Gurevych , 2014 ) , which is useful for qualifying essay organization ( Persing et al , 2010 ) , argumentation ( Persing and Ng , 2016 ; Wachsmuth et al , 2016 ) and general writing ( Burstein et al , 2003 ; Ong et al , 2014 ; Song et al , 2014 ) . The major feature classes proposed by Burstein et al ( 2003 ) and Stab and Gurevych ( 2014 ) are used to build a baseline . The features include : position , cue words , lexical features ( main verbs , adverbs and connectives ) and structural features ( such as number of clauses ) . Some of these features are based on manually collected lexicons . Deep Learning Methods have achieved great success in many NLP tasks . proposed neural argumentation mining models based on sequence tagging or dependency parsing . It exploits inter - sentence relations but needs sophisticated language processing . exploited CNN and LSTM for classifying sentences to identify claims from different domains . It mainly depends on the content of components but does not sufficiently model positions and exploit inter - sentence relatedness .", "entities": [[167, 168, "TaskName", "Classification"], [185, 186, "MethodName", "SVM"], [309, 311, "TaskName", "Feature engineering"], [557, 559, "TaskName", "dependency parsing"], [575, 576, "MethodName", "LSTM"]]}
{"text": "Vaswani et al ( 2017 ) proposed the self - attention mechanism and achieved state of the art results in many NLP tasks . Since then , self - attention has drawn increasing interests due to flexibility in modeling long range interactions . Self - attention ignores word order in a sentence . As a result , position representations are developed to cooperate with self - attention . In addition to the sinusoidal position representation proposed by Vaswani et al ( 2017 ) , there are also other variations to bias the selection of attentive regions ( Shen et al , 2018 ; Shaw et al , 2018 ; Yang et al , 2019 ) . In NLP , self - attention is mostly applied to sequential structures such as a sequence of words . Mihaylov and Frank ( 2019 ) proposed a discourse - aware selfattention encoder for reading comprehension on narrative texts , where event chains , discourse relations and coreference relations are used for connecting sentences . Self - attention can be also extended to 2d - dimensions for image processing ( Parmar et al , 2018 ) and lattice inputs ( Sperber et al , 2019 ) .", "entities": [[149, 151, "TaskName", "reading comprehension"]]}
{"text": "We use Hierarchical BiLSTM ( HBiLSTM ) , which is similar to ( Yang et al , 2016 ) , as the base model to model sentence and discourse level representations . The task is to assign discourse element labels y = ( y 1 , ... , y n ) to sentences ( x 1 , ... , x n ) in a text , where x i , 1 \u2264 i \u2264 n , is a sentence of a sequence of words and y i Y , Y is a set of pre - defined discourse elements .", "entities": [[3, 4, "MethodName", "BiLSTM"]]}
{"text": "A sequence of words x = { w 1 , ... , w N } is modeled with a RNN encoder and is converted into a sequence of hidden states H = { h 1 , ... , h N } . The hidden state at the i - th step is h i = f ( e ( w i ) , h i\u22121 ) , ( 1 ) where f is a RNN unit , e ( w i ) R d is the embedding of a word , and h i\u22121 is the hidden state of the previous step . The whole sequence could be represented as a fixed length vector c = \u03c6 ( { h 1 , , h N } ) to represent the semantic of a sentence , where \u03c6 ( ) is a function to summarize hidden states . In this work , Long Short - Term Memory ( LSTM ) ( Hochreiter and Schmidhuber , 1997 ) is used as the RNN unit and the sequence is encoded in a Bidirectional way that a hidden state h i = [ \u2212 h i ; \u2212 h i ] is the concatenation of the corresponding hidden states from both directions . The summarization function \u03c6 ( ) could be based on the attention mechanism .", "entities": [[151, 156, "MethodName", "Long Short - Term Memory"], [157, 158, "MethodName", "LSTM"], [210, 211, "TaskName", "summarization"]]}
{"text": "In the discourse element layer , we feed sentence representations C = ( c 1 , ... , c n ) R d\u00d7n to a BiL - STM and use a nonlinear layer to map semantic representations to discourse element representations , D = tanh ( BiLSTM ( C ) ) . ( 2 )", "entities": [[46, 47, "MethodName", "BiLSTM"]]}
{"text": "Finally , we use a linear and a softmax layer to predict the discourse element of every sentence , Y = softmax ( linear ( D ) ) , ( 3 ) where Y R | Y | \u00d7n refers to the probabilities of every sentence over discourse element categories . The baseline mainly exploits interactions between adjacent sentences , but long distance interactions and sentence positions are not explicitly considered , which may be also important to determine the function of sentences in argumentative discourse .", "entities": [[8, 9, "MethodName", "softmax"], [21, 22, "MethodName", "softmax"]]}
{"text": "We propose the Discourse Self - Attention ( DiSA ) layer to improve the baseline by explicitly modeling sentence positions and inter - sentence interactions . The architecture is illustrated in Figure 1 . The sentences in an essay are converted to sentence embeddings C through the BiLSTM encoder introduced in Section 3.1 , which are used as the input of DiSA . DiSA explicitly represents sentence positions , which are integrated with the content representations of sentences to get element representations . DiSA also has an inter - sentence attention module to get both element - wise and content - wise attention vectors of sentences to capture sentence interactions . The attention vectors and element representations are concatenated and fed to a linear layer and a softmax layer for prediction .", "entities": [[42, 44, "TaskName", "sentence embeddings"], [47, 48, "MethodName", "BiLSTM"], [123, 125, "MethodName", "linear layer"], [127, 128, "MethodName", "softmax"]]}
{"text": "We also use the English student essay dataset released by . This dataset marks argument components , i.e. , major claim , claim , and premise , at clause level . Table 2 shows an example sentence . The consecutive words in bold form three components , corresponding to claim , major claim and premise , respectively . Because our model is at sentence level , we convert the original annotations to sentence level . First , an essay is split into sentences by NLTK . Then if a sentence contains only one argument component , we annotate this sentence as the type of this component ; if a sentence contains more than one argument component , we further separate it into multiple sentences to ensure that each sentence has only one argument . The beginning of a new sentence is from the end of the last component . The end of a new sentence is the end of the component it contains . As shown in Table 2 , three sentences s 1 , s 2 and s 3 are generated from the original example sentence . If a sentence does not have any argument component , its label is other . Table 3 shows the basic statistics of the converted dataset .", "entities": [[5, 7, "DatasetName", "student essay"]]}
{"text": "We compare with the following systems . Feature - based . We adapt features from previous feature - based methods ( Burstein et al , 2003 ; Stab and Gurevych , 2014 ; Song et al , 2015 ) to build a feature - based CRF model . HBiLSTM . The baseline described in Section 3 uses two BiLSTM layers to encode word sequences and sentences . BERT . We fine - tune BERT on training data to train a sentence classifier , because the lengths of many Chinese essays exceed the length constraint of BERT and it is expensive to train BERT - like models at discourse level .", "entities": [[45, 46, "MethodName", "CRF"], [58, 59, "MethodName", "BiLSTM"], [67, 68, "MethodName", "BERT"], [73, 74, "MethodName", "BERT"], [95, 96, "MethodName", "BERT"], [102, 103, "MethodName", "BERT"]]}
{"text": "Less is More : Attention Supervision with Counterfactuals for Text Classification", "entities": [[9, 11, "TaskName", "Text Classification"]]}
{"text": "We aim to leverage human and machine intelligence together for attention supervision . Specifically , we show that human annotation cost can be kept reasonably low , while its quality can be enhanced by machine selfsupervision . Specifically , for this goal , we explore the advantage of counterfactual reasoning , over associative reasoning typically used in attention supervision . Our empirical results show that this machine - augmented human attention supervision is more effective than existing methods requiring a higher annotation cost , in text classification tasks , including sentiment analysis and news categorization .", "entities": [[85, 87, "TaskName", "text classification"], [90, 92, "TaskName", "sentiment analysis"]]}
{"text": "The practical importance of attention mechanism has been well - established , for both ( a ) improving NLP models ( Vaswani et al , 2017 ) , and also ( b ) enhancing human understanding of models ( Serrano and Smith , 2019 ; Wiegreffe and Pinter , 2019 ) . This paper pursues the former direction , but unlike existing models , typically using attention in \" unsupervised \" nature . Adding human supervision to attention has been shown to improve model predictions and explanations ( Jain and Wallace , 2019 ) . For example , consider a review in ( Tang et al , 2019 ) \" this place is small and crowded but the service is quick \" . Models with unsupervised attention may attend highly on \" quick \" , a generic strong signal for restaurant reviews , but one may supervise to focus on \" crowded \" to guide models to predict a negative sentiment correctly . For this goal , attention supervision task ( Yu et al , 2017 ; Liu et al , 2017 ) treats attention as output variables so that models can be trained to generate similar attention to human supervision . We categorize such human supervision into the following two levels : Sample level rationale : In the above example , whether to attend on quick or crowded depends on the ground - truth sentiment class . Human annotator is required to examine each training sample , and highlight important words specific to a sample and its class label . Task level : An alternative with lower annotation overhead would be annotating vocabulary , separately from training samples . That is , both quick and crowded are annotated to attend , since both have high importance for the target task of sentiment classification . A naive belief would be assuming the former with a higher annotation cost is more effective at supervising the model 's attention . Our key claim , in contrast , is that requiring more annotation , or , sample - specific supervision , can be less effective than requiring less from human then augmenting it by machine ( less - is - more - hypothesis ) . Similar skepticism on asking more , or sample - level rationales from humans , was explored in ( Bao et al , 2018 ) , where machine attention from large additional annotations was more effective supervisions than rationales . In this paper , we validate less - is - more without additional annotation overhead , by proposing a holistic approach of combining both human annotation and machine attention . Key distinctions from ( Bao et al , 2018 ) are ( a ) humans annotate even less , and ( b ) without additional training resources . Specifically , we start by loosening the definition of human annotation ( Camburu et al , 2018 ; Zhong et al , 2019 ) into the task - level annotation : it reduces annotation cost to the size of vocabulary , or often to zero , when public resources such as sentiment lexicon replace such annotation . We show the effectiveness of this zero - cost supervision , for both sentiment classification and news categorization scenarios , after our proposed adaptation . Our adaptation goal is an unsupervised adaptation of task - level human annotation to samplelevel supervision signals for attention / classification models . Specifically , we propose Sample - level AttentioN Adaptation ( SANA ) . Specifically , for self - supervising such adaptation , SANA conducts what - if tests per each sample , of whether the permutation on human annotation changes the machine prediction . That is , we collect the counterfactual ( machine ) supervisions for free , by observing whether highly attended word by human leads to the same machine prediction , compared to when such attention is counterfactually lowered . In such a case , SANA supervises to reduce the importance of the word . We validate such counterfactual signals are missing pieces for adapting word importance to sample - specific prediction . We evaluate SANA on three popular datasets , SST2 , IMDB , and 20NG . In all of the text classification datasets , SANA achieves significant improvements over baselines , using unsupervised attention or supervised with task - or sample - level human annotations , in the following four dimensions : Models supervised by SANA predict more accurately , explain causality of attention better , and are more robust over adversarial attacks , and more tolerant of the scarcity of training samples .", "entities": [[697, 698, "DatasetName", "SST2"], [699, 700, "DatasetName", "IMDB"], [708, 710, "TaskName", "text classification"]]}
{"text": "Sample - level annotation is reportedly too expensive in many practical settings ( Zhong et al , 2019 ) , and is far difficult for humans to capture the dependency with corresponding class labels . In contrast , annotators may select important words for a target task , namely task - level attention annotation ( Def . 3.1 ) , without looking up individual samples and their labels . Definition 3.1 ( Task - level Attention Annotation ) Assuming the existence of the vocabulary V , the vocab - level annotation A task { 0 , 1 } | V | is a binary vector of the hard selection for words in V , i.e. , \u2200w t V : A task ( w t ) { 0 , 1 } . Based on A task , when given an input sample x , we can use a proxy of the sample - level annotation A , i.e. , \u2200w t x : A ( w t ) = A task ( w t ) . As shown in Tab . 1 , the annotation space , which is referred to as a word set size for annotation , is 10\u223c36 times smaller at task - level than at samplelevel . Generally , the vocabulary size is far smaller than the total number of word occurrences in training samples . Our goal is thus to keep annotation cost cognitively reasonable ( Zou et al , 2018 ; Zhao et al , 2018 ) , leaving machine self - supervision to close the annotation quality gap ( Sec . 3.1 and 3.2 ) . Meanwhile , we present a setup of zero - cost supervision , which allows us attention supervision without any human efforts in all scenarios using public resources and tools ( Sec . 3.3 ) .", "entities": [[94, 95, "DatasetName", "0"], [127, 128, "DatasetName", "0"]]}
{"text": "From this point on , for task - level supervision , we assume zero - cost human annotation efforts , either by using public resources or self - supervision . Supervision by public resources Task - level annotation are often publicly available as resources or tools . For example , sentiment lexicon ( Esuli and Sebastiani , 2006 ) consists of sentiment words , which are important to the sentiment classification task , and named - entity recognizer ( NER ) ( Peters et al , 2017 ) can collect entity words commonly attended in news categorization task . We empirically show that both lexicon and NER can be adequate substitutes for the manual task - level annotation .", "entities": [[79, 80, "TaskName", "NER"], [106, 107, "TaskName", "NER"]]}
{"text": "To validate the effectiveness of SANA , we use the following three text classification datasets , which are widely used Jain and Wallace , 2019 ) and statistically diverse as well . We split the official training split into 90 % and 10 % as training and validation sets respectively . We expect SANA in two - sentence tasks , such as SNLI and MPQA , would be promising , which we leave as future work . SST2 ( Socher et al , 2013 ) : Stanford Sen - timent Treebank provides around 11 K sentences tagged with sentiment on a scale from 1 ( most negative ) to 5 ( most positive ) . We filter out neutral samples and dichotomize the remaining sentences into positive ( 4 , 5 ) and negative ( 1 , 2 ) . We set the maximum sequence length as 30 . IMDB ( Maas et al , 2011 ) : IMDB Large Movie Review Corpus is a binary sentiment classification dataset containing 50 K polarized ( positive or negative ) movie reviews , split into half for training and testing . We set the maximum sequence length as 180 . 20NG : 20 Newsgroups 2 contains around 19 K documents evenly categorized into 20 different categories . Following ( Jain and Wallace , 2019 ) , we extract samples belonging to baseball and hockey classes , which we designate as 0 and 1 , deriving a binary classification task ( Hockey vs Baseball ) . We set the maximum sequence length as 300 .", "entities": [[12, 14, "TaskName", "text classification"], [62, 63, "DatasetName", "SNLI"], [64, 65, "DatasetName", "MPQA"], [77, 78, "DatasetName", "SST2"], [149, 150, "DatasetName", "IMDB"], [158, 159, "DatasetName", "IMDB"], [200, 202, "DatasetName", "20 Newsgroups"], [238, 239, "DatasetName", "0"]]}
{"text": "This section studies whether attention , after supervision , is more effective for human consumption as model explanation . Existing metrics for explainability measure whether attention correlates with ( a ) class prediction or ( b ) feature importance , discussed in the next sections respectively .", "entities": [[37, 39, "TaskName", "feature importance"]]}
{"text": "One measure for the explainability of attention is whether each attention weight captures the causality of word and class prediction , by permuting words and observing prediction changes . If the learning is successful , such causal signals should be consistently observed in the test predictions . To validate this , we employ the attention - permutation experiments designed in ( Jain and Wallace , 2019 ) , i.e. , what - if simulation . Specifically , when given an input sample in the test phase , we look into whether the randomly mutated attention ( i.e. , cause ) from the original attention yields any changes in the corresponding prediction result ( i.e. , effect ) . Here , T V D for the permutation can be regarded as a desirable evaluation measure : as T V D is lower , the ( original ) learned attention has a weak mapping with the model prediction , and vice versa . The results are presented in Fig . 2 , where x - axis refers to T V D values , i.e. , the difference of model predictions , and y - axis refers to the frequency of what - if simulations on their returning T V D value . To carefully analyze this , we divide the simulation results by four different intervals of input sequence length , which can be an influencing factor : as the perturbations on longer texts are unlikely to make prediction changes ( Sen et al , 2020 ) . In this figure , we can observe that SANA has the lowest frequency on T V D = 0 in all cases , showing the distribution skewed to larger T V D ( i.e. , right on x - axis ) compared to baselines . Such distribution suggests that attention in SANA strongly affects model prediction by the causal signals . In unsupervised and vocab ( i.e. , task - level ) , the distributions are skewed to lower T V D ( i.e. , left on x - axis ) , having larger frequency on zero T V D than SANA . These patterns indicate the baselines have weak attentions loosely aligned to model predictions , motivating SANA even working well in long texts .", "entities": [[274, 275, "DatasetName", "0"]]}
{"text": "As an alternative metric of attention explainablity , ( Jain and Wallace , 2019 ) considers the relationship between attention weights and gradient - based feature importance score of each word . However , prior research suggests using word as a unit of importance feature is rather artificial , as word is contextualized by , and interacts with other words : ( Wiegreffe and Pinter , 2019 ) observes such limitation , and Shapley ( Chen et al , 2018 ) measures interaction between features for capturing dependency of arbitrary subsets . For this purpose , we report the KL divergence between C - Shapley 4 and attention weights , D KL ( Shapley ( x ) | | attention ( x ) ) . We present the results in Tab . 4 , showing SANA approach is the most well correlated method with Shapley scores , well capturing word dependency . Intuitively , C - Shapley observes the interaction in n - gram , and our work , attending upon hidden representations of RNN , which are soft n - grams , captures similar interactions . This result manifests that , standing on self - supervision signals , our counterfactual process can improve the explanation on the contextualization ability of RNN architectures .", "entities": [[25, 27, "TaskName", "feature importance"]]}
{"text": "This work is supported by AI Graduate School Program ( 2020 - 0 - 01361 ) and IITP grant ( No.2017 - 0 - 01779 , XAI ) supervised by IITP . Hwang is a corresponding author .", "entities": [[12, 13, "DatasetName", "0"], [22, 23, "DatasetName", "0"]]}
{"text": "The FST described here is primarily built by Jack Rueter , beginning with work in the 1990s . The Komi - Zyrian finite - state description began with \u2076http://videocorpora.ru a trilingual glossary \u04e6\u0448\u043a\u0430\u043c\u04e7\u0448\u043a\u0430 \u0438\u0447\u04e7\u0442 \u043a\u044b\u0432\u0432\u043e\u0440 , \u043a\u043e\u043c\u0438\u0430 - \u0430\u043d\u0433\u043b\u0438\u0441\u043a\u04e7\u044f - \u0444\u0438\u043d\u0441\u043a\u04e7\u044f ( Rueter , 1995 ) , designed for use by Finnish and English speaking students of Komi , without previous knowledge of Russian , to accompany the \u043a\u043e\u043c\u0438 \u043a\u044b\u0432 ' Komi language ' reader ( \u0426\u044b\u043f\u0430\u043d\u043e\u0432 , 1992 ) , used for instruction in the Universities of Helsinki and Turku . Later , with a scholarship from the Kordelin Foundation , this vocabulary was augmented . First , the extension was intended to complement a second Komi reader by \u041c\u0430\u043d\u043e\u0432\u0430 ( 1994 ) , and then to outline the Komi stem vocabulary of the Komi - Russian dictionary by \u041b\u044b\u0442\u043a\u0438\u043d and \u0422\u0438\u043c\u0443\u0448\u0435\u0432 ( 1961 ) . A large portion of the work done with this dictionary was only possible with the painstaking hours spent by Vera Chernykh . Thus , the approximately 3000 - word glossary providing the lexical base for a finite - state description of Komi - Zyrian , presented at Permistika 6 at the Udmurt State University in Izhevsk , 1996 ( published in Rueter , 2000 , was extended to over 6000 lexical entries . In 2004 Trond Trosterud invited Rueter to Troms\u00f8 to learn more about the Xerox Finite - state technology ( XFST ) being implemented at Giellatekno as described in ( Trosterud , 2004 ) and for Komi in ( Trosterud , 2004b ) . Here the Komi transducer and lexicon were to be developed further than before , and to be connected to an infrastructure that was compatible with a larger array of languages . To summarise some of the new improvements , there were no longer problems with Cyrillic letters requiring representation as conversions from Latin letters . It was now possible to write rules directly addressing elements of the Komi orthography . This direct use of the vernacular in the code may have , in fact , contributed to the belief of the developer that only the normative language needed description . ( It was not until many years later that work with other under - resourced languages , such as Mansi ( 2015present ) , Olonets - Karelian ( 2013 - present ) , Skolt Saami ( 2015 and V\u00f5ro ( 2014 - present ) , made it obvious that non - standard words also require description . ) One of the most important items at this point was that the lexicon and morphology were open - source . This meant , in turn , that Komi could be worked on by others and tested in projects . Here , Komi was ideal . The morphology is very concatenative , and the orthography contains only two more letters than the Russian , i.e. problems with some rarer Cyrillic letters could be evaluated and solved . In 2012 - 2016 Paula Kokkonen worked in conjunction with one of Rueter 's projects , where she improved the Finnish translations and inspected the English translations for Komi lexemes . This work significantly increased the coverage of Finnish translations in the multilingual dictionary that was created in this point . During the period 2012 - 2021 , FU - Lab and Giellatekno collaboration has featured active FST development , including multiple use , and especially improvement in the disambiguation rules and lexical coverage . Morphological analysis is a central component in a modern corpus , and issues such as ambiguity are also always present when FST is used in this context ( \u04e6\u043d\u044c\u04e7 \u041b\u0430\u0432 , 2015 , 140 ) . Collaboration may also lead to unforeseeable development . When two infrastructures are aligned , there are often competing priorities . This has also been the case here , i.e. whereas FU - Lab has demonstrated immediate interest in the facilitation of writing , spell checking , dictionaries and corpora for the language community , Giellatekno has pushed for research - related morphological description , analysis and lexica for the research community , but which can , in fact , later be applied to the production of spell checking and other derivative tools . This divergence in priority lead to some duplicate work in morphology . Helsinki Finite - State Technology ( HFST ) ( Lind\u00e9n et al , 2013 ) at Giellatekno with multi - use priorities was pitted against the quick but single - use Hunspell strategies practiced at FU - Lab . Thus , some of the technical complexities on the Giellatekno side had to be simplified so that one set of lexica might be shared . Giellatekno had plenty to gain from the lexical work done at FU - Lab , on the one hand , but it was not able to capitalize on its own sophisticated two - level description as a result of it , on the other . As regards morphophonological descriptions , stem - final variation had to be moved one step away from the initial LEMMA + COLON + STEM + CONTINUATIONLEXICON declaration in the code . While Jack Rueter has often quickly followed the suggestion of XML maintenance of lexical materials , it has turned out that collaboration pulls away from this write - only - once policy . The more people there are working with one data set , the more documentation required for maintaining mutual working principles . Simple and complex XML systems alike require a working front - end , otherwise , as has been the case here , the workers opt out of the XML database and end up working more on materials that can not be readily integrated back into the system . At the moment the XML transformation is not being used in FST development . Instead , other solutions for database implementation are being worked on , see Alnajjar et al ( 2020a ) ; . Only time will reveal which directions of development have contributed the most to the infrastructure . In 2018 - 2021 , Niko Partanen has been improving the dialectal lexicon coverage of the transducer while conducting his doctoral studies in Komi dialectology . In connection to this work , in 2020 - 2021 , Jack Rueter has improved the coverage of dialectal morphology , specifically taking into account the phenomena found in the Izhma dialect . This work by both of them was done within a Kone Foundation funded research project Language Documentation Meets Language Technology : The Next Step in the Description of Komi . The work shows that it is a feasible strategy to improve the analyser so that the work aligns with specific goals and needs of an individual project or dataset . It does create an imbalance in to which degree different dialects are represented , but for a language as large as Komi doing everything at the same time is not possible either . Mika H\u00e4m\u00e4l\u00e4inen 's role has been central in building more widely accessible computational infrastructure to access these transducers ( H\u00e4m\u00e4l\u00e4inen , 2019 ) . In the recent work to create an online editing platform that would allow improved access to the lexical materials , Khalid Alnajjar has been in an irreplaceable position ( Alnajjar et al , 2020a ) . This all shows that managing a transducer for a language like Komi is a multi - partnered operation that calls for wide collaboration between different groups and even infrastructures . Since 2017 , work has been conducted within the Universal Dependencies project to better cover Komi varieties , most recently ( Zeman et al , 2021 ) , see also Nivre et al ( 2020 ) . There are two Zyrian treebanks ( Partanen et al , 2018 ) , and work with Permyak progresses at many levels ( Rueter et al , 2020c ) . Especially in the initial phase of the treebank , building the finite - state descriptions is in a pivotal role , and maintaining interoperability between the FST and treebank development allows very efficient use of both systems . A similar approach has also been systematically used for other languages , such as Karelian ( Pirinen , 2019a ) and both Mordvinic languages . Indeed , managing systematic and comparable use of tags and conventions across languages is one of the primary concerns in our work as well , and there have been specific surveys that try to track the progress of different Uralic treebanks ( Rueter and Partanen , 2019 ) . We can also mention that the practices described here have also be adopted for the development of Amazon minority language description for Apurin\u00e3 in Helsinki - Bel\u00e9m . In the approach discussed here , this harmonization starts at the transducer level and the documentation therein . In the context of concrete applications of the Komi FST , we can highlight work by Gerstenberger et al ( 2017 ) , where the analyser was integrated into the popular multimedia annotation software ELAN . In addition , the most significant Komi online resource , the National Komi Corpus , contains annotations done with the transducer\u2077. Next we describe some of the challenges and important phenomena that have been addressed in various ways when creating the Komi analyser .", "entities": [[582, 584, "TaskName", "Morphological analysis"], [851, 852, "DatasetName", "LEMMA"], [1267, 1269, "DatasetName", "Universal Dependencies"]]}
{"text": "The Komi - Zyrian language is known to display a typologically common l - vocalization , which is a process where a lateral approximant is replaced by a labiodental fricative /v/ or labiodental approximant /\u028b/. In the Komi grammaticography this is known as l / v variation . Another comparable stem - alterating \u2077http://komicorpora.ru phenomena are the paragogic consonants in some word stems . These phenomena can be dealt with in much the same way , as they share a common trigger . Words with l / v or paragogic consonant variation in their stems can be identified on the basis of whether the stem is followed by an vowel - initial suffix , on the one hand , or a consonant - initial suffix ( alternatively word boundary ) , on the other . In the description of these words it has been suggested that erroneous forms be specifically identified . Special tags indicating the absence of paragogic consonants or substandard realization of the stem - final l / v have been implemented for Komi - Zyrian and reflect parallel tags previously implemented in the FST descriptions of other languages in the GiellaLT infrastructure , Northern and Skolt Saami , Erzya , Moksha , V\u00f5ro to mention a few . When we include more dialectal materials in the description , we also have to account for processes where l - vocalization triggers vowel lengthening . There are also secondary types of l - vocalization , influencing stems ending in the sequence / - el/ , and triggering change / - ej/. Currently this is treated at the lemma level , so that the non - standard forms are connected to the standard lemmas , with an additional tag indicating dialectal form or error . Even the dialectal variants where neither types of the variation are met are exceptions from the point of view of the standard language . We have devised a tagging system for various subtypes , but the exact implementation is still being designed and planned further . We discuss in Section 4.2 related challenges in more detail .", "entities": [[268, 269, "DatasetName", "lemma"]]}
{"text": "In recent years many neural network based approaches have been becoming popular and also shown good results . In a recent study by Pirinen ( 2019b ) the neural models were better than the traditional rule - based approaches for Finnish . Our team is always following new developments of the field , but we also believe that different approaches can be successfully combined . We already see studies emerging where a neural network has been used to learn to generate predictions from an FST . Their research is also used the Komi - Zyrian FST presented in this paper . The results were promising and we are eager to see how this ideology of using neural networks and rule - based systems side by side rather than as competing systems plays out in the future . For the NLP pipeline of Komi the most important new developments will be connected to improvements in the dependency parsing side of the analysis , ideally in connection to automatic and rule - based methods of disambiguation . Komi Constraint Grammar has currently focused to disambiguation , and the tagging and parsing sections are largely missing . It remains to be seen what kind of an approach will be the most successful here . At the same time Komi Universal Dependencies treebanks have started to be large enough that their further modeling with deep learning starts to be an attractive and possibly fruitful task . Komi texts are also present in many different orthographies , and taking all of them into account is a large and important task ( Rueter and Ponomareva , 2019 ) . Since the corpora of Latin Komi texts are also now available\u2079 , the future for these lines of research is exciting and promising . This also connects to various transcription systems used in linguistic publications and text collections : these materials should be republished in the contemporary orthography in order to make them maximally useful for the language communities themselves . Yet another future task is to provide access to the multilingual Komi lexicon the FST is based on in a form that is truly accessible and openly available . One solution could be to use online dictionary editing platforms , which are strongly linked to the FST development work , and thereby benefit it directly ( Alnajjar et al , 2020b ) . These lexicons have already been published in Zenodo ( Rueter et al , 2020b ) , and already their earliest version has been published in print ( Rueter , 1995 ) . Thereby the work described here in various ways continues an already nological and practical changes that these decades have shown . We believe this line of investigation of the Komi language will boldly continue the next 25 years , but also hope the reports of how the work progresses will become even more regularly . We also foresee that further development of the Komi FST will bring new tools to benefit both the public and research communities . Such might be machine translation , on the one hand ( Tiedemann , 2021 ) , and translation studies , on the other ( cf . \u0426\u044b\u043f\u0430\u043d\u043e\u0432 , 2021 ) . This , of course , does not close the circle , but merely the ever continuous spiral of development .", "entities": [[155, 157, "TaskName", "dependency parsing"], [216, 218, "DatasetName", "Universal Dependencies"], [510, 512, "TaskName", "machine translation"]]}
{"text": "We consider entity - level sentiment analysis in Arabic , a morphologically rich language with increasing resources . We present a system that is applied to complex posts written in response to Arabic newspaper articles . Our goal is to identify important entity \" targets \" within the post along with the polarity expressed about each target . We achieve significant improvements over multiple baselines , demonstrating that the use of specific morphological representations improves the performance of identifying both important targets and their sentiment , and that the use of distributional semantic clusters further boosts performances for these representations , especially when richer linguistic resources are not available .", "entities": [[5, 7, "TaskName", "sentiment analysis"]]}
{"text": "Target - specific sentiment analysis has recently become a popular problem in natural language processing . In interpreting social media posts , analysis needs to include more than just whether people feel positively or negatively ; it also needs to include what they like or dislike . The task of finding all targets within the data has been called \" open - domain targeted sentiment \" ( Mitchell et al , 2013 ; Zhang et al , 2015 ) . If we could successfully identify the targets of sentiment , it would be valuable for a number of applications including sentiment summarization , question answering , understanding public opinion during political conflict , or assessing needs of populations during natural disasters . In this paper , we address the open - domain targeted sentiment task . Input to our system consists of online posts , which can be comprised of one or multiple sentences , contain multiple entities with different sentiment , and have different domains . Our goal is to identify the important entities towards which opinions are expressed in the post ; these can include any nominal or noun phrase , including events , or concepts , and they are not restricted to named entities as has been the case in some previous work . The only constraint is that the entities need to be explicitly mentioned in the text . Our work also differs from much work on targeted sentiment analysis in that posts are long , complex , with many annotated targets and a lack of punctuation that is characteristic of Arabic online language . Figure 1 shows an example post , where targets are either labeled positive ( green ) if a positive opinion is expressed about them and negative ( yellow ) if a negative opinion is expressed . To identify targets and sentiment , we develop two sequence labeling models , a target - specific model and a sentiment - specific model . Our models try to learn syntactic relations between entities and opinion words , but they also make use of ( 1 ) Arabic morphology and ( 2 ) entity semantics . Our use of morphology allows us to capture all \" words \" that play a role in identification of the target , while our use of entity semantics allows us to group together similar entities which may all be targets of the same sentiment ; for example , if a commenter expresses negative sentiment towards the United States , they may also express negative sentiment towards America or Obama . Our results show that morphology matters when identifying entity targets and the sentiment expressed towards them . We find for instance that the attaching Arabic definite article Al+ is an important indicator of the presence of a target entity and splitting it off boosts recall of targets , while sentiment models perform better when less tokens are split . We also conduct a detailed analysis of errors revealing that the task generally entails hard problems such as a considerable amount of implicit sentiment and the presence of multiple targets with varying importance . In what follows , we describe related work ( 2 ) , data and models ( 3 and 4 ) , and linguistic decisions made for Arabic ( 5 ) . In 6 , we describe our use of word vector clusters learned on a large Arabic corpus . Finally , 7 presents experiments and detailed error analysis .", "entities": [[3, 5, "TaskName", "sentiment analysis"], [101, 102, "TaskName", "summarization"], [103, 105, "TaskName", "question answering"], [242, 244, "TaskName", "sentiment analysis"]]}
{"text": "Aspect - based and Entity - specific Analysis Early work in target - based sentiment looked at identifying aspects in a restricted domain : product or customer reviews . Many of these systems used unsupervised and topical methods for determining aspects of products ; Hu and Liu ( 2004 ) used frequent feature mining to find noun phrase aspects , Brody and Elhadad ( 2010 ) used topic modeling to find important keywords in restaurant reviews , and Somasundaran and Wiebe ( 2009 ) mined the web to find important aspects associated with debate topics and their corresponding polarities . SemEval 2014 Task 4 ( Pontiki et al , 2014 ) ran several subtasks for identifying aspect terms and sentiment towards aspects and terms in restaurant and laptop reviews . Entity - specific sentiment analysis has been frequently studied in social media and online posts . Jiang et al ( 2011 ) proposed identifying sentiment of a tweet towards a specific named entity , taking into account multiple mentions of the given entity . Biyani et al ( 2015 ) studied sentiment towards entities in online posts , where the local part of the post that contained the entity or mentions of it was identified and the sentiment was classified using a number of linguistic features . The entities were selected beforehand and consisted of known , named entities . More recent work uses LSTM and RNN networks to determine sentiment toward aspects in product reviews ( Wang et al , 2016 ) and towards entities in Twitter ( Dong et al , 2014 ; Tang et al , 2015 ) . SemEval 2016 ran two tasks on sentiment analysis ( Nakov et al , 2016 ) and stance ( Mohammad et al , 2016 ) towards pre - defined topics in Twitter , both on English data . Open domain targeted analysis In early work . Kim and Hovy ( 2006 ) proposed finding opinion target and sources in news text by automatic labeling of semantic roles . Here , opinion - target relationships were restricted to relations that can be captured using semantic roles . Ruppenhofer et al ( 2008 ) discussed the challenges of identifying targets in open - domain text which can not be addressed by semantic role labeling , such as implicitly conveyed sentiment , global and local targets related to the same entity , and the need for distinguishing between entity and proposition targets . Sequence labeling models became more popular for this problem : Mitchell et al ( 2013 ) used CRF model combinations to identify named entity targets in English and Spanish , and Yang and Cardie ( 2013 ) used joint modeling to predict opinion expressions and their source and target spans in news articles , improving over several single CRF models . Their focus was on identifying directly subjective opinion expressions ( e.g \" I hate [ this dictator ] \" vs. \" [ This dictator ] is destroying his country . \" ) Recent work ( Deng and Wiebe , 2015 ) identifies entity sources and targets , as well as the sentiment expressed by and towards these entities . This work was based on probablistic soft logic models , also with a focus on direct subjective expressions . There is also complementary work on using neural networks for tagging open - domain targets ( Zhang et al , 2015 ; in shorter posts . Previous work listed did not consider word morphology , or explicitly model distributional entity semantics as indicative of the presence of sentiment targets .", "entities": [[133, 135, "TaskName", "sentiment analysis"], [234, 235, "MethodName", "LSTM"], [278, 280, "TaskName", "sentiment analysis"], [380, 383, "TaskName", "semantic role labeling"], [428, 429, "MethodName", "CRF"], [469, 470, "MethodName", "CRF"]]}
{"text": "Past work in Arabic machine translation ( Habash and Sadat , 2006 ) and named entity recognition ( Benajiba et al , 2008 ) considered the tokenization of complex Arabic words as we do in our sequence labeling task . Analysis of such segmentation schemes has not been reported for Arabic sentiment tasks , which cover mostly sentence - level sentiment analysis and where the lemma or surface bag - of - word representations have typically been sufficient . There are now many studies on sentence - level sentiment analysis in Arabic news and social media ( Abdul - Mageed and Diab , 2011 ; Mourad and Darwish , 2013 ; Refaee and Rieser , 2014 ; Salameh et al , 2015 ) . Elarnaoty et al ( 2012 ) proposed identifying sources of opinions in Arabic using a CRF with a number of patterns , lexical and subjectivity clues ; they did not discuss morphology or syntactic relations . developed a dataset and built a majority baseline for finding targets in Arabic book reviews of known aspects ; Obaidat et al ( 2015 ) also developed a lexicon - based approach to improve on this baseline . Abu - Jbara et al ( 2013 ) created a simple opinion - target system for Arabic by identifying noun phrases in polarized text ; this was done intrinsically as part of an effort to identify opinion subgroups in online discussions . There are no other sentiment target studies in Arabic that we know of . In our experiments , we compare to methods similar to these baseline systems , as well as to results of English work that is comparable to ours . Entity Clusters It has been shown consistently that semantic word clusters improve the performance of named entity recognition ( T\u00e4ckstr\u00f6m et al , 2012 ; Zirikly and Hagiwara , 2015 ; Turian et al , 2010 ) and semantic parsing ( Saleh et al , 2014 ) ; we are not aware of such work for identifying entity targets of sentiment .", "entities": [[4, 6, "TaskName", "machine translation"], [14, 17, "TaskName", "named entity recognition"], [60, 62, "TaskName", "sentiment analysis"], [65, 66, "DatasetName", "lemma"], [88, 90, "TaskName", "sentiment analysis"], [139, 140, "MethodName", "CRF"], [297, 300, "TaskName", "named entity recognition"], [320, 322, "TaskName", "semantic parsing"]]}
{"text": "For modeling the data , we choose Conditional Random Fields ( CRF ) ( Lafferty et al , 2001 ) for the ability to engineer Arabic linguistic features and because of the success of CRF models in the past for entity identification and classification related tasks . We build two linear chain CRF models : 1 . Target Model This model predicts a sequence of labels E for a sequence of input tokens x , where E i { T ( target ) , O ( not_target ) } and each token x i is represented by a feature vector f it . A token is labeled T if it is part of a target ; a target can contain one or more consecutive tokens .", "entities": [[11, 12, "MethodName", "CRF"], [34, 35, "MethodName", "CRF"], [52, 53, "MethodName", "CRF"]]}
{"text": "In Arabic , clitics and affixes can attach to the beginning and end of the word stem , making words complex . For example , in the sentence ' So they welcomed her ' , the discourse conjuction ( so + ) , the opinion target ( her + ) , opinion holder ( they ) , and the opinion expression itself ( welcomed ) are all collapsed in the same word . Clitics , such as conjunctions + w+ , prepositions + b+ , the definite article Al+ ' the ' ( all of which attach at the beginning ) , and possessive pronouns and object pronouns + + h + + hA ' his / her ' or ' him / her ' ( which attach at the end ) can all function as individual words . Thus , they can be represented as separate tokens in the CRF . The morphological analyzer MADAMIRA ( Pasha et al , 2014 ) enables the tokenization of a word using multiple schemes . We consider the following two schemes : D3 : the Declitization scheme which splits off conjunction clitics , particles and prepositions , Al+ , and all the enclitics at the end . ATB : the Penn Arabic Treebank tokenization , which separates all clitics above except the definite article Al+ , which it keeps attached . For a detailed description of Arabic concatenative morphology and tokenization schemes , the reader is referred to Habash ( 2010 ) . For each token , we add a part of speech feature . For word form ( non - clitic ) tokens , we use the part of speech ( POS ) feature produced by the morphological analyzer . We consider the surface word and the lemma for representing the word form . For the clitics that were split off , we use a detailed POS feature that is also extracted from the output of the analyzer and can take such forms as DET for Al+ or poss_pron_3MP for third person masculine possessive pronouns . Table 2 shows the words and part of speech for the input sentence ' so they welcomed her ' fa - istaqbalu - ha , using the lemma representation for the word form and the D3 tokenization scheme . These lexical and POS features are added to both our target model and sentiment model .", "entities": [[150, 151, "MethodName", "CRF"], [180, 181, "DatasetName", "D3"], [296, 297, "DatasetName", "lemma"], [333, 334, "DatasetName", "DET"], [372, 373, "DatasetName", "lemma"], [380, 381, "DatasetName", "D3"]]}
{"text": "The choice of sentiment lexicon is an important consideration when developing systems for new and/or low - resource languages . We consider three lexicons : ( 1 ) SIFAAT , a manually constructed Arabic lexicon of 3982 adjectives ( Abdul - Mageed and Diab , 2011 ) , ( 2 ) ArSenL , an Arabic lexicon developed by linking English SentiWord - Net with Arabic WordNet and an Arabic lexical database ( Badaro et al , 2014 ) , and ( 3 ) the English MPQA lexicon ( Wilson et al , 2005 ) , where we look up words by matching on the English glosses produced by the morphological analyzer MADAMIRA . For the target model , we add token - level binary features representing subjectivity , and for the sentiment model , we add both subjectivity and polarity features . We also add a feature specifying respectively the subjectivity or polarity of the parent word of the token in the dependency tree in the target or sentiment model .", "entities": [[85, 86, "DatasetName", "MPQA"]]}
{"text": "We ran the CATiB ( Columbia Arabic Treebank ) dependency parser ( Shahrour et al , 2015 ) on our data . CATiB uses a number of intuitive labels specifying the token 's syntactic role : e.g SBJ , OBJ , MOD , and IDF for the Arabic idafa construct ( e.g president of government ) , as well as its part of speech role . In addition to the sentiment dependency features specifying the sentiment of parent words , we added dependency features specifying the syntactic role of the token in relation to its parent , and the path from the token to the parent , e.g nom_obj_vrb or nom_idf_nom , as well as the sentiment path from the token to the parent , e.g nom ( neutral ) _ obj_vrb ( negative ) .", "entities": [[41, 42, "DatasetName", "MOD"]]}
{"text": "The morphological analyzer MADAMIRA also produces base phrase chunks ( BPC ) and named entity tags ( NER ) for each token . We add features for these as well , based on the hypothesis that they will help define the spans for entity targets , whether they are named entities or any noun phrases . We refer to the sentiment and target models that utilize Arabic morphology , sentiment , syntactic relations and entity chunks as best - linguistic .", "entities": [[17, 18, "TaskName", "NER"]]}
{"text": "Similar entities which occur in the context of the same topic or the same larger entity are likely to occur as targets alongside each other and to have similar sentiment expressed towards them . They may repeat frequently in a post even if they do not explicitly or lexically refer to the same person or object . For example , someone writing about American foreign policy may frequently refer to entities such as { the United States , America , Obama , the Americans , Westerners } . Such entities can cluster together semantically and it is likely that a person expressing positive or negative sentiment towards one of these entities may also express the same sentiment towards the other entities in this set . Moreover , cluster features serve as a denser feature representation with a reduced feature space compared to Arabic lexical features . Such features can benefit the CRF where a limited amount of training data is available for target entities . To utilize the semantics of word clusters , we build word embedding vectors using the skip - gram method ( Mikolov et al , 2013 ) and cluster them using the K - Means algorithm ( MacQueen , 1967 ) , with Euclidean distance as a metric . Euclidean distance serves as a semantic similarity metric and has been commonly used as a distance - based measure for clustering word vectors . The vectors are built on Arabic Wikipedia 2 on a corpus of 137 M words resulting in a vocabulary of 254 K words . We preprocess the corpus by tokenizing ( using the schemes described in section 5 ) and lemmatizing before building the word vectors . We vary the number of clusters and use the clusters as binary features in our target and sentiment models .", "entities": [[151, 152, "MethodName", "CRF"], [218, 220, "TaskName", "semantic similarity"]]}
{"text": "Setup To build our sentiment and target models , we use CRF++ ( Kudo , 2005 ) to build linear - chain sequences . We use a context window of + / - 2 for all features except the syntactic dependencies , where we use a window of + / - 4 to better capture syntactic relations in the posts . For the sentiment model , we include the context of the previous predicted label , to avoid predicting consecutive tokens with opposite polarity . We evaluate all our experiments on the development set which contains 116 posts and 442 targets , and present a final result with the best models on the unseen test . For the SentiWordNetbased lexicon ArSenL , we tune for the sentiment score threshold and use t=0.2 . We use Google 's word2vec tool 3 for building and clustering word vectors with dimension 200 . We vary the number of clusters k between 10 ( 25 K words / cluster ) and 20 K ( 12 words / cluster ) . Baselines For evaluating the predicted targets , we follow work in English ( Deng and Wiebe , 2015 ) and use the all - NP baseline , where all nouns and noun phrases in the post are predicted as important targets . For evaluating sentiment towards targets , we consider four baselines : the majority baseline which always predicts negative , and the lexicon baseline evaluated in the case of each of our three lexicons : manually created , WordNet - based , and English - translated . The strong lexicon baseline splits the post into sentences or phrases by punctuation , finds the phrase that contains the predicted target , and returns positive if there are more positive words than negative words , and negative otherwise . These baselines are similar to the methods of previously published work for Arabic targeted sentiment Obaidat et al , 2015 ; Abu - Jbara et al , 2013 ) . We run our pipelined models for all morphological representation schemes : surface word ( no token splits ) , lemma ( no clitics ) , lemma with ATB clitics ( contain all token splits except Al+ ) , and lemma with D3 clitics ( contains all token splits ) . We explore the effect of semantic word clusters in these scenarios . Finally we show our bestlinguistic ( high - resource ) model , and the resulting integration with word clusters .", "entities": [[135, 136, "DatasetName", "Google"], [353, 354, "DatasetName", "lemma"], [359, 360, "DatasetName", "lemma"], [373, 374, "DatasetName", "lemma"], [375, 376, "DatasetName", "D3"]]}
{"text": "We analyzed the output of our best linguistic models on the development set , and observed the following kind of errors : Implicit Sentiment This was the most common kind of error observed . Commenters frequently expressed complex subjective language without using sentiment words , often resorting to sarcasm , metaphor , and argumentative language . We also observed persistent errors where positive sentiment was identified towards an entity because of misleading polar words ; e.g minds was consistently predicted to be positive even though the post in question was using implicit language to express negative sentiment ; the English gloss Example 1 Till when will [ the world ] - wait before it intervenes against these [ crimes against humanity ] - committed by this [ criminal bloody regime ] - which will not stop doing that ... because its presence has always been associated with oppression and murder and crime ... But now it 's time for it to disappear and descend into [ the trash of history ] - . Output the world : neg crimes : neg criminal bloody regime : neg the trash of history : neg Example 2 [ Malaysia ] + is considered the most successful country in Eastern Asia , and its economic success has spread to other [ aspects of life in Malaysia ] + , for its [ services to its citizens ] + have improved , and there has been an increase in [ the quality of its health and educational and social and financial and touristic services ] + , which has made it excellent for foreign investments . Output Malaysia : pos health : pos educational and social : neg financial : neg is brains , which appears as a positive subjective word in the MPQA lexicon . The posts also contained cases of complex coreference where subjective statements were at long distances from the targets they discussed . Annotation Errors Our models often correctly predicted targets with reasonable sentiment which were not marked as important targets by annotators ; this points to the subjective nature of the task . Sentiment lexicon misses These errors resulted from mis - match between the sentiment of the English gloss and the intended Arabic meaning , leading to polar sentiment being missed . Primary Targets The data contains multiple entity targets and not all are of equal importance . Out of the first 50 posts manually analyzed on the dev set , we found that in 38 out of 50 cases ( 76 % ) the correct primary targets were identified ( the most important topical sentiment target ( s ) addressed by the post ) ; in 4 cases , a target was predicted where the annotations contained no polar targets at all , and in the remaining cases the primary target was missed . Correct sentiment polarity was predicted for 31 out of the 38 correct targets ( 81.6 % ) . In general , our analysis showed that our system does well on posts where targets and subjective language are well formed , but that the important target identification task is difficult and made more complex by the long and repetitive nature of the posts . Table 7 shows two examples of the translated output of SMARTies , the first on more wellformed text and the second on text that is more difficult to parse .", "entities": [[298, 299, "DatasetName", "MPQA"]]}
{"text": "We presented a linguistically inspired system that can recognize important entity targets along with sentiment in opinionated posts in Arabic . The targets can be any type of entity or event , and they are not known beforehand . Both target and sentiment results significantly improve multiple lexical baselines and are comparable to previously published results in similar tasks for English , a similarly hard task . Our task is further complicated by the informal and very long sentences that are used in Arabic online posts . We showed that the choice of morphological representation significantly affects the performance of the target and sentiment models . This could shed light on further research in target - specific sentiment analysis for morphologically complex languages , an area little investigated previously . We also showed that the use of semantic clusters boosts performance for both target and sentiment identification . Furthermore , semantic clusters alone can achieve performance close to a more resource - rich linguistic model relying on syntax and sentiment lexicons , and would thus be a good approach for low - resource languages . Integrating different morphological preprocessing schemes along with clusters gives our best result . Our code and data is publicly available 6 . Future work will consider cross - lingual clusters and morphologically different languages .", "entities": [[117, 119, "TaskName", "sentiment analysis"]]}
{"text": "This work was supported in part by grant NPRP 6 - 716 - 1 - 138 from the Qatar National Research Fund , by DARPA DEFT grant FA8750 - 12 - 2 - 0347 and by DARPA LORELEI grant HR0011 - 15 - 2 - 0041 . The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S government . We thank anonymous reviewers for their helpful comments . We thank Yves Petinot for providing feedback on the paper . We thank Nizar Habash and Mona Diab for helpful discussions .", "entities": [[24, 25, "DatasetName", "DARPA"], [36, 37, "DatasetName", "DARPA"]]}
{"text": "UWB at SemEval - 2016 Task 2 : Interpretable Semantic Textual Similarity with Distributional Semantics for Chunks", "entities": [[9, 12, "TaskName", "Semantic Textual Similarity"]]}
{"text": "We introduce a system focused on solving SemEval 2016 Task 2 - Interpretable Semantic Textual Similarity . The system explores machine learning and rule - based approaches to the task . We focus on machine learning and experiment with a wide variety of machine learning algorithms as well as with several types of features . The core of our system consists in exploiting distributional semantics to compare similarity of sentence chunks . The system won the competition in 2016 in the \" Gold standard chunk scenario \" . We have not participated in the \" System chunk scenario \" .", "entities": [[13, 16, "TaskName", "Semantic Textual Similarity"]]}
{"text": "The goal of the Interpretable Semantic Textual Similarity task is to go deeper with the assessment of semantic textual similarity of sentence pairs . It is requested to add an explanatory layer that offers a deeper insight into the sentence similarities . The sentences are split into chunks and the first goal is to find corresponding chunks ( with respect to their meanings ) among the compared sentences . When the corresponding chunks are known , the chunks are annotated with their similarity scores and their relation types ( e.g. equivalent , more specific , etc ) . The task follows a pilot task from the preceding SemEval 2015 competition ( Agirre et al , 2015 ) . The best performing systems adopted various approaches , ( Banjade et al , 2015 ) relied on handcrafter rules , ( Karumuri et al , 2015 ) employed a classifier for relation types and they associated each relation with a precomputed similarity score and ( H\u00e4nig et al , 2015 ) extended their word alignment algorithm for the task .", "entities": [[5, 8, "TaskName", "Semantic Textual Similarity"], [17, 20, "TaskName", "semantic textual similarity"], [171, 173, "TaskName", "word alignment"]]}
{"text": "The data consist of sentence pairs S a i and S b i , where a denotes the first item of the pair , b denotes the second item of the pair and i indexes the sentences ( for simiplicity we further omit i for sentences ) . We perceive a sentence S a to be an ordered set of chunks CH a j S a and the chunks to be ordered sets of words w k CH a j ( and analogically for sentence S b ) . Next we define two functions : sim ( CH a i , CH b j ) { 0 , 1 , 2 , 3 , 4 , 5 } for chunk similarity and rel ( CH a i , CH b j ) TYPE for chunk relation type . The possible types are : TYPE = { EQUI , OPPO , SPE1 , SPE2 , SIMI , REL } . These are the main types . All these types can have two modifiers ( FACT , POL ) . The modifiers are optionally attached to the main types . For example , you can generate SPE1 FACT . For more information , please see the annotation guidelines 1 .", "entities": [[106, 107, "DatasetName", "0"]]}
{"text": "As a first step of our approach we perform the following text preprocessing : Stopwords removal - we mark the words found in a predefined list of 32 stopwords . Special character removal we remove special characters that violate the tokenization . E.g. in one of the datasets , dots , commas , quotation marks and other punctuation characters were present in tokens . Lowercasing - we remove casing from the words . Lemmatization - we find lemmas with the Stanford CoreNLP tool . Our preprocessing rather adds new information and does not modify the original information . Thus , the original word and all the generated variants are always available . In this way , we can generate the output file with identical words ( including the special characters ) from the input . The dataset are already tokenized .", "entities": [[73, 74, "TaskName", "Lemmatization"]]}
{"text": "We divide the employed features into four categories : lexical , syntactic , semantic , external . Lexical features consist of the following features : word base form overlap , word lemma overlap , chunk length difference , word sentence positions difference . Syntactic features contains closest common parent comparison ( we compute the closest common parent of all words for each chunk in the parse tree and retrieve the name of the parent node ) , parse tree path comparison ( we compute the path from the root of the sentence to the chunk ) . POS ( Part Of Speech ) count difference ( e.g. differences in counts of nouns , adjectives , verbs , etc ) . POS tagging and syntactic parsing are performed with Stanford CoreNLP . Semantic features are described in Section 2.2 . Additionally , some members of our team participated in the STS task ( task 1 ) of the SemEval 2016 ( Brychc\u00edn and Svoboda , 2016 ) and they annotated the semantic similarity of the whole sentences with their system for us . This score is used as one feature . External features consist of the WordNet - Lin similarity metric ( Lin , 1998 ) and the paraphrase database ( Ganitkevitch et al , 2013 ) feature .", "entities": [[31, 32, "DatasetName", "lemma"], [149, 150, "TaskName", "STS"], [170, 172, "TaskName", "semantic similarity"]]}
{"text": "We attempt to solve the task with a rule - based approach as well . First , we define the similarity of chunks as described in Section 2.2 . The similarity is then used for the chunk alignment . We employ an algorithm inspired by the IBM word model II for machine translation ( Brown et al , 1993 ) . We iterate over all chunks from sentence S a and find the chunk with maximal similarity from sentence S b . More chunks from sentence S a can be aligned to one chunk in the sentence S b . In this way , we obtain N:1 mapping . Then , we do the same with the reversed order of sentences and get the 1 : M mapping . Then , we compare the mappings and take the one with the highest overall similarity . In this way , it is ensured that we generate only valid mappings ( unlike in the previous case of machine learning - see Section 3.3 ) . The relation types are then determined by an extremely simple algorithm : If the similarity is 5 , then the relation type is EQUI . If the similarity is 4 or 3 and chunks contain the same amount of words , then the relation is SIMI . If the similarity is 4 or 3 , then chunk with more words is more specific . If the similarity is 2 or 1 , then the relation is SIMI . If the similarity is 0 , then the relation type is NOALI .", "entities": [[51, 53, "TaskName", "machine translation"], [255, 256, "DatasetName", "0"]]}
{"text": "The machine learning approach with combination of methods for the distributional semantics ( Word2Vec and GloVe ) proved to be very capable of solving the advanced task of Interpretable Semantic Textual Similarity . We have chosen not to tune the system for individual datasets but to tune it for the task as a whole . The modified lexical semantic vectors approach seems to be an attractive alternative to the more traditional vector composition .", "entities": [[15, 16, "MethodName", "GloVe"], [29, 32, "TaskName", "Semantic Textual Similarity"]]}
{"text": "Event2Mind : Commonsense Inference on Events , Intents , and Reactions", "entities": [[0, 1, "DatasetName", "Event2Mind"]]}
{"text": "The first type of pragmatic inference is about intent . We define intent as an explanation of why the agent causes a volitional event to occur ( or \" none \" if the event phrase was unintentional ) . The intent can be considered a mental pre - condition of an action or an event . For example , if the event phrase is PersonX takes a stab at , the annotated intent might be that \" PersonX wants to solve a problem \" . The second type of pragmatic inference is about emotional reaction . We define reaction as an explanation of how the mental states of the agent and other people involved in the event would change as a result . The reaction can be considered a mental post - condition of an action or an event . For example , if the event phrase is that PersonX gives PersonY as a gift , PersonX might \" feel good about themselves \" as a result , and PersonY might \" feel grateful \" or \" feel thankful \" .", "entities": [[19, 20, "DatasetName", "agent"], [109, 110, "DatasetName", "agent"]]}
{"text": "We extract phrasal events from three different corpora for broad coverage : the ROC Story training set ( Mostafazadeh et al , 2016 ) , the Google Syntactic N - grams ( Goldberg and Orwant , 2013 ) , and the Spinn3r corpus ( Gordon and Swanson , 2008 ) . We derive events from the set of verb phrases in our corpora , based on syntactic parses ( Klein and Manning , 2003 ) . We then replace the predicate subject and other entities with the typed variables ( e.g. , PersonX , PersonY ) , and selectively substitute verb arguments with blanks ( ) . We use frequency thresholds to select events to annotate ( for details , see Appendix A.1 ) . Additionally , we supplement the list of events with all 2 , 000 verb idioms found in Wiktionary , in order to cover events that are less compositional . 2 Our final annotation corpus contains nearly 25 , 000 event phrases , spanning over 1 , 300 unique verb predicates ( Table 2 ) .", "entities": [[26, 27, "DatasetName", "Google"]]}
{"text": "We design an Amazon Mechanical Turk task to annotate the mental pre - and post - conditions of event phrases . A snippet of our MTurk HIT design is shown in Figure 2 . For each phrase , we ask three annotators whether the agent of the event , PersonX , intentionally causes the event , and if so , to provide up to three possible textual descriptions of their intents . We then ask annotators to provide up to three possible reactions that PersonX might experience as a result . We also ask annotators to provide up to three possible reactions of other people , when applicable . These other people can be either explicitly mentioned ( e.g. , \" PersonY \" in PersonX punches PersonY 's lights out ) , or only implied ( e.g. , given the event description PersonX yells at the classroom , we can infer that other people such as \" students \" in the classroom may be affected by the act of PersonX ) . For quality control , we periodically removed workers with high disagreement rates , at our discretion .", "entities": [[44, 45, "DatasetName", "agent"]]}
{"text": "Our dataset contains nearly 25 , 000 event phrases , with annotators rating 91 % of our extracted events as \" valid \" ( i.e. , the event makes sense ) . Of those events , annotations for the multiple choice portions of the task ( whether or not there exists intent / reaction ) agree moderately , with an average Cohen 's \uf8ff = 0.45 ( Table 2 ) . The individual \uf8ff scores generally indicate that turkers disagree half as often as if they were randomly selecting answers . Importantly , this level of agreement is acceptable in our task formulation for two reasons . First , unlike linguistic annotations on syntax or semantics where experts in the corresponding theory would generally agree on a single correct label , pragmatic interpretations may better be defined as distributions over multiple correct labels ( e.g. , after PersonX takes a test , PersonX might feel relieved and/or stressed ; de Marneffe et al , 2012 ) . Second , because we formulate our task as a conditional language modeling problem , where a distribution over the textual descriptions of intents and reactions is conditioned on the event description , this variation in the labels is only as expected . A majority of our events are annotated as willingly caused by the agent ( 86 % , Cohen 's \uf8ff = 0.48 ) , and 26 % involve other people ( \uf8ff = 0.41 ) . Most event patterns in our data are fully instantiated , with only 22 % containing blanks ( ) . In our corpus , the intent annotations are slightly longer ( 3.4 words on average ) than the reaction annotations ( 1.5 words ) .", "entities": [[221, 222, "DatasetName", "agent"]]}
{"text": "Through Event2Mind inference , we can attempt to bring to the surface what is implied about people 's behavior and mental states . We employ this inference to analyze implicit bias in modern films . As shown in Figure 7 , our model is able to analyze character portrayal beyond what is explicit in text , by performing pragmatic inference on character actions to explain aspects of a character 's mental state . In this section , we use our model 's inference to shed light on gender differences in intents behind and reactions to characters ' actions .", "entities": [[1, 2, "DatasetName", "Event2Mind"]]}
{"text": "For our portrayal analyses , we use scene descriptions from 772 movie scripts released by Gorinski and Lapata ( 2015 ) , assigned to over 21 , 000 characters as done by Sap et al ( 2017 ) . We extract events from the scene descriptions , and generate their 10 most probable intent and reaction sequences using our BiRNN sequence model ( as in Figure 7 ) . We then categorize generated intents and reactions into groups based on LIWC category scores of the generated output ( Tausczik and Pennebaker , 2016 ) . 3 The intent and reaction categories are then ( 1990 , bottom ) , augmented with Event2mind inferences on the characters ' intents and reactions . E.g. , our model infers that the event PersonX sits on PersonX 's bed , lost in thought implies that the agent , Vivian , is sad or worried . aggregated for each character , and standardized ( zero - mean and unit variance ) . We compute correlations with gender for each category of intent or reaction using a logistic regression model , testing significance while using Holm 's correction for multiple comparisons ( Holm , 1979 ) . 4 To account for the gender skew in scene presence ( 29.4 % of scenes have women ) , we statistically control for the total number of words in a character 's scene descriptions . Note that the original event phrases are all gender agnostic , as their participants have been replaced by variables ( e.g. , PersonX ) . We also find that the types of gender biases uncovered remain similar when we run these analyses on the human annotations or the generated words and phrases from the BiRNN with n - gram re - ranking decoding setup . and Needs ' , ' Personal Concerns ' , ' Biological Processes ' , ' Cognitive Processes ' , ' Social Words ' , ' Affect Words ' , ' Perceptual Processes ' . We refer the reader to Tausczik and Pennebaker ( 2016 ) or http://liwc.wpengine.com/ compare - dictionaries/ for a complete list of category descriptions . 4 Given the data limitation , we represent gender as a binary , but acknowledge that gender is a more complex social construct .", "entities": [[111, 112, "DatasetName", "Event2mind"], [142, 143, "DatasetName", "agent"], [181, 183, "MethodName", "logistic regression"]]}
{"text": "Intents and Reactions Our Event2Mind inferences automate portrayal analyses that previously required manual annotations ( Behm - Morawitz and Mastro , 2008 ; Prentice and Carranza , 2002 ; England et al , 2011 ) . Shown in Table 4 , our results indicate a gender bias in the behavior ascribed to characters , consistent with psychology and gender studies literature ( Collins , 2011 ) . Specifically , events with female semantic agents are intended to be helpful to other people ( intents involving FRIEND , FAMILY , and AFFILIATION ) , particularly relating to eating and making food for themselves and others ( INGEST , BODY ) . Events with male agents on the other hand are motivated by and resulting in achievements ( ACHIEVE , MONEY , REWARDS , POWER ) . Women 's looks and sexuality are also emphasized , as their actions ' intents and reactions are sexual , seen , or felt ( SEXUAL , SEE , PERCEPT ) . Men 's actions , on the other hand , are motivated by violence or fighting ( DEATH , ANGER , RISK ) , with strong negative reactions ( SAD , ANGER , NEGA -", "entities": [[4, 5, "DatasetName", "Event2Mind"]]}
{"text": "Prior work has sought formal frameworks for inferring roles and other attributes in relation to events ( Baker et al , 1998 ; Das et al , 2014 ; Schuler et al , 2009 ; Hartshorne et al , 2013 , inter alia ) , implicitly connoted by events ( Reisinger et al , 2015 ; White et al , 2016 ; Greene , 2007 ; Rashkin et al , 2016 ) , or sentiment polarities of events ( Ding and Riloff , 2016 ; Choi and Wiebe , 2014 ; Russo et al , 2015 ; Ding and Riloff , 2018 ) . In addition , recent work has studied the patterns which evoke certain polarities ( Reed et al , 2017 ) , the desires which make events affective ( Ding et al , 2017 ) , the emotions caused by events ( Vu et al , 2014 ) , or , conversely , identifying events or reasoning behind particular emotions ( Gui et al , 2017 ) . Compared to this prior literature , our work uniquely learns to model intents and reactions over a diverse set of events , includes inference over event participants not explicitly mentioned in text , and formulates the task as predicting the textual descriptions of the implied commonsense instead of classifying various event attributes . Previous work in natural language inference has focused on linguistic entailment ( Bowman et al , 2015 ; Bos and Markert , 2005 ) while ours focuses on commonsense - based inference . There also has been inference or entailment work that is more generation focused : generating , e.g. , entailed statements ( Zhang et al , 2017 ; Blouw and Eliasmith , 2018 ) , explanations of causality ( Kang et al , 2017 ) , or paraphrases ( Dong et al , 2017 ) . Our work also aims at generating inferences from sentences ; however , our models infer implicit information about mental states and causality , which has not been studied by most previous systems . Also related are commonsense knowledge bases ( Espinosa and Lieberman , 2005 ; Speer and Havasi , 2012 ) . Our work complements these ex - isting resources by providing commonsense relations that are relatively less populated in previous work . For instance , ConceptNet contains only 25 % of our events , and only 12 % have relations that resemble intent and reaction . We present a more detailed comparison with ConceptNet in Appendix C.", "entities": [[227, 230, "TaskName", "natural language inference"], [389, 390, "DatasetName", "ConceptNet"], [417, 418, "DatasetName", "ConceptNet"]]}
{"text": "We thank the anonymous reviewers for their insightful comments . We also thank xlab members at the University of Washington , Martha Palmer , Tim O'Gorman , Susan Windisch Brown , Ghazaleh Kazeminejad as well as other members at the University of Colorado at Boulder for many helpful comments for our development of the annotation pipeline . This work was supported in part by National Science Foundation Graduate Research Fellowship Program under grant DGE - 1256082 , NSF grant IIS - 1714566 , and the DARPA CwC program through ARO ( W911NF - 15 - 1 - 0543 ) .", "entities": [[85, 86, "DatasetName", "DARPA"]]}
{"text": "Learning to Generalize to More : Continuous Semantic Augmentation for Neural Machine Translation", "entities": [[11, 13, "TaskName", "Machine Translation"]]}
{"text": "The principal task in supervised neural machine translation ( NMT ) is to learn to generate target sentences conditioned on the source inputs from a set of parallel sentence pairs , and thus produce a model capable of generalizing to unseen instances . However , it is commonly observed that the generalization performance of the model is highly influenced by the amount of parallel data used in training . Although data augmentation is widely used to enrich the training data , conventional methods with discrete manipulations fail to generate diverse and faithful training samples . In this paper , we present a novel data augmentation paradigm termed Continuous Semantic Augmentation ( CSANMT ) , which augments each training instance with an adjacency semantic region that could cover adequate variants of literal expression under the same meaning . We conduct extensive experiments on both rich - resource and low - resource settings involving various language pairs , including WMT14 English { German , French } , NIST Chinese English and multiple low - resource IWSLT translation tasks . The provided empirical evidences show that CSANMT sets a new level of performance among existing augmentation techniques , improving on the state - of - theart by a large margin . 1", "entities": [[6, 8, "TaskName", "machine translation"], [70, 72, "TaskName", "data augmentation"], [103, 105, "TaskName", "data augmentation"], [157, 158, "DatasetName", "WMT14"]]}
{"text": "Data Augmentation ( DA ) Kobayashi , 2018 ; Gao et al , 2019 ; Khayrallah et al , 2020 ; Pham et al , 2021 ) has been widely used in neural machine translation . The most popular one is the family of back - translation ( Sennrich et al , 2016a ; Nguyen et al , 2020 ) , which utilizes a target - to - source model to translate monolingual target sentences back into the source language . Besides , constructing adversarial training instances with diverse literal forms via word replacing or embedding interpolating ( Wang et al , 2018 ; Cheng et al , 2020 ) is beneficial to improve the generalization performance of NMT models . Vicinal Risk Minimization ( VRM ) ( Chapelle et al , 2000 ) is another principle of data augmentation , in which DA is formalized as extracting additional pseudo samples from the vicinal distribution of observed instances . Typically the vicinity of a training example is defined using datasetdependent heuristics , such as color ( scale , mixup ) augmentation ( Simonyan and Zisserman , 2014 ; Krizhevsky et al , 2012 ; Zhang et al , 2018 ) in computer vision and adversarial augmentation with manifold neighborhoods ( Ng et al , 2020 ; Cheng et al , 2021 ) in NLP . Our approach relates to VRM that involves with an adjacency semantic region as the vicinity manifold for each training instance . Sentence Representation Learning is a well investigated area with dozens of methods ( Kiros et al , 2015 ; . In recent years , the methods built on large pre - trained models ( Devlin et al , 2019 ; Conneau et al , 2020 ) have been widely used for learning sentence level representations ( Reimers and Gurevych , 2019 ; Huang et al , 2019 ; Yang et al , 2019 ) . Our work is also related to the methods that aims at learning the uni - versal representation ( Zhang et al , 2016 ; Schwenk and Douze , 2017 ; for multiple semantically - equivalent sentences in NMT . In this context , contrastive learning has become a popular paradigm in NLP ( Kong et al , 2020 ; Clark et al , 2020 ; Gao et al , 2021 ) . The most related work are and Chi et al ( 2021 ) , which suggested transforming cross - lingual sentences into a shared vector by contrastive objectives .", "entities": [[0, 2, "TaskName", "Data Augmentation"], [33, 35, "TaskName", "machine translation"], [138, 140, "TaskName", "data augmentation"], [178, 179, "MethodName", "mixup"], [247, 249, "TaskName", "Representation Learning"], [364, 366, "MethodName", "contrastive learning"]]}
{"text": "Sampling Strategies ( i ) , r x \u2032 ( j ) \u03c9 ( k ) \u223c \u03b7N 0 , diag ( W 2 r ) + ( 1.0 \u2212 \u03b7 ) N 0 , 1 2 ditto \u03c9 ( k ) \u223c \u03b7U \u2212 W r , W r + ( 1.0 \u2212 \u03b7 ) U \u0101 \u2212 1 , 1 \u2212\u0101 where\u0101 = 1 k\u22121 k\u22121 i=1 \u03c9 ( i ) 3 E ( x ( i ) , y ( i ) ) \u223cB \u2212 KL p ( r x ( i ) ) \u2225 q ( r x ( i ) , r y ( i ) ) r x = \u00b5 + \u03f5 \u03c3 where p ( r x ( i ) ) \u223c N ( \u00b5 , \u03c3 2 ) and q ( r x ( i ) , r y ( i ) ) \u223c N ( \u00b5 \u2032 , \u03c3 \u20322 ) where \u03f5 is a standard Gaussian noise 4 E ( x ( i ) , y ( i ) ) \u223cB r T x ( i ) r y ( i ) \u2225r x ( i ) \u2225 \u2225r y ( i ) \u2225 \u03c9 ( k ) \u223c \u03b7N 0 , diag ( W 2 r ) + ( 1.0 \u2212 \u03b7 ) N 1 k\u22121 k\u22121 i=1 \u03c9 ( i ) , 1", "entities": [[18, 19, "DatasetName", "0"], [33, 34, "DatasetName", "0"], [210, 211, "DatasetName", "0"]]}
{"text": "In this section , we study the robustness of our CSANMT towards both noisy inputs and the translationese effect ( Volansky et al , 2013 ) on new - stest2014 for the WMT14 English - German task . Noisy Inputs . Inspired by ( Gao et al , 2019 ) , we construct noisy test sets via several strategies described as follows : Original : the original testset without any manipulations ; WS : word swap , randomly swap words in nearby positions within a window size 3 ( Artetxe et al , 2018 ; Lample et al , 2018b ) ; WD : word dropout , randomly drop words with a ratio of 15 % ( Iyyer et al , 2015 ; Lample et al , 2018b ) ; WR : word replace , randomly replace word tokens with a placeholder token ( e.g. , [ UNK ] ) ( Xie et al , 2017 ) or with a relevant ( measured by the similarity of word embeddings ) alternative . The replacement ratio also is 15 % . natural source translationese target ( X Y * ) ; translationese source natural target ( X * Y ) ; round - trip translationese source translationese target ( X * * Y * ) , where X Y * X * * . Results . As shown in Table 9 , our approach shows better robustness over two baseline methods across various artificial noises . Moreover , CSANMT consistently outperforms the baseline in all three translationese scenarios , the same is true for back - translation . However , Edunov et al ( 2020 ) shows that BT improves only in the X * Y scenario . Our explanation for the inconsistency is that BT without monolingual data in our setting benefits from the natural parallel data to deal with the translationese sources .", "entities": [[32, 33, "DatasetName", "WMT14"], [41, 42, "DatasetName", "Inspired"], [167, 169, "TaskName", "word embeddings"]]}
{"text": ") ) codes with 60 K merge operations to build two vocabularies comprising 47 K Chinese sub - words and 30 K English sub - words . For the En De task , we employ the popular WMT14 dataset , which consists of approximately 4.5 M sentence pairs for training . We select newstest2013 as the validation set and newstest2014 as the test set . All sentences had been jointly byte - pair - encoded with 32 K merge operations , which results in a shared source - target vocabulary of about 37 K tokens . For the En Fr task , we use the significantly larger WMT14 dataset consisting of 36 M sentence pairs . The combination of { newstest2012 , 2013 } was used for model selection and the experimental results were reported on newstest2014 .", "entities": [[37, 38, "DatasetName", "WMT14"], [107, 108, "DatasetName", "WMT14"], [127, 129, "TaskName", "model selection"]]}
{"text": "Traditionally , dialogue systems have been characterized in terms of whether they are task - or non - task - oriented . In task - oriented dialogue systems , such as an airline ticket reservation system ( Hemphill et al , 1990 ) , eliciting specific information from the user , such as the date , time , and destination of the flight , is an important functionality for completing the task . However , in non - task - oriented dialogue systems , the system does not have a clear goal of eliciting information from the user , and the content of the dialogue is free . In this study , as another type of dialogue system , we focus on interviewing systems , in which the goal is to acquire a user model through a flexible flow of dialogue . Specifically , we propose Figure 1 : Overview of the proposed method : taking dialogue history as input , a model predicts the interviewer 's intent ( communicative function ) , another model decides the content of the utterance ( semantic content ) , and the outputs of these models are combined to generate a response . ( For details , refer to Section 4 ) a method for interviewing a user 's preference for food . To generate such dialogues , the system must be able to generate appropriate questions to elicit the user 's preferences for food while touching on various topics in the food domain , such as how to eat , how to cook , etc . , without limiting the content of the dialogue as a task - oriented dialogue does . One possible approach for achieving the requirements discussed above is end - to - end neural network , where dialogue generation is the task of predicting the next utterance using dialogue history as input ( Vinyals and Le , 2015 ; . This method is widely used to generate open - domain dialogues , such as chitchats . However , it requires a large amount of dialogue data to learn the model . Otherwise , less informative and contextually inappropriate utterances are frequently generated . To overcome this drawback , we propose a method that first determines the intention and semantic content of the interviewer 's next utterance and then combines these to generate questions from the interviewer . Figure 1 shows the proposed approach . First , we trained two models . The first is a classification model that takes the dialogue history as input and determines the interviewer 's intention for the next utterance . The second is a generator model , which also takes the dialogue history as input and outputs the semantic content of the utterance , including the target ( e.g. , dish or ingredient ) mentioned in the utterance and its related information ( e.g. , taste or how to eat ) . Next , a template for sentence generation is selected based on these two outputs , and they are applied to the selected template to generate sentences . Compared to learning a model that directly generates a surface expression , the models for predicting the intent and semantic content of an utterance can be learned using a smaller amount of data . Additionally , because the content of an utterance is determined based on the context obtained from the dialogue history , appropriate utterances that are related to the preceding utterances can be generated . The contributions of this study are as follows : Collection of 118 text - based dialogues for interviewing food preferences . Proposal of an annotation schema for utterance intention and semantic content of utterances , and creation of a dataset with these annotations . Creation of a classification model for utterance intention and a generative model of semantic content of utterances . Demonstration of the effectiveness of the proposed method using an automated evaluation method . Presentation of examples of dialogues generated by the proposed method , and discussion of the quality of the dialogues .", "entities": [[23, 28, "TaskName", "task - oriented dialogue systems"], [78, 83, "TaskName", "task - oriented dialogue systems"], [298, 300, "TaskName", "dialogue generation"]]}
{"text": "Task - oriented dialog systems are typically designed to collect information from users . For example , previous studies have proposed an airline ticket reservation system ( TIS ) ( Hemphill et al , 1990 ) , a restaurant reservation system ( Henderson et al , 2014 ) , and interview systems to collect information , such as public opinion polls and class evaluation interview systems ( Johnston et al , 2013 ; Stent et al , 2006 ) . In these systems , the purpose of the dialogue is to obtain information to accomplish a predefined task . Meanwhile , chitchat does not have a clear goal as a task - oriented dialogue does , but this type of dialogue has the potential to elicit a variety of information from the user . For example , the system asks follow - up questions such as \" Please tell me more about the keyword \" by using a keyword from the user 's preceding utterance . To improve such interviewing functionality , relevant topics and questions should be selected and the dialogue strategies should be modified . To address these issues , we propose a method to determine the target object and semantic content of the system response based on the dialogue context . Previous studies on dialogue generation have proposed different techniques to generate task - and non - task - oriented dialogue . Early studies on generating open - domain chitchat proposed DNN - based techniques to generate system responses by exploiting the data - driven approach ( Sordoni et al , 2015a ; Vinyals and Le , 2015 ; . Recent studies have proposed incorporating useful information ( that is relevant to the domain ) and responses into the model , thus improving the quality of generated responses ( Li et al , 2018 ) . Some studies have exploited word - based information , such as nouns extracted from the user 's preceding utterances and a set of keywords predicted to be used in the response ( Serban et al , 2017 ; Xu et al , 2021 ) . Other studies have used knowledge ontologies , including commonsense ( Wu et al , 2020 ; Moon et al , 2019 ; Galetzka et al , 2021 ) . However , these end - to - end methods , in which training models directly generate system responses , require a large amount of training data , and our corpus was not sufficiently large for this approach . In traditional task - oriented dialogue systems , the information required to achieve the dialogue goals is limited to the task domain . Therefore , the internal state of the system is defined as a slot - value pair , and the system generates responses through the following modules : a ) understanding the user 's utterance , b ) determining the system action ( e.g. , the intention and the slot - value as the utterance content ) based on the internal state , and c ) generating a response sentence from the system action . The action of the system is determined by rule - based , statistical - based ( Young et al , 2010 ) , deep learning ( Chen et al , 2019 ) and reinforcement learning approaches ( Sankar and Ravi , 2019 ) . In this study , we exploited the approach described above , which represents the interviewer 's utterance as structured semantic content composed of the intent of the utterance , the objects mentioned in the utterance , and their attributes and values . We created a machine learning model to predict these types of information and generate responses based on the determined actions .", "entities": [[217, 219, "TaskName", "dialogue generation"], [423, 428, "TaskName", "task - oriented dialogue systems"]]}
{"text": "This section proposes a Communicative Function Prediction ( CFP ) model that predicts the communicative function label to specify the intention of the next interviewer 's message , such as selfdisclosure and questions . A fine - tuning approach was employed to train the CFP model . We used the BERT ( Devlin et al , 2019 ) Japanese pre - trained model 3 . As demonstrated in Figure 4 , the input is a dialogue context consisting of multiple previous messages concatenated using [ SEP ] . This sequence is the same as that used to train the SCG model in Section 4.1 . Using this sequence as the input , we trained a model that predicted the communicative function label of the interviewer 's next message . We use the representation of the final layer of the special classification token ( [ CLS ] ) , which is placed at the beginning of the input , as the input for a downstream classification task . As described in Section 5.1 , the communicative function classifier predicts 7 labels , reduced from the 32 labels presented in Section 3.2 .", "entities": [[50, 51, "MethodName", "BERT"]]}
{"text": "Table 4 presents the sequence of five context utterances and the interviewer 's utterance which follows the context . \" Human \" is the real interviewer utterance ( ground truth ) . \" Retrieval , \" \" Text Generation , \" and \" Proposed \" are the outputs by the methods examined in our experiment . In Dialogue - 1 in Table 4 , the interviewer utterance generated by the retrieval model asks whether the user eats vegetables . This utterance is not appropriate because in previous - 3 , the customer had already said that he / she eats vegetables . By contrast , the proposed model generated a question to elicit more information according to the current context of the hot - pot dish by asking the favorite ingredients for the dish . In Dialogue - 2 in Table 4 , all three models failed to generate an utterance about the current topic focus ( cheese ) , but the retrieval and text generation models still successfully generated a natural response . However , the utterance generated by the proposed model appears to be abrupt . This is because the selected template was not appropriate or expressive . Providing more templates and improving the template selection mechanism are necessary to generate more expressive responses .", "entities": [[37, 39, "TaskName", "Text Generation"], [164, 166, "TaskName", "text generation"]]}
{"text": "The # MeToo movement on social media platforms initiated discussions over several facets of sexual harassment in our society . Prior work by the NLP community for automated identification of the narratives related to sexual abuse disclosures barely explored this social phenomenon as an independent task . However , emotional attributes associated with textual conversations related to the # MeToo social movement are complexly intertwined with such narratives . We formulate the task of identifying narratives related to the sexual abuse disclosures in online posts as a joint modeling task that leverages their emotional attributes through multitask learning . Our results demonstrate that positive knowledge transfer via context - specific shared representations of a flexible cross - stitched parameter sharing model helps establish the inherent benefit of jointly modeling tasks related to sexual abuse disclosures with emotion classification from the text in homogeneous and heterogeneous settings . We show how for more domain - specific tasks related to sexual abuse disclosures such as sarcasm identification and dialogue act ( refutation , justification , allegation ) classification , homogeneous multitask learning is helpful , whereas for more general tasks such as stance and hate speech detection , heterogeneous multitask learning with emotion classification works better . 1", "entities": [[136, 138, "TaskName", "emotion classification"], [192, 195, "TaskName", "hate speech detection"], [200, 202, "TaskName", "emotion classification"]]}
{"text": "The # MeToo movement 2 was started as an initiative to empower women against long - standing issues related to sexual abuse at workplaces , public spaces , and private organizations ( McKenna and Chughtai , 2020 ) . The usage of a dedicated hashtag # MeToo on media platforms signified a social support system for women from different sections 1 Code & Implementation : https://github.com/ midas - research / metoo - mtl - naacl 2 https://metoomvmt.org/ of society . The movement initiated discussions on many socially stigmatized issues that were missing from the virtual space ( Clark - Parsons , 2019 ) . Such conversations invited various reactions on the web , involving support to the cause of the movement and even outright bullying . While many users took part in the vilification of the survivors , the movement also saw opposition by factions of the society that felt threatened by the impact of social media in raising awareness about the scale of everyday sexual harassment faced by women in workplaces and institutions ( Tambe , 2018 ) . In many instances , the public disclosures of survivor - narrated incidents involved widespread use of hate - language and online trolling , both against the victims and alleged oppressors ( Franks , 2019 ) . The # MeToo movement also led to people coming out with allegations , refutations , and justifications about traumatic experiences as they transitioned to active participants in the mainstream conversation . A closer look at the online posts about the # MeToo movement revealed that sarcasm was often used as a thin veil in such discussions to humorously mask disapproval , wit , and personal attacks ( Sandhu et al , 2019 ) . The complex narratives present in the conversations on stigmatized issues like sexual abuse create an opportunity for researchers to study how people express their opinions on a sensitive topic in an informal social setting . It also offers a chance to social media regulators for fostering social inclusion , community integration , and improving the individual perception of being supported by others . This paper aims at categorizing the posts related to the # MeToo movement on the basis of stance ( support or opposition ) , hate - speech , sarcasm , and dialogue acts ( allegation , refutation , or justification of sexual misconduct ) . We focus our analysis on a publicly available dataset that is created in the backdrop of mass instances of sexual harassment disclosures and includes nuanced labels to identify accompanying linguistic behaviors . Existing literature has emphasized that the text 's emotional attributes have a high correlation with dialogue narratives describing instances of sexual harassment ( Lane and Hedin , 2020 ) . Prior works ( Anzovino et al , 2018 ; Sharifirad et al , 2018 ) have mostly focused on label specific detection of linguistic narratives related to sexual harassment disclosures in isolation by exploiting lexical features ( Chowdhury et al , 2019 ; Karlekar and Bansal , 2018 ) . However , subtle intricacies present in the discussion of sexual abuse disclosures often reflect the speaker 's affective and psychological state , which are overlooked by feature - engineered models . For instance , part ( a ) of Figure 1 shows a tweet expressing support towards the # MeToo movement but in a tone that might be difficult for naive neural learning models to capture without context . Part ( b ) of Figure 1 presents a tweet in which the author has an initial positive outlook , which later reverses to disgust for the subject . The lack of context about the event and contrasting qualifications describing the oppressor makes the correct classification of the sexual harassment disclosure label extremely challenging for traditional classifiers without emotional labels ' additional supervision . Moreover , apart from their inherent complexity , conversations related to the # MeToo movement also pose a challenge of emotional ambiguity . This work is the first attempt at joint modeling of narratives related to sexual abuse disclosures and emotion classification to learn the patterns of their interaction via parameter sharing techniques offered by Multitask Learning ( MTL ) . The affective features , which result from a joint learning setup through shared parameters , will encompass the text 's emotional content that is likely to be predictive of narratives corresponding to sexual abuse disclo - sures . More specifically , we formulate an MTL framework for multi - label classification of narratives related to sexual abuse disclosures ( stance , hate - speech , sarcasm , dialogue acts ) and emotional classification in the context of the # MeToo movement . MTL ( Caruana , 1997 ) allows two or more related tasks to be learned jointly . This facilitates the transfer of inductive bias and better generalization across related tasks on account of shared representations of linguistic features .", "entities": [[683, 685, "TaskName", "emotion classification"], [751, 755, "TaskName", "multi - label classification"]]}
{"text": "We experiment with MTL architectures employing a flexible cross - stitched parameter sharing method that benefits from both hard - parameter sharing and soft parameter sharing through a gated mechanism using a weighted summation ( Section 4 ) . Hard parameter sharing allows for sharing lower - level word representations , and soft parameter sharing permits the sharing of task - specific networks . We explore two flavors of multitask learning : ( i ) Homogeneous MTL - Intradomain MTL between related tasks of sexual abuse disclosure narratives , and ( ii ) Heterogeneous MTL - cross - domain MTL between pairs of tasks in emotion classification and narratives of sexual abuse disclosure ( Section 5.2 ) . Our results demonstrate that both Homogeneous and Heterogeneous MTL setups outperform the Single Task Learning ( STL ) technique across various tasks ( Section 6 ) . Further , we conduct a qualitative analysis of several samples to analyze the benefit of joint training of related tasks ( Section 6.4 ) , keeping in mind the ethical concerns of communities affected by this research ( Section 7 ) .", "entities": [[105, 107, "TaskName", "emotion classification"]]}
{"text": "Sexual Harassment Disclosures on Social Media Several works have focused on identifying sexual violence ( Leatherman , 2011 ) , harassment and sexism ( Wekerle et al , 2018 ; Manikonda et al , 2018b ) in social media posts by analyzing factors such as linguistic themes , social engagement , and lexical attributes . Jha and Mamidi ( 2017 ) experimented with algorithms such as SVM and BiLSTM along with fastText to categorize hostility of sexist posts . ( Parikh et al , 2019 ) proposed a multi - label CNN - based neural architecture along with word and sentence level embeddings for identifying variants of sexism present in online social platforms . Chowdhury et al ( 2019 ) emphasized the use of linguistic themes , contextual meta - data , and semantic cues for evaluating human behaviors related to sex - ual abuse disclosures . All of these works have dealt with modeling sexual disclosure narratives as single - task learning problems and were restricted to label specific detection ( Marwa et al , 2018 ; . Multitask Learning Frameworks for learning representations across two different sources within the same domain follow multitask learning ( Caruana , 1997 ) . The ability to utilize knowledge from various sources compensates for missing data and complements existing meta - data ( Tan et al , 2013 ; Ding et al , 2014 ) , thus allowing for effective sharing of task - invariant features ( Caruana , 1997 ; Zhang and Wang , 2016 ; Zhang et al , 2018 ) . MTL has been utilized for name error recognition ( Cheng et al , 2015 ) , tagging - chunking ( Collobert et al , 2011 ) , machine translation ( Luong et al , 2015 ) and relation extraction ( Gupta et al , 2016 ) . Liu et al ( 2017 ) used shared and private latent features leveraging multitask learning for different text classification tasks . Rajamanickam et al ( 2020 ) ; Duong et al ( 2016 ) ; Liu et al ( 2016 ) proposed a joint framework for modeling abuse and emotion detection and showed improvements over STL and transfer learning . Akhtar et al ( 2018 ) proposed a multitask ensemble architecture for jointly modeling emotion , sentiment , and intensity , which gave improvements over single - label classification .", "entities": [[66, 67, "MethodName", "SVM"], [68, 69, "MethodName", "BiLSTM"], [71, 72, "MethodName", "fastText"], [280, 281, "TaskName", "chunking"], [289, 291, "TaskName", "machine translation"], [299, 301, "TaskName", "relation extraction"], [326, 328, "TaskName", "text classification"], [358, 359, "DatasetName", "emotion"], [366, 368, "TaskName", "transfer learning"], [383, 384, "DatasetName", "emotion"]]}
{"text": "We aim to analyze different perspectives of the complex narratives pertaining to the # MeToo movement on social media platforms . Specifically , given a tweet text , we formulate for it a multi - label multiclass classification problem with definitions taken from previous works ( ElSherief et al , 2018 ) Stance Detection : Determining the opinion of the author of a tweet , regarding a particular target of interest ( Augenstein et al , 2016 ) . Stance detection is categorized into three classes : Support for when the author favors the # MeToo movement or it 's cause ; Opposition , representing opposing stance or indifference towards the movement ; or Neither , when the text does not have a clear viewpoint ( Mohammad and Turney , 2013 ) . Hate Speech Identification : Detection of hate speech involves labeling the tweets as Directed Hate if the comment is targeted towards an individual or an entity , Generalized Hate if it is targeted towards a community or a section of people or Neither otherwise ( Basile et al , 2019 ) . Sarcasm Detection : Given a tweet t i , we aim to map it to either be Sarcastic or Not Sarcastic based on the presence of implicit sarcastic tone of the post ( Bamman and Smith , 2015 ) . Dialogue Act Classification : These are a function of a speaker 's utterance during a conversation , for example , question , answer , suggestion , etc . , and are classified into three classes , namely Allegation ( when the author intends to allege an individual or group of sexual misconduct ) ( Hutchings , 2012 ) , Justification ( tweets where the author is justifying their actions ) , and Refutation ( for when the author refutes any accusation with or without evidence ) . Modeling Settings To validate MTL 's performance across different domains , we also experiment with emotion detection as the auxiliary task . We aim to predict one or more of the several emotions representing the affective state of the authors - ( anger , disgust , anticipation , fear , joy , love , optimism , pessimism , sadness , surprise and trust ) . We conceptualize three diverse problem settings and compare them to analyze MTL within and across domains . These are ( i ) Single Task Learning : Independent optimization of the four mentioned tasks associated with sexual abuse disclosure narrative classification , ( ii ) Homogeneous Multitask Learning : Simultaneous optimization of a pair selected from the four tasks associated with the sexual abuse disclosure posts , and ( iii ) Heterogeneous Multitask Learning : Classification of narratives associated with sexual abuse disclosure as the primary task and emotion detection as the auxiliary task .", "entities": [[52, 54, "TaskName", "Stance Detection"], [79, 81, "TaskName", "Stance detection"], [133, 135, "DatasetName", "Hate Speech"], [139, 141, "DatasetName", "hate speech"], [185, 187, "TaskName", "Sarcasm Detection"], [225, 228, "TaskName", "Dialogue Act Classification"], [327, 328, "DatasetName", "emotion"], [451, 452, "TaskName", "Classification"], [464, 465, "DatasetName", "emotion"]]}
{"text": "Building on the success of transformer - based models in NLP , we chose BERTweet ( Dat Quoc Nguyen and Nguyen , 2020 ) , a pre - trained language model trained on 850 million English tweets . BERTweet has been trained with the same training procedure as RoBERTa ( Liu et al , 2019 ) and has the same model configuration as the BERT base architecture ( Devlin et al , 2019 ) . The key component in transformer - based models is the token level selfattention ( Vaswani et al , 2017 ) that enables them to generate dynamic contextualized embeddings as opposed to static embeddings of GloVe ( Pennington et al , 2014 ) . Let ( w 1 , w 2 , ... , w n ) represent the sequence of tokens from a given tweet t. These tokens are pre - processed and passed through BERTweet 3 . We consider embeddings from the last layer of BERTweet and obtain an embedding e i for a given tweet t i . Embedding for each tweet is of dimension m \u00d7 k , where k represents the dimension size of BERT based model and m represents the maximum length for the tweets . e i = BERT weet ( t i ) ( 1 ) These representations from Equation 1 are passed through a stacked BiLSTM encoder . Dropout is then applied to these encoded representations h ( t ) ( Equation 4represents general formulation for both the tasks ) . These are then passed to a BiLSTM decoder , followed by a dropout layer and then a linear output layer to get output o ( p ) ( p representing primary task ) or o ( a ) ( a representing auxiliary task ) . \u2212 \u2212 h ( f ) t = BiLST M ( f ) ( e t , h ( f ) t\u22121 ) ( 2 ) \u2212 \u2212 h ( b ) t = BiLST M ( b ) ( e t , h ( b ) t+1 ) ( 3 ) h t = [ \u2212 \u2212 h ( f ) t , \u2212\u2212 h ( b ) T \u2212t ] ( 4 )", "entities": [[48, 49, "MethodName", "RoBERTa"], [64, 65, "MethodName", "BERT"], [109, 110, "MethodName", "GloVe"], [193, 194, "MethodName", "BERT"], [209, 210, "MethodName", "BERT"], [228, 229, "MethodName", "BiLSTM"], [231, 232, "MethodName", "Dropout"], [260, 261, "MethodName", "BiLSTM"]]}
{"text": "Single Task Learning STL experiments optimize each of the tasks associated with identifying narratives related to sexual abuse disclosures within # MeToo movement ( Section 3 ) and emotion 4 The publicly available dataset can be found at https : //doi.org/10.7910 / DVN / JN4EYU . 5 https://competitions.codalab.org/ competitions/17751 detection , independently . We experiment with two distinct embedding spaces - GloVe - Twitter and BERTweet . Based on the superior performance of BERTweet with respect to GloVe - Twitter , we preferred it for further experimentation and studies . Homogeneous Multitask Learning For this setup , we test the simultaneous optimization of two different tasks - both related to sexual harassment disclosure narratives , with one of them being primary and another coupled as the auxiliary . The results were obtained for a total of 12 pairs . Heterogeneous Multitask Learning In these sets of experiments , we evaluate the positive transfer of representations across datasets by considering the identification of narratives associated with sexual abuse disclosure as the primary task and emotion detection as the auxiliary task .", "entities": [[28, 29, "DatasetName", "emotion"], [61, 62, "MethodName", "GloVe"], [77, 78, "MethodName", "GloVe"], [173, 174, "DatasetName", "emotion"]]}
{"text": "The aim of this paper is not limited to achieving the state of the art performance in terms of evaluation metrics but rather to conduct a thorough study to compare and contrast different methodologies for the benefit of the research community . As per our hypothesis and preliminary results on STL experiments on the # MeTooMA dataset , models trained using BERTweet embeddings perform far better than GloVe - Twitter . This is largely true because BERTweet is specifically pre - trained on English tweets and is better suited to handle Twitter - specific data , typically having a short length , informal grammar , and irregular vocabulary ( e.g. , abbreviations and typographical errors ) ( Kireyev et al , 2009 ) .", "entities": [[54, 56, "DatasetName", "# MeTooMA"], [67, 68, "MethodName", "GloVe"]]}
{"text": "Results in highly correlated to emotion recognition . This is indicative of positive knowledge transfer between the two domains . Such joint optimization boosts the overall performance of both primary and auxiliary tasks through parameter sharing to learn common representations that may be mutually beneficial to both related tasks .", "entities": [[5, 7, "TaskName", "emotion recognition"]]}
{"text": "To emphasize our proposed approach , we perform a qualitative study by handpicking examples from the dataset . We analyze token - level attention assigned to individual terms by BERTweet , where color intensity corresponds to the attention score . These results are shown in Table 4 . We infer that Homogeneous and Heterogeneous multitask learning shows superior performance in every instance compared to STL . Learning effective features across the joint formulation of pair - wise tasks in Homogeneous MTL is evident from T 4 , where BERT 's self - attention allots a higher weight to words such as ideology , stigma , and forward in line with the actual label as Support . Similarly for T 5 , highlighted terms such as trap and bait are indicative of the opposing nature of the tweets , hence identified as belonging to Refutation . On the other hand , due to positive knowledge transfer from the emotion recognition task , Heterogeneous MTL obtains better performance in several cases . Words such as grave , mistake and swindling in T 2 connoted a negative emotion , hence accordingly being identified as belonging to the Oppose category . Similarly , terms such as hope and pain were given higher token - level attention in T 1 emphasizing a positive emotion and thus can be correlated with belonging to the Support category . An interesting observation is the presence of named entities in T 5 and T 6 , resulting in the incorrect prediction via Heterogeneous MTL . Therefore , a limitation of the single task learning and Heterogeneous MTL is the inability to mitigate the effect of named entities or specific events in the text to influence the knowledge transfer and create negative shared representations .", "entities": [[88, 89, "MethodName", "BERT"], [157, 159, "TaskName", "emotion recognition"], [184, 185, "DatasetName", "emotion"], [218, 219, "DatasetName", "emotion"]]}
{"text": "In this work , we have proposed a flexible crossstitched multitask learning framework for the de - tection of narratives linked with sexual abuse disclosure on social media . Our methodology takes advantage of the affective features from emotions and related tasks to encourage knowledge transfer and attain auxiliary knowledge . Qualitative and quantitative results demonstrate how joint optimization of Stance detection and Sarcasm identification benefit each other , indicating their relatedness and dependence on each other . Similarly , we observe that tasks like Hate - Speech classification and Stance labeling benefit from each other and from emotion detection , thus reinforcing the benefit of joint linguistic learning between the related tasks . In the future , we aim to explore how this joint learning paradigm can be effectively leveraged for improving performance on downstream tasks like emotion analysis , identifying suicidal tendencies among abuse survivors . Application from this work also has utility for problems such as identification of patterns of reported sexual harassment narratives , hate speech detection , the spread of rumors and fake news , and entity extraction for digital vigilantism ( Yuce et al , 2014 ; Hosterman et al , 2018 ) .", "entities": [[60, 62, "TaskName", "Stance detection"], [98, 99, "DatasetName", "emotion"], [138, 139, "DatasetName", "emotion"], [168, 171, "TaskName", "hate speech detection"]]}
{"text": "The Power of Prompt Tuning for Low - Resource Semantic Parsing", "entities": [[9, 11, "TaskName", "Semantic Parsing"]]}
{"text": "Prompt tuning has recently emerged as an effective method for adapting pre - trained language models to a number of language understanding and generation tasks . In this paper , we investigate prompt tuning for semantic parsing - the task of mapping natural language utterances onto formal meaning representations . On the low - resource splits of Overnight and TOPv2 , we find that a prompt tuned T5 - xl significantly outperforms its fine - tuned counterpart , as well as strong GPT - 3 and BART baselines . We also conduct ablation studies across different model scales and target representations , finding that , with increasing model scale , prompt tuned T5 models improve at generating target representations that are far from the pre - training distribution .", "entities": [[35, 37, "TaskName", "semantic parsing"], [59, 60, "DatasetName", "TOPv2"], [67, 68, "MethodName", "T5"], [82, 83, "MethodName", "GPT"], [86, 87, "MethodName", "BART"], [112, 113, "MethodName", "T5"]]}
{"text": "With the widespread success of pre - trained language models ( LMs ; Devlin et al 2019 ; Raffel et al 2020 ; Bommasani et al 2021 ) , it becomes increasingly important to explore how such models can be adapted to downstream tasks . One adaptation method which has recently attracted much attention is prompt design ( Brown et al , 2020 ; Shin et al , 2020 ) , which modulates the behaviour of a LM through a task description and a few inputoutput examples . Brown et al ( 2020 ) show that this adaptation strategy is increasingly effective for larger LMs . However , prompt design is sensitive to the exact phrasing of the prompt , and , more importantly , performs worse than fine - tuning models on task - specific examples ( Lester et al , 2021 ) . Prompt tuning has recently arisen as a strong performing alternative adaption method ( Lester et al , 2021 ) . Rather than hand - designing discrete prompts , prompt tuning optimizes the embeddings of a number of task - specific prompt tokens . In contrast to fine - tuning , this method keeps almost all LM parameters frozen . On a set of language understanding tasks , Lester et al ( 2021 ) show that prompt tuning becomes competitive with finetuning for the largest pre - trained T5 models ( Raffel et al , 2020 ) . Li and Liang ( 2021 ) also explore a related parameter - efficient adaptation method called prefix - tuning , finding that it outperforms fine - tuning on low - resource natural language generation tasks . In this paper , we investigate prompt tuning for semantic parsing . This task is fundamentally different from the aforementioned language understanding and generation tasks , as it requires that models output formal meaning representations which do not resemble the natural language distribution seen during pre - training . In particular , we focus on the low - resource setup because examples for semantic parsing are difficult and expensive to collect ( Wang et al , 2015 ; Marzoev et al , 2020 ) . We therefore evaluate prompt tuning on two datasets : the 200 - shot version of Overnight ( Wang et al , 2015 ; Shin et al , 2021 ) and the low - resource splits TOPv2 ( Chen et al , 2020 ) . On both datasets , we compare prompt tuning T5 against fine - tuning and investigate the effect of canonicalizing the meaning representation , i.e. to what extent naturalizing the logical forms influences performance . In addition , we study the effect of T5 model scale on Overnight as well as varying data regimes on TOPv2 . Our main findings can be summarized as follows : For large T5 models , prompt tuning significantly outperforms fine - tuning in the low - data regime , resulting in an absolute improvement of 6 % and 15 % on Overnight and TOPv2 , respectively . This performance gap decreases when more training data becomes available . With growing model size , prompt tuned T5 models are increasingly capable of outputting diverse target representations ( see Figure 1 ) . On Overnight , we find that the disparity between canonical and meaning representations shrinks from 17 % to 4 % for T5 - small and T5 - xl , respectively . On TOPv2 , prompt tuned T5 - large models are much better at generating out - of - vocabulary tokens than T5 - small .", "entities": [[232, 233, "MethodName", "T5"], [287, 289, "TaskName", "semantic parsing"], [341, 343, "TaskName", "semantic parsing"], [398, 399, "DatasetName", "TOPv2"], [415, 416, "MethodName", "T5"], [449, 450, "MethodName", "T5"], [461, 462, "DatasetName", "TOPv2"], [474, 475, "MethodName", "T5"], [505, 506, "DatasetName", "TOPv2"], [527, 528, "MethodName", "T5"], [564, 565, "MethodName", "T5"], [568, 569, "MethodName", "T5"], [575, 576, "DatasetName", "TOPv2"], [579, 580, "MethodName", "T5"], [595, 596, "MethodName", "T5"]]}
{"text": "Our work is related to recent work on semantic parsing and prompt tuning , which we briefly describe below .", "entities": [[8, 10, "TaskName", "semantic parsing"]]}
{"text": "Semantic parsing is the task of converting a natural language utterance u = ( u 1 , . . . , u N ) to a formal meaning representation z = ( z 1 , . . . , z M ) . These meaning representations , also referred to as logical forms , can be interpreted by machines and executed in a real environment . For example , ThingTalk ( Campagna et al , 2019 ) and TOP ( Gupta et al , 2018 ) are meaning representations for executing commands of virtual assistants , while SQL is a representation for interacting with relational databases . In recent years , neural sequence - to - sequence models have become the dominant approach for semantic parsing tasks ( Dong and Lapata , 2016 ) . Canonicalization A common simplification step in semantic parsing is to canonicalize the meaning representations . That is , the meaning representation z is naturalized to a canonical form c through a grammar or set of rules . Examples of the meaning and canonical representation for Overnight and TOPv2 ( Wang et al , 2015 ; Chen et al , 2020 ) can be found in Fig . 2 . When canonical representations are available , Berant and Liang ( 2014 ) argue that semantic parsing can be seen as a paraphrase task . They propose to use a paraphrase model - using e.g. word vectors trained on Wikipedia - to find the best paraphrase of utterance u among a set of canonical utterances . They show this paraphrase model improves results over directly generating logical forms on two question - answering datasets . Marzoev et al ( 2020 ) extends this work by showing that pre - trained language models like BERT can be effective paraphrasers . While Berant and Liang ( 2014 ) ; Marzoev et al ( 2020 ) use models to score canonical utterances , Shin et al ( 2021 ) propose to constrain the generation process of autoregressive models like BART and GPT - 3 . On a number of few - shot semantic parsing tasks , they demonstrate the benefit of generating canonical representations over meaning representations .", "entities": [[0, 2, "TaskName", "Semantic parsing"], [124, 126, "TaskName", "semantic parsing"], [141, 143, "TaskName", "semantic parsing"], [182, 183, "DatasetName", "TOPv2"], [218, 220, "TaskName", "semantic parsing"], [296, 297, "MethodName", "BERT"], [339, 340, "MethodName", "BART"], [341, 342, "MethodName", "GPT"], [352, 354, "TaskName", "semantic parsing"]]}
{"text": "Lester et al ( 2021 ) evaluates prompt tuning on SuperGLUE , a benchmark consisting of eight language understanding tasks . They find that prompt tuning becomes competitive with fine - tuning for the largest T5 model . Li and Liang ( 2021 ) propose prefix - tuning to adapt BART and GPT - 2 for natural language generation tasks . This method differs from Lester et al ( 2021 ) in that it prepends trainable embeddings for each layer of the language model rather than introducing token embeddings at the input layer . They demonstrate that pre - fix outperforms fine - tuning baselines . Similarly , Liu et al ( 2021 ) also show encouraging results for prompt tuning on natural language understand and generation tasks . Qin and Eisner ( 2021 ) also explores prompt tuning but for a knowledge extraction task . Inserting general adapter layers into pre - trained language models is also proposed in Houlsby et al ( 2019 ) ; Mahabadi et al ( 2021 ) . Related to our work are also other few - shot adaptation techniques like PET ( Schick and Sch\u00fctze , 2021 ) . Moreover , adapter layers have also been explored in the computer vision domain ( Rebuffi et al , 2017 ; de Vries et al , 2017 ) .", "entities": [[10, 11, "DatasetName", "SuperGLUE"], [35, 36, "MethodName", "T5"], [50, 51, "MethodName", "BART"], [52, 53, "MethodName", "GPT"], [187, 188, "DatasetName", "PET"]]}
{"text": "To evaluate low - resource prompt tuning , we compare against fine - tuned variants of the same model on two semantic parsing datasets with canonical representations available . We compare both large and small variants of the T5 architecture on these datasets and experiment with various canonicalized representations .", "entities": [[21, 23, "TaskName", "semantic parsing"], [38, 39, "MethodName", "T5"]]}
{"text": "Chen et al apply a set of simple modifications to the TOPv2 meaning representations to arrive at a canonical form used in all their experiments . Unlike Overnight , these pre - processing steps are largely small encoding differences and do not change the syntactic structure of the logical forms . We adopt all of these canonicalization steps ( except for lexicographic sorting of the semantic parse tree ) and add an ontology label shortening step . Examples of these transformations can be seen in Fig . 2 and are briefly described below . Simplify removes redundant utterance tokens unnecessary for interpreting the meaning representation . Out - of - Vocab adds the entire intent or slot label to the tokenizer as a new single tokens with a corresponding randomly initialized embedding . In - Vocab replaces the intent and slot labels with a short unique identifier representable by the pre - trained tokenizer . We perform an ablation over these canonicalization choices , repeating each experiment three times with varying random seed . For each domain , we report the average over 5 runs trained on randomly sampled splits of 200 examples for fine - tuned ( FT ) and prompt tuned ( PT ) models .", "entities": [[11, 12, "DatasetName", "TOPv2"], [72, 73, "MethodName", "ontology"]]}
{"text": "In ( 2020 ) . In Table 4 , we summarize the results of the canonicalization ablation study for TOPv2 .", "entities": [[19, 20, "DatasetName", "TOPv2"]]}
{"text": "Our main finding is that prompt tuned T5 models become better at generating meaning representations with increased model size . On Overnight , we see the absolute difference between canonical and meaning representations shrink from 17.5 points for T5 - small to 3.4 points for T5 - xl ( Table 1 ) . This gap shrinks another 18 % to 2.8 points when we apply constrained decoding to T5 - xl ( Table 2 ) . By contrast , Shin et al ( 2021 ) reports an 11.7 point difference when prompting GPT - 3 . For our finetuning baselines , we observe a small performance gap of 4 points across target representations for BART and T5 - xl , while we observe no gap for T5 - small , T5 - base , and T5 - large models . In our TOPv2 experiments we find similar evidence of large T5 model flexibility for generating sequences far from the training distribution . In particular , for our most intrusive canonicalization scheme Out - of - Vocab , which adds novel tokens to the vocabulary and leaves these embeddings un - trained , we find no significant reduction in performance for T5 - large across all data resource levels . T5 - small , in comparison , sees almost a 50 % drop in performance relative to no canonicalization ( None ) at the 10 SPIS level and continues to underperform by 33 % at the 500 SPIS level . Interestingly , we find that In - Vocab drastically reduces performance for T5 - small at the 10 SPIS level - 30.9 % vs. 43.4 % for None - but slightly outperforms it at 500 SPIS . We speculate that In - Vocab effectively anonymizes the ontol - ogy tokens , obscuring information that is useful for prediction . In low - data regimes there is not enough training data to learn the semantics of these anonymized tokens , whereas with enough data this problem vanishes .", "entities": [[7, 8, "MethodName", "T5"], [38, 39, "MethodName", "T5"], [45, 46, "MethodName", "T5"], [68, 69, "MethodName", "T5"], [92, 93, "MethodName", "GPT"], [114, 115, "MethodName", "BART"], [116, 117, "MethodName", "T5"], [126, 127, "MethodName", "T5"], [130, 131, "MethodName", "T5"], [135, 136, "MethodName", "T5"], [142, 143, "DatasetName", "TOPv2"], [150, 151, "MethodName", "T5"], [200, 201, "MethodName", "T5"], [209, 210, "MethodName", "T5"], [261, 262, "MethodName", "T5"]]}
{"text": "We find that prompt tuning is an effective method for adapting language models to the semantic parsing task . Prompt tuning significantly outperforms fine - tuning in low - data regimes , and remains competitive in the fully supervised setting . We furthermore find that while canonicalizing meaning representations can slightly improve performance , the disparity between target representations decreases when prompt tuning larger T5 models . This result differs from previous work ( Shin et al , 2021 ) which suggested that pre - trained LMs are much better equipped to output canonical than meaning representations . However , a significant limitation of prompt tuning is that it takes more time to converge than fine - tuning . We believe one fruitful direction for future research is to find ways to reduce the compute required to prompt tune .", "entities": [[15, 17, "TaskName", "semantic parsing"], [64, 65, "MethodName", "T5"]]}
{"text": "Prompt tuned parameter efficiency comes at a cost : we find that prompt tuning takes significantly longer to train with early stopping than does finetuning . On the Overnight dataset , fine - tuned models typically took 250 epochs before validation performance plateaued . Our prompt tuned models frequently took more than 1000 epochs when predicting canonical representations , and up to 5 , 000 when predicting meaning representations . In Figure 3 , we show example training curves for prompt tuning and fine - tuning .", "entities": [[20, 22, "MethodName", "early stopping"]]}
{"text": "5 : Results across all model size , target representation , tuning method , and decoding method for Overnight dataset . BART , GPT - 2 , and GPT - 3 results results are included from Shin et al ( 2021 )", "entities": [[21, 22, "MethodName", "BART"], [23, 24, "MethodName", "GPT"], [28, 29, "MethodName", "GPT"]]}
{"text": "Many NLP learning tasks can be decomposed into several distinct sub - tasks , each associated with a partial label . In this paper we focus on a popular class of learning problems , sequence prediction applied to several sentiment analysis tasks , and suggest a modular learning approach in which different sub - tasks are learned using separate functional modules , combined to perform the final task while sharing information . Our experiments show this approach helps constrain the learning process and can alleviate some of the supervision efforts .", "entities": [[39, 41, "TaskName", "sentiment analysis"]]}
{"text": "Many natural language processing tasks attempt to replicate complex human - level judgments , which often rely on a composition of several sub - tasks into a unified judgment . For example , consider the Targeted - Sentiment task ( Mitchell et al , 2013 ) , assigning a sentiment polarity score to entities depending on the context that they appear in . Given the sentence \" according to a CNN poll , Green Book will win the best movie award \" , the system has to identify both entities , and associate the relevant sentiment value with each one ( neutral with CNN , and positive with Green Book ) . This task can be viewed as a combination of two tasks , entity identification , locating contiguous spans of words corresponding to relevant entities , and sentiment prediction , specific to each entity based on the context it appears in . Despite the fact that this form of functional task decomposition is natural for many learning tasks , it is typically ignored and learning is defined as a monolithic process , combining the tasks into a single learning problem . Our goal in this paper is to take a step towards modular learning architectures that exploit the learning tasks ' inner structure , and as a result simplify the learning process and reduce the annotation effort . We introduce a novel task decomposition approach , learning with partial labels , in which the task output labels decompose hierarchically , into partial labels capturing different aspects , or sub - tasks , of the final task . We show that learning with partial labels can help support weakly - supervised learning when only some of the partial labels are available . Given the popularity of sequence labeling tasks in NLP , we demonstrate the strength of this approach over several sentiment analysis tasks , adapted for sequence prediction . These include target - sentiment prediction ( Mitchell et al , 2013 ) , aspect - sentiment prediction ( Pontiki et al , 2016 ) and subjective text span identification and polarity prediction . To ensure the broad applicability of our approach to other problems , we extend the popular LSTM - CRF ( Lample et al , 2016 ) model that was applied to many sequence labeling tasks 1 . The modular learning process corresponds to a task decomposition , in which the prediction label , y , is deconstructed into a set of partial labels { y 0 , .. , y k } , each defining a sub - task , capturing a different aspect of the original task . Intuitively , the individual sub - tasks are significantly easier to learn , suggesting that if their dependencies are modeled correctly when learning the final task , they can constrain the learning problem , leading to faster convergence and a better overall learning outcome . In addition , the modular approach helps alleviate the supervision problem , as often providing full supervision for the overall task is costly , while providing additional partial labels is significantly easier . For example , annotating entity segments syntactically is considerably easier than determining their associated sentiment , which requires understanding the nuances of the context they appear in semantically . By exploiting modularity , the entity segmentation partial labels can be used to help improve that specific aspect of the overall task . Our modular task decomposition approach is partially inspired by findings in cognitive neuroscience , namely the two - streams hypothesis , a widely accepted model for neural processing of cognitive information in vision and hearing ( Eysenck and Keane , 2005 ) , suggesting the brain processes information in a modular way , split between a \" where \" ( dorsal ) pathway , specialized for locating objects and a \" what \" ( ventral ) pathway , associated with object representation and recognition ( Mishkin et al , 1983 ; Geschwind and Galaburda , 1987 ; Kosslyn , 1987 ; Rueckl et al , 1989 ) . Jacobs et al ( 1991 ) provided a computational perspective , investigating the \" what \" and \" where \" decomposition on a computer vision task . We observe that this task decomposition naturally fits many NLP tasks and borrow the notation . In the target - sentiment tasks we address in this paper , the segmentation tagging task can be considered as a \" where \" - task ( i.e. , the location of entities ) , and the sentiment recognition as the \" what \" - task . Our approach is related to multi - task learning ( Caruana , 1997 ) , which has been extensively applied in NLP ( Toshniwal et al , 2017 ; Eriguchi et al , 2017 ; Collobert et al , 2011 ; Luong , 2016 ; Liu et al , 2018 ) . However , instead of simply aggregating the objective functions of several different tasks , we suggest to decompose a single task into multiple inter - connected sub - tasks and then integrate the representation learned into a single module for the final decision . We study several modular neural architectures , which differ in the way information is shared between tasks , the learning representation associated with each task and the way the dependency between decisions is modeled . Our experiments were designed to answer two questions . First , can the task structure be exploited to simplify a complex learning task by using a modular approach ? Second , can partial labels be used effectively to reduce the annotation effort ? To answer the first question , we conduct experiments over several sequence prediction tasks , and compare our approach to several recent models for deep structured prediction ( Lample et al , 2016 ; Ma and Hovy , 2016 ; Liu et al , 2018 ) , and when available , previously published results ( Mitchell et al , 2013 ; Zhang et al , 2015 ; Li and Lu , 2017 ; Ma et al , 2018 ) We show that modular learning indeed helps simplify the learning task compared to traditional monolithic approaches . To answer the second question , we evaluate our model 's ability to leverage partial labels in two ways . First , by restricting the amount of full labels , and observing the improvement when providing increasing amounts of partial labels for only one of the sub - tasks . Second , we learn the sub - tasks using completely disjoint datasets of partial labels , and show that the knowledge learned by the sub - task modules can be integrated into the final decision module using a small amount of full labels . Our contributions : ( 1 ) We provide a general modular framework for sequence learning tasks . While we focus on sentiment analysis task , the framework is broadly applicable to many other tagging tasks , for example , NER ( Carreras et al , 2002 ; Lample et al , 2016 ) and SRL ( Zhou and Xu , 2015 ) , to name a few . ( 2 ) We introduce a novel weakly supervised learning approach , learning with partial labels , that exploits the modular structure to reduce the supervision effort . ( 3 ) We evaluated our proposed model , in both the fullysupervised and weakly supervised scenarios , over several sentiment analysis tasks .", "entities": [[311, 313, "TaskName", "sentiment analysis"], [370, 371, "MethodName", "LSTM"], [372, 373, "MethodName", "CRF"], [419, 420, "DatasetName", "0"], [776, 780, "TaskName", "multi - task learning"], [970, 972, "TaskName", "structured prediction"], [1156, 1158, "TaskName", "sentiment analysis"], [1174, 1175, "TaskName", "NER"], [1251, 1253, "TaskName", "sentiment analysis"]]}
{"text": "From a technical perspective , our task decomposition approach is related to multi - task learning ( Caruana , 1997 ) , specifically , when the tasks share information using a shared deep representation ( Collobert et al , 2011 ; Luong , 2016 ) . However , most prior works aggregate multiple losses on either different pre - defined tasks at the final layer ( Collobert et al , 2011 ; Luong , 2016 ) , or on a language model at the bottom level ( Liu et al , 2018 ) . This work suggests to decompose a given task into sub - tasks whose integration comprise the original task . To the best of our knowledge , Ma et al ( 2018 ) , focusing on targeted sentiment is most similar to our approach . They suggest a joint learning approach , modeling a sequential relationship between two tasks , entity identification and target sentiment . We take a different approach viewing each of the model components as a separate module , predicted independently and then integrated into the final decision module . As we demonstrate in our experiments , this approach leads to better performance and increased flexibil - ity , as it allows us to decouple the learning process and learn the tasks independently . Other modular neural architectures were recently studied for tasks combining vision and language analysis ( Andreas et al , 2016 ; Hu et al , 2017 ; Yu et al , 2018 ) , and were tailored for the grounded language setting . To help ensure the broad applicability of our framework , we provide a general modular network formulation for sequence labeling tasks by adapting a neural - CRF to capture the task structure . This family of models , combining structured prediction with deep learning showed promising results ( Gillick et al , 2015 ; Lample et al , 2016 ; Ma and Hovy , 2016 ; Zhang et al , 2015 ; Li and Lu , 2017 ) , by using rich representations through neural models to generate decision candidates , while utilizing an inference procedure to ensure coherent decisions . Our main observation is that modular learning can help alleviate some of the difficulty involved in training these powerful models .", "entities": [[12, 16, "TaskName", "multi - task learning"], [289, 290, "MethodName", "CRF"], [302, 304, "TaskName", "structured prediction"]]}
{"text": "Using neural networks to generate emission potentials in CRFs was applied successfully in several sequence prediction tasks , such as word segmentation ( Chen et al , 2017 ) , NER ( Ma and Hovy , 2016 ; Lample et al , 2016 ) , chunking and PoS tagging ( Liu et al , 2018 ; Zhang et al , 2017 ) . A sequence is represented as a sequence of L tokens : x = [ x 1 , x 2 , . . . , x L ] , each token corresponds to a label y Y , where Y is the set of all possible tags . An inference procedure is designed to find the most probable sequence y * = [ y 1 , y 2 , . . . , y L ] by solving , either exactly or approximately , the following optimization problem : y * = arg max y P ( y | x ) . Despite the difference in tasks , these models follow a similar general architecture : ( 1 ) Characterlevel information , such as prefix , suffix and capitalization , is represented through a character embedding layer learned using a bi - directional LSTM ( BiLSTM ) . ( 2 ) Word - level information is obtained through a word embedding layer . ( 3 ) The two representations are concatenated to represent an input token , used as input to a word - level BiLSTM which generates the emission potentials for a succeeding CRF . ( 4 ) The CRF is used as an inference layer to generate the globally - normalized probability of possible tag sequences .", "entities": [[30, 31, "TaskName", "NER"], [45, 46, "TaskName", "chunking"], [205, 206, "MethodName", "LSTM"], [207, 208, "MethodName", "BiLSTM"], [247, 248, "MethodName", "BiLSTM"], [256, 257, "MethodName", "CRF"], [262, 263, "MethodName", "CRF"]]}
{"text": "A CRF model describes the probability of predicted labels y , given a sequence x as input , as , \u1ef9 ) is the partition function that marginalize over all possible assignments to the predicted labels of the sequence , and \u03a6 ( x , y ) is the scoring function , which is defined as : P \u039b ( y | x ) = e \u03a6 ( x , y ) Z , where Z = \u1ef9 e \u03a6 ( x \u03a6 ( x , y ) = t \u03c6 ( x , y t ) + \u03c8 ( y t\u22121 , y t ) . The partition function Z can be computed efficiently via the forward - backward algorithm . The term \u03c6 ( x , y t ) corresponds to the score of a particular tag y t at position t in the sequence , and \u03c8 ( y t\u22121 , y t ) represents the score of transition from the tag at position t \u2212 1 to the tag at position t. In the Neural CRF model , \u03c6 ( x , y t ) is generated by the aforementioned Bi - LSTM while \u03c8 ( y t\u22121 , y t ) by a transition matrix .", "entities": [[1, 2, "MethodName", "CRF"], [179, 180, "MethodName", "CRF"], [196, 197, "MethodName", "LSTM"]]}
{"text": "To accommodate our task decomposition approach , we first define the notion of partial labels , and then discuss different neural architectures capturing the dependencies between the modules trained over the different partial labels . Partial Labels and Task Decomposition : Given a learning task , defined over an output space y Y , where Y is the set of all possible tags , each specific label y is decomposed into a set of partial labels , { y 0 , .. , y k } . We refer to y as the full label . According to this definition , a specific assignment to all k partial labels defines a single full label . Note the difference between partially labeled data ( Cour et al , 2011 ) , in which instances can have more than a single full label , and our setup in which the labels are partial . In all our experiments , the partial labels refer to two sub - tasks , ( 1 ) a segmentation task , identifying Beginning , Inside and Outside of an entity or aspect . ( 2 ) one or more type recognition tasks , recognizing the aspect type and/or the sentiment polarity associated with it . Hence , a tag y t at location t is divided into y seg t and y typ t , corresponding to segmentation and type ( sentiment type here ) respectively . Fig . 1 provides an example of the target - sentiment task . Note that the sentiment labels do not capture segmentation information . Text ABC News ' President Tag B - neu O O Christiane Amanpour Exclusive Interview with Seg Senti Mubarak E - neu B - neu E - neu B - neu E - neu O B O O E B E B E O neu O O neu neu neu neu neu O Figure 1 : Target - sentiment decomposition example . Modular Learning architectures : We propose three different models , in which information from the partial labels can be used . All the models have similar modules types , corresponding to the segmentation and type sub - tasks , and the decision module for predicting the final task . The modules are trained over the partial segmentation ( y seg ) and type ( y typ ) labels , and the full label y information , respectively . These three models differ in the way they share information . Model 1 , denoted Twofold Modular , LSTM - CRF - T , is similar in spirit to multi - task learning ( Collobert et al , 2011 ) with three separate modules . Model 2 , denoted Twofold modular Infusion , ( LSTM - CRF - TI ) and Model 3 , denoted Twofold modular Infusion with guided gating , ( LSTM - CRF - TI ( g ) ) both infuse information flow from two sub - task modules into the decision module . The difference is whether the infusion is direct or goes through a guided gating mechanism . The three models are depicted in Fig . 2 and described in details in the following paragraphs . In all of these models , underlying neural architecture are used for the emission potentials when CRF inference layers are applied on top .", "entities": [[79, 80, "DatasetName", "0"], [265, 266, "MethodName", "ABC"], [278, 279, "DatasetName", "Interview"], [422, 423, "MethodName", "LSTM"], [424, 425, "MethodName", "CRF"], [433, 437, "TaskName", "multi - task learning"], [458, 459, "MethodName", "LSTM"], [460, 461, "MethodName", "CRF"], [477, 478, "MethodName", "LSTM"], [479, 480, "MethodName", "CRF"], [551, 552, "MethodName", "CRF"]]}
{"text": "The twofold modular model enhances the original monolithic model by using multi - task learning with shared underlying representations . The segmentation module and the type module are trained jointly with the decision module , and all the modules share information by using the same embedding level representation , as shown in Figure 2a . Since the information above the embedding level is independent , the LSTM layers in the different modules do not share information , so we refer to these layers of each module as private . The segmentation module predicts the segmentation BIO labels at position t of the sequence by using the representations extracted from its private word level bi - directional LSTM ( denoted as H seg ) as emission for a individual CRF : h seg t = H seg ( e t , \u2212 h seg t\u22121 , \u2212 h seg t+1 ) , \u03c6 ( x , y seg t ) = W seg h seg t + b seg , where W seg and b seg denote the parameters of the segmentation module emission layer , and H seg denotes its private LSTM layer . This formulation allows the model to forge the segmentation path privately through backpropagation by providing the segmentation information y seg individually , in addition to the complete tag information y. The type module , using y typ , is constructed in a similar way . By using representations from the its own private LSTM layers , the type module predicts the sentiment ( entity ) type at position t of the sequence : h typ t = H typ ( e t , \u2212 h typ t\u22121 , \u2212 h typ t+1 ) , \u03c6 ( x , y typ t ) = W typ h typ t + b typ . Both the segmentation information y seg and the type information y typ are provided together with the complete tag sequence y , enabling the model to learn segmentation and type recognition simultaneously using two different paths . Also , the decomposed tags naturally augment more training data to the model , avoiding over - fitting due to more complicated structure . The shared representation beneath the private LSTMs layers are updated via the back - propagated errors from all the three modules .", "entities": [[11, 15, "TaskName", "multi - task learning"], [66, 67, "MethodName", "LSTM"], [116, 117, "MethodName", "LSTM"], [128, 129, "MethodName", "CRF"], [191, 192, "MethodName", "LSTM"], [247, 248, "MethodName", "LSTM"]]}
{"text": "The twofold modular infusion model provides a stronger connection between the functionalities of the two sub - tasks modules and the final decision module , differing from multi - task leaning . In this model , instead of separating the pathways from the decision module as in the previous twofold modular model , the segmentation and the type representation are used as input to the final decision module . The model structure is shown in Figure 2b , and can be described formally as : I seg t = W seg h seg t + b seg , I typ t = W typ h typ t + b typ , S t = W [ h t ; I seg t ; I typ t ] + b , where S t is the shared final emission potential to the CRF layer in the decision module , and ; is the Figure 2 : Three modular models for task decomposition . In them , blue blocks are segmentation modules , detecting entity location and segmentation , and yellow blocks are the type modules , recognizing the entity type or sentiment polarity . Green blocks are the final decision modules , integrating all the decisions . ( G ) refers to \" Guided Gating \" concatenation operator , combining the representation from the decision module and that from the type module and the segmentation module . The term \" Infusion \" used for naming this module is intended to indicate that both modules actively participate in the final decision process , rather than merely form two independent paths as in the twofold modular model . This formulation provides an alternative way of integrating the auxiliary sub - tasks back into the major task in the neural structure to help improve learning .", "entities": [[141, 142, "MethodName", "CRF"]]}
{"text": "Our experimental evaluation is designed to evaluate the two key aspects of our model : ( Q1 ) Can the modular architecture alleviate the difficulty of learning the final task ? To answer this question , we compare our modular architecture to the traditional neural - CRF model and several recent competitive models for sequence labeling combining inference and deep learning . The results are summarized in Tables 1 - 3 . ( Q2 ) Can partial labels be used effectively as a new form of weak - supervision ? To answer this question we compared the performance of the model when trained using disjoint sets of partial and full labels , and show that adding examples only associated with partial labels , can help boost performance on the final task . The results are summarized in Figures 3 - 5 .", "entities": [[46, 47, "MethodName", "CRF"]]}
{"text": "We adapted the SemEval 2013 Task 2 subtask A as another task to evaluate our model . In this task , the system is given a marked phrase inside a longer text , and is asked to label its polarity . Unlike the original task , we did not assume the sequence is known , resulting in two decisions , identifying subjective expressions ( i.e. , a segmentation task ) and labeling their polarity , which can be modeled jointly as a sequence labeling task .", "entities": [[3, 5, "DatasetName", "SemEval 2013"]]}
{"text": "Our first set of results are designed to compare our modular learning models , utilize partial labels decomposition , with traditional monolithic models , that learn directly over the full labels . In all three tasks , we compare with strong sequence prediction models , including LSTM - CRF ( Lample et al , 2016 ) , which is directly equivalent to our baseline model ( i.e. , final task decision without the modules ) , and LSTM - CNN - CRF ( Ma and Hovy , 2016 ) and LSTM - CRF - LM ( Liu et al , 2018 ) which use a richer latent representation for scoring the emission potentials . ( denoted E+A ) . The second adds a third module that predicts the sentiment polarity associated with the aspect ( denoted E+A+S ) . I.e. , for a given sentence , label its entity span , the aspect category of the entity and the sentiment polarity of the entity at the same time . The results over four languages are summarized in Tab . 2 . In all cases , our modular approach outperforms all monolithic approaches . Subjective Phrase Identification and Classification This dataset contains tweets annotated with sentiment phrases , used for training the models . As in the original SemEval task , it is tested in two settings , in - domain , where the test data also consists of tweets , and out - of - domain , where the test set consists of SMS text messages . We present the results of experiments on these data set in Table 3 .", "entities": [[46, 47, "MethodName", "LSTM"], [48, 49, "MethodName", "CRF"], [77, 78, "MethodName", "LSTM"], [81, 82, "MethodName", "CRF"], [90, 91, "MethodName", "LSTM"], [92, 93, "MethodName", "CRF"], [197, 198, "TaskName", "Classification"]]}
{"text": "We present and study several modular neural architectures designed for a novel learning scenario : learning from partial labels . We experiment with several sentiment analysis tasks . Our models , inspired by cognitive neuroscience findings ( Jacobs et al , 1991 ; Eysenck and Keane , 2005 ) and multitask learning , suggest a functional decomposition of the original task into two simpler sub - tasks . We evaluate different methods for sharing information and integrating the modules into the final decision , such that a better model can be learned , while converging faster 5 . As our experiments show , modular learning can be used with weak supervision , using examples annotated with partial labels only . The modular approach also provides interesting directions for future research , focusing on alleviating the supervision bottleneck by using large amount of partially labeled data that are cheaper and easy to obtain , together with only a handful amount of annotated data , a scenario especially suitable for low - resource languages .", "entities": [[24, 26, "TaskName", "sentiment analysis"]]}
{"text": "In Figure 6 , we show an example of task decomposition for standard NER . In Figure 7 , we show another example of task decomposition for target sentiment , in addition to the one in the main text .", "entities": [[13, 14, "TaskName", "NER"]]}
{"text": "NER datasets We evaluated our models on three NER datasets , the English , Dutch and Spanish parts of the 2002 and 2003 CoNLL shared tasks ( Sang and F. , 2002 ; Sang et al , 2003 ) . We used the original division of training , validation and test sets . The task is defined over four different entity types : PERSON , LOCATION , ORGANIZATION , MISC . We used the BIOES tagging scheme during the training , and convert them back to original tagging scheme in testing as previous studies show that using this tagging scheme instead of BIO2 can help improve performance ( Ratinov and Roth , 2009 ; Lample et al , 2016 ; Ma and Hovy , 2016 ; Liu et al , 2018 ) . As a result , the segmentation module had 5 output labels , and the entity module had 4 . The final decision task , consisted of the Cartesian product of the segmentation set ( BIES ) and the entity set , plus the \" O \" tag , resulting in 17 labels . Results on NER We compared our models with the state - of - the - art systems on English 6 , Dutch and Spanish . For Dutch and Spanish , we used cross - lingual embedding as a way to exploit lexical information . The results are shown in Tab . 5 and Tab . 6 7 . Our best - performing model outperform all the competing systems .", "entities": [[0, 1, "TaskName", "NER"], [8, 9, "TaskName", "NER"], [188, 189, "TaskName", "NER"]]}
{"text": "We conducted additional experiments on knowledge integration in the same setting as in the main text to investigate the properties of the modules . Figure 8 shows the results for Dutch and Spanish NER datasets , while Figure 9 shows the results for the Subjective Polarity Disambiguation Datasets using the in - domain data .", "entities": [[33, 34, "TaskName", "NER"]]}
{"text": "We thank the reviewers for their insightful comments . We thank the NVIDIA Corporation for their GPU donation , used in this work . This work was partially funded by a Google Gift .", "entities": [[31, 32, "DatasetName", "Google"]]}
{"text": "Identifying Aggression and Toxicity in Comments using Capsule Network", "entities": [[7, 9, "MethodName", "Capsule Network"]]}
{"text": "Early works in automated detection of abusive language made use of basic machine learning like Tf - Idf ( Yin et al , 2009 ) , SVM ( Warner and Hirschberg , 2012 ) , Naive Bayes , random forests , or logistic regression over a bag - of - ngrams and achieved limited success . Newer approaches include solving problems using deep learning architectures like CNNs ( Kim , 2014 ; Zhang et al , 2015 ; Conneau et al , 2017b ; Park and Fung , 2017 ) which just focus on spatial patterns or LSTMs which treat text as sequences ( Tai et al , 2015 ; Mousa and Schuller , 2017 ) . Another popular approach completely ignores the order of words but focuses on their compositions as a collection , like probabilistic topic modeling ( Blei et al , 2003 ; Mcauliffe and Blei , 2008 ) and Earth Movers Distance based modeling ( Kusner et al , 2015 ; Ye et al , 2017 ) . Recently Capsule Network ( Sabour et al , 2017 ) has been used in text classification ( Zhao et al , 2018 ) .It makes use of the dynamic routing process to alleviate the disturbance of some noise capsules which may contain background information such as stop words and words that are unrelated to specific categories and show that capsule networks achieves significant improvement over strong baseline methods . As we focus to solve the problem of toxic comments and cyberbullying , we are confronted with the issue of large class imbalance . We use focal loss ( Lin et al , 2017 ) to tackle it as it prevents the vast number of easy negatives from overwhelming the detector during training . Also , in the online space people tend to talk using different languages in the same comment and often use transliteration . We show that our model is suitable for such data as well .", "entities": [[6, 8, "TaskName", "abusive language"], [26, 27, "MethodName", "SVM"], [42, 44, "MethodName", "logistic regression"], [173, 175, "MethodName", "Capsule Network"], [186, 188, "TaskName", "text classification"], [267, 269, "MethodName", "focal loss"]]}
{"text": "In this section we attempt to describe different models that we have used for the classification process . We seek to answer the following questions : ( 1 ) Is combination of Capsules and focal loss the new apotheosis for toxic comment classification problems ? ( 2 ) Can capsules solve the problem of OOV and transliteration implicitly ?", "entities": [[34, 36, "MethodName", "focal loss"], [40, 43, "TaskName", "toxic comment classification"]]}
{"text": "Recently , Kaggle hosted a competition named Toxic Comment Classification . This dataset has been contributed by Conversation AI , which is a research initiative founded by Jigsaw and Google . The task was comprised of calculating the log - likelihood of a sentence for the six classes , i.e. , given a sentence calculate the probability of it belonging to six classes . The six different classes were toxic , severe toxic , obscene , threat , insult and identity hate .", "entities": [[7, 10, "TaskName", "Toxic Comment Classification"], [27, 28, "MethodName", "Jigsaw"], [29, 30, "DatasetName", "Google"], [38, 41, "MetricName", "log - likelihood"]]}
{"text": "\" First Shared Task on Aggression Identification \" released a dataset for Aggression Identification . The task was to classify the comments into one of the three different classes Overtly Aggressive , Covertly Aggressive , and Non - aggressive . The train data was given in English and Hindi , where some of the comments in Hindi dataset were transliterated to English .", "entities": [[5, 7, "TaskName", "Aggression Identification"], [12, 14, "TaskName", "Aggression Identification"]]}
{"text": "We evaluate and compare our model with several strong baseline methods including : LSTM with Maxpool ( Lai et al , 2015 ) , Attention networks ( Raffel and Ellis , 2015 ) , Pre - trained LSTMs ( Dai and Le , 2015 ) , Hierarchical ConvNet ( Conneau et al , 2017a ) , Bi - LSTM with Skip - connections , variation of CNN - LSTM ( Wang et al , 2016 ) , CNN - multifilter ( Kim , 2014 ) , Bi - LSTM with xgboost and logistic regression . We experiment with these models on three datasets . The models were first evaluated on Kaggle competition for Toxic Comment Classification . All the model parameters and attributes were decided on the basis of our best performing model , and were kept same for the rest of experimentations and datasets .", "entities": [[13, 14, "MethodName", "LSTM"], [58, 59, "MethodName", "LSTM"], [68, 69, "MethodName", "LSTM"], [88, 89, "MethodName", "LSTM"], [92, 94, "MethodName", "logistic regression"], [113, 116, "TaskName", "Toxic Comment Classification"]]}
{"text": "In this work , we have proposed to automatically detect toxicity and aggression in comments , we show that with minimal preprocessing techniques we are able to achieve a good model performance and demonstrated how OOV words and semantic sense are learnt implicitly with random initialisation . We show the effectiveness of our proposed model against strong benchmark algorithms and that it outperforms others . In this work , we did basic preprocessing of the data , however in future we intend to explore more preprocessing techniques for the dataset , like data augmentation using translation approaches and methods to deal with mispelled words . We further would examine the results of capsule net by visualising which words or phrases does the model correctly recognises for classification as opposed to benchmark algorithms . Also , we would like to examine the usage of focal loss with the rest of the baseline models .", "entities": [[92, 94, "TaskName", "data augmentation"], [143, 145, "MethodName", "focal loss"]]}
{"text": "In one of our experiments , the summary vector obtained from LSTMs was concatenated with the vector obtained after appying Max Over Time on the hidden state representation of the input . The intuition behind this was that , by passing most relevant features along with summary of the input to the softmax layer may enhance the clasification process . From the experiments we obtained competetive results using this model .", "entities": [[52, 53, "MethodName", "softmax"]]}
{"text": "In ( Dai and Le , 2015 ) , authors claimed that by pretraining LSTMs on some related task as Auto - Encoder or as a Language Model , could optimize the stability of the LSTMs training process . The authors reported improvenemt in error rates by good margin in many tasks like , text classification on 20 Newsgroup , IMDB etc . For our experiments we gathered many related datasets like all of Wikimedia datasets ( Wulczyn et al , 2017 ) , TRAC shared dataset , IMDB movie reviews dataset . An autoencoder was trained on these datasets and the LSTMs from the encoder part were extracted and used in the classifcation task .", "entities": [[54, 56, "TaskName", "text classification"], [60, 61, "DatasetName", "IMDB"], [88, 91, "DatasetName", "IMDB movie reviews"], [94, 95, "MethodName", "autoencoder"]]}
{"text": "The idea of applying CNNs for text classification was proposed in ( Kim , 2014 ) , where authors applied filters of different length to extract N - gram features from text . The authors tried static and dynamic embedding channels and concluded that the model with combination of both outperformed others . For our setting we found that filters of length [ 2 , 3 , 4 ] have outperformed other filter sizes , we tried various combinations from range [ 2 , 5 ] . For activations we used Leaky ReLU , and performed Batch Normalization to stablize the data .", "entities": [[6, 8, "TaskName", "text classification"], [91, 93, "MethodName", "Leaky ReLU"], [96, 98, "MethodName", "Batch Normalization"]]}
{"text": "A joint architecture of CNNs and RNNs were proposed in ( Wang et al , 2016 ) , where the authors tried combination of CNNs with different RNNs like GRUs and LSTMs . In our experiment , we again used Leaky ReLU for CNNs activations , filter size of 3 was fixed for the experiments to decide the dropout values and other hyperparameters tuning .", "entities": [[40, 42, "MethodName", "Leaky ReLU"]]}
{"text": "In ( Lai et al , 2015 ) , authors took Max Over Time on the RNN representation of the input . Their model RNN outperformed other models in 3 out of 4 datasets . In our experiments , we fixed LSTM units to be 51 , and rest of the parameters were decided on the basis of validation - data experiments .", "entities": [[41, 42, "MethodName", "LSTM"]]}
{"text": "BERT Post - Training for Review Reading Comprehension and Aspect - based Sentiment Analysis", "entities": [[0, 1, "MethodName", "BERT"], [6, 8, "TaskName", "Reading Comprehension"], [9, 14, "TaskName", "Aspect - based Sentiment Analysis"]]}
{"text": "Question - answering plays an important role in e - commerce as it allows potential customers to actively seek crucial information about products or services to help their purchase decision making . Inspired by the recent success of machine reading comprehension ( MRC ) on formal documents , this paper explores the potential of turning customer reviews into a large source of knowledge that can be exploited to answer user questions . We call this problem Review Reading Comprehension ( RRC ) . To the best of our knowledge , no existing work has been done on RRC . In this work , we first build an RRC dataset called ReviewRC based on a popular benchmark for aspectbased sentiment analysis . Since ReviewRC has limited training examples for RRC ( and also for aspect - based sentiment analysis ) , we then explore a novel post - training approach on the popular language model BERT to enhance the performance of fine - tuning of BERT for RRC . To show the generality of the approach , the proposed post - training is also applied to some other review - based tasks such as aspect extraction and aspect sentiment classification in aspect - based sentiment analysis . Experimental results demonstrate that the proposed posttraining is highly effective 1 .", "entities": [[29, 31, "TaskName", "decision making"], [32, 33, "DatasetName", "Inspired"], [38, 41, "TaskName", "machine reading comprehension"], [77, 79, "TaskName", "Reading Comprehension"], [118, 120, "TaskName", "sentiment analysis"], [133, 138, "TaskName", "aspect - based sentiment analysis"], [154, 155, "MethodName", "BERT"], [164, 165, "MethodName", "BERT"], [193, 195, "TaskName", "aspect extraction"], [200, 205, "TaskName", "aspect - based sentiment analysis"]]}
{"text": "For online commerce , question - answering ( QA ) serves either as a standalone application of customer service or as a crucial component of a dialogue system that answers user questions . Many intelligent personal assistants ( such as Amazon Alexa and Google Assistant ) support online shopping by allowing the user to speak directly to the assistants . One major hindrance for this mode of shopping is that such systems have limited capability to answer user questions about products ( or services ) , which are vital for customer decision making . As such , an intelligent agent that can automatically answer customers ' questions is very important for the success of online businesses . Given the ever - changing environment of products and services , it is very hard , if not impossible , to pre - compile an up - to - date and reliable knowledge base to cover a wide assortment of questions that customers may ask , such as in factoidbased KB - QA ( Xu et al , 2016 ; Fader et al , 2014 ; Kwok et al , 2001 ; Yin et al , 2015 ) . As a compromise , many online businesses leverage community question - answering ( CQA ) ( McAuley and Yang , 2016 ) to crowdsource answers from existing customers . However , the problem with this approach is that many questions are not answered , and if they are answered , the answers are delayed , which is not suitable for interactive QA . In this paper , we explore the potential of using product reviews as a large source of user experiences that can be exploited to obtain answers to user questions . Although there are existing studies that have used information retrieval ( IR ) techniques ( McAuley and Yang , 2016 ; Yu and Lam , 2018 ) to find a whole review as the response to a user question , giving the whole review to the user is undesirable as it is quite time - consuming for the user to read it . Inspired by the success of Machine Reading Comphrenesions ( MRC ) ( Rajpurkar et al , 2016 ( Rajpurkar et al , , 2018 , we propose a novel task called Review Reading Comprehension ( RRC ) as following . Problem Definition : Given a question q = ( q 1 , . . . , q m ) from a customer ( or user ) about a product and a review d = ( d 1 , . . . , d n ) for that product containing the information to answer q , find a sequence of tokens ( a text span ) a = ( d s , . . . , d e ) in d that answers q correctly , where 1 \u2264 s \u2264 n , 1 \u2264 e \u2264 n , and s \u2264 e. A sample laptop review is shown in Table 1 . We can see that customers may not only ask factoid Questions Q1 : Does it have an internal hard drive ? Q2 : How large is the internal hard drive ? Q3 : is the capacity of the internal hard drive OK ? Review Excellent value and a must buy for someone looking for a Macbook . You ca n't get any better than this price and it come with A1 an internal disk drive . All the newer MacBooks do not . Plus you get 500 GB A2 which is also a great A3 feature . Also , the resale value on this will keep . I highly recommend you get one before they are gone . Table 1 : An example of review reading comprehension : we show 3 questions and their corresponding answer spans from a review . questions such as the specs about some aspects of the laptop as in the first and second questions but also subjective or opinion questions about some aspects ( capacity of the hard drive ) , as in the third question . RRC poses some domain challenges compared to the traditional MRC on Wikipedia , such as the need for rich product knowledge , informal text , and fine - grained opinions ( there is almost no subjective content in Wikipedia articles ) . Research also shows that yes / no questions are very frequent for products with complicated specifications ( McAuley and Yang , 2016 ; Xu et al , 2018b ) . To the best of our knowledge , no existing work has been done in RRC . This work first builds an RRC dataset called ReviewRC , using reviews from SemEval 2016 Task 5 2 , which is a popular dataset for aspect - based sentiment analysis ( ABSA ) ( Hu and Liu , 2004 ) in the domains of laptop and restaurant . We detail ReviewRC in Sec . 5 . Given the wide spectrum of domains ( types of products or services ) in online businesses and the prohibitive cost of annotation , ReviewRC can only be considered to have a limited number of annotated examples for supervised training , which still leaves the domain challenges partially unresolved . This work adopts BERT ( Devlin et al , 2018 ) as the base model as it achieves the state - ofthe - art performance on MRC ( Rajpurkar et al , 2016 ( Rajpurkar et al , , 2018 . Although BERT aims to learn contextualized representations across a wide range of NLP tasks ( to be task - agnostic ) , leveraging BERT alone still leaves the domain challenges un - 2 http://alt.qcri.org/semeval2016/ task5/. We choose these review datasets to align RRC with existing research on sentiment analysis . resolved ( as BERT is trained on Wikipedia articles and has almost no understanding of opinion text ) , and it also introduces another challenge of task - awareness ( the RRC task ) , called the task challenge . This challenge arises when the taskagnostic BERT meets the limited number of finetuning examples in ReviewRC ( see Sec . 5 ) for RRC , which is insufficient to fine - tune BERT to ensure full task - awareness of the system 3 . To address all the above challenges , we propose a novel joint post - training technique that takes BERT 's pre - trained weights as the initialization 4 for basic language understanding and adapt BERT with both domain knowledge and task ( MRC ) knowledge before fine - tuning using the domain end task annotated data for the domain RRC . This technique leverages knowledge from two sources : unsupervised domain reviews and supervised ( yet out - of - domain ) MRC data 5 , where the former enhances domain - awareness and the latter strengthens MRC task - awareness . As a general - purpose approach , we show that the proposed method can also benefit ABSA tasks such as aspect extraction ( AE ) and aspect sentiment classification ( ASC ) . The main contributions of this paper are as follows . ( 1 ) It proposes the new problem of review reading comprehension ( RRC ) . ( 2 ) To solve this new problem , an annotated dataset for RRC is created . ( 3 ) It proposes a general - purpose posttraining approach to improve RRC , AE , and ASC . Experimental results demonstrate that the proposed approach is effective .", "entities": [[43, 44, "DatasetName", "Google"], [91, 93, "TaskName", "decision making"], [99, 100, "DatasetName", "agent"], [297, 299, "TaskName", "information retrieval"], [352, 353, "DatasetName", "Inspired"], [384, 386, "TaskName", "Reading Comprehension"], [629, 631, "TaskName", "reading comprehension"], [799, 804, "TaskName", "aspect - based sentiment analysis"], [882, 883, "MethodName", "BERT"], [921, 922, "MethodName", "BERT"], [943, 944, "MethodName", "BERT"], [967, 969, "TaskName", "sentiment analysis"], [973, 974, "MethodName", "BERT"], [1016, 1017, "MethodName", "BERT"], [1042, 1043, "MethodName", "BERT"], [1072, 1073, "MethodName", "BERT"], [1088, 1089, "MethodName", "BERT"], [1176, 1178, "TaskName", "aspect extraction"], [1179, 1180, "MethodName", "AE"], [1209, 1211, "TaskName", "reading comprehension"], [1247, 1248, "MethodName", "AE"]]}
{"text": "Many datasets have been created for MRC from formally written and objective texts , e.g. , Wikipedia ( WikiReading ( Hewlett et al , 2016 ) , SQuAD ( Rajpurkar et al , 2016 ( Rajpurkar et al , , 2018 , Wiki - Hop ( Welbl et al , 2018 ) , DRCD ( Shao et al , 2018 ) , QuAC ( Choi et al , 2018 ) , HotpotQA ) news and other articles ( CNN / Daily Mail ( Hermann et al , 2015 ) , NewsQA ( Trischler et al , 2016 ) , RACE ( Lai et al , 2017 ) ) , fictional stories ( MCTest ( Richardson et al , 2013 ) , CBT ( Hill et al , 2015 ) , NarrativeQA ( Ko\u010disk\u1ef3 et al , 2018 ) ) , and general Web documents ( MS MARCO ( Nguyen et al , 2016 ) , TriviaQA ( Joshi et al , 2017 ) , SearchQA ( Dunn et al , 2017 ) ) . Also , CoQA ( Reddy et al , 2018 ) is built from multiple sources , such as Wikipedia , Reddit , News , Mid / High School Exams , Literature , etc . To the best of our knowledge , MRC has not been used on reviews , which are primarily subjective . As such , we created a review - based MRC dataset called Re - viewRC . Answers from ReviewRC are extractive ( similar to SQuAD ( Rajpurkar et al , 2016 ( Rajpurkar et al , , 2018 ) rather than abstractive ( or generative ) ( such as in MS MARCO ( Nguyen et al , 2016 ) and CoQA ( Reddy et al , 2018 ) ) . This is crucial because online businesses are typically cost - sensitive and extractive answers written by humans can avoid generating incorrect answers beyond the contents in reviews by an AI agent . Community QA ( CQA ) is widely adopted by online businesses ( McAuley and Yang , 2016 ) to help users . However , since it solely relies on humans to give answers , it often takes a long time to get a question answered or even not answered at all as we discussed in the introduction . Although there exist researches that align reviews to questions as an information retrieval task ( McAuley and Yang , 2016 ; Yu and Lam , 2018 ) , giving a whole review to the user to read is time - consuming and not suitable for customer service settings that require interactive responses . Knowledge bases ( KBs ) ( such as Freebase ( Dong et al , 2015 ; Xu et al , 2016 ; Yao and Van Durme , 2014 ) or DBpedia ( Lopez et al , 2010 ; Unger et al , 2012 ) ) have been used for question answering ( Yu and Lam , 2018 ) . However , the ever - changing nature of online businesses , where new products and services appear constantly , makes it prohibitive to build a highquality KB to cover all new products and services . Reviews also serve as a rich resource for sentiment analysis ( Pang et al , 2002 ; Hu and Liu , 2004 ; Liu , 2012Liu , , 2015 . Although documentlevel ( review ) sentiment classification may be considered as a solved problem ( given ratings are largely available ) , aspect - based sentiment analysis ( ABSA ) is still an open challenge , where alleviating the cost of human annotation is also a major issue . ABSA aims to turn unstructured reviews into structured fine - grained aspects ( such as the \" battery \" of a laptop ) and their associated opinions ( e.g. , \" good battery \" is positive about the aspect battery ) . Two important tasks in ABSA are aspect extraction ( AE ) and aspect sentiment classification ( ASC ) ( Hu and Liu , 2004 ) , where the former aims to extract aspects ( e.g. , \" battery \" ) and the latter targets to identify the polarity for a given aspect ( e.g. , positive for battery ) . Recently , supervised deep learning models dominate both tasks ( Wang et al , , 2017Xu et al , 2018a ; Tang et al , 2016 ; and many of these models use handcrafted features , lexicons , and complicated neural network architectures to remedy the insufficient training examples from both tasks . Although these approaches may achieve better performances by manually injecting human knowledge into the model , human baby - sat models may not be intelligent enough 6 and automated representation learning from review corpora is always preferred ( Xu et al , 2018a ; . We push forward this trend with the recent advance in pre - trained language models from deep learning ( Peters et al , 2018 ; Howard and Ruder , 2018 ; Devlin et al , 2018 ; Radford et al , 2018a , b ) . Although it is practical to train domain word embeddings from scratch on large - scale review corpora ( Xu et al , 2018a ) , it is impractical to train language models from scratch with limited computational resources . As such , we show that it is practical to adapt language models pre - trained from formal texts to domain reviews .", "entities": [[18, 19, "DatasetName", "WikiReading"], [27, 28, "DatasetName", "SQuAD"], [53, 54, "DatasetName", "DRCD"], [62, 63, "DatasetName", "QuAC"], [71, 72, "DatasetName", "HotpotQA"], [78, 82, "DatasetName", "CNN / Daily Mail"], [90, 91, "DatasetName", "NewsQA"], [99, 100, "DatasetName", "RACE"], [112, 113, "DatasetName", "MCTest"], [121, 122, "DatasetName", "CBT"], [130, 131, "DatasetName", "NarrativeQA"], [145, 147, "DatasetName", "MS MARCO"], [155, 156, "DatasetName", "TriviaQA"], [164, 165, "DatasetName", "SearchQA"], [176, 177, "DatasetName", "CoQA"], [194, 195, "DatasetName", "Reddit"], [252, 253, "DatasetName", "SQuAD"], [278, 280, "DatasetName", "MS MARCO"], [288, 289, "DatasetName", "CoQA"], [328, 329, "DatasetName", "agent"], [399, 401, "TaskName", "information retrieval"], [471, 472, "DatasetName", "DBpedia"], [490, 492, "TaskName", "question answering"], [543, 545, "TaskName", "sentiment analysis"], [587, 592, "TaskName", "aspect - based sentiment analysis"], [662, 664, "TaskName", "aspect extraction"], [665, 666, "MethodName", "AE"], [798, 800, "TaskName", "representation learning"], [867, 869, "TaskName", "word embeddings"]]}
{"text": "In this section , we briefly review BERT and derive its fine - tuning formulation on three ( 3 ) reviewbased end tasks .", "entities": [[7, 8, "MethodName", "BERT"]]}
{"text": "As a subsequent task of AE , aspect sentiment classification ( ASC ) aims to classify the sentiment polarity ( positive , negative , or neutral ) expressed on an aspect extracted from a review sentence . There are two inputs to ASC : an aspect and a review sentence mentioning that aspect . Consequently , ASC is close to RRC as the question is just about an aspect and the review is just a review sentence but ASC only needs to output a class of polarity instead of a textual span . Let As a summary of these tasks , insufficient supervised training data significantly limits the performance gain across these 3 review - based tasks . Al - though BERT 's pre - trained weights strongly boost the performance of many other NLP tasks on formal texts , we observe in Sec . 5 that BERT 's weights only result in limited gain or worse performance compared with existing baselines . In the next section , we introduce the post - training step to boost the performance of all these 3 tasks . x = ( [ CLS ] , q 1 , . . . , q m , [ SEP ] , d 1 , . . . , d n , [ SEP ] ) ,", "entities": [[5, 6, "MethodName", "AE"], [121, 122, "MethodName", "BERT"], [147, 148, "MethodName", "BERT"]]}
{"text": "As there are no existing datasets for RRC and to be consistent with existing research on sentiment analysis , we adopt the laptop and restaurant reviews of SemEval 2016 Task 5 as the source to create datasets for RRC . We do not use SemEval 2014 Task 4 or SemEval 2015 Task 12 because these datasets do not come with the review ( document ) level XML tags to recover whole reviews from review sentences . We keep the split of training and testing of the SemEval 2016 Task 5 datasets and annotate multiple QAs for each review following the way of constructing QAs for the SQuAD 1.1 datasets ( Rajpurkar et al , 2016 ) . To make sure our questions are close to realworld questions , 2 annotators are first exposed to 400 QAs from CQA ( under the laptop category in Amazon.com or popular restaurants in Yelp.com ) to get familiar with real questions . Then they are asked to read reviews and independently label textual spans and ask corresponding questions when they feel the textual spans contain valuable information that customers may care about . The textual spans are labeled to be as concise as possible but still human - readable . Note that the annotations for sentiment analysis tasks are not exposed to annotators to avoid biased annotation on RRC . Since it is unlikely that the two annotators can label the same QAs ( the same questions with the same answer spans ) , they further mutually check each other 's annotations and disagreements are discussed until agreements are reached . Annotators are encouraged to label as many questions as possible from testing reviews to get more test examples . A training review is encouraged to have 2 questions ( training examples ) on average to have good coverage of reviews . The annotated data is in the format of SQuAD 1.1 ( Rajpurkar et al , 2016 ) to ensure compatibility with existing implementations of MRC models . The statistics of the RRC dataset ( ReviewRC ) are shown in Table 2 . Since SemEval datasets do not come with a validation set , we further split 20 % of reviews from the training set for validation . Statistics of datasets for AE and ASC are given in Table 3 . For AE , we choose SemEval 2014 Task 4 for laptop and SemEval - 2016 Task 5 for restaurant to be consistent with ( Xu et al , 2018a ) and other previous works . For ASC , we use SemEval 2014 Task 4 for both laptop and restaurant as existing research frequently uses this version . We use 150 examples from the training set of all these datasets for validation .", "entities": [[16, 18, "TaskName", "sentiment analysis"], [106, 107, "DatasetName", "SQuAD"], [211, 213, "TaskName", "sentiment analysis"], [316, 317, "DatasetName", "SQuAD"], [379, 380, "MethodName", "AE"], [389, 390, "MethodName", "AE"]]}
{"text": "For domain knowledge post - training , we use Amazon laptop reviews ( He and McAuley , 2016 ) and Yelp Dataset Challenge reviews 8 . For laptop , we filtered out reviewed products that have appeared in the validation / test reviews to avoid training bias for test data ( Yelp reviews do not have this issue as the source reviews of SemEval are not from Yelp ) . Since the number of reviews is small , we choose a duplicate factor of 5 ( each review For the restaurant domain , we use Yelp reviews from restaurant categories that the SemEval reviews also belong to ( Xu et al , 2018a ) . We choose 700 K reviews to ensure it is large enough to generate training examples ( with a duplicate factor of 1 ) to cover all post - training steps that we can afford ( discussed in Section 5.3 ) 9 . This gives us 2 , 677 , 025 post - training examples for restaurant domain knowledge learning . For MRC task - awareness post - training , we leverage SQuAD 1.1 ( Rajpurkar et al , 2016 ) that come with 87 , 599 training examples from 442 Wikipedia articles .", "entities": [[186, 187, "DatasetName", "SQuAD"]]}
{"text": "The results of RRC , AE and ASC are shown in Tables 4 , 5 and 6 , respectively . To answer RQ1 , we observed that the proposed joint post - training ( BERT - PT ) has the best performance over all tasks in all domains , which show the benefits of having two types of knowledge . To answer RQ2 , to our surprise we found that the vanilla pre - trained weights of BERT do not work well for review - based tasks , although it achieves state - of - the - art results on many other NLP tasks ( Devlin et al , 2018 ) . This justifies the need to adapt BERT to review - based tasks . To answer RQ3 , we noticed that the roles of domain knowledge and task knowledge vary for different tasks and domains . For RRC , we found that the performance gain of BERT - PT mostly comes from task - awareness ( MRC ) post - training ( as indicated by BERT - MRC ) . The domain knowledge helps more for restaurant than for laptop . We suspect the reason is that certain types of knowledge ( such as specifications ) of laptop are already present in Wikipedia , whereas Wikipedia has little knowledge about restaurant . We further investigated the examples improved by BERT - MRC and found that the boundaries of spans ( especially short spans ) were greatly improved . For AE , we found that great performance boost comes mostly from domain knowledge posttraining , which indicates that contextualized representations of domain knowledge are very important for AE . BERT - MRC has almost no improvement on restaurant , which indicates Wikipedia may have no knowledge about aspects of restaurant . We suspect that the improvements on laptop come from the fact that many answer spans in SQuAD are noun terms , which bear a closer relationship with laptop aspects . For ASC , we observed that large - scale annotated MRC data is very useful . We suspect the reason is that ASC can be interpreted as a special MRC problem , where all questions are about the polarity of a given aspect . MRC training data may help BERT to understand the input format of ASC given their closer input formulation . Again , domain knowledge post - training also helps ASC . We further investigated the errors from BERT - PT over the 3 tasks . The errors on RRC mainly come from boundaries of spans that are not concise enough and incorrect location of spans that may have certain nearby words related to the question . We believe precisely understanding user 's experience is challenging from only domain posttraining given limited help from the RRC data and no help from the Wikipedia data . For AE , errors mostly come from annotation inconsistency and boundaries of aspects ( e.g. , apple OS is predicted as OS ) . Restaurant suffers from rare aspects like the names of dishes . ASC tends to have more errors as the decision boundary between the negative and neutral examples is unclear ( e.g. , even annotators may not sure whether the reviewer shows no opinion or slight negative opinion when mentioning an aspect ) . Also , BERT - PT has the problem of dealing with one sentence with two opposite opinions ( \" The screen is good but not for windows . \" ) . We believe that such training examples are rare .", "entities": [[5, 6, "MethodName", "AE"], [34, 35, "MethodName", "BERT"], [77, 78, "MethodName", "BERT"], [118, 119, "MethodName", "BERT"], [157, 158, "MethodName", "BERT"], [176, 177, "MethodName", "BERT"], [230, 231, "MethodName", "BERT"], [250, 251, "MethodName", "AE"], [277, 278, "MethodName", "AE"], [279, 280, "MethodName", "BERT"], [317, 318, "DatasetName", "SQuAD"], [380, 381, "MethodName", "BERT"], [411, 412, "MethodName", "BERT"], [479, 480, "MethodName", "AE"], [557, 558, "MethodName", "BERT"]]}
{"text": "We proposed a new task called review reading comprehension ( RRC ) and investigated the possibility of turning reviews as a valuable resource for answering user questions . We adopted BERT as our base model and proposed a joint post - training approach to enhancing both the domain and task knowledge . We further explored the use of this approach in two other review - based tasks : aspect extraction and aspect sentiment classification . Experimental results show that the post - training approach before fine - tuning is effective .", "entities": [[7, 9, "TaskName", "reading comprehension"], [30, 31, "MethodName", "BERT"], [68, 70, "TaskName", "aspect extraction"]]}
{"text": "In this paper , we propose a novel framework for sarcasm generation ; the system takes a literal negative opinion as input and translates it into a sarcastic version . Our framework does not require any paired data for training . Sarcasm emanates from context - incongruity which becomes apparent as the sentence unfolds . Our framework introduces incongruity into the literal input version through modules that : ( a ) filter factual content from the input opinion , ( b ) retrieve incongruous phrases related to the filtered facts and ( c ) synthesize sarcastic text from the filtered and incongruous phrases . The framework employs reinforced neural sequence to sequence learning and information retrieval and is trained only using unlabeled non - sarcastic and sarcastic opinions . Since no labeled dataset exists for such a task , for evaluation , we manually prepare a benchmark dataset containing literal opinions and their sarcastic paraphrases . Qualitative and quantitative performance analyses on the data reveal our system 's superiority over baselines , built using known unsupervised statistical and neural machine translation and style transfer techniques .", "entities": [[109, 112, "MethodName", "sequence to sequence"], [114, 116, "TaskName", "information retrieval"], [179, 181, "TaskName", "machine translation"], [182, 184, "TaskName", "style transfer"]]}
{"text": "Sarcasm 1 is an intensive , ironic construct that is intended to express contempt or ridicule . It is often linked with intelligence , creativity , and wit , and therefore empowering machines to generate sarcasm is in line with the key goals of Strong AI 2 . From the perspective of Natural Language Generation ( NLG ) , sarcasm generation remains an important problem and can prove useful in downstream applications such as conversation systems , recommenders , and online content generators . For instance , in a conversational setting , a more natural and intriguing form of conversation between humans and machines could happen if machines can intermittently generate sarcastic responses , like their human counterparts . Over the years , a lot of research and development efforts have gone into the problem of detecting sarcasm in text , which aims to classify whether a given text contains sarcasm or not ( Joshi et al ( 2017b ) provide an overview ) . However , systems for generation of sarcasm have been elusive . This is probably due to the fact that in sarcasm generation both selection of contents for sarcastic opinion generation and surface realization of contents in natural language form are highly nuanced . In the broader area of style transformation of texts , most of the existing works have focused narrowly on transformations at lexical and syntax levels , i.e. , text simplification ( Siddharthan , 2014 ) , text formalization ( Jain et al , 2018 ) , sentiment style transfer ( Shen et al , 2017 ; Xu et al , 2018 ) , sentiment flipping and understanding humor ( West and Horvitz , 2019 ) . However , very little work has been done ( ( Piwek , 2003 ) , ( Hovy , 1987 ) ) on incorporating pragmatics into generation tasks such as sarcasm . Sarcasm generation offers a rich playground to study this challenge and push the state - of - the - art in text transformation . Moreover , being a pragmatic task , sarcasm construction offers diverse ways to convey the same intent , based on cultural , social and demographic backgrounds . Hence , a supervised treatment of sarcasm generation using paired labeled data ( such as parallel sentences ) will be highly restrictive . This further motivates the need for exploring unsupervised approaches as the one we propose in this paper . We make the first attempt towards automatic sarcasm generation where the generation is conditioned on a literal input sentence . For example , the literal opinion \" I hate it when my bus is late . \" should be transformed into \" Absolutely love waiting for the bus \" . As sarcasm conveys a negative sentiment , our system expects a negative sentiment opinion as input . Out of various possible theories proposed to explain the phenomenon of sarcasm construction ( Joshi et al , 2017b ) , our framework relies on the theory of context incongruity ( Campbell and Katz , 2012 ) . Context incongruity is prevalent in textual sarcasm ( Riloff et al , 2013 ; Joshi et al , 2015b ) . The theory presents sarcasm as a contrast between positive sentiment context ( e.g. , absolutely loved it ) and negative situational context ( e.g. , my bus is late ) . In our framework , translation of literal sentences to sarcastic ones happens in four stages viz . , ( 1 ) Sentiment Neutralization , during which sentiment - bearing words and phrases are filtered from the input , ( 2 ) Positive Sentiment Induction , where the neutralized input is translated into phrases conveying a strong positive sentiment , ( 3 ) Negative Situation Retrieval , during which a negative situation related to the input is retrieved , and ( 4 ) Sarcasm Synthesis , where appropriate sarcastic constructs are formed from the positive sentiment and negative situation phrases gathered in the first three stages . Training and development of these modules require only three unpaired corpora of positive , negative , and sarcastic opinions . For evaluating the system , we manually prepare a small benchmark dataset which contains a set of literal opinions and their corresponding sarcastic paraphrases . Quantitative evaluation of our system is done using popular translation - evaluation metrics , and document similarity measurement metrics . For qualitative evaluation , we consider the human judgment of sarcastic intensity , fluency , and adequacy of the generated sentences . As baselines , we consider some of our simplistic model variants and existing systems for unsupervised machine translation and style transfer . Our overall observation is that our system often generates sarcasm of better quality than the baselines . The code , data , and resources are available at https://github.com/ TarunTater / sarcasm generation .", "entities": [[236, 238, "TaskName", "text simplification"], [255, 257, "TaskName", "style transfer"], [772, 775, "TaskName", "unsupervised machine translation"], [776, 778, "TaskName", "style transfer"]]}
{"text": "Once the neutralized output is extracted , it is passed to the positive sentiment induction module which transforms it into a positive sentiment sentence . For this , we use a traditional sequence to sequence pipeline ( Bahdanau et al , 2014 ) with attention and copy mechanisms ( Gulcehre et al , 2016 ) . The input is a set of words coming from the neutralization module . These are transformed into embeddings and are then encoded with the help of LSTM layers . The decoder attends over the encoded output and produces the output tokens based on the attended vector and the previously generated tokens . This is a standard technique , typically used in neural machine translation . As the output from the system is expected to be positive in sentiment , for training the framework , we use only a set of positive sentences from P . Each sentence in the data is filtered using the neutralization module . The filtered version , and the original positive sentence are used as source , target pairs .", "entities": [[32, 35, "MethodName", "sequence to sequence"], [82, 83, "MethodName", "LSTM"], [118, 120, "TaskName", "machine translation"]]}
{"text": "Negative situations present in sarcastic opinions are typically extrinsic and are loosely related to the semantics of its literal version . Hence , a sequence to sequence module analogous to Section 5 - grams : getting up for school facts , getting yelled at by people , trying to schedule my classes , feeling like every single person , walking to class in pouring , making people who already hate , working on my last day , spending countless hours at doctors , getting overdraft statements in mail 4 - grams : talking about world politics , stuck in a generation , sitting in class wondering , canceled at short notice , distancing myself from certain , wipe my own tears 3 - grams : born not breathing , paid to sleep , scared those faces , taking a shower , starting your monday , accused of everything , worrying about someone , fight jealousy arguments , license to trill , awarded literature prize 2 - grams : scratching itchy , looking chair , getting hiv , shot first , collecting death , lost respect 1 - gram : canceled , sleeping , trying , buying , stapling ( Riloff et al , 2013 ) 3.2 may not be very useful . Moreover , for sarcasm generation , for a certain topic , it is safe to assume that there can be a finite set of negative situations . From this set appropriate situation phrases can be \" retrieved \" depending on the given input . Thus , finding out appropriate negative situations boils down to two sub - problems of ( a ) preparing a finite set of negative situations , and ( b ) setting up the negative situation retrieval process . We discuss each of these two steps below .", "entities": [[24, 27, "MethodName", "sequence to sequence"]]}
{"text": "The idea is to find negative situations relevant to the input sentence . We implement an information retrieval system based on PyLucene . All the negative situations from the gazetteer ( Sec . 3.3.1 ) are first indexed . The input sentence is considered as the query for which the most relevant negative situation is retrieved from the indexed list . The factors involved in PyLucene 's retrieval algorithm include tf - idf , number of matching terms in the query sentence and the retrieved sentence , and importance measure of a term according to the total number of terms in the search . Once the positive sentiment and negative situations are generated for the input sentence , they undergo a post - processing step where stopwords and redundant words are removed and given as input to the sarcasm synthesis module .", "entities": [[16, 18, "TaskName", "information retrieval"]]}
{"text": "As stated earlier , our system does not rely on any paired data for training . It requires three corpora of positive sentences , negative sentences , and sarcastic sentences collected independently . For positive and negative sentiment corpora P and N , we considered short sentences / snippets from the following well - known sources such as ( a ) Stanford Sentiment Treebank Dataset , ( b ) Amazon Product Reviews , ( c ) Yelp Reviews ( d ) Sentiment 140 dataset ( See Kotzias et al ( 2015 ) for sources ) . The above datasets primarily contain tweets and short snippets . Tweets are normalized by removing hashtags , usernames , and performing spell checking and lexical normalization using NLTK ( Loper and Bird , 2002 ) . We then filtered out sentences with more than 30 words . Approximately 50 , 000 sentences from each category are retained . Then , based on the vocabulary overlap with our sarcasm corpus S , 47 , 827 sentences are finally retained from each category ( total number of instances is 95654 ) . For the unlabelled sarcasm corpus S , we relied on popular datasets used for sarcasm detection tasks such as the ones by Ghosh and Veale ( 2016 ) , Riloff et al ( 2013 ) , and the Reddit Sarcasm Corpus 4 . Sentences are denoised , spell corrected and normalized . Average sentence length is kept as 30 words . A total number of 306 , 141 sentences are thus collected . A common vocabulary of size 20 , 000 is extracted ( based on frequency ) for all the modules from the three corpora . Each corpus is divided into a train - valid - test split of 80 % - 10 % - 10 % .", "entities": [[120, 122, "TaskName", "lexical normalization"], [200, 202, "TaskName", "sarcasm detection"], [224, 225, "DatasetName", "Reddit"]]}
{"text": "We also consider human judgment scores indicating whether the generated output is nonsarcastic / sarcastic ( 0/1 labels ) , how fluent it is ( in a scale of 1 - 5 , 1 being lower ) , and to what extent it is related to the input ( in a scale of 1 - 5 ) . The relatedness measure is important as the objective of the task is to produce a sarcastic version of the input text without altering the semantics much . For human evaluation , we consider only the 30 sentences randomly picked from the benchmark ( test ) dataset . Sarcasm is a difficult topic , so we stuck to only two annotators who had a better understanding of the language and socio - cultural diversities .", "entities": [[98, 100, "DatasetName", "the benchmark"]]}
{"text": "For comparison , we consider the following four systems : 1 . SarcasmBot : This is an open - sourced sarcasm generation chatbot released by Joshi et al ( 2015a ) . The system generates a sarcastic response to an input utterance .", "entities": [[22, 23, "TaskName", "chatbot"]]}
{"text": "Monoses : This is similar to UNMT but based on unsupervised Statistical Machine Translation ( Artetxe et al , 2018 ) .", "entities": [[12, 14, "TaskName", "Machine Translation"]]}
{"text": "This is based on the cross alignment technique proposed by Shen et al ( 2017 ) , used for the task of sentiment translation . 5 . FLIP : This is based on heuristics for sentiment reversal . For this , the input sentence is first dependency - parsed . The root verb is determined along with its tense and aspects with the help of its part - of - speech tags 7 . The sentiment of root verb is determined using sentiment lexicon 8 . If the verb has non - zero positive or negative sentiment score , its antonym is found using WordNet . Appropriate tense and aspect form of the antonym is then obtained 9 . The modified antonym replaces the original root verb . Similarly , we replace adjective and adverbs with words carrying opposite sentiment . For training the above systems ( except FLIP ) , we used S at one side and a larger version of combined P and N containing 558 , 235 sentences on the other side , curated from the same sources as mentioned earlier . Apart from this system , we also tested some of our model variants , which are presumably inferior and can be considered as baselines . These are termed as : 1 . SG NORMAL : a system with only the sarcasm synthesizer module which takes the input directly ( after removing stopwords from the input ) , 2 . SG RL : , same as SG NORMAL but also applies reinforcement learning , 3 . ALL NORMAL : , the complete system , with sarcasm synthesizer trained without reinforcement learning strategy . 4 . ALL RL : , the complete system with reinforcement learning .", "entities": [[66, 69, "DatasetName", "part - of"]]}
{"text": "As stated earlier , not many systems for sarcasm generation exist today . The closest work to ours is the one by Joshi et al ( 2015a ) which employs a heuristic driven approach for generating a sarcastic response to an input utterance . Since , the output of the system is a response , the system is not suitable for translating a literal input text into a sarcastic version . Unlike sarcasm generation , sar - casm detection has been a well - known problem with several available solutions . For this problem , traditional supervised and deep neural network based solutions have been proposed . The supervised approaches rely on : ( a ) Unigrams and Pragmatic features ( Gonz\u00e1lez - Ib\u00e1nez et al , 2011 ; Barbieri et al , 2014 ; Joshi et al , 2015b ) ( b ) Stylistic patterns ( Davidov et al , 2010 ) and patterns related to situational disparity ( Riloff et al , 2013 ) and ( c ) Cognitive features extracted from gaze patterns ( Mishra et al , 2016 ( Mishra et al , , 2017 . Recent systems are based on variants of deep neural networks built on the top of embeddings . Deep neural networks based solutions for sarcasm detection include ( Ghosh and Veale , 2016 ) who uses a combination of RNNs and CNNs for sarcasm detection , and ( Tay et al , 2018 ) , who propose a variant of CNN for extracting features related to context incongruity . A few works exist in the domains of irony , pun and humour generation and are summarized by Wallace ( 2015 ) , Ritchie ( 2005 ) and Strapparava et al ( 2011 ) respectively . However , most of these are heuristic driven and , hence , may not be easily scaled to new domains and languages . From the perspective of language style transfer . Shen et al ( 2017 ) propose an unsupervised scheme to learn latent content distribution across different text corpora and use it for sentiment style transfer . Xu et al ( 2018 ) introduce an unsupervised sentiment translation technique through sentiment neutralization and reinforced sequence generation . propose a style transfer technique based on unsupervised MT inspired by Artetxe et al ( 2017 ) . Artetxe et al ( 2018 ) have recently proposed an unsupervised statistical machine translation scheme . We adopt some of these modules for the task of sarcasm generation . As far as we know , our proposed model is the first of its kind for end - to - end neural sarcasm generation .", "entities": [[213, 215, "TaskName", "sarcasm detection"], [232, 234, "TaskName", "sarcasm detection"], [322, 324, "TaskName", "style transfer"], [349, 351, "TaskName", "style transfer"], [374, 376, "TaskName", "style transfer"], [402, 404, "TaskName", "machine translation"]]}
{"text": "We proposed a first of its kind approach for textual sarcasm generation from literal opinionated texts . We designed a modular framework for extracting facts from the input , generating incongruous positive and negative situational phrases related to the facts , and finally generating sarcastic variations . For evaluation , we prepared a benchmark dataset containing literal opinions and their sarcastic versions . Through qualitative and quantitative anal - ysis of the system 's performance on the benchmark dataset , we observed that our system often generates better sarcastic sentences compared to some of our trivial model variants , and unsupervised systems used for machine translation and sentiment style transfer . In the future , we would like to extend this framework for cross - lingual and cross - cultural sarcasm and irony generation .", "entities": [[76, 78, "DatasetName", "the benchmark"], [104, 106, "TaskName", "machine translation"], [108, 110, "TaskName", "style transfer"]]}
{"text": "Detection of Adverse Drug Reaction mentions in tweets using ELMo", "entities": [[9, 10, "MethodName", "ELMo"]]}
{"text": "This paper describes the models used by our team in SMM4H 2019 shared task ( Weissenbacher et al , 2019 ) . We submitted results for subtasks 1 and 2 . For task 1 which aims to detect tweets with Adverse Drug Reaction ( ADR ) mentions we used ELMo embeddings which is a deep contextualized word representation able to capture both syntactic and semantic characteristics . For task 2 , which focuses on extraction of ADR mentions , first the same architecture as task 1 was used to identify whether or not a tweet contains ADR . Then , for tweets positively classified as mentioning ADR , the relevant text span was identified by similarity matching with 3 different lexicon sets .", "entities": [[10, 11, "DatasetName", "SMM4H"], [49, 50, "MethodName", "ELMo"]]}
{"text": "Solving Aspect Category Sentiment Analysis as a Text Generation Task", "entities": [[3, 5, "TaskName", "Sentiment Analysis"], [7, 9, "TaskName", "Text Generation"]]}
{"text": "Aspect category sentiment analysis has attracted increasing research attention . The dominant methods make use of pre - trained language models by learning effective aspect category - specific representations , and adding specific output layers to its pre - trained representation . We consider a more direct way of making use of pre - trained language models , by casting the ACSA tasks into natural language generation tasks , using natural language sentences to represent the output . Our method allows more direct use of pre - trained knowledge in seq2seq language models by directly following the task setting during pre - training . Experiments on several benchmarks show that our method gives the best reported results , having large advantages in few - shot and zero - shot settings .", "entities": [[2, 4, "TaskName", "sentiment analysis"], [90, 91, "MethodName", "seq2seq"]]}
{"text": "Aspect - based sentiment analysis ( ABSA ) is a finegrained sentiment analysis task that includes a number of subtasks , two of which are aspect category sentiment analysis ( ACSA ) and aspect category detection ( ACD ) . Figure 1 shows an example , where the input is \" The restaurant was expensive , but the menu was great \" . ACD detects the aspect categories , such as price and food , and ACSA predicts the sentiment polarities toward each aspect category . In this work , we focus on these two tasks as well as the joint task that combines both . Previous studies have investigated various methods that treat ACSA and ACD as classification tasks , learning aspect - specific sentence representations ( Wang et al , 2016 ; Ruder et al , 2016 ) . Recently , pre - trained language models ( PLM ) have shown their effectiveness to this end ( Jiang et al , 2019 ) . The main idea is to make use of pre - trained models such as BERT ( Devlin et al , 2019a ) for representing an aspect - specific form of the input ( e.g. , by concatenating the aspect category to the end of the input sentence ( Figure 3 ( a ) ) ) , which provides useful semantic features for ACSA and ACD classifiers . Such methods have given highly competitive results Li et al , 2020b ) . The above classification models benefit from contextualized representations , which contain knowledge learned by pre - training over large data ( Lin et al , 2019 ) . However , their use of pre - trained knowledge can be viewed as indirect due to at least two reasons . First , the classification task is performed by using a neural network on top of pretrained representation , with separate network parameters . Second , the integration of aspect category makes the aspect - specific input representation not exactly a natural language sentence , which differs from the pre - training setting . Intuitively , more pre - trained knowledge could be leveraged by connecting pre - training and ACSA at the task level , rather than only at the representation level . We investigate the above potentials by casting the sentiment classification tasks into language modelling tasks . In particular , as shown in Figure 2 , both ACSA and ACD are transformed into sequence - to - sequence ( seq2seq ) tasks , where the encoder takes the input sentence and the decoder generates a natural language sentence . For ACD , the output follows a template stating whether the specific aspect is discussed ( e.g. , \" The category_type category is discussed \" ) ; for ACSA , the sentiment polarity of a specific aspect is stated ( e.g. , \" The sentiment polarity of given_category is polarity_type \" ) . The setting corresponds closely to the denoising auto -", "entities": [[0, 5, "TaskName", "Aspect - based sentiment analysis"], [11, 13, "TaskName", "sentiment analysis"], [27, 29, "TaskName", "sentiment analysis"], [33, 36, "TaskName", "aspect category detection"], [180, 181, "MethodName", "BERT"], [391, 393, "TaskName", "language modelling"], [417, 418, "MethodName", "seq2seq"], [496, 497, "TaskName", "denoising"]]}
{"text": "The price category is discussed ( scoring : 0.9 ) The price category is not discussed ( scoring : 0.1 ) encoder training scheme of BART ( Lewis et al , 2020 ) , which we use as the pre - trained model . Compared with classification - based methods , our method does not include more network parameters , and thus can potentially generalize better to new domains ( Brown et al , 2020 ; Gao et al , 2020 ) . Given a new domain with completely unseen aspect categories and sentiment labels , our method can be applied without changing output layer structure .", "entities": [[25, 26, "MethodName", "BART"]]}
{"text": "In addition to classification - based methods , we take masked language models ( MLM ) as a baseline also , for which a natural counterpart of our method is a mask - refilling task . As shown in Figure 3 ( b ) , different from our method , the output template is concatenated to the input , with the keyword being masked for prediction . This MLM task corresponds closely to BERT ( Devlin et al , 2019a ) pre - training . In comparison to this MLM method , a generation method can better learn the correlation between the input and output template as two related sequences , which has been demonstrated by the strong performance of BART for abstractive text summarization ( Lewis et al , 2020 ) . Experimental results on three standard benchmarks datasets show that both generation and MLM methods outperform classification methods using the same pre - trained language models . Finally , generation methods give stronger performances than MLM methods , outperforming the previous stateof - the - art methods by a large margin . In addition , using the generation method , we show that jointly performing ACSA and ACD leads to better results than the traditional pipeline . To our knowledge , we are the first to employ a generative pre - trained language model to address an ACSA / ACD problem . We release our code at https://github . com / lgw863 / ACSA - generation .", "entities": [[14, 15, "DatasetName", "MLM"], [68, 69, "DatasetName", "MLM"], [73, 74, "MethodName", "BERT"], [89, 90, "DatasetName", "MLM"], [120, 121, "MethodName", "BART"], [122, 125, "TaskName", "abstractive text summarization"], [145, 146, "DatasetName", "MLM"], [167, 168, "DatasetName", "MLM"]]}
{"text": "Aspect Category Sentiment Analysis Wang et al ( 2016 ) propose an attention - based LSTM network , which can concentrate on different parts of a sentence when different aspect categories are taken as input . Ruder et al ( 2016 ) model the interdependencies of sentences in a text with a hierarchical bidirectional LSTM . Yin et al ( 2017 ) model the task as a machine comprehension problem by constructing pseudo question - answer pairs . Xue and Li ( 2018 ) Liang et al ( 2019 ) and incorporate aspect category information into sentence encoders in the context modeling stage . construct auxiliary sentences from the aspect categories and convert ACSA to a sentence - pair classification task . Li et al ( 2020b ) predict the sentiment of an aspect category mentioned in a sentence by aggregating the sentiments of the words indicating the aspect category in the sentence . Several joint models were proposed to avoid error propagation , which perform ACD and ACSA jointly . Schmitt et al ( 2018 ) propose two joint models : end - to - end LSTM and end - to - end CNN , which produce all the aspect categories and their corresponding sentiment polarities at once . Hu et al ( 2019 ) propose constrained attention networks ( CAN ) to constrain the attention weight allocation . propose the aspect - level sentiment capsules model ( AS - Capsules ) , which utilizes the correlation between aspect category and sentiment through shared components . Li et al ( 2020a ) propose a novel joint model which contains a shared sentiment prediction layer . All the models above are classification methods , which use a separate output network to give the output label . In contrast , we investigate natural language generation methods by directly following the pre - training process of language models . Masked Language Model Methods There is a line of work using the masked language model ( MLM ) for natural language understanding tasks . The basic idea is to leverage information from pre - trained models by defining specific sentence prompt in a language modelling task . Brown et al ( 2020 ) use prompt for few - shot learning in text classification tasks . rephrase inputs as cloze questions for text classification . and Gao et al ( 2020 )", "entities": [[2, 4, "TaskName", "Sentiment Analysis"], [15, 16, "MethodName", "LSTM"], [53, 55, "MethodName", "bidirectional LSTM"], [187, 188, "MethodName", "LSTM"], [333, 334, "DatasetName", "MLM"], [336, 339, "TaskName", "natural language understanding"], [360, 362, "TaskName", "language modelling"], [373, 377, "TaskName", "few - shot learning"], [378, 380, "TaskName", "text classification"], [388, 390, "TaskName", "text classification"]]}
{"text": "The menu was great < /s > extend by automatically generating label words and templates , respectively . Petroni et al ( 2019 ) extract relation between entities from BERT by constructing cloze - style templates . We are the first to apply such methods to ACSA , taking it as a baseline . Different from these template - based models , our final model uses BART for text generation , which better models the correlations between the input sentence and the output sentence compared with BERT . Generation Methods There has been work casting NLP problems as sequence generation tasks ( Vinyals et al , 2015 ; Ma et al , 2017 ; Stanovsky and Dagan , 2018 ; Raffel et al , 2020 ) , where the output is a sequence of tokens rather than a natural language sentence . Daza and Frank ( 2018 ) treat semantic role labelling as a sequence - to - sequence process . solve the entity - relation extraction task as a multi - turn question answering generation method . Our work is similar in casting an NLP task as a generation task . Different from the above methods , our goal is to make the most of pre - trained knowledge in BART for ACSA .", "entities": [[29, 30, "MethodName", "BERT"], [66, 67, "MethodName", "BART"], [68, 70, "TaskName", "text generation"], [86, 87, "MethodName", "BERT"], [165, 167, "TaskName", "relation extraction"], [173, 175, "TaskName", "question answering"], [211, 212, "MethodName", "BART"]]}
{"text": "Formally for ACD , the input is a sentence X = { x 1 , . . . , x n } = x 1 : n , where x i denotes the i - th word . For ACSA , a set of pre - identified aspect categories are also given . We introduce relevant pre - trained language models in 3.1 , classification methods in Section 3.2 , MLM methods in Section 3.3 , and our generation method in Section 3.4 .", "entities": [[70, 71, "DatasetName", "MLM"]]}
{"text": "We take BERT ( Devlin et al , 2019a ) and BART ( Lewis et al , 2020 ) as the pre - trained language models . Both are built on the Transformer ( Vaswani et al , 2017 ) architecture . BERT ( Devlin et al , 2019a ) is an encoder stack of Transformer for masked text filling , where a model uses the context words to predict masked words . BART ( Lewis et al , 2020 ) is a denoising auto - encoder seq2seq model pre - training for natural language generation . Its training applies document corruption such as randomly deleting tokens from the input and corrupting text with an arbitrary noising function . BART is trained to reconstruct the original text .", "entities": [[2, 3, "MethodName", "BERT"], [11, 12, "MethodName", "BART"], [32, 33, "MethodName", "Transformer"], [42, 43, "MethodName", "BERT"], [55, 56, "MethodName", "Transformer"], [73, 74, "MethodName", "BART"], [83, 84, "TaskName", "denoising"], [87, 88, "MethodName", "seq2seq"], [119, 120, "MethodName", "BART"]]}
{"text": "We use a multi - layer perceptrons network as the classifier model , which takes a representation vector as input . Both BERT and BART are considered as the encoders . BERT Classification BERT adopts \" [ CLS ] in - put sentence [ SEP ] given_category [ SEP ] \" as input . The final hidden state corresponding to \" [ CLS ] \" is used as the representation for classification . BART Classification BART adopts \" S input sentence /S given_category /S \" as input and predicts the sentiment polarity of the sentence towards the given category . The same input is fed into the encoder and decoder ( see Figure 3 ( a ) ) . Formally , suppose that the query category is a , x 0 = S , x n+1 = /S , x n+2 = a , x n+3 = /S , then the input to BART is x 0 : n+3 = S x 1 , . . . , x n /S a /S . The output hidden vec - tors obtained by the BART encoder ( ENCODER ) and BART decoder ( DECODER ) are : h enc = ENCODER ( x 0 : n+3 ) h 0 . . . h n+3 = DECODER ( h enc ; x 0 : n+3 ) The output vector h n+3 is then taken as the representation vector for classification .", "entities": [[22, 23, "MethodName", "BERT"], [24, 25, "MethodName", "BART"], [31, 32, "MethodName", "BERT"], [32, 33, "TaskName", "Classification"], [33, 34, "MethodName", "BERT"], [73, 74, "MethodName", "BART"], [74, 75, "TaskName", "Classification"], [75, 76, "MethodName", "BART"], [130, 131, "DatasetName", "0"], [153, 154, "MethodName", "BART"], [156, 157, "DatasetName", "0"], [183, 184, "MethodName", "BART"], [189, 190, "MethodName", "BART"], [202, 203, "DatasetName", "0"], [207, 208, "DatasetName", "0"], [220, 221, "DatasetName", "0"]]}
{"text": "Masked language models ( MLM ) ( Devlin et al , 2019a ) complete a given prompt by filling missing tokens . We refer to the template including a given category and MASK token together as a prompt . For sentiment analysis tasks , BERT MLM adopts the input sentence and the prompt as the model input and predicts the sentiment polarity label word towards the given category . For BART MLM , the same input is fed into the encoder and decoder , and the highest decoder prediction from label words of the MASK token is the predicted polarity label ( see Figure 3 ( b ) ) . We use the same template in the MLM method and generation method , following the template creation method in section 3.4.1 .", "entities": [[4, 5, "DatasetName", "MLM"], [40, 42, "TaskName", "sentiment analysis"], [44, 45, "MethodName", "BERT"], [45, 46, "DatasetName", "MLM"], [70, 71, "MethodName", "BART"], [71, 72, "DatasetName", "MLM"], [117, 118, "DatasetName", "MLM"]]}
{"text": "We take both ACSA and ACD as language model ranking problems under a seq2seq framework ( see Figure 3 ( c ) ) . The target sequence T a i , p k ( T a i ) = { t 1 , . . . , t m } is a template filled by the given category a i and the polarity type p k . We first introduce how to create templates in Section 3.4.1 , and then show the inference and training details in Section 3.4.2 and Section 3.4.3 , respectively .", "entities": [[13, 14, "MethodName", "seq2seq"]]}
{"text": "We compare our generation method with classification and MLM baselines ( Figure 3 ) using the same encoder . In particular , BART generation ( i.e. , Figure 3 ( c ) ) is compared with BART classification ( Figure 3 ( a ) ) and BART MLM ( Figure 3 ( b ) ) , as well as BERT classification and BERT MLM . In addition , our method is also compared with other models in the literature as follows . For sentence - level ACSA , we also compare our method with the following state - of - the - art methods in the literature . ( 1 ) non - BERT models : GCAE ( Xue and Li , 2018 ) , As - capsule and CapsNet ( Jiang et al , 2019 ) ; ( 2 ) BERT ( Devlin et al , 2019b ) based models : BERT - pair - QA - B , CapsNet - BERT ( Jiang et al , 2019 ) and AC - MIMLLN - BERT ( Li et al , 2020b ) . For document - level ACSA , we compare our method with the following methods . ( 1 ) non - BERT models : LSTM ( Tang et al , 2015 ) , HAN ( Yang et al , 2016 )", "entities": [[8, 9, "DatasetName", "MLM"], [22, 23, "MethodName", "BART"], [36, 37, "MethodName", "BART"], [46, 47, "MethodName", "BART"], [47, 48, "DatasetName", "MLM"], [59, 60, "MethodName", "BERT"], [62, 63, "MethodName", "BERT"], [63, 64, "DatasetName", "MLM"], [113, 114, "MethodName", "BERT"], [129, 130, "MethodName", "CapsNet"], [141, 142, "MethodName", "BERT"], [152, 153, "MethodName", "BERT"], [160, 161, "MethodName", "CapsNet"], [162, 163, "MethodName", "BERT"], [175, 176, "MethodName", "BERT"], [204, 205, "MethodName", "BERT"], [207, 208, "MethodName", "LSTM"]]}
{"text": "Figure 6 shows typical examples from the test set which can not be inferred by the BART classification model . In sentence ( a ) , the given category miscellaneous does not occur as a term in the given sentence . Our method can synthesize different sentiment polarities with different aspects to obtain correct polarity . In sentence ( b ) , \" the value on the kids menu is good \" , good modifies the value , rather than the given category menu . Our method gives the correct polarity , not being affected by the surrounding other aspect sentiments . The last instance ( c ) has conditional reasoning which is difficult for BART classification . In contrast , BART generation gives the correct label by correctly recognizing the negativity in \" if there was ... would be a bit more inviting \" . This is likely because our method makes use of pre - trained knowledge to infer the inter - sentential correlations between the input and the output sequences , which the BART classification model failed to achieve due to the indirect use of BART in the additional classification network .", "entities": [[16, 17, "MethodName", "BART"], [115, 116, "MethodName", "BART"], [121, 122, "MethodName", "BART"], [176, 177, "MethodName", "BART"], [188, 189, "MethodName", "BART"]]}
{"text": "We investigated a generation method for aspect category detection ( ACD ) and aspect category sentiment analysis ( ACSA ) , which can make better use of BART 's advantages in making semantic level summaries to the input by not introducing additional model parameters . Experiments show that our proposed method obtains superior performance over the baseline models for both sentence - level and document - level aspect sentiment analysis . In contrast to the traditional sentiment classification methods , our method is also more powerful on zero - shot and few - shot tasks .", "entities": [[6, 9, "TaskName", "aspect category detection"], [15, 17, "TaskName", "sentiment analysis"], [27, 28, "MethodName", "BART"], [68, 70, "TaskName", "sentiment analysis"]]}
{"text": "TripAdvisor ( Wang et al , 2010 ) and BeerAdvocate ( McAuley et al , 2012 ; Lei et al , 2016 ) contain seven aspects ( value , room , location , cleanliness , check in / front desk , service , and business service ) and four aspects ( feel , look , smell , and taste ) respectively . We randomly split them into training , development , and testing sets with 80/10/10 % . Statistics of these three sentence - level datasets are given in Table 9 and two document - level datasets are described in Table 10 .", "entities": [[9, 10, "DatasetName", "BeerAdvocate"]]}
{"text": "Each method is trained for 30 epochs , during which the model with the best performance on the validation set is saved . We also apply early stopping in training , which means that the training will stop if the performance on validation set does not improve in 5 epochs .", "entities": [[26, 28, "MethodName", "early stopping"]]}
{"text": "While describing the architecture of our system , we use the notions of a VMWE token ( its occurrence in running text ) and a VMWE type ( abstraction over all occurrences of a given VMWE ) , as introduced by Savary et al ( 2019b ) . We represent VMWE types as multisets of lemmas and POS . 2 Our system uses a mixture of discovery and identification methods , as defined by Constant et al ( 2017 ) . Namely , VMWE discovery consists in generating lists of MWE types out of context , while VMWE identification marks VMWE tokens in running text . The system is freely available online ( https://gitlab.com/ cpasquer / st_2020 ) . This work is licensed under a Creative Commons Attribution 4.0 International License . License details : http:// creativecommons.org/licenses/by/4.0/. 1 http://hdl.handle.net/11234/1 - 3367 2 VMWEs are represented as multisets ( i.e. bags of elements with repetition allowed ) , since the same lemma and/or POS can occur twice , as in appeler un chat un chat ' to call a cat a cat'\u21d2'to call a spade a spade ' . Seen2Seen in a nutshell Seen2Seen is a VMWE identification system dedicated to only those VMWEs which have been previously seen in the training data . Its detailed description is provided in Pasquer et al ( 2020 ) , but a brief overview is included here to make the current paper self - contained . Seen2Seen extracts lemma combinations of VMWEs seen in Train , looking for the same combinations ( within one sentence ) in Test , with an expected high recall . To improve precision , up to eight independent criteria can be used : ( 1 ) component lemmas should be disambiguated by their POS , ( 2 ) components should appear in specific orders ( e.g. the determiner before the noun ) , ( 3 ) the order of \" gap \" words possibly occurring between components is also considered , ( 4 ) components should not be too far from each other in a sentence , ( 5 ) closer components are preferred over distant ones , ( 6 ) components should be syntactically connected , ( 7 ) nominal components should appear with a previously seen inflection , and ( 8 ) nested VMWEs should be annotated as in Train . We select the combination of criteria with maximal performance on Dev among all 2 8 = 256 possibilities . The candidates remaining after applying the criteria are annotated as VMWEs . This relatively simple system relying on morphosyntactic filters and tuned for 8 parameters was evaluated on 11 languages of the PARSEME shared task 1.1 ( Ramisch et al , 2018 ) . Seen2Seen outperformed the best systems not only on seen ( F=0.8276 ) , but even on all seen and unseen VMWEs ( F=0.6653 ) . 3 In edition 1.2 of the PARSEME shared task , Seen2Seen scored best ( out of 2 ) in the global ranking of the closed track and second ( out of 9 ) across both tracks . It outperformed 6 other open track systems , notably those using complex neural architectures and contextual word embeddings . We believe that these competitive results are due to carefully taking the nature of VMWEs into account ( Savary et al , 2019a ) . Since Seen2Seen , by design , does not account for unseen VMWEs , its score in this category is very low ( F=1.12 ) . 4 Therefore , it was later extended with a VMWE discovery module . Seen2Unseen is precisely this extended system . It relies on Seen2Seen for seen VMWEs and on discovery methods described below for unseen VMWEs .", "entities": [[160, 161, "DatasetName", "lemma"], [183, 184, "MethodName", "spade"], [185, 186, "MethodName", "spade"], [244, 245, "DatasetName", "lemma"], [535, 537, "TaskName", "word embeddings"]]}
{"text": "We assume that seen VMWEs could help identify unseen ones by using ( i ) lexical variation , tolerated by some VMWEs ( e.g. take a bath / shower ) , and ( ii ) translation , e.g. ( FR ) prendre d\u00e9cision ' take decision ' = ( PL ) podejmowa\u0107 decyzj\u0119 = ( PT ) tomar decis\u00e3o = ( SV ) fatta beslut . 5 We also expect seen and unseen VMWEs to share characteristics , such as the distance between components or their syntactic dependency relations , e.g. nouns often being objects of verbs . The categories that should benefit from our strategy are , mainly , light - verb constructions ( LVCs ) containing nouns and , in some cases , verbal idioms ( VIDs ) . These categories are universal , so our method can be applied to the 14 languages of the PST . Since LVCs are often verb - noun pairs , Seen2Unseen quasiexclusively focuses on them . 6 Consequently , we do not aim at exhaustively identifying unseen VMWEs , but at determining to what extent seen verb - noun VMWEs can help us discover new unseen ones . Resources In addition to the PST Train , Dev and Test corpora , we used the CoNLL 2017 shared task parsed corpora , hereafter CoNLL - ST ( Ginter et al , 2017 ) . 7 The CoNLL - ST corpora were preferred over the PST - provided parsed corpora because they are conveniently released with pre - trained 100 - dimensional word2vec embeddings for the 14 languages of the PST , which we used to generate lexical variants . Additionally , we used a free library to implement translation towards French and Italian . 8 We automatically translated all VMWEs in the other 13 languages into French ( resp . Italian ) , privileged due to the availability of two Wikitionary - based lexicons in the same format for both languages . 3 In this paragraph we refer to macro - averaged MWE - based F - scores . 4 The score is not null due to different implementations of unseen VMWEs in the evaluation script and in Seen2Seen . 5 Languages are referred to with their PST identifier : e.g. FR for French . 6 We also model inherently reflexive verbs with cranberry words , i.e. verbs which never occur without a reflexive pronoun , e.g. ( FR ) s'\u00e9vanouir vs. * \u00e9vanouir . With 1 VMWE discovered in Portuguese and 3 in French , this module is omitted here . 7 http://hdl.handle.net/11234/1 - 1989 8 Googletrans : https://pypi.org/project/googletrans , implementing the Google Translate API . 9 For French : http://redac.univ - tlse2.fr/lexicons/glaff_en.html , for Italian : http://redac . univ - tlse2.fr/lexiques/glaffit.html 10 In case of multiple POS or lemmas , the most frequent verb - noun combination in CoNLL - ST was selected . Unseen VMWE identification To support identification of unseen VMWEs we use a combination of semi - supervised discovery and identification methods : lexical replacement , translation and statistical ranking . For a language L , let SeenV N L be the set of all seen LVC and VID types having exactly one verb and one noun ( and any number of components with other POS tags ) . Let each type in SeenV N L be linked with its manually annotated occurrences in Train . This set is used in the following steps : 2 Translation : By translating seen VMWE types in one language we obtain a list of VMWE type candidates in another language : T RAN S L is built only for French and Italian , and is empty for other languages . T RAN S F R ( resp . T RAN S IT ) contains automatic translations of each VMWE in SeenV N L , with L = FR ( resp . L = IT ) , into French ( resp . Italian ) . We eliminate translations which do not contain exactly one verb and one noun ( and possible components of other POS ) , e.g. due to a wrong translation . For the remaining translations , we keep only the verb and the noun lemmas . 3 Statistical ranking : This approach is based on statistical characteristics of both seen VMWEs and unseen VMWE candidates . We first calculate 3 sets of features for the whole SeenV N L list : Dist L is the maximal verb - noun distance for all VMWE tokens occurring at least twice in SeenV N L . This should help eliminate candidates whose components are too distant in a sentence . P L Dep ( Dep V , Dep N ) is the ratio of VMWE tokens in SeenV N L in which the incoming dependencies of the verb and of the noun are Dep V and Dep N . For instance , P F R Dep ( root , obj ) is higher than P F R Dep ( root , nsubj ) because , in French , active voice ( e.g. rendre une visite ' pay a visit ' ) is more frequent than passive voice ( e.g. malediction fut lanc\u00e9e ' curse was cast ' ) . We thus favour the most commonly observed VMWE dependencies . P L Dist ( i ) is the ratio of VMWE tokens in SeenV N L in which the number of words inserted between the verb and the noun is i. For instance , P F R Dist ( 0 ) = 0.46 , i.e. occurrences in which the verb and the noun are contiguous represent 46 % of SeenV N F R . This ratio tends to decrease as i increases : P F R Dist ( 2 ) = 0.11 , P F R Dist ( 5 ) = 0.006 , etc . Candidates whose number of intervening words i has higher P L Dist ( i ) likely are true VMWEs . Given these characteristics of seen VMWEs , we proceed to extracting and ranking unseen VMWE candidates . Namely , Cand L is the list of all occurrences of verb - noun pairs in Test such that : ( i ) the verb and the noun are directly connected by a syntactic dependency , ( ii ) the distance between the verb and the noun does not exceed Dist L , and ( iii ) the verb and the noun never co - occur with a direct dependency link in Train or in Dev . The latter condition excludes both seen VMWEs ( already covered by Seen2Seen ) and verb - noun constructions not annotated as VMWEs in Train or Dev , i.e. being no VMWEs , e.g. ( FR ) avoir an ' have year ' in elle a quinze ans ' she is 15 years old ' . Cand L is then ranked by considering statistical properties . For each candidate c in Cand L , we calculate three measures : P ( c ) is the estimated joint dependency - and distance - based probability . Suppose that i is the number of words inserted between c 's verb and noun , and their incoming dependencies are Dep V and Dep N , respectively . Then , P ( c ) = 21 ) 0 ( 0 ) 0 ( 0 ) 0 ( 0 ) SIM L 0 ( 0 ) 0 ( 0 ) 0.45 ( 11 ) 0.17 ( 6 ) 0 ( 0 ) 0 ( 0 ) 0 ( 0 ) RAN K L 0.19 ( 101 AM I ( c ) is the augmented mutual information of c 's type in the CoNLL - ST corpus . MWEs are known to have a Zipfian distribution and to often mix very frequent words with very rare ones . AMI is designed specifically to address this phenomenon , so as to leverage the rarely occurring expressions or components ( Zhang et al , 2009 ) : AM I ( x , y ) = log 2 c ) . Cand L is then ranked by RR ( c ) . We keep n top - ranked candidates , where n is estimated by scaling the number ( provided the organizers ) of VIDs and LVCs in Test - when all the expressions annotated as seen during the Seen2Seen phase have been eliminated - by the recall of our method on Dev on the target constructions ( unseen verb - noun LVCs and VIDs ) . 12 This n - best list is called RAN K L n . 4 Identification proper : In step 3 we obtain a list of unseen VMWE candidate tokens Cand L extracted from Test . The aim of identification is to discriminate among true and false VMWEs on this list . Statistical ranking and retaining top - n candidates is one possible statistically - based criterion . But we hypothesise that some candidates whose rank is worse than n , notably due to data sparseness , can still be correct if they result from lexical replacement or translation of seen VMWEs . Therefore , every c in Cand L is annotated as an LVC if c belongs to RAN K L n or if c 's type belongs to M IX L \u222a SIM L \u222a T RAN S L . P L Dep ( Dep V , Dep N ) \u00d7 P L Dist ( i ) . P ( x , y ) P ( x ) P ( y ) ( 1\u2212 P ( x , y ) P ( x ) ) ( 1\u2212 P ( x , y ) P ( y ) ) RR ( c )", "entities": [[441, 442, "DatasetName", "Google"], [579, 580, "TaskName", "Translation"], [602, 603, "MethodName", "RAN"], [621, 622, "MethodName", "RAN"], [629, 630, "MethodName", "RAN"], [927, 928, "DatasetName", "0"], [1229, 1230, "DatasetName", "0"], [1231, 1232, "DatasetName", "0"], [1233, 1234, "DatasetName", "0"], [1235, 1236, "DatasetName", "0"], [1237, 1238, "DatasetName", "0"], [1239, 1240, "DatasetName", "0"], [1243, 1244, "DatasetName", "0"], [1245, 1246, "DatasetName", "0"], [1247, 1248, "DatasetName", "0"], [1249, 1250, "DatasetName", "0"], [1259, 1260, "DatasetName", "0"], [1261, 1262, "DatasetName", "0"], [1263, 1264, "DatasetName", "0"], [1265, 1266, "DatasetName", "0"], [1267, 1268, "DatasetName", "0"], [1269, 1270, "DatasetName", "0"], [1271, 1272, "MethodName", "RAN"], [1364, 1365, "DatasetName", "RR"], [1442, 1443, "MethodName", "RAN"], [1552, 1553, "MethodName", "RAN"], [1571, 1572, "MethodName", "RAN"], [1633, 1634, "DatasetName", "RR"]]}
{"text": "Although Seen2Unseen uses 4 lists of candidates , here we analyse their contribution separately , that is , we use one list at a time in step 4 above . We report unseen MWE / token - based precision . 13 Sec . 3.1 analyses the impact of M IX L , SIM L and RAN K L n , while Sec . 3.2 discusses T RAN S L for French . 3.1 Impact of M IX L , SIM L and RAN K L n As shown in Table 1 , using M IX L alone leads to precision values above 0.29 for 7 languages out of 14 . Conversely , RAN K L alone mostly leads to values below 0.22 ( except for Hindi with P = 0.46 ) . The precision using SIM L alone reaches a maximum of 0.45 for Basque . The error analysis below suggests ways to improve precision . In French , using M IX F R alone yields 21 candidates in Test . Among the 5 false positives , there is one literal reading ( faire dessin ' make drawing ' ) , one omitted VMWE ( recevoir aide ' receive help ' ) and three other verb - noun pairs that could have been disregarded ( being coincidental occurrences ) if we had taken into account not only the existence of the syntactic dependency but also its nature ( e.g. nous avons VERB cinq points \u00e0 l'ordre NOUN.xcomp du jour ' we have five items on the agenda ' ) . This major problem for M IX L is shared by SIM L , but a specific drawback with SIM L is that not all words that occur in similar contexts are actually similar . Indeed , we obtain relevant generated unseen verb - noun pairs , including synonyms , antonyms and hyponyms , but also irrelevant ones . We should therefore either use more reliable resources , such as synonym / antonym dictionaries , and/or disregard frequent verbs ( to have , to do , etc . ) . For these frequent verbs , the more reliable equivalences obtained by M IX L compared to SIM L should be preferred ( faire ' do ' M IX F R = subir ' suffer ' vs. faire ' do ' SIM F R = passer ' pass ' ) . Indeed , as shown in Table 1 , over 5 languages with M IX L and SIM L candidates , 4 exhibit a better precision and higher number of candidates for M IX L . In French , by dividing n by 4 in RAN K F R n , the precision would have increased from 0.19 to 0.45 ( 18 VMWEs over 40 candidates ) . In other words , using RAN K L n in step 4 can slightly increase recall but causes a drop in precision , unless n is low . Hindi appears as an exception : no negative impact is observed with RAN K HI n due to a bias in the corpora ( compound mentioned in the dependency label ) . 3.2 Impact of T RAN S L : ( IT ) Traduttore , traditore ' translator , traitor ' ? With translational equivalences , we hypothesized that T RAN S L would lead to situations such as : exact matches : ( PT ) cometer crime ' commit a crime ' ( FR ) commettre crime , partial matches leading to VMWEs nonetheless : ( PT ) causar problema 'cause problem ' ( FR ) causer ennui , instead of causer probl\u00e8me , no match , but another VMWE : ( PT ) ter destaque ' highlight ' ( FR ) mettre en \u00e9vidence . literal , non - fluent or ambiguous translations ( Constant et al , 2017 ) : ( PT ) jogar o toalha ' throw the towel ' \u21d2'give up ' ( FR ) jeter la serviette instead of jeter l'\u00e9ponge ' throw the sponge ' , non - existing VMWEs in the target language : ( TR ) el atma ( FR ) lancer main ' throw hand ' We focus on French due to the high number of candidates in T RAN S F R . In Test - FR , among the 44 annotated verb - noun candidates using T RAN S F R alone , 18 are actually VMWEs and 3 partially correspond to VMWEs due to omitted determiners , yielding an unseen MWE - based precision of 0.41 and an unseen token - based precision value of 0.48 . These 21 candidates are mainly provided by Greek ( 10 vs. 6 from PT and 0 from IT or RO ) . Thus , the size of the training corpora may have more influence on the probability to obtain good translations than the source language family . The 23 false positives include ( i ) 13 candidates that can be VMWEs or not depending on the context , including coincidental co - occurrences , literal readings and errors in the manually annotated reference Test corpus , and ( ii ) 10 candidates that are not VMWEs , whatever the context , e.g. the inchoative commencer recherche ' start research ' ( from Hebrew ) or payer taxe ' pay tax ' ( from ( PL ) uiszcza\u0107 op\u0142at\u0119 ) . Consequently , translation may be a clue to discover unseen VMWEs , since 78 % of Cand F R \u2229 T RAN S F R are VMWEs out of context , but barely half of them were manually annotated in context . As highlighted above , a restriction to the most frequent VMWE syntactic relations could help filter out coincidental occurrences corresponding to 39 % of false positives ( e.g. lancer la balle \u00e0 la main OBL : MOD ' throw the ball with the hand ' ) .", "entities": [[55, 56, "MethodName", "RAN"], [66, 67, "MethodName", "RAN"], [82, 83, "MethodName", "RAN"], [112, 113, "MethodName", "RAN"], [443, 444, "MethodName", "RAN"], [471, 472, "MethodName", "RAN"], [506, 507, "MethodName", "RAN"], [530, 531, "MethodName", "RAN"], [554, 555, "MethodName", "RAN"], [713, 714, "MethodName", "RAN"], [733, 734, "MethodName", "RAN"], [789, 790, "DatasetName", "0"], [925, 926, "MethodName", "RAN"], [982, 983, "DatasetName", "MOD"]]}
{"text": "We proposed an error analysis for our system Seen2Unseen dedicated to unseen verb - noun VMWE identification . It reveals that lexical variation and translation can produce valid unseen VMWEs but their ambiguity in context must be solved : we should take into account both the dependency labels ( to avoid coincidental occurrences ) and the probability of the verb to be light in Train ( to avoid frequent co - ocurrences like fumer cigarette ' smoke cigarette ' ) . Using contextual rather than non - contextual word embeddings might also be helpful , even if computationally more intensive . We could also combine T RAN S L and M IX L \u222a SIM L by applying lexical substitution to the translated VMWEs .", "entities": [[88, 90, "TaskName", "word embeddings"], [106, 107, "MethodName", "RAN"]]}
{"text": "We examine three standard datasets in our experiments . Two have binary class labels ( Yelp , IMDB ) and the third has multi class labels ( AG News ) . These have been used in adversarial generation and defense research ( Zeng et al , 2021 ; Li et al , 2020 ) . All datasets can be found via huggingface 2 . 1 . IMDB - Movie review dataset for binary sentiment classification . 25k examples are provided for training and testing respectively . 2 . Yelp - Yelp dataset for binary sentiment classification on reviews of businesses extracted from the Yelp Dataset Challenge 3 . 560k examples are provided for training and 38k for testing . 3 . AG News - News articles from over 2000 news sources annotated by type of news : Sports , World , Business , and Science / Tech . 120k training and 7k test sets are provided . Following previous research , ( Li et al , 2020 ; we use all training data , and evaluate our method on random 1k samples of each dataset for the case where the local classifier does not employ Sample Shielding . Due to the high amount of queries used by the adversaries , we test on a subset of 100 samples for the case where the attacker 's local classifier employs Sample Shielding . 4", "entities": [[17, 18, "DatasetName", "IMDB"], [27, 29, "DatasetName", "AG News"], [66, 67, "DatasetName", "IMDB"], [121, 123, "DatasetName", "AG News"]]}
{"text": "We test our text classifier shielding strategy against 4 state - of - the - art ( SOTA ) text classifier attack algorithms . These algorithms have shown excellent performance in causing misclassifications while still producing readable texts . We defend against 3 word based attacks : TextFooler , Bert - Attack ( Li et al , 2020 ) , PWWS ( Ren et al , 2019 ) . TextFooler leverages word embeddings for word replacements , Bert - Attack leverages BERT itself by masking words and using BERT suggestions , PWWS selects and weights word replacements from WordNet . All three use some form of greedy selection for determining which words to replace . We also defend against a character based attack algorithm , TextBugger ( Li et al , 2019 ) .", "entities": [[71, 73, "TaskName", "word embeddings"], [81, 82, "MethodName", "BERT"], [88, 89, "MethodName", "BERT"]]}
{"text": "We test our shielding approach against 3 standard classifiers 5 used in previous research , e.g. ( Li et al , 2021a ; Li et al , 2020 ) : 1 . CNN - A word based CNN ( Kim , 2014 ) , with three window sizes ( 3 , 4 , 5 ) , 100 filters per window with dropout of 0.3 and Glove embeddings . 2 . LSTM - A word based bidirectional LSTM with 150 hidden units . As with the CNN a dropout of 0.3 is used and Glove embeddings are leveraged . 3 . BERT - The 12 layer BERT base model which has been fine - tuned on the corresponding dataset . These are provided by textattack via huggingface 6 .", "entities": [[65, 67, "MethodName", "Glove embeddings"], [70, 71, "MethodName", "LSTM"], [75, 77, "MethodName", "bidirectional LSTM"], [93, 95, "MethodName", "Glove embeddings"], [100, 101, "MethodName", "BERT"], [105, 106, "MethodName", "BERT"]]}
{"text": "Results are in Table 3 . As in the previous condition , classifiers perform well on original texts ( Table 1 ) with BERT often achieving the highest accuracies . In this setting , every query by an attacker requires k samples to be processed , which greatly increases attack time . Thus , we reduce k to 30 for these experiments . Sample Shielding repels attacks even when attacker uses Sample Shielding . We see that shielding is extremely successful in almost completely removing the negative effects of the attacks . For example , on the IMDB - TextFooler combination , attack success rate drops from 100 to 5 for LSTM , 100 to 1 for CNN , and 99 to 6 against BERT . The largest protection provided by Sample Shielding ( 100 % ) is for TextBugger vs CNN in IMDB . The smallest is for 85 % ( PWWS vs LSTM ) . On average the protection is 88.8 % . The recovered accuracies are only 13 to 0 percent away from the originals . These results show the power of Sample Shielding as even with knowledge of both the classifier and Sample Shielding , attacks struggle to perturb the text in a manner that causes W to fail . Furthermore , the attacks do worse with feedback from Sample Shielding . This shows the misleading nature of feedback from Sample Shielding , and unreliability when guiding attacks . 5 Additional Analysis", "entities": [[23, 24, "MethodName", "BERT"], [97, 98, "DatasetName", "IMDB"], [111, 112, "MethodName", "LSTM"], [124, 125, "MethodName", "BERT"], [143, 144, "DatasetName", "IMDB"], [154, 155, "MethodName", "LSTM"], [172, 173, "DatasetName", "0"]]}
{"text": "Due to the randomness of samples , there may be concern over the consistency of Sample Shielding . To address this , we ran Sample Shielding 100 times on the IMDB attacked texts from Table 3 against BERT classifier . Each time 30 random samples were used to vote . As can be observed from Figure 8 , Sample Shielding consistently protects against attacks . Median accuracies are above 80 % dropping only to 75 % in the worst case . This points to Sample Shielding as a consistent , reliable defense .", "entities": [[30, 31, "DatasetName", "IMDB"], [37, 38, "MethodName", "BERT"]]}
{"text": "Comparisons are limited as threat models differ . As noted earlier , other defenses assume a weaker threat model where the attacker queries the web - site 's shielded W directly . To make ours equivalent we compare SOTA results with our accuracies obtained by the attacker using W \u2032 alone ( with W = W \u2032 ) . We calculate accuracies right after the final perturbed text is generated using W \u2032 eliminating a followup round of W with Sample Shielding . 3 ) . While we do not know how FreeLB++ , RanMask , and similar defenses would perform with our threat model any deterministic shield would give the exact same results when the classifier is applied once again by the website .", "entities": [[75, 76, "DatasetName", "followup"]]}
{"text": "First , in future work we will add in direct comparisons to the two closest methods to Sample Shielding ( Zeng et al , 2021 ; Wang et al , 2021a ) . They are similar in spirit as they also work off samples though these are generated differently . We have not compared with them because these two papers appeared very recently , one last revised in July ( Zeng et al , 2021 ) and the other appeared in arXiv in September 2021 ( Wang et al , 2021a ) . Second , the neural net summarizer leverages a simple linear layer . Other networks , e.g. , LSTM , maybe better at finding patterns in sequential data . In future work we will also explore layering Sample Shielding onto other defense strategies . Another limitation of our current method is that we do not measure Sample Shielding 's effectiveness on other common text tasks including Natural Language Understanding . Additionally , datasets which contain the shortest texts ( e.g. not currently tested in our experiments . Since sample shielding removes texts , it 's performance could drop for these tasks and short texts . Thus , future work will include these comparisons .", "entities": [[81, 82, "DatasetName", "arXiv"], [102, 104, "MethodName", "linear layer"], [110, 111, "MethodName", "LSTM"], [158, 161, "TaskName", "Natural Language Understanding"]]}
{"text": "Defenses using voting . The most similar methods to our own are RanMask and RS&V both appearing within the last five months . RanMask ( Zeng et al , 2021 ) randomly masks tokens in input texts . This random masking occurs n times generating n inputs to be fed to a classifier . RS&V ( Wang et al , 2021a ) randomly replaces words in the input with synonyms . This it does k times to produce k samples which are then voted on . If the samples vote for a different label than the label produced by the unsampled input , then the text is labeled as an adversarial text . Our method is advantageous since it does not rely on specific models ( i.e. Masked Language Model ) or synonym sources . Adversarial training . Classifiers train on perturbed data , learning to identify modified versions of the original input ( Wang and Wang , 2020 ; Wang et al , 2021b ; Zhu et al , 2020 ; Li et al , 2021b ) . As an example , Gil et al ( 2019 ) propose HotFlip which uses white - box knowledge to generate adversarial attacks to train on . Specifically , they flip tokens based on the gradients of the one - hot input vectors . However , adversarial defenses are limited to known attackers . In contrast , Sample Shielding is ' plug - and - play ' as it is a pre - processing step . Other defenses . Several other shielding methods exist ( Keller et al , 2021 ; Eger et al , 2019 ; Zhu et al , 2021 ) . For example , Rodriguez and Galeano ( 2018 ) defend Perspective ( Google 's toxicity classification model ) by neutralizing adversarial inputs via a negated predicates list . Again , these defenses are restricted to contexts where specific lists may be identified , this is not so with Sample Shielding .", "entities": [[110, 112, "TaskName", "adversarial text"], [294, 295, "DatasetName", "Google"]]}
{"text": "IIE 's Neural Machine Translation Systems for WMT20", "entities": [[3, 5, "TaskName", "Machine Translation"]]}
{"text": "We participate in the WMT20 shared news translation task in one language pair and two language directions , German French and French German . Our methods are based on techniques and approaches used in submissions from past years ( Deng et al , 2018 ; Ng et al , 2019 ; Sun et al , 2019 ; Li et al , 2019 ; Xia et al , 2019 ) , including the use of subword models ( Sennrich et al , 2016 ) , iterative back - translation , knowledge distillation , model ensembling and several techniques we proposed recently ( Wei et al , 2020b , a ) . For our submissions of two language directions , we adopt the deep transformer architectures ( 48layer ) based on multiscale collaboration mechanism ( Wei et al , 2020b ) as our baseline , which outperformed the standard Transformer - Big as well as shallower models significantly in terms of translation quality . We also use an iterative back - translation approach with the controllable sampling to extend the back translation method by jointly training source - to - target and target - to - source NMT models . Moreover , the knowledge distillation ( Freitag et al , 2017 ) is employed to leverage the source - side monolingual data . For our final models , we apply a domainspecific fine - tuning process and model ensembling , and decode using noisy channel model re - ranking . The paper is structured as follows : Section 2 describes the techniques we used , then section 3 shows the experimental settings and results . Finally , we conclude our work in Section 4 .", "entities": [[39, 42, "DatasetName", "Deng et al"], [89, 91, "MethodName", "knowledge distillation"], [147, 148, "MethodName", "Transformer"], [201, 203, "MethodName", "knowledge distillation"]]}
{"text": "Back - translation ( BT ) is an effective and commonly used data augmentation technique to incorporate monolingual data into a translation system . Back - translation first trains an intermediate targetto - source system that is used to translate monolingual target data into additional synthetic parallel data . This data is used in conjunction with human translated bitext data to train the desired source - totarget system . In our work , we use an iterative back - translation approach to jointly train source - to - target and targetto - source NMT models . The process can be summarized as below : step 1 : we train both a source - to - target model ( M 0 x y ) and a target - to - source model ( M 0 y x ) using the human translated data . step 2 : we use M t x y to translate source - side monolingual data to target language , and use M t y x to translate target - side monolingual data to source language , where t starts from 0 . step 3 : we combine both the human translated data and pseudo data synthesized in step 2 to further optimize the two NMT models respectively . Repeat steps 2 - 3 until the models converge . In practice , we repeat 3 times for steps 2 - 3 . We apply the controllable sampling strategy ( Wei et al , 2020a ) to synthesize reasonable sentences which are at both high quality and diversity .", "entities": [[12, 14, "TaskName", "data augmentation"], [119, 120, "DatasetName", "0"], [133, 134, "DatasetName", "0"], [184, 185, "DatasetName", "0"]]}
{"text": "The early adoption of knowledge distillation ( KD ) ( Kim and Rush , 2016 ) is for model compression . We use the same method as in Sun et al ( 2019 ) that adopts hybrid heterogeneous teacher : base transformer , deep transformer , big transformer and RNMT+ . For each individual model , we use the other two models as the teacher model to further improve the performance . In addition , model ensemble is also used to boost the performance by combining the predictions of above four models at each decoding step .", "entities": [[4, 6, "MethodName", "knowledge distillation"], [18, 20, "TaskName", "model compression"]]}
{"text": "Fine - tuning with domain - specific data is a common and effective method to improve translation quality for a downstream task . After completing training on the bitext and back - translated data , we train for an additional epoch on a smaller in - domain corpus . We first select 100 K sentence - pairs from the bilingual as well as pseudo - generated data according to the filter method in Deng et al ( 2018 ) and continue to train the model on the filtered data .", "entities": [[73, 76, "DatasetName", "Deng et al"]]}
{"text": "N - best reranking is a method of improving translation quality by scoring and selecting a candidate hypothesis from a list of n - best hypotheses generated by a source - to - target model . For our submissions , we rerank the n - best hypotheses using two aspects as follows : log p ( y | x ) + \u03bb 1 log p ( x | y ) + \u03bb 2 log p ( y ) ( 7 ) The weights \u03bb 1 and \u03bb 2 are determined by tuning them with a random search on a validation set and selecting the weights that give the best performance .", "entities": [[95, 97, "MethodName", "random search"]]}
{"text": "We use all available bilingual datasets and select 10 M bilingual data from WMT'20 corpora using the script filter interactive.py 1 . We share a vocabulary for the two languages and apply BPE for word segmentation with 32 K merge operations . For monolingual data , we use 18 M German sentences and 18 M French sentences from Newscrawl , and pre - process them in the same way as bilingual data . We split 9k sentences from the \" dev08 - 14 \" as the validation set and use newstest 2019 as the test set .", "entities": [[32, 33, "MethodName", "BPE"]]}
{"text": "Through subsumption and instantiation , individual instances ( \" artificial intelligence \" , \" the spotted pig \" ) otherwise spanning a wide range of domains can be brought together and organized under conceptual hierarchies . The hierarchies connect more specific concepts ( \" computer science subfields \" , \" gastropubs \" ) to more general concepts ( \" academic disciplines \" , \" restaurants \" ) through IsA relations . Explicit or implicit properties applicable to , and defining , more general concepts are inherited by their more specific concepts , down to the instances connected to the lower parts of the hierarchies . Subsumption represents a crisp , universally - applicable principle towards consistently representing IsA relations in any knowledge resource . Yet knowledge resources often exhibit significant differences in their scope , representation choices and intended usage , to cause significant differences in their expected usage and impact on various tasks . This tutorial examines the theoretical foundations of subsumption , and its practical embodiment through IsA relations compiled manually or extracted automatically . It addresses IsA relations from their formal definition ; through practical choices made in their representation within the larger and more widely - used of the available knowledge resources ; to their automatic acquisition from document repositories , as opposed to their manual compilation by human contributors ; to their impact in text analysis and information retrieval . As search engines move away from returning a set of links and closer to returning results that more directly answer queries , IsA relations play an increasingly important role towards a better understanding of documents and queries . The tutorial teaches the audience about definitions , assumptions and practical choices related to modeling and representing IsA relations in existing , human - compiled resources of instances , concepts and resulting conceptual hierarchies ; methods for automatically extracting sets of instances within unlabeled or labeled concepts , where the concepts may be considered as a flat set or organized hierarchically ; and applications of IsA relations in information retrieval .", "entities": [[232, 234, "TaskName", "information retrieval"], [341, 343, "TaskName", "information retrieval"]]}
{"text": "Marius Pa\u015fca is a research scientist at Google . Current research interests include factual information extraction from unstructured text within documents and queries and its applications to Web search .", "entities": [[7, 8, "DatasetName", "Google"]]}
{"text": "The use of word embeddings is an important NLP technique for extracting meaningful conclusions from corpora of human text . One important question that has been raised about word embeddings is the degree of gender bias learned from corpora . Bolukbasi et al ( 2016 ) proposed an important technique for quantifying gender bias in word embeddings that , at its heart , is lexically based and relies on sets of highly gendered word pairs ( e.g. , mother / father and madam / sir ) and a list of professions words ( e.g. , doctor and nurse ) . In this paper , we document problems that arise with this method to quantify gender bias in diachronic corpora . Focusing on Arabic and Chinese corpora , in particular , we document clear changes in profession words used over time and , somewhat surprisingly , even changes in the simpler gendered defining set word pairs . We further document complications in languages such as Arabic , where many words are highly polysemous / homonymous , especially female professions words .", "entities": [[3, 5, "TaskName", "word embeddings"], [28, 30, "TaskName", "word embeddings"], [55, 57, "TaskName", "word embeddings"]]}
{"text": "Natural Language Processing ( NLP ) plays a significant role in many powerful applications such as speech recognition , text translation , and autocomplete and is at the heart of many critical automated decision systems making crucial recommendations about our future world . Word embedding systems are widely used to represent text data as vectors and enable NLP computation . Systems such as Word2Vec ( Mikolov et al , 2013 ) , GloVe ( Pennington et al , 2014 ) , and BERT ( Devlin et al , 2018 ) ingest large corpora of human text and can be used to learn semantic and syntactic relationships between words . At the same time , it has been demonstrated that these systems learn a wide variety of societal biases embedded in human text including racial bias , gender bias , and religious bias ( Caliskan et al , 2017 ; Abid et al , 2021 ) . In a widely cited paper , Bolukbasi et al ( 2016 ) demonstrated that a system trained with a corpora of Google News would complete the word comparison \" man is to computer programmer as woman is to what ? \" with the response \" homemaker \" suggesting an alarming level of gender bias when used in tasks such as sorting resumes for computer programming jobs . Chen et al ( 2021 ) extended these techniques beyond English to eight other languages ( Chinese , Spanish , Arabic , German , French , Farsi , Urdu , and Wolof ) and applied them to Wikipedia corpora in each of these languages . They documented persistent gender bias and lack of representation in the modern NLP pipeline . NLP research often uses large , modern datasets like Google News and Wikipedia . Developers of a wide variety of NLP - based applications begin with large pre - trained models that are also based on large corpora of human text ( Bender et al , 2021 ) . These pre - trained models also largely reflect the speech / writing of modern English speakers producing digital text . The speech / writing of speakers of the more than 7 , 000 languages spoken worldwide is often under - represented ( Wali et al , 2020 ) . Similarly , historical speech / writing is often under - represented despite the fact that historical speech / writing is often considered foundational to cultural identity . Investments in multilingual NLP and processing of diachronic corpora are essential if we want our NLP - based automated decision making systems to more widely reflect foundational cultural norms and identity from around the world . The inspiration for this paper was to re - examine Bolukbasi et al 's popular NLP - technique for quantifying gender bias from the perspective of applying it to diachronic corpora in Arabic and Chinese . Specifically , Bolukbasi et al 's method begins with identifying a set of profession words and a set of highly gendered word pairs ( defining set ) . In this paper , we explore the degree to which these words might change over time . We document ways in which this method is fundamentally fragile for diachronic corpora because of the way these sets of words would change over time . In Section 2 , for background , we elaborate on Bolukbasi et al and Chen et al 's multilingual extensions and some other relevant related work . Section 3 describes our experience with two different diachronic Arabic corpora , especially the impact on changes in profession set words over time . In Section 4 , we discuss changes in some defining set words in Chinese using the Google Ngram Viewer . We conclude and discuss future work in Section 5 . 2 Background and Related Work Bolukbasi et al ( 2016 ) pioneered a method for quantifying the amount of gender bias learned in by word embedding systems and many researchers have built on their techniques including Chen et al ( 2021 ) who observed substantial hurdles in extending the techniques beyond English . In this paper , we build on both Bolukbasi et al and Chen et al 's work to examine additional hurdles that would arise when attempting to apply these techniques to diachronic corpora . Bolukbasi et al 's original method is based on two sets of words . The first set ( the defining set ) consists of 10 highly gendered word pairs ( she - he , daughter - son , her - his , mother - father , woman - man , gal - guy , Mary - John , girl - boy , herself - himself , and female - male ) and the second ( profession set ) consists of 327 profession words such as nurse , teacher , writer , engineer , scientist , manager , driver , banker , musician , artist , and chef . They used the difference between the defining set word pairs to define a gendered vector space and then evaluated the relationship of the profession words relative to this gendered vector space . Ideally , profession words would not reflect a strong gender bias . However , in practice , they often do . According to such a metric , the word doctor might be male biased or the word nurse female biased based on how these words are used in the corpora from which the word embedding model was produced . Bolukbasi et al ( 2016 ) uses these two sets of words to compute a gender bias metric for each word and from there to express the gender bias of a corpora . Specifically , each word is expressed as a vector by Word2Vec and then the center of the vectors for each defining set pair is calculated . For example , to calculate the center of the definitional pair woman / man , they average the vector for \" woman \" with the vector for \" man \" . Then , they calculate the distance of each word in the definitional pair from the center by subtracting the center from each word in the pair ( e.g. , \" woman \" - center ) . They then apply Principal Component Analysis ( PCA ) to the matrix of these distances . PCA is an approach that compresses multiple dimensions into fewer dimensions , ideally in a way that the information within the original data is not lost . Usually , the number of reduced dimensions is 1 - 3 as it allows for easier visualization of a dataset . Bolukbasi et al ( 2016 ) used the first eigenvalue from the PCA matrix ( i.e. the one that is larger than the rest ) . Because the defining set pairs were chosen to be highly gendered , they expected this dimension to be related primarily to gender and therefore called it the gender direction or the g direction . Finally , the g direction is a vector , and there is a vector representing each word . Therefore , they used cosine similarity between the vector for each word , w , and the g direction vector as the measure of gender bias for that word . For a corpora or other collection of words , one can average the gender bias of words contained in the corpora as a measure of gender bias in the corpora using the equation of Bolukbasi et al ( 2016 ) for the direct gender bias of an embedding : DirectBias c = 1 | N | w N | cos ( \u20d7 w , g ) | c where N is the given gender neutral words , and c is a parameter that determines the strictness in measuring gender bias . Chen et al ( 2021 ) extended the Bolukbasi et al ' method to eight languages besides English - Chinese , Spanish , Arabic , German , French , Farsi , Urdu , and Wolof . In order to do so , they first made modifications to the defining set to make it more translatable across the 9 languages . For example , they dropped pairs like she - he , her - his , gal - guy , Mary - John , herself - himself , and femalemale because of problems in translation for some languages and adding pairs like queen - king , wifehusband , and madam - sir . Second , they observed that the Bolukbasi et al 's method can not be applied directly to languages such as Spanish , Arabic , German , French , and Urdu that primarily use grammatically gendered nouns ( e.g. , escritor / escritora in Spanish vs. writer in English ) . They solved this problem using a weighted average of the number of occurrences of each variant of the professional word ( male , female , or neutral ) multiplied by the gender bias score for that variant . In this work , we build on both ( Bolukbasi et al , 2016 ; Chen et al , 2021 ) and focus on the unique challenges that arise when applying these techniques to diachronic corpora . Specifically , we examined changes in both the profession set and defining set over time in Arabic and Chinese . Certainly , professions have changed drastically over that amount of time and so a method based on profession set words like Bolukbasi et al 's method will have substantial challenges . We explored this using corpora including a database of Arabic poems spanning 11 eras from the Pre - Islamic period ( before 610 ) to modern day . While we saw less change over time in the usage of the simpler defining set words than in the profession set words , we did observe some interesting changes in even the defining set words over time , especially in Chinese . In the process of this work , we also documented further complications in languages such as Arabic , where many words are highly polysemous / homonymous , especially female professions words . Wevers ( 2019 ) also used word embeddings to examine gender bias over time . They used a collection of Dutch Newspaper articles spanning over four eras , training four embedding models per newspaper , one per era , using the Gensim implementation of Word2Vec to demonstrate how word embeddings can be used to examine historical language change . They observed clear differences in gender bias and changes within and between newspapers over time . Slight shifting of bias was observed in some themes like shifting towards female bias in themes related to sexuality and leisure ( mostly seen in newspapers with religious background ) . Shifting towards male bias in themes related ' money ' , ' grooming ' , and negative emotions , especially in newspapers with a liberal background , was also observed . Rudolph and Blei ( 2018 ) developed dynamic embeddings building on exponential family embeddings to capture the language evolution or how the meanings of words change over time . They used three datasets of the U.S. Senate speeches from 1858 to 2009 , the history of computer science ACM abstracts from 1951 to 2014 , and machine learning papers on the ArXiv from 2007 to 2015 . They demonstrated how words like Intelligence , Iraq , computer , Bush , data change their meaning over time . They observed that the dynamic embeddings provided a better fit than classical embeddings and captured interesting patterns about how language changes . For example , a word 's meaning can change ( e.g. , computer ) ; its dominant meaning can change ( e.g. , values ) ; or its related subject matter can change ( e.g. , Iraq ) . Xu et al ( 2019 ) demonstrated the characterization of the semantic weights of subword units in the composition of word meanings . They used a subword - incorporated or a word embedding model variant for the evaluation and revealed interesting patterns change in multiple languages . Their training datasets consist of Wikimedia dumps for 6 Languages ( up until July 2017 ) consisting of Chinese and other Indo - European languages like English , French , German , and Italian . The results revealed major differences in the long - term temporal patterns of semantic weights between Chinese and five Indo - European languages . For example , in Chinese , the weights on subword units ( characters ) show a decreasing trend , i.e. , individual characters play less semantic roles in newer words than older ones whereas the opposite trend was observed in other languages . Therefore , Chinese words are treated more as a whole semantic unit \" synthetically \" , while words in Indo - European languages require more attention into the subword units \" analytically \" . These results provide evidence towards word formations to the linguistic theories . For example , the notion of \" word \" in Chinese is always changing : Modern Chinese has multiple characters as a whole semantic unit opposite to its older counterpart . The semantic weight carried by a single character is decreasing over time . This is strong evidence in support of the claim that Chinese has been evolving towards more detailed multisyllabic words from concise and monosyllabic words . , and token size ( all words ) for each time period . We did not train a GloVe model on the unknown books alone or the duplicate books and therefore are not reporting vocab size and token size .", "entities": [[16, 18, "TaskName", "speech recognition"], [72, 73, "MethodName", "GloVe"], [82, 83, "MethodName", "BERT"], [177, 178, "DatasetName", "Google"], [251, 252, "DatasetName", "Urdu"], [292, 293, "DatasetName", "Google"], [410, 412, "TaskName", "multilingual NLP"], [427, 429, "TaskName", "decision making"], [618, 619, "DatasetName", "Google"], [1051, 1052, "MethodName", "PCA"], [1060, 1061, "MethodName", "PCA"], [1120, 1121, "MethodName", "PCA"], [1338, 1339, "DatasetName", "Urdu"], [1448, 1449, "DatasetName", "Urdu"], [1703, 1705, "TaskName", "word embeddings"], [1745, 1747, "TaskName", "word embeddings"], [1882, 1883, "DatasetName", "ACM"], [1895, 1896, "DatasetName", "ArXiv"], [2264, 2265, "MethodName", "GloVe"]]}
{"text": "In this paper , we use two Arabic datasets : Shamela Library ( ) that is released by Shamela Library Foundation ( 2012 ) , and Arabic Poem Comprehensive Dataset ( APCD ) by ( Yousef et al , 2018 ) . Shamela Library is a free project that collects thousands of Islamic religious and other related sciences books . APCD is a collection of Arabic poems spanning 11 eras , from the Pre - Islamic ( before 610 ) to the Modern age ( 1924 - Now ) . Arabic NLP researchers commonly use these two datasets to study Arabic classics . We processed the Shamela Library dataset version of 6 , 538 Arabic books ( 6 , 527 unique books after removing duplicates ) in Microsoft Word format ( 1997 ) ( 1998 ) ( 1999 ) ( 2000 ) ( 2001 ) ( 2002 ) ( 2003 ) ( 2004 ) . 1 The books in this corpora were not labeled according to the publication dates . Thus , to study the language change over time in the Arabic language , we further classified Shamela 's Arabic books into three different time periods based ei - 1 We contribute the scripts we wrote to process these corpora and overcome several challenges with the data . For example , one challenge we faced was correctly converting back and forth between the Arabic Windows - 1256 to the Unicode ( UTF - 8 ) encoding schemes . The Arabic books were written in an old version of Microsoft Word ( 1997Word ( - 2004 , which caused encoding scheme conversion errors , resulting in unreadable characters by native Arabic speakers or even NLP tools . Scripts can be found here : https://github.com/ Clarkson - Accountability - Transparency/ gBiasRoadblocks ther on their publication date or the authors ' date of death when publication date was not available . We identified books written before Islam or before 610 ( only three books ) , books written before 1900 ( 2 , 820 books ) , and books written on or after 1900 ( 773 books ) . We were not able to identify publication dates or the authors ' dates of death of the remaining 2 , 931 books due to not having any ; Table 1 summarizes some key attributes of this dataset . We also processed the APCD , an Arabic poetry dataset that is collected mainly from the Poetry Encyclopedia ( ) that is released by Abu Dhabi Department of Culture and Tourism ( 2016 ) and Diwan ( ) ( Diwan , 2013 ) . Unlike Shamela , this dataset was already labeled by era , making it a good choice for studying language change over time . It has , before preprocessing , approximately 1 , 831 , 770 poetic verses labeled by their meter , the poet 's name , and the era they were written in . One drawback of this corpora is that it is relatively small . Table 2 summarizes some key attributes of this dataset . We then produced a total of 16 GloVe models ( Pennington et al , 2014 ) from the three time periods of Shamela , the 11 eras of APCD , all Shamela , and all APCD . 2 Each GloVe model is a contextindependent model that produces a one - word vector ( word embedding ) for each word even if that word appears in the context a few times unlike BERT and ELMo ( Devlin et al , 2018 ; Peters et al , 2018 ) . Each GloVe model provides vocabulary size , token size , and word vectors . It is important to note that before training GloVe models , it was necessary to preprocess the two datasets using Linux / Unix command - line utilities like tr ( for translating or deleting characters ) , sed ( for filtering and transforming text ) , iconv ( for converting between encoding schemes ) , and awk ( for pattern scanning and language processing ) , along with CAMeL tools ( Obeid et al , 2020 ) , an open - source python toolkit for Arabic NLP , to dediacritize the Arabic diacritical marks and remove unnecessary characters .", "entities": [[522, 523, "MethodName", "GloVe"], [554, 555, "MethodName", "GloVe"], [586, 587, "MethodName", "BERT"], [588, 589, "MethodName", "ELMo"], [604, 605, "MethodName", "GloVe"], [625, 626, "MethodName", "GloVe"], [637, 638, "DatasetName", "Linux"]]}
{"text": "We began with a consideration of how the profession sets used in Bolukbasi et al ( 2016 ) and Chen et al ( 2021 ) would need to change over time . First , we identified 50 modern profession words that we expect would simply not exist in the older time periods / eras in Shamela and APCD datasets . 3 For example , the profession of electrician would not have existed before the advent of electricity . Second , we identified 50 historical profession words that we think exist in older time periods / eras in Shamela and APCD datasets but which are much less common in modern times . As in Chen et al ( 2021 ) , we further categorized each word based on gender . In Arabic , most profession words have a male variant and a female variant in which the spelling is changed slightly based on gender , for example female pilot ( ) and male pilot ( ) . Linguistically , many professions that would be extremely uncommon for men or women do have a male or female version of the word ( e.g. , it is rare for a woman to have the profession cham - 3 We point to an expanded technical report with the full list of used modern and historical profession words . The report can be accessed here : https://lin - web.clarkson . edu/~jmatthew / LChange2022/ berlain / head of staff ( ) , but there is a female word for that profession ) . However , in some cases , either the male or female version does not even exist linguistically ( e.g. , there is no male word of midwife ( ) profession ) . There are also more rare neutral words , like musician ( ) , that is used for both genders with no spelling changes . In the APCD dataset , we found , as expected , that there are some modern professions that occur noticeably only in the modern era of the Arabic poems , but do not appear at all in the previous historical eras , such as the male engineer ( ) that occurs 17 times , and the neutral profession of an electrician ( ) that occurs only four times in the modern age , indicating that those modern professions are increasingly appearing in the modern age of the Arabic poems and confirming that Arabic native speakers ( i.e. , Arabs ) still use the poems as an effective way to document the Arabic language changes over time . On the other side of history , in the Shamela dataset , we found that a few historical professions frequently occur in the time periods before 1900 but not significantly after 1900 . Some professions reflect essential shifts in legality . For example , one profession that is fortunately no longer legal or acceptable is male slaver ( ) . Fortunately , the male slaver profession appears much less often ( only 12 times ) in the time period after 1900 , while it appears unpleasantly 118 times before the 1900 time periods . As another example , male chamberlain / head of staff ( ) appears 9 , 518 before the 1900 time periods , but only appears 914 times in ) in Shamela Library dataset in the time period before 1900 , demonstrating that its word cluster is including different words with different meanings due to its homonymy . b. A word cluster of chosen GloVe 's most similar words of the female profession trader ( ) in Shamela Library dataset in the time period after 1900 , illustrating that a new related - trading activity word joining the profession word cluster , ( trade/ ) 4 the time period after 1900 , showing that this male profession / position is on its way to extinction .", "entities": [[587, 588, "MethodName", "GloVe"]]}
{"text": "The Arabic language is one of the most morphologically rich languages , with a high level of orthographic ambiguity , causing native speakers to use the optional diacritical marks to differentiate between two words ( Grosvald et al , 2019 ) . 5 We noticed in the Shamela Library dataset that a few modern profession words change their connotations over time , and many profession words have alternate meanings due to the Arabic 's orthographical ambiguity . We also found that this was especially true of female profession words . For example , the word ( ) for female teacher also means a school building ( ) , another word ( ) for a female pilot also means an airplane 4 English translations of the word clusters are automatically generated using Google Translator API that is included in the deep - translator Python model ( https:// deep - translator.readthedocs.io ) . 5 In our preprocessing , we removed the optional diacritical marks as is generally recommended for Arabic NLP as a first step to reducing some data sparsity ( Obeid et al , 2020 ) . Unfortunately , removing diacritical marks increases the orthographic ambiguity , but retaining them would lead to a high degree of variance for the same word because the placement of diacritical marks varies with the grammatical placement of the word in a sentence . It is a difficult tradeoff for Arabic NLP that other researchers are attempting to tackle with advanced techniques , such as stemming and lemmatization ( Kadri and Nie , 2006 ; Mubarak , 2017 ) .", "entities": [[131, 132, "DatasetName", "Google"], [252, 253, "TaskName", "lemmatization"]]}
{"text": ") . In all these cases , this complicates the use of both word counts and word embeddings in tracking the relative uses of profession words over time . One homonymous example is the female trader ( ) profession . The same word ( ) also means common , famous , familiar , or circulating to describe a current news event . We see this alternate meaning dominate the usage of the word , complicating any attempt to study the prevalence of females engaged in this profession . Interestingly , we see evidence of change over time in the usage of this word . To investigate the semantic meaning of related words to the trading activity , we studied GloVe 's most similar words ( calculated based on the cosine similarity between two word vectors ) for this profession word in two time periods of the Shamela Library dataset : before 1900 and after 1900 . As shown in Figure 1a , before 1900 , none of most similar words reflect the trading profession word ( ) . However , in Figure 1b , after 1900 , we see a word related to trading activity ( trade/ ) appear in the most similar words of GloVe model . Thus , the connotation of the female trader ( ) profession is changing over time to more often reflect the actual profession of female trader ( ) and not just the alternate meaning of current news events .", "entities": [[16, 18, "TaskName", "word embeddings"], [119, 120, "MethodName", "GloVe"], [205, 206, "MethodName", "GloVe"]]}
{"text": "Although our primary focus in this study has been on Arabic , we found interesting evidence of change over time in Chinese as well . Classical Chinese ( before 1900 ) uses a vocabulary and grammar that differs significantly from modern Chinese . We were surprised to find evidence not just of changes in professions over time , but also changes in defining set words . As we found in the diachronic corpora in Arabic , we expected changes in profession words over hundreds of years , but thought that the more fundamental defining set words like woman / man , girl / boy and madam / sir would not change substantially . In Chinese , the word ' woman ' can be translated in many ways , including \" \u5973\u5b50 \" , \" \u5973\u4eba \" , and \" \u5987 \u5973 \" . The word \" \u5973\u5b50 \" was popularly used in an - Figure 3 : A timeline of word frequencies of different translations of word ' woman ' : \" \u5973\u5b50 \" , \" \u5973\u4eba \" , and \" \u5987 \u5973 \" that were found in multi - sources printed between 1500 and 2019 using Google Books Ngram Viewer . cient times , but its usage has decreased in modern writing . In Figure 3 , we used Google Books Ngram Viewer to chart the word frequencies of the different translations of the word ' woman ' : \" \u5973\u5b50 \" , \" \u5973 \u4eba \" , and \" \u5987\u5973 \" found in sources printed between 1500 and 2019 in Google 's Books corpora in English , Chinese , French , German , Hebrew , Italian , Russian , or Spanish ( Karch , 2021 ) . This shows us that as languages evolve over time , defining sets , like profession sets , may also have to evolve to measure gender bias using methods like the Bolukbasi et al ( 2016 ) 's method . Besides using Google Books Ngram Viewer , we also assembled a small collection of works that might be considered \" classics \" in Chinese spanning the period 475 BC - 1992 , for example \u53f8 \u9a6c\u8fc1 ( Records of the Grand Historian ) by Qian Sima , \u8427\u7ea2 ( Tales of Hulan River ) by Hong Xiao , and \u8bba\u8bed ( The Analects ) . We found that roughly half of the profession words used by Chen et al ( 2021 ) did not appear , and that also two of the defining set words \" boy \" and \" madam \" used did not appear . Interestingly , Google Books Ngram Viewer showed that the word ' madam ' was used very frequently between 1905 and 1910 , but our small classics corpora did not include texts written in that time period . Again , these results indicate that as languages evolve over time , profession sets and even defining set words would have to evolve to measure gender bias .", "entities": [[197, 198, "DatasetName", "Google"], [220, 221, "DatasetName", "Google"], [262, 263, "DatasetName", "Google"], [330, 331, "DatasetName", "Google"], [437, 438, "DatasetName", "Google"]]}
{"text": "In order for NLP to reflect the rich multilingual , multicultural , and historical heritage of human text , it is essential that NLP techniques be extended beyond modern digital English text to multilingual and diachronic corpora . In this paper , we have explored the challenges of applying an important technique for measuring the gender bias learned by word embedding systems to diachronic corpora . We also have shown how techniques like those pioneered by Bolukbasi et al ( 2016 ) and extended by Chen et al ( 2021 ) have fundamental limitations when analyzing corpora spanning large periods of time . We showed that their technique based on analyzing the gender bias of profession words would have difficulty because professions change drastically over hundreds of years . Interestingly , we also documented changes in defining and profession set words over time and also challenges with polysemous / homonymous profession words especially female profession words in Arabic . In this paper , we have focused mostly on identifying the problems with techniques applied successfully to measure gender bias in modern corpora like Google News or Wikipedia . In the future work , we plan to focus more on modifying profession sets and defining sets over time to overcome these problems . Our results indicate that as languages evolve over time , defining sets and profession sets would have to evolve to measure gender bias . In this study , we focused on Arabic and Chinese , but we would like to extend our work to more languages . Adding an English corpora may be our next step . Although we like to actively focus on languages besides English , English can serve as an important comparison point because so much of the modern NLP tool chain has been optimized for English . We may be able to study the impact of changes in profession sets and defining sets over time with fewer complicating factors . We would also like to experiment with different advanced Arabic NLP techniques like stemming and lemmatization ( Kadri and Nie , 2006 ; Mubarak , 2017 ) and see how applying such techniques could improve the results and reduce Arabic 's orthographical ambiguity or even other Arabic NLP - related current issues like correcting spelling errors , especially in Arabic dialects , where there are no official orthography rules ( Habash et al , 2018 ) .", "entities": [[183, 184, "DatasetName", "Google"], [341, 342, "TaskName", "lemmatization"]]}
{"text": "Pretrained multilingual language models have become a common tool in transferring NLP capabilities to low - resource languages , often with adaptations . In this work , we study the performance , extensibility , and interaction of two such adaptations : vocabulary augmentation and script transliteration . Our evaluations on part - of - speech tagging , universal dependency parsing , and named entity recognition in nine diverse low - resource languages uphold the viability of these approaches while raising new questions around how to optimally adapt multilingual models to low - resource settings .", "entities": [[50, 56, "TaskName", "part - of - speech tagging"], [58, 60, "TaskName", "dependency parsing"], [62, 65, "TaskName", "named entity recognition"]]}
{"text": "We expand on the dependency parsing evaluations of Chau et al ( 2020 ) by additionally considering named entity recognition and part - of - speech tagging . We follow Kondratyuk and Straka ( 2019 ) and compute the CWR for each token as a weighted sum of the activations at each MBERT layer . For dependency parsing , we follow the setup of Chau et al ( 2020 ) and Muller et al ( 2021 ) and use the CWRs as input to the graph - based dependency parser of Dozat and Manning ( 2017 ) . For named entity recognition , the CWRs are used as input to a CRF layer , while part - of - speech tagging uses a linear projection atop the representations . In all cases , the underlying CWRs are finetuned during downstream task training , and we do not add an additional encoder layer above the transformer outputs . We train models on five different random seeds and report average scores and standard errors .", "entities": [[4, 6, "TaskName", "dependency parsing"], [17, 20, "TaskName", "named entity recognition"], [21, 27, "TaskName", "part - of - speech tagging"], [52, 53, "MethodName", "MBERT"], [56, 58, "TaskName", "dependency parsing"], [99, 102, "TaskName", "named entity recognition"], [111, 112, "MethodName", "CRF"], [115, 121, "TaskName", "part - of - speech tagging"], [164, 165, "DatasetName", "seeds"]]}
{"text": "We select a set of nine typologically diverse lowresource languages for evaluation , including three of the original four used by Chau et al ( 2020 ) . These languages use three different scripts and are chosen based on the availability of labeled datasets and their exemplification of the three language types identified by Chau et al ( 2020 ) . Of the lan - guages seen by MBERT , all selected Type 0 languages are within the 45 largest Wikipedias , while the remaining Type 1 languages are within the top 100 . The Type 2 languages , which are excluded from MBERT , are all outside of the top 150 . 6 Additional information about the evaluation languages is given in Tab . 1 . Unlabeled Datasets Following Chau et al ( 2020 ) , we use articles from Wikipedia as unlabeled data for additional pretraining in order to reflect the original pretraining data . We downsample full articles from the largest Wikipedias to be on the order of millions of tokens in order to simulate a low - resource unlabeled setting , and we remove sentences that appear in the labeled validation or test sets . Labeled Datasets For dependency parsing and part - of - speech tagging , we use datasets and train / test splits from Universal Dependencies ( Nivre et al , 2020 ) , version 2.5 ( Zeman et al , 2019 ) . POS tagging uses language - specific partof - speech tags ( XPOS ) to evaluate understanding of language - specific syntactic phenomena . The Belarusian treebank lacks XPOS tags for certain examples , so we use universal part - of - speech tags instead . Dependency parsers are trained with gold word segmentation and no part - of - speech features . Experiments with named entity recognition use the WikiAnn dataset ( Pan et al , 2017 ) , following past work ( Muller et al , 2021 ; Pfeiffer et al , 2020 ; Wu and Dredze , 2020 ) . Specifically , we use the balanced train / test splits of ( Rahimi et al , 2019 ) . We note that UD datasets were unavailable for Meadow Mari , and partitioned WikiAnn datasets were missing for Wolof .", "entities": [[68, 69, "MethodName", "MBERT"], [73, 74, "DatasetName", "0"], [103, 104, "MethodName", "MBERT"], [202, 204, "TaskName", "dependency parsing"], [205, 211, "TaskName", "part - of - speech tagging"], [221, 223, "DatasetName", "Universal Dependencies"], [278, 281, "DatasetName", "part - of"], [296, 299, "DatasetName", "part - of"], [305, 308, "TaskName", "named entity recognition"], [310, 311, "DatasetName", "WikiAnn"], [365, 366, "DatasetName", "UD"], [375, 376, "DatasetName", "WikiAnn"]]}
{"text": "To measure the effectiveness of VA , we benchmark it against unadapted MBERT , as well as directly pretraining MBERT on the unlabeled data without modifying the vocabulary ( Chau et al , 2020 ; Muller et al , 2021 ; Pfeiffer et al , 2020 ) . Following Chau et al ( 2020 ) , we refer to the latter approach as language - adaptive pretraining ( LAPT ) . We also evaluate two monolingual baselines that are trained on our unlabeled data : fastText embeddings ( FASTT ; Bojanowski et al , 2017 ) , which represent a static word vector approach ; and a BERT model trained from scratch ( BERT ) . For ( Liu et al , 2019 ) with a language - specific SentencePiece tokenizer ( Kudo and Richardson , 2018 ) . For a fair comparison to VA , we use the same task - specific architectures and modify only the input representations .", "entities": [[12, 13, "MethodName", "MBERT"], [19, 20, "MethodName", "MBERT"], [85, 86, "MethodName", "fastText"], [107, 108, "MethodName", "BERT"], [113, 114, "MethodName", "BERT"], [129, 130, "MethodName", "SentencePiece"]]}
{"text": "Tab . 2 presents performance of the different input representations on POS tagging , dependency parsing , and named entity recognition . VA achieves strong results across all languages and tasks and is the top performer in the majority of them , suggesting that augmenting the vocabulary addresses MBERT 's limited vocabulary coverage of the target language and is beneficial during continued pretraining . The relative gains that VA provides appear to correlate not only with language type , as in the findings of Chau et al ( 2020 ) , but also with each language 's script . For instance , in Vietnamese , which is a Type 0 Latin script language , the improvements from VA are marginal at best , reflecting the Latindominated pretraining data of MBERT . Irish , the Type 1 Latin script language , is only slightly more receptive . However , Type 0 languages in Cyrillic and Arabic scripts , which are less represented in MBERT 's pretraining data , are more receptive to VA , with VA even outperforming all other methods for Urdu . This trend is amplified in the Type 2 languages , as the improvements for Maltese and Wolof are small but significant . However , they are dwarfed in magnitude by those of Uyghur , where VA achieves up to a 57 % relative error reduction over LAPT . This result corroborates the findings of both Chau et al ( 2020 ) and Muller et al ( 2021 ) and answers RQ1 . Prior to specialization , MBERT is especially poorly equipped to handle unseen lowresource languages and languages in non - Latin scripts due to its inability to model the script itself . In such cases , specialization via VA is beneficial , providing MBERT with explicit signal about the target language and script while maintaining its language - agnostic insights . On the other hand , this also motivates additional investigation into reme - dies for the script imbalance at a larger scale , e.g. , more diverse pretraining data .", "entities": [[14, 16, "TaskName", "dependency parsing"], [18, 21, "TaskName", "named entity recognition"], [48, 49, "MethodName", "MBERT"], [109, 110, "DatasetName", "0"], [129, 130, "MethodName", "MBERT"], [149, 150, "DatasetName", "0"], [162, 163, "MethodName", "MBERT"], [181, 182, "DatasetName", "Urdu"], [259, 260, "MethodName", "MBERT"], [297, 298, "MethodName", "MBERT"]]}
{"text": "Tab . 4 gives the results of our transliteration mix - in experiments . For the MBERT - based models , both VA and transliteration provide strong improvements over their respective baselines . Specifically , the improvements from LAPT to VA and LAPT to LAPT with transliteration are most pronounced . This verifies the independent results of Chau et al ( 2020 ) and Muller et al ( 2021 ) and suggests that in the non - Latin low - resource setting , unadapted additional pretraining is insufficient , but that the mix - in stage between initial and additional pretraining is amenable to performance - improving modifications . Unsurprisingly , transliteration provides no consistent improvement to the monolingual baselines , since the noisy transliteration process removes information without improving crosslingual alignment . However , VA and transliteration appear to interact negatively . Although VA with transliteration i m - proves over plain VA for Uyghur POS tagging and dependency parsing , it still slightly underperforms LAPT with transliteration for the latter . For the two NER experiments , VA with transliteration lags both methods independently . One possible explanation is that transliteration into Latin script serves as implicit vocabulary augmentation , with embeddings that have already been updated during the initial pretraining stage ; as a result , the two sources of augmentation conflict . Alternatively , since the transliteration process merges certain characters that are distinct in the original script , VA may augment the vocabulary with misleading character clusters . Either way , additional vocabulary augmentation is generally not as useful when combined with transliteration , answering RQ2 . Nonetheless , additional investigation into the optimal amount of vocabulary augmentation might yield a configuration that is consistently complementary to transliteration and is an interesting direction for future work . Furthermore , designing linguistically - informed transliteration schemes like those devised by Muller et al ( 2021 ) for Uyghur requires large amounts of time and domain knowledge . VA 's fully data - driven nature and relatively comparable performance suggest that it achieves an appealing balance between performance gain and implementation difficulty .", "entities": [[16, 17, "MethodName", "MBERT"], [159, 161, "TaskName", "dependency parsing"], [176, 177, "TaskName", "NER"]]}
{"text": "Our work follows a long line of studies investigating the performance of multilingual language models like MBERT in various settings . The exact source of such models ' crosslingual ability is contested : early studies attributed MBERT 's success to vocabulary overlap between languages ( Cao et al , 2020 ; Pires et al , 2019 ; Wu and Dredze , 2019 ) but subsequent studies find typological similarity and parameter sharing to be better explanations ( Conneau et al , 2020b ; K et al , 2020 ) . Nonetheless , past work has consistently highlighted the limitations of multilingual models in the context of low - resource languages . Conneau et al ( 2020a ) highlight the tension between crosslingual transfer and per - language model capacity , which poses a challenge for low - resource languages that require both . Indeed , Wu and Dredze ( 2020 ) find that MBERT is unable to outperform baselines in the lowest - resource seen languages . Our experiments build off these insights , which motivate the development of methods for adapting MBERT to target low - resource languages . Adapting Language Models Several prior studies have proposed methods for adapting pretrained models to a downstream task . The simplest of these is to perform additional pretraining on unlabeled data in the target language ( Chau et al , 2020 ; Muller et al , 2021 ; Pfeiffer et al , 2020 ) , which in turn builds off similar approaches for domain adaptation ( Gururangan et al , 2020 ; Han and Eisenstein , 2019 ) . Recent work uses one or more of these additional pretraining stages to specifically train modular adapter layers for specific tasks or languages , with the goal of maintaining a language - agnostic model while improving performance on individual languages ( Pfeiffer et al , 2020 ( Pfeiffer et al , , 2021aVidoni et al , 2020 ) . However , as Muller et al ( 2021 ) note , the typological diversity of the world 's languages ultimately limits the viability of this approach . On the other hand , many adaptation techniques have focused on improving representation of the target language by modifying the model 's vocabulary or tokenization schemes ( Chung et al , 2020 ; Clark et al , 2021 ; Wang et al , 2021 ) . This is wellmotivated : Artetxe et al ( 2020 ) emphasize representation in the vocabulary as a key factor for effective crosslingual transfer , while Rust et al ( 2021 ) find that MBERT 's tokenization scheme for many languages is subpar . Pfeiffer et al ( 2021b ) further observe that for languages with unseen scripts , a large proportion of the language is mapped to the generic \" unknown \" wordpiece , and they propose a matrix factorization - based approach to improve script representation . Wang et al ( 2020 ) extend MBERT 's vocabulary with an entire new vocabulary in the target language to facilitate zero - shot transfer to low - resource languages from English . The present study most closely derives from Chau et al ( 2020 ) , who select 99 wordpieces with the greatest amount of coverage to augment MBERT 's vocabulary while preserving the remainder ; and Muller et al ( 2021 ) , who transliterate target language data into Latin script to improve vocabulary coverage . We deliver new insights on the effectiveness and applicability of these methods .", "entities": [[16, 17, "MethodName", "MBERT"], [36, 37, "MethodName", "MBERT"], [153, 154, "MethodName", "MBERT"], [182, 183, "MethodName", "MBERT"], [252, 254, "TaskName", "domain adaptation"], [432, 433, "MethodName", "MBERT"], [471, 472, "MethodName", "wordpiece"], [494, 495, "MethodName", "MBERT"], [546, 547, "MethodName", "MBERT"]]}
{"text": "We explore the interactions between vocabulary augmentation and script transliteration for specializing multilingual contextual word representations in low - resource settings . We confirm vocabulary augmentation 's effectiveness on multiple languages , scripts , and tasks ; identify the mix - in stage as amenable to specialization ; and observe a negative interaction between vocabulary augmentation and script transliteration . Our findings highlight several open questions in model specialization and low - resource natural language processing at large , motivating further study in this area . Future directions for investigation are manifold . In particular , our results in this work unify the separate findings of past works , which use MBERT as a case study ; a natural continuation would extend these methods to a broader set of multilingual models , such as mT5 ( Xue et al , 2021 ) and XLM - R ( Conneau et al , 2020a ) , in order to obtain a clearer understanding of the factors behind specialization methods ' patterns of success . While we intentionally choose a set of small unlabeled datasets to evaluate on a setting applicable to the vast majority of the world 's low - resource languages , we acknowl - edge great variation in the amount of unlabeled data available in different languages . Continued study on the applicability of these methods to datasets of different sizes is an important future step . An interesting direction of work is to train multilingual models on data where script respresentation is more balanced , which might also allow for different output scripts for transliteration . Given that the mix - in stage is an effective opportunity to specialize models to target languages , constructing mix - ins at both the data and model level that are complementary by design has potential to be beneficial . Finally , future work might shed light on the interaction between different configurations of the adaptations studied here ( e.g. , the number of wordpiece types used in vocabulary augmentation ) .", "entities": [[111, 112, "MethodName", "MBERT"], [134, 135, "MethodName", "mT5"], [143, 144, "MethodName", "XLM"], [331, 332, "MethodName", "wordpiece"]]}
{"text": "We propose a framework that captures the denotational probabilities of words and phrases by embedding them in a vector space , and present a method to induce such an embedding from a dataset of denotational probabilities . We show that our model successfully predicts denotational probabilities for unseen phrases , and that its predictions are useful for textual entailment datasets such as SICK and SNLI .", "entities": [[62, 63, "DatasetName", "SICK"], [64, 65, "DatasetName", "SNLI"]]}
{"text": "The goal of textual entailment is to predict whether a hypothesis sentence is true , false , or neither based on the premise text ( Dagan et al , 2013 ) . Due in part to the Recognizing Textual Entailment ( RTE ) challenges ( Dagan et al , 2006 ) , the task of textual entailment recognition has received a lot of attention in recent years . Although full entailment recognition systems typically require a complete NLP pipeline , including coreference resolution , etc . , this paper considers a simplified variant of this task in which the premise and hypothesis are each a single sentence . This simplified task allows us to ignore the complexities that arise in longer texts , and instead focus on the purely semantic problem of how to represent the meaning of sentences . This version of the textual entailment task has been popularized by two datasets , the Sentences Involving Compositional Knowl - edge ( SICK ) dataset ( Marelli et al , 2014 ) and the Stanford Natural Language Inference ( SNLI ) corpus ( Bowman et al , 2015 ) , both of which involve a 3 - way classification for textual entailment . SICK was created for SemEval 2014 based on image caption data and video descriptions . The premises and hypotheses are automatically generated from the original captions and so contain some unintentional systematic patterns . Most approaches to SICK involve hand - engineered features or large collections of entailment rules ( Beltagy et al , 2015 ) . SNLI is the largest textual entailment dataset by several orders of magnitude . It was created with the goal of training neural network models for textual entailment . The premises in SNLI are captions from the FLICKR30 K corpus ( Young et al , 2014 ) . The hypotheses ( entailed , contradictory , or neutral in relation to the premise ) were solicited from workers on Mechanical Turk . Bowman et al ( 2015 ) initially illustrated the effectiveness of LSTMs ( Hochreiter and Schmidhuber , 1997 ) on SNLI , and recent approaches have focused on improvements in neural network architectures . These include sentence embedding models ( Liu et al , 2016 ; Munkhdalai and Yu , 2017a ) , neural attention models ( Rockt\u00e4schel et al , 2016 ; Parikh et al , 2016 ) , and neural tree - based models ( Munkhdalai and Yu , 2017b ; Chen et al , 2016 ) . In contrast , in this paper we focus on using a different input representation , and demonstrate its effectiveness when added to a standard neural network model for textual entailment . We demonstrate that the results of the LSTM model of Bowman et al ( 2015 ) can be improved by adding a single feature based on our predicted denotational probabilities . We expect to see similar improvements when our predicted probabilities are added to more complex neural network entailment models , but we leave those experiments for future work .", "entities": [[41, 42, "DatasetName", "RTE"], [81, 83, "TaskName", "coreference resolution"], [162, 163, "DatasetName", "SICK"], [175, 178, "TaskName", "Natural Language Inference"], [179, 180, "DatasetName", "SNLI"], [203, 204, "DatasetName", "SICK"], [240, 241, "DatasetName", "SICK"], [260, 261, "DatasetName", "SNLI"], [291, 292, "DatasetName", "SNLI"], [350, 351, "DatasetName", "SNLI"], [366, 368, "TaskName", "sentence embedding"], [458, 459, "MethodName", "LSTM"]]}
{"text": "Several related works have explored different approaches to learning vector space representations that express entailment more directly . Kruszewski et al ( 2015 ) learn a mapping from an existing distributional vector representation to a structured Boolean vector representation that expresses entailment as feature inclusion . They evaluate the resulting representation on lexical entailment tasks and on sentence entailment in SICK , but they re - strict SICK to a binary task and their sentence vectors result from simple composition functions ( e.g. addition ) over their word representations . Henderson and Popa ( 2016 ) learn a mapping from an existing distributional vector representation to an entailment - based vector representation that expresses whether information is known or unknown . However , they only evaluate on lexical semantic tasks such as hyponymy detection . Other approaches explore the idea that it may be more appropriate to represent a word as a region in space instead of a single point . Erk ( 2009 ) presents a word vector representation in which the hyponyms of a word are mapped to vectors that exist within the boundaries of that word vector 's region . Vilnis and McCallum ( 2015 ) use Gaussian functions to map a word to a density over a latent space . Both papers evaluate their models only on lexical relationships .", "entities": [[52, 54, "TaskName", "lexical entailment"], [60, 61, "DatasetName", "SICK"], [67, 68, "DatasetName", "SICK"]]}
{"text": "In contrast to traditional distributional similarities , Young et al ( 2014 ) introduced the concept of \" denotational similarities \" to capture which expressions can be used to describe similar situations . Young et al first define the visual denotation of a sentence ( or phrase ) s , s , as the ( sub ) set of images that s can describe . They estimate the denotation of a phrase and the resulting similarities from FLICKR30 K , a corpus of 30 , 000 images , each paired with five descriptive captions . In order to compute visual denotations from the corpus , they define a set of normalization and reduction rules ( e.g. lemmatization , dropping modifiers , replacing nouns with their hypernyms , dropping PPs , extracting NPs ) that augment the original FLICKR30 K captions with a large number of shorter , more generic phrases that are each associated with a subset of the FLICKR30 K images . The result is a large subsumption hierarchy over phrases , which Young et al call a denotation graph ( see Figure 1 ) . The structure of the denotation graph is similar to the idea of an entailment graph ( Berant et al , 2012 ) . Each node in the denotation graph corresponds to a phrase s , associated with its denotation s , i.e. the set of images that correspond to the original captions from which this phrase could be derived . For example , the denotation of a phrase \" woman jog on beach \" is the set of images in the corpus that depict a woman jogging on a beach . Note that the deno - tation of a node ( e.g. \" woman jog on beach \" ) is always a subset of the denotations of any of its ancestors ( e.g. \" woman jog \" , \" person jog \" , \" jog on beach \" , or \" beach \" ) . The denotational probability of a phrase s , P ( s ) , is a Bernoulli random variable that corresponds to the probability that a randomly drawn image can be described by s. Given a denotation graph over N images , P ( s ) = | s | N . The joint denotational probability of two phrases x and y , P ( x , y ) = | x \u2229 y | N , indicates how likely it is that a situation can be described by both x and y. Young et al propose to use pointwise mutual information scores ( akin to traditional distributional similarities ) and conditional probabilities P ( x | y ) = | x \u2229 y | | y | as so - called denotational similarities . In this paper , we will work with denotational conditional probabilities , as they are intended to capture entailment - like relations that hold due to commonsense knowledge , hyponymy , etc . ( what is the probability that x is true , given that y can be said about this situation ? ) . In an ideal scenario , if the premise p entails the hypothesis h , then the conditional probability P ( h | p ) is 1 ( or close to 1 ) . Conversely , if h contradicts p , then the conditional probability P ( h | p ) is close to 0 . We therefore stipulate that learning to predict the conditional probability of one phrase h given another phrase p would be helpful in predicting textual entailment . We also note that by the definition of the denotation graph , if x is an ancestor of y in the graph , then y entails x and P ( x | y ) = 1 . Young et al ( 2014 ) and show that denotational probabilities can be at least as useful as traditional distributional similarities for tasks that require semantic inference such as entailment or textual similarity recognition . However , their systems can only use deno - o x1 x2 y x P ( X , Y ) P ( X ) o x1 x2 y z x P ( X , Y ) P ( Y ) P ( X ) Figure 2 : An embedding space that expresses the individual probability of events X and Y and the joint probability P ( X , Y ) . tational probabilities between phrases that already exist in the denotation graph ( i.e. phrases that can be derived from the original FLICKR30 K captions ) . Here , we present a model that learns to predict denotational probabilities P ( x ) and P ( x | y ) even for phrases it has not seen during training . Our model is inspired by Vendrov et al ( 2016 ) , who observed that a partial ordering over the vector representations of phrases can be used to express an entailment relationship . They induce a so - called order embedding for words and phrases such that the vector x corresponding to phrase x is smaller than the vector y , i.e. x y , for phrases y that are entailed by x , where corresponds to the reversed product order on R N + ( x y \u21d4 x i \u2265 y i \u2200i ) . They use their model to predict entailment labels between pairs of sentences , but it is only capable of making a binary entailment decision .", "entities": [[116, 117, "TaskName", "lemmatization"], [574, 575, "DatasetName", "0"]]}
{"text": "We generalize this idea to learn an embedding space that expresses not only the binary relation that phrase x is entailed by phrase y , but also the probability that phrase x is true given phrase y. Specifically , we learn a mapping from a phrase x to an N - dimensional vector x R N + such that the vector x = ( x 1 , ... , x N ) defines the denotational probability of x as P ( x ) = exp ( \u2212 i x i ) . The origin ( the zero vector ) therefore has probability exp ( 0 ) = 1 . Any other vector x that does not lie on the origin ( i.e. i x i > 0 ) has probability less than 1 , and a vector x that is farther from the origin than a vector y represents a phrase x that has a smaller denotational probability than phrase y. We can visualize this as each phrase vector occupying a region in the embedding space that is proportional to the denotational probability of the phrase . Figure 2 illustrates this in two dimensions . The zero vector at the origin has a probability pro - portional to the entire region of the positive orthant , while other points in the space correspond to smaller regions and thus probabilities less than 1 . The joint probability P ( x , y ) in this embedding space should be proportional to the size of the intersection of the regions of x and y. Therefore , we define the joint probability of two phrases x and y to correspond to the vector z that is the element - wise maximum of x and y : z i = max ( x i , y i ) . This allows us to compute the conditional probability P ( x | y ) as follows : P ( x | y ) = P ( x , y ) P ( y ) = exp ( \u2212 i z i ) exp ( \u2212 i y i ) = exp ( i y i \u2212 i z i ) Shortcomings We note that this embedding does not allow us to represent the negation of x as a vector . We also can not represent two phrases that have completely disjoint denotations : in Figure 2 , the P ( X ) and P ( Y ) regions will always intersect and therefore the P ( X , Y ) region will always have an area greater than 0 . In fact , in our embedding space , the joint probability represented by the vector z will always be greater than or equal to the product of the probabilities represented by the vectors x and y. For any pair x = ( x 1 , ... , x N ) and y = ( y 1 , ... , y N ) , P ( X , Y ) \u2265 P ( X ) P ( Y ) : P ( X , Y ) = exp \u2212 i max ( x i , y i ) \u2265 exp \u2212 i x i \u2212 i y i = P ( X ) P ( Y ) ( Equality holds when x and y are orthogonal , and thus i x i + i y i = i max ( x i , y i ) ) . Therefore , the best we can do for disjoint phrases is learn an embedding that assumes the phrases are independent . In other words , we can map the disjoint phrases to two vectors whose computed joint probability is the product of the individual phrase probabilities . Although our model can not represent two events with completely disjoint denotations , we will see below that it is able to learn that some phrase pairs have very low denotational conditional probabilities . We note also that our model \u2026 \u2026 w 0 w i w i+1 w N Glove embeddings LSTM RNN 512D FF p ( x ) LSTM is run once per phrase x , y p ( y )", "entities": [[104, 105, "DatasetName", "0"], [126, 127, "DatasetName", "0"], [433, 434, "DatasetName", "0"], [672, 673, "DatasetName", "0"], [679, 681, "MethodName", "Glove embeddings"], [681, 682, "MethodName", "LSTM"], [689, 690, "MethodName", "LSTM"]]}
{"text": "Our model up to this point has only been trained on short phrases , since conditional probabilities in the denotation graph are only reliable for phrases that occur with multiple images ( see Figure 5 for the distribution of phrase lengths in the training data ) . To improve our model 's performance on longer sentences , we add the SNLI training data ( which has a mean sentence length of 11 words ) to our training data . We train a new model from scratch on a corpus consisting of the previously described 42 million phrase pairs and the 550 , 000 SNLI training sentence pairs ( lemmatized to match our phrase pairs ) . We do not train on SICK because the corpus is much smaller and has a different distribution of phenomena , including explicit negation . We augment the SNLI data with approximate gold denotational probabilities by assigning a probability P ( S ) = s / N to a sentence S that occurs s times in the N training sentences . We assign approximate gold conditional probabilities for each sentence pair p , h according to the entailment label : if p entails h , then P ( h | p ) = 0.9 . If p contradicts h , then P ( h | p ) = 0.001 . Otherwise , P ( h | p ) = 0.5 . Figure 6 shows the predicted probabilities on the SNLI test data when our model is trained on different distributions of data . The top row shows the predictions of our model when trained only on short phrases from the denotation graph . We observe that the median probabilities increase from contradiction to neutral to entailment , even though this model was only trained on short phrases with a limited vocabulary . Given the training data , we did not expect these probabilities to align cleanly with the entailment labels , but even so , there is already some information here to distinguish between entailment classes . The bottom row shows that when our model is trained on both denotational phrases and SNLI sentence pairs with approximate conditional probabilities , its probability predictions for longer sentences improve . This model 's predicted conditional probabilities align much more closely with the entailment class labels . Entailing sentence pairs have high conditional probabilities ( median 0.72 ) , neutral sentence pairs have mid - range conditional probabilities ( median 0.46 ) , and contradictory sentence pairs have conditional probabilities approaching 0 ( median 0.19 ) .", "entities": [[60, 61, "DatasetName", "SNLI"], [103, 104, "DatasetName", "SNLI"], [121, 122, "DatasetName", "SICK"], [143, 144, "DatasetName", "SNLI"], [244, 245, "DatasetName", "SNLI"], [357, 358, "DatasetName", "SNLI"], [423, 424, "DatasetName", "0"]]}
{"text": "Neutral 6 A group of young people with instruments are on stage . People are playing music . 0.86 7 Two doctors perform surgery on patient . Two doctors are performing surgery on a man . 0.56 8 Two young boys of opposing teams play football , while wearing full protection uniforms and helmets . Boys scoring a touchdown . 0.30 9 Two men on bicycles competing in a race . Men are riding bicycles on the street . 0.24 Contradiction 10 Two women having drinks and smoking cigarettes at the bar . Three women are at a bar . 0.79 11 A man in a black shirt is playing a guitar . The man is wearing a blue shirt . 0.47 12 An Asian woman sitting outside an outdoor market stall . A woman sitting in an indoor market . 0.22 13 A white dog with long hair jumps to catch a red and green toy . A white dog with long hair is swimming underwater . 0.09 14 Two women are embracing while holding to go packages . The men are fighting outside a deli . 0.06 In examples 10 and 11 , our model predicts low probabilities for occasionally co - occurring events , which are still more likely than the improbable cooccurrence in example 13 . Table 5 demonstrates similar patterns for pairs where both phrases were unseen . Table 6 has examples of predicted conditional probabilities for sentence pairs from the SNLI development data . Some cases of entailment are straightforward , so predicting high conditional probability is relatively easy . This is the case with example 2 , which simply involves dropping words from the premise to reach the hypothesis . In other cases , our model correctly predicts high conditional probability for an entailed hypothesis that does not have such obvious word - to - word correspondence with the premise , such as example 1 . Our model 's predictions are less accurate when the sentence structure differs substantially between premise and hypothesis , or when there are many unknown words , as in example 5 . For neutral pairs , our model usually predicts midrange probabilities , but there are some exceptions . In example 6 , it is not certain that the people are playing music , but it is a reasonable assumption from the premise . It makes sense that in this case , our model assigns this hypothesis a higher conditional probability given the premise than for most neutral sentence pairs . In example 7 , we might guess that the patient is a man with 50 % probability , so the predicted conditional probability of our model seems reasonable . Our model can not reason about numbers and quantities , as example 10 shows . It also fails to predict in example 11 that a man wearing a black shirt is probably not wearing a blue shirt as well . However , our model does correctly predict low probabilities for some contradictory examples that have reasonably high word overlap , as in example 13 . Finally , example 14 shows that our model can correctly predict very low conditional probability for sentences that share no common subject matter .", "entities": [[246, 247, "DatasetName", "SNLI"]]}
{"text": "This work was supported by NSF Grants 1563727 , 1405883 , and 1053856 , and by a Google Research Award . Additional thanks to Yonatan Bisk and Pooya Khorrami .", "entities": [[17, 18, "DatasetName", "Google"]]}
{"text": "Research areas related to this work include integrated logical KRR and RL , relational RL , and integrated KRR and probabilistic planning . Logical KRR has previously been integrated with RL . Action knowledge ( McDermott et al , 1998 ; Jiang et al , 2019 ) has been used to reason about action sequences and help an RL agent explore only the states that can potentially contribute to achieving the ultimate goal ( Leonetti et al , 2016 ) . As a result , their agents learn faster by avoiding choosing \" unreasonable \" actions . A similar idea has been applied to domains with nonstationary dynamics ( Ferreira et al , 2017 ) . More recently , task planning was used to interact with the high level of a hierarchical RL framework ( Yang et al , 2018 ) . The goal shared by these works is to enable RL agents to use knowledge to improve the performance in learning ( e.g. , to learn faster and/or avoid risky exploration ) . However , the KRR capabilities of these methods are limited to logical action knowledge . By contrast , we use a logicalprobabilistic KRR paradigm that can directly reason with probabilities learned from RL . Relational RL ( RRL ) combines RL with relational reasoning ( D\u017eeroski et al , 2001 ) . Action models have been incorporated into RRL , resulting in a relational temporal difference learning method ( Asgharbeygi et al , 2006 ) . Recently , RRL has been deployed for learning affordance relations that forbid the execution of specific actions ( Sridharan et al , 2017 ) . These RRL methods , including deep RRL ( Zambaldi et al , 2018 ) , exploit structural representations over states and actions in ( only ) current tasks . In this research , KRR - RL supports the KRR of world factors beyond those in state and action representations , e.g. , time in navigation tasks , as detailed in Section 4.2 . The research area of integrated KRR and probabilistic planning is related to this research . Logicalprobabilistic reasoning has been used to compute informative priors and world dynamics Amiri et al , 2020 ) for probabilistic planning . An action language was used to compute a deterministic sequence of actions for robots , where individual actions are then implemented using probabilistic controllers ( Sridharan et al , 2019 ) . Recently , human - provided information has been incorporated into belief state representations to guide robot action selection ( Chitnis et al , 2018 ) . In comparison to our approach , learning ( from reinforcement or not ) was not discussed in the abovementioned algorithms . Finally , there are a number of robot reasoning and learning architectures ( Tenorth and Beetz , 2013 ; Oh et al , 2015 ; Hanheide et al , 2017 ; , which are relatively complex , and support a variety of functionalities . In comparison , we aim at a concise representation for robot KRR and RL capabilities . To the best of our knowledge , this is the first work on a tightly coupled integration of logical - probabilistic KRR with model - based RL .", "entities": [[59, 60, "DatasetName", "agent"], [216, 218, "TaskName", "relational reasoning"]]}
{"text": "Following the Markov assumption , a Markov decision process ( MDP ) can be described as a fourtuple S , A , T , R ( Puterman , 1994 ) . S defines the state set , where we assume a factored space in this work . A is the action set . T : S \u00d7 A \u00d7 S [ 0 , 1 ] specifies the state transition probabilities . R : S \u00d7 A R specifies the rewards . Solving an MDP produces an action policy \u03c0 : s a that maps a state to an action to maximize long - term rewards . RL methods fall into classes including modelbased and model - free . Model - based RL methods learn a model of the domain by approximating R ( s , a ) and P ( s | s , a ) for state - action pairs , where P represents the probabilistic transition system . An agent can then use planning methods to calculate an action policy ( Sutton , 1990 ; Kocsis and Szepesv\u00e1ri , 2006 ) . Model - based methods are particularly attractive in this work , because they output partial world models that can better accommodate the diversity of tasks we are concerned with , c.f . , modelfree RL that is typically goal - directed . One of the best known examples of model - based RL is R - Max ( Brafman and Tennenholtz , 2002 ) , which is guaranteed to learn a near - optimal policy with a polynomial number of suboptimal ( exploratory ) actions . The algorithm classifies each state - action pair as known or unknown , according to the number of times it was visited . When planning on the model , known state - actions are modeled with the learned reward , while unknown stateactions are given the maximum one - step reward , R max . This \" maximum - reward \" strategy automatically enables the agent to balance the exploration of unknown states and exploitation . We use R - Max in this work , though KRR - RL practitioners can use supervised machine learning methods , e.g. , imitation learning ( Osa et al , 2018 ) , to build the model learning component .", "entities": [[61, 62, "DatasetName", "0"], [161, 162, "DatasetName", "agent"], [335, 336, "DatasetName", "agent"], [369, 371, "TaskName", "imitation learning"]]}
{"text": "KRR paradigms are concerned with concisely representing and robustly reasoning with declarative knowledge . Answer set programming ( ASP ) is a non - monotonic logical KRR paradigm ( Baral , 2010 ; Gelfond and Kahl , 2014 ) building on the stable model semantics ( Gelfond and Lifschitz , 1988 ) . An ASP program consists of a set of logical rules , in the form of \" head : - body \" , that read \" head is true if body is true \" . Each ASP rule is of the form : where a ... f are literals that correspond to true or false statements . Symbol not is a logical connective called default negation ; not l is read as \" it is not believed that l is true \" , which does not imply that l is false . ASP has a variety of applications ( Erdem et al , 2016 ) . Traditionally , ASP does not explicitly quantify degrees of uncertainty : a literal is either true , false or unknown . P - log extends ASP to allow probability atoms ( or pr - atoms ) ( Baral et al , 2009 ; Balai and Gelfond , 2017 ) . The following pr - atom states that , if B holds , the probability of a ( t ) = y is v : pr ( a ( t ) = y | B ) = v. where B is a collection of literals or their default negations ; a is a random variable ; t is a vector of terms ( a term is a constant or a variable ) ; y is a term ; and v [ 0 , 1 ] . Reasoning with an ASP program generates a set of possible worlds : { W 0 , W 1 , } . The pr - atoms in P - log enable calculating a probability for each possible world . Therefore , P - log is a KRR paradigm that supports both logical and probabilistic inferences . We use P - log in this work for KRR purposes .", "entities": [[288, 289, "DatasetName", "0"], [307, 308, "DatasetName", "0"]]}
{"text": "KRR - RL integrates logical - probabilistic KRR and model - based RL , and is illustrated in Figure 1 . The KRR component includes both declarative qualitative knowledge from humans and the probabilistic knowledge from model - based RL . When the robot is free , the robot arbitrarily selects goals ( different navigation goals in our case ) to work on , and learns the world dynamics , e.g. , success rates and costs of navigation actions . When a task becomes available , the KRR component dynamically constructs a partial world model ( excluding unrelated factors ) , on which a task - oriented controller is computed using planning algorithms . Human knowledge concerns environment variables and their dependencies , i.e. , what variables are related to each action . For instance , the human provides knowledge that navigation actions ' success rates depend on current time and area ( say elevator areas are busy in the mornings ) , while the robot must learn specific probabilities by interacting with the environment . Why is KRR - RL needed ? Consider an indoor robot navigation domain , where a robot wants to maximize the success rate of moving to goal positions through navigation actions . Shall we include factors , such as time , weather , positions of human walkers , etc , into the state space ? On the one hand , to ensure model completeness , the answer should be \" yes \" . Human walkers and sunlight ( that blinds robot 's LiDAR sensors ) reduce the success rates of the robot 's navigation actions , and both can cause the robot irrecoverably lost . On the other hand , to ensure computational feasibility , the answer is \" no \" . Modeling whether one specific grid cell being occupied by humans or not introduces one extra dimension in the state space , and doubles the state space size . If we consider ( only ) ten such grid cells , the state space becomes 2 10 \u2248 1000 times bigger . As a result , RL practitioners frequently have to make a trade - off between model completeness and computational feasibility . In this work , we aim at a framework that retains both model scalability and computational feasibility , i.e. , the agent is able to learn within relatively little memory while computing action policies accounting for a large number of domain variables .", "entities": [[186, 188, "TaskName", "robot navigation"], [390, 391, "DatasetName", "agent"]]}
{"text": "In factored spaces , state variables V = { V 0 , V 1 , ... , V n\u22121 } can be split into two categories , namely endogenous variables V en and exogenous variables V ex ( Chermack , 2004 ) , where V en = { V en 0 , V en 1 , ... , V en p\u22121 } and V ex = { V ex 0 , V ex 1 , ... , V ex q\u22121 } . In our integrated KRR - RL context , V en is goal - oriented and includes the variables whose values the robot wants to actively change so as to achieve the goal ; and V ex corresponds to the variables whose values affect the robot 's action outcomes , but the robot can not ( or does not want to ) change their values . Therefore , V en and V ex both depend on task \u03c4 . Continuing the navigation example , robot position is an endogenous variable , and current time is an exogenous variable . For each task , V = V en \u222a V ex and n = p + q , and RL agents learn in spaces specified by V en . The KRR component models V , their dependencies from human knowledge , and conditional probabilities on how actions change their values , as learned through model - based RL . When a task arrives , the KRR component uses probabilistic rules to generate a task - oriented Markov decision process ( MDP ) ( Puterman , 1994 ) , which only contains a subset of V that are relevant to the current task , Procedure 1 Learning in KRR - RL Framework Require : Logical rules \u03a0 L ; probabilistic rules \u03a0 P ; random variables V = { V 0 , V 1 , ... , V n\u22121 } ; task selector \u2206 ; and guidance functions ( from human knowledge ) of f V ( V , \u03c4 ) and f A ( \u03c4 ) 1 : while Robot has no task do 2 : \u03c4 \u2206 ( ) : a task is heuristically selected 3 : V en f V ( V , \u03c4 ) , and V ex V \\ V en 4 : A f A ( \u03c4 ) 5 : M Procedure - 2 ( \u03a0 L , \u03a0 P , V en , V ex , A ) 6 : Initialize agent : agent R - Max ( M ) 7 : RL agent repeatedly works on task \u03c4 , and keeps maintaining task model M , until policy convergence 8 : end while 9 : Use M to update \u03a0 P i.e. , V en , and their transition probabilities . Given this task - oriented MDP , a corresponding action policy is computed using value iteration or policy iteration . Procedures 1 and 2 focus on how our KRR - RL agent learns by interacting with an environment when there is no task assigned . 1 Next , we present the details of these two interleaved processes . Procedure 1 includes the steps of the learning process . When the robot is free , it interacts with the environment by heuristically selecting a task 2 , and repeatedly using a model - based RL approach , R - Max ( Brafman and Tennenholtz , 2002 ) in our case , to complete the task . The two guidance functions come from human knowledge . For instance , given a navigation task , it comes from human knowledge that the robot should model its own position ( specified by f V ) and actions that help the robot move between positions ( specified by f A ) . After the policy converges or this learning process is interrupted ( e.g. , by task arrivals ) , the robot uses the learned probabilities to update the corresponding world dynamics in KRR . For instance , the robot may have learned the probability and cost of navigating through a particular area in early morning . In case this learning process is interrupted , the sofar - \" known \" probabilities are used for knowledge base update . Procedure 2 includes the steps for building the probabilistic transition system of MDPs . The key point is that we consider only endogenous variables in the task - specific state space . However , when 1 As soon as the robot 's learning process is interrupted by the arrival of a real service task ( identified via dialog ) , it will call Procedure 2 to generate a controller to complete the task . This process is not included in the procedures . 2 Here curriculum learning in RL ( Narvekar et al , 2017 ) can play a role to task selection and we leave this aspect of the problem for future work . Figure 2 : Transition system specified for delivery tasks , where question - asking actions are used for estimating the service request in dialog . Once the robot becomes confident about the service request , it starts to work on the navigation subtask . After the robot arrives , the robot might have to come back to the dialog subtask and redeliver , depending on whether the service request was correctly identified . reasoning to compute the transition probabilities ( Line 5 ) , the KRR component uses both \u03a0 P and V ex . The computed probabilistic transition systems are used for building task - oriented controllers , i.e. , \u03c0 , for task completions . In this way , the dynamically constructed controllers do not directly include exogenous variables , but their parameters already account for the values of all variables . Next , we demonstrate how our KRR - RL framework is instantiated on a real robot .", "entities": [[10, 11, "DatasetName", "0"], [50, 51, "DatasetName", "0"], [69, 70, "DatasetName", "0"], [309, 310, "DatasetName", "0"], [417, 418, "DatasetName", "agent"], [419, 420, "DatasetName", "agent"], [429, 430, "DatasetName", "agent"], [499, 500, "DatasetName", "agent"]]}
{"text": "We consider a mobile service robot domain where a robot can do navigation , dialog , and delivery tasks . A navigation task requires the robot to use a sequence of ( unreliable ) navigation actions to move from one point to another . In a dialog task , the robot uses spoken dialog actions to specify service requests from people under imperfect language understanding . There is the trend of integrating language and navigation in the NLP and CV communities ( Chen et al , 2019 ; Shridhar et al , 2020 ) . In this paper , they are integrated into delivery tasks that require the robot to use dialog to figure out the delivery request and conduct navigation tasks to physically fulfill the request . Specifically , a delivery task requires the robot to deliver item I to room R for person P , resulting in services in the form of < I , R , P > . The challenges come from unreliable human language understanding ( e.g. , speech recognition ) and unforeseen obstacles that probabilistically block the robot in navigation .", "entities": [[173, 175, "TaskName", "speech recognition"]]}
{"text": "The robot needs spoken dialog to identify the request under unreliable language understanding , and navigation controllers for physically making the delivery . The service request is not directly observable to the robot , and has to be estimated by asking questions , such as \" What item do you want ? \" and \" Is Procedure 2 Model Construction for Task Completion Require : \u03a0 L ; \u03a0 P ; V en ; V ex ; Action set A 1 : for V i V en , i in [ 0 , , | V en | \u22121 ] do 2 : for each possible value v in range ( V i ) do 3 : for each a A do 4 : for each possible value v in range ( V i ) do 5 : M ( v | a , v ) Reason with \u03a0 L and \u03a0 P w.r.t V ex 6 : end for 7 : end for 8 : end for 9 : end for 10 : return M this delivery for Alice ? \" Once the robot is confident about the request , it takes a delivery action ( i.e. , serve ( I , R , P ) ) . We follow a standard way to use partially observable MDPs ( POMDPs ) ( Kaelbling et al , 1998 ) to build our dialog manager , as reviewed in ( Young et al , 2013 ) . The state set S is specified using curr s. The action set A is specified using serve and question - asking actions . Question - asking actions do not change the current state , and delivery actions lead to one of the terminal states ( success or failure ) . 3 After the robot becomes confident about the request via dialog , it will take a delivery action serve { I , R , P } . This delivery action is then implemented with a sequence of act move actions . When the request identification is incorrect , the robot needs to come back to the shop , figure out the correct request , and redeliver , where we assume the robot will correctly identify the request in the second dialog . We use an MDP to model this robot navigation task , where the states and actions are specified using sorts cell and move . We use pr - atoms to represent the success rates of the unreliable movements , which are learned through model - based RL . The dialog system builds on our previous work ( Lu et al , 2017 ) . Figure 2 shows the probabilistic transitions in delivery tasks .", "entities": [[91, 92, "DatasetName", "0"], [385, 387, "TaskName", "robot navigation"]]}
{"text": "We use R - Max ( Brafman and Tennenholtz , 2002 ) , a model - based RL algorithm , to help our robot learn the success rate of navigation actions in different positions . The agent first initializes an MDP , from which it uses R - Max to learn the partial world model ( of navigation tasks ) . Specifically , it initializes the transition function with T N ( s , a , s v ) = 1.0 , where s S and a A , meaning that starting from any state , after any action , the next state is always s v . The reward function is initialized with R ( s , a ) = R max , where R max is an upper bound of reward . The initialization of T N and R enables the learner to automatically balance exploration and exploitation . There is a fixed small cost for each navigation action . The robot receives a big bonus if it successfully achieves the goal ( R max ) , whereas it receives a big penalty otherwise ( \u2212R max ) . A transition probability in navigation , T N ( s , a , s ) , is not computed until there are a minimum number ( M ) of transition samples visiting s . We recompute the action policy after E action steps .", "entities": [[36, 37, "DatasetName", "agent"]]}
{"text": "The update of knowledge base is achieved through updating the success rate of delivery actions serve ( I , R , P ) ( in dialog task ) using the success rate of navigation actions act move = M in different positions . T D ( s r , a d , s t ) = P N ( s sp , s gl ) , if s r a d P N ( s sp , s mi ) \u00d7P N ( s mi , s sp ) \u00d7P N ( s sp , s gl ) , if s r \u2297 a d where T D ( s r , a d , s t ) is the probability of fulfilling request s r using delivery action a d ; s t is the \" success \" terminal state ; s sp , s mi and s gl are states of the robot being in the shop , a misidentified goal position , and real goal position respectively ; and P N ( s , s ) is the probability of the robot successfully navigating from s to s positions . When s r and a d are aligned in all three dimensions ( i.e. , s r a d ) , the robot needs to navigate once from the shop ( s sp ) to the requested navigation goal ( s gl ) . P N ( s sp , s gl ) is the probability of the corresponding navigation task . When the request and delivery action are not aligned in at least one dimension ( i.e. , s r \u2297 a d ) , the robot has to navigate back to the shop to figure out the correct request , and then redeliver , resulting in three navigation tasks . Intuitively , the penalty of failures in a dialog subtask depends on the difficulty of the wrongly identified navigation subtask . For instance , a robot supposed to deliver to a near ( distant ) location being wrongly directed to a distant ( near ) location , due to a failure in the dialog subtask , will produce a higher ( lower ) penalty to the dialog agent .", "entities": [[372, 373, "DatasetName", "agent"]]}
{"text": "In this section , the goal is to evaluate our hypothesis that our KRR - RL framework enables a robot to learn from model - based RL , reason with both the learned knowledge and human knowledge , and dynamically construct task - oriented controllers . Specifically , our robot learns from navigation tasks , and applied the learned knowledge ( through KRR ) to navigation , dialog , and delivery tasks . We also evaluated whether the learned knowledge can be represented and applied to tasks under different world settings . In addition to simulation experiments , we have used a real robot to demonstrate how our robot learns from navigation to perform better in dialog . Figure 3 shows the map of the working environment ( generated using a real robot ) used in both simulation and real - robot experiments . Human walkers in the blocking areas ( \" BA \" ) can probabilistically impede the robot , resulting in different success rates in navigation tasks . We have implemented our KRR - RL framework on a mobile robot in an office environment . As shown in Figure 3 , the robot is equipped with two Lidar sensors for localization and obstacle avoidance in navigation , and a Kinect RGB - D camera for human - robot interaction . We use the Speech Application Programming Interface ( SAPI ) package ( http://www.iflytek.com/en ) for speech recognition . The robot software runs in the Robot Operating System ( ROS ) ( Quigley et al , 2009 ) . ( e ) Robot was on the way to the kitchen to pick up the object ; and ( f ) Robot arrived at the kitchen , and was going to pick up the object for delivery . An Illustrative Trial on a Robot : Figure 4 shows the screenshots of milestones of a demo video , which will be made available given its acceptance . After hearing \" a coke for Bob to office2 \" , the three sub - beliefs are updated ( turn1 ) . Since the robot is aware of its unreliable speech recognition , it asked about the item , \" Which item is it ? \" After hearing \" a coke \" , the belief is updated ( turn2 ) , and the robot further confirmed on the item by asking \" Should I deliver a coke ? \" It received a positive response ( turn3 ) , and decided to move on to ask about the delivery room : \" Should I deliver to office 2 ? \" After this question , the robot did not further confirm the delivery room , because it learned through model - based RL that navigating to office2 is relatively easy and it decided that it is more worth risking an error and having to replan than it is to ask the person another question . The robot became confident in three dimensions of the service request ( < coke , Bob , office2 > in turn4 ) without asking about person , because of the prior knowledge ( encoded in P - log ) about Bob 's office . Figure 5 shows the belief changes ( in the di - mensions of item , person , and room ) as the robot interacts with a human user . The robot started with a uniform distribution in all three categories . It should be noted that , although the marginal distributions are uniform , the joint belief distribution is not , as the robot has prior knowledge such as Bob 's office is office2 and people prefer deliveries to their own offices . Demo video is not included to respect the anonymous review process .", "entities": [[152, 153, "DatasetName", "BA"], [237, 239, "TaskName", "speech recognition"], [356, 358, "TaskName", "speech recognition"]]}
{"text": "In this experiment , the robot learns in the shop - room1 navigation task , and extracts the learned partial world model to the shop - room2 task . It should be noted that navigation from shop to room2 requires traveling in areas that are unnecessary in the shop - room1 task . Figure 6 presents the results , where each data points corresponds to an average of 1000 trials . Each episode allows at most 200 ( 300 ) steps in small ( large ) domain . The curves are smoothed using a window of 10 episodes . The results suggest that with knowledge extraction ( the dashed line ) the robot learns faster than without extraction , and this performance improvement is more significant in a larger domain ( the Right subfigure ) . Learning to Dialog and Navigate from Navigation Tasks Robot delivering objects requires both tasks : dialog management for specifying service request ( under unreliable speech recognition ) and navigation for physically delivering objects ( under unforeseen obstacles ) . Our office domain includes five rooms , two persons , and three items , resulting in 30 possible service requests . In the dialog manager , the reward function gives delivery actions a big bonus ( 80 ) if a request is fulfilled , and a big penalty ( - 80 ) otherwise . General questions and confirming questions cost 2.0 and 1.5 respectively . In case a dialog does not end after 20 turns , the robot is forced to work on the most likely delivery . The cost / bonus / penalty values are heuristically set in this work , following guidelines based on studies from the literature on dialog agent behaviors ( Zhang and Stone , 2015 ) . Table 1 reports the robot 's overall performance in delivery tasks , which requires accurate dialog for identifying delivery tasks and safe navigation for object delivery . We conduct 10 , 000 simulation trials under each blocking rate . Without learning from RL , the robot uses a world model ( outdated ) that was learned under br = 0.3 . With learning , the robot updates its world model in domains with different blocking rates . We can see , when learning is enabled , our KRR - RL framework produces higher overall reward , higher request fulfillment rate , and lower question - asking cost . The improvement is statistically significant , i.e. , the p\u2212values are 0.028 , 0.035 , and 0.049 for overall reward , when br is 0.1 , 0.5 , and 0.7 respectively ( 100 randomly selected trials with / without extraction ) . Learning to Adjust Dialog Strategies from Navigation In the last experiment , we quantify the information collected in dialog in terms of entropy reduction . The hypothesis is that , using our KRR - RL framework , the dialog manager wants to collect more information before physically working on more challenging tasks . In each trial , we randomly generate a belief distribution over all possible service requests , evaluate the entropy of this belief , and record the suggested action given this belief . We then statistically analyze the entropy values of beliefs , under which delivery actions are suggested . Table 2 shows that , when br grows from 0.1 to 0.7 , the means of belief entropy decreases ( i.e. , belief is more converged ) . This suggests that the robot collected more information in dialog in environments that are more challenging for navigation , which is consistent with Table 1 in the main paper . Comparing the three columns of results , we find the robot collects the most information before it delivers to room5 . This is because such delivery tasks are the most difficult due to the location of room5 . The results support our hypothesis that learning from navigation tasks enables the robot to adjust its information gathering strategy in dialog given tasks of different difficulties .", "entities": [[140, 141, "TaskName", "Navigate"], [160, 162, "TaskName", "speech recognition"], [229, 230, "DatasetName", "General"], [287, 288, "DatasetName", "agent"]]}
{"text": "We develop a KRR - RL framework that integrates computational paradigms of logical - probabilistic knowledge representation and reasoning ( KRR ) , and model - based reinforcement learning ( RL ) . Our KRR - RL agent learns world dynamics via modelbased RL , and then incorporates the learned dynamics into the logical - probabilistic reasoning module , which is used for dynamic construction of efficient run - time task - specific planning models . Experiments were conducted using a mobile robot ( simulated and physical ) working on delivery tasks that involve both navigation and dialog . Results suggested that the learned knowledge from RL can be represented and used for reasoning by the KRR component , enabling the robot to dynamically generate task - oriented action policies . The integration of a KRR paradigm and modelbased RL paves the way for at least the following research directions . We plan to study how to sequence source tasks to help the robot perform the best in the target task ( i.e. , a curriculum learning problem within the RL context ( Narvekar et al , 2017 ) ) . Balancing the efficiencies between service task completion and RL is another topic for further study - currently the robot optimizes for task completions ( without considering the potential knowledge learned in this process ) once a task becomes available . Fundamentally , all domain variables are endogenous , because one can hardly find variables whose values are completely independent from robot actions . However , for practical reasons ( such as limited computational resources ) , people have to limit the number of endogenous . It remains an open question of how to decide what variables should be considered as being endogenous .", "entities": [[37, 38, "DatasetName", "agent"]]}
{"text": "Partner Personas Generation for Dialogue Response Generation", "entities": [[5, 7, "TaskName", "Response Generation"]]}
{"text": "Incorporating personas information allows diverse and engaging responses in dialogue response generation . Unfortunately , prior works have primarily focused on self personas and have overlooked the value of partner personas . Moreover , in practical applications , the availability of the gold partner personas is often not the case . This paper attempts to tackle these issues by offering a novel framework that leverages automatic partner personas generation to enhance the succeeding dialogue response generation . Our framework employs reinforcement learning with a dedicatedly designed critic network for reward judgement . Experimental results from automatic and human evaluations indicate that our framework is capable of generating relevant , interesting , coherent and informative partner personas , even compared to the ground truth partner personas . This enhances the succeeding dialogue response generation , which surpasses our competitive baselines that condition on the ground truth partner personas .", "entities": [[10, 12, "TaskName", "response generation"], [74, 76, "TaskName", "response generation"], [131, 133, "TaskName", "response generation"]]}
{"text": "Building informative and engaging dialogue agents Roller et al , 2021 ) is a popular research direction within the area of natural language processing . For the sake of engagement , diverse and consistent responses ( Song et al , 2020 ( Song et al , , 2021 are important factors , and personas information gives rise to both . There are two types of personas , namely self persona and partner persona . The former refers to a self profile consisting of several sentences representing the dialogue agents . Such a persona allows producing consistent responses rather than solely relying on the personas that are randomly learned and embedded in the model parameters ( Kim et al , 2020 ) . The latter refers to a profile that represents the users . Leveraging such partner personas has been empirically shown to be helpful for dialogue response selection ( Gu et al , 2021 ) . Unfortunately , the existence of partner personas suffers from the cold start ( Schein et al , 2002 ; Zhang et al , 2014 ; at the beginning of the conversation . Most of the works , if not all , ( Li et al , 2016b ; Mazar\u00e9 et al , 2018 ; Gu et al , 2019 ; Zhao et al , 2019 ; Madotto et al , 2019 ; Majumder et al , 2020 ; Wu et al , 2020a ; Song et al , 2020 ) have been either overlooking partner personas or simply focusing on the impractical situation where partner personas guarantee to exist . In contrast , our work does not suffer from the practical issue when partner personas are missing during inference , and our proposed framework surpasses the baseline that conditions on the ground truth partner personas . To our knowledge , this is the first attempt to formulate partner personas generation for improved performance on the downstream dialogue response generation . Our work is motivated by the underlying hypothesis that partner personas generation is plausible given the self personas and dialogue context . Automatic and human evaluation results support the hypothesis and indicate that generated personas are even more interesting than the ground truth , which improves the downstream dialogue response generation . This paper thus paves the way to exploit partner personas generation ( PPG ) for dialogue response generation ( DRG ) . We propose a novel framework composed of three major components , namely a personas generator , a dialogue response generator and a critic network . The personas generator generates partner personas , which the dialogue response generator conditions on . We employ reinforcement learning with a critic network that propagates the reward back to the generators for joint training . Prior works have investigated partner persona retrieval . The human - constructed ground truth personas serve as the upper bound for such retrieval - based systems , and we argue that the ground truth is not coherent and diverse enough . Interestingly , we observe that the generative counterpart proposed in our framework generates relevant , informative and coherent partner personas , which further improves the succeeding dialogue response generation . It follows another advantage that our framework does not need an external database to retrieve from ( Madotto et al , 2020 ; . One close work to ours is a multi - task framework for meta - learning ( Lee et al , 2021 ) that uses personas reconstruction as an auxiliary task to improve response consistency . The differences are that theirs does not differentiate between self personas and partner personas , while ours does . Theirs indicates an improvement over personality consistency , while ours report improvements for the overall quality . We conduct an empirical comparison with their model by reconstructing the partner personas . Experimental results indicate that such a multi - task model does not work well in our problem setting . Very recently , formulates personas generation as a Seq2Seq task for improved downstream response generation via multi - task learning . In contrast , our work leverages reinforcement learning to jointly train the partner personas generator and the response generator . Automatic and human evaluation results indicate that our framework can generate partner personas that are more diverse and interesting than the ground truth partner personas and generate more diverse and engaging responses than the baseline conditioned on ground truth partner personas . 1 2 Related Work", "entities": [[323, 325, "TaskName", "response generation"], [375, 377, "TaskName", "response generation"], [394, 396, "TaskName", "response generation"], [528, 530, "TaskName", "response generation"], [567, 570, "TaskName", "meta - learning"], [667, 668, "MethodName", "Seq2Seq"], [672, 674, "TaskName", "response generation"], [675, 679, "TaskName", "multi - task learning"]]}
{"text": "Conditioning on personas helps to produce informative and engaging responses . The most wellknown multi - turn dialogue dataset conditioned on personal profiles is PERSONACHAT , in which two crowdsourcers converse and find more about each other . The community has proposed many methods to better utilize self personas . Mazar\u00e9 et al ( 2018 ) employs a pre - training stage based on dedicatedly extracted large - scale persona - based dialogues . Zhao et al ( 2019 ) fuses information in personas and dialogue context into individual contextualized representations by attending to different parts of both . Gu et al ( 2019 ) exploits the interaction between personas , dialogue context and response to improve retrieval - based dialogue agents . Madotto et al ( 2019 ) leverages meta - learning with several dialogues of the current speakers to enhance response personality . Welleck et al ( 2019 ) releases a dataset for measuring dialogue consistency . Song et al ( 2020 ) employs a multi - stage pipeline to improve response personality by response rewriting . Lee et al ( 2021 ) uses multi - task learning for improved personality consistency in the meta - learning scenario . Gu et al ( 2021 ) employs four different strategies for personas fusing to leverage both self persona . However , most of these works focus on exploiting self personas rather than partner personas , and they assume the existance of the gold partner personas . Li et al ( 2014 ) leverages distant supervision to classify the spouse , education and job information from user twitters . Wu et al ( 2020b ) proposes a twostaged profile extractor that extracts attributes before extracting the underlying relationship . proposes to categorize the profile extraction task into two different difficulties , namely ' extraction ' and ' inference ' , and they leverage a GPT - based generator to extract user profiles . These works have formulated user profile extraction as a classification task that conditions on an input sentence , and they aim at better profile extraction . In contrast , we propose to formulate personas generation to be conditioned dialogue input to be jointly trained with response generation . While ground truth personas serve as the upper bound for these user profile extractors , we empirically demonstrate that our reinforcement learning algorithm surpasses the response model conditioned on the ground truth partner personas . As supported by our human evaluation , we believe the underlying reason is that our model can leverage pre - trained generators to generate coherent and relevant partner personas .", "entities": [[30, 31, "DatasetName", "converse"], [130, 133, "TaskName", "meta - learning"], [186, 190, "TaskName", "multi - task learning"], [196, 199, "TaskName", "meta - learning"], [314, 315, "MethodName", "GPT"], [368, 370, "TaskName", "response generation"]]}
{"text": "We propose a novel framework composed of three major components , namely a partner personas generator , a dialogue response generator and a reinforcement learning component with a critic network . Figure 1 depicts the inference flow of our setting . The input dialogue context with self persona is first fed into the partner personas generator . The generated partner personas output is then concatenated with the dialogue context and the self personas as the input into the dialogue response generator . In the beginning , we train our partner personas generator and dialogue response generator under supervised learning . In the training stage , we use the ground truth partner personas to train the dialogue response generator , and we replace it with generated partner personas in the inference stage . After the supervised learning stage , the second stage is a reinforcement learning stage which jointly optimizes both partner personas generator and dialogue response generator as depicted in Figure 2 to train the partner personas generator under the reward signal that is relevant to dialogue response generation as well as fine - tuning dialogue response generator trained on the generated partner personas . 2 Particularly , we employ a dedicatedly designed critic network that receives generated partner personas and generated dialogue responses as the input and output a reward that measures the relevance between the generated personas and responses and propagates back to the generators .", "entities": [[177, 179, "TaskName", "response generation"]]}
{"text": "A Seq2Seq neural network ( Sutskever et al , 2014 ) is adopted as our partner personas generator for the task of partner personas generation ( PPG ) . The concatenation of dialogue context c and self personas s is fed as an input into the partner personas generator . The personas generator then outputs an approximated partner personasp conditioned on the input that maximises the following likelihood : P ( p | s , c ) = T t=1 P ( p t | p 1 , ... , p t\u22121 , s , c ) , where T represents the length of the generated partner personas andp t represents the word at the position t that has been inferenced . For training , the ground truth partner personas p is used and we train our generator to maximise the likelihood P ( p | s , c ) . We generate the complete partner personas profiles in an one - off shot for all the dialogue samples .", "entities": [[1, 2, "MethodName", "Seq2Seq"]]}
{"text": "We also adopt a Seq2Seq neural network for the task of dialogue response generation ( DRG ) . During inference , the concatenation of dialogue context c , self personas s , and generated partner personasp is fed as an input into the dialogue response generator . The response generator then outputs a dialogue responser conditioned on the input , which maximises the conditional likelihood : P ( r | s , p , c ) . For training , the ground truth partner personas p and the ground truth dialogue responses r are used .", "entities": [[4, 5, "MethodName", "Seq2Seq"], [12, 14, "TaskName", "response generation"]]}
{"text": "We build our baselines , the partner personas generator and the dialogue response generator based on a state - of - the - art pre - trained dialogue model DIALOGPT for parameters Table 2 : Case studies that compare our framework against the baseline with the complete partner personas as well as the human response . We present the preceding partner utterance as dialogue context , and we give the most salient ground truth partner personas ( Gold Partner ) and generated partner personas ( Generated Partner ) for clarity . initialization . More implementation details can be found in Appendx B. The dialogue response generation results are presented in Table 1 . Our framework with reinforcement learning attains the best over all the metrics on both PERSONACHAT - ORI and PERSONACHAT - REV . This supports the usefulness of our framework , which generates reasonable personas and effectively enhances the succeeding dialogue response generation , through the use of RL . Although TRANSFERTRANSFO attains a better score on the PPL than the fine - tuned GPT - 2 , GPT - 2 have better extrinsic scores than TRANSFER - TRANSFO . GPT - 2 also has better overall scores than the E2E baselines without the complete partner personas . However , it is surpassed by the E2E baseline with the complete partner personas during training and inference . The E2E baseline with the complete ground truth partner personas attains better scores on all of the metrics than our remaining baselines . Our framework with RL succeeds the performance of such a competitive baseline for both PERSONACHAT - ORI and PERSONACHAT - REV , indicating our proposed framework 's robustness against paraphrasal . The multi - task learning comparison model ( Lee et al , 2021 ) produces less promising results . Concretely , we postulate that the nature of PPG and DRG largely differs . The textual format of partner personas always initiates with first - person sentence starters , while dialogue responses are more general , ranging from greetings to goodbyes . Therefore , it could be hard to capture both in a single model .", "entities": [[104, 106, "TaskName", "response generation"], [153, 155, "TaskName", "response generation"], [176, 177, "MethodName", "GPT"], [180, 181, "MethodName", "GPT"], [192, 193, "MethodName", "GPT"], [202, 203, "DatasetName", "E2E"], [217, 218, "DatasetName", "E2E"], [230, 231, "DatasetName", "E2E"], [284, 288, "TaskName", "multi - task learning"]]}
{"text": "Table 4 presents the quality measurements of the generated partner personas from our PPG with no RL . We observe that our models have much higher Distinct - N scores as the number of unique words in the generated output is much higher than the ground truth test personas . Compared to the ground truth personas that are limited sets of traits , our generator can leverage the power of pre - trained models for better diversity . The remaining metrics also report reasonable scores , suggesting the plausbility to formulate personas generation as a Seq2Seq task .", "entities": [[95, 96, "MethodName", "Seq2Seq"]]}
{"text": "We hired experienced annotators who have degrees relevant to English Linguistics to conduct evaluation on PERSONACHAT - ORI . For both DRG and PPG , we present a questionnaire composed of 800 questions with randomly sampled 200 test instances to three annotators who compare model outputs under A / B testing . As in Zou et al ( 2021 ) and ACUTE - Evals ( Li et al , 2020 ) , annotators follow the criteria which we present in Appendix D. trained under RL surpasses the E2E model that leverages both training and inference ground truth partner personas from all the aspects . Table 6 presents the human evaluation results on PPG . We observe that our PPG generates personas that are more coherent and interesting than the ground truth , which align with the facts observed in Section 5.4 and Section 5.5 indicating that our generated partner personas are more coherent and diverse .", "entities": [[87, 88, "DatasetName", "E2E"]]}
{"text": "We conduct an ablation study on PERSONACHAT - ORI as reported in Table 7 to present the performance of our framework when one of the components is frozen during RL . The result indicates that our proposed framework yields the best result when both of the components are actively trained under RL . We also notice that scaling the RL reward for either PPG or DRG by 10 leads to minor decrease in the performance . Further scaling deteriorates the quality of response generation .", "entities": [[82, 84, "TaskName", "response generation"]]}
{"text": "Our novel framework incorporates partner personas generation into dialogue response generation . It effectively mitigates the problem that partner personas are not available in practical applications as well as the cold start problem during early conversation . The experimental results with both automatic and human evaluation demonstrate that our framework generates coherent , diverse , interesting and engaging partner personas , even compared to the ground truth partner personas . We employ reinforcement learning with a dedicatedly designed critic network that boosts the response generation by conditioning on the generated personas . Automatic and human evaluation results indicate that our response generator surpasses our competitive baselines that condition on the ground truth partner personas . Extensive case studies demonstrate that our framework can generate satisfying dialogue responses and partner personas .", "entities": [[9, 11, "TaskName", "response generation"], [83, 85, "TaskName", "response generation"]]}
{"text": "Our work uses an off - the - shelf persona - based conversational dataset PERSONACHAT , which is collected and built by crowdsourcing to converse based on a fake set of discrete traits . There is no personal information and hence no ethics concern , but this might result in limited usefulness as there could be discrepancies between the collected samples and real - life conversation . It is also more expensive to collect real data . However , PERSONACHAT has been widely used by the community as a standard dataset . Many well - known persona - based datasets suffer from the same problem ( Urbanek et al , 2019 ) as widely known . Although Mazar\u00e9 et al ( 2018 ) proposed a useful method to collect large - scale persona - based dialogue datasets by extracting persona from user comments with classifiers trained on revised personas from PERSONACHAT which can improve the model performance on PERSONACHAT . For legal reasons , they did not release this dataset at the time of writing . Similarly , Zheng et al ( 2019 ) proposed a persona - based dialogue dataset with diversified traits , but it is not currently online readily available . Zhong et al ( 2020 ) has followed the approach suggested by Mazar\u00e9 et al ( 2018 ) to build an empathetic conversation dataset based on personas . 8 : More generated personas . We highlight in pink for informativeness and in yellow for coherence . However , their main focus is to investigate the impact of personas on empathetic dialogue generation . Therefore , we choose to follow the community to investigate our method on the most well - known dataset , PERSONACHAT .", "entities": [[24, 25, "DatasetName", "converse"], [264, 266, "TaskName", "dialogue generation"]]}
{"text": "Active Learning via Membership Query Synthesis for Semi - supervised Sentence Classification", "entities": [[0, 2, "TaskName", "Active Learning"], [10, 12, "TaskName", "Sentence Classification"]]}
{"text": "Active learning ( AL ) is a technique for reducing manual annotation effort during the annotation of training data for machine learning classifiers . For NLP tasks , pool - based and stream - based sampling techniques have been used to select new instances for AL while generating new , artificial instances via Membership Query Synthesis was , up to know , considered to be infeasible for NLP problems . We present the first successful attempt to use Membership Query Synthesis for generating AL queries for natural language processing , using Variational Autoencoders for query generation . We evaluate our approach in a text classification task and demonstrate that query synthesis shows competitive performance to pool - based AL strategies while substantially reducing annotation time .", "entities": [[0, 2, "TaskName", "Active learning"], [92, 93, "MethodName", "Autoencoders"], [103, 105, "TaskName", "text classification"]]}
{"text": "Active learning ( AL ) has the potential to substantially reduce the amount of labeled instances needed to reach a certain classifier performance in supervised machine learning . It works by selecting new instances that are highly informative for the classifier , so that comparable classification accuracies can be obtained on a much smaller training set . AL strategies can be categorized into pool - based sampling , stream - based sampling and Membership Query Synthesis ( MQS ) . The first two strategies sample new instances either from a data pool or from a stream of data . The third , MQS , generates artificial AL instances from the region of uncertainty of the classifier . While it is known that MQS can reduce the predictive error rate more quickly than pool - based sampling ( Ling and Du , 2008 ) , so far it has not been used for NLP tasks because artificially created textual instances are uninterpretable for human annotators . We provide proof of concept that generating highly informative artificial training instances for text classification is feasible . We use Variational Autoencoders ( VAE ) ( Kingma and Welling , 2013 ) to learn representations from unlabeled text in an unsupervised fashion by encoding individual sentences as low - dimensional vectors in latent space . In addition to mapping input sequences into latent space , the VAE can also learn to generate new instances from this space . We utilize these abilities to generate new examples for active learning from a region in latent space where the classifier is most uncertain , and hand them over to the annotator who then provides labels for the newly created instances . We test our approach in a text classification setup with a real human annotator in the loop . Our experiments show that query synthesis for NLP is not only feasible but can outperform other AL strategies in a sentiment classification task with respect to annotation time . The paper is structured as follows . We first review related work ( 2 ) and introduce a formal description of the problem ( 3 ) . Then we describe our approach ( 4 ) , present the experiments ( 5 ) and analyze the results ( 6 ) . We discuss limitations and possible further experiments ( 7 ) and finally conclude our findings ( 8 ) .", "entities": [[0, 2, "TaskName", "Active learning"], [178, 180, "TaskName", "text classification"], [186, 187, "MethodName", "Autoencoders"], [188, 189, "MethodName", "VAE"], [231, 232, "MethodName", "VAE"], [252, 254, "TaskName", "active learning"], [290, 292, "TaskName", "text classification"]]}
{"text": "Membership query synthesis was introduced by Angluin ( 1988 ) and describes a setting where the model generates new queries instead of selecting existing ones . Early experiments in image processing ( Lang and Baum , 1992 ) , however , showed that the generated queries are hard to interpret by human annotators . This holds true even for recent approaches using Generative Adversarial Networks ( GANs ) ( Goodfellow et al , 2014 ) to create uncertain instances ( Zhu and Bento , 2017 ; Huijser and van Gemert , 2017 ) . In contrast to i m - age processing , discrete domains like natural language do not exhibit a direct mapping from feature to instance space . Strategies that circumvent this problem include the search for nearest ( observed ) neighbors in feature space ( Wang et al , 2015 ) or crafting queries by switching words ( Awasthi and Kanade , 2012 ) . Sentence representation learning ( Kiros et al , 2015 ; Conneau et al , 2017 ; Subramanian et al , 2018 ; in combination with new methods for semi - supervised learning Hu et al , 2017 ; Xu et al , 2017 ; Odena , 2016 ; Radford et al , 2017 ) have shown to improve classification tasks by leveraging unlabeled text . Methods based on deep generative models like GANs or VAEs are able to generate sentences from any point in representation space . Mehrjou et al ( 2018 ) use VAEs to learn structural information from unlabeled data and use it as an additional criterion in conventional active learning to make it more robust against outliers and noise . We use VAEs to generate AL queries from specific regions in latent space . To ensure that the generated instances are not only informative for the ML classifier but also meaningful for the human annotator , we adapt the approach of Wang et al ( 2015 ) ( see 3.1 ) . In contrast to their work , however , we do not sample existing instances from the pool that are similar to the synthetic ones but directly generate the new queries . To our best knowledge , our work is the first to present positive results for Membership Query Synthesis for text classification .", "entities": [[159, 161, "TaskName", "representation learning"], [269, 271, "TaskName", "active learning"], [383, 385, "TaskName", "text classification"]]}
{"text": "Arbitrary points in feature space are hard to interpret for humans . To evade this problem , Wang et al ( 2015 ) use the nearest neighbor in a pool of unlabeled data as a representative which is then presented to the human annotator . To identify uncertain points along the separating hyperplane of an SVM the following approach is proposed . First the location of the decision boundary is approximated by a binary - search like procedure . An initial Opposite Pair ( z + , z \u2212 ) is formed by centroid c + and centroid c \u2212 of positive and negative labeled instances respectively . The mid point z s is queried and , depending on the annotated label l , replaces the corresponding z l . This step is repeated b times , reducing the distance between the initial centroids by a factor of 2 b . Figure 1a depicts this process . Then the mid - perpendicular vector of the Opposite Pair is calculated by using the Gram - Schmidt process to orthogonalize a random vector z r and normalize its magnitude to \u03bb . The new point z s = z r + ( z + + z \u2212 ) /2 is close to the decision boundary and queried for its class . Depending on the receive label the point z s replaces z + or z \u2212 in the Opposite Pair . This process ( Figure 1b ) is repeated until n \u2212 b points along the separating hyperplane are queried .", "entities": [[55, 56, "MethodName", "SVM"]]}
{"text": "We train a Variational Autoencoder on an unlabeled corpus of sentences . The text classification task is performed on a binary sentiment dataset split into training , development and test set . As depicted in Figure 2 , the sentences in the classification dataset are vectorized using the VAE encoder which generates the latent variable z for each sentence x. This is done deterministically by dropping the \u03c3 term in Equation 1 , further referred to as z = enc ( x ) . Next , a Learner is trained to fit a linear hyperplane to separate the positive from the negative instances . We use the procedure described in 3.1 to select new query points for AL . But instead of searching for the nearest neighbor in the pool , we decode the point x = dec ( z ) into a human readable sentence which is then handed over to the human annotator . The annotator assigns a binary label to the instance and the next query point is calculated . One important parameter for active learning determines how many new instances are to be selected in each AL iteration . Wang et al ( 2015 ) use a predefined number of instances to be selected along the hyperplane . Because we know that a Gaussian prior is imposed on the feature space , we instead stop the selection process when the magnitude of z s exceeds the expectation . The expected distance of a point sampled from the k - dimensional Gaussian prior to the origin is E [ \u03c7 2 k ] = \u221a k. Then the schedule restarts , learning a new decision boundary , and ultimately ter - minates when the annotation budget is exhausted . We refer to this method as gen wang . When nearest neighbor search is used instead , we refer to the selection method as nn wang . In addition , we explore a method , gen uniform , where step b ) in Figure 1 is reduced to generating only one midperpendicular vector with a magnitude drawn from a uniform distribution . In each iteration this vector will point to a random direction with a different magnitude , selecting diverse points close to the hyperplane . The maximum magnitude is set in a way that the resulting point is not further away than \u221a k from the origin . Similar to above we refer to this method as nn uniform when using nearest neighbor search . The number of possible directions along the hyperplane grows with the size of the latent variable . With this modification we expect to explore more diverse points than following the same direction for several steps .", "entities": [[3, 5, "MethodName", "Variational Autoencoder"], [13, 15, "TaskName", "text classification"], [48, 49, "MethodName", "VAE"], [177, 179, "TaskName", "active learning"]]}
{"text": "In this section we want to explore how the ability to generate human readable sentences from arbitrary points in the feature space affects active learning performance . We compare our approach to a number of baselines ( 5.3 ) , where in each experiment we select / generate 500 instances , present them to a human annotator to get a label and evaluate the performance of each setting in a sentiment classification task . We start the active learning process with two utterances in the seed set , namely ' good movie ' and ' bad movie ' . tator can skip neutral or uninterpretable instances . These skip actions also count towards the annotation budget .", "entities": [[23, 25, "TaskName", "active learning"], [77, 79, "TaskName", "active learning"]]}
{"text": "We compare our approach to Membership Query Synthesis for text classification to four baselines . The first baseline selects instances from the pool by random choice . The least confidence baseline computes the distance of the instances in the pool to the separating hyperplane and chooses the one closest to the hyperplane . The third and fourth baseline follow the procedure described in 4 but search for the nearest neighbor ( nn uniform , nn wang ) instead of synthesising the exact query point . Nearest neighbor is defined by the minimal euclidean distance between the query point and the latent representation of the pool instance .", "entities": [[9, 11, "TaskName", "text classification"]]}
{"text": "To explore the ability of the model to generate unseen instances we calculate the percentage of instances not seen in the pool . We only look at instances with an annotated sentiment label , because skipped examples often include noise and thus are unlikely to be present in the pool . 41 and 51 percent of labeled instances are newly generated by gen uniform and gen wang respectively . This provides more evidence that the model is capable of generating new and informative instances . No . Instance Label 1 . the acting is excellent 1 2 . powerful and moving 1 3 . this movie is very enjoyable 1 4 . a complete mess 0 5 . nothing spectacular 0 6 . absolutely terrible ! 0 7 . the plot is UNK skip 8 . well done by UNK 1 9 . the UNK is a disappointment 0 Label 1 for positive and 0 for negative class . Figure 6 : Plot of the 2 most important dimensions of selected / generated instances in latent space . Gray points indicate negative , black points positive labels . The blue square denotes ' bad movie ' and the red cross ' good movie ' .", "entities": [[115, 116, "DatasetName", "0"], [120, 121, "DatasetName", "0"], [126, 127, "DatasetName", "0"], [148, 149, "DatasetName", "0"], [154, 155, "DatasetName", "0"]]}
{"text": "To further analyze the behavior of the different AL strategies , we apply dimensionality reduction and visualize the instances in latent space ( Figure 6 ) . The two largest absolute coefficients of the trained SVM 's linear kernel identify the most important dimensions . Figure 6 plots the points , represented by theses two dimensions , selected by different active learning schedules . The generated instances lie densely around the seed points , while pool instances are more distributed . In gen wang one can see how the instances are loosely following one direction similar to Figure1 . As indicated in Figure 2 a pool instance is represented as z = enc ( x ) . The same is true for the instances in the development , test and seed set . For the generated instances there are two options . If z is a point selected in feature space and x = dec ( z ) is the decoded query sequence , the annotated instance can either be represented as z or as\u1e91 = enc ( x ) . In a perfect VAE z and\u1e91 should be nearly identical . In practice however\u1e91 ends up at a different location in feature space . Figure 7 depicts the distribution of distances between z and\u1e91 generated with the gen uniform method . We observe that models trained on\u1e91 perform better than those trained on z , presumably because the test instances are represented the same way . To evaluate if\u1e91 is still an informative point and not just positioned randomly in feature space , we train a model on actual randomly sampled points . The sampled point z \u223c N ( 0 , I ) is decoded to query sequence x , labeled and subsequently re - encoded to\u1e91 = enc ( x ) . With the same amount of instances , this model performs much worse than gen uniform , indicating that point\u1e91 still preserves some of the informativeness of z. We thus assume that the closer\u1e91 is to selected point z , the better the generation based active learning schedules will work .", "entities": [[13, 15, "TaskName", "dimensionality reduction"], [35, 36, "MethodName", "SVM"], [60, 62, "TaskName", "active learning"], [184, 185, "MethodName", "VAE"], [281, 282, "DatasetName", "0"], [349, 351, "TaskName", "active learning"]]}
{"text": "Related work in the context of semi - supervised learning has focused on developing methods to generate synthetic training instances for different tasks ( Sennrich et al , 2016 ; Hayashi et al , 2018 ; Alberti et al , 2019 ; Winata et al , 2019 ) , in order to accelerate the learning process . Sennrich et al ( 2016 ) create artificial training instances for machine translation , using monolingual data paired with automatic back - translations . Their work obtains substantial improvements for several languages and has triggered many follow - up studies that apply the idea of back - translation to different tasks . For example , Hayashi et al ( 2018 ) Alberti et al ( 2019 ) use a large number of synthetic instances to pre - train a Question Answering ( QA ) model that is then fine - tuned on the target QA dataset . Their approach results in significant improvements over models that are trained without the synthetic datapoints . While these studies show that huge amounts of synthetic training data can crucially improve the learning process , our approach uses a different paradigm . Instead of generating millions of synthetic data points , our method is data - lean and only needs a few hundred instances to improve the classifier . Another difference is that we do not rely on automatically generated labels but use human annotations instead . Due to the practical constraints of the active learning process , we need to keep the training time short enough so that the human annotator does not have to wait for the next set of instances to annotate . This rules out the use of computation - intensive models and large training sets . Given that we use an SVM for classification , we do not expect a strong effect for adding large numbers of additional training instances , given that the majority of those data points will not be positioned close to the decision boundary . One of the main drawbacks of our work is its limitation to binary sentence classification . However , multi - class classification in an one - vs - rest schema is compatible with our method and worth further exploration . Another interesting direction for future work is the synthesis of data for more complex tasks like Natural Language Inference ( NLI ) or QA . This , however , requires modifications to the structure of the autoencoder and exceeds the scope of this work . Membership Query Synthesis might also be an interesting approach for tasks where the automatic extraction of large amounts of unlabelled data is not straight - forward . One example that comes to mind is the detection of offensive language or ' hate speech ' , where we have to deal with highly unbalanced training sets with only a small number of positive instances , and attempts to increase this number have been shown to result in systematically biased datasets ( Davidson et al , 2019 ; Wiegand et al , 2019 ) . Table 2 suggests that the generator produces instances with a more balanced class ratio ( 1.7 and 1.2 ) than the pool data ( 2.6 ) it was trained on . It might be worthwhile to explore whether the generation of synthetic training instances can help to mitigate the problem to select instances from both classes in an highly imbalanced data pool .", "entities": [[68, 70, "TaskName", "machine translation"], [136, 138, "TaskName", "Question Answering"], [247, 249, "TaskName", "active learning"], [299, 300, "MethodName", "SVM"], [350, 352, "TaskName", "sentence classification"], [355, 359, "TaskName", "multi - class classification"], [393, 396, "TaskName", "Natural Language Inference"], [413, 414, "MethodName", "autoencoder"], [463, 465, "DatasetName", "hate speech"]]}
{"text": "This work is the first to show that Membership Query Synthesis in an NLP setting is feasible . Our approach uses a Variational Autoencoder as a representation learner and generates informative ac - tive learning queries from latent space . The classification performance for the generated instances is competitive with pool - based active learning strategies and outperforms other AL strategies with regard to annotation cost ( time ) and computational complexity . The main advantage of Membership Query Synthesis for active learning is that it allows us to target specific points along the separating hyperplane and thus to provide the classifier with information on specific areas of uncertainty in the data space . While pool - based active learning has the same objective , Membership Query Synthesis gives us a more precise tool to explore the data space and to generate exactly those instances that we need , making MQS a promising approach for future work in active learning .", "entities": [[22, 24, "MethodName", "Variational Autoencoder"], [53, 55, "TaskName", "active learning"], [81, 83, "TaskName", "active learning"], [118, 120, "TaskName", "active learning"], [158, 160, "TaskName", "active learning"]]}
{"text": "DAMO - NLP at SemEval - 2022 Task 11 : A Knowledge - based System for Multilingual Named Entity Recognition", "entities": [[16, 20, "TaskName", "Multilingual Named Entity Recognition"]]}
{"text": "The MultiCoNER shared task aims at detecting semantically ambiguous and complex named entities in short and low - context settings for multiple languages . The lack of contexts makes the recognition of ambiguous named entities challenging . To alleviate this issue , our team DAMO - NLP proposes a knowledge - based system , where we build a multilingual knowledge base based on Wikipedia to provide related context information to the named entity recognition ( NER ) model . Given an input sentence , our system effectively retrieves related contexts from the knowledge base . The original input sentences are then augmented with such context information , allowing significantly better contextualized token representations to be captured . Our system wins 10 out of 13 tracks in the MultiCoNER shared task . 1 * : project lead . \u22c6 : equal contributions .", "entities": [[1, 2, "DatasetName", "MultiCoNER"], [71, 74, "TaskName", "named entity recognition"], [75, 76, "TaskName", "NER"], [127, 128, "DatasetName", "MultiCoNER"]]}
{"text": "The MultiCoNER shared task ( Malmasi et al , 2022b ) aims at building Named Entity Recognition ( NER ) systems for 11 languages , including English , Spanish , Dutch , Russian , Turkish , Korean , Farsi , German , Chinese , Hindi , and Bangla . The task has three kinds of tracks including one multilingual track , 11 monolingual tracks and one code - mixed track . The multilingual track requires training multilingual NER models that are able to handle all languages . The monolingual tracks require training individual monolingual models where each model works for only one language . The code - mixed track requires handling code - mixed samples ( sentences that may involve multiple languages ) . The datasets mainly contain sentences from three domains : Wikipedia , web questions and user queries , k\u00f6pings is rate which are usually short and low - context sentences . Moreover , these short sentences usually contain semantically ambiguous and complex entities , which makes the problem more difficult . In practice , professional annotators usually use their domain knowledge to disambiguate such kinds of entities . They may retrieve the related documents from a knowledge base ( KB ) or from a search engine to better guide them the annotation of ambiguous named entities ( Wang et al , 2019 ) . Therefore , we believe retrieving related knowledge can help the NER model to disambiguate hard samples in the shared task as well . A motivating example is shown in Figure 1 , which shows how the retrieval results could help to improve the prediction in practice . In this paper , we propose a general knowledgebased system for the MultiCoNER shared task . We propose to retrieve the related documents of the input sentence so that the recognition of difficult entities can be significantly eased . Based on Wikipedia of the 11 languages , we build a multilingual KB to search for the related documents of the input sentence . We then feed the input sentence and the related documents into the NER model . Moreover , we propose an iterative retrieval approach to i m - prove the retrieval quality . During training , we propose multi - stage fine - tuning . We first train a multilingual model so that the NER model can learn from all annotations . Next , we train the monolingual models ( one for each language ) and a code - mixed model by using the fine - tuned XLM - RoBERTa ( XLM - R ) ( Conneau et al , 2020 ) embeddings in the multilingual model as initialization to further boost model performance on monolingual and code - mixed tracks . For each track , we train multiple models with different random seeds and use majority voting to form the final predictions . Besides the system description , we make the following observations based on our experiments : 1 . Knowledge - based systems can significantly improve both in - and out - of - domain performance compared with system without knowledge inputs . 2 . Our multi - stage fine - tuning approach can help improve model performance in all the monolingual and code - mixed tracks . The approach can also reduce the training time to speed up our system building at different stages . 3 . Our iterative retrieval strategy can further improve the retrieval quality and result in significant improvement on the performance of codemixed track . 4 . Searching over Wikipedia KB performs better than using online search engines on the Multi - CoNER datasets . 5 . Comparing with other model variants we have tried , our NER model enjoys a good balance between model performance and speed .", "entities": [[1, 2, "DatasetName", "MultiCoNER"], [14, 17, "TaskName", "Named Entity Recognition"], [18, 19, "TaskName", "NER"], [77, 78, "TaskName", "NER"], [237, 238, "TaskName", "NER"], [286, 287, "DatasetName", "MultiCoNER"], [349, 350, "TaskName", "NER"], [390, 391, "TaskName", "NER"], [423, 424, "MethodName", "XLM"], [425, 426, "MethodName", "RoBERTa"], [427, 428, "MethodName", "XLM"], [469, 470, "DatasetName", "seeds"], [620, 621, "TaskName", "NER"]]}
{"text": "We introduce how our knowledge - based NER system works in this section . Given a sentence of n tokens x = { x 1 , , x n } , the sentence is fed into our knowledge retrieval module . The knowledge retrieval module takes the sentence as the query and retrieves top - k related paragraphs in KB . The system then concatenates the input sentence and the related paragraphs together and feeds the concatenated sequence into the embeddings . The output token representations of the input sentence are fed into a linear - chain conditional random field ( CRF ) ( Lafferty et al , 2001 ) layer and the CRF layer produces the label predictions . Given the label predictions of multiple NER models with different random seeds , the ensemble module uses a voting strategy to decide the final prediction\u015d y = { \u0177 1 , , \u0177 n } of the sentence . The architecture of our framework is shown in Figure 2 .", "entities": [[7, 8, "TaskName", "NER"], [97, 100, "MethodName", "conditional random field"], [101, 102, "MethodName", "CRF"], [113, 114, "MethodName", "CRF"], [126, 127, "TaskName", "NER"], [131, 132, "DatasetName", "seeds"]]}
{"text": "Retrieval - augmented context is effective for named entity recognition tasks ( Wang et al , 2021b ) , as external relevant contexts can provide auxiliary information for disambiguating complex named entities . We construct multilingual KBs based on Wikipedia pages of the 11 languages , and then retrieve relevant documents by using the input sentence as a query . These retrieved documents act as contexts and are fed into the NER module . To enhance the retrieval quality , we further designed an iterative retrieval approach , which incorporates predicted entities of NER models into the search query . Knowledge Base Building Wikipedia is an evolving source of knowledge that can facilitate many NLP tasks ( Chen et al , 2017 ; Verlinden et al , 2021 ) . Wikipedia provides a rich collection of mention hyperlinks ( referred to as wiki anchors ) . For example , in the sentence \" Steve Jobs founded Apple \" , entities \" Steve Jobs \" and \" Apple \" are linked to the wiki entries Steve_Jobs and Apple_Inc respectively . For the NER task , these anchors provide useful clues on where the entities are to the model . Based on Wikipedia we can build local Wikipedia search engines to retrieve the relevant context of the input sentences for each language . We download the latest ( 2021 - 12 - 20 ) version of the Wikipedia dump from Wikimedia 3 and convert it to plain texts . Then we use ElasticSearch ( ES ) 4 to index them . ElasticSearch is document - oriented , and the document is the least searchable unit . We define the document in our local Wikipedia search engines with three fields : sentence , paragraph and title . We create inverted indexes on both the sentence field and the title field . The former is used as a sentence - level full - text retrieval field , while the latter indicates the core entity described by the wiki page and can be used as an entity - level retrieval field . The paragraph field stores the contexts of the sentence . To take advantage of the rich wiki anchors in Wikipedia paragraphs , we marked them with special markers . For example , to incorporate the hyperlinks [ Apple Apple Inc ] and [ Steve Jobs Steve Jobs ] to the paragraph , we transformed \" Steve Jobs founded Apple \" into \" < e : Steve Jobs > Steve Jobs</e > founded < e : Apple_inc > Apple</e > \" 5 . Sentence Retrieval Retrieval at the sentence level takes the input sentence as a query and retrieves the top - k documents on the sentence field . Given an input sentence , we select the corresponding search engine according to the language of the sentence . Iterative Entity Retrieval The core of the NER task lies in the entities , while retrieval at the sentence level overlooks the key entities in the sentences . For this reason , we consider the relevance of the entities in the sentence to the title field in the documents during retrieval . We concatenate the entities in the sentences with \" | \" and then retrieve them on the title field . On the training and development sets , we utilize the ground - truth entities directly . On the test set , we first perform the sentence retrieval and then use the entity mentions 6 predicted by the model for entity retrieval . This bootstrapping manner can be applied for T turns . Context Processing After top - k results from the KB are retrieved , the system post - processes the retrieved documents into the contexts of the input sentence . There are three options of utilizing the texts in the documents , which are : 1 ) use the matched paragraph ; 2 ) use the matched sentence ; 3 ) use the matched sentence but remove the wiki anchors . We compare the performance of each option in section 5.4 . In each retrieved document , we concatenate the title and texts together to form the contextx i . The results are then concatenated into { x 1 , , x k } based on the retrieval ranking .", "entities": [[7, 10, "TaskName", "named entity recognition"], [71, 72, "TaskName", "NER"], [93, 94, "TaskName", "NER"], [181, 182, "TaskName", "NER"], [475, 477, "TaskName", "Entity Retrieval"], [481, 482, "TaskName", "NER"], [585, 587, "TaskName", "entity retrieval"]]}
{"text": "We compare several types of KBs and contexts during our system building . Online Search Engine In the early stage , we tried to use the knowledge retrieved from Google Search , which can retrieve related knowledge from a large scale of webs and is believed to be a strong multilingual search engine .", "entities": [[29, 30, "DatasetName", "Google"]]}
{"text": "Table 6 shows the speed of each module in our system . In the table , we also show that the retrieval speed of our local KB is significantly faster than that of Google Search . The bottleneck of the system speed is the NER module rather than the knowledge retrieval module . The main reason for the slow speed of the NER module is that the input length of the knowledge - based system is significantly longer than the original input . Taking the EN test set as an example , there are on average 10 tokens for each input sentence in the original test set while there are 218 tokens for the input of our knowledge - based system . The longer inputs slow down the encoding at XLM - R embeddings .", "entities": [[33, 34, "DatasetName", "Google"], [44, 45, "TaskName", "NER"], [62, 63, "TaskName", "NER"], [130, 131, "MethodName", "XLM"]]}
{"text": ") CE is one of the usual approaches to NER , which concatenates different kinds of embeddings to improve the token representations . In the early stage of our system building , we compare CE with only using the XLM - R embeddings based on the knowledge retrieved from the Google Search . Results in Table 7 show that CE models are stronger than the models using XLM - R embeddings only in all the cases , which show the effectiveness of CE . ACE ( Automated Concatenation of Embeddings ) ACE ( Wang et al , 2021a ) is an improved version of CE which automatically selects a better concatenation of the embeddings . We use the same embedding types as CE and the knowledge are from our Wikipedia KB . We experiment on EN , ES , NL , RU , TR , KO and FA , which are strong with PARA contexts . In Table 9 , we further compare ACE with ensemble XLM - R models . Results show ACE can improve the model performance and even outperform the ensemble models 13 . The results in Table 7 and 9 show the advantage of the embedding concatenation . However , as we have shown in Section 5.5 , the prediction speed is quite slow with the single XLM - R embeddings . The CE models further slow down the prediction speed since the models contain more embeddings . The ACE models usually have faster prediction speed than the CE models . However , training the ACE models is quite slow . It takes about four days to train a single ACE model . Moreover , the ACE models can not use the development set to train the model since they use development score as the reward to select the embedding concatenations . Therefore , due to the time constraints , we did not use these two variants in our submission during the shared task period .", "entities": [[9, 10, "TaskName", "NER"], [39, 40, "MethodName", "XLM"], [50, 51, "DatasetName", "Google"], [67, 68, "MethodName", "XLM"], [147, 148, "MethodName", "FA"], [166, 167, "MethodName", "XLM"], [221, 222, "MethodName", "XLM"]]}
{"text": "In this paper , we describe our knowledge - based system for the MultiCoNER shared task , which wins 10 out of 13 tracks in the shared task . We construct multilingual KBs and retrieve the related documents from KBs to enhance the token representations of input text . We show that the NER models can use the retrieved knowledge to facilitate complex entity prediction , significantly improving both the in - domain and out - of - domain performance . Multi - stage fine - tuning can help the monolingual models learn from the training data of all the languages and improve the model performance and training efficiency . We also show that the system presents a good balance between the model performance and prediction efficiency to meet the time requirement in the test phase . We believe this system can be widely applied to other domains for the task of NER . For future work , we plan to improve the retrieval quality and adopt the system to support other kinds of entity - related tasks .", "entities": [[13, 14, "DatasetName", "MultiCoNER"], [53, 54, "TaskName", "NER"], [152, 153, "TaskName", "NER"]]}
{"text": "As we state in Section 3.3 , we use majority voting as the ensemble algorithm in our system . We show an experiment about how the voting threshold affect the ensemble model performance during our system building on the development set . We ensemble the models on DE , ZH , HI , BN , MIX with PARA since these five tracks have relatively lower performance than the other 7 tracks . In Figure 4 , we show how the threshold of the majority voting affects the model performance . From the figure , we can see that the best threshold varies over the language . Therefore , we simply choose 0.5 as there is no best threshold value . Moreover , we compare the majority voting ensemble and CRF level ensemble in Table 12 . The CRF level ensemble averages the emission and transition scores in the Eq . 1 predicted by the candidate models and uses the Viterbi algorithm to get the prediction . The results show that CRF level ensemble performs inferior to the majority voting ensemble . The possible reason is that training with different random seeds may lead to different emission transition scores at different Avg . scales . As a result , the models with larger scales have higher weights in the ensemble .", "entities": [[129, 130, "MethodName", "CRF"], [137, 138, "MethodName", "CRF"], [170, 171, "MethodName", "CRF"], [190, 191, "DatasetName", "seeds"]]}
{"text": "The detailed statistics of the MultiCoNER dataset are listed in Table 10 and the statistics of our KBs ares shown in Table 11 .", "entities": [[5, 6, "DatasetName", "MultiCoNER"]]}
{"text": "English - Portuguese Biomedical Translation Task Using a Genuine Phrase - Based Statistical Machine Translation Approach", "entities": [[4, 5, "TaskName", "Translation"], [13, 15, "TaskName", "Machine Translation"]]}
{"text": "Our approach to produce translations for the ACL - 2016 Biomedical Translation Task on the English - Portuguese language pair , in both directions , is described . Own preliminary tests results and final results , measured by the shared task organizers , are also presented .", "entities": [[11, 12, "TaskName", "Translation"]]}
{"text": "This paper shows how we obtained our results using our patented Machine Translation system to produce translations for the English - Portuguese language pair from the Biomedical Translation Task . Our approach differs from common Statistical Machine Translation approaches like Moses ( Koehn et al , 2007 ) in several aspects : phrases are not analyzed at their word level in any model ; the language model depends on the target al ernatives of given adjacent sources and does not try to avoid null scores to phrases that do not occur ; the translation score is not log - linear , but instead a tuned weighted average between the translation model and the language model , and so no smoothing techniques are required ; several models can be used with different relevances or weights ; and instead of simply relying on statistics , we include human validation and correction on several stages of the system , namely for validating extracted term translations , to improve the quality of the source data used in the automatically produced translations . As requested , the translation results were produced using the sentence - aligned training data described below ( for the English - Portuguese language pair , in our case ) , provided by the shared task organizers : medline - pubmed : parallel corpora from medline ; scielo - gma - biological : parallel biological documents from the Scielo database ( Neves et al , 2016 ) ; and scielo - gma - health : parallel health documents from the Scielo database ( Neves et al , 2016 ) . Table 1 shows the features of the English ( en ) and Portuguese ( pt ) languages of each provided corpora , namely their number of lines and words . The training corpora had to undergo several processing stages in order to support the production of the intended translations , as described in the following section .", "entities": [[11, 13, "TaskName", "Machine Translation"], [27, 28, "TaskName", "Translation"], [36, 38, "TaskName", "Machine Translation"]]}
{"text": "In order to produce translations , our system ( like any other Statistical Machine Translation system ) requires a translation model and a language model to support the translation decoding stage . To calculate such models the available data had to go through several processing steps described in the following subsections . Since each of the training corpus has been made available separately , we also opted to process each of them separately so that we were then able to use them with different weights , assigning more or less weight to models with higher or lower relevance , respectively . See extended explanation in 4 .", "entities": [[13, 15, "TaskName", "Machine Translation"]]}
{"text": "The scores of our submitted translations are shown in The results obtained were clearly below what we had expected . And what is most disturbing is the negative impact of features we expected to improve results , an expectation backed by our own tests . However , there are a few reasons we can think of for these values , namely the way the BLEU measure has been calculated ( case sensitivity and synonyms penalty - translating \" home \" instead of \" house \" might be perfectly fine ) , the differences between European Portuguese and Brazilian Portuguese , and the presence of several spelling and alignment errors in the training data . Nonetheless , we can still take several actions to improve our system : namely testing both parallel corpora , health and biology , with identical weights : using Europarl and eventually EMEA corpus ; the refinement of our phrase translation extraction ; the extraction of specific bilingual terminology , additionally to the use of cognaticity ; subsentence realignment after the bilingual terminology extraction , and a more efficient implementation of the patterns ( comparable to a hierarchical translation ) application .", "entities": [[64, 65, "MetricName", "BLEU"], [145, 146, "MethodName", "EMEA"]]}
{"text": "Improving Distantly - supervised Entity Typing with Compact Latent Space Clustering", "entities": [[4, 6, "TaskName", "Entity Typing"]]}
{"text": "Recently , distant supervision has gained great success on Fine - grained Entity Typing ( FET ) . Despite its efficiency in reducing manual labeling efforts , it also brings the challenge of dealing with false entity type labels , as distant supervision assigns labels in a contextagnostic manner . Existing works alleviated this issue with partial - label loss , but usually suffer from confirmation bias , which means the classifier fit a pseudo data distribution given by itself . In this work , we propose to regularize distantly supervised models with Compact Latent Space Clustering ( CLSC ) to bypass this problem and effectively utilize noisy data yet . Our proposed method first dynamically constructs a similarity graph of different entity mentions ; infer the labels of noisy instances via label propagation . Based on the inferred labels , mention embeddings are updated accordingly to encourage entity mentions with close semantics to form a compact cluster in the embedding space , thus leading to better classification performance . Extensive experiments on standard benchmarks show that our CLSC model consistently outperforms state - of - the - art distantly supervised entity typing systems by a significant margin .", "entities": [[12, 14, "TaskName", "Entity Typing"], [59, 60, "MetricName", "loss"], [191, 193, "TaskName", "entity typing"]]}
{"text": "Fine - grained entity typing takes a corpus and an external knowledge base ( KB ) with a type hierarchy Y as input . Given an entity mention ( i.e. , a sequence of token spans representing an entity ) in the corpus , our task is to uncover its corresponding type - path in Y based on the context . By applying distant supervision , each mention is first linked to an existing entity in KB , and then labeled with all its possible types . Formally , a labeled corpus can be represented as triples D = { ( m i , c i , Y i ) } n i=1 , where m i is the i - th mention , c i is the context of m i , Y i is the set of candidate types of m i . Note that types in Y i can form one or more type paths . In addition , we denote all terminal ( leaf ) types of each type path in Y i as the target type set Y t i ( e.g. , for Y i = { artist , teacher , person } , Y t i = { artist , teacher } ) . This setting is also adopted by ( Xu and Barbosa , 2018 ) . As each entity in KB can have several type paths , out - of - context noise may exist when Y i contains type paths that are irrelevant to m i in context c i . In this work , we argue triples where Y i contains only one type path ( i.e. , | Y t i | = 1 ) as clean data . Other triples are treated as noisy data , where Y i contains both the true type path and irrel - evant type paths . Noisy data usually takes a considerable portion of the entire dataset . The major challenge for distantly supervised typing systems is to incorporate both clean and noisy data to train high - quality type classifiers .", "entities": [[3, 5, "TaskName", "entity typing"]]}
{"text": "We evaluate our method on two standard benchmarks : OntoNotes and BBN : OntoNotes : The OntoNotes dataset is composed of sentences from the Newswire part of OntoNotes corpus ( Weischedel et al , 2013 ) . ( Gillick et al , 2014 ) annotated the training part with the aid of DBpedia spotlight ( Daiber et al , 2013 ) , while the test data is manually annotated . BBN : The BBN dataset is composed of sentences from Wall Street Journal articles and is manually annotated by ( Weischedel and Brunstein , 2005 ) . ( Ren et al , 2016a ) regenerated the training corpus via distant supervision . In this work we use the preprocessed datasets provided by ( Abhishek et al , 2017 ; Xu and Barbosa , 2018 ) . Table 2 shows detailed statistics of the datasets .", "entities": [[9, 10, "DatasetName", "OntoNotes"], [13, 14, "DatasetName", "OntoNotes"], [16, 17, "DatasetName", "OntoNotes"], [27, 28, "DatasetName", "OntoNotes"], [52, 53, "DatasetName", "DBpedia"]]}
{"text": "In this paper , we propose a new method for distantly supervised fine - grained entity typing , which leverages imperfect annotations as model regularization via Compact Latent Space Clustering ( CLSC ) . Experiments on two standard benchmarks demonstrate that our method consistently outperforms state - of - the - art models . Further study reveals our method is more robust than the former state - of - the - art approach as the portion of noisy data rises . The proposed method is general for other tasks with imperfect annotation . As a part of future investigation , we plan to apply the approach to other distantly supervised tasks , such as relation extraction .", "entities": [[15, 17, "TaskName", "entity typing"], [114, 116, "TaskName", "relation extraction"]]}
{"text": "Complementary Evidence Identification in Open - Domain Question Answering", "entities": [[4, 9, "TaskName", "Open - Domain Question Answering"]]}
{"text": "This paper proposes a new problem of complementary evidence identification for opendomain question answering ( QA ) . The problem aims to efficiently find a small set of passages that covers full evidence from multiple aspects as to answer a complex question . To this end , we proposes a method that learns vector representations of passages and models the sufficiency and diversity within the selected set , in addition to the relevance between the question and passages . Our experiments demonstrate that our method considers the dependence within the supporting evidence and significantly improves the accuracy of complementary evidence selection in QA domain .", "entities": [[12, 14, "TaskName", "question answering"], [96, 97, "MetricName", "accuracy"]]}
{"text": "Vector Space Modeling We apply BERT model to estimate the likelihood of a paragraph p being the supporting evidence to the question q , denoted as P ( p | q ) . Let q and p i denote the input texts of a question and a passage . We feed q and the concatenation of q and p i into the BERT model , and use the hidden states of the last layer to represent q and p i in vector space , denoted as q and p i respectively . A fully connected layer f ( ) followed by sigmoid activation is added to the end of the BERT model , and outputs a scalar P ( p i | q ) to estimate how relevant the paragraph p i is to the question . Note that in our implementation p i is based on both q and p i , but we omit the condition on q for simplicity . Complementary Conditions Previous works extract evidence paragraphs according to P ( p | q ) , which is estimated on each passage separately without considering the dependency among selected paragraphs . To extract complementary evidence , we propose that the selected passages P sel should satisfy the following conditions that intuitively encourage each selected passage to be a basis to support the question : Relevancy : P sel should have a high probability of p i P sel P ( p i | q ) ; Diversity : P sel should cover passages as diverse as possible , which can be measured by the average distance between any pairs in P sel , e.g. , maximizing i , j { i , j | p i , p j P sel , i = j } 1 ( p i , p j ) . Here 1 ( , ) denotes L 1 distance ; Compactness : P sel should optimize the aforementioned conditions while the size being minimal . In this work we constrain the compactness by fixing | P sel | and meanwhile maximizing cos ( i { i | p i P sel } p i , q ) . We use cos ( , ) to encourage the collection of evidence covers what needed by the question .", "entities": [[5, 6, "MethodName", "BERT"], [62, 63, "MethodName", "BERT"], [101, 103, "MethodName", "sigmoid activation"], [110, 111, "MethodName", "BERT"]]}
{"text": "Datasets Considering the prerequisite of sentence - level evidence annotations , we evaluate our approach on two datasets , a synthetic dataset MNLI - 12 and a real application HotpotQA - 50 . Data sampling is detailed in Appendix B. MNLI - 12 is constructed based on the textual entailment dataset MNLI ( Williams et al , 2018 ) , in order to verify the ability of our method in finding complementary evidence . In original MNLI , each premise sentence corresponds to three hypotheses sentences : entailment , neutral and contradiction . To generate complementary pairs for each premise sentence , we split each hypothesis sentence into two segments . The goal is to find the segment combination that entails premise sentence , and our dataset , by definition , ensures that only the combination of two segments from the entailment hypothesis can entail the premise , not any of its subset or other combinations . The original train / dev / test splits from MNLI are used . HotpotQA - 50 is based on the open - domain setting of the multi - hop QA benchmark HotpotQA ( Yang et al , 2018 ) . The original task requires to find evidence passages from abstract paragraphs of all Wikipedia pages to support a multi - hop question . For each q , we collect 50 relevant passages based on bigram BM25 ( Godbole et al , 2019 ) . Two positive evidence passages to each question are provided by human annotators as the ground truth . Note that there is no guarantee that P 50 covers both evidence passages here . We use the original development set from HotpotQA as our test set and randomly split a subset from the original training set as our development set .", "entities": [[22, 23, "DatasetName", "MNLI"], [29, 30, "DatasetName", "HotpotQA"], [40, 41, "DatasetName", "MNLI"], [51, 52, "DatasetName", "MNLI"], [76, 77, "DatasetName", "MNLI"], [166, 167, "DatasetName", "MNLI"], [170, 171, "DatasetName", "HotpotQA"], [188, 189, "DatasetName", "HotpotQA"], [280, 281, "DatasetName", "HotpotQA"]]}
{"text": "Data : Vector representation of question ( q ) , vector representation of all the N passages { pn } ( { pn } ) ; the maximum number of passage to select ( L ) ; the beam size ( M ) ; a vector of weights for all regularization terms \u03bb . Result : The top ranked complementary passages . / * Predict the probability P ( p i ) of being a supporting passage for each passage B Data Sampling In original MNLI , each premise sentence P corresponds to one entailment EP , one neutral NP and one contradiction CP . We take the premise P as q , and split each of its corresponding hypotheses into two segments with a random cutting point near the middle of the sentence , resulting in a total of 6 segments { E 1 P , E 2 P , N 1 P , N 2 P , C 1 P , C 2 P } . Mixing them with the 6 segments corresponding to another premise X , we can finally have P + = { E 1 P , E 2 P } and P \u2212 = { N 1 P , N 2 P , C 1 P , C 2 P , E 1 X , E 2 X , N 1 X , N 2 X , C 1 X , C 2 X } . Consequently , we sample one positive and eight negative pairs respectively from P + and P \u2212 . A pair like { E 1 P , C 2 X } is considered as negative . To ensure the segments are literally meaningful , each segment is guaranteed to be longer than 5 words . p i given q * / 1 for i [ 1 , N ] do 2 P ( p i ) f ( q , p i ) ; HotpotQA In HotpotQA , the true supporting paragraphs of each question q are given . Therefore , we can easily form P + and P \u2212 and sample positive and negative pairs of paragraphs respectively from P + and P \u2212 . A special pair that contains one true supporting paragraph and one non - supporting paragraph is considered as a negative pair .", "entities": [[85, 86, "DatasetName", "MNLI"], [325, 326, "DatasetName", "HotpotQA"], [327, 328, "DatasetName", "HotpotQA"]]}
{"text": "Domain adaptation in practice : Lessons from a real - world information extraction pipeline", "entities": [[0, 2, "TaskName", "Domain adaptation"]]}
{"text": "Advances in transfer learning and domain adaptation have raised hopes that oncechallenging NLP tasks are ready to be put to use for sophisticated information extraction needs . In this work , we describe an effort to do just that - combining state - of - the - art neural methods for negation detection , document time relation extraction , and aspectual link prediction , with the eventual goal of extracting drug timelines from electronic health record text . We train on the THYME colon cancer corpus and test on both the THYME brain cancer corpus and an internal corpus , and show that performance of the combined systems is unacceptable despite good performance of individual systems . Although domain adaptation shows improvements on each individual system , the model selection problem is a barrier to improving overall pipeline performance .", "entities": [[2, 4, "TaskName", "transfer learning"], [5, 7, "TaskName", "domain adaptation"], [51, 53, "TaskName", "negation detection"], [56, 58, "TaskName", "relation extraction"], [61, 63, "TaskName", "link prediction"], [118, 120, "TaskName", "domain adaptation"], [128, 130, "TaskName", "model selection"]]}
{"text": "We began this work by developing several NLP components necessary to extract drug temporality signatures , including negation detection , relation to document creation time ( DocTimeRel ) , and aspectual link extraction ( ALINK ) , all detailed below . Detecting negation helps us avoid false positives from mentions corresponding to , for example , decisions to not use a drug . DocTimeRel helps us distinguish mentions of drugs that are current from those that predate the current time period , or are being speculated about for future use . ALINK can model drug start , stop , and continuation events , which can help to distinguish whether a missing mention in the middle of a record corresponds to a stop and restart , or an incidentally omitted mention . Figure 1 shows an example instance of a drug mention to be classified for all three tasks . The THYME dataset ( Styler IV et al , 2014 ) , released as part of Clinical TempEval , contains all three of these annotation types , on 1200 notes of patients with colon and brain cancer . We train all models on the colon cancer section ( details on data are in Section 4 ) . While our bigger project is specific to drug mentions , the problem is not limited to drug mentions , so we train and evaluate on all annotated events in the THYME corpus . We also assume that events are given , to allow a straightforward metric of how many events we \" get right \" when combining all property predictions . In the real world , events will have to be automatically detected , so our metric will be an upper bound on how often the combined models get everything correct .", "entities": [[17, 19, "TaskName", "negation detection"]]}
{"text": "DocTimeRel classification is the task of relating an event to the document creation time . The categories are BEFORE , OVERLAP , AFTER , and BE - FORE / OVERLAP . As above , we model this as a span - in - context classification , and we again compare a feature - based approach with a RoBERTabased approach . The feature - based approach again uses the default cTAKES SVM - based implementation ( Lin et al , 2016 ) , with features based on bags of words in and around the event , and verb tense information for verbs on either side of the event . We train a separate RoBERTa - based model with the same architecture as the negation model , with the only difference being that the output layer is a softmax over the four categories rather than a sigmoid .", "entities": [[70, 71, "MethodName", "SVM"], [112, 113, "MethodName", "RoBERTa"], [136, 137, "MethodName", "softmax"]]}
{"text": "Aspectual link extraction ( ALINK ) is the task of classifying whether an event mention is related to an aspectual temporal modifier , for example , discontinued . This is annotated as a relation between an event and a modifier , but we model it as an event property classification task since each event can only participate in one type of relation . The set of possible labels is INITIATES , CONTINUES , TERMINATES , and REINITIATES . We are not aware of any existing open - source models for this task , so for our feature - based baseline we train a model with the same SVM classification approach and feature set as the DocTimeRel model in cTAKES . We did not perform extensive feature engineering for this task , so further gains in the SVM system are probably possible . For the RoBERTa - based model , we used the same architecture as both systems above , with a softmax over the 5 categories - the 4 ALINK categories above as well as NONE , indicating a drug mention does not participate in any ALINK relation . NONE is by far the most common category .", "entities": [[107, 108, "MethodName", "SVM"], [125, 127, "TaskName", "feature engineering"], [136, 137, "MethodName", "SVM"], [144, 145, "MethodName", "RoBERTa"], [161, 162, "MethodName", "softmax"]]}
{"text": "The results also show that combining NLP systems for new , complex , information needs is likely to run into issues even when individual systems perform well . In particular , our experiments raise questions about real - world use of domain adaptation . If we treated THYME colon and brain sets as representative in - domain and out - of - domain datasets we would select RoBERTa or RoBERTa+LM for everything . But an oracle optimizing PH performance would tell us to use the SVM for negation and DocTimeRel and RoBERTa+LM for ALINK . One of the difficulties in even studying domain adaptation is model selection - if labeled target data is not available , standard practices like tuning on held out data are impossible . But the reality our results suggest is that different algorithms work well on different tasks and datasets , and selecting the best model for each task is an unsolved and under - studied problem . One direction of research that may address these concerns is on better modeling of domains themselves . The problem has been exacerbated with the move from feature - based classifiers to pre - trained black box models , as it is now even more difficult to understand the cause of errors in new domains without interpretable features . Domain adaptation should leverage \" BERTology \" and interpretability research to help understand how different aspects of domains contribute to performance differences . For example , in clinical notes , variation in institutions , specialties , note types , authors , etc . , all probably contribute differently to domain shift , and these sources of variation should be empirically explored . Future work will explore this direction to develop unsupervised model selection algorithms that better predict target domain performance .", "entities": [[41, 43, "TaskName", "domain adaptation"], [67, 68, "MethodName", "RoBERTa"], [85, 86, "MethodName", "SVM"], [102, 104, "TaskName", "domain adaptation"], [105, 107, "TaskName", "model selection"], [220, 222, "TaskName", "Domain adaptation"], [291, 293, "TaskName", "model selection"]]}
{"text": "XGLUE : A New Benchmark Dataset for Cross - lingual Pre - training , Understanding and Generation", "entities": [[0, 1, "DatasetName", "XGLUE"]]}
{"text": "In this paper , we introduce XGLUE , a new benchmark dataset that can be used to train large - scale cross - lingual pre - trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross - lingual tasks . Comparing to GLUE ( Wang et al , 2019 ) , which is labeled in English for natural language understanding tasks only , XGLUE has two main advantages : ( 1 ) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios ; ( 2 ) for each task , it provides labeled data in multiple languages . We extend a recent cross - lingual pre - trained model Unicoder to cover both understanding and generation tasks , which is evaluated on XGLUE as a strong baseline . We also evaluate the base versions ( 12 - layer ) of Multilingual BERT , XLM and XLM - R for comparison . 1", "entities": [[6, 7, "DatasetName", "XGLUE"], [49, 50, "DatasetName", "GLUE"], [64, 67, "TaskName", "natural language understanding"], [70, 71, "DatasetName", "XGLUE"], [87, 90, "TaskName", "natural language understanding"], [133, 134, "DatasetName", "XGLUE"], [152, 153, "MethodName", "BERT"], [154, 155, "MethodName", "XLM"], [156, 157, "MethodName", "XLM"]]}
{"text": "Pre - training + Fine - tuning has become a new NLP paradigm , where the general knowledge are firstly learnt from large - scale corpus by self - supervised learning and then transferred to downstream tasks by task - specific fine - tuning . Three different types of pre - trained models are explored recently , including monolingual pre - trained models ( Radford et al , 2018 ; Devlin et al , 2019 ; Yang et al , 2019b ; Lewis et al , 2019a ) , multilingual and cross - lingual pre - trained models ( Devlin et al , 2019 ; Conneau and Lample , 2019 ; and multimodal pre - trained models ( Lu et al , 2019 ; Li et al , 2020 ; . In this paper , we focus on the cross - lingual pretrained models , due to their importance to alleviating the low - resource issue among languages , where an NLP task often has rich training data in one language ( such as English ) but has few or no training data in other languages ( such as French and German ) . In order to further advance the development of cross - lingual pre - trained models for various downstream tasks in different languages , this paper introduces XGLUE , a new benchmark dataset that can be used to : ( i ) train large - scale cross - lingual pre - trained models using multilingual and bilingual corpora , ( ii ) evaluate generalization capabilities of the cross - lingual pre - trained models across a diverse set of cross - lingual tasks . The contribution of XGLUE is two - fold . First , it provides 11 diversified cross - lingual tasks covering both understanding and generation scenarios . XTREME ) is a concurrent work of XGLUE . But it includes cross - lingual understanding tasks only . Besides , XGLUE introduces 6 new tasks selected from Search , Ads and News scenarios , which makes XGLUE have more practical values . Second , an extended version of Unicoder is described and evaluated as a strong cross - lingual pre - trained model baseline on XGLUE for both understanding and generation tasks . We also evaluate the base versions ( 12 - layer ) of Multilingual BERT ( Devlin et al , 2019 ) , XLM ( Conneau and Lample , 2019 ) and XLM - R for comparison .", "entities": [[16, 18, "TaskName", "general knowledge"], [27, 31, "TaskName", "self - supervised learning"], [220, 221, "DatasetName", "XGLUE"], [280, 281, "DatasetName", "XGLUE"], [303, 304, "DatasetName", "XTREME"], [310, 311, "DatasetName", "XGLUE"], [324, 325, "DatasetName", "XGLUE"], [340, 341, "DatasetName", "XGLUE"], [369, 370, "DatasetName", "XGLUE"], [390, 391, "MethodName", "BERT"], [399, 400, "MethodName", "XLM"], [408, 409, "MethodName", "XLM"]]}
{"text": "Multilingual Corpus We extract raw sentences from Wikipedia using WikiExtractor . It leads to a 101 G multilingual corpus covering 100 languages . Bilingual Corpus We use an in - house pipeline to extract bilingual sentence pairs from the Web , which leads to a 146 G bilingual corpus covering 27 languages , including Arabic , Bulgarian , Danish , German , Greek , English , Spanish , Finnish , French , Hebrew , Hindi , Hungarian , Indonesian , Italian , Japanese , Korean , Dutch , Polish , Portuguese , Russian , Swedish , Swahili , Thai , Turkish , Urdu , Vietnamese and Chinese . All the bilingual pairs are English to another language .", "entities": [[102, 103, "DatasetName", "Urdu"]]}
{"text": "Multilingual Corpus Following , we construct a clean version of Common Crawl ( CC ) 2 as the multilingual corpus . First , we use a language identification model trained based on Wikipedia to classify the language of each page in CC . Then , we train a language model for each language using the corresponding part of the Wikipedia corpus , and use it to filter documents as did . We use one CC dump for English and twelve CC dumps for other languages . It leads to a 2 , 500 G multilingual corpus covering 89 languages . We also include the 101 G multilingual corpus described in Section 2.1.1 .", "entities": [[10, 12, "DatasetName", "Common Crawl"], [26, 28, "TaskName", "language identification"]]}
{"text": "We reuse the bilingual corpus described in Section 2.1.1 . We will add CCMatrix ( Schwenk et al , 2019 ) in the future .", "entities": [[13, 14, "DatasetName", "CCMatrix"]]}
{"text": "We select 11 cross - lingual tasks in XGLUE , which are categorized into 3 groups : single - input understanding tasks , pair - input understanding tasks , and generation tasks . For each task , training set is only available in English . In order to obtain a good performance on XGLUE , a model should be able to learn how to do a task well using its English training set , and then transfer this ability to test sets in other languages . Table 2 gives the dataset statistics and Table 3 lists languages covered by all tasks . 2 https://commoncrawl.org/.", "entities": [[8, 9, "DatasetName", "XGLUE"], [53, 54, "DatasetName", "XGLUE"]]}
{"text": "QG M - BERT - - 0.1 - 7.8 0.1 0.1 - 0.2 - - 0.1 - - - - - - - 1.4 XLM - Rbase - - 0.1 - 6.0 0.0 0.0 - 0.1 - - 0.0 - - - - - - - 1 . We find ( 1 ) Unicoder LC performs slightly better than M - BERT and XLM - R base on the 9 understanding tasks , as it is pre - trained based on multilingual and bilingual corpora at the same time and uses TLM ; . But it is not a fair comparison , because they use different text denoising tasks ( sentence prediction vs. span prediction ) and different generation mechanisms ( single - token prediction vs. multi - token prediction ) . We leave combining these two tasks for future work .", "entities": [[3, 4, "MethodName", "BERT"], [24, 25, "MethodName", "XLM"], [61, 62, "MethodName", "BERT"], [63, 64, "MethodName", "XLM"], [107, 108, "TaskName", "denoising"]]}
{"text": "We define pivot - language ( pl ) fine - tuning as finetune a pre - trained model for a downstream task using its labeled data in a pivot language ( e.g. English ) and then apply the fine - tuned model to all languages . Table 4 chooses English as the pivot language , as all tasks in XGLUE have labeled data in English . But is English always the optimal choice ? Will the results become better , if we do fine - tuning using other pivot languages ? To answer these questions , we evaluate Unicoder on XNLI and NTG using different pivot languages in fine - tuning and list comparison results in Table 5 and Table 6 , respectively . ( 1 ) For each test set in language l i in Table 5 and Table 6 , its best result is often achieved when the model is fine - tuned using l i as the pivot language ; ( 2 ) For XNLI in Table 5 , the best pivot languages are Spanish ( es ) , Greek ( el ) and Turkish ( tr ) , rather than English ( en ) . For NTG in Table 6 , the best pivot language is French ( fr ) for both Unicoder xDAE SC and Unicoder xF N P SC . It means the average quality of a cross - lingual pre - trained model could be further improved on a downstream task , by selecting a specific pivot language in finetuning .", "entities": [[59, 60, "DatasetName", "XGLUE"], [100, 101, "DatasetName", "XNLI"], [167, 168, "DatasetName", "XNLI"]]}
{"text": "We define multi - language ( ml ) fine - tuning as finetune a pre - trained model for a downstream task using all its available labeled data in different languages . We evaluate Unicoder on XNLI and NTG using this fine - tuning method and list evaluation results in Table 7 and Table 8 , respectively . We find multi - language fine - tuning can achieve better results than pivot - language fine - tuning on both XNLI and NTG . It means the average quality of a cross - lingual pre - trained model could be significantly improved on a downstream task , by using combined labeled data in multiple languages .", "entities": [[36, 37, "DatasetName", "XNLI"], [79, 80, "DatasetName", "XNLI"]]}
{"text": "We define multi - task ( mt ) fine - tuning as fine - tune a pre - trained model for multiple downstream tasks using their combined labeled data . To reduce the experimental cost , we evaluate Unicoder on 5 understanding tasks : XNLI , PAWS - X , NC , QAM and QADSM , using their merged English labeled data in fine - tuning . Results are listed in Table 9 . We find PAWS - X and QADSM can benefit from the joint fine - tuning strategy , but XNLI , NC and QAM can not . We leave discovering relationships between different tasks for better downstream task fine - tuning for future work .", "entities": [[44, 45, "DatasetName", "XNLI"], [46, 49, "DatasetName", "PAWS - X"], [76, 79, "DatasetName", "PAWS - X"], [92, 93, "DatasetName", "XNLI"]]}
{"text": "Dataset GLUE includes 9 natural language understanding tasks that are labeled in English only . Comparing to GLUE , XGLUE not only expands task annotations to multiple languages , but also includes natural language generation tasks . XNLI ( Conneau et al , 2018 ) , NER ( Sang , 2002 ; Sang and De Meulder , 2003 ) , POS Tagging ( Kim et al , 2017 ) , MLQA ( Lewis et al , 2019b ) and PAWS - X ( Yang et al , 2019a ) are 5 multilingual datasets built for specific tasks . XGLUE not only includes these 5 existing tasks , but also introduces 6 new tasks selected from real - world scenarios ( i.e. , Search , Ads and News ) . This makes XGLUE have more practical values . XTREME ) is a concurrent work of XGLUE . Comparing to it , XGLUE includes both understanding and generation tasks , which , to the best of our knowledge , is the first attempt in the cross - lingual dataset construction efforts . ) is a RoBERTa - version XLM without using translation language model in pre - training . It is trained based on a much larger multilingual corpus ( i.e. Com - mon Crawl ) and become the new state - of - the - art on XNLI . In this paper , we use both the Common Crawl corpus and the bilingual corpus , aiming to build a stronger baseline model on XGLUE . BART ( Lewis et al , 2019a ) and ProphetNet ( Yan et al , 2020 ) are two latest generative pre - trained models . We borrow ideas from these two works and extend Unicoder to cross - lingual generation tasks , which goes a step further to verify and explore different text generation approaches in the cross - lingual scenario .", "entities": [[1, 2, "DatasetName", "GLUE"], [4, 7, "TaskName", "natural language understanding"], [17, 18, "DatasetName", "GLUE"], [19, 20, "DatasetName", "XGLUE"], [37, 38, "DatasetName", "XNLI"], [46, 47, "TaskName", "NER"], [70, 71, "DatasetName", "MLQA"], [79, 82, "DatasetName", "PAWS - X"], [98, 99, "DatasetName", "XGLUE"], [131, 132, "DatasetName", "XGLUE"], [137, 138, "DatasetName", "XTREME"], [144, 145, "DatasetName", "XGLUE"], [150, 151, "DatasetName", "XGLUE"], [183, 184, "MethodName", "RoBERTa"], [186, 187, "MethodName", "XLM"], [226, 227, "DatasetName", "XNLI"], [236, 238, "DatasetName", "Common Crawl"], [252, 253, "DatasetName", "XGLUE"], [254, 255, "MethodName", "BART"], [263, 264, "MethodName", "ProphetNet"], [265, 268, "DatasetName", "Yan et al"], [307, 309, "TaskName", "text generation"]]}
{"text": "We present XGLUE as a new cross - lingual benchmark and conduct comprehensive evaluations with interesting findings observed . We thank STC - A NLP , Bing Answers , Bing Ads , Bing Relevance and Microsoft News for providing the datasets . A The fine - tune parameters of Unicoder on XGLUE .", "entities": [[2, 3, "DatasetName", "XGLUE"], [51, 52, "DatasetName", "XGLUE"]]}
{"text": "WikiGraphs : A Wikipedia Text - Knowledge Graph Paired Dataset", "entities": [[0, 1, "DatasetName", "WikiGraphs"]]}
{"text": "We present a new dataset of Wikipedia articles each paired with a knowledge graph , to facilitate the research in conditional text generation , graph generation and graph representation learning . Existing graph - text paired datasets typically contain small graphs and short text ( 1 or few sentences ) , thus limiting the capabilities of the models that can be learned on the data . Our new dataset WikiGraphs is collected by pairing each Wikipedia article from the established WikiText - 103 benchmark ( Merity et al , 2016 ) with a subgraph from the Freebase knowledge graph ( Bollacker et al , 2008 ) . This makes it easy to benchmark against other state - of - the - art text generative models that are capable of generating long paragraphs of coherent text . Both the graphs and the text data are of significantly larger scale compared to prior graph - text paired datasets . We present baseline graph neural network and transformer model results on our dataset for 3 tasks : graph text generation , graph text retrieval and text graph retrieval . We show that better conditioning on the graph provides gains in generation and retrieval quality but there is still large room for improvement . 1", "entities": [[20, 23, "TaskName", "conditional text generation"], [24, 26, "TaskName", "graph generation"], [27, 30, "TaskName", "graph representation learning"], [69, 70, "DatasetName", "WikiGraphs"], [175, 177, "TaskName", "text generation"]]}
{"text": "Parallel datasets that pair data from different sources and modalities have enabled large amounts of research on cross modality learning . Paired image - caption datasets enable models to describe visual scenes in natural language ( Lin et al , 2014 ; , paired streams of speech and transcription data makes it possible to train speech recognition systems ( Garofolo et al , 1993 ; Panayotov et al , 2015 ) or text - to - speech synthesis models ( Oord et al , 2016 ) , and parallel corpus of text in different languages enable learned machine translation models ( Barrault et al , 2020 ) . We present a new dataset of Wikipedia text articles each paired with a relevant knowledge graph ( KG ) , which enables building models that can generate long text conditioned on a graph structured overview of relevant topics , and also models that extract or generate graphs from a text description . There has been many prior efforts trying to build datasets for learning graph text generation models ( Jin et al , 2020 ; Gardent et al , 2017 ; Lebret et al , 2016 ) . However , existing graph - text paired datasets are mostly small scale , where the graphs tend to have 10 - 20 or even less nodes , and the text typically only contains one or a few sentences . This represents a significant contrast with the state - ofthe - art text generation models ( Dai et al , 2019 ; Brown et al , 2020 ) , which can already generate very fluent and long text that spans thousands of tokens over multiple paragraphs . We attempt to bridge this gap , with the goal of advancing the state - of - the - art graph text generation models , graph representation learning models and also text - conditioned graph generative models . Each text document in our dataset is a full - length Wikipedia article , and we pair each of them with a KG that are significantly bigger than prior datasets of similar nature and includes much richer information . Hand labelling text articles with KGs is expensive and not scalable ( Lebret et al , 2016 ) , therefore we utilize an existing and established knowledge base , Freebase ( Bollacker et al , 2008 ) , and designed an automated process to extract a relevant subgraph from it for each Wikipedia article . To make the text generation results on our dataset directly comparable to the state - of - the - art , we chose the set of Wikipedia articles from the established language modeling benchmark WikiText - 103 ( Merity et al , 2016 ) , which contains a subset of high - quality Wikipedia articles . This gives us a dataset of 23 , 522 graph - text pairs in total , covering 82.3 % of Wikitext - 103 articles . On average each graph has 38.7 nodes and 48.3 edges , and each text article contains 3 , 533.8 tokens . In addition to structural information , our graphs also contain rich text information with an average of 895.1 tokens in each graph . Furthermore , the automatic process we used to create this dataset can be extended to pair any Wikipedia document with Freebase , and can be scaled up to create over 3 M graph - text pairs . Out of many exciting new tasks that this dataset enables , we present 3 possibilities : graph text generation , graph text retrieval , and text graph retrieval . We benchmarked a few baseline models on these tasks . The models we considered were based on the recent Transformer - XL ( Dai et al , 2019 ) model , and we adapted it to condition the text generation on the KG in different ways . Our results show that better conditioning on the graph indeed improves the relevance of the generated text and the retrieval quality . However , there is still significant room for improvement on these tasks , which makes this an exciting dataset for research . Our data and code for baseline models will be made publicly available .", "entities": [[55, 57, "TaskName", "speech recognition"], [72, 78, "TaskName", "text - to - speech synthesis"], [97, 99, "TaskName", "machine translation"], [173, 175, "TaskName", "text generation"], [247, 249, "TaskName", "text generation"], [303, 305, "TaskName", "text generation"], [307, 310, "TaskName", "graph representation learning"], [417, 419, "TaskName", "text generation"], [593, 595, "TaskName", "text generation"], [624, 627, "MethodName", "Transformer - XL"], [643, 645, "TaskName", "text generation"]]}
{"text": "Graph - text paired data There has been a lot of prior work on creating graph - text paired datasets . Example applications include generating text summaries conditioned on Abstract Meaning Representation graphs ( Liu et al , 2018 ) , generating the abstract of a scientific article given a KG and title ( Koncel - Kedziorski et al , 2019 ) and generating text from RDF triples ( Gardent et al , 2017 ; Jin et al , 2020 ) . In the following we will mostly review related work on KG - text paired datasets . Annotating KG or text to create paired datasets is expensive , as a good quality annotation requires annotators that understand the content and structure of the text and the corresponding KG ( Jin et al , Dataset 2020 ) . Therefore previous KG - text paired datasets that rely on human annotation have limited scale . Among these , Gardent et al ( 2017 ) crowdsourced human annotators to verbalize RDF triplets taken from DBpedia ( Auer et al , 2007 ) to a few sentences ( WebNLG ) and this caused errors in annotation that were fixed with a few updates through years . Parikh et al ( 2020 ) paired Wikipedia Table with one sentence text that is created by annotators that revise Wikipedia text . Another line of research focuses on eliminating the need of human annotations by automatically matching KG - text pairs or generating KGs from text using existing tools . Lebret et al ( 2016 ) automatically matched Wikipedia infobox of biographies with their first sentence . Koncel - Kedziorski et al ( 2019 ) utilized an earlier information extraction system that extracts entities , co - reference and relations from given text to build KG 's . The Gen - Wiki dataset ( Jin et al , 2020 ) is automatically constructed by querying KGs in DBpedia with the title of articles in Wikipedia followed by filtering and entity annotation . We construct our WikiGraphs dataset by extracting a subgraph from Freebase ( Bollacker et al , 2008 ) for each Wikipedia article following a scalable automatic process . Compared to previous work , our WikiGraphs dataset contains significantly larger graphs and longer text ( Table 1 ) . Models for graph - text paired data Recent state of art language models are based on the Transformer architecture ( Vaswani et al , 2017 ) that uses the self attention mechanism . The Transformer - XL ( Dai et al , 2019 ) model further introduces a segment level recurrence with a novel positional encoding resulting in impressive performance in long sequences by capturing dependencies beyond a fixed length window . Graph neural networks ( GNNs ) ( Battaglia et al , 2018 ; Gilmer et al , 2017 ) learn representations for graph structured data through a message passing process . This class of models naturally exploit the graph structures , making them a good fit for graph data . GNNs have been used in many applications on KG 's ( Kipf and Welling , 2016 ; Xu et al , 2019 ) . Fundamentally , transformers can also be understood as a special type of GNNs with a fully - connected graph structure . The most recent prior work on graph - to - text generation follows an encoder - decoder architecture ( Koncel - Kedziorski et al , 2019 ; Jin et al , 2020 ) , where the graph part is encoded with a GNN model , e.g. Graph Attention Network ( GAT ) ( Veli\u010dkovi\u0107 et al , 2018 ) . The text part is typically modeled using an attention based decoder with a copy mechanism ( e.g. BiLSTMs as in ( Jin et al , 2020 ) ) to process input from both the KG and text . The models we benchmarked for graph - to - text generation were based on the Transformer - XL architecture and conditioned on the graph through a GNN , making full use of the graph structure and capable of generating very long text comparable to the state - of - the - art .", "entities": [[172, 173, "DatasetName", "DBpedia"], [185, 186, "DatasetName", "WebNLG"], [321, 322, "DatasetName", "DBpedia"], [339, 340, "DatasetName", "WikiGraphs"], [370, 371, "DatasetName", "WikiGraphs"], [401, 402, "MethodName", "Transformer"], [418, 421, "MethodName", "Transformer - XL"], [561, 563, "TaskName", "text generation"], [597, 600, "MethodName", "Graph Attention Network"], [601, 602, "MethodName", "GAT"], [658, 660, "TaskName", "text generation"], [664, 667, "MethodName", "Transformer - XL"]]}
{"text": "Basic statistics about our WikiGraphs dataset are listed in Table 2 . An illustration of a graph - text pair is shown in Figure 1 . A few actual examples from our dataset are included in the Appendix ( Figure 7 , 8 ) . All of the articles come from the WikiText - 103 dataset ( Merity et al , 2016 ) , which contains highquality articles that fit the Good or Featured criteria specified by the Wikipedia editors when the data was collected . Merity et al ( 2016 ) have already cleaned up and tokenized the articles , therefore they appear as plain text without any markup tags . As will be described in Section 3.2 , we try to pair each article with a subgraph from Freebase , centered at the entity node that has a Wikipedia link to the title of the article . We are not able to match every article to an entity in Freebase , but through this process we retained a significant portion of 82.3 % of the WikiText - 103 articles . We kept the original train / valid / test split . As we will see in Section 4.2 , training models on this set gives us results that are very close to training on the full WikiText - 103 dataset when evaluated on our test set . Therefore the text part of WikiGraphs appears to be sufficient to reproduce and benchmark against the state - of - the - art text generative models . Figure 2 shows the distribution of graph sizes and article lengths across our dataset . All the distributions are skewed with a long tail . Notably , average graph size in our dataset is 38.7 nodes and 48.3 edges , considerably larger than the graphs in previous datasets ( Jin et al , 2020 ; Gardent et al , 2017 ) . Also the length of the text articles averages to 3 , 533.8 tokens and can go up to 26 , 994 tokens , which is orders of magnitudes longer than the text data in previous graph - text paired datasets that typically only contains a single or few sentences ( Jin et al , 2020 ; Gardent et al , 2017 ; Lebret et al , 2016 ) .", "entities": [[4, 5, "DatasetName", "WikiGraphs"], [234, 235, "DatasetName", "WikiGraphs"]]}
{"text": "The graphs in our dataset contains two types of nodes : entities and string literals . Each entity is labeled by a unique Freebase entity ID , e.g. ns / m.0f9q9z , and each string literal contains some natural language text , that could be for example a name , date , or description of an entity . Each edge in the graphs also has an associated edge label , e.g. ns / common.topic.description , indicating which type of edge it is . There are a total of 522 different edge types in our dataset . Figure 3 shows the frequency of all the different edge types in our dataset . Every graph always has one entity node ( we call it \" center node \" ) that has a link to the paired Wikipedia article , through a special edge key / wikipedia.en , and the whole graph is a 1 - hop neighborhood of entities around the center node within the bigger Freebase KG , plus the string literals associated with all the entities included . Note that it is possible to have edges between the 1 - hop neighbors of the center node , therefore the graphs typically are not star structured . Section 3.2 provides more details about how these graphs are constructed and any additional filtering we did . One special characteristic about our graph data is that the natural language text contained in the string literal nodes can sometimes be quite long ( see e.g. Figure 7 , 8 ) , and therefore provide much richer information not included in the graph structure itself . On average , each graph contains 895.1 tokens across all the string literal nodes in one graph ( Table 2 , Figure 2 , \" Tokens per graph \" ) . Figure 4 shows the distribution of per - graph number of entity nodes and string literal nodes in our dataset . We can see that our graphs tend to have more string literal nodes than entity nodes , indicating that the entities are supplemented with the rich information in the string literals . The distribution of information is not uniform across the nodes in a graph . Figure 5 shows that most entity nodes in our graph has a small degree , while few nodes have much larger degrees . Also most string literal nodes contain short text , while fewer nodes contain longer text . The skewed distribution of nodes and edges in our dataset reflect the nature of KG 's like Freebase , and presents new challenges to graph representation learning models .", "entities": [[432, 435, "TaskName", "graph representation learning"]]}
{"text": "We perform a set of experiments to showcase how the text and graph information can be combined in a language model . Specifically , we consider three tasks : text generation conditioned on the graph , graph retrieval given the text , and text retrieval given the graph .", "entities": [[29, 31, "TaskName", "text generation"]]}
{"text": "In order to incorporate graph information into an advanced language model , we adapt the recent Transformer - XL model ( Dai et al , 2019 ) to also attend to the graph features . At a high - level our model embeds the graph into a set of embedding vectors , and then exposes these embeddings to the Transformer - XL model as extra \" token \" embeddings to condition on . The size of this set depends on the graph model we choose . Given the features for T text tokens H t R T \u00d7d and features for T graph \" tokens \" H g R T \u00d7d , we illustrate the graph - conditioned attention procedure with a single head as follows : Q t , K t , V t = H t W t q , H t W t k , H t W t v K g , V g = H g W g k , H g W g v A t , A g = Q t K t , Q t K g A , V = [ A t A g ] , [ V t V g ] O = Masked - Softmax ( A ) V where [ a b ] stands for concatenation on the sequence dimension and thus A R T \u00d7 ( T + T ) and V R ( T + T ) \u00d7d h , where d h is the head dimension . In other words , comparing to the original Transformer - XL , our model also computes the attention scores between the text queries Q t and both the text keys K t and the graph keys K g . As a result , the attention outputs contain information from both the graph and the text context . Note that this formulation is compatible with an additional memory ( Dai et al , 2019 ) with minimal changes , as it simply adds in an extra set of \" tokens \" for the model to attend to . We do n't use position encodings for the graph \" tokens \" as there is no sequential ordering for them . In this work we consider three different approaches for encoding the graph structure : Bag - of - words ( BoW ) : we construct a single bag - of - words representation of all the tokens from both the nodes and edges in the graph . Entity IDs and numeric values in the graph are replaced with special tokens < entity > and < number > . The BoW vector is further projected using a linear layer to a latent space . In this case T = 1 . Nodes only ( Nodes ) : we construct separate BoW representations for each node and project each to an embedding and ignore the edges . In this case T is equal to the number of nodes in the graph . Graph neural network ( GNN ) : we embed BoW representations for both nodes and edges and then use a graph neural network ( Battaglia et al , 2018 ) on top of those embeddings to compute a new set of node embeddings . T is equal to the number of nodes . The T graph embeddings from this process are shared across all the time steps for text tokens . This model can be further improved , e.g. by using word embeddings and text summarization techniques , but we leave these for future work .", "entities": [[16, 19, "MethodName", "Transformer - XL"], [59, 62, "MethodName", "Transformer - XL"], [206, 207, "MethodName", "Softmax"], [261, 264, "MethodName", "Transformer - XL"], [447, 449, "MethodName", "linear layer"], [582, 584, "TaskName", "word embeddings"], [585, 587, "TaskName", "text summarization"]]}
{"text": "We show a few ablations on the graph model and sampling parameters , to provide some insights into the models . Table 4 shows the effect of varying the number of message passing layers in the GNN . We can observe that there is a big difference between using message passing ( \u2265 1 layers ) or not ( 0 layers ) in terms of rBLEU score , but increasing the number of message passing layers does not change the results significantly . We believe however , that these results can be improved by employing bigger and more powerful graph representation learning models , and potentially use initial node and edge representations better than bag - of - words . In Figure 6 we show the effect of the graph size on model performance . In this experiment we subsample the nodes in each graph to control for the amount of context the model has access to . It is clear from the results that when we heavily subsample and keep only a small portion of the graphs , the GNN model performs similarly as the simpler BoW model , but GNNs benefit more as we keep more of the graph structure .", "entities": [[59, 60, "DatasetName", "0"], [99, 102, "TaskName", "graph representation learning"]]}
{"text": "In this paper , we present WikiGraphs , a new graphtext paired dataset with significantly larger graphs and longer text compared to previous datasets of similar nature . We show that the text part of this data is a good benchmark for state - of - the - art text generation models , and the paired dataset can help us benchmark models that are capable of generating long and coherent text conditioned on a graph structure . In the first set of experiments on this dataset we showcase 3 different tasks using our dataset , and demonstrate the benefit of better models that make more use of the graph structure . There is still significant room for improvement for these tasks on our dataset , and we hope the release of the data and baseline code can help spur more interest in developing models that can generate long text conditioned on graphs , and generate graphs given text , which is another exciting direction our dataset enables but we did not explore , and eventually bridging the graph and text modalities .", "entities": [[6, 7, "DatasetName", "WikiGraphs"], [49, 51, "TaskName", "text generation"]]}
{"text": "We show additional ablation results on the sample length ( Table 10 ) and the temperature ( Table 11 ) for greedy sampling . Note that for each case we show the rBLEU score based on the validation set computed with a single sampling run ( 20 samples per graph ) . Note that the GNN model has overall the best performance . However as the sample length increases the advantage of the GNN model also decreases . This indicates that it is still very challenging to generate long text that stays on - topic , and potentially the noise overwhelms the signal when number of tokens increases to 4096 . ns / type.object.id ns / freebase.object hints.best hrid \" A \\\"protected site\\ \" is any location that is protected under law , usually by being designated as a park , preserve , monument , etc . , and which are usually under the control ( at least in part ) by some form of government agency . This most often will apply to areas of land or water , but may also apply to human - made structures . This type is distinct from \\\"listed sites\\ \" , which have been designated as significant , but may not have any legal protection thereby . However , many such places will be both , ... Visualization Ground Truth Text Figure 7 = Where the Streets Have No Name = \" Where the Streets Have No Name \" is a song by Irish rock band U2 . It is the opening track from their 1987 album The Joshua Tree and was released as the album 's third single in August 1987 . The song 's hook is a repeating guitar arpeggio using a delay effect , played during the song 's introduction and again at the end . Lead vocalist Bono wrote the lyrics in response to the notion that it is possible to identify a person 's religion and income based on the street on which they lived , particularly in Belfast . During the band 's difficulties recording the song , producer Brian Eno considered erasing the song 's tapes to have them start from scratch . \" Where the Streets Have No Name \" was praised by critics and became a commercial success , peaking at number thirteen in the US , number fourteen in Canada , number ten in the Netherlands , and number four in the United Kingdom . The song has remained a staple of their live act since the song debuted in 1987 on The Joshua Tree Tour . The song was performed on a Los Angeles rooftop for the filming of its music video , which won a Grammy Award for Best Performance Music Video . = = Writing and recording = = The music for \" Where the Streets Have No Name \" originated from a demo that guitarist The Edge composed the night before the group resumed The Joshua Tree sessions . In an upstairs room at Melbeach House - his newly purchased home - The Edge used a four @ - @ track tape machine to record an arrangement of keyboards , bass , guitar , and a drum machine . Realising that the album sessions were approaching the end and that the band were short on exceptional live songs , The Edge wanted to \" conjure up the ultimate U2 live @ - @ song \" , so he imagined what he would like to hear at a future U2 show if he were a fan . After finishing the rough mix , he felt he had come up with \" the most amazing guitar part and song of [ his ] life \" . With no one in the house to share the demo with , The Edge recalls dancing around and punching the air in celebration . Although the band liked the demo , it was difficult for them to record the song . Bassist Adam Clayton said , \" At the time it sounded like a foreign language , whereas now we understand how it works \" . The arrangement , with two time signature shifts and frequent chord changes , was rehearsed many times , but the group struggled to get a performance they liked . According to co @ - @ producer Daniel Lanois , \" that was the science project song . Figure 8 = Fort Scott National Historic Site = Fort Scott National Historic Site is a historical area under the control of the United States National Park Service in Bourbon County , Kansas , United States . Named after General Winfield Scott , who achieved renown during the Mexican @ - @ American War , during the middle of the 19th century the fort served as a military base for US Army action in what was the edge of settlement in 1850 . For the next quarter century , it was used as a supply base and to provide security in turbulent areas during the opening of the West to settlement , a period which included Bleeding Kansas and the American Civil War . The current national historic site protects 20 historic structures , a parade ground , and five acres ( 20 @ , @ 000 m 2 ) of restored < unk > prairie , inside the city of Fort Scott . It is open to visitors most days of the year . = = History = = In 1842 , Fort Scott was named after Winfield Scott , was established on the American frontier on the military road in eastern Kansas between Fort Leavenworth and Fort Gibson . It was established to provide protection to the rapidly increasing number of settlers , who were migrating from the Eastern United States . Fort Scott became one of a chain of forts intended to protect the new settlers from the Plains Indians , as well as to protect the Indians from the settlers ' encroachment . The United States government intention to reserve permanent Indian lands west of the Missouri River gave way to the competition of settlers continuing to encroach on the Indian settlements . Fort Scott 's most active days were between 1842 and 1853 , although it was also used during the Civil War . = = = Army days = = = The Cherokee of Indian Territory ( now Oklahoma ) were upset to have Fort Wayne in their proximity . After some delay , the US Army decided to abandon Fort Wayne and move its soldiers to a new fort to be built between Fort Leavenworth and the site . The Army both wanted to placate the Cherokee ( who were supervised by the Department of Defense ) and provide more defense for white settlers and other Indians against the Osage , who had been conducting frequent raids in the area . On April 1 , 1842 , some soldiers of Fort Wayne left their fort and on April 22 arrived where Fort Scott would be built , in the Osage < unk > section of modern @ - @ day Kansas . After haggling with the Cherokees to acquire the land , the rest of Fort Wayne 's garrison left the fort on May 26 and arrived at the Fort Scott site on May 30 . Unlike most forts for military use , the fort did not have defensive walls or structures", "entities": [[668, 669, "MethodName", "Adam"], [778, 779, "DatasetName", "General"]]}
{"text": "Generated Text - = Final Fantasy Adventure = Final Fantasy Adventure is a platform video game developed by Square ( now Square Enix ) and published by Square ( then Square Enix ) for the Super Nintendo Entertainment System , and published by Nintendo for the Nintendo 64 and the Nintendo 64 in North America and Europe . It was released in North America on August 10 , 1995 for the Nintendo 64 and in Europe in March 1995 for the Wii . It was also re @ - @ released for the Wii in Japan on March 25 , 2015 for the Wii U 's Virtual Console service . Final Fantasy Adventure is set in the fantasy world of the fictional land of Ooo , between 1000 and 1000 years before the events of the story of the kingdom of Ooo . The story follows the protagonists , Cid , a boy who is sent to Ooo by the legendary J\u00f6rmungandr to find the Wizard of Ooo . Throughout the game , players train for a series of random battles , which are played out over a single screen . In the end , players control a \" Scarlet Witch \" who is summoned to the realm of Ooo . The game was well @ - @ received , with praise particularly directed at its graphics and its gameplay , despite criticism for its repetitive gameplay . Critics generally praised its graphics , plot , and simplistic gameplay . The game was commercially successful , selling over 20 @ , @ 000 copies , and in 2004 it was re @ - @ released in Japan as part of the Final Fantasy Collections series . = = Gameplay = = The game is a third @ - @ person shooter , with gameplay elements related to worlds , time travel , and exploration . Characters are made up of two polygons ; the main character , Aya , is a sprite @ - @ based character and inhabits a world of trees and caves , where she is able to perform offensive actions along the way . The protagonist , Aya , is a heavily armored , humanoid creature with red eyes and black eyes . In contrast to other Final Fantasy games , her prominent appearances in this game are not as straightforward as in previous Final Fantasy installments . The game 's world is divided into two sections : the world of Ooo and the world of the dinosaurs . The player 's adventure begins with Aya , a girl who initially leads the plot of Twilight Town , but later returns to Ooo to save her sister . Aya explores the area and is able to save her family . After interacting with the player , Aya is able to resume the story in the future . The game 's main family member , Aya , is a woman named Alice . She is a member of the religious order , the Jehovah 's Witnesses , who is a member of BoW = Zoo Station ( song ) = \" Zoo Station \" is a song by Australian rock band U2 from their second studio album , PopMart , released in 1992 as the album 's lead single . Rise to popularity after its release prompted U2 to record two songs with bassist Adam Clayton and guitarist Joe Bono , as they felt that it was their breakthrough album . The song comprises lyrics set in an acoustic , with lyrics that have always focused on a woman being stuck in an abusive relationship . It was developed due to the band 's involvement in the construction of Zoo Station and its accompanying soundtrack , and it was inspired by U2 's admiration of the groups they both represented . The song was inspired by a series of incidents that occurred in El Salvador when a man repeatedly attacked an apartment building . The song received positive reviews from music critics , and it was labelled a \" masterpiece \" by several publications . Thirty @ - @ five tracks were released as singles , and the song has been covered by numerous artists , including such figures as Sam & Max , The < unk > , and Mickey Rourke . The band released their version of \" Zoo Station \" on non @ - @ consecutive studio albums , with \" Where the Streets Have No Name \" and \" Numb \" released in 1999 and 2000 respectively . \" Zoo Station \" is on the band 's greatest hits album , 10 Mile Wild , and on their greatest hits album Across the Universe . It was performed live at the Vertigo Tour in 2007 . The song was included on the band 's fifth studio album , New Order ( 2008 ) , and was included on the film The Last Years ( 2012 ) . = = Background = = \" Zoo Station \" was written by U2 frontman Bono and produced by U2 and was the first song on the album . Clayton still had a strong fan base and was a regular singer . The two met while performing in the amphitheaters of Los Angeles , and in a live performance he was the lead singer on the last leg of the Vertigo Tour . The Edge and Clayton were both fans of the band , and the pair decided to collaborate on the album . Both performed on their 2004 tour , and made a solo appearance on the 2004 The Zoo TV Tour . Clayton and Clayton had been close friends , and the pair became friends again in 2008 . In late April 2004 , U2 announced that the song had been released as the first single for the album , and would be released on 31 May , five weeks after the album 's release . \" Zoo Station \" was released as the fourth single from PopMart and premiered on 13 June 2005 . The song is a Nodes = You Can ' t Take This Away ( U2 song ) = \" You Can ' t Take This Away ( U2 song ) \" is a song by the Irish rock band U2 . It was written by vocalist Bono and produced by The Smiths for their third solo album , The Joshua Tree . Inspired by Romania roots rock and roll , \" You Can ' t Take This Away \" is a song about a man who suffers from severe nightmares . The song was first released on the album and premiered on radio on 19 August 1996 , where it reached number 40 on the Billboard Hot 100 . \" You Can ' t Take This Away ( U2 song ) \" received mixed reviews from critics ; some tracks , such as the opening chorus and bridge , were praised as highlights by some reviewers . The song was well received by critics , as the record company 's highest @ - @ selling single at that time , where \" The Edge of Forever \" peaked at number 64 . It was later re @ - @ released on the band 's 2006 compilation album No Line on the Horizon , but has since been re @ - @ released on live performances in 2006 and 2009 . \" You Can ' t Take This Away ( U2 song ) \" was performed on the Late Show with David Letterman on 31 December 2005 . \" You Can ' t Take This Away ( U2 song ) \" has since been covered by many groups and has achieved enormous commercial success . A video for the song was filmed by then @ - @ frontman Bono , for which it was nominated for a Grammy Award . = = Background and writing = = \" You Can ' t Take This Away ( Kingdom of Ireland song ) \" is a track that features Bono and The Smiths discussing their relationship and how they changed their lives . His father , Jack Clayton , was assassinated in 1981 at the age of 23 . Bono was born in Philadelphia and worked for a business , first working as a secretary in Los Angeles , then as a photographer for a commercial for Primus . He later worked for the Coca @ - @ Cola Company as a drummer in the music industry . The musician picked up the song again after Nikolai < unk > , an engineer who worked with The Smiths , heard it and decided to play it for him after U2 agreed to record it for a solo album . The band originally intended to release \" You Can ' t Take This Away \" as a single ; however , with a critical failure , the song eventually became a single . In 2004 , \" You Can ' t Take This Away ( U2 song ) \" is one of two songs on the album that U2 released as a single with approval of the record label . The [ ] GNN = The Edge of Glory = \" The Edge of Glory \" is a song by Irish rock band U2 , released as a single . It was written by guitarist Larry Mullen , Jr . , who plays lead guitar on the song , and was produced by Alex < unk > , who described the song as \" a track with a lot of meaning , but no connection . \" The song contains several pop rock elements and is set in that time period , and is among the most prominent in the album . In addition to its lyrics , the song 's lyrics detail hypocrisy , and also deals with the effects of adultery . The song 's lyrics have been described by music critics as being autobiographical . The lyrics have been described as \" a bold exploration of the figure of a New York City man \" , and \" an expression of the inability of freedom to live in a world that is also a place in the world of space . \" The song 's lyrics describe a \" Manhattan @ - @ like place \" , with Bono calling the arrival a \" pleasant little optimism from before it came to life . \" \" The Edge of Glory \" was a success in the United Kingdom , reaching number two in the charts in the United States , and topping the charts in Australia and New Zealand . The song has been certified platinum by the Recording Industry Association of America , and has sold over four million copies worldwide . The song has been covered by several artists , including German band U2 . The music video for \" The Edge of Glory \" won Best Video at the 2004 MTV Video Music Awards . The video also served as an inspiration for the film U2 360 ( 1998 ) . = = Background = = The song has been described as a \" relaxed representation \" of globalization , with Bono proclaiming himself the \" lost king of rock ' n ' roll \" , and Chris McGuinness as \" the only one who has ever achieved the sound of a rock ' n ' roll . \" Bono 's lyrics have been described as a parody of Lord Byron 's \" My Own Time \" , and as an \" attack on social and political issues \" . = = Recording and production = = Bono and U2 made the final stages of recording the song at the Abbey Road Studios in Dublin , Dublin . The sessions were divided into two sessions : Sessions at Damien the flautist and Context at the Cave of Christ . The results of the sessions were mixed by Brian Eno . U2 was very excited to record the result , with Eno referring to the recording as a \" special event \" , and they decided to change the track", "entities": [[567, 568, "MethodName", "Adam"], [1082, 1083, "DatasetName", "Inspired"]]}
{"text": "Generated Text - = Carbuncle = Bloodlines is a 2011 action role @ - @ playing video game developed by < unk > and published by Activision for the Wii . The game is part of a sequel of the 2013 game Eclipse : The Series . It was published by Activision on November 19 , 2011 , for Microsoft Windows , Apple II , Xbox 360 , iOS , and Xbox 360 . The game , set in the fictional Middle Ages , revolves around the player @ - @ character centaurs battling for control of their universe . The protagonist of the game , Percy , the leader of the Knights of the Old Republic , is a member of the Knights of the Old Republic , and is appointed to lead a military coup to overthrow the Irish Republic and destroy the Home Nations ' military forces . Though the game mainly took place in a new version of the New York City , the original plan was to make it more easily accessible to players unfamiliar with the New Republic . It was also a commercial success , selling more than 900 @ , @ 000 copies . The game received mostly positive reviews from most video game publications , with many praising the visual style and the gameplay , but many said that it was not as good as that of the previous game . Reviewers noted the game 's title forward addressing issues such as the difficulty level , a general danger of being too difficult to fight , and the difficulty of playing the game as the player @ - @ character 's pattern of character . = = Gameplay = = Bloodlines is a crossover action role @ - @ playing game that takes place in the fictional Middle Ages , which is composed of medieval countries and locales . Valhalla , a medieval stronghold , is the game 's main setting . The player @ - @ character is a 3 @ - @ D miniature character with a sword and shield , which have multiple colored attacks , and has two of the four abilities , which are progressively reduced from the first one and allow for greater size and movement . The available weapons are bolt @ - @ fired weapons , advanced weapons , and weapons that can be used in battle . The player is able to summon magical powers to attack targets , and can use magical powers to enhance the character 's abilities . < unk > are also available via a < unk > system , which enables players to throw stones at enemies and attack enemy characters who have not encountered them . The player character also has an ability to revive foes by performing a touch @ - @ screen action . The game can be played as a side @ - @ scrolling through a View Mode , which can be used in the single @ - @ player mode . The first act features a \" < unk > \" displayed from a first @ - @ person perspective . The player character can move around BoW = Civil War Pass = Civil War Pass , also known as the Battle of the Crater or the Battle of Fort Sumner , was an important battle fought on September 7 , 1864 , at Fort Coldwater , in the state of Montana . After seeing repeated attacks on the fort , Gen. James A. Douglas , the commander of the Confederate forces in the South , decided to abandon the fort and flee to the north . After Union forces struck the fort , they decided to flee south to the Ohio River . There they quickly encountered a group of horses , who were used to build a pontoon bridge . The ditches and wooden planks were removed and replaced with stone blocks to make them float ( plow ) . The obstacles that were created in the river valley , however , proved treacherous and were not bridged by mountain passes . The young general and his troops eventually reached the Ohio and the Mississippi rivers , but the new Presidential candidate , Abraham Lincoln , resigned after the war . After the defeat at Fort Sumner , General Douglas , the commander of the Union forces , planned and executed a number of attacks on Fort Sumner . When soldiers arrived , they found two now @ - @ deserted locations . The attacks had been made more than a year before . When the line of retreat of the Union forces , which stretched from Fort Sumner to Fort Sumner , reached Fort Sumner on August 19 , 1864 , the cavalrymen captured it on September 30 . In November 1864 , General Douglas was defeated at the Battle of Lake Logan . = = Background = = In 1861 , with the Mexican @ - @ American War nearing its conclusion , the American public began to think of an armistice treaty , or peace treaty between Mexico and the United States . On July 1 , 1861 , General Douglas sent three large armies from the Mexican @ - @ American War , a series of forts west of the Rockies , to attack Fort Vicksburg . The forts were destroyed in a siege in June . These were built during the years it was fought by the Confederate States of America . The British and Americans were unprepared for the chance of victory , and the Americans were now planning to take control of the Gulf Coast . Like the Americans , the British were planning an attack into central Canada . The British were aware that the main invasion of Canada would occur on July 8 . The British were near the Niagara River and the Union were hopefully midway along the river , approaching Fort Sumner from the west . The British were reluctant to move toward the Carolinas , and so , in the event the Port of Boston was abandoned , the British would be forced to travel to the lower Mississippi . The Nodes = Fort Scott = Fort Scott is an American military post located in Fort Lee , Kansas . It is named in honor of General William Scott , a U.S. Army general and the first commander of the Army of the Potomac . The site was designated as a National Historic Landmark in 1991 , and has been designated a National Historic Landmark under the title of Fort Scott Historical Site since 1929 . It is located in the Rocky Mountains in Kansas and is known as the \" James Scott National Historic Site \" . = = History = = The original having been settled by the Caddo on the Black River , and later moved to Fort Lee in present @ - @ day Decatur County , Virginia . On July 10 , 1810 , the Hennepin reported that the Caddo had acquired the territory of Fort Lee , but it is unclear whether he was present there . He may have taken a position that had previously been occupied by other people . Around 1800 , the first Governor of Kansas , Colonel Andrew H. Sharpe , established Fort Scott in what is now a part of Fort Lee . The fort was constructed on a site that he had named Fort Scott , and was known as Fort Douglas . The fort was used for administrative purposes and for administration of the Missouri Territory . In 1808 , William Bolivar Buckner led a large movement to remove the western boundary of Texas , including Fort Scott . Congress authorized a survey of the territory in 1817 , and a survey of the Old South boundary was completed in 1818 , making Fort Scott the first governor to apply federal law . Although the West Texas Aftermath quickly became a national concern , the new governor was unable to raise sufficient funds to maintain Fort Scott . The fort 's construction and construction were completed in 1821 , but the state legislature refused to grant the commissioners the land they were granted . The new land , called Fort Dix , was consequently purchased by the U.S. Army . The fort 's name was later changed to Fort Lee . While the two states were in dispute by the 1832 treaty , Fort Dix was located in the Horn of the Midget Valley , part of the Pan @ - @ American Native Reservation . Confederate forces launched a cavalry attack on Fort Dix in early 1835 , but both sides suffered defeats to the Union . Fort Dix was declared a U.S. Army national monument by President Andrew H. Sharpe in September 1836 . Fort Dix was named after General John Scott , a U.S. Army general and the first governor of Texas , who was killed in an assassination attempt on June 20 , 1855 . Military historian John P. Pickett wrote that it was the first military governor in the United States to serve in the Confederate States GNN = Fort Scott National Historical Park = Fort Scott National Historical Park is an Illinois state historic park in the U.S. state of Kentucky . It is located at 53 25 4 N 65 41 16 W , at the edge of the Clay Creek valley , southwest of New Orleans . It is located at the intersection of Washington Boulevard and State Route 63 , and is the largest National Historic Landmark in the state . The site was purchased by Native Americans in 1803 and the site was added to the National Register of Historic Places in 1962 . Since 1998 , the site has been subject to an extensive series of historic markers and features that are important in preservation of American historic sites in Texas . The National Park Service includes the nation 's oldest extant log cabins , historic buildings , historic facilities , and historic structures . The park is home to the Mississippi River National Historical Park , a U.S. National Monument that supplies historic sites and historic sites . The original fort was built in 1818 to protect U.S. statehood . In 1899 , the state legislature constructed a small blockhouse at the site of the original fort to defend it from Native Americans . The blockhouse first appeared in 1868 , when land in the city of Lisbon was granted to the state . The fort has remained in use since then . = = History = = = = = Early history = = = Fort Scott was established as a civil and military fortification in 1803 and named after an American Indian . The land that would become Fort Scott was originally part of the Louisiana Purchase , which was granted to the United States by the Louisiana Purchase Act of 1825 . The original fort was established in 1828 by an act of Congress . The American Revolutionary War came to an end in 1830 , but Independence was declared in 1831 and Independence was declared on June 3 , 1830 . The post @ - @ war Treaty of Paris signed at Fort Scott ended military activity in the region . War by the United States reached an end in 1830 , and most of the land was put aside for use as a military park . Fort Scott was garrisoned by 90 soldiers from the 55th Louisiana Regiment during the War of 1812 . In 1837 , the Illinois General Assembly passed legislation creating Fort Scott as a federal park , and in the same year the state agreed to purchase the site in honor of the site 's new state of Louisiana . Originally , only about half of Fort Scott was owned , but the size of the park changed in the 1880s from a forest reserve to a dirt road . The park was significantly expanded during the 1910s , but the exact date is disputed . The", "entities": [[146, 147, "DatasetName", "Nations"], [729, 730, "DatasetName", "General"], [815, 816, "DatasetName", "General"], [873, 874, "DatasetName", "General"], [1069, 1070, "DatasetName", "General"], [1300, 1301, "DatasetName", "Texas"], [1343, 1344, "DatasetName", "Texas"], [1498, 1499, "DatasetName", "General"], [1511, 1512, "DatasetName", "Texas"], [1647, 1648, "DatasetName", "Places"], [1678, 1679, "DatasetName", "Texas"], [1963, 1964, "DatasetName", "General"]]}
{"text": "UMDuluth - CS8761 at SemEval - 2018 Task 9 : Hypernym Discovery using Hearst Patterns , Co - occurrence frequencies and Word Embeddings", "entities": [[10, 12, "TaskName", "Hypernym Discovery"], [21, 23, "TaskName", "Word Embeddings"]]}
{"text": "Hypernym Discovery is the task of identifying potential hypernyms for a given term . A hypernym is a more generalized word that is super - ordinate to more specific words . This paper explores several approaches that rely on co - occurrence frequencies of word pairs , Hearst Patterns based on regular expressions , and word embeddings created from the UMBC corpus . Our system Babbage participated in Subtask 1A for English and placed 6th of 19 systems when identifying concept hypernyms , and 12th of 18 systems for entity hypernyms .", "entities": [[0, 2, "TaskName", "Hypernym Discovery"], [55, 57, "TaskName", "word embeddings"]]}
{"text": "Hypernym - hyponym pairs exhibit an is - a relationship where a hypernym is a generalization of a hyponym . The objective of SemEval - 2018 Task 9 ( Camacho - Collados et al , 2018 ) is to generate a ranked list of hypernyms when given an input hyponym and a vocabulary of candidate hypernyms . For example , the input hyponym lemongrass could yield the hypernyms [ grass , oil plant , herb ] , where herb would be the best candidate . This scenario is illustrated in Figure 1 , where the three leaf nodes are hyponyms and the root is a hypernym . Note that hypernym discovery is distinct from hypernym detection , where the problem is to detect if a hyponym - hypernym relationship exists between a given pair , such as lemongrass - grass . In our first module , we retrieve candidate hypernyms for an input term using a paragraphlength context - window and calculate their cooccurrence frequencies , which is later used for ranking the candidates . Our second module uses Hearst Patterns ( Hearst , 1992 ) to extract hyponym - hypernym pairs and ranks candidate hypernyms based on co - occurrence frequency of the pairs . Our final module employs wordembeddings created using word2vec ( Mikolov et al , 2013 ) . This paper continues with a more detailed discussion of each module , and then a review of our results .", "entities": [[6, 9, "DatasetName", "is - a"], [109, 111, "TaskName", "hypernym discovery"]]}
{"text": "Babbage begins by pre - processing ( 2.2.1 ) the UMBC Corpus ( 2.1 ) and extracting candidate hypernyms using four different strategies ( 2.2.2 ) . The first and second module calculates the cooccurrence frequencies between the input term and words in context using the pre - processed UMBC Corpus and the Hearst Pattern set extracted from the UMBC Corpus . The third module uses the IS - A Hearst Pattern set extracted from UMBC Corpus to obtain hypernyms . The final module constructs a word embedding over the UMBC corpus and uses a distance measure to fetch candidate hypernyms for a given input term .", "entities": [[41, 44, "DatasetName", "words in context"], [67, 70, "DatasetName", "IS - A"]]}
{"text": "Our training corpus is the University of Maryland , Baltimore County ( UMBC ) WebBase Corpus ( Han et al , 2013 ) . It contains 3 billion words from paragraphs obtained from more than 100 million web pages over various domains . We use the 28 GB tokenized version of UMBC corpus which is part - of - speech tagged and divided among 408 files . There is also a vocabulary file with 218 , 755 unigram , bigram and trigram hypernym terms provided by task organizers . This file defines the set of possible candidate hypernyms .", "entities": [[55, 58, "DatasetName", "part - of"]]}
{"text": "The following are the steps involved in constructing our system : ( a ) Co - occurrence frequencies from Normalized Corpus : A co - occurrence map is built for the input terms with the words in the context of the input term and the frequency of their co - occurrence using the Normalized Corpus . Words with co - occurrence frequency higher than 5 are listed as candidate hypernyms for an input term . This is considered the first module result . ( b ) Co - occurrence frequencies from Hearst Corpus : A co - occurrence map similar to the previous step is built by using the Hearst Corpus . All the words which occur at least once in context of the input term in the Hearst Patterns are listed as candidate hypernyms for this term . This is considered the second module result . ( c ) Co - occurrence frequencies from IS - A Corpus : All the words which occur at least once in the context of the input term in the IS - A Corpus are listed as candidate hypernyms for this term . If the input term is a concept and is a bigram or trigram term , then part of it is considered as a hypernym for that term . This is considered the third module result . ( d ) Applying word similarity to word embeddings : A fixed distance value called Phi is used to extract words at this distance to the input term in the UMBC Embedding . These words are listed as the candidate hypernyms for an input term . This is considered our final module result .", "entities": [[155, 158, "DatasetName", "IS - A"], [177, 180, "DatasetName", "IS - A"], [230, 232, "TaskName", "word similarity"], [233, 235, "TaskName", "word embeddings"]]}
{"text": "The task description states that our system should predict candidate hypernyms for an input word which is either a concept or an entity . Hence , the part - of - speech tag for all candidate hypernyms is noun . This restricts our search space to words with noun part - of - speech tag and bigram or trigram phrases with a noun head word . Our system focuses on concepts , so we do not have any module specific for entities . To refine the input corpus as per these specifications , the input UMBC Corpus is processed through the following modules : Normalized Corpus : The POS tagged input corpus is processed per paragraph . Each paragraph is converted to lower - case text . Then , bigram and trigram noun phrases from each paragraph are obtained using the POS tags given for each word . It is further filtered by removing punctuation marks and words with part - of - speech tags other than noun , verb , adverb or adjective . This filtered line is modified by appending it with bigram and trigram noun phrases obtained earlier . Hearst Corpus : The original input paragraph is searched for the Hearst Patterns ( shown in Figure 2 ) and all the possible matches are returned in the form of hypernym : one or more hyponyms . Figure 2 shows the extraction of Hearst Patterns , where NP represents a noun - phrase where the head word is tagged as a noun , the loved - ones such as family and friends is a match for Hearst Patterns ( from Figure 2 ) with noun phrases the loved - ones , family and friends .", "entities": [[27, 30, "DatasetName", "part - of"], [49, 52, "DatasetName", "part - of"], [159, 162, "DatasetName", "part - of"]]}
{"text": "This module uses hypernym - hyponym pairs from the IS - A Corpus 2.2.1 which are in the form hyponym : hypernym . We use the same strategy as Co - occurrence frequency from Hearst Corpus to obtain the result .", "entities": [[9, 12, "DatasetName", "IS - A"]]}
{"text": "For this task , our system is required to report the 15 most probable hypernyms for each input term . We have four modules each reporting their top 15 candidate hypernyms . By looking at the training scores of these modules , we merge the co - occurrence frequencies from IS - A corpus that have higher ranks followed by the co - occurrence frequencies from Normalized corpus and Hearst Pattern corpus . Results from word embedding module are given the lowest ranks .", "entities": [[50, 53, "DatasetName", "IS - A"]]}
{"text": "This project was carried out as a part of CS 8761 , Natural Language Processing , a graduate level class offered in Fall 2017 at the University of Minnesota , Duluth by Dr. Ted Pedersen . All authors of this paper have contributed equally and are listed in alphabetical order by first name .", "entities": [[9, 10, "DatasetName", "CS"]]}
{"text": "Large Scale Substitution - based Word Sense Induction", "entities": [[5, 8, "TaskName", "Word Sense Induction"]]}
{"text": "We present a word - sense induction method based on pre - trained masked language models ( MLMs ) , which can cheaply scale to large vocabularies and large corpora . The result is a corpus which is sense - tagged according to a corpus - derived sense inventory and where each sense is associated with indicative words . Evaluation on English Wikipedia that was sense - tagged using our method shows that both the induced senses , and the per - instance sense assignment , are of high quality even compared to WSD methods , such as Babelfy . Furthermore , by training a static word embeddings algorithm on the sense - tagged corpus , we obtain high - quality static senseful embeddings . These outperform existing senseful embeddings methods on the WiC dataset and on a new outlier detection dataset we developed . The data driven nature of the algorithm allows to induce corpora - specific senses , which may not appear in standard sense inventories , as we demonstrate using a case study on the scientific domain .", "entities": [[106, 108, "TaskName", "word embeddings"], [133, 134, "DatasetName", "WiC"], [139, 141, "TaskName", "outlier detection"]]}
{"text": "Word Sense Induction and Disambiguation Previous challenges like Jurgens and Klapaftis ( 2013 ) focused on word sense induction for small sized datasets . To the best of our knowledge we are the first to perform large - scale all - words WSI . The closest work to our method is the substitution - based method proposed in Goldberg ( 2018 , 2019 ) which is the starting point to our paper . In that paper , the authors suggested a WSI algorithm designed for a small dataset ( SemEval 2010 with a predefined set of ambiguous target words ( See ( 3 ) for more details on the algorithm ) . In our work , we change Amrami and Goldberg ( 2019 ) such that we can efficiently run sense induction on all the words in very large corpora . An alternative approach for sense tagging is based on Word Sense Disambiguation ( WSD ) . The two main WSD methods are Supervised WSD and Knowledge - based WSD . Supervised WSD suffers from the difficulty of obtaining an adequate amount of annotated data . Indeed , even SemCor , the largest manually annotated tagged corpus , consists of only 226 , 036 annotated tokens . Among different supervisied WSD methods , Zhong and Ng ( 2010 ) suggested a SVM based approach and Melamud et al ( 2016 ) ; Yuan et al ( 2016 ) suggested LSTMs paired with nearest neighbours classification . Knowledgebase WSD ( Moro et al , 2014 ; Pasini and Navigli , 2017 ) , on the other hand , avoids the reliance on large annotated word - to - sense corpus and instead maps words to senses from a closed sense inventory ( e.g. WordNet ( Miller , 1992 ) , BabelNet ( Navigli andPonzetto , 2010 ) ) . As such , the quality of knowledge - based WSD heavily depends on the availability , quality and coverage of the associated annotated resources . Sense Embeddings In 8 we exploit the sense - induced corpus to train sense embeddings . Reisinger and Mooney ( 2010 ) were the first to suggest creating multiple representations for ambiguous words . Numerous recent papers ( Chen et al , 2014 ; Rothe and Sch\u00fctze , 2015 ; Iacobacci et al , 2015 ; Pilehvar and Collier , 2016 ; Mancini et al , 2017 ; Iacobacci and Navigli , 2019 ) aim to produce similar embeddings , all of which use either WordNet or BabelNet as semantic network . Our method is similar to Iacobacci et al ( 2015 ) , with the difference being that they rely on semantic networks ( via Babelfy ( Moro et al , 2014 ) ) . In contrast and similarly to us , Pelevina et al ( 2016 ) does not rely on lexical resources such as WordNet . The authors proposed splitting pretrained embeddings ( such as word2vec ) to a number of prototype senseembeddings . Yet in our work , we directly learn the multi - prototype sense - embeddings which is only possible due to the large - scale corpus annotation . When comparing both methods in 9.1 we infer it is better to directly learn multi - prototype senseembeddings . 3 Large Scale Sense Induction", "entities": [[0, 3, "TaskName", "Word Sense Induction"], [16, 19, "TaskName", "word sense induction"], [150, 153, "TaskName", "Word Sense Disambiguation"], [221, 222, "MethodName", "SVM"]]}
{"text": "Contextualized BERT vectors contain sense information , and clustering the contextualized vectors results in sense clusters . However , storing a 1024 dimensional vector of 32bit floats for each relevant token in the English Wikipedia corpus requires over 8 TB of disk - space , making the approach cumbersome and not - scalable . However , as shown by Amrami and Goldberg ( 2019 ) , MLM based wordsubstitutes also contain the relevant semantic information , and are much cheaper to store : each word - i d in BERTLARGE 's vocabulary can be represented by 2 bytes , and storing the top - 5 substitutes for each corpus position requires less than 20 GB of storage space . 3 Figure 2 : Scalable WSI flow . Given raw text , we annotate each word with its top - k substitutes , create inverted word index , find best clusters for each distinct lemma and associate all corpus words with a matching cluster . In order to perform WSI at scale , we keep the main intuition from Amrami and Goldberg ( 2019 ) , namely to cluster sparse vectors of lemmas of the top - k MLM - derived word substitutions . This results in vast storage saving , and also in a more interpretable representations . However , for scalability , we iterate over the corpus sentences and collect the top - k substitutes for all words in the sentence at once based on a single BERT call for that sentence . This precludes us from using the dynamic - patterns component of their method , which requires separately running BERT for each word in each sentence . However , as we show in Section 5.1 we still obtain sufficiently high WSI results . The steps for performing Scalable WSI are summarized in Fig . 2 . We elaborate on each step below , using English Wikipedia as a running example . 4 Annotation : We run BERT - large - cased - wholeword - masking on English Wikipedia , inferring substitutes for all corpus positions . For positions that correspond to single - token words , 5 we consider the predicted words , filter stop - words , lemmatize the remaining words ( Honnibal et al , 2020 ) , and store the top - 5 most probable lemmas to disk . This step takes 5 hours on 20 cloud - based GPU machines ( total of 100 GPU hours ) , resulting in 1.63B tokens with their corresponding top - 5 lemmas . Inverted Word Index : We create an inverted index mapping from each single - token word to its corpus occurrences ( and their corresponding top - 5 lemmas ) . This takes 5 minutes on a 96 cores CPU machine , and 10 GB of disk . Sense Induction : For each of 16 , 081 lemmas corresponding to single - token words , we retrieve random 1000 instances , 6 and induce senses using 4 The Wikipedia corpus is based on a dump from August 2020 , with text extracted using WikiExtractor ( Attardi , 2015 ) . 5 We exclude single - character tokens , stopwords and punctuation . 6 The clustering algorithm scales super - linearly with the number of instances . To reduce computation cost for tokens that appear more than 1000 times in the dataset , we sample min ( numOccur , 1000 ) instances for each token word , and cluster given the subset of instances . We then associate each of the remaining instances to one of the clusters as explained This process requires 30 minutes on the 96 - core CPU machine , and uses 100 MB of disk space . The average number of senses per lemma is 3.13 . Each sense is associated with up to 100 representative words , which represent the highest - degree words in the sense 's community . Table 1 shows the 5 senses found for the word bass with their top - 5 representative words . See additional examples in Fig . 1 and Appendix A. Tagging : Each of the remaining wordoccurrences is associated with a sense cluster by computing the Jaccard similarity between the occurrences ' top - 5 lemmas and the cluster representatives , and choosing the cluster that maximizes this score . For example , an occurrence of the word bass with lemmas tenor , baritone , lead , opera , soprano will be associated with bass 3 . This takes 100 minutes on 96 - core machine , and 25 GB of storage .", "entities": [[1, 2, "MethodName", "BERT"], [66, 67, "DatasetName", "MLM"], [153, 154, "DatasetName", "lemma"], [197, 198, "DatasetName", "MLM"], [248, 249, "MethodName", "BERT"], [272, 273, "MethodName", "BERT"], [329, 330, "MethodName", "BERT"], [632, 633, "DatasetName", "lemma"]]}
{"text": "We replace the hierarchical clustering algorithm used by Goldberg ( 2018 , 2019 ) with a community - detection , graph - based clustering algorithm . One major benefit of the community detection algorithms is that they naturally produces a dynamic number of clusters , and provide a list of interpretable discrete representative lemmas for each cluster . We additionally found this method to be more stable . Graph - based clustering for word - sense induction typically constructs a graph from word occurrences in the final step of the algorithm . or collocations , where the goal is to identify sensespecific sub - graphs within the graph that best induce different senses Manandhar , 2008 , 2010 ) . We instead construct the graph based on word substitutes . Following Jurgens ( 2011 ) , we pose identifying sense - specific clusters as a community detection problem , where a community is defined as a group of connected nodes that are more connected to each other than to the rest of the graph .", "entities": [[31, 33, "TaskName", "community detection"], [145, 147, "TaskName", "community detection"]]}
{"text": "A benefit of a WSI approach compared to WSD methods is that it does not rely on a pre - specified sense inventory , and can be applied to any corpus for which a BERT - like model is available . Thus , in addition to the Wikipedia dataset that has been presented throughout the paper , we also automatically induce senses over a corpus of 31 million PubMed Abstracts , 8 using SciBERT ( Beltagy et al , 2019 ) . As this dataset is larger than the Wikipedia dump , the process required roughly 145 GPU hours and resulting in 14 , 225 sense - annotated lemmas , with an average number of 2.89 senses per lemma . This dataset highlights the data - driven advantages of sense - induction : the algorithm recovers many senses that are science specific and are not represented in the Wikipedia corpora . While performing a wide - scale evaluation of the scientific WSI is beyond our scope in this work , we do show a few examples to qualitatively demonstrate the kinds of induced senses we get for scientific texts . For each of the words mosaic , race and swine we show the induced clusters and the top - 5 cluster representatives for each cluster . While senses mosaic 0 ( the common mosaic virus of plants ) and mosaic 2 ( \" something resembling a mosaic \" , \" mosaic of .. \" ) are represented in Wikipedia , senses mosaic 1 ( the mosaic genetic disorder ) and mosaic 3 ( mosaic is a quality , e.g. , \" mosaic border \" , \" mosaic pattern \" ) are specific to the scientific corpora ( The Wikipedia corpora , on the other hand , includes a sense of mosaic as a decorative art - form , which is not represented in Pubmed ) . 7 Sense - aware Information Retrieval An immediate application of a high quality sensetagged corpus is sense - aware retrieval . We incorporate the sense information in the SPIKE extractive search system ( Shlain et al , 2020 ) 9 for Wikipedia and Pubmed datasets . When entering a search term , suffixing it with @ triggers sense selection allowing 9 spike.apps.allenai.org to narrow the search for the specific sense . Consider a scientist looking for PubMed occurrences of the word \" swine \" in its influenza meaning . As shown in Figure 3 , this can be easily done by writing \" swine@ \" and choosing the second item in the resulting popup window . The outputs are sentences with the word \" swine \" in the matching sense . As far as we know , SPIKE is the first system with such WSI capabilities for IR . Similarly , Blloshmi et al ( 2021 ) suggested to enhance IR with sense information , but differently from us , this is done by automatically tagging words with senses from a predefined inventory .", "entities": [[34, 35, "MethodName", "BERT"], [118, 119, "DatasetName", "lemma"], [219, 220, "DatasetName", "0"], [313, 314, "DatasetName", "Pubmed"], [320, 322, "TaskName", "Information Retrieval"], [359, 360, "DatasetName", "Pubmed"]]}
{"text": "Learning static word embeddings of senseambiguous words is a long standing research goal ( Reisinger and Mooney , 2010 ; Huang et al , 2012 ) . There are numerous real - world tasks where context is not available , precluding the use of contextualized - embeddings . These include Outlier Detection ( Camacho - Collados and Navigli , 2016 ; Blair et al , 2016 ) , Term Set Expansion ( Roark and Charniak , 2000 ) the Hypernymy task ( Breit et al , 2021 ) , etc . Additionally , static embeddings are substantially more efficient to use , can accommodate larger vocabulary sizes , and can accommodate efficient indexing and retrieval . Yet , despite their flexibility and success , common word embedding methods still represent ambiguous words as a single vector , and suffer from the inability to distinguish between different meanings of a word ( Camacho - Collados and Pilehvar , 2018 ) . Using our sense - tagged corpus we suggest a simple and effective method for deriving sense - aware static embeddings : We run an off - the - shelf embedding algorithm , 10 on the corpus where single - token words are replaced with a concatenation of the word and its induced sense ( e.g. \" I caught a bass . \" becomes \" I caught@0 a bass@2 . \" ) . This makes the embedding algorithm learn embeddings for all senses of each word out - of - the - box . 11 An integral property of the embedding algorithm is that it represents both the sense - annotated tokens and the other vocabulary items in the same embedding space - 10 We use the CBOW variant of the word2vec algorithm ( Mikolov et al , 2013 ) as implemented in Gensim ( \u0158eh\u016f\u0159ek and Sojka , 2010 ) . We derive 100 - dimensional embeddings using the negative - sampling algorithm and a window size of 5 . 11 A similar approach was used by Iacobacci et al ( 2015 ) over a corpus which was labeled with BabelNet and WordNet senses . Figure 3 : User interaction in SPIKE when looking for the word \" swine \" in its \" swine flu \" sense . ( Unlike the animal / experimental pig senses ) this helps sense inferring about words that are represented in the MLM as multi - tokens words ( Even though these correspond to less - frequent and often less ambiguous words ( Hern\u00e1ndez - Fern\u00e1ndez et al , 2016 ; Fenk - Oczlon et al , 2010 ; Zipf , 1945 ) ) . For example , in the top - 5 nearest neighbours for the different bass senses as shown below , smallmouth and pumpkinseed , multi - token words in BERTLARGE 's vocabulary , are close neighbours the bass instances that correspond to the fish sense . Note that some neighbours are sense annotated ( single - token words that were tagged by our system ) , while others are not ( multi - token words ) . For English Wikipedia , we obtain a total vocabulary of 1.4 M forms , 90 , 023 of which are senseannotated . Compared to the community - based representative words , the top neighbours in the embedding space tend to capture members of the same semantic class rather than direct potential replacements . 9 Sense - aware Embeddings Evaluation", "entities": [[2, 4, "TaskName", "word embeddings"], [50, 52, "TaskName", "Outlier Detection"], [398, 399, "DatasetName", "MLM"], [574, 576, "TaskName", "Embeddings Evaluation"]]}
{"text": "Pilehvar and Camacho - Collados ( 2019 ) introduced the WiC dataset for the task of classifying word meaning in context . Each instance in WiC has a target word and two contexts in which it appears . The goal is to classify whether the word in the different contexts share the same meaning . e.g. given two contexts : There 's a lot of trash on the bed of the river and I keep a glass of water next to my bed when I sleep , our method should return False as the sense of the target word bed is different .", "entities": [[10, 11, "DatasetName", "WiC"], [25, 26, "DatasetName", "WiC"]]}
{"text": "We show that substitution - based word - sense induction algorithms based on word - substitutions derived from MLMs are easily scalable to large corpora and vocabulary sizes , allowing to efficiently obtain high - quality sense annotated corpora . We demonstrate the utility of such large - scale sense annotation , both in the context of a scientific search application , and for deriving high - quality senseaware static word embeddings . As a secondary contribution , we also develop a new variant of the Outlier Detection evaluation task , which explicitly targets ambiguous words .", "entities": [[70, 72, "TaskName", "word embeddings"], [86, 88, "TaskName", "Outlier Detection"]]}
{"text": "This project has received funding from the European Research Council ( ERC ) under the European Union 's Horizon 2020 research and innovation programme , grant agreement No . 802774 ( iEX - TRACT ) .", "entities": [[33, 34, "DatasetName", "TRACT"]]}
{"text": "When using a single - prototype vector - space models , Camacho - Collados and Navigli ( 2016 ) proposed a procedure for detecting outliers based on semantic similarity using compactness score : c ( w ) = 1 n 2 \u2212 n w i W \\ { w } w j W \\ { w } w i = w j sim ( w i , w j ) Where D is the entire dataset and W is defined as { w 1 , w 2 , , w n , w n+1 } where w.l.o.g . { w 1 , w 2 , , w n } are the group elements ( including the distractor ) and w n+1 is the outlier . We use the same procedure with an additional nuance , we expanded the procedure to receive more than a single vector representation per word such that it will fit multi - prototype embeddings ( e.g. ( e.g. word2vec ) . When given as set of words ( like W \\ { w } when calculating c ( w ) ) we first find the relevant sense for each element before inferring the outlier . Camacho - Collados and Navigli ( 2016 ) suggested calculating c ( w ) using the pseudo inverted compactness score .", "entities": [[27, 29, "TaskName", "semantic similarity"]]}
{"text": "In - Batch Negatives for Knowledge Distillation with Tightly - Coupled Teachers for Dense Retrieval", "entities": [[5, 7, "MethodName", "Knowledge Distillation"]]}
{"text": "We present an efficient training approach to text retrieval with dense representations that applies knowledge distillation using the Col - BERT late - interaction ranking model . Specifically , we propose to transfer the knowledge from a bi - encoder teacher to a student by distilling knowledge from ColBERT 's expressive MaxSim operator into a simple dot product . The advantage of the bi - encoder teacherstudent setup is that we can efficiently add inbatch negatives during knowledge distillation , enabling richer interactions between teacher and student models . In addition , using Col - BERT as the teacher reduces training cost compared to a full cross - encoder . Experiments on the MS MARCO passage and document ranking tasks and data from the TREC 2019 Deep Learning Track demonstrate that our approach helps models learn robust representations for dense retrieval effectively and efficiently . * Contributed equally . The standard reranker architecture , while effective , exhibits high query latency , on the order of seconds per query ( Hofst\u00e4tter and Hanbury , 2019 ; Khattab and Zaharia , 2020 ) because expensive neural inference must be applied at query time on query - passage pairs . This design is known as a cross - encoder ( Humeau et al , 2020 ) , which exploits query - passage attention interactions across all transformer layers . As an alternative , a biencoder design provides an approach to ranking with dense representations that is far more efficient than cross - encoders (", "entities": [[14, 16, "MethodName", "knowledge distillation"], [20, 21, "MethodName", "BERT"], [77, 79, "MethodName", "knowledge distillation"], [95, 96, "MethodName", "BERT"], [113, 115, "DatasetName", "MS MARCO"], [117, 119, "TaskName", "document ranking"], [124, 125, "DatasetName", "TREC"]]}
{"text": "d \u2212 q0 q 0 d + q0 q 0 q q 2 q d \u2212 q 1 d + q q 2 d \u2212 q2 d + q2 q 0 q 1 q 2 d + q0 d + q 1 d + q2 d \u2212 q0 d \u2212 q 1 d \u2212 q2", "entities": [[4, 5, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [30, 31, "DatasetName", "0"]]}
{"text": "Target : Pairwise KD Target : In - batch KD Teacher Student Embeddings q 0 q q 2 d \u2212 q0 d \u2212 q 1 d \u2212 q2 d + q0 d + q 1 d + q2 d \u2212 q2 d + q2 d \u2212 q 1 d + q 1 d \u2212 q0 d + q0 d \u2212 q2 d + q2 d \u2212 q 1 d + q d \u2212 q0 d + q0 d \u2212 q2 d + q2 d \u2212 q 1 d + q 1 d \u2212 q0 d + q0 q0 q1 q2 d \u2212 q0 d \u2212 q1 d \u2212 q2 d + q0 d + q1 d + q2", "entities": [[14, 15, "DatasetName", "0"]]}
{"text": "Figure 1 : Illustration of the differences between pairwise knowledge distillation and our proposed in - batch knowledge distillation . computed due to the computational costs of crossencoders ( Hofst\u00e4tter et al , 2020 ; Gao et al , 2020a ; Barkan et al , 2020 ) . The contribution of this work is a simple technique for efficiently adding in - batch negative samples during knowledge distillation when training a single - vector bi - encoder . For the remainder of this paper , we refer to this technique as \" in - batch KD \" for convenience . We empirically show that our model , even trained with BM25 negatives , can be more effective than cross - encoder teachers . With hard negatives , our method approaches the state of the art in dense retrieval . Our in - batch KD technique is able to incorporate hard negatives in a computationally efficient manner , without requiring large amounts of GPU memory for large batch sizes or expensive periodic index refreshes .", "entities": [[9, 11, "MethodName", "knowledge distillation"], [17, 19, "MethodName", "knowledge distillation"], [66, 68, "MethodName", "knowledge distillation"]]}
{"text": "We focus on improving the training efficiency and retrieval effectiveness of dense retrieval and begin by formalizing it as a dense representation learning problem . To be more specific , we propose to use knowledge distillation to enrich training signals and stabilize the representation learning procedure of bi - encoder models in the context of the well - known Noise - Contrastive Estimation ( NCE ) framework .", "entities": [[21, 23, "TaskName", "representation learning"], [34, 36, "MethodName", "knowledge distillation"], [43, 45, "TaskName", "representation learning"]]}
{"text": "A cross - encoder has been shown to be an effective teacher ( Hofst\u00e4tter et al , 2020 ; Gao et al , 2020a ) since it allows rich interactions between the intermediate transformer representations of a query q and a passage p. For example , a \" vanilla \" crossencoder design using BERT can be denoted as : \u03c6\u03b8 ; Cat W f ( h q p ) , ( 5 ) where the ranking score is first computed by the hidden representation of the concatenation q p from BERT ( along with the standard special tokens ) and then mapped to a scalar by a pooling operation f and a mapping matrix W . Although effective , due to BERT 's quadratic complexity with respect to input sequence length , this design makes exhaustive combinations between a query and possible candidates impractical , since this requires evaluating cross - encoders | B | 2 times to compute Eq . ( 3 ) using Eq . ( 5 ) . Thus , an alternative is to conduct pairwise KD by computing the KL divergence of only two probabilities of a positive pair ( q , p ) and a negative pair ( q , p ) for each query q. However , this might not yield a good approximation of Eq . ( 2 ) . A bi - encoder can also be leveraged as a teacher model , which has the advantage that it is more feasible to perform exhaustive comparisons between queries and passages since they are passed through the encoder independently . Among biencoder designs , ColBERT is a representative model that uses late interactions of multiple vectors ( { h 1 q , . . . , h i q } , { h 1 p , . . . , h j p } ) to improve the robustness of dense retrieval , as compared to inner products of pairs of single vectors ( h q , h p ) . Specifically , Khattab and Zaharia ( 2020 ) propose the following fine - grained scoring function : \u03c6\u03b8 ; MaxSim i | hq | max j | hp | h i q h j p , ( 6 ) where i and j are the indices of token representations of a query q and a passage p of Col - BERT ( Khattab and Zaharia , 2020 ) . The contribution of our work is in - batch knowledge distillation with a tightly - coupled teacher . The computation of \u03c6\u03b8 ; MaxSim enables exhaustive inference over all query - passage combinations in the minibatch B with only 2 | B | computation cost , enabling enriched interactions between teacher and student . We call this design Tightly - Coupled Teacher ColBERT ( TCT - ColBERT ) . Table 1 pro TCT - ColBERT provides a flexible design for biencoders , as long as the encoders produce query and passage representations independently . For simplicity , our student model adopts shared encoder weights for both the query and the passage , just like the teacher model ColBERT . Following Khattab and Zaharia ( 2020 ) , for each query ( passage ) , we prepend the [ CLS ] token and another special [ Q ] ( [ D ] ) token in the input sequence for both our teacher and student models . The student encoder outputs single - vector dense representations ( h q , h p ) by performing average pooling over the token embeddings from the final layer .", "entities": [[53, 54, "MethodName", "BERT"], [90, 91, "MethodName", "BERT"], [121, 122, "MethodName", "BERT"], [397, 398, "MethodName", "BERT"], [415, 417, "MethodName", "knowledge distillation"], [589, 591, "MethodName", "average pooling"]]}
{"text": "Given that in - batch negative sampling is an efficient way to add more information into knowledge distillation , we wonder whether our tightly - coupled teacher design works well when applied to more sophisticated sampling methods . Following the work of , we use our pretrained bi - encoder model , namely TCT - ColBERT , to encode the corpus and sample \" hard \" negatives for each query to create new training triplets by using the negatives D \u2212 of the bi - encoder instead of BM25 . Specifically , we explore three different training strategies : 1 . HN : we train the bi - encoder using in - batch hard negatives without the guide of ColBERT . 2 . TCT HN : we train the bi - encoder with TCT - ColBERT ; 3 . TCT HN+ : we first fine - tune our ColBERT teacher with augmented training data containing hard negatives and then distill its knowledge into the bi - encoder student through TCT - ColBERT . We empirically explore the effectiveness of these strategies for both passage and document retrieval .", "entities": [[16, 18, "MethodName", "knowledge distillation"]]}
{"text": "Improving the effectiveness of single - vector biencoders is an important research direction in dense retrieval because of lower latency and storage requirements compared to multi - vector approaches . We propose a teacher - student knowledge distillation approach using tightly coupled bi - encoders that enables exhaustive use of query - passage combinations in each minibatch . More importantly , a bi - encoder teacher requires less computation than a cross - encoder teacher . Finally , our approach leads to robust learned representations . Overall , our hard negative sampling strategy leads to an effective and efficient dense retrieval technique , which can be further combined with sparse retrieval techniques in dense - sparse hybrids . Together , these designs provide a promising solution for end - to - end text retrieval that balances quality , query latency , and storage requirements .", "entities": [[36, 38, "MethodName", "knowledge distillation"]]}
{"text": "BERT : Pre - training of Deep Bidirectional Transformers for Language Understanding", "entities": [[0, 1, "MethodName", "BERT"]]}
{"text": "Learning widely applicable representations of words has been an active area of research for decades , including non - neural ( Brown et al , 1992 ; Ando and Zhang , 2005 ; Blitzer et al , 2006 ) and neural Pennington et al , 2014 ) methods . Pre - trained word embeddings are an integral part of modern NLP systems , offering significant improvements over embeddings learned from scratch ( Turian et al , 2010 ) . To pretrain word embedding vectors , left - to - right language modeling objectives have been used ( Mnih and Hinton , 2009 ) , as well as objectives to discriminate correct from incorrect words in left and right context . These approaches have been generalized to coarser granularities , such as sentence embeddings Logeswaran and Lee , 2018 ) or paragraph embeddings ( Le and Mikolov , 2014 ) . To train sentence representations , prior work has used objectives to rank candidate next sentences ( Jernite et al , 2017 ; Logeswaran and Lee , 2018 ) , left - to - right generation of next sentence words given a representation of the previous sentence , or denoising autoencoder derived objectives ( Hill et al , 2016 ) . ELMo and its predecessor ( Peters et al , 2017 ( Peters et al , , 2018a generalize traditional word embedding research along a different dimension . They extract context - sensitive features from a left - to - right and a right - to - left language model . The contextual representation of each token is the concatenation of the left - to - right and right - to - left representations . When integrating contextual word embeddings with existing task - specific architectures , ELMo advances the state of the art for several major NLP benchmarks ( Peters et al , 2018a ) including question answering ( Rajpurkar et al , 2016 ) , sentiment analysis ( Socher et al , 2013 ) , and named entity recognition ( Tjong Kim Sang and De Meulder , 2003 ) . Melamud et al ( 2016 ) proposed learning contextual representations through a task to predict a single word from both left and right context using LSTMs . Similar to ELMo , their model is feature - based and not deeply bidirectional . Fedus et al ( 2018 ) shows that the cloze task can be used to improve the robustness of text generation models .", "entities": [[52, 54, "TaskName", "word embeddings"], [131, 133, "TaskName", "sentence embeddings"], [198, 200, "MethodName", "denoising autoencoder"], [210, 211, "MethodName", "ELMo"], [287, 289, "TaskName", "word embeddings"], [296, 297, "MethodName", "ELMo"], [316, 318, "TaskName", "question answering"], [326, 328, "TaskName", "sentiment analysis"], [337, 340, "TaskName", "named entity recognition"], [380, 381, "MethodName", "ELMo"], [412, 414, "TaskName", "text generation"]]}
{"text": "As with the feature - based approaches , the first works in this direction only pre - trained word embedding parameters from unlabeled text ( Collobert and Weston , 2008 ) . More recently , sentence or document encoders which produce contextual token representations have been pre - trained from unlabeled text and fine - tuned for a supervised downstream task ( Dai and Le , 2015 ; Howard and Ruder , 2018 ; Radford et al , 2018 ) . The advantage of these approaches is that few parameters need to be learned from scratch . At least partly due to this advantage , OpenAI GPT ( Radford et al , 2018 ) achieved previously state - of - the - art results on many sentencelevel tasks from the GLUE benchmark ( Wang et al , 2018a ) . Left - to - right language model - BERT BERT E [ CLS ] E 1 E [ SEP ] ... E N E 1 ' ... E M ' C T 1 T [ SEP ] ... ... ing and auto - encoder objectives have been used for pre - training such models ( Howard and Ruder , 2018 ; Radford et al , 2018 ; Dai and Le , 2015 ) .", "entities": [[106, 107, "MethodName", "GPT"], [130, 131, "DatasetName", "GLUE"], [148, 149, "MethodName", "BERT"], [149, 150, "MethodName", "BERT"]]}
{"text": "There has also been work showing effective transfer from supervised tasks with large datasets , such as natural language inference ( Conneau et al , 2017 ) and machine translation ( McCann et al , 2017 ) . Computer vision research has also demonstrated the importance of transfer learning from large pre - trained models , where an effective recipe is to fine - tune models pre - trained with I m a - geNet ( Deng et al , 2009 ; Yosinski et al , 2014 ) .", "entities": [[17, 20, "TaskName", "natural language inference"], [28, 30, "TaskName", "machine translation"], [47, 49, "TaskName", "transfer learning"], [74, 75, "MethodName", "geNet"], [76, 79, "DatasetName", "Deng et al"]]}
{"text": "The pre - training procedure largely follows the existing literature on language model pre - training . For the pre - training corpus we use the BooksCorpus ( 800 M words ) and English Wikipedia ( 2 , 500 M words ) . For Wikipedia we extract only the text passages and ignore lists , tables , and headers . It is critical to use a document - level corpus rather than a shuffled sentence - level corpus such as the Billion Word Benchmark ( Chelba et al , 2013 ) in order to extract long contiguous sequences .", "entities": [[81, 84, "DatasetName", "Billion Word Benchmark"]]}
{"text": "Fine - tuning is straightforward since the selfattention mechanism in the Transformer allows BERT to model many downstream taskswhether they involve single text or text pairs - by swapping out the appropriate inputs and outputs . For applications involving text pairs , a common pattern is to independently encode text pairs before applying bidirectional cross attention , such as Parikh et al ( 2016 ) ; Seo et al ( 2017 ) . BERT instead uses the self - attention mechanism to unify these two stages , as encoding a concatenated text pair with self - attention effectively includes bidirectional cross attention between two sentences . For each task , we simply plug in the taskspecific inputs and outputs into BERT and finetune all the parameters end - to - end . At the input , sentence A and sentence B from pre - training are analogous to ( 1 ) sentence pairs in paraphrasing , ( 2 ) hypothesis - premise pairs in entailment , ( 3 ) question - passage pairs in question answering , and ( 4 ) a degenerate text - pair in text classification or sequence tagging . At the output , the token representations are fed into an output layer for tokenlevel tasks , such as sequence tagging or question answering , and the [ CLS ] representation is fed into an output layer for classification , such as entailment or sentiment analysis . Compared to pre - training , fine - tuning is relatively inexpensive . All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU , or a few hours on a GPU , starting from the exact same pre - trained model . 7 We describe the task - specific details in the corresponding subsections of Section 4 . More details can be found in Appendix A.5 .", "entities": [[11, 12, "MethodName", "Transformer"], [13, 14, "MethodName", "BERT"], [73, 74, "MethodName", "BERT"], [120, 121, "MethodName", "BERT"], [174, 176, "TaskName", "question answering"], [187, 189, "TaskName", "text classification"], [215, 217, "TaskName", "question answering"], [237, 239, "TaskName", "sentiment analysis"]]}
{"text": "In this section , we present BERT fine - tuning results on 11 NLP tasks .", "entities": [[6, 7, "MethodName", "BERT"]]}
{"text": "In this section , we perform ablation experiments over a number of facets of BERT in order to better understand their relative importance . Additional ablation studies can be found in Appendix C.", "entities": [[14, 15, "MethodName", "BERT"]]}
{"text": "We demonstrate the importance of the deep bidirectionality of BERT by evaluating two pretraining objectives using exactly the same pretraining data , fine - tuning scheme , and hyperparameters as BERT BASE : No NSP : A bidirectional model which is trained using the \" masked LM \" ( MLM ) but without the \" next sentence prediction \" ( NSP ) task .", "entities": [[9, 10, "MethodName", "BERT"], [30, 31, "MethodName", "BERT"], [31, 32, "MethodName", "BASE"], [49, 50, "DatasetName", "MLM"]]}
{"text": "A left - context - only model which is trained using a standard Left - to - Right ( LTR ) LM , rather than an MLM . The left - only constraint was also applied at fine - tuning , because removing it introduced a pre - train / fine - tune mismatch that degraded downstream performance . Additionally , this model was pre - trained without the NSP task . This is directly comparable to OpenAI GPT , but using our larger training dataset , our input representation , and our fine - tuning scheme . We first examine the impact brought by the NSP task . In Table 5 , we show that removing NSP hurts performance significantly on QNLI , MNLI , and SQuAD 1.1 . Next , we evaluate the impact of training bidirectional representations by comparing \" No NSP \" to \" LTR & No NSP \" . The LTR model performs worse than the MLM model on all tasks , with large drops on MRPC and SQuAD . For SQuAD it is intuitively clear that a LTR model will perform poorly at token predictions , since the token - level hidden states have no rightside context . In order to make a good faith attempt at strengthening the LTR system , we added a randomly initialized BiLSTM on top . This does significantly improve results on SQuAD , but the results are still far worse than those of the pretrained bidirectional models . The BiLSTM hurts performance on the GLUE tasks . We recognize that it would also be possible to train separate LTR and RTL models and represent each token as the concatenation of the two models , as ELMo does . However : ( a ) this is twice as expensive as a single bidirectional model ; ( b ) this is non - intuitive for tasks like QA , since the RTL model would not be able to condition the answer on the question ; ( c ) this it is strictly less powerful than a deep bidirectional model , since it can use both left and right context at every layer .", "entities": [[26, 27, "DatasetName", "MLM"], [78, 79, "MethodName", "GPT"], [122, 123, "DatasetName", "QNLI"], [124, 125, "DatasetName", "MNLI"], [127, 128, "DatasetName", "SQuAD"], [161, 162, "DatasetName", "MLM"], [171, 172, "DatasetName", "MRPC"], [173, 174, "DatasetName", "SQuAD"], [176, 177, "DatasetName", "SQuAD"], [223, 224, "MethodName", "BiLSTM"], [233, 234, "DatasetName", "SQuAD"], [251, 252, "MethodName", "BiLSTM"], [256, 257, "DatasetName", "GLUE"], [287, 288, "MethodName", "ELMo"]]}
{"text": "In Section 3.1 , we mention that BERT uses a mixed strategy for masking the target tokens when pre - training with the masked language model ( MLM ) objective . The following is an ablation study to evaluate the effect of different masking strategies . Note that the purpose of the masking strategies is to reduce the mismatch between pre - training and fine - tuning , as the [ MASK ] symbol never appears during the fine - tuning stage . We report the Dev results for both MNLI and NER . For NER , we report both fine - tuning and feature - based approaches , as we expect the mismatch will be amplified for the feature - based approach as the model will not have the chance to adjust the representations . The results are presented in Table 8 . In the table , MASK means that we replace the target token with the [ MASK ] symbol for MLM ; SAME means that we keep the target token as is ; RND means that we replace the target token with another random token . The numbers in the left part of the table represent the probabilities of the specific strategies used during MLM pre - training ( BERT uses 80 % , 10 % , 10 % ) . The right part of the paper represents the Dev set results . For the feature - based approach , we concatenate the last 4 layers of BERT as the features , which was shown to be the best approach in Section 5.3 . From the table it can be seen that fine - tuning is surprisingly robust to different masking strategies . However , as expected , using only the MASK strategy was problematic when applying the featurebased approach to NER . Interestingly , using only the RND strategy performs much worse than our strategy as well .", "entities": [[7, 8, "MethodName", "BERT"], [27, 28, "DatasetName", "MLM"], [90, 91, "DatasetName", "MNLI"], [92, 93, "TaskName", "NER"], [95, 96, "TaskName", "NER"], [163, 164, "DatasetName", "MLM"], [207, 208, "DatasetName", "MLM"], [212, 213, "MethodName", "BERT"], [250, 251, "MethodName", "BERT"], [304, 305, "TaskName", "NER"]]}
{"text": "Language Understanding \" We organize the appendix into three sections : Additional implementation details for BERT are presented in Appendix A ; Additional details for our experiments are presented in Appendix B ; and Additional ablation studies are presented in Appendix C. We present additional ablation studies for BERT including : - 10 % of the time : Replace the word with a random word , e.g. , my dog is hairy my dog is apple 10 % of the time : Keep the word unchanged , e.g. , my dog is hairy my dog is hairy . The purpose of this is to bias the representation towards the actual observed word . The advantage of this procedure is that the Transformer encoder does not know which words it will be asked to predict or which have been replaced by random words , so it is forced to keep a distributional contextual representation of every input token . Additionally , because random replacement only occurs for 1.5 % of all tokens ( i.e. , 10 % of 15 % ) , this does not seem to harm the model 's language understanding capability . In Section C.2 , we evaluate the impact this procedure . Compared to standard langauge model training , the masked LM only make predictions on 15 % of tokens in each batch , which suggests that more pre - training steps may be required for the model", "entities": [[15, 16, "MethodName", "BERT"], [48, 49, "MethodName", "BERT"], [121, 122, "MethodName", "Transformer"]]}
{"text": "Often each section of a novel is written from the perspective of a different main character . The characters each take turns in the spot - light , with their own parallel storylines being unfolded by the author . As readers , we have often desired to read just one storyline at a time , particularly when reading the book a second - time . In this paper , we present a tool , NovelPerspective , to give the consumer this choice . Our tool allows the consumer to select which characters of the book they are interested in , and to generate a new ebook file containing just the sections from that character 's point of view ( POV ) . The critical part of this system is the detection of the POV character . This is not an insurmountable task , building upon the well established field of named entity recognition . However to our knowl - edge there is no software to do this . Such a tool would have been useless , in decades past when booked were distributed only on paper . But today , the surge in popularity of ebooks has opened a new niche for consumer narrative processing . Methods are being created to extract social relationships between characters ( Elson et al , 2010 ; Wohlgenannt et al , 2016 ) ; to align scenes in movies with those from books ( Zhu et al , 2015 ) ; and to otherwise augment the literature consumption experience . Tools such as the one presented here , give the reader new freedoms in controlling how they consume their media . Having a large cast of characters , in particular POV characters , is a hallmark of the epic fantasy genre . Well known examples include : George R.R. Martin 's \" A Song of Ice and Fire \" , Robert Jordan 's \" Wheel of Time \" , Brandon Sanderson 's \" Cosmere \" universe , and Steven Erikson 's \" Malazan Book of the Fallen \" , amongst thousands of others . Generally , these books are written in limited third - person POV ; that is to say the reader has little or no more knowledge of the situation described than the main character does . We focus here on novels written in the limited third - person POV . In these stories , the main character is , for our purposes , the POV character . Limited third - person POV is written in the thirdperson , that is to say the character is referred to by name , but with the observations limited to being from the perspective of that character . This is in - contrast to the omniscient third - person POV , where events are described by an external narrator . Limited third - person POV is extremely popular in modern fiction . It preserves the advantages of first - person , in allowing the reader to observe inside the head of the character , while also allowing the flexibility to the perspective of another character ( Booth , 1961 ) . This allows for multiple concurrent storylines around different characters . Our tool helps users un - entwine such storylines , giving the option to process them sequentially . The utility of dividing a book in this way varies with the book in question . Some books will cease to make sense when the core storyline crosses over different characters . Other novels , particularly in epic fantasy genre , have parallel storylines which only rarely intersect . While we are unable to find a formal study on this , anecdotally many readers speak of : \" Skipping the chapters about the boring characters . \" \" Only reading the real main character 's sections . \" \" Reading ahead , past the side - stories , to get on with the main plot . \" Particularly if they have read the story before , and thus do not risk confusion . Such opinions are a matter of the consumer 's personal taste . The Nov - elPerspective tool gives the reader the option to customise the book in this way , according to their personal preference . We note that sub - setting the novel once does not prevent the reader from going back and reading the intervening chapters if it ceases to make sense , or from sub - setting again to get the chapters for another character whose path intersects with the storyline they are currently reading . We can personally attest for some books reading the chapters one character at a time is indeed possible , and pleasant : the first author of this paper read George R.R. Martin 's \" A Song of Ice and Fire \" series in exactly this fashion . The primary difficulty in segmenting ebooks this way is attributing each section to its POV character . That is to say detecting who is the point of view character . Very few books indicate this clearly , and the reader is expected to infer it during reading . This is easy for most humans , but automating it is a challenge . To solve this , the core of our tool is its character classification system . We investigated several options which the main text of this paper will discuss .", "entities": [[150, 153, "TaskName", "named entity recognition"]]}
{"text": "The full NovelPerspective pipeline is shown in Figure 1 . The core character classification step ( step 3 ) , is detailed in Figure 2 . In this step the raw text is first enriched with parts of speech , and named entity tags . We do not perform coreference resolution , working only with direct entity mentions . From this , features are extracted for each named entity . These feature vectors are used to score the entities for the most - likely POV character . The highest scoring character is returned by the system . The different systems presented modify the Feature Extraction and Character Scoring steps . A broadly similar idea , for detecting the focus location of news articles , was presented by ( Imani et al , 2017 ) .", "entities": [[49, 51, "TaskName", "coreference resolution"]]}
{"text": "One can see the determination of the main character as a multi - class classification problem . From the set of all named entities in the section , classify that section as to which one is the main character . Unlike typical multi - class classification problems the set of possible classes varies per section being classified . Further , even the total set of possible named characters , i.e. classes , varies from book to book . An information extraction approach is required which can handle these varying classes . As such , a machine learning model for this task can not incorporate direct knowledge of the classes ( i.e. character names ) . We reconsider the problem as a series of binary predictions . The task is to predict if a given named entity is the point of view character . For each possible character ( i.e. each named - entity that occurs ) , a feature vector is extracted ( see Section 2.2.1 ) . This feature vector is the input to a binary classifier , which determines the probability that it represents the main character . The Character Scoring step is thus the running of the binary classifier : the score is the output probability normalised over all the named entities .", "entities": [[11, 15, "TaskName", "multi - class classification"], [42, 46, "TaskName", "multi - class classification"]]}
{"text": "We investigated two feature sets as inputs for our machine learning - based solution . They correspond to different Feature Extraction steps in Figure 2 . A hand - engineered feature set , that we call the \" Classical \" feature set ; and a more modern \" Word Embedding \" feature set . Both feature sets give information about how the each named entity token was used in the text . The \" Classical \" feature set uses features that are well established in NLP related tasks . The features can be described as positional features , like in the First Mentioned baseline ; occurrence count features , like in the Most Mentioned baseline and adjacent POS counts , to give usage context . The positional features are the index ( in the token counts ) of the first and last occurrence of the named entity . The occurrence count features are simply the number of occurrences of the named entity , supplemented with its rank on that count compared to the others . The adjacent POS counts are the occurrence counts of each of the 46 POS tags on the word prior to the named entity , and on the word after . We theorised that this POS information would be informative , as it seemed reasonable that the POV character would be described as doing more things , so co - occurring with more verbs . This gives 100 base features . To allow for text length invariance we also provide each of the base features expressed as a portion of its maximum possible value ( e.g. for a given POS tag occurring before a named entity , the potion of times this tag occurred ) . This gives a total of 200 features . The \" Word Embedding \" feature set uses Fast - Text word vectors ( Bojanowski et al , 2017 ) . We use the pretrained 300 dimensional embeddings trained on English Wikipedia 1 . We concatenate the 300 dimensional word embedding for the word immediately prior to , and immediately after each occurrence of a named entity ; and take the element - wise mean of this concatenated vector over all occurrences of the entity . Such averages of word embeddings have been shown to be a useful feature in many tasks ( White et al , 2015 ; Mikolov et al , 2013 ) . This has a total of 600 features .", "entities": [[377, 379, "TaskName", "word embeddings"]]}
{"text": "The binary classifier , that predicts if a named entity is the main character , is the key part of the Character Scoring step for the machine learning systems . From each text in the training dataset we generated a training example for every named entity that occurred . All but one of these was a negative example . We then trained it as per normal for a binary classifier . The score for a character is the classifier 's predicted probability of its feature vector being for the main character . Our approach of using a binary classifier to rate each possible class , may seem similar to the one - vs - rest approach for multi - class classification . However , there is an important difference . Our system only uses a single binary classifier ; not one classifier per class , as the classes in our case vary with every item to be classified . The fundamental problem is information extraction , and the classifier is a tool for the scoring which is the correct information to report . With the classical feature set we use logistic regression , with the features being preprocessed with 0 - 1 scaling . During preliminary testing we found that many classifiers had similar high degree of success , and so chose the simplest . With the word embedding feature set we used a radial bias support vector machine , with standardisation during preprocessing , as has been commonly used with word embeddings on other tasks .", "entities": [[117, 121, "TaskName", "multi - class classification"], [190, 192, "MethodName", "logistic regression"], [199, 200, "DatasetName", "0"], [236, 239, "MethodName", "support vector machine"], [251, 253, "TaskName", "word embeddings"]]}
{"text": "We make use of three series of books selected from our own personal collections . The first four books of George R. R. Martin 's \" A Song of Ice and Fire \" series ( hereafter referred to as ASOIAF ) ; The two books of Leigh Bardugo 's \" Six of Crows \" duology ( hereafter referred to as SOC ) ; and the first 9 volumes of Robert Jordan 's \" Wheel of Time \" series ( hereafter referred to as WOT ) . In Section 4 we consider the use of each as a training and testing dataset . In the online demonstration ( Section 5 ) , we deploy models trained on the combined total of all the datasets . To use a book for the training and evaluation of our system , we require a ground truth for each section 's POV character . ASOIAF and SOC provide ground truth for the main character in the chapter names . Every chapter only uses the POV of that named character . WOT 's ground truth comes from an index created by readers . 2 We do not have any datasets with labelled sub - chapter sections , though the tool does support such works . The total counts of chapters and characters in the datasets , after preprocessing , is shown in Table 1 . Preprocessing consisted of discarding chapters for which the POV character was not identified ( e.g. prologues ) ; and of removing the character names from the chapter titles as required .", "entities": [[60, 61, "DatasetName", "SOC"], [151, 152, "DatasetName", "SOC"]]}
{"text": "We have presented a tool to allow consumers to restructure their ebooks around the characters they find most interesting . The system must discover the named entities that are present in each section of the book , and then classify each section as to which character 's point of view the section is narrated from . For named entity detection we make use of standard tools . However , the classification is non - trivial . In this design we implemented several systems . Simply selecting the most commonly named character proved successful as a baseline approach . To improve upon this , we developed several machine learning based approaches which perform very well . While none of the classifiers are perfect , they achieve high enough accuracy to be useful . A future version of our application will allow the users to submit corrections , giving us more training data . However , storing this information poses copyright issues that are yet to be resolved .", "entities": [[127, 128, "MetricName", "accuracy"]]}
{"text": "Evaluation of Scientific Elements for Text Similarity in Biomedical Publications", "entities": [[5, 7, "TaskName", "Text Similarity"]]}
{"text": "Rhetorical elements from scientific publications provide a more structured view of the document and allow algorithms to focus on particular parts of the text . We surveyed the literature for previously proposed schemes for rhetorical elements and present an overview of its current state of the art . We also searched for available tools using these schemes and applied four tools for our particular task of ranking biomedical abstracts based on text similarity . Comparison of the tools with two strong baselines shows that the predictions provided by the ArguminSci tool can support our use case of mining alternative methods for animal experiments .", "entities": [[71, 73, "TaskName", "text similarity"]]}
{"text": "We aim to mine alternative methods to animal experiments from the biomedical literature . These are methods that address any of the so - called 3R principles of replacement ( no animals at all or use of invertebrates over vertebrates ) , reduce ( use of less animals ) , or refinement ( cause less harm to animals ) ( Gruber and Hartung , 2004 ; Doke and Dhawale , 2015 ) . For such complex natural language processing ( NLP ) applications , it is necessary to rely on appropriate tools to precisely understand the text and better find the potential relevant documents . The rhetorical elements , such as zones or particular entities , can support NLP algorithms by focusing on the relevant elements of the text ( Mann and Thompson , 1987 ) . Given a certain document that describes an animal experiment for a certain research goal , hereafter called input document , we would like to find potential publications , hereafter called candidate documents , that describe an alternative method for the same research goal . Thus , some of the scientific elements should be similar between input and candidate documents , e.g. research goals and outcomes , while some others should be different , e.g. methods . Finding an alternative method to animal experiment requires two tasks : ( a ) performing a text similarity task with respect to some aspects of the publication , and ( b ) precisely understanding the proposed method with respect to the 3R principles . Therefore , the extraction of rhetorical elements has the potential to boost performance for these tasks . Previous works have proposed many schemes for rhetorical elements in scientific publication , as reviewed in Webber et al ( 2012 ) . In a more recent survey , Nasar et al ( 2018 ) present a good overview on both metadata and schemes for scientific articles . On the one hand , many of these schemes are not supported by an annotated corpus for training suitable information extraction tools . On the other hand , some tools based on these schemes are readily available for use . We surveyed published schemes for rhetorical elements , whether focused on the biomedical domain or not , and we present a short overview on these . For those schemes for which we could find available tools , the latter was used to process a collection of 562 biomedical abstracts . We performed a comparison of the output ( rhetorical elements ) from the tools in the scope of a text similarity task on a manually annotated dataset . In this work , we limited our evaluation for text similarity but did not address whether the proposed methods comply with the 3R principles . In summary , the contributions of this work are the following : ( a ) a short survey on existing schemes and corpora for rhetorical elements in scientific publications ; ( b ) the identification of the schemes for which available tools are readily available for use ; and ( c ) the evaluation of the available tools on a biomedical use case for text similarity . The next section presents a survey on the available schemes , followed by the methodology that we propose to compare the tools in the scope of text similarity . We present the results in Section 4 and our discussion in Section 5 .", "entities": [[229, 231, "TaskName", "text similarity"], [431, 433, "TaskName", "text similarity"], [449, 451, "TaskName", "text similarity"], [529, 531, "TaskName", "text similarity"], [558, 560, "TaskName", "text similarity"]]}
{"text": "Many schemes model scientific elements on the level of sentences or phrases , i.e. , for document zoning . It consists of splitting the publications ( whether abstracts or full texts ) on zones according to its scientific content , e.g. introduction , methods , results . Shimbo et al ( 2003 ) proposed five categories and used structured abstracts from Medline while Hirohata et al ( 2008 ) suggested four zoning categories . Further , Mullen et al ( 2005 ) proposed a schema in which labels are grouped in three groups . Agarwal and Yu ( 2009 ) defined four categories ( IMRAD schema ) and manually annotated 148 articles , which was also used by Varga et al ( 2012 ) for the annotation of more than 1 , 000 biomedical articles . Ruch et al ( 2007 ) also annotated and tried machine learning in biomedical abstracts . However , none of the above data seems to be available for use , but we found many schemes with available corpora : AZ ( Teufel and Moens , 2002 ) . The Argumentative Zoning ( AZ ) schema was first proposed by Teufel and Moens ( 2002 ) and an annotated corpus is freely available for download 1 . The schema is composed of seven rhetorical categories and the corresponding corpus contains 80 articles on computational linguistics . Teufel et al ( 2009 ) extended the schema to 11 categories ( the AZ - II schema ) , applied it to chemistry papers , and later compared it to the CoreSC schema ) . 2 Later , Kova\u010devi\u0107 et al ( 2012 ) annotated 110 articles in computational linguistics with a modified version of the AZ labels . Mizuta et al ( 2006 ) also adapted the AZ schema to biomedicine by annotating 20 full - text articles . CoreSC . This schema consists of three layers of labels and the corresponding ART corpus 4 is composed of 225 full texts . The corpus and schema were used in Guo et al ( 2010 ) ( just the first layer ) and in Liakata et al ( 2012a ) for two life sciences applications , while Liakata et al ( 2012b ) compared it to a schema for biomedical events and developed the the SAPIENTA software 5 . Dr. Inventor Fisas et al , 2015 ) . The Dr. Inventor Framework proposes five categories and annotated 40 Computer Graphics papers , the so - called Dr. Inventor Rhetorically Annotated Corpus . Later , they also annotated another layer for citation purposes ( Fisas et al , 2016 ) . An extension of this schema with argumentative components and relations was recently published ( Lauscher et al , 2018b ) , along with a tool for the prediction of the scientific elements ( Lauscher et al , 2018a ) . MAZEA ( Dayrell et al , 2012 ) . This schema considers six categories and the corpus was annotated for 645 abstracts from Physical Sciences and Engineering and Life and Health Sciences . 6 A Web application is available for tagging abstracts . PIBOSO ( Kim et al , 2011 ) . It was designed for the clinical domain and proposes six categories of a modified version of the PICO criteria . It was used for the ALTA - NICTA shared task 7 and recent works using this corpus include Hassanzadeh et al ( 2014 ) and Jin and Szolovits ( 2018 ) . The latter relies on deep learning methods and the implementation is readily available . PubMed RCT ( Dernoncourt and Lee , 2017 ) . It is a collection that includes two corpora of 20 , 000 and 200 , 000 medical abstracts annotated ( Green , 2018 ) bio CL Hybrid Green [ Levels 1 - 3 ] 1 . Causation , 1.1 One Group , 1.1.1 Agreement Argu - ments , 1.1.2 Eliminate Candidates , 1.1.3 Explanation - Based , 1.2 Two Group , 1.2.1 Difference , 1.2.2 Analogy ( Causal ) , 1.2.3 Explanation - Based , 2 . Other , 2.1 Classification , 2.2 Confirma - tion one Table 1 : Summary of the selected schemes and corresponding categories , size of the annotated corpora , and topic of the latter . Only the categories from the certain levels were shown for some schemes with various layers . Numbers or the corpora refer to full - text documents , unless otherwise stated . Regarding the topics , \" CL \" stands for computational linguistics , \" bio \" for biomedicine , \" chem \" for chemistry , \" CG \" for Computer Graphics , \" phy \" for Physics , \" eng \" for Engineering , \" LS \" for Life Sciences , and \" CS \" for Computer Science . with five categories . The corpus is freely available 8 as well as at least two tools for its detection , namely the one from Jin and Szolovits ( 2018 ) ( cf . PIBOSO above ) and one based on AllenNLP ( Achakulvisut et al , 2018 ) . Wilbur ( Wilbur et al , 2006 ) . It consists of a schema developed for biomedical articles on five dimensions . Later , the authors annotated 10 , 000 sentences from full - text publications ( Shatkay et al , 2008 ) , which was made available after a detailed analysis ( Rzhetsky et al , 2009 ) . 9 The annotation are on the level of fragments , which usually correspond to either the sentences or phrases .", "entities": [[600, 602, "DatasetName", "PubMed RCT"], [690, 691, "TaskName", "Classification"], [804, 805, "DatasetName", "CS"]]}
{"text": "Entity - level schemes aim at annotating the elements on the level of entities . Gupta and Manning ( 2011 ) proposed a simple schema based on three concepts and labeled 474 abstracts of computational linguistics . More recently , Jung ( 2017 ) defined five entity types and annotated 1 , 000 articles about information and communication technology ( ICT ) and chemical engineering . Blake ( 2010 ) also proposed a schema based on various levels of evidence ( implicit and explicit claims ) and annotated 29 full - text biomedical articles . However , none of the above data seems to be available but we found one schema with annotated corpus : ScienceIE ( Augenstein et al , 2017 ) . This schema proposes three elements on the entity level as well as the annotation of keyphrases . The corpus contains 500 articles about Computer Science , Material Sciences and Physics , which were split into training , development and test datasets and used for the a SemEval task in 2017 . We found the implementation from two of the participants on the shared task , namely ( Prasad and Kan , 2017 ) and ( Eger et al , 2017 ) .", "entities": [[115, 116, "DatasetName", "ScienceIE"]]}
{"text": "Previous work also considered schemes that consider relations between scientific elements . Prasad et al ( 2011 ) defined eight discourse relations in the Biomedical Discourse Relation Bank ( Bio - DRB ) and annotated 24 articles from the GENIA corpus , which was later used in a couple of works ( Ramesh and Yu , 2010 ; Polepalli Ramesh et al , 2012 ) . Tateisi et al ( 2013 ) defined 16 relations and annotated 30 articles , while Meyers et al ( 2014 ) proposed five relations and sub - relations with which they annotated 200 biomedical articles . However , none of the data above seems to be available , but we found corpora for the following two schemes : G\u00e1bor ( G\u00e1bor et al , 2016 ) It is a schema in the form of an ontology of 18 relations for the scientific literature , besides three more general relations . Six of these relations were recently addressed in the SemEval'18 Task 7 , for which annotated data is available ( G\u00e1bor et al , 2018 ) . For sub - task 2 in SemEval'18 Task 7 , the code from the team that obtained the best scores in this task is available ( Luan et al , 2018 ) . SciDTB ( Yang and Li , 2018 ) . It is a discourse treebank for scientific articles that includes 17 coarse - grained and 26 fine - grained relation types . They annotated 798 abstracts from the ACL Anthology that are available for download . 10", "entities": [[29, 30, "DatasetName", "Bio"], [39, 40, "DatasetName", "GENIA"], [141, 142, "MethodName", "ontology"]]}
{"text": "Hybrid schemes contain labels which cover more than one of the levels above . Tateisi et al ( 2016 ) created an ontology of entities and relations and annotated 400 abstracts about computational linguistic . However , we found only one hybrid schema for which annotated data is available : Green ( Green , 2018 ) . It is schema of 15 arguments annotated for one single article from the biomedical domain . The schema includes both entities and relations that are organized in a short taxonomy . Both schema and the annotated article are available . 11", "entities": [[22, 23, "MethodName", "ontology"]]}
{"text": "We evaluated tools that consider some of the schemes that we found ( cf . Section 2 ) for the task of text similarity in the scope of our use case of mining alternative methods for animal experiments . In this section we described the data and the tools that we used as well as the evaluation methodology .", "entities": [[22, 24, "TaskName", "text similarity"]]}
{"text": "We evaluated the selected schemes and tools for the task of text similarity . For this purpose , we model our problem as the following : given an input document that describes an animal experiment , we would like to mine similar candidate documents that are potential alternatives to animal testing . Our definition of similarity requires that both input and candidate documents should have similar research goal and comparable outcomes . However , the methods in the input document should be substantial different from those in the candidate documents . Therefore , we aim to compare input and candidate documents based on certain rhetorical elements as opposed to using the whole text . Our evaluation datasets consist of seven input documents from Medline whose identifiers ( PMIDs ) are 11489449 , 11932745 , 16192371 , 16850029 , 19735549 , 21494637 and 24204323 . For each input document , we collected the top 200 documents ( titles and abstracts ) retrieved from PubMed 's \" similar articles \" functionality . On one hand , the candidate documents are already very similar to the input document . On the other hand , the list of candidates returned by PubMed does not consider our definition of similarity . In order to build a suitable test set for our use case , a biomedical researcher manually validated at least the top 100 documents with regards to three degrees of similarity : very similar , similar and not similar . These three labels only consider the similarity of the research goals of each pair of abstracts ( input vs. candidate documents ) but do not address the 3R principles . Some documents were ignored because either they were only partially similar or because no decision could be made only based on the title and the abstract . After manual validation by the expert , our seven datasets encompass a total of 562 publications ( titles and abstracts ) . Figure 1 illustrates the distribution of the labels for each input document . Only four from the seven input documents had very similar publications ( from only 2 to 8 of them ) , while similar ones ( from only 4 to 19 ) could be found for all of them . However , the non similar publications are still the largest part ( from 56 to 76 ) of the list . The annotated data is available for download 12 . Some of the tools that we compared require some linguistic information not originally included in our documents , such as sentences and tokens . We utilized syntok 13 for both sentence splitting and tokenization to build input data for one of 12 https://github.com/mariananeves/ scientific - elements - text - similarity 13 https://github.com/fnl/syntok the tools , namely , Prasad and Kan ( 2017 ) .", "entities": [[11, 13, "TaskName", "text similarity"]]}
{"text": "We found a few available tools that address some of schemes discussed in Section 2 . However , we had dismiss some of them due to various problems . We experienced many problems with the Ten - sorFlow library while trying the tool 14 developed by ( Eger et al , 2017 ) for the ScienceIE schema . The tool seems to require a version of the library that it is no longer available and we could not resolve this issue not even after contacting the tool 's developers . We also dismissed the tool 15 from Jin and Szolovits ( 2018 ) for the PIBOSO and Pub - MedRCT schemes . The installation worked but we were not able to train it due to memory problems . Finally , we did not try the tool 16 from Luan et al ( 2018 ) since it addresses a relationbased schema ( G\u00e1bor ) that requires pre - tagged entities . Using named entities provided by other tools would probably add too much noise to the experiment . Finally , we had to dismiss the SAPI - ENTA tool ( Liakata et al , 2012b ) because it only allows uploading documents one by one to the Web application and we could not overcome this problem . We describe below the four tools that we tried for the extraction of rhetorical elements . Examples for the sentence - based ( zones ) and entitybased annotations are shown in Figure 2 . We released in the GitHub repository the annotations extracted by the tools in the JSON format supported by the TextAE tool 17 . Achakulvisut et al 18 ( Achakulvisut et al , 2018 ) ( PubMedRCT schema ) . It addresses the PubMed RCT schema , thus provides predictions for five zoning labels , namely , \" Background \" , \" Objective \" , \" Method \" , \" Results \" and \" Conclusions \" . We utilized the pre - trained models for Conditional Random Fields ( CRF ) as provided by the tool . Given that there is no publication , it is not clear what methods are behind the available models , but probably CRF . ArguminSci 19 ( Lauscher et al , 2018a ) ( Dr. Inventor schema extended ) . ArguminSci is available both for download as well as on - line ( Web application ) . It provides predictions for five schemes but we considered only the \" Discourse Role Classification ( DRC ) \" whose labels are \" Background \" , \" Challenge \" , \" Approach \" , \" Outcome \" and \" Future Work \" . ArguminSci 's models are based on bidirectional recurrent networks with long shortterm memory cells ( Bi - LSTMs ) and we utilized the command line version of the tool . MAZEA tool 20 and schema ( Dayrell et al , 2012 ) . The tool addresses six categories , namely , \" Background \" , \" Gap \" , \" Purpose \" , \" Method \" , \" Result \" and \" Conclusion \" . It is currently not available for download but only as a Web tool that requires to manually upload each document individually . However , the developers kindly processed our documents locally and sent the predictions back to us . The tool utilizes machine learning algorithms , such as Support Vector Machines ( SVM ) and Decision Trees . repository , we utilized the scripts for feature processing and the template to train the model with CRF++ 22 . We had to correct the provided template in order to successfully train the system . The entity recognition approach is based on various features and uses the CRF algorithm .", "entities": [[55, 56, "DatasetName", "ScienceIE"], [292, 294, "DatasetName", "PubMed RCT"], [338, 339, "MethodName", "CRF"], [367, 368, "MethodName", "CRF"], [416, 417, "TaskName", "Classification"], [572, 573, "MethodName", "SVM"], [625, 626, "MethodName", "CRF"]]}
{"text": "We carried out a total of 38 experiments that involved diverse tools , single labels and combination of various labels . We ran an error analysis to learn more about the false negatives and false positives that we obtained . At least one positive document was missed by any of the tools , i.e. was not placed among the top 10 positions . Many of the documents that we missed are certainly due to the limitation of considering only the top 10 highest ranked positions . However , none of the experiments obtained a recall of 1.0 . The highest recall that we obtained was 0.9 for the dataset 3 ( 16192371 ) using the Argu - minSci tool and either the single label \" Outcome \" or the combination of labels \" Challenge - Outcome - FutureWork \" . On one hand , five documents were missed by all experiments ( 38 times ) , namely , candidate documents \" 19155551 \" , \" 29133591 \" , \" 21362567 \" , \" 19667187 \" and \" 26047474 \" from datasets 3 , 5 , 6 , 6 , and 7 , respectively . On the other hand , the candidate document \" 25174890 \" from dataset 6 was the least missed one : only by three experiments . A total of 333 documents were wrongly classified as positive , i.e. were placed among the top 10 ones , by any of the 38 experiments . No candidate document was mistakenly classified by all approaches , but the more frequent ones were : \" 21501651 \" ( 27 times ) and \" 23571276 \" ( 25 times ) , both from dataset 4 , and \" 11494364 \" ( 25 times ) from dataset 7 . Our expert checked again the labels assigned to the top FPs and FNs above described and confirmed that their labels are correct and that the documents have been wrongly classified by the corresponding approaches . Our experiments have shown that many of the tools can indeed support our use case , specially when compared to the original list provided by PubMed . Regarding the integration of these tools into a workflow , one of the tools is currently not available ( MAZEA ) , while all the others need some adaptations to be used in real - life applications . With respect to the methods behind the tools , ArguminSci , which is based on LSTM , performed slightly better than the ones based on CRF ( Achakulvisut et al Prasad and Kan ) and superior than the machine learning algorithms in MAZEA . However , we did not evaluate the predictions made by the tools , but only their impact in a specific text similarity task . We expected that the best performing tools would be the ones that utilized corpora specifically built for the biomedical domain . From the tools that we evaluated , only Achakulvisut et al and MAZEA were specifically trained on documents from the biomedical or health domains . Nevertheless , ArguminSci , the best performing one , was trained on documents from computer graphics while and Prasad and Kan utilizes documents about computational linguistics . We also investigated whether there was any impact of the document type in the corpora , i.e. either full texts or only abstracts , on the performance of the corresponding tools . However , we did not observe any clear association between these two aspects . While the best performing tool ( Argu - minSci ) was trained on full texts , Achakulvisut et al utilizes only Medline abstracts . Similar to Ar - guminSci , the tool from Prasad and Kan is also based on full text documents . We carried out experiments with various tools but limited to a very specific use case . Even though our datasets contains a reasonable number of documents ( 562 ) , the similarity of the candidate documents was computed with respect to only seven input documents , and datasets were annotated by only one annotator . Further , we only considered titles and abstracts in our evaluation , while some tools were trained on full - text documents . Previous work has already shown the differences of information and performance of NLP tools in biomedical abstracts and full texts ( Verspoor et al , 2012 ; Mons et al , 2004 ) . Our future work will ad - dress many aspects : ( i ) use of full texts ; ( ii ) improvement of the datasets with additional annotators ; ( iii ) estimation of the compliance with the 3R principles by a candidate document , in addition to the calculation of similarity ; ( iv ) evaluation of the relation - based tool ( Luan et al , 2018 ) and the one for which we experienced memory problems ( Jin and Szolovits , 2018 ) ; and ( v ) evaluation of other schemes ( e.g. Wilbur et al ( 2006 ) ) for which an implementation is currently not available .", "entities": [[411, 412, "MethodName", "LSTM"], [421, 422, "MethodName", "CRF"], [460, 462, "TaskName", "text similarity"]]}
{"text": "We surveyed schemes that model scientific elements in publications and selected four schemes for which we could find an available tool . We utilized the predictions from these tools for assessing the text similarity between documents and further ranking them in the scope of mining alternative methods to animal testing . Our experiments show that a considerable improvement can be obtained when using ArguminSci , with respect to the original ranking returned by PubMed and to the strong baseline that we considered . However , there is still much room for improvement given that the obtained scores are still far below the possible maximum values .", "entities": [[32, 34, "TaskName", "text similarity"]]}
{"text": "For Task 1 our final submission consisted of an ensemble of two different multilingual models , that differ in the way they process the input source ( original sentence ) and hypothesis ( machine translation ) . Both models are based on the predictor - estimator architecture , using different pre - trained models to extract features and different training approaches to optimise for the QE task . The key idea explored with our first model ( denoted by M1 variations in the experiments ) , revolved around pursuing highly generalisable multilingual models , robust to overfitting . To this end , we train a cross - lingual transformer ( XLM - RoBERTa ( Conneau et al , 2020 ) ) on large , multilingual data with direct assessments and then use adapters ( Houlsby et al , 2019 ; Pfeiffer et al , 2020 ) to adapt to the domain specific data of the QE task with minimal training effort . In line with our efforts for good generalisation , we use only task - specific adapters and refrain from using specific adapters for each language pair . For these experiments we build on the OpenKiwi architecture ( Kepler et al , 2019 ) , using a pre - trained xlm - roberta - large encoder as a feature predictor . The source and hypothesis sentences are jointly encoded with hypothesis first . Then , source and hypothesis features are generated using average pooling over the hypothesis embeddings and forwarded to the estimator module which corresponds to a feed - forward layer . Figure 1 provides the general architecture 1 The model was first trained on the direct assessment data provided in the Metrics shared tasks ( Mathur et al , 2020 ) , as described in 3.1.2 . Upon training , the XML - R encoder is frozen and the the model is fine - tuned on sentence regression with the task - specific data , using stacked adapters . We hence manage to maintain a low number of trainable parameters during fine - tuning and minimize training time while learning to predict task - specific sentence scores . For the second model ( denoted by M2 - KL - G - MCD ) we aimed to explore the potential of a large pre - trained multilingual model ( trained with MT objectives ) . We use the mBART ( Liu et al , 2020 ) encoder - decoder architecture to encode the source and force - decode the hypothesis . We specifically use the mBART50 model ( Tang et al , 2020 ) which is trained with multilingual finetuning on 50 languages , including all languages of interest for the QE 2021 task . We obtain the features by averaging the decoder embeddings and concatenating with the < eos > token of the sequence . The estimator part of the model consists of a bottleneck feed - forward layer that reduces the dimensionality of the decoder output , and is concatenated with a vector with additional glass - box features from the NMT models ( see 3.1.1 ) . The combined vector is then forwarded to a feed - forward estimator and the full model is fine - tuned on the task specific QE data . Apart from the glass - box features we experimented further with methods that allow the model to be more robust towards the underlying uncertainty of its predictions . We elaborate that in the next section . Figure 2 provides a general architecture of the M2 model variations .", "entities": [[33, 35, "TaskName", "machine translation"], [110, 111, "MethodName", "XLM"], [112, 113, "MethodName", "RoBERTa"], [211, 212, "MethodName", "xlm"], [243, 245, "MethodName", "average pooling"], [400, 401, "MethodName", "mBART"]]}
{"text": "Multiple neural models are involved in the process of obtaining and scoring machine translations , which naturally leads to several sources of uncertainty . These sources can be very informative and useful for MT evaluation . In this work we try to consider three types of uncertainty : ( 1 ) uncertainty of the NMT models used to obtain the hypotheses , ( 2 ) data ( aleatoric ) uncertainty for which we use the inter - annotator disagreement as a proxy , and ( 3 ) uncertainty of the MT evaluation model itself . NMT model uncertainty The idea of extracting uncertainty - related features from the MT systems in order to estimate the quality of their predictions , was originally introduced by Fomicheva et al ( 2020 ) . This glass - box approach to QE is mostly focusing on capturing epistemic uncertainty , and the proposed features are extracted either using Monte Carlo ( MC ) dropout on the NMT or using the output probability distributions obtained from a standard deterministic MT system . In our last year 's submission ( Moura et al , 2020 ) the integration of such features proved to be effective , thus we decided to incorporate it into our new model as well . We list the extracted features below : TP sentence average of word translation probability - of MT output generated in different stochastic passes .", "entities": [[224, 226, "TaskName", "word translation"]]}
{"text": "For Task 2 we submitted an ensemble of two variations of the first model ( M1 - ADAPT and M1 M - ADAPT ) presented for Task 1 ( see 3.1 ) . In both cases , we use multi - task training and a feedforward for each output types : hypothesis word tags , hypothesis gap tags , source word tags , and sentence regression ( on HTER scores ) . Both variations use a pre - trained XLM - RoBERTa ( large ) encoder to extract features as described for Task 1 , but differ in the training of the encoder . In the first case we use the pre - trained model 3 and finetune on the QE data using stacked adapters . In the second variation we swap the original pre - trained model with the XLM - RoBERTa model that has been trained on the Metrics data as described in 3.1.2 . We note that the two variations favor different language pairs , hence we combine multiple checkpoints from each variation ( ranging training steps ) . We use the test - 20 split of the data to optimise the hyper - parameters and following this approach we use the estimated top - 3 checkpoints from each variation using the combined dataset 4 and the top checkpoint for the non - augmented model trained exclusively on the train set , resulting in total 7 checkpoints in our final ensemble .", "entities": [[79, 80, "MethodName", "XLM"], [81, 82, "MethodName", "RoBERTa"], [140, 141, "MethodName", "XLM"], [142, 143, "MethodName", "RoBERTa"]]}
{"text": "In Table 6 is an excerpt of the training configuration used for training the M2 models using the mBART encoder - decoder :", "entities": [[18, 19, "MethodName", "mBART"]]}
{"text": "We are grateful to Alon Lavie and Craig Stewart for their valuable feedback and discussions . This work was supported by the P2020 programs MAIA ( contract 045909 ) and Unbabel4EU ( contract 042671 ) , by the European Research Council ( ERC StG Deep - SPIN 758969 ) , and by the Funda\u00e7\u00e3o para a Ci\u00eancia e Tecnologia through contract UIDB/50008/2020 .", "entities": [[46, 47, "MethodName", "SPIN"]]}
{"text": "Structure - Aware Abstractive Conversation Summarization via Discourse and Action Graphs", "entities": [[5, 6, "TaskName", "Summarization"]]}
{"text": "Abstractive conversation summarization has received much attention recently . However , these generated summaries often suffer from insufficient , redundant , or incorrect content , largely due to the unstructured and complex characteristics of human - human interactions . To this end , we propose to explicitly model the rich structures in conversations for more precise and accurate conversation summarization , by first incorporating discourse relations between utterances and action triples ( \" WHO - DOING - WHAT \" ) in utterances through structured graphs to better encode conversations , and then designing a multi - granularity decoder to generate summaries by combining all levels of information . Experiments show that our proposed models outperform state - of - theart methods and generalize well in other domains in terms of both automatic evaluations and human judgments . We have publicly released our code at https://github.com/ GT - SALT / Structure - Aware - BART .", "entities": [[2, 3, "TaskName", "summarization"], [59, 60, "TaskName", "summarization"], [153, 154, "MethodName", "BART"]]}
{"text": "Online interaction has become an indispensable component of everyday life and people are increasingly using textual conversations to exchange ideas , make plans , and share information . However , it is time - consuming to recap and grasp all the core content within every complex conversation ( Gao et al , 2020 ; Feng et al , 2020 ) . As a result , how to organize massive everyday interactions into natural , concise , and informative text , i.e. , abstractive conversation summarization , starts to gain importance . Significant progress has been made on abstractive summarization for structured document via pointer generator ( See et al , 2017 ) , reinforcement methods ( Paulus et al , 2018 ; and pre - trained models ( Liu and Lapata , 2019 ; Lewis et al , 2020 ; . Despite the huge success , it is challenging to directly apply document models to summarize conversations , due to ( Gliwa et al , 2019 ) . The annotated summary is Simon was on the phone before , so he did n't here Helen calling . Simon will fetch Helen some tissues . a set of inherent differences between conversations and documents ( Gliwa et al , 2019 ) . First , speaker interruptions like repetitions , false - starts , and hesitations are frequent in conversations ( Sacks et al , 1978 ) , and key information resides in different portions of a conversation . These unstructured properties pose challenges for models to focus on salient contents that are necessary for generating both abstractive and informative summaries . Second , there is more than one speaker in conversations and people interact with each other in different language styles . The complex interactions among multiple speakers make it harder for mod - els to identify and associate speakers with correct actions so as to generate factual summaries . In order to summarize the unstructured and complex conversations , a growing body of research has been conducted , such as transferring document summarization methods to conversation settings ( Shang et al , 2018 ; Gliwa et al , 2019 ) , adopting hierarchical models , or incorporating conversation structures like topic segmentation ( Liu et al , 2019b ; Chen and Yang , 2020 ) , dialogue acts ( Goo and Chen , 2018 ) , and conversation stages ( Chen and Yang , 2020 ) . However , current approaches still face challenges in terms of succinctness and faithfulness , as most prior studies ( i ) fail to explicitly model dependencies between utterances which can help identify salient portions of conversations ( Bui et al , 2009 ) , and ( ii ) lack structured representations to learn the associations between speakers , actions and events . We argue that these rich linguistic structures associated with conversations are key components towards generating abstractive and factual conversation summaries . To this end , we present a structure - aware sequence - to - sequence model , in which we equip abstractive conversation summarization models with rich conversation structures through two types of graphs : discourse relation graph and action graph . Discourse relation graphs are constructed based on dependency - based discourse relations ( Kirschner et al , 2012 ; Stone et al , 2013 ; Asher et al , 2016 ; Qin et al , 2017 ) between intertwined utterances , where each Elementary Discourse Unit ( EDU ) is one single utterance and they are linked through 16 different types of relations ( Asher et al , 2016 ) . As shown in Figure 1 ( a ) , highly related utterances are linked based on discourse relations like Question Answer Pairs , Comment and Explanation . Explicitly modeling these utterances relations in conversations can aid models in recognizing key content for succinct and informative summarization . Action graphs are constructed as the \" WHO - DOING - WHAT \" triplets in conversations which express socially situated identities and activities ( Gee , 2014 ) . For instance , in Figure 1 ( b ) , the action graph provides explicit information between Simon , fetch , and tissues for the utterance it is Simon who will fetch the tissues , making models less likely to generate summaries with wrong references ( e.g. , Helen will fetch the tissues ) . To sum up , our contributions are : ( 1 ) We pro - pose to utilize discourse relation graphs and action graphs to better encode conversations for conversation summarization . ( 2 ) We design structureaware sequence - to - sequence models to combine these structured graphs and generate summaries with the help of a novel multi - granularity decoder . ( 3 ) We demonstrate the effectiveness of our proposed methods through experiments on a largescale conversation summarization dataset , SAM - Sum ( Gliwa et al , 2019 ) . ( 4 ) We further show that our structure - aware models can generalize well in new domains such as debate summarization .", "entities": [[84, 85, "TaskName", "summarization"], [98, 99, "TaskName", "summarization"], [184, 185, "DatasetName", "Helen"], [190, 191, "DatasetName", "Helen"], [341, 343, "TaskName", "document summarization"], [513, 514, "TaskName", "summarization"], [648, 649, "TaskName", "summarization"], [727, 728, "DatasetName", "Helen"], [763, 764, "TaskName", "summarization"], [813, 814, "TaskName", "summarization"], [848, 849, "TaskName", "summarization"]]}
{"text": "Document Summarization Compared to extractive document summarization ( Gupta and Lehal , 2010 ; Narayan et al , 2018 ; Liu and Lapata , 2019 ) , abstractive document summarization is generally considered more challenging and has received more attention . Various methods have been designed to tackle abstractive document summarization like sequence - to - sequence models ( Rush et al , 2015 ) , pointer generators ( See et al , 2017 ) , reinforcement learning methods ( Paulus et al , 2018 ; and pre - trained models ( Lewis et al , 2020 ; . To generate faithful abstractive document summaries ( Maynez et al , 2020 ) , graphbased models were introduced recently such as extracting entity types ( Fernandes et al , 2018 ; , leveraging knowledge graphs Zhu et al , 2020a ) or designing extra fact correction modules . Inspired by these graph - based methods , we also construct action graphs for generating more factual conversation summaries . Conversation Summarization Extractive dialogue summarization ( Murray et al , 2005 ) has been studied extensively via statistical machine learning methods such as skip - chain CRFs ( Galley , 2006 ) , SVM with LDA models ( Wang and Cardie , 2013 ) , and multi - sentence compression algorithms ( Shang et al , 2018 ) . Such methods struggled with generating succinct , fluent , and natural summaries , especially when the key information needs to be aggregated from multiple first - person point - of - view utterances ( Song et al , 2020 ) . Abstractive conversation summarization overcomes these issues by designing hierarchical models , incorporating commonsense knowledge ( Feng et al , 2020 ) , or leveraging conversational structures like dialogue acts ( Goo and Chen , 2018 ) , key point sequences ( Liu et al , 2019a ) , topic segments ( Liu et al , 2019b ; and stage developments ( Chen and Yang , 2020 ) . Some recent research has also utilized discourse relations as input features in classifiers to detect important content in conversations ( Murray et al , 2006 ; Bui et al , 2009 ; Qin et al , 2017 ) . However , current models still have not explicitly utilized the dependencies between different utterances , making models hard to leverage long - range dependencies and utilize these salient utterances . Moreover , less attention has been paid to identify the actions of different speakers and how they interact with or refer to each other , leading to unfaithful summarization with incorrect references or wrong reasoning ( Gliwa et al , 2019 ) . To fill these gaps , we propose to explicitly model actions within utterances , and relations between utterances in conversations in a structured way , by using discourse relation graphs and action graphs and further combining these through relational graph encoders and multigranularity decoders for abstractive conversation summarization .", "entities": [[0, 2, "TaskName", "Document Summarization"], [4, 7, "TaskName", "extractive document summarization"], [28, 30, "TaskName", "document summarization"], [49, 51, "TaskName", "document summarization"], [132, 134, "TaskName", "knowledge graphs"], [147, 148, "DatasetName", "Inspired"], [168, 169, "TaskName", "Summarization"], [171, 172, "TaskName", "summarization"], [200, 201, "MethodName", "SVM"], [202, 203, "MethodName", "LDA"], [215, 217, "DatasetName", "sentence compression"], [269, 270, "TaskName", "summarization"], [432, 433, "TaskName", "summarization"], [494, 495, "TaskName", "summarization"]]}
{"text": "The \" who - doing - what \" triples from utterances can provide explicit visualizations of speakers and their actions , the key to understanding concrete details happened in conversations ( Moser , 2001 ; Gee , 2014 ; Sacks et al , 1978 ) . Simply relying on neural models to identify this information from conversations often fail to produce factual characterizations of concrete details happened ( Cao et al , 2018 ; . To this end , we extract \" WHO - DOING - WHAT \" triples from utterances and construct action graphs for conversation summarization Huang et al , 2020b , a ) . Specifically , we first transform the first - person point - of - view utterances to its thirdperson point - of - view forms based on simple rules : ( i ) substituting first / second - person pronouns with the names of current speaker or surrounding speakers and ( ii ) replacing third - person pronouns based on coreference clusters in conversations detected by the Stanford CoreNLP ( Manning et al , 2014 ) . For example , an utterance \" I 'll bring it to you to - morrow \" from Amanda to Jerry will be transformed into \" Amanda'll bring cakes to Jerry tomorrow \" . Then we extract \" WHO - DOING - WHAT \" ( subjectpredicate - object ) triples from transformed conversations using the open information extraction ( Ope - nIE ) systems 1 ( Angeli et al , 2015 ) . We then construct the Action Graph G A = ( V A , E A ) from the extracted triples by taking arguments ( \" WHO \" , \" DOING \" , or \" WHAT \" ) as nodes in V A , and connect them with edge E A [ i ] [ j ] = 1 if they are adjacent in one \" WHO - DOING - WHAT \" triple .", "entities": [[97, 98, "TaskName", "summarization"], [237, 240, "TaskName", "open information extraction"]]}
{"text": "We initialize our utterance encoder F U ( . ) with a pre - trained encoder , i.e. , BART - base ( Lewis et al , 2020 ) , and encode tokens { x i , 0 , ... , x i , l } in an utterance u i into its hidden representation : { h U i , 0 , ... , h U i , l } = F U ( { x i , 0 , ... , x i , l } ) ( 1 ) Here we add a special token x i , 0 = < S > at the beginning of each utterance to represent it .", "entities": [[19, 20, "MethodName", "BART"], [37, 38, "DatasetName", "0"], [61, 62, "DatasetName", "0"], [79, 80, "DatasetName", "0"], [101, 102, "DatasetName", "0"]]}
{"text": "We trained and evaluated our models on a conversation summarization dataset SAMSum ( Gliwa et al , 2019 ) covering messenger - like conversations about daily topics , such as arranging meetings and discussing events . We also showed the generalizability of our models on the Argumentative Dialogue Summary Corpus ( ADSC ) ( Misra et al , 2015 ) , a debate summarization corpus . The data statistics of two datasets were shown in Table 1 , with the discourse relation types distributions in the Appendix .", "entities": [[9, 10, "TaskName", "summarization"], [11, 12, "DatasetName", "SAMSum"], [63, 64, "TaskName", "summarization"]]}
{"text": "We compare our methods with several baselines : Pointer Generator ( See et al , 2017 ) : We followed the settings in Gliwa et al ( 2019 ) and used special tokens to separate each utterance . Transformer ( Vaswani et al , 2017 ) : We trained transformer seq2seq models following the OpenNMT ( Klein et al , 2017 ) . D - HGN ( Feng et al , 2020 ) incorporated commonsense knowledge from ConceptNet ( Liu and Singh , 2004 ) for dialogue summarization . ( Dror et al , 2018 ) and found that S - BART w. Discourse & Action significantly outperformed the base BART ( p < 0.05 ) . BART ( Lewis et al , 2020 ) : We utilized BART 2 , and separated utterances by a special token . Multi - View Seq2Seq ( Chen and Yang , 2020 ) utilized topic and stage views on top of BART for summarizing conversations . Here we implemented it based on BART - base models .", "entities": [[38, 39, "MethodName", "Transformer"], [50, 51, "MethodName", "seq2seq"], [77, 78, "DatasetName", "ConceptNet"], [87, 88, "TaskName", "summarization"], [101, 102, "MethodName", "BART"], [110, 111, "MethodName", "BART"], [117, 118, "MethodName", "BART"], [128, 129, "MethodName", "BART"], [142, 143, "MethodName", "Seq2Seq"], [158, 159, "MethodName", "BART"], [169, 170, "MethodName", "BART"]]}
{"text": "We conducted human evaluation to qualitatively evaluate the generated summaries . Specifically , we asked annotators from Amazon Mechanical Turk to score a set of randomly sampled 100 generated summaries from ground - truth , BART and our structured models , using a Likert scale from 1 ( worst ) to 5 ( best ) in terms of factualness ( e.g. , associates actions with the right actors ) , succinctness ( e.g. , does not contain redundant information ) , and informativeness ( e.g. , covers the most important content ) ( Feng et al , 2020 ; . To increase annotation quality , we required turkers to have a 98 % approval rate and at least 10 , 000 approved tasks for their previous work . Each message was rated by three workers . The scores for each summary were averaged . The Intra - Class Correlation was 0.543 , showing moderate agreement ( Koo and Li , 2016 ) . As shown in Table 4 , S - BART that utilized structured information from discourse relation graphs and action graphs generated significantly better summaries with respect to factualness , succinctness , and informativeness . This might because that the incorporation of structured information such as discourse relations helped S - BART to recognize the salient parts in conversations , and thus improve the succinctness and informativeness over BART . Modeling the connections between speakers and actions greatly helped generate more factual summaries than the baselines , e.g. , with an increase of 0.27 from BART to S - BART w. Action .", "entities": [[35, 36, "MethodName", "BART"], [171, 172, "MethodName", "BART"], [213, 214, "MethodName", "BART"], [230, 231, "MethodName", "BART"], [257, 258, "MethodName", "BART"], [261, 262, "MethodName", "BART"]]}
{"text": "In this work , we introduced a structure - aware sequence - to - sequence model for abstractive conversation summarization by incorporating discourse relations between utterances , and the connections between speakers and actions within utterances . Experiments and ablation studies on SAMSum corpus showed the effectiveness of these structured graphs in aiding the task of conversation summarization via both quantitative and qualitative eval - uation metrics . Results in zero - shot settings on ADCS Corpus further demonstrated the generalizability of our structure - aware models . In the future , we plan to extend our current conversation summarization models for various application domains such as emails , debates , and podcasts , and in conversations that might involve longer utterances and more participants in an unsynchronized way . ( Asher et al , 2016 ) with default settings 5 to get the link prediction and relation classification models to label discourse relations in SAMSum and ADSC corpus . The distribution of the relation types in two datasets were shown in Table 9 . The major discourse relations in daily conversations are Comment , Clarification and QA pairs , while the main discourse relations in debate are Comment , Contrast , Clarification and QA pairs .", "entities": [[19, 20, "TaskName", "summarization"], [42, 44, "DatasetName", "SAMSum corpus"], [57, 58, "TaskName", "summarization"], [99, 100, "TaskName", "summarization"], [144, 146, "TaskName", "link prediction"], [147, 149, "TaskName", "relation classification"], [155, 156, "DatasetName", "SAMSum"]]}
{"text": "We would like to thank the anonymous reviewers for their helpful comments , and the members of Georgia Tech SALT group for their feedback . This work is supported in part by grants from Google , Amazon and Salesforce .", "entities": [[34, 35, "DatasetName", "Google"]]}
{"text": "SeqMix : Augmenting Active Sequence Labeling via Sequence Mixup", "entities": [[8, 9, "MethodName", "Mixup"]]}
{"text": "Active learning is an important technique for low - resource sequence labeling tasks . However , current active sequence labeling methods use the queried samples alone in each iteration , which is an inefficient way of leveraging human annotations . We propose a simple but effective data augmentation method to improve label efficiency of active sequence labeling . Our method , SeqMix , simply augments the queried samples by generating extra labeled sequences in each iteration . The key difficulty is to generate plausible sequences along with token - level labels . In SeqMix , we address this challenge by performing mixup for both sequences and token - level labels of the queried samples . Furthermore , we design a discriminator during sequence mixup , which judges whether the generated sequences are plausible or not . Our experiments on Named Entity Recognition and Event Detection tasks show that SeqMix can improve the standard active sequence labeling method by 2.27 % - 3.75 % in terms of F 1 scores . The code and data for SeqMix can be found at https://github . com / rz - zhang / SeqMix .", "entities": [[0, 2, "TaskName", "Active learning"], [46, 48, "TaskName", "data augmentation"], [101, 102, "MethodName", "mixup"], [123, 124, "MethodName", "mixup"], [139, 142, "TaskName", "Named Entity Recognition"], [143, 145, "TaskName", "Event Detection"]]}
{"text": "Mixup in the Embedding Space Mixup in the Label Space ( c ) Label - constrained sub - sequence mixup Figure 1 : Illustration of the three variants of SeqMix . We use s = 5 , \u03b7 0 = 3 5 for whole - sequence mixup and s = 3 , \u03b7 0 = 2 3 for sub - sequence mixup and label - constrained sub - sequence mixup . The solid red frames indicate paired sequences or sub - sequences , and the red dotted frames indicate generated sequence or sub - sequence . In the original sequences , the parts not included in the solid red frames will be unchanged in the generated sequences . For the mixup in the embedding space , we take the embedding in E which is closest to the raw mixed embedding as the generated embedding . For the mixup in the label space , the mixed label can be used as the pseudo label . version is called label - constrained sub - sequence mixup . Comparing the three variants , label - constrained sub - sequence mixup gives the most restrictions to pairing parent samples , sub - sequence mixup sets the sub - sequence - level pattern , while wholesequence mixup just requires \u03b7 \u2265 \u03b7 0 for the sequences with the same length .", "entities": [[0, 1, "MethodName", "Mixup"], [5, 6, "MethodName", "Mixup"], [19, 20, "MethodName", "mixup"], [38, 39, "DatasetName", "0"], [46, 47, "MethodName", "mixup"], [53, 54, "DatasetName", "0"], [61, 62, "MethodName", "mixup"], [69, 70, "MethodName", "mixup"], [120, 121, "MethodName", "mixup"], [147, 148, "MethodName", "mixup"], [173, 174, "MethodName", "mixup"], [186, 187, "MethodName", "mixup"], [199, 200, "MethodName", "mixup"], [211, 212, "MethodName", "mixup"], [217, 218, "DatasetName", "0"]]}
{"text": "Active Sequence Labeling Sequence labeling has been studied extensively for different NLP problems . Different neural architectures has been proposed ( Huang et al , 2015 ; Lample et al , 2016 ; Peters et al , 2018 ; Akbik et al , 2018 ) in recent years , which have achieved state - of - the - art performance in a number of sequence labeling tasks . However , these neural models usually require exhaustive human efforts for generating labels for each token , and may not perform well in lowresource settings . To improve the performance of low - resource sequence labeling , several approaches have been applied including using semi - supervised methods Chen et al , 2020b ) , external weak supervision ( Lison et al , 2020 ; Liang et al , 2020 ; Ren et al , 2020 ; Zhang et al , 2019 ; Yu et al , 2020 ) and active learning ( Shen et al , 2017 ; Hazra et al , 2019 ; Liu et al , 2018 ; Fang et al , 2017 ; Gao et al , 2019 ) . In this study , we mainly focus on active learning approaches which select samples based on the query policy design . So far , various uncertainty - based ( Scheffer et al , 2001 ; Culotta and McCallum , 2005 ; Kim et al , 2006 ) and committee - based approaches ( Dagan and Engelson , 1995 ) have been proposed for improving the sample efficiency . More recently , Shen et al ( 2017 ) ; Hazra et al ( 2019 ) ; Liu et al ( 2018 ) ; Fang et al ( 2017 ) further improve the aforementioned active learning approaches to improve the sampling diversity as well as the generalization ability of models on low - resource scenarios . These works mainly claim the sample efficiency provided by the active learning approach but do not study data augmentation for active sequence labeling . Interpolation - based Regularizations Mixup implements interpolation in the input space to regularize models ( Zhang et al , 2018 ) . Recently , the Mixup variants ( Verma et al , 2019 ; Summers and Dinneen , 2019 ; Guo et al , 2019b ) turn to perform interpolation in the hidden space to capture higher - level information . Guo et al ( 2019a ) ; Chen et al ( 2020a ) apply hidden - space Mixup for text classification . These works , however , have not explored how to perform mixup for sequences with token - level labels , nor do they consider the quality of the mixed - up samples . Text Augmentation Our work is also related to text data augmentation . Zhang et al ( 2015 ) ; Wei and Zou ( 2019 ) utilize heuristic approaches including synonym replancement , random insertion , swap and deletion for text augmentation , Kafle et al ( 2017 ) ; Silfverberg et al ( 2017 ) employ heuristic rules based on specific task , Hu et al ( 2017 ) propose to augment text data in an encoder - decoder manner . Very recently , ( Anaby - Tavor et al , 2020 ; Kobayashi , 2018 ) harness the power of pre - trained language models and augmenting the text data based on contextual patterns . Although these methods can augment the training set and improve the performance of text classification model , they fail to generate sequences and labels simultaneously , thus can not be adapted to our problem where tokenlevel labels are required during training . Instead , in our study , we propose a new framework SeqMix for data augmentation to facilitate sequence labeling task . Our method can generate token - level labels and preserve the semantic information in the augmented sentences . Moreover , it can be naturally combined with existing active learning approaches and further promote the performance .", "entities": [[158, 160, "TaskName", "active learning"], [200, 202, "TaskName", "active learning"], [294, 296, "TaskName", "active learning"], [326, 328, "TaskName", "active learning"], [332, 334, "DatasetName", "study data"], [344, 345, "MethodName", "Mixup"], [365, 366, "MethodName", "Mixup"], [418, 419, "MethodName", "Mixup"], [420, 422, "TaskName", "text classification"], [434, 435, "MethodName", "mixup"], [465, 467, "TaskName", "data augmentation"], [585, 587, "TaskName", "text classification"], [627, 629, "TaskName", "data augmentation"], [662, 664, "TaskName", "active learning"]]}
{"text": "We propose a simple data augmentation method SeqMix to enhance active sequence labeling . By performing sequence mixup in the latent space , Se - qMix improves data diversity during active learning , while being able to generate plausible augmented sequences . This method is generic to different active learning policies and various sequence labeling tasks . Our experiments demonstrate that SeqMix can improve active learning baselines consistently for NER and event detection tasks ; and its benefits are especially prominent in low - data regimes . For future research , it is interesting to enhance SeqMix with language models during the mixup process , and harness external knowledge for further improving diversity and plausibility .", "entities": [[4, 6, "TaskName", "data augmentation"], [17, 18, "MethodName", "mixup"], [30, 32, "TaskName", "active learning"], [48, 50, "TaskName", "active learning"], [64, 66, "TaskName", "active learning"], [69, 70, "TaskName", "NER"], [71, 73, "TaskName", "event detection"], [102, 103, "MethodName", "mixup"]]}
{"text": "Here we list the link to datasets used in our experiments . CoNLL - 03 : https://github.com/ synalp / NER / tree / master / corpus/ CoNLL - 2003 . ACE05 : We are unable to provide the downloadable version due to it is not public . This corpus can be applied through the website of LDC : https://www.ldc.upenn.edu/ collaborations / past - projects/ ace . Webpage : Please refer the link in the paper ( Ratinov and Roth , 2009 ) .", "entities": [[19, 20, "TaskName", "NER"]]}
{"text": "All the mentioned dataset has been split into train / validate / test set in the released version . We keep consistent with the validation set and the test set in our experiment . For the active learning paradigm , we split the training set as Table 3 . The active learners are initialized on the seed set , then they implement 5 active learning rounds .", "entities": [[36, 38, "TaskName", "active learning"], [63, 65, "TaskName", "active learning"]]}
{"text": "For the baselines , we take random sampling and 3 active learning approaches - LC sampling , NTE sampling , and QBC sampling as Section 2.2 .", "entities": [[10, 12, "TaskName", "active learning"]]}
{"text": "We implement bert - base - cased as the underlying model for the NER task and bert - base - multilingualcased as the underlying model for the event detection task . We use the model from Huggingface Transformer codebase 3 , and the repository 4 to finetune our model for sequence labeling task .", "entities": [[13, 14, "TaskName", "NER"], [27, 29, "TaskName", "event detection"], [37, 38, "MethodName", "Transformer"]]}
{"text": "In our model , we use bert - base - cased and bertbase - multilingual - cased both of them occupy 12layer , 768 - hidden , 12 - heads with 110 M parameters . 3 https://github.com/huggingface/ transformers 4 https://github.com/kamalkraj/BERT - NER", "entities": [[41, 42, "TaskName", "NER"]]}
{"text": "In Section 3.2 , we construct a table of tokens W and their corresponding contextual embedding E. For our underlying BERT model , we use the vocabulary provided by the tokenizer to build up W , and the embedding initialized on the training set as E. We also need to construct a special token collection to exclude some generation in the process of sequence mixing . For example , BERT places token [ CLS ] and [ SEP ] at the starting position and the ending position for sentence , and pad the inputs with [ PAD ] . We exclude these disturbing tokens and the parent tokens .", "entities": [[20, 21, "MethodName", "BERT"], [69, 70, "MethodName", "BERT"], [96, 97, "DatasetName", "PAD"]]}
{"text": "For the 5 - round active learning with SeqMix augmentation , our program runs about 500 seconds for WebPage dataset , 1700 seconds for the CoNLL slicing dataset , and 3.5 hours for ACE 2005 . If the QBC query policy used , all the runtime will be multiplied about 3 times .", "entities": [[5, 7, "TaskName", "active learning"], [33, 35, "DatasetName", "ACE 2005"]]}
{"text": "Social media has lately become one of the primary venues where users express their opinions about various products and services . These opinions are extremely useful in understanding the user 's perceptions and sentiment about these services . They are also useful in identifying potential defects ( Abrahams et al , 2012 ) and thus critical to the execution of downstream customer service responses . Therefore , automatic detection of user complaints on social media could prove beneficial to both the clients and the service providers . To build such detection systems , we could employ supervised approaches that would typically require a large corpus of labeled training samples . However , labeling social media posts that capture complaints about a particular service is challenging because of their low prevalence and also the vast amounts of inevitable noise ( Kietzmann et al , 2011 ; Lee , 2018 ) . Additionally , social media platforms are also likely to be plagued with redundancy , where the posts are rephrased or structurally morphed before being re - posted ( Ellison et al , 2011 ; Harrigan et al , 2012 ) . Prior work in event detection ( Ritter et al , 2012 ) has demonstrated that simple linguistic indicators ( phrases or n - grams ) can be useful in the accurate discovery of events in social media . Though user complaints are not the same as events , more of a speech act ( Preotiuc - Pietro et al , 2019 ) , we posit that similar indicators can be used in complaint detection . To pursue this hypothesis , we propose a semi - supervised iterative approach to identify social media posts that complain about a specific service . In our approach , we first begin with a small , manually curated dataset containing samples of social media posts complaining about a service . We then identify linguistic indicators ( phrases or n - grams ) that serve as strong evidence of this phenomenon . These indicators are then used to extract more posts from the unannotated corpus . This newly obtained data is then used to create a new set of indicators . This process is repeated until it reaches a certain convergence point . Since the set of indicators is growing after each iteration , they are re - evaluated continuously in terms of their relevance . This process is similar to the mutual bootstrapping approach for information extraction proposed in ( Riloff et al , 2003 ) . We employ this approach to the problem of complaint detection for transportation services on Twitter . Transportation and its related logistic services are critical aspects of every economy as they account for nearly 40 % of the value of international trade ( Rodrigue , 2007 ) . As with most businesses ( Gallaugher and Ransbotham , 2010 ; Gottipati et al , 2018 ) , transportation also often relies on social media to ascertain feedback and initiate appropriate responses ( Stelzer et al , 2016 ( Stelzer et al , , 2014 . In our experimental work , we started with an annotated set of 326 samples of transportation complaints , and after four iterations of the approach , we collected 2 , 840 indicators and over 3 , 700 tweets . We annotated a random sample of 700 tweets from the final dataset and observed that over 47 % of the samples were actual transportation complaints . We also characterize the performance of basic classification algorithms on this dataset . In doing so , we also study how different linguistic features contribute to the performance of a supervised model in this domain . The main contributions of this paper are as follows : We propose a semi - supervised iterative approach to collect user complaints about a service from social media platforms . We evaluate the proposed approach for the problem of complaint detection for transportation services on Twitter . We annotate a random sample of the resulting dataset to establish that nearly half the tweets were actual complaints . We release a curated dataset for the task of traffic - related complaint detection in social media 1 . Lastly , we characterize the performance of basic classification algorithms on the dataset .", "entities": [[194, 196, "TaskName", "event detection"]]}
{"text": "Complaints are often considered dialogue acts used to express a mismatch between the expectation and reality ( Olshtain and Weinbach , 1985 ) . The problem of complaint detection is of great interest to the marketing and research teams of various service providers . Previous works on complaint identification have applied text mining with LDA and sentiment analysis on user - generated content Duan et al , 2013 ) . Prior works have also focused on leveraging data streamed from social media platforms for outage and complaint detection as they are publicly available ( Augustine et al , 2012 ; Kursar and Gopinath , 2013 ) . ( Yang et al , 2019 ) inspected customer support dialogue for support . Different complaint expressions have been explored by analyzing variations across cultures ( Cohen and Olshtain , 1993 ) , sociodemographic traits ( Boxer , 1993 ) and temporal representations ( Raghavan , 2014 ) . However , mentioned works on user - generated content have focused on static data repositories only . These have not been robust to linguistic variations ( Shah and Zimmermann , 2017 ) and morphological changes ( Abdul - Mageed and Korayem , 2010 ) . Our pipeline builds on linguistic identifiers to expand on lexical cues in order to identify complaint relevant posts . Researches have proposed many semisupervised architectures for identification of events pertaining to societal and civil unrest ( Hua et al , 2013 ) , using speech modality ( Serizel et al , 2018 ; Wu et al , 2014 ; Zhang et al , 2017 ) and Hidden Markov Models ( Zhang , 2005 ) . These have been documented to give better performance as compared against their counterparts ( Lee et al , 2017 ; Zheng et al , 2017 ) with minimal intervention ( Rahimi et al , 2018 ) . For our analysis , the semi - supervised approach has been preferred as opposed to supervised ones because : ( a ) usage of supervised approach relies on carefully choosing the training set making it cumbersome and less attractive for practical use ( Watanabe , 2018 ) and ( b ) imbalance between the subjective and objective classes lead to poor performance ( Yu et al , 2015 ) .", "entities": [[54, 55, "MethodName", "LDA"], [56, 58, "TaskName", "sentiment analysis"]]}
{"text": "We also wanted to understand the predictive power of different types of linguistic features towards the detection of complaints . These features can be broadly broken down into four groups . ( i ) The first group of features are based on simple semantic properties such as n - grams , word embeddings , and part of speech tags . ( ii ) The second group of features are based on pre - trained sentiment models or lexicons . ( iii ) The third group of features use orthographic information such as hashtags , user mentions , and intensifiers . ( iv ) The last group of features again use pre - trained models or lexicons associated with request , which is a closely related speech act ( \u0160v\u00e1rov\u00e1 , 2008 ) .", "entities": [[51, 53, "TaskName", "word embeddings"]]}
{"text": "We expect sentiment to contribute strongly towards the prediction of complaints . We experiment with two pre - trained models : Stanford Sentiment ( Socher et al , 2013 ) and VADER ( Hutto and Gilbert , 2014 ) . Namely , we use the scores predicted by these models as representations of tweets . Likewise , we also experiment with two sentiment lexicons : MPQA ( Wilson et al , 2005 ) , NRC ( Mohammad et al , 2013 ) for assigning sentiment scores to tweets .", "entities": [[65, 66, "DatasetName", "MPQA"]]}
{"text": "The ever - increasing amount of user - generated data introduces new challenges in terms of automatic content moderation , especially regarding hate speech and offensive language detection . User content mostly consists of microposts , where the context of a post can be missing or inferred only from current events . The challenge of automatic identification and detection of online aggressiveness has therefore gained increasing popularity in the scientific community over the last years . Several recent workshops and conferences such as TRAC ( Kumar et al , 2018 ) , ALW2 ( Fi\u0161er et al , 2018 ) , and GermEval ( Wiegand et al , 2018 ) show the growing importance of this subject . The SemEval 2019 shared task 6 ( Zampieri et al , 2019b ) further addresses this topic by introducing the Offensive Language Identification Dataset ( OLID ) , which consists of tweets , labeled with a three - level annotation model ( Zampieri et al , 2019a ) . Sub - task A is composed of a binary classification problem of whether a tweet in the dataset is offensive or not . Sub - task B focuses on different categories of offensive language and the goal of sub - task C is to identify the targeted individual of an offensive tweet . In the following paper , we present our contribution to sub - task A. After the related work section , we outline our conducted experiments in section 3 and further describe the used baseline model , as well as the submitted model . In section 4 we report the results of our experiments on the OLID dataset and the additionally used GermEval dataset . Section 5 discusses our results and section 6 concludes our work and describes possible future work .", "entities": [[22, 27, "DatasetName", "hate speech and offensive language"], [85, 86, "DatasetName", "Kumar"], [139, 141, "TaskName", "Language Identification"], [143, 144, "DatasetName", "OLID"], [275, 276, "DatasetName", "OLID"]]}
{"text": "Several methods and models have been presented in literature over the last decade to address the predicament of identifying hate speech , offensive language , and online aggressiveness . In the following section , we present the most notable contributions related to our work . The tweets collected by Davidson et al ( 2017 ) were divided into Hate , Offensive , and Neither . Their proposed algorithm uses unigram , bigram , and trigram tokens as features , weighted by the respective TF - IDF , as well as Part - of - Speech ( POS ) tagging and different metrics to determine the readability and sentiment of a tweet . Logisticregression and linear SVM result in the best performance for a wide range of assessed classifiers . Nobata et al ( 2016 ) collected comments from Yahoo ! Finance and News articles over a time period of one year and labeled them as either ' Abusive ' or ' Clean ' . They experimented with various different features , including n - gram , linguistic , syntactic , and distributional semantics features . Various approaches utilized deep learning models for text categorization . proposed a character - level convolutional network for text classification on large - scale datasets . Their network uses 1 - dimensional convolutional filters to extract features from different character embed - dings . Gamb\u00e4ck and Sikdar ( 2017 ) further experimented with convolutional networks in the context of online hate speech classification . Their research work compares different types of convolutional models , namely character - level , word vectors with a pretrained word2vec ( w2v ) model , randomly generated word vectors , and w2v in combination with character n - grams . The results of their experiments suggest that w2v embeddings are the most suitable for this task . Zhang et al ( 2018 ) suggest an architecture similar to our network , where a convolutional filter extracts features from pretrained word embeddings . After max pooling , the feature maps are processed using a unidirectional GRU . Their model is compared to a bag - of - n - gram model on various multi - class hate speech datasets and shows promising results . A detailed survey on different architectures , methods and features for offensive language detection is provided by Schmidt and Wiegand ( 2017 ) .", "entities": [[19, 21, "DatasetName", "hate speech"], [90, 93, "DatasetName", "Part - of"], [115, 116, "MethodName", "SVM"], [192, 194, "TaskName", "text categorization"], [203, 205, "TaskName", "text classification"], [245, 247, "DatasetName", "hate speech"], [329, 331, "TaskName", "word embeddings"], [333, 335, "MethodName", "max pooling"], [344, 345, "MethodName", "GRU"], [365, 367, "DatasetName", "hate speech"]]}
{"text": "A TF - IDF bag - of - words model as baseline approach is chosen to evaluate the performance of our model . We limit our feature space to the 10 , 000 most frequently used unigrams , bigrams , and trigrams in a corpus . Furthermore , we stem each token in the preprocessing phase and remove stopwords . We compare the performance of several classifiers , namely multinomial Naive Bayes ( NB ) , SVM , Decision Tree ( DT ) , and Logistic Regression ( LogR ) and conduct a grid search to optimize our hyper - parameters .", "entities": [[76, 77, "MethodName", "SVM"], [85, 87, "MethodName", "Logistic Regression"]]}
{"text": "The COVID - 19 pandemic , like many of the disease outbreaks that have preceded it , is likely to have a profound effect on mental health . Understanding its impact can inform strategies for mitigating negative consequences . In this work , we seek to better understand the effects of COVID - 19 on mental health by examining discussions within mental health support communities on Reddit . First , we quantify the rate at which COVID - 19 is discussed in each community , or subreddit , in order to understand levels of pandemic - related discussion . Next , we examine the volume of activity in order to determine whether the number of people discussing mental health has risen . Finally , we analyze how COVID - 19 has influenced language use and topics of discussion within each subreddit .", "entities": [[66, 67, "DatasetName", "Reddit"]]}
{"text": "The implications of COVID - 19 extend far beyond its immediate physical health effects . Uncertainty and fear surrounding the disease and its effects , in addition to a lack of consistent and reliable information , contribute to rising levels of anxiety and stress ( Torales et al , 2020 ) . Policies designed to help contain the disease also have significant consequences . Social distancing policies and lockdowns lead to increased feelings of isolation and uncertainty ( Huremovi\u0107 , 2019 ) . They have also triggered an economic downturn ( \u015e ahin et al , 2020 ) , resulting in soaring unemployment rates and causing many to experience financial stress . Therefore , in addition to the profound effects on physical health around the world , psychiatrists have warned that we should also brace for a mental health crisis as a result of the pandemic ( Qiu et al , 2020 ; Greenberg et al , 2020 ; Yao et al , 2020 ; Torales et al , 2020 ) . * Denotes equal contribution . Indeed , the literature on the impact of past epidemics indicates that they are associated with a myriad of adverse mental health effects . In a review of studies on the 2002 - 2003 SARS outbreak , the 2009 H1N1 influenza outbreak , and the 2018 Ebola outbreak , Chew et al ( 2020 ) found that anxiety , fear , depression , anger , guilt , grief , and post - traumatic stress were all commonly observed psychological responses . Furthermore , many of the factors commonly cited for inducing these responses are applicable to the COVID - 19 setting . These include : fear of contracting the disease , a disruption in daily routines , isolation related to being quarantined , and uncertainty regarding the disease treatment process and outcomes , the well - being of loved ones , and one 's economic situation . While disease outbreaks pose a risk to the mental health of the general population , research suggests that this risk is heightened for those with preexisting mental health concerns . People with mental health disorders are particularly susceptible to experiencing negative mental health consequences during times of social isolation ( Usher et al , 2020 ) . Further , as Yao et al ( 2020 ) warn , they are likely to have a stronger emotional response to the feelings of fear , anxiety , and depression that come along with COVID - 19 than the general population . Given the potential for the COVID - 19 outbreak to have devastating consequences for mental health , it is critical that we work to understand its psychological effects . In this work , we use Reddit , a popular social media platform , to study how COVID - 19 has impacted the behavior of groups of users who express mental health concerns . We analyze the content of discussions ( COVID - related discussions , psycholinguistic categories , and topics ) as well as the volume of communication ( daily user count ) and find notable changes in each category . Some of these changes appear in multiple mental health subreddits , but some are more specific to individual communities that relate to specific diagnoses . We believe that our findings can help us better understand and potentially alleviate the negative mental health effects of the pandemic ; for instance , this type of analysis could help moderators to more effectively support users through future crises . To the best of our knowledge , the method that we propose has not been used previously to study changes in mental health subreddits , and could be applied to understand the effects of other major events like political elections and natural disasters .", "entities": [[458, 459, "DatasetName", "Reddit"]]}
{"text": "In the past decade , social media has emerged as a powerful tool for understanding human behav - ior , and correspondingly mental health . A growing number of studies have applied computational methods to data collected from social media platforms in order to characterize behavior associated with mental health illnesses and to detect and forecast mental health outcomes ( see Chancellor and De Choudhury ( 2020 ) for a comprehensive review ) . Reddit is a particularly well - suited platform for studying mental health due to its semi - anonymous nature , which encourages user honesty and reduces inhibitions associated with self - disclosure ( De Choudhury and De , 2014 ) . Additionally , Reddit contains subreddits that act as mental health support forums ( e.g. , r / Anxiety , r / depression , r / SuicideWatch ) , which enable a more targeted analysis of users experiencing different mental health conditions . A number of existing works have focused on characterizing patterns of discourse within these mental health communities on Reddit . These include studies that have analyzed longitudinal trends in topic usage and word choice ( Chakravorti et al , 2018 ) , the relationship between user participation styles and topic usage ( Feldhege et al , 2020 ) , and the discourse patterns specific to self - disclosure , social support , and anonymous posting ( Pavalanathan and De Choudhury , 2015 ; De Choudhury and De , 2014 ) . Other studies of Reddit mental health communities have aimed to quantify and forecast changes in user behavior . De Choudhury et al ( 2016 ) presented a model for predicting the likelihood that users transition from discussing mental health generally to engaging in suicidal ideation . Li et al ( 2018 ) analyzed linguistic style measures associated with increasing vs decreasing participation in mental health subreddits over the course of a year . Kumar et al ( 2015 ) examined how posting activity in r / SuicideWatch changes following a celebrity suicide . Our work similarly focuses on analyzing temporal patterns in user activity , but we aim to characterize changes associated with COVID - 19 .", "entities": [[74, 75, "DatasetName", "Reddit"], [117, 118, "DatasetName", "Reddit"], [175, 176, "DatasetName", "Reddit"], [251, 252, "DatasetName", "Reddit"], [321, 322, "DatasetName", "Kumar"]]}
{"text": "Since the first cases of COVID - 19 were reported in December 2019 , there have been a number of preliminary studies of its impact on mental health . In a survey of the general public of China , a majority of respondents perceived the psychological impact of the outbreak to be moderate - to - severe and about one - third reported experiencing moderate - tosevere anxiety . Studies of the impact of COVID - 19 among residents of Liaoning Province , China ( Zhang and Ma , 2020 ) and the adult Indian population ( Roy et al , 2020 ) also found notable rates of mental distress . There is a set of studies that have examined the mental health consequences of COVID - 19 by analyzing online behaviors . Jacobson et al ( 2020 ) explored the short - term impact of stay - at - home orders in the United States by analyzing changes in the rates of mental health - related Google search queries immediately after orders were issued . Their results showed that rates of mental health queries increased leading up to the issuance of stay - at - homeorders , but then plateaued after they went into effect ; however they did not consider the longer - term implications of the stay - at - home orders on mental health . Li et al ( 2020 ) measured psycholinguistic attributes of posts on Weibo , a Chinese social media platform , before and after the Chinese National Health Commission declared COVID - 19 to be an epidemic . Their findings showed that expressions of negative emotions and sensitivity to social risks increased following the declaration . Wolohan ( 2020 ) used a Long Short - Term Memory model to classify depression among Reddit users in April 2020 , finding a higher than normal depression rate . Our work similarly aims to measure changes in online behavior as a means of understanding the relationship between COVID - 19 and mental health . However , two notable differences are : ( 1 ) instead of analyzing the short - term impact of a specific COVID - related event , we examine more general changes that have occurred during a threemonth period of the outbreak ; and ( 2 ) we focus our analysis on activity within mental health forums , which allows us to examine the impact of COVID - 19 specifically on individuals who have expressed mental health concerns .", "entities": [[167, 168, "DatasetName", "Google"], [241, 242, "DatasetName", "Weibo"], [290, 295, "MethodName", "Long Short - Term Memory"], [300, 301, "DatasetName", "Reddit"]]}
{"text": "We collect Reddit posts from three mental health subreddits using the Pushshift API 1 ( Baumgartner et al , 2020 ) : r / Anxiety , r / depression , and r / SuicideWatch , from January 2017 to May 2020 . The reasons for analyzing these three subreddits are twofold : first , over the three and a half years represented in our data , these subreddits have a significant amount of activity ( \u2265 40 posts every 1 As with other social media datasets , there may be noise in the form of API changes and data removed after collection . For the dates involved in our study , static Pushshift dump files were not yet available . day ) , making it feasible to treat daily values as a time series . Second , because the subreddits provide support for different mental health disorders , their users may have been affected differently by COVID - 19 . We separate the data into two time periods : pre - COVID ( January 1 , 2017 - February 29 , 2020 ) and post - COVID ( March 1 , 2020 - May 31 , 2020 ) , roughly delineating when COVID - 19 began to have a serious impact on those in the United States , where the majority of Reddit users are concentrated . 2 This choice of dates was informed by our analysis of the rates at which COVID - 19 related words were discussed in each subreddit ( see Section 5.1 ) , which we found hovered around 0 - 5 % before rising sharply near the beginning of March . We exclude posts where the author or text is marked as ' [ removed ] ' or ' [ deleted ] ' , because posts with deleted authors offer no value for user count metrics , and deleted content means that we are unable to capture linguistic signals ( see Section 4.1 for more details on these metrics ) . Figure 1 shows the average number of daily posts for r / Anxiety , r / depression , and r / SuicideWatch .", "entities": [[2, 3, "DatasetName", "Reddit"], [132, 134, "TaskName", "time series"], [222, 223, "DatasetName", "Reddit"], [263, 264, "DatasetName", "0"]]}
{"text": "Our goal is to identify how mental health subreddit activity has changed during the pandemic . We first create time series for a number of metrics that could be affected by the pandemic , encompassing activity levels and text content ( Section 4.1 ) . We then use a time series intervention analysis technique to determine whether there are significant changes in our metrics during the pandemic ( Section 4.2 ) .", "entities": [[19, 21, "TaskName", "time series"], [49, 51, "TaskName", "time series"]]}
{"text": "We treat the task of identifying changes in subreddit activity patterns as a time series intervention analysis problem . Our basic approach involves : ( 1 ) fitting a time series model to the pre - COVID observations for each of the metrics described above and then ( 2 ) examining how the values forecasted by the model compare to the observed values during the post - COVID time period . It is worth noting that the one study we found examining the impact of an event on activity within mental health subreddits employs a different approach : they use a t - test to compare the observations from \" before \" vs \" after \" the event ( Kumar et al , 2015 ) . However , their problem setup differs from ours in that they consider a much shorter period of time ( four weeks total ) , so the effects of seasonality ( regular changes that recur each year ) and longer - term trends are likely reduced . In contrast , we find that there is often a strong trend over time and seasonal component in our data , making a direct comparison of two time periods with a t - test unreliable . We smooth each time series and remove day - ofweek related fluctuations by computing a sevenday rolling mean over the time series . We use the Prophet model ( Taylor and Letham , 2018 ) to create a model of the period before COVID - 19 . This model was initially created by Facebook to forecast time series on their platform , such as the number of events created per day or the number of active users ; we find that our time series , also compiled from social media , have many similar properties . The Prophet model is an additive regression model with three components : y ( t ) = g ( t ) + s ( t ) + h ( t ) + t ( 1 ) The trend is encapsulated by g ( t ) , a piecewise linear model . The seasonality of the data is cap - tured by s ( t ) , which is approximated using a Fourier series . As we smooth our data on a weekly basis , we utilize only yearly seasonality , excluding the optional weekly and daily seasonality components . The third term , h ( t ) , represents holidays ; we find that adding the default list of US holidays provided by Prophet reduces error for most our our time series in the pre - COVID period , likely because the Reddit population is centered in the United States . Finally , t represents the error , in this case fluctuations in the time series that are not captured by the model . After training the model on the pre - COVID data , we predict values for the post - COVID period . If we assume that there is no change during this time period , we would expect the predicted values to be near the true values , given that the model does a good job fitting the trend and seasonal components . The model computes uncertainty intervals over the predicted values by simulating ways in which the trend may change during the period of the forecast . We use this method to compute the 95 % prediction interval . Our null hypothesis is that there has been no change in trend . In this case , we would expect 5 % of the data in the post - COVID period to fall outside of the prediction interval . Our alternate hypothesis is that there was a change in the trend of the time series ( which may be attributable to . In this case , more than 5 % of the data in the post - COVID period will fall outside of the prediction interval . We apply a one - sample proportion test to assess whether the proportion of observations outside of the prediction interval in the post - COVID period is significantly greater than 5 % . The details of this test are in Appendix C.", "entities": [[13, 15, "TaskName", "time series"], [29, 31, "TaskName", "time series"], [119, 120, "DatasetName", "Kumar"], [211, 213, "TaskName", "time series"], [228, 230, "TaskName", "time series"], [264, 266, "TaskName", "time series"], [290, 292, "TaskName", "time series"], [434, 436, "TaskName", "time series"], [446, 447, "DatasetName", "Reddit"], [468, 470, "TaskName", "time series"], [630, 632, "TaskName", "time series"]]}
{"text": "5.1 How often do people in different mental health subreddits discuss COVID - 19 ? Using our COVID - 19 lexicon ( Section 4.1 ) , we compute the percentage of posts per day that mention any words related to COVID - 19 , as shown in Figure 1 . We see that COVID - 19 began to have a serious impact on discussions in all three subreddits around the beginning of March 2020 , as is clear from the spikes in Figure 1 . Although COVID - 19 is discussed on all subreddits , we see a stark difference in the volume of discussion across each of them ; in r / Anxiety , discussion of COVID - 19 is more frequent than it is in r / depression or r / SuicideWatch , and begins earlier . Discussion When choosing the date to consider as the beginning of the post - COVID period in our time series analysis , we considered March 1st , 2020 as a sensible date , as it aligns with the time at which the United States ( where the majority of Reddit users reside ) began to take COVID - 19 seriously . March 1st closely followed the first announced COVID - 19 death in the United States on February 28th , 2020 , and preceded state lockdowns and school closures . The spikes at the beginning of March suggest that this date also reflects the time at which COVID - 19 began to have a notable impact on mental health subreddit discussions . Although most COVID - 19 related discussion started in March , we also see that a small spike in discussion rates occurred earlier in r / Anxiety . This suggests that users in this subreddit began to notice some impact from COVID - 19 in late January , when reports of lockdowns in China first appeared in the news . Based on the early start and elevated rate of COVID - 19 discussion within r / Anxiety , we conclude that all of our metrics are likely to be more strongly affected by COVID - 19 in r / Anxiety .", "entities": [[157, 160, "TaskName", "time series analysis"], [188, 189, "DatasetName", "Reddit"]]}
{"text": "To give us a better idea of how common language dimensions have changed , while the LDA - derived topics allow us to explore areas of discussion that are typically of concern in these subreddits . LIWC has been used extensively in mental health analysis , and there are some LIWC categories and LDAderived topics that overlap , such as ANXIETY , DEATH , and FAMILY , but there are also unique categories covered by each method , such as WE and MOTIVATION . For each of the metrics , we examine changes that have occurred since COVID - 19 by computing the proportion of outliers produced by our forecasting model ( see Section 4.2 ) in the post - COVID period . We acknowledge that this analysis may occasionally capture misleading changes . For example , the death keyword may yield changes in the suicide topic ( see Appendix B ) that are actually related to infectious disease , and observing increased mentions of family does not indicate the polarity of their sentiment . We leave it to future work to do a more in - depth analysis of the context surrounding specific outliers that are detected .", "entities": [[16, 17, "MethodName", "LDA"]]}
{"text": "We see changes in some categories that appear to be directly related to the new experience of living during a global pandemic under social distancing rules ; this includes the decrease in MOTION , which makes sense as people are traveling and moving around far less . The increase in categories such as BIO and BODY within r / Anxiety may reflect concerns regarding the physical health implications of COVID - 19 . Moreover , it appears that physical health concerns are especially salient for people who experience anxiety , as the rise in these categories is not present in the other subreddits . The statistically significant drop in FOCUSFU - TURE within r / Anxiety and r / depression indicates that users are less inclined to speak about their concerns for the future in light of the more pressing current concerns related to the pandemic . The sharp increase in WE ( Figure 3a ) indicates a general feeling of community and togetherness , which speaks positively to the support that those in these mental health communities are getting during the pandemic . This finding aligns with a study by Zhang and Ma ( 2020 ) on the effects of COVID - 19 on mental well - being in China , which found that participants received increased support from friends and family during the pandemic . In addition , seeking social support was listed as a common coping strategy during infections disease outbreaks by Chew et al ( 2020 ) . An increase in \" we \" is not specific to mental health communities ; researchers have found increases in usage of the pronoun during the early stages of COVID - 19 on other subreddits ( Ashokkumar and Pennebaker , 2020 ) . The decrease in I words in r / Anxiety is accompanied by an increase in r / depression ( Figure 3b ) . The increase of usage of the I pronoun is concerning because it has been shown to correlate with depression , indicating that an increase in its use could be related to worsening symptoms ( Rude et al , 2004 ) . The drop in discussion of WORK ( Figure 3c ) is unexpected , as the economic downturn could be a significant motivator of posts . The drop indicates that up to this point , the stress and change associated with adapting to working from home , or worse , losing one 's job has not been a frequent topic of discussion in these forums . This drop could be due to a decrease in work - related stressors , which have been shown to cause anxiety and depression ( Melchior et al , 2007 ; Cherry , 1978 ) , or it could simply indicate that the stressors became secondary to other concerns . It is also possible that compared to the general population , Reddit users are more likely to have jobs that can be done remotely during the pandemic , as they are more likely to have college degrees than the general population . 3", "entities": [[483, 484, "DatasetName", "Reddit"]]}
{"text": "In this study , we examined how COVID - 19 has influenced the online behavior of individuals who discuss mental health concerns by analyzing activity within the r / Anxiety , r / depression , and r / SuicideWatch communities on Reddit . We found substantial evidence of increases in anxiety ; we observed an increase in user activity in r / Anxiety , as well as significant increases in discussions of anxiety and the symptoms associated with it . Interestingly , we observed a decrease in activity within the r / depression and r / SuicideWatch subreddits . The literature on the impact of disease outbreaks on depression rates contains somewhat contradictory findings ; we therefore believe that this is an interesting area for future work . We also observed interesting changes in the content of discussions within each subreddit . Our results suggest that concerns related to COVID - 19 , such as health and family , have become more prominent discussion topics compared to other common concerns , such as work and school , which have generated relatively less discussion since the outbreak . While our findings largely confirm the warnings offered by psychiatrists regarding the potential for COVID - 19 to have an adverse effect on mental health , we also found some reason for optimism ; increases in the usage of WE as well as the INFORMATION SHARING topic ( associated with words such as \" story \" and \" hope \" ) , suggest a heightened sense of community and shared experience , which may help individuals cope with these stressful times .", "entities": [[41, 42, "DatasetName", "Reddit"]]}
{"text": "Figure 4 shows the topics identified by the LDA model . 4 : Topics identified by the LDA topic model . For each topic , we provide a summary label and the ten most probable words . We omit labels for topics whose keywords did not have a clear interpretation .", "entities": [[8, 9, "MethodName", "LDA"], [17, 18, "MethodName", "LDA"]]}
{"text": "Recent work has focused on the analysis of usergenerated text in various online venues , including labeling certain qualities of individual comments , comment pairs , or the roles of individual commenters . The largest and most extensively annotated corpus predating this work is the Internet Argument Corpus ( IAC ) , which contains approximately 480k comments in 16.5k threads from on - line forums in which users debate contentious issues . The IAC has been coded for for topic ( 3k threads ) , stance ( 2k authors ) , and agreement , sarcasm , and hostility ( 10k comment pairs ) ( Abbott et al , 2016 ; Walker et al , 2012 ) . Comments from online news articles are annotated in the SEN - SEI corpus , which contains human - authored summaries of 1.8k comments posted on Guardian articles ( Barker et al , 2016 ) . Participants described each comment with short , free - form text labels and then wrote a 150 - 250 - word comment summary with these labels . Barker et al ( 2016 ) recognized that comments have diverse qualities , many of which are coded in this work ( 3 ) , but did not explicitly collect labels of them . Previous works present a survey of how editors and readers perceive the quality of comments posted in online news publications ( Diakopoulos and Naaman , 2011 ) and review the criteria professional editors use to curate comments ( Diakopoulos , 2015 ) . The latter identifies 15 criteria for curating user - generated responses , from online and radio comments to letters to the editor . Our annotation scheme overlaps with those criteria but also diverges as we wish for the labels to reflect the nature of all comments posted on online articles instead of just the qualities sought in editorially curated comments . ERICs can take many forms and may not reflect the formal tone or intent that editors in traditional news outlets seek . Our coding scheme intersects with attributes examined in several different areas of research . Some of the most recent and relevant discourse corpora from online sources related to this work include the following : Concepts related to persuasiveness have been studied , including annotations for \" convincing - ness \" in debate forums ( Habernal and Gurevych , 2016 ) , influencers in discussions from blogs and Wikipedia ( Biran et al , 2012 ) , and user relations as a proxy of persuasion in reddit ( Tan et al , 2016 ; Wei et al , 2016 ) . Politeness was labeled and identified in Stack Exchange and Wikipedia discussions ( Danescu - Niculescu - Mizil et al , 2013 ) . Some previous work focused on detecting agreement has considered blog and Wikipedia discussions and debate forums ( Skeppstedt et al , 2016 ) . Sarcasm has been identified in a corpus of microblogs identified with the hashtag # sarcasm on Twitter ( Gonz\u00e1lez - Ib\u00e1nez et al , 2011 ; Davidov et al , 2010 ) and in online forums ( Oraby et al , 2016 ) . Sentiment has been studied widely , often in the context of reviews ( Pang and Lee , 2005 ) , and in the context of user - generated exchanges , positive and negative attitudes have been identified in Usenet discussions ( Hassan et al , 2010 ) . Other qualities of user - generated text that are not covered in this work but have been investigated before include metaphor ( Jang et al , 2014 ) and tolerance ( Mukherjee et al , 2013 ) in online discussion threads , \" dogmatism \" of reddit users ( Fast and Horvitz , 2016 ) , and argumentation units in discussions related to technology .", "entities": [[126, 127, "DatasetName", "SEN"], [424, 425, "DatasetName", "reddit"], [624, 625, "DatasetName", "reddit"]]}
{"text": "Agreement Agreement expressed with explicit phrasing ( e.g. , I disagree ... ) or implicitly , such as in Figure 2 . Annotating the target of ( dis ) agreement is left to future work due to the number of other codes the annotators need to attend to . Multiple labels can be chosen per comment , since a comment can express agreement with one statement and disagreement with another . Agreement with another commenter Disagreement with another commenter Adjunct opinion : Contains a perspective that has not yet been articulated in the thread . Audience The target audience of a comment . Reply to specific commenter : Can be explicit ( i.e. , @HANDLE ) or implicit ( not directly naming the commenter ) . The target of a reply is not coded . Broadcast message : Is not directed to a specific person ( s ) . Persuasiveness A binary label indicating whether a comment contains persuasive language or an intent to persuade . Persuasive Not persuasive Sentiment The overall sentiment of a comment , considering how the user feels with respect to what information they are trying to convey . Negative Neutral Positive Mixed : Contains both positive and negative sentiments . Tone These qualities describe the overall tone of a comment , and more than one can apply . Controversial : Puts forward a strong opinion that will most likely cause disagreement . Funny : Expresses or intends to express humor . Informative : Contributes new information to the discussion . Mean : The purpose of the comment is to be rude , mean , or hateful . Sarcastic : Uses sarcasm with either intent to humor ( overlaps with Funny ) or offend . Sympathetic : A warm , friendly comment that expresses positive emotion or sympathy . Topic The topic addressed in a comment , and more than one label can be chosen . Comments are on - topic unless either Off - topic label is selected . Off - topic with the article Off - topic with the conversation : A digression from the conversation . Personal story : Describes the user 's personal experience with the topic .", "entities": [[299, 300, "DatasetName", "emotion"]]}
{"text": "With the taxonomy described above , we coded comments from two separate domains : online news articles and debate forums . Threads from online news articles YNACC contains threads from the \" comments section \" of Yahoo News articles from April 2016 . 2 Yahoo filters comments containing hate speech ( Nobata et al , 2016 ) and abusive language using a combination of manual review and automatic algorithms , and these comments are not included in our corpus . From the remaining comments , we identified threads , which contain an initial comment and at least one comment posted in reply . Yahoo threads have a single - level of embedding , meaning that users can only post replies under a top - level comment . In total , we collected 521 , 608 comments in 137 , 620 threads on 4 , 714 articles on topics including finance , sports , entertainment , and lifestyle . We also collected the following metadata for each comment : unique user ID , time posted , headline , URL , category , and the number of thumbs up and thumbs down received . We included comments posted on a thread regardless of how much time had elapsed since the initial comment because the vast majority of comments were posted in close sequence : 48 % in the first hour after an initial comment , 67 % within the first three hours , and 92 % within the first 24 hours . We randomly selected 2 , 300 threads to annotate , oversampling longer threads since the aver -", "entities": [[48, 50, "DatasetName", "hate speech"], [58, 60, "TaskName", "abusive language"]]}
{"text": "In this paper , we have advanced a synthetic categorization of the sources for well - being and happiness . We have used a corpus of private microblogs from the ECHO application to explore how well we can map linguistic expressions of wellbeing to this classification . We have shown that FrameNet provides useful generalizations , while the linguistic pattern learner AutoSlog illustrates the details and challenges of the compositional nature of user 's descriptions of their daily experiences . Moreover , we have demonstrated that , independently , each of these methods can produce performance similar to that of conventional lexical methods with a feature space that is smaller , and , in the case of FrameNet features , psychologically grounded . Our Autoslog exploration moreover reveals a way of exploring the space of patterns that our FrameNet mapping has missed . In future work , we aim to automatically combine these two methods and bring the Autoslog patterns under the well - being categorization we have advocated here . We also plan to investigate new models with the untouched 6224 Echo posts , as well as larger public corpus like LiveJournal . In addition , we plan to explore the source of the fact that there are more positive patterns ( both as types and the tokens they capture ) than the negative ones , which directly relates to the lower Neg recall for all classifiers we tested . While we could not find any clear reason in our examination of the data , this asymmetry may indicate that markers of negativity are more syntactically distributed than our current list of patterns looks for , or perhaps less linguistically reliable .", "entities": [[51, 52, "DatasetName", "FrameNet"], [117, 118, "DatasetName", "FrameNet"], [138, 139, "DatasetName", "FrameNet"]]}
{"text": "Tired of Topic Models ? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too !", "entities": [[2, 4, "TaskName", "Topic Models"], [8, 10, "TaskName", "Word Embeddings"]]}
{"text": "Topic models are a useful analysis tool to uncover the underlying themes within document collections . The dominant approach is to use probabilistic topic models that posit a generative story , but in this paper we propose an alternative way to obtain topics : clustering pretrained word embeddings while incorporating document information for weighted clustering and reranking top words . We provide benchmarks for the combination of different word embeddings and clustering algorithms , and analyse their performance under dimensionality reduction with PCA . The best performing combination for our approach performs as well as classical topic models , but with lower runtime and computational complexity .", "entities": [[0, 2, "TaskName", "Topic models"], [23, 25, "TaskName", "topic models"], [46, 48, "TaskName", "word embeddings"], [68, 70, "TaskName", "word embeddings"], [79, 81, "TaskName", "dimensionality reduction"], [82, 83, "MethodName", "PCA"], [96, 98, "TaskName", "topic models"]]}
{"text": "Topic models are the standard approach for exploratory document analysis ( Boyd - Graber et al , 2017 ) , which aims to uncover main themes and underlying narratives within a corpus . But in times of distributed and even contextualized embeddings , are they the only option ? This work explores an alternative to topic modeling by casting ' key themes ' or ' topics ' as clusters of word types under the modern distributed representation learning paradigm : unsupervised pre - trained word embeddings provide a representation for each word type as a vector , allowing us to cluster them based on their distance in high - dimensional space . The goal of this work is not to strictly outperform , but rather to benchmark standard clustering of modern embedding methods against the classical approach of Latent Dirichlet Allocation ( LDA ; Blei et al , 2003 ) . We restrict our study to influential embedding methods and focus on centroid - based clustering algorithms as they provide a natural way to obtain the top words in each cluster based on distance from the cluster center . 1 Aside from reporting the best performing combination of word embeddings and clustering algorithm , we are also interested in whether there are consistent patterns : embeddings which perform consistently well across clustering algorithms might be good representations for unsupervised document analysis , clustering algorithms that perform consistently well are more likely to generalize to future word embedding methods . To make our approach reliably work as well as LDA , we incorporate corpus frequency statistics directly into the clustering algorithm , and quantify the effects of two key methods , 1 ) weighting terms during clustering and 2 ) reranking terms for obtaining the top J representative words . Our contributions are as follows : We systematically apply centroid - based clustering algorithms on top of a variety of pretrained word embeddings and embedding methods for document analysis . Through weighted clustering and reranking of top words we obtain sensible topics ; the best performing combination is comparable with LDA , but with smaller time complexity and empirical runtime . We show that further speedups are possible by reducing the embedding dimensions by up to 80 % using PCA .", "entities": [[0, 2, "TaskName", "Topic models"], [76, 78, "TaskName", "representation learning"], [84, 86, "TaskName", "word embeddings"], [142, 143, "MethodName", "LDA"], [198, 200, "TaskName", "word embeddings"], [258, 259, "MethodName", "LDA"], [320, 322, "TaskName", "word embeddings"], [349, 350, "MethodName", "LDA"], [378, 379, "MethodName", "PCA"]]}
{"text": "Analyzing documents by clustering word embeddings is a natural idea - clustering has been used for readability assessment ( Cha et al , 2017 ) , argument mining ( Reimers et al , 2019 ) , document classification and document clustering ( Sano et al , 2017 ) , inter alia . So far , however , clustering word embeddings has not seen much success for the purposes of topic modeling . While many modern efforts have attempted to incorporate word embeddings into the probabilistic LDA framework ( Liu et al , 2015 ; Nguyen et al , 2015 ; Das et al , 2015 ; Batmanghelich et al , 2016 ; Xun et al , 2017 ; Dieng et al , 2019 ) , relatively little work has examined the feasibility of clustering embeddings directly . Xie and Xing ( 2013 ) and Viegas et al ( 2019 ) first cluster documents and subsequently find words within each cluster for document analysis . Sridhar ( 2015 ) targets short texts where LDA performs poorly in particular , fitting GMMs to learned word2vec representations . De Miranda et al ( 2019 ) cluster using self - organising maps , but provide only qualitative results . In contrast , our proposed approach is straightforward to implement , feasible for regular length documents , requires no retraining of embeddings , and yields qualitatively and quantitatively convincing results . We focus on centroid based k - means ( KM ) , Spherical k - means ( SK ) , and k - medoids ( KD ) for hard clustering , and von Mises - Fisher Models ( VMFM ) and Gaussian Mixture Models ( GMM ) for soft clustering ; as pre - trained embeddings we consider word2vec ( Mikolov et al , 2013 ) , GloVe ( Pennington et al , 2014 ) , FastText ( Bojanowski et al , 2017 , Spherical ( Meng et al , 2019 ) , ELMo ( Peters et al , 2018 ) , andBERT ( Devlin et al , 2018 ) .", "entities": [[4, 6, "TaskName", "word embeddings"], [26, 28, "TaskName", "argument mining"], [36, 38, "TaskName", "document classification"], [58, 60, "TaskName", "word embeddings"], [80, 82, "TaskName", "word embeddings"], [85, 86, "MethodName", "LDA"], [172, 173, "MethodName", "LDA"], [303, 304, "MethodName", "GloVe"], [312, 313, "MethodName", "FastText"], [329, 330, "MethodName", "ELMo"]]}
{"text": "In traditional topic modeling ( LDA ) , the top J words are those with highest probability under each topic - word distribution . For centroid based clustering algorithms , the top words of some cluster i are naturally those closest to the cluster center c ( i ) , or with highest probability under the cluster parameters . Formally , this means choosing the set of types J as argmin J : | J | = 10 j J \uf8f4 \uf8f2 \uf8f4 \uf8f3 c ( i ) \u2212 x j 2 2 for KM / KD , cos ( c ( i ) , x j ) for SK , f ( x j | c ( i ) , \u03a3 i ) for GMM / VMFM . Our results in 6 focus on KM and GMM , as we observe that k - medoids , spherical KM and von Mises - Fisher tend to perform worse than KM and GMM ( see App . A , App . B ) . Note that it is possible to extend this approach to obtain the top topics given a document : compute similarity scores between learned topic cluster centers and all word embeddings from that particular document , and normalize them using softmax to obtain a ( non - calibrated ) probability distribution . Crucial to our method is the incorporation of corpus statistics on top of vanilla clustering algorithms , which we will describe in the remainder of this section .", "entities": [[5, 6, "MethodName", "LDA"], [201, 203, "TaskName", "word embeddings"], [212, 213, "MethodName", "softmax"]]}
{"text": "The complexity of KM is O ( tknm ) , and of GMM is O ( tknm 3 ) , for t iterations , 3 k clusters ( topics ) , n word types ( unique vocabulary ) , and m embedding dimensions . Weighted variants have a oneoff cost of weight initialization , and contribute a constant factor when recalulculating the centroid during clustering . Reranking has an additional O ( n log ( n k ) ) factor , where n k is the average number of elements in a cluster . In contrast , LDA via collapsed Gibbs sampling has a complexity of O ( tkN ) , where N is the number of all tokens , so when N n , clustering methods can potentially achieve better performance - complexity tradeoffs . Note that running ELMo and BERT over documents also requires iterating over all tokens , but only once , and not for every topic and iteration .", "entities": [[97, 98, "MethodName", "LDA"], [139, 140, "MethodName", "ELMo"], [141, 142, "MethodName", "BERT"]]}
{"text": "For readily available pretrained word embeddings such as word2vec , FastText , GloVe and Spherical , the embeddings can be considered as ' given ' as the practioner does not need to generate these embeddings from scratch . However for contextual embeddings such as ELMo and BERT , there is additional computational cost in obtaining these embeddings before clustering , which requires passing through RNN and transformer layers respectively . This can be trivially parallelised by batching the context window ( usually a sentence ) . We use standard pretrained ELMo and BERT models in our experiments and therefore do not consider the runtime of training these models from scratch .", "entities": [[4, 6, "TaskName", "word embeddings"], [10, 11, "MethodName", "FastText"], [12, 13, "MethodName", "GloVe"], [44, 45, "MethodName", "ELMo"], [46, 47, "MethodName", "BERT"], [90, 91, "MethodName", "ELMo"], [92, 93, "MethodName", "BERT"]]}
{"text": "We adopt a standard 60 - 40 train - test split for 20NG and 70 - 30 for Reuters . The top 10 words ( 3.1 ) were evaluated using normalized pointwise mutual information ( NPMI ; Bouma , 2009 ) which has been shown to correlate with human judgements ( Lau et al , 2014 ) . NPMI ranges from [ \u22121 , 1 ] with 1 indicating perfect association . The train split is used to obtain the top topic words in an unsupervised fashion ( we do not use any document labels ) , and the test split is used to evaluate the \" topic coherence \" of these top words . NPMI scores are averaged across all topics . For both datasets we use 20 topics ; which gives best NPMI out of 20 , 50 , 100 topics for Reuters , and is the ground truth number for 20NG . The NPMI scores presented in Table 1 are averaged across cluster centers initialized using 5 random seeds .", "entities": [[171, 172, "DatasetName", "seeds"]]}
{"text": "We lowercase tokens , remove stopwords , punctuation and digits , and exclude words that appear in less than 5 documents and appear in long sentences of more than 50 words , removing email artifacts and noisy token sequences which are not valid sentences . An analysis on the effect of rare word removal can be found in 6.2 . For contextualized word embeddings ( BERT and ELMo ) , sentences served as the context window to obtain the token representations . Subword representations were averaged for BERT , which performs better than just using the first subword .", "entities": [[62, 64, "TaskName", "word embeddings"], [65, 66, "MethodName", "BERT"], [67, 68, "MethodName", "ELMo"], [87, 88, "MethodName", "BERT"]]}
{"text": "Running LDA with MALLET ( McCallum , 2002 ) takes a minute , but performs no better than KM w r , which takes little more than 10 seconds on CPU using sklearn ( Pedregosa et al , 2011 ) , and 3 - 4 seconds using a simple implementation using JAX ( Bradbury et al , 2018 ) on GPU .", "entities": [[1, 2, "MethodName", "LDA"]]}
{"text": "From Table 1 , we see that reranking and weighting greatly improves clustering performance across different embeddings . As a first step to uncover why , we investigate how sensitive our methods are to restricting the clustering to only frequently appearing word types . Visualized in Fig . 3 , we find that as we vary the cutoff term frequency , thus changing the vocabulary size and allowing more rare words on the x - axis , NPMI is more affected for the models without reweighting . This suggests that reweighting using term frequency is effective for clustering without the need for ad - hoc restriction of infrequent terms - without it , all combinations perform poorly compared to LDA . In general , GMM outperforms KM for both weighted and unweighted variants averaged across all embedding methods ( p < 0.05 ) . 7", "entities": [[119, 120, "MethodName", "LDA"]]}
{"text": "For KM , extracted topics before reranking results in reasonable looking themes , but scores poorly on NPMI . Reranking strongly improves KM on average ( p < 0.02 ) for both Reuters and 20NG . Examples before and after reranking are provided in Table 2 . This indicates that while cluster centers are centered around valid themes , they are surrounded by low frequency word types . We observe that when applying reranking to GMM w the gains are much less pronounced than KM w . The top topic words before and after reranking for BERT - GMM w have an average Jaccard similarity score of 0.910 , indicating that the cluster centers learned by weighted GMMs are already centered at word types of high frequency in the training corpus .", "entities": [[96, 97, "MethodName", "BERT"]]}
{"text": "Spherical embeddings and BERT perform consistently well across both datasets . For 20NG , KM w r Spherical and LDA both achieve 0.26 NPMI . For Reuters , GMM w r BERT achieves the top NPMI score of 0.15 compared to 0.12 of LDA . Word2vec and ELMo ( using only the last layer 8 ) perform poorly compared to the other embeddings . Fast - Text and GloVe can achieve similar performance to BERT on 20NG but are slightly inferior on Reuters . Training or fine - tuning embeddings on the given data prior to clustering could potentially achieve better performance , but we leave this to future work .", "entities": [[3, 4, "MethodName", "BERT"], [19, 20, "MethodName", "LDA"], [31, 32, "MethodName", "BERT"], [43, 44, "MethodName", "LDA"], [47, 48, "MethodName", "ELMo"], [68, 69, "MethodName", "GloVe"], [74, 75, "MethodName", "BERT"]]}
{"text": "We find that our approach yields a greater diversity within topics as compared to LDA while achieving comparable coherence scores ( App . D ) . Such topics are arguably more valuable for exploratory analysis .", "entities": [[14, 15, "MethodName", "LDA"]]}
{"text": "We outlined a methodology for clustering word embeddings for unsupervised document analysis , and presented a systematic comparison of various influential embedding methods and clustering algorithms . Our experiments suggest that pretrained word embeddings ( both contextualized and non - contextualized ) , combined with tf - weighted k - means and tf - based reranking , provide a viable alternative to traditional topic modeling at lower complexity and runtime .", "entities": [[6, 8, "TaskName", "word embeddings"], [32, 34, "TaskName", "word embeddings"]]}
{"text": "We thank Aaron Mueller , Pamela Shapiro , Li Ke , Adam Poliak , Kevin Duh and the anonymous reviewers for their feedback .", "entities": [[11, 12, "MethodName", "Adam"]]}
{"text": "We present the different topics generated using LDA ( Table 7 ) and topics generated using BERT KM w r for the Reuters dataset (", "entities": [[7, 8, "MethodName", "LDA"], [16, 17, "MethodName", "BERT"]]}
{"text": "Despite the widespread success of selfsupervised learning via masked language models ( MLM ) , accurately capturing fine - grained semantic relationships in the biomedical domain remains a challenge . This is of paramount importance for entity - level tasks such as entity linking where the ability to model entity relations ( especially synonymy ) is pivotal . To address this challenge , we propose SAPBERT , a pretraining scheme that selfaligns the representation space of biomedical entities . We design a scalable metric learning framework that can leverage UMLS , a massive collection of biomedical ontologies with 4M+ concepts . In contrast with previous pipelinebased hybrid systems , SAPBERT offers an elegant one - model - for - all solution to the problem of medical entity linking ( MEL ) , achieving a new state - of - the - art ( SOTA ) on six MEL benchmarking datasets . In the scientific domain , we achieve SOTA even without taskspecific supervision . With substantial improvement over various domain - specific pretrained MLMs such as BIOBERT , SCIBERT and PUB - MEDBERT , our pretraining scheme proves to be both effective and robust . 1", "entities": [[12, 13, "DatasetName", "MLM"], [42, 44, "TaskName", "entity linking"], [83, 85, "TaskName", "metric learning"], [89, 90, "DatasetName", "UMLS"], [126, 128, "TaskName", "entity linking"]]}
{"text": "Biomedical entity 2 representation is the foundation for a plethora of text mining systems in the medical domain , facilitating applications such as literature search ( Lee et al , 2016 ) , clinical decision making ( Roberts et al , 2015 ) and relational knowledge discovery ( e.g. chemical - disease , drug - drug and protein - protein relations , Wang et al 2018 ) . The heterogeneous naming of biomedical concepts * Work conducted prior to joining Amazon . 1 For code and pretrained models , please visit : https : //github.com / cambridgeltl / sapbert . 2 In this work , biomedical entity refers to the surface forms of biomedical concepts , which can be a single word ( e.g. fever ) , a compound ( e.g. sars - cov - 2 ) or a short phrase ( e.g. abnormal retinal vascular development ) . poses a major challenge to representation learning . For instance , the medication Hydroxychloroquine is often referred to as Oxichlorochine ( alternative name ) , HCQ ( in social media ) and Plaquenil ( brand name ) .", "entities": [[16, 18, "DatasetName", "medical domain"], [34, 36, "TaskName", "decision making"], [154, 156, "TaskName", "representation learning"]]}
{"text": "Data Preparation Details for UMLS Pretraining . We download the full release of UMLS 2020AA version . 9 We then extract all English entries from the MRCONSO.RFF raw file and convert all entity names into lowercase ( duplicates are removed ) . Besides synonyms defined in MRCONSO.RFF , we also include tradenames of drugs as synonyms ( extracted from MRREL.RRF ) . After pre - processing , a list of 9 , 712 , 959 ( name , CUI ) entries is obtained . However , random batching on this list can lead to very few ( if not none ) positive pairs within a mini - batch . To ensure sufficient positives present in each mini - batch , we generate offline positive pairs in the format of ( name 1 , name 2 , CUI ) where name 1 and name 2 have the same CUI label . This can be achieved by enumerating all possible combinations of synonym pairs with common CUIs . For balanced training , any concepts with more than 50 positive pairs are randomly trimmed to 50 pairs . In the end we obtain a training list with 11 , 792 , 953 pairwise entries .", "entities": [[4, 5, "DatasetName", "UMLS"], [13, 14, "DatasetName", "UMLS"]]}
{"text": "We present SAPBERT , a self - alignment pretraining scheme for learning biomedical entity representations . We highlight the consistent performance boost achieved by SAPBERT , obtaining new SOTA in all six widely used MEL benchmarking datasets . Strikingly , without any fine - tuning on task - specific labelled data , SAPBERT already outperforms the previous supervised SOTA ( sophisticated hybrid entity linking systems ) on multiple datasets in the scientific language domain . Our work opens new avenues to explore for general domain self - alignment ( e.g. by leveraging knowledge graphs such as DBpedia ) . We plan to incorporate other types of relations ( i.e. , hypernymy and hyponymy ) and extend our model to sentence - level representation learning . In particular , our ongoing work using a combination of SAPBERT and ADAPTER is a promising direction for tackling sentence - level tasks .", "entities": [[62, 64, "TaskName", "entity linking"], [92, 94, "TaskName", "knowledge graphs"], [96, 97, "DatasetName", "DBpedia"], [122, 124, "TaskName", "representation learning"]]}
{"text": "NCBI disease ( Dogan et al , 2014 ) is a corpus containing 793 fully annotated PubMed abstracts and 6 , 881 mentions . The mentions are mapped into the MEDIC dictionary ( Davis et al , 2012 ) . We denote this dataset as \" NCBI \" in our experiments . BC5CDR ( Li et al , 2016 ) consists of 1 , 500 PubMed articles with 4 , 409 annotated chemicals , 5 , 818 diseases and 3 , 116 chemical - disease interactions . The disease mentions are mapped into the MEDIC dictionary like the NCBI disease corpus . The chemical mentions are mapped into the Comparative Toxicogenomics Database ( CTD ) ( Davis et al , 2019 ) chemical dictionary . We denote the disease and chemical mention sets as \" BC5CDRd \" and \" BC5CDR - c \" respectively . For NCBI and BC5CDR we use the same data and evaluation protocol by Sung et al ( 2020 ) . 11 MedMentions ( Mohan and Li , 2018 ) is a verylarge - scale entity linking dataset containing over 4 , 000 abstracts and over 350 , 000 mentions linked to UMLS 2017AA . According to Mohan and Li ( 2018 ) , training TAGGERONE , a very popular MEL system , on a subset of MedMentions require > 900 GB of RAM . Its massive number of mentions and more importantly the used reference ontology ( UMLS 2017AA has 3M+ concepts ) make the application of most MEL systems infeasible . However , through our metric learning formulation , SAPBERT can be applied on MedMentions with minimal effort .", "entities": [[0, 2, "DatasetName", "NCBI disease"], [30, 31, "DatasetName", "MEDIC"], [52, 53, "DatasetName", "BC5CDR"], [94, 95, "DatasetName", "MEDIC"], [98, 101, "DatasetName", "NCBI disease corpus"], [139, 140, "DatasetName", "BC5CDR"], [148, 149, "DatasetName", "BC5CDR"], [166, 167, "DatasetName", "MedMentions"], [179, 181, "TaskName", "entity linking"], [196, 197, "DatasetName", "UMLS"], [221, 222, "DatasetName", "MedMentions"], [227, 228, "MethodName", "RAM"], [240, 241, "MethodName", "ontology"], [242, 243, "DatasetName", "UMLS"], [261, 263, "TaskName", "metric learning"], [270, 271, "DatasetName", "MedMentions"]]}
{"text": "AskAPatient ( Limsopatham and Collier , 2016 ) includes 17 , 324 adverse drug reaction ( ADR ) annotations collected from askapatient.com blog posts . The mentions are mapped to 1 , 036 medical concepts grounded onto SNOMED - CT ( Donnelly , 2006 ) and AMT ( the Australian Medicines Terminology ) . For this dataset , we follow the 10 - fold evaluation protocol stated in the original paper . 12 COMETA ( Basaldella et al , 2020 ) is a recently released large - scale MEL dataset that specifically focuses on MEL in the social media domain , containing around 20k medical mentions extracted from health - related discussions on reddit.com . Mentions are mapped to SNOMED - CT . We use the \" stratified ( general ) \" split and follow the evaluation protocol of the original paper . 13", "entities": [[73, 74, "DatasetName", "COMETA"]]}
{"text": "We list all the versions of BERT models used in this study , linking to the specific versions in Tab . 5 . Note that we exhaustively tried all official variants of the selected models and the best performing ones are chosen . All BERT models refer to the BERT Base architecture in this paper . S denotes the set of all surface forms / synonyms of all concepts in C ; M denotes the set of mentions / queries . COMETA ( s.g . ) and ( z.g . ) are the stratified ( general ) and zeroshot ( general ) split respectively . model NCBI BC5CDR - d BC5CDR - c MedMentions AskAPatient COMETA @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 @1 @5 SIEVE - BASED ( D'Souza and Ng , 2015 ) 84.7 - 84.1 - 90.7 - - - WORDCNN ( Limsopatham and Collier , 2016 ) - - - - - - - - 81.4 - - - WORDGRU+TF - IDF ( Tutubalina et al , 2018 ) - - - - - - - - 85.7 - - - TAGGERONE 87.7 - 88.9 - 94.1 - OOM OOM - - - - NORMCO ( Wright et al , 2019 ) 87.8 - 88.0 - - - - - - - - - BNE ( Phan et al , 2019 ) 87.7 - 90.6 - 95.8 - - - - - - - BERTRANK ( Ji et al , 2020 ) 89 .", "entities": [[6, 7, "MethodName", "BERT"], [44, 45, "MethodName", "BERT"], [49, 50, "MethodName", "BERT"], [81, 82, "DatasetName", "COMETA"], [107, 108, "DatasetName", "BC5CDR"], [110, 111, "DatasetName", "BC5CDR"], [113, 114, "DatasetName", "MedMentions"], [115, 116, "DatasetName", "COMETA"]]}
{"text": "We thank the three reviewers and the Area Chair for their insightful comments and suggestions . FL is supported by Grace & Thomas C.H. Chan Cambridge Scholarship . NC and MB would like to", "entities": [[25, 26, "DatasetName", "Cambridge"]]}
{"text": "Practical summarization systems are expected to produce summaries of varying lengths , per user needs . While a couple of early summarization benchmarks tested systems across multiple summary lengths , this practice was mostly abandoned due to the assumed cost of producing reference summaries of multiple lengths . In this paper , we raise the research question of whether reference summaries of a single length can be used to reliably evaluate system summaries of multiple lengths . For that , we have analyzed a couple of datasets as a case study , using several variants of the ROUGE metric that are standard in summarization evaluation . Our findings indicate that the evaluation protocol in question is indeed competitive . This result paves the way to practically evaluating varying - length summaries with simple , possibly existing , summarization benchmarks .", "entities": [[1, 2, "TaskName", "summarization"], [21, 22, "TaskName", "summarization"], [103, 104, "TaskName", "summarization"], [137, 138, "TaskName", "summarization"]]}
{"text": "Automated summarization systems typically produce a text that mimics a manual summary . In these systems , an important aspect is the output summary length , which may vary according to user needs . Consequently , output length has been a common tunable parameter in pre - neural summarization systems and has been incorporated recently in few neural models as well ( Kikuchi et al , 2016 ; Fan et al , 2017 ; Ficler and Goldberg , 2017 ) . It was originally assumed that summarization systems should be assessed across multiple summary lengths . For that , the earliest Document Understand Conference ( DUC ) ( NIST , 2011 ( NIST , ) benchmarks , in 2001 ( NIST , and 2002 , defined several target summary lengths and evaluated each summary against ( manually written ) reference summaries of the same length . However , due to the high cost incurred , subsequent DUC and TAC ( NIST , 2018 ) benchmarks ( 2003 ) ( 2004 ) ( 2005 ) ( 2006 ) ( 2007 ) ( 2008 ) ( 2009 ) ( 2010 ) ( 2011 ) ( 2012 ) ( 2013 ) ( 2014 ) , as well as the more recently popular datasets CNN / Daily Mail ( Nallapati et al , 2016 ) and Gigaword ( Graff et al , 2003 ) , included references and evaluation for just one summary length per input text . Accordingly , systems were asked to produce a single summary , of corresponding length . This decision was partly supported by an observation that system rankings tended to correlate across different summary lengths ( Over et al , 2007 ) , even though , as we show in Section 2 , this correlation is limited . In this paper , we propose that the summarization community should consider resuming evaluating summarization systems over multiple length outputs , as it would allow better assessment of length - related performance within and across systems ( illustrated in Section 3 ) . To avoid the need in multiple - length reference summaries we raise the following research question : can reference summaries of a single length be used to evaluate system summaries of multiple lengths , as reliably as when using references of multiple lengths , with respect to different standard evaluation metrics ? Recently , Kikuchi et al ( 2016 ) evaluated system summaries of three different lengths against reference summaries of a single length . Yet , their evaluation methodology was not assessed through correlation to human judgment , as has been commonly done for other automatic evaluation protocols . Here , we provide a closer look into this methodology , given its potential value . As a first accessible case study , we test our research question over the DUC 2001 and 2002 data ( Section 2 ) . To the best of our knowledge , these are the only two datasets that include multiple length reference and submitted system summaries , as well as manual assessment of the latter . Our analysis reveals that , for this data and with respect to various highly utilized automatic ROUGE metrics , the answer to our question is affirmative , in terms of correlation with human judgment . Our promising results suggest repeating the assessment methodology presented here in future work , to test our question over more recent and broader summarization datasets and human evaluation schemes . This , in turn , would allow the community to feasibly resume proper evaluation and deliberate development of systems that target effective summaries across a range of lengths .", "entities": [[1, 2, "TaskName", "summarization"], [48, 49, "TaskName", "summarization"], [86, 87, "TaskName", "summarization"], [210, 214, "DatasetName", "CNN / Daily Mail"], [308, 309, "TaskName", "summarization"], [314, 315, "TaskName", "summarization"], [573, 574, "TaskName", "summarization"]]}
{"text": "We proposed the potential value of evaluating summarization systems at different summary lengths . Such evaluations would allow proper evaluation of systems ' \" length knob \" , tracking how their ranking changes across summary lengths as well as tracking the cross - length behavior of individual systems . Given that reference summaries of a single length are usually available in practice , we analyzed the potential use of reference summaries of a single length for evaluating system summaries of multiple lengths . We found , on the only two datasets readily available for such analysis , that this configuration is as reliable as the standard configuration , which evaluates each system summary against a reference of a matching length . To broadly substantiate our findings , we propose future work that would follow our assessment methodology over test samples from current datasets ( e.g. CNN / DailyMail ) , judging performance of current systems and utilizing current manual evaluation protocols . This would require preparing , for limited samples , additional manually crafted summaries of several lengths and manually evaluating system summaries of corresponding lengths . Using such data , it will be possible to repeat our analysis and test the broader validity of the single - reference - length configuration . If broadly assessed , it will be possible to start evaluating system summaries of multiple lengths over most currently available datasets , leveraging the available single - length reference summaries . Fu - ture benchmarks could require systems to produce different length outputs , while feasibly evaluating them using the existing , single length , reference summaries . This , in turn , is likely to drive research to better address the need for producing high quality summaries flexibly across a range of summary lengths , a dimension that has been disregarded for long .", "entities": [[7, 8, "TaskName", "summarization"]]}
{"text": "Being able to perform in - depth chat with humans in a closed domain is a precondition before an open - domain chatbot can ever be claimed . In this work , we take a close look at the movie domain and present a large - scale high - quality corpus with fine - grained annotations in hope of pushing the limit of moviedomain chatbots . We propose a unified , readily scalable neural approach which reconciles all subtasks like intent prediction and knowledge retrieval . The model is first pretrained on the huge general - domain data , then finetuned on our corpus . We show this simple neural approach trained on high - quality data is able to outperform commercial systems replying on complex rules . On both the static and interactive tests , we find responses generated by our system exhibits remarkably good engagement and sensibleness close to human - written ones . We further analyze the limits of our work and point out potential directions for future work 1 .", "entities": [[22, 23, "TaskName", "chatbot"]]}
{"text": "Being able to converse like humans in a closed domain is a precondition before an intelligent opendomain chatbot , which further requires transiting among various domains , can be designed Su et al , 2020 ) . Nonetheless , even if constrained in a specific domain , current chatbots are still far from satisfactory . Unlike task - oriented systems that can be relatively well - resolved with handcrafted templates , human conversations feature a complex mixture of QA , chitchat , recommendation , etc . without pre - specified goals or conversational patterns ( Dodge et al , 2016 ; Akasaki and Kaji , 2017 ; . Selecting proper domain knowledge to support response generation at all the different situations is challenging ( Milward and Beveridge , 2003 ; Shen et al , 2019 ) . In this work , we direct our focus to the movie domain and present a large - scale , crowdsourced Chinese dataset with fine - grained annotations in hope of boosting the study towards a human - like closed - domain chatbot . A variety of dialogue datasets with grounded domain knowledge have already been proposed . However , they are collected either through ( 1 ) online forum crawling ( Dodge et al , 2016 ; Ghazvininejad et al , 2018 ; Liu et al , 2018 ; Zhou et al , 2018a ; , which are noisy , multi - party , mostly contain only single - exchange QA , or ( 2 ) crowdsourced ( Zhu et al , 2017 ; Zhou et al , 2018b ; Moon et al , 2019 ; , which are small - scale and often created in an overconstrained setting like teacher - student ( Moghe et al , 2018 ) . Even for datasets crowd - sourced in unconstrained scenarios , suggestive domain knowledge is provided for humans before an utterance is provided . This would inevitably prompt humans to utilize these knowledge deliberately , yielding unnatural conversations simply connecting the knowledge ( Dinan et al , 2019 ; . We show examples from other datasets in Appendix Table 10 . In comparison , our dataset has the following advantages : 1 . Natural : Crowdworkers chat in a free environment without further constraint or prompt in order to mimic the human daily conversations to the largest extent . 2 . Large - scale : It covers 270k human dialogues with over 3 M utterances , which is at least one order of magnitude larger than all the other crowd - sourced datasets . 3 . Annotated : Utterances are labeled with entity information and dialogue acts classified into 15 fine - grained aspects , based on which linked into different types of knowledge . Different from previous crowd - sourced works , our annotation process is conducted posteriori so that it will not interfere with human conversations , e.g. , prompt them to overuse suggested knowledge . Built upon our dataset , we propose a simple unified language model approach to push the limits of movie - domain chatbots . The model is first pretrained on 2.2B words collected from various general - domain conversational resources , then finetuned on the movie dataset with additional knowledge and dialogue acts incorporated . We pool all components like intent prediction and knowledge retrieval into a sequence prediction task and solve them with a unified language model architecture . It avoids designing complex systems for individual components separately and all subtasks can be easily trained simultaneously ( Hosseini - Asl et al , 2020 ; Peng et al , 2020 ) . We show our simple unified approach outperforms strong baselines for each separate subtask . Knowledge retrieval , dialogue acts prediction and general - domain pretrain benefit from each other and altogether bring improvement to the generation quality . In the online interactive test , our best model succeeds at chatting with humans for 11.4 turns without being detected to be a machine , outperforming even commercial chatbots Mitsuku 2 and Microsoft XiaoIce 3 which further rely on complex rules . By analyzing the limitations of our model , we find it especially has difficulty at dealing with in - depth discussions over long turns . Future research can consider employing larger knowledge base or explicit state tracking . In summary , our main contributions are ( 1 ) presenting a high - quality , large - scale Chinese conversational corpus with fine - grained annotations in the movie domain to benefit future study , ( 2 ) showing that a simple unified neural model trained on the high - quality dataset can approach human performance and even outperform commercial systems replying on complex rules , and ( 3 ) studying the shortcomings of current techniques , providing suggestive directions for future research .", "entities": [[3, 4, "DatasetName", "converse"], [17, 18, "TaskName", "chatbot"], [114, 116, "TaskName", "response generation"], [178, 179, "TaskName", "chatbot"]]}
{"text": "Language models have demonstrated impressive performance as a universal learner across NLP tasks ( Shen et al , 2017 ; Peters et al , 2018 ; Radford et al , 2019 ; Brown et al , 2020 ) . Inspired by this , our dialogue generation model is implemented as a Transformer - based language model like GPT2 ( Radford et al , 2019 ; . It contains a pipeline process of movie tracker , intent prediction , knowledge retrieval and text gener - 7 We only consider recommending movies as for the DA about recommendation . Recommending other aspects require assembling recommendation systems of different domains , which is beyond the scope of this paper .", "entities": [[39, 40, "DatasetName", "Inspired"], [44, 46, "TaskName", "dialogue generation"], [51, 52, "MethodName", "Transformer"], [102, 104, "TaskName", "recommendation systems"]]}
{"text": "The knowledge retrieval component is similar to the classical DSSM model ( Huang et al , 2013 ) . We replace the MLP with our language model encoder to get the embedding for knowledge . Note that we only select knowledge from the current movie , which can be obtained from the movie tracker , so it is possible to \" will be fed to the language model to generate the response . To make it consistent with the pretrained general - domain dialogue , the position embedding of the decoded response will skip the concatenated intent and knowledge and directly follow the dialogue context . We find this beneficial when combined with pretrained models . The objective also follows the pretrained model mixing maximum lilkelihood and unlikelihood training .", "entities": [[22, 23, "DatasetName", "MLP"]]}
{"text": "We present MovieChats : a movie - domain chatbot built upon a large - scale , high - quality conversational corpus with fine - grained annotations . The model can be trained end - to - end with a simple unified language model architecture . We show that our model , powered by well - defined knowledge grounding , is able to approach human performance in some perspective , though still lagged behind when it comes to dealing with detailed knowledge or long - turn consistency .", "entities": [[8, 9, "TaskName", "chatbot"]]}
{"text": "We thank anonymous reviewers and the dialogue system team at Wechat AI for their valuable comments . Xiaoyu Shen was funded by IMPRS - CS fellowship . Ernie Chang is supported by SFB 248 \" Foundations of Perspicuous Software Systems \" ( E2 ) .", "entities": [[10, 11, "DatasetName", "Wechat"], [24, 25, "DatasetName", "CS"]]}
{"text": "Suicide is amongst the most pressing public health issues facing today 's society , stressing the need for rapid and effective detection tools . As people are increasingly self - expressing their distress on social media , an unprecedented volume of data is currently available to detect a person 's suicide risk ( Roy et al , 2020 ; Tadesse et al , 2020 ; Luo et al , 2020 ) . In this shared task , we aim to construct tools to identify suicidal Twitter users ( who attempted suicide ) based on their tweets collected from spans of 30 - days ( subtask 1 ) and six months ( subtask 2 ) before the adverse event 's occurrence date ( Macavaney et al , 2021 ) . The small number of users in the labeled collections of subtask 1 ( 57 suicidal/57 control ) and subtask 2 ( 82 suicidal/82 control ) and the scarcity of tweets for some users pose these tasks as small - dataset classification challenges . On that note , Coppersmith et al ( 2018 ) reported high performance with deep learning ( DL ) methods on these collections after enriching them with additional data ( 418 suicidal/418 control ) . When formulating the strategy to attack the challenge , we were motivated by the real - life applicability of the methods . Some social media domains already started implementing auto - detection tools to prevent suicide ( Ji et al , 2020 ) . These tools continuously monitor the presence of suicide risk in new posts . Therefore , we chose to train the models at the tweet level . Next , we develop a majority voting scheme over the classified tweets to report an overall suicide risk score for a user . We employ simple machine learning ( ML ) methods and create an ensemble . We also experiment with DL methods to assess whether complexity would improve the results . Since successful ML applications thrive on feature engineering ( Domingos , 2012 ) , we conduct feature selection to evaluate and determine the best feature sets for the models . Our experiments suggest that majority voting ( MV ) over tweet - level classification scores is a viable approach for the short - term prediction of suicide risk . We observe that DL methods require plentiful resources despite the small size of the datasets . Simple ML methods with feature selection return satisfactory results , and the performance further improves by the ensemble classifier . We also observe that the MV approach falls short on the six - month - long data regardless of the applied model . Yet this limitation provides the invaluable insight that suicidal ideation signals are more significant when the date of the suicidal event is closer , which stresses the need for more complex , noise immune models for longer time - spanning data . In this context , we consider a noise - immune model as a suicidal ideation detection model that is not affected by tweets lacking suicidal ideation .", "entities": [[335, 337, "TaskName", "feature engineering"], [345, 347, "MethodName", "feature selection"], [408, 410, "MethodName", "feature selection"]]}
{"text": "Before ML experiments , we initially explore a simple approach that constructs graphs from training sets and computes how well the given texts match the graphs ( Bayram et al , 2018 ) . However , tweets proved to be unfit for the method due to low word counts . As most ML methods depend on learning from features , we select n - gram features where n \u2264 2 for their popularity in suicide studies ( O'Dea et al , 2015 ; De Choudhury et al , 2016 ; Pestian et al , 2020 ) . For bigrams ( n = 2 ) , we apply a sliding window over concurrent words using the NLTK library ( Bird et al , 2009 ) . Next , we eliminate infrequent n - grams from the training set to reduce uninformative features ( occurring in \u22643 tweets in 30days , \u226410 tweets in 182 - days training sets ) . Subsequently , we scale the features by row - normalizing them with the root of the sum of the square ( i.e. variation ) of the feature values . Among the popular ML methods in suicide literature is logistic regression ( LR ) ( Walsh et al , 2017 ; De Choudhury et al , 2016 ; O'Dea et al , 2015 ) . We select the \" liblinear \" solver with default settings for being recommended for small datasets ( Buitinck et al , 2013 ) . To cover diverse mathematical frameworks and assumptions , we also include two naive Bayes methods ( Gaussian ( GNB ) and Multinomial ( MNB ) with default settings ) ( Buitinck et al , 2013 ) . We also experiment with K - Nearest Neighbors with different distance ( uniform , weighted ) and neighborhood ( k { 3 , 5 , 8 } ) settings , but we eliminate it for low within - dataset results . Similarly , ensemblelearning methods ( Adaboost , XGBoost , Random Forest ) also return underwhelming performance despite the parameter tuning , and thus , were eliminated . Additionally , we evaluate support vector machines ( SVM ) for their popularity in suicide research ( Zhu et al , 2020 ; Pestian et al , 2020 ; O'Dea et al , 2015 ) . SVM with rbf kernel proves to be successful but requires costly parameter tuning , while linear SVM ( lSVM ) shows success on withindataset evaluations with less cost . Consequently , we select lSVM of sklearn ( default settings ) for the shared task ( Buitinck et al , 2013 ) , which returns only binary classification results . To convert them to probabilities , we apply probability calibration with logistic regression ( CalibratedClassifierCV ) . Feature selection : Following the ML method selections , we evaluate the effect of feature selection on ML performance . To compute feature importance scores , we also use the LR . For each selected number of features , we gather top suicidal and control features . Next , we train and evaluate the ML methods in a leave - one - out ( LOO ) framework using those features . The feature selection results of the selected ML methods for two subtasks are in Figure 2 . We select the best ML models from these plots . Experiments with Ensemble : Ensemble classifiers previously showed success in ML challenges ( Niculescu - Mizil et al , 2009 ) . Since every classifier renders predicted probabilities for every data point , we build an ensemble classifier to optimize the results of four selected ML methods ( LR , GNB , MNB , lSVM ) . We adopt a weighting ensemble method where the weight of each classifier is set proportional to its performance ( Rokach , 2010 ) . We call this method weighted Ensemble ( wEns ) .", "entities": [[197, 199, "MethodName", "logistic regression"], [288, 292, "MethodName", "K - Nearest Neighbors"], [360, 361, "MethodName", "SVM"], [388, 389, "MethodName", "SVM"], [404, 405, "MethodName", "SVM"], [458, 460, "MethodName", "logistic regression"], [464, 466, "MethodName", "Feature selection"], [478, 480, "MethodName", "feature selection"], [486, 488, "TaskName", "feature importance"], [536, 538, "MethodName", "feature selection"]]}
{"text": "To measure whether re - sults would improve with complexity , we also evaluate shallow DL methods . We use the pre - trained transformer model Bert - base - uncased ( Devlin et al , 2018 ) to catch the linguistics features of the tweets . The embeddings are then fed to a DL Recurrent Units - based architecture to learn text sequence orders . We experiment with two types of recurrent neural networks ( RNNs ) : Long Short Term Memory ( LSTM ) ( Gers et al , 1999 ) , and Gated Recurrent Unit ( GRU ) known for overcoming vanishing and exploding gradient problems faced by vanilla RNNs during training ( Cho et al , 2014 ) . After assessing various configurations of both architectures , we settle on a multi - layer bi - directional GRU with the following characteristics : embedding dimen - sion=256 , number of layers=2 , batch size=32 . We call this model GRU - Bert . We include a drop - out to regularise learning and a fully connected layer with a Sigmoid activation to produce the classification for each tweet . Finally , we include the same majority voting framework to infer the classification on the user level . We use Pytorch ( Paszke et al , 2019 ) and scikit - learn ( Buitinck et al , 2013 ) libraries for implementation .", "entities": [[84, 85, "MethodName", "LSTM"], [95, 98, "MethodName", "Gated Recurrent Unit"], [99, 100, "MethodName", "GRU"], [141, 142, "MethodName", "GRU"], [163, 164, "MethodName", "GRU"], [183, 185, "MethodName", "Sigmoid activation"]]}
{"text": "In subtask 1 , the test set results show that feature selection can considerably enhance the performance of ML models compared to the baseline . We also find that the ensemble classifier is comparably better than the baseline in this subtask . Meanwhile , though the baseline of CLPsych2021 is the same as our LR , our additional MV and feature selection together enable LR to substantially outperform the baseline . These successes of simple ML methods indicate that a collection of tweets from within the 30 - days of a suicidal event is good enough to capture the existence of suicidal ideation , which is an important finding for future real - life suicide prevention applications . In contrast to the observations from subtask 1 , our test results on subtask 2 are unsatisfactory . Yet , they provide the valuable insight that suicidal signals are more significant in the short - term , and older tweets lacking suicidal ideation generate noise . This insight suggests the need to account for a time - domain aspect . To investigate the viability of this claim , we experiment with a simple time - decay coefficient in the MV framework and evaluate it through LR on the test set . We multiply each vote by the coefficient 2 \u2212timeDif f half Lif e where timeDif f is the number of days between the current and last tweets , and half Lif e ( = 7 days ) is a hyperparameter that reflects the weight of a vote in the final suicide risk score of a user . Initial experiments show that even this simple time - decay coefficient improves the test results significantly . This observation suggests that tweet dates are critical features for this subtask and should be included in future work . Notwithstanding , on both subtasks , the shallow DL methods we experimented with perform poorly . These results could be attributed to overfitting on the small dataset and noise sensitivity for the larger time - spanning dataset . Additionally , regardless of the dataset size , these methods proved to be computationally expensive . As within - dataset experiments using simple ML methods outperformed these expensive shallow DL methods , we excluded the latter from the test set evaluation . Future work on DL will include deeper , more complex , and noise immune methods that could integrate Convolutional neural networks ( CNN ) , deeper LSTM or GRU layers , and experiments with various word embedding models . If we compare our findings with those in Coppersmith et al ( 2018 ) , we observe different results in terms of short - term versus long - term dataset classifications . We attribute these different outcomes to the fact that the original study optimizes the design for detecting trait - level ( relevant to risk for any point in time ) suicide risk when we endeavor to identify suicidal ideation at the state level ( immediate risk presence ) . This design choice , along with tweet - level classification , enabled our model to recognize suicidal nuances in short - term tweets . Meanwhile , we were unable to detect any suicidal ideation through manual inspection ( reading and interpreting the tweets ) over most of these tweets due to their noisy and ambiguous nature .", "entities": [[10, 12, "MethodName", "feature selection"], [60, 62, "MethodName", "feature selection"], [409, 410, "MethodName", "LSTM"], [411, 412, "MethodName", "GRU"]]}
{"text": "In this shared task , we investigate various models for identifying suicide risk based on user 's tweets . Inspired by real - life applications , we focus on assessing suicide risk on the tweet level . Experimental results reveal that the ensemble classifier can identify suicidal users from 30 - days tweets with a high performance rate , demonstrating the power of majority voting over tweet - level classifications for short - term suicide risk detection . Meanwhile , we construe from the underwhelming results on the six - month dataset that these models were more sensitive to the signals relevant to short term risk than those relevant to long term risk . In future work , we will incorporate a temporal aspect to improve the noise immunity of our models , and we will continue experimenting with more complex models .", "entities": [[19, 20, "DatasetName", "Inspired"]]}
{"text": "Preface : General Chair", "entities": [[2, 3, "DatasetName", "General"]]}
{"text": "In addition , oral presentations were shortened to fourteen ( twelve ) minutes for long ( short ) papers , plus time for questions . While this places a greater demand on speakers to be concise , we believe it is worth the effort , allowing far more work to be presented orally . We also took advantage of the many halls available and expanded the number of parallel talks to five during most of the conference sessions . In keeping with changes introduced in the ACL community from last year , we continued the practice of recognizing outstanding papers at ACL . The 22 outstanding papers ( 15 long , 7 short , 1.6 % of submissions ) represent a broad spectrum of exciting contributions and have been specially placed on the final day of the main conference where the program is focused into two parallel sessions of these outstanding contributions . From these , a best paper and a best short paper those will be announced in the awards session on Wednesday afternoon . Chris has already mentioned our introduction of the chairs ' blog 2 , where we strove to make the selection process of the internal workings of the scientific committee more transparent . We have publicly documented our calls for area chairs , reviewers and accepted papers selection process . Via the blog , we communicated several innovations in the conference organization workflow , of which we would call attention to two key ones here . In the review process , we pioneered the use of the Toronto Paper Matching System , a topic model based approach to the assignment of reviewers to papers . We hope this decision will spur other program chairs to adopt the system , as increased coverage will better the reviewer / submission matching process , ultimately leading to a higher quality program . For posterity , we also introduced the usage of hyperlinks in the bibliography reference sections of papers , 1 These numbers exclude papers that were not reviewed due to formatting , anonymity , or double submission violations or that were withdrawn prior to review , which was unfortunately a substantial number . 2 https://chairs - blog.acl2017.org/ viii and have worked with the ACL Anthology to ensure that digital object identifiers ( DOIs ) appear in the footer of each paper . These steps will help broaden the long - term impact of the work that our community has on the scientific world at large . There are many individuals we wish to thank for their contributions to ACL 2017 , some multiple times : The 61 area chairs who volunteered for our extra duty . They recruited reviewers , led discussions on each paper , replied to authors ' direct comments to them and carefully assessed each submission . Their input was instrumental in guiding the final decisions on papers and selecting the outstanding papers . Our full program committee of BUG hard - working individuals who reviewed the conference 's 1 , 318 submissions ( including secondary reviewers ) . TACL editors - in - chief Mark Johnson , Lillian Lee , and Kristina Toutanova , for coordinating with us on TACL presentations at ACL . Noah Smith and Katrin Erk , program co - chairs of ACL 2016 and Ani Nenkova and Owen Rambow , program co - chairs of NAACL 2016 , who we consulted several times on short order for help and advice . Wei Lu and Sameer Singh , our well - organized publication chairs , with direction and oversight from publication chair mentor Meg Mitchell . Also , Christian Federmann who helped with the local handbook . The responsive team at Softconf led by Rich Gerber , who worked quickly to resolve problems and who strove to integrate the use of the Toronto Paper Matching System ( TPMS ) for our use . Priscilla Rasmussen and Anoop Sarkar and the local organization team , especially webmaster Nitin Madnani . Christopher Calliston - Burch , our general chair , who kept us coordinated with the rest of the ACL 2017 team and helped us free our time to concentrate on the key duty of organizing the scientific program . Key - Sun Choi , Jing Jiang , Graham Neubig , Emily Pitler , and Bonnie Webber who carefully reviewed papers under consideration for best paper recognition . Our senior correspondents for the blog , who contributed guest posts and advice for writing and reviewing : Waleed Ammar , Yoav Artzi , Tim Baldwin , Marco Baroni , Claire Cardie , Xavier Carreras , Hal Daum\u00e9 , Kevin Duh , Chris Dyer , Marti Hearst , Mirella Lapata , Emily M. Bender , Aur\u00e9lien Max , Kathy McKeown , Ray Mooney , Ani Nenkova , Joakim Nivre , Philip Resnik , and Joel Tetreault . Without them , the participation of the community through the productive comments , and without you the readership , our blog for disseminating information about the decision processes would not have been possible and a success . With twin upward trends in the interest in computational linguistics and natural language processing and the size of our annual meeting , ACL has begun the practice of recognizing outstanding papers that represent a select cross - section of the entire field , as nominated by reviewers and vetted by the area chairs and program co - chairs . These papers have been centrally located in the program , on the last day of our meeting , in a more focused two parallel tracks format . This year , we have nominated 15 long papers and 7 short papers , representing 1.8 % of all submissions and approximately 5 % of the accepted ACL program . Congratulations , authors ! ( in alphabetical order by first author surname ) Long Papers Jan Buys and Phil Blunsom . Robust Incremental Neural Semantic Graph Parsing . Xinchi Chen , Zhan Shi , Xipeng Qiu and Xuanjing Huang . Adversarial Multi - Criteria Learning for Chinese Word Segmentation .", "entities": [[495, 496, "DatasetName", "BUG"], [1012, 1015, "TaskName", "Chinese Word Segmentation"]]}
{"text": "The computational linguistics and natural language processing community is experiencing an episode of deep fascination with representation learning . Like many other presenters at this conference , I will describe new ways to use representation learning in models of natural language . Noting that a data - driven model always assumes a theory ( not necessarily a good one ) , I will argue for the benefits of language - appropriate inductive bias for representation - learning - infused models of language . Such bias often comes in the form of assumptions baked into a model , constraints on an inference algorithm , or linguistic analysis applied to data . Indeed , many decades of research in linguistics ( including computational linguistics ) put our community in a strong position to identify promising inductive biases . The new models , in turn , may allow us to explore previously unavailable forms of bias , and to produce findings of interest to linguistics . I will focus on new models of documents and of sentential semantic structures , and I will emphasize abstract , reusable components and their assumptions rather than applications . In this talk I will argue that in order to render electronic data more accessible to individuals and computers alike , new types of translation models need to be developed . I will focus on three examples , text simplification , source code generation , and movie summarization . I will illustrate how recent advances in deep learning can be extended in order to induce general representations for different modalities and learn how to translate between these and natural language .", "entities": [[16, 18, "TaskName", "representation learning"], [34, 36, "TaskName", "representation learning"], [230, 232, "TaskName", "text simplification"], [234, 236, "TaskName", "code generation"], [239, 240, "TaskName", "summarization"]]}
{"text": "Mirella Lapata is professor of natural language processing in the School of Informatics at the University of Edinburgh . Her research focuses on getting computers to understand , reason with , and generate . She is as an associate editor of the Journal of Artificial Intelligence Research and has served on the editorial boards of Transactions of the ACL and Computational Linguistics . She was the first recipient of the Karen Sparck Jones award of the British Computer Society , recognizing key contributions to NLP and information retrieval . She received two EMNLP best paper awards and currently holds a prestigious Consolidator Grant from the European Research Council . xxiii", "entities": [[86, 88, "TaskName", "information retrieval"]]}
{"text": "Pretrained language models have served as the backbone for many state - of - the - art NLP results . These models are large and expensive to train . Recent work suggests that continued pretraining on task - specific data is worth the effort as pretraining leads to improved performance on downstream tasks . We explore alternatives to full - scale task - specific pretraining of language models through the use of adapter modules , a parameter - efficient approach to transfer learning . We find that adapter - based pretraining is able to achieve comparable results to task - specific pretraining while using a fraction of the overall trainable parameters . We further explore direct use of adapters without pretraining and find that the direct finetuning performs mostly on par with pretrained adapter models , contradicting previously proposed benefits of continual pretraining in full pretraining fine - tuning strategies . Lastly , we perform an ablation study on task - adaptive pretraining to investigate how different hyperparameter settings can change the effectiveness of the pretraining .", "entities": [[0, 3, "TaskName", "Pretrained language models"], [81, 83, "TaskName", "transfer learning"], [141, 143, "TaskName", "continual pretraining"]]}
{"text": "Pre - trained language model We use RoBERTa ( Liu et al , 2019 ) , a Transformer - based language model that is pre - trained on a massive text corpus , following Gururangan et al , 2020 . RoBERTa is an extension of BERT ( Devlin et al , 2019 ) with optimized hyperparameters and a modification of the pretraining objective , which excludes next sentence prediction and only uses the randomly masked tokens in the input sentence . To evaluate the performance of RoBERTa on a certain task , a classification layer is appended on top of the language model after the pretraining and all the parameters in RoBERTa are trained in a supervised way using the label of the dataset . In this paper , training word representations using RoBERTa on a masked language modeling task will be referred to as pretraining . Further , taking this pretrained model and adding a classification layer with additional updates to the language model parameters will be referred to as fine - tuning . Task - adaptive pretraining ( TAPT ) Although RoBERTa achieves strong performance by simply fine - tuning the PLMs on a target task , there can be a distributional mismatch between the pretraining and target corpora . To address this issue , pretraining on the target task or the domain of the target task can be usefully employed to adapt the language models to the target task and it further improves the performance of the PLMs . Such methods can be referred to as Domain - Adaptive Pretraining ( DAPT ) or Task Adaptive - Pretraining ( TAPT ) ( Gururangan et al , 2020 ) . In this paper , we limit the scope of our works to TAPT as domain text corpus is not always available for each task , whereas TAPT can be easily applied by directly using the dataset of the target task while its performance often matches with DAPT ( Gururangan et al , 2020 ) . In TAPT , the second phase of pretraining is per - Figure 1 : The adapter achitecture in the Transformer layer ( Pfeiffer et al , 2020a ) formed with RoBERTa using the unlabeled text corpus of the target task , and then it is fine - tuned on the target task . Adapter Adapter modules have been employed as a feature extractor in computer vision ( Rebuffi et al , 2017 ) and have been recently adopted in the NLP literature as an alternative approach to fully fine - tuning PLMs . Adapters are sets of new weights that are typically embedded in each transformer layer of PLMs and consist of feed - forward layers with normalizations , residual connections , and projection layers . The architectures of adapters vary with respect to the different configuration settings . We use the configuration proposed by Pfeiffer et al , 2020a in Figure 1 , which turned out to be effective on diverse NLP tasks , and add the adapter layer to each transformer layer . Pfeiffer et al , 2020c use two types of adapter : language - specific adapters and taskspecific adapters for cross - lingual transfer . These two types of adapter modules have similar architecture as in Figure 1 . However , the language adapters involve invertible adapters after the embedding layer to capture token - level language representation when those are trained via masked language modeling in the pretraining stage , whereas the task adapters are simply embedded in each transformer layer and trained in the fine - tuning stage to learn the task representation . Following Pfeiffer et al , 2020c , we employ language adapter modules with invertible adapter layers to perform pretraining adapters on the unlabeled target dataset . However , we perform fine - tuning pre - trained parameters of the language adapter modules for evaluation to align with ( Maas et al , 2011 ) ) and low - resource ( CHEMPROT ( Kringelum et al , 2016 ) , ACL - ARC ( Jurgens et al , 2018 ) , SCIERC ( Luan et al , 2018 ) , HYPERPARTISAN ( Kiesel et al , 2019 ) settings . TAPT , whereas Pfeiffer et al , 2020c employ both the language and the task adapters by stacking task adapters on top of the language adapters .", "entities": [[7, 8, "MethodName", "RoBERTa"], [17, 18, "MethodName", "Transformer"], [40, 41, "MethodName", "RoBERTa"], [45, 46, "MethodName", "BERT"], [86, 87, "MethodName", "RoBERTa"], [111, 112, "MethodName", "RoBERTa"], [133, 134, "MethodName", "RoBERTa"], [136, 139, "TaskName", "masked language modeling"], [183, 184, "MethodName", "RoBERTa"], [356, 357, "MethodName", "Transformer"], [367, 368, "MethodName", "RoBERTa"], [390, 391, "MethodName", "Adapter"], [391, 392, "MethodName", "Adapter"], [531, 535, "TaskName", "cross - lingual transfer"], [574, 577, "TaskName", "masked language modeling"], [667, 668, "DatasetName", "CHEMPROT"], [676, 679, "DatasetName", "ACL - ARC"], [687, 688, "DatasetName", "SCIERC"]]}
{"text": "We now propose an adapter - based approach that is a parameter efficient variant of Task - Adaptive Pretraining ( TAPT ) and measure the margin of the performance between the pre - trained adapter model and the adapter model without pretraining . For pretraining adapters , we added the adapter module in each transformer layer of RoBERTa using adaptertransformer ( Pfeiffer et al , 2020b ) 1 and continued pretraining all the weights in adapter layers on target text corpus while keeping the original parameters in RoBERTa fixed . After finishing the second phase of pretraining , we performed fine - tuning of RoBERTa by training the weights in the adapters and the final classification layers while keeping all of the parameters in RoBERTa frozen .", "entities": [[57, 58, "MethodName", "RoBERTa"], [87, 88, "MethodName", "RoBERTa"], [104, 105, "MethodName", "RoBERTa"], [124, 125, "MethodName", "RoBERTa"]]}
{"text": "Following Gururangan et al , 2020 2 , we consider 8 classification tasks from 4 different domains . The specification of each task is shown in Table 1 . We covered news and review texts that are similar to the pretraining corpus of RoBERTa as well as scientific domains in which text corpora can have largely different distributions from those of RoBERTa . Furthermore , the pretraining corpora of the target tasks include both large and small cases to determine whether the adapter - based approach can be applicable in both low and high - resource settings . 1 https://github.com/Adapter - Hub/ adapter - transformers 2 Downloadble link for task dataset : https://github . com / allenai / dont - stop - pretraining", "entities": [[43, 44, "MethodName", "RoBERTa"], [61, 62, "MethodName", "RoBERTa"]]}
{"text": "Our work demonstrates that adapters provide a competitive alternative to large - scale task - adaptive pretraining for NLP classification tasks . We show that it is possible to achieve similar performance to TAPT with pretraining training just 1.32 % of the parameters through pretraining with adapters . However , the most computationally efficient option is to skip pretraining and only perform fine - tuning with adapters . We found that skipping pretraining altogether and just fine - tuning with adapters outperforms or performs mostly on par with TAPT and the adapter model with pretraining across our tasks while substantially reducing the training time . Xiang Zhang , Junbo Zhao , and Yann LeCun . 2015 . Character - level convolutional networks for text classification . In Advances in Neural Information Processing Systems , volume 28 , pages 649 - 657 . Curran Associates , Inc.", "entities": [[123, 125, "TaskName", "text classification"]]}
{"text": "Comparing CRF and LSTM performance on the task of morphosyntactic tagging of non - standard varieties of South Slavic languages", "entities": [[1, 2, "MethodName", "CRF"], [3, 4, "MethodName", "LSTM"]]}
{"text": "This paper presents two systems taking part in the Morphosyntactic Tagging of Tweets shared task on Slovene , Croatian and Serbian data , organized inside the VarDial Evaluation Campaign . While one system relies on the traditional method for sequence labeling ( conditional random fields ) , the other relies on its neural alternative ( bidirectional long short - term memory ) . We investigate the similarities and differences of these two approaches , showing that both methods yield very good and quite similar results , with the neural model outperforming the traditional one more as the level of non - standardness of the text increases . Through an error analysis we show that the neural system is better at long - range dependencies , while the traditional system excels and slightly outperforms the neural system at the local ones . We present in the paper new state - of - the - art results in morphosyntactic annotation of non - standard text for Slovene , Croatian and Serbian .", "entities": [[56, 61, "MethodName", "long short - term memory"]]}
{"text": "In this paper we present two systems taking part in the MTT ( Morphosyntactic Tagging of Tweets ) shared task , part of the VarDial Evaluation Campaign ( Zampieri et al , 2018 ) . In the task , general - domain and indomain datasets with tokens manually annotated with morphosyntactic descriptions ( MSDs ) , are given , together with large web - based datasets , for three South Slavic languages : Slovene , Croatian and Serbian . The challenge of the task is to exploit similarity of standard vs. non - standard variants , as well as the overall proximity of the three languages in question . While the first system , JANES , relies on the traditional method for sequence labeling , namely conditional random fields ( CRF ) , the second system , JSI , relies on the currently hugely popular neural networks , more precisely bidirectional long short - term memories ( BiLSTM ) . The contributions of this paper are the following : ( 1 ) a direct comparison of CRFs and BiLSTMs on a series of datasets , where CRFs are equipped with carefully engineered features , not generic ones , and ( 2 ) a new state - of - the - art in tagging non - standard varieties of the three languages in question . 2 System Descriptions 2.1 Datasets Distributed inside the Shared Task Before we describe our two systems participating in the task , we quickly quantify the available resources through token number in Table 1 as these heavily influence our decisions in the system setup . The twitter . * datasets come from the Janes - Tag manually annotated dataset of Slovene computermediated communication ( Erjavec et al , 2017 ) and the ReLDI - NormTagNER - * manually annotated datasets of Croatian ( Ljube\u0161i\u0107 et al , 2017b ) and Serbian ( Ljube\u0161i\u0107 et al , 2017c ) tweets . They are all similar in size , with cca . 40 thousand tokens available for training , 8 thousand for development and 20 thousand for testing . The standard.train datasets mostly cover the general domain . While the Slovene and Croatian datasets are similar in size with around 500 thousand tokens , the Serbian dataset is significantly smaller with only 87 thousand tokens . The web.auto datasets are large web - based datasets , slWac for Slovene , hrWaC for Croatian and srWaC for Serbian ( Ljube\u0161i\u0107 and Klubi\u010dka , 2014 ) . These are automatically annotated with state - of - the - art taggers of standard language for Slovene ( Ljube\u0161i\u0107 and Erjavec , 2016 ) and Croatian and Serbian", "entities": [[130, 131, "MethodName", "CRF"], [157, 158, "MethodName", "BiLSTM"]]}
{"text": "The JSI system ( the name comes from the name of our current employer , the Jo\u017eef Stefan Institute ) is an adaptation of the BiLSTM tagger written in pytorch 2 , with some added modifications . The architecture of the submitted system is the following : a character - level subnetwork , consisting of a character embedding layer of 16 dimensions and a BiLSTM layer with 25 units the main network concatenating the character - level representation of a word from the subnetwork described above ( 25 * 2 , i.e. , 50 dimensions ) , and the word embedding layer ( 100 dimensions ) feeding this concatenated 150 - dimensional character - and word - level representation into a BiL - STM layer with 100 units the per - token BiLSTM output being fed to a fully - connected layer with 256 units and a final softmax layer for prediction While developing this architecture , we investigated the impact of various setups on the Slovene dataset . The results of experimenting with ( 1 ) different pretrained word embeddings , ( 2 ) the impact of adding different character - level representations , ( 3 ) fine - tuning the model on in - domain data and ( 4 ) pretraining the character - level encoder on a inflectional lexicon , are shown in Table 2 . We performed our experiments on each of the above mentioned issues subsequently , always propagating to the next experiment set the setup achieving best results in the previous one . The setup we start with consists only of the main network , without the character - level subnetwork .", "entities": [[25, 26, "MethodName", "BiLSTM"], [64, 65, "MethodName", "BiLSTM"], [132, 133, "MethodName", "BiLSTM"], [148, 149, "MethodName", "softmax"], [179, 181, "TaskName", "word embeddings"]]}
{"text": "The third experiment considers the impact of not training the network on a simple merge of all the available relevant training data , but also fine - tuning the network exclusively on in - domain data . Running three epochs on the concatenation of all datasets , and then additional two epochs only on the in - domain Twitter data , consistently improved the results for around half an accuracy point . This method is somewhat similar to the oversampling method applied on the JANES system . It is , however , more elegant as it gives greater control over the amount and order of data fed into the system .", "entities": [[69, 70, "MetricName", "accuracy"]]}
{"text": "In this section we perform an analysis of confusion matrices of the JANES and the JSI - simpler system . We perform the analysis on the output of the system on the Croatian test set . We analyze and compare the 10 most frequent confusions for each system , which covers roughly 20 % of all errors done by each of the systems . The confusion pairs are given in Table 4 . Both systems make similar most frequent mistakes , some of which are typical for morphosyntactic tagging of standard varieties of South Slavic languages , other being more specific for the Twitter variety . The typical mistakes on the standard language include confusing nominative masculinum common nouns ( Ncmsn ) for accusative masculinum common nouns ( Ncmsan ) and vice versa , confusing the word \" i \" ( English \" and \" ) in its coordinating conjunction ( Cc ) and particle ( Qo ) usage , confusing adverbs ( Rgp ) for adjectives ( Agpnsny for instance ) and confusing the word \" kada \" ( English \" when \" ) in its subordinative conjunction ( Cs ) and adverbial ( Rgp ) usage . The errors that are more due to the specificity of the Twitter variety are confusing proper names ( Np . * ) or common nouns ( Nc . * ) for foreign residuals ( mostly foreign words or foreign sequences of words , Xf ) and vice versa . When comparing the most frequent errors between the two systems , the JANES CRF - based system seems to have more problems with the traditional discrimination between different context - dependent cases of nouns , which points to the direction that BiLSTMs are better at modeling long - range dependencies as discriminating between the nominative and the accusative case often requires a very wide context . On the other hand , what the BiLSTM system seems to be worse at is discriminating between different cases for prepositions , which heavily depends on the following adjective or noun . While confusing an accusative preposition ( Sa ) for a locative one ( Sl ) the BiLSTM system did 23 times , this happened to the CRF system 17 times . In the opposite direction , the BiLSTM system did 19 mistakes while the CRF system did one mistake less , namely 18 of them . While it is clear why CRFs excel at predicting prepositional cases correctly as this dependence is in the scope of the local features , it seems that the BiLSTMs trade more mistakes in the local context for less mistakes in a wider one .", "entities": [[261, 262, "MethodName", "CRF"], [321, 322, "MethodName", "BiLSTM"], [362, 363, "MethodName", "BiLSTM"], [372, 373, "MethodName", "CRF"], [383, 384, "MethodName", "BiLSTM"], [390, 391, "MethodName", "CRF"]]}
{"text": "An Empirical Investigation of Word Alignment Supervision for Zero - Shot Multilingual Neural Machine Translation", "entities": [[4, 6, "TaskName", "Word Alignment"], [13, 15, "TaskName", "Machine Translation"]]}
{"text": "Zero - shot translations is a fascinating feature of Multilingual Neural Machine Translation ( MNMT ) systems . These MNMT models are usually trained on English - centric data , i.e. English either as the source or target language , and with a language label prepended to the input indicating the target language . However , recent work has highlighted several flaws of these models in zero - shot scenarios where language labels are ignored and the wrong language is generated or different runs show highly unstable results . In this paper , we investigate the benefits of an explicit alignment to language labels in Transformer - based MNMT models in the zero - shot context , by jointly training one cross attention head with word alignment supervision to stress the focus on the target language label . We compare and evaluate several MNMT systems on three multilingual MT benchmarks of different sizes , showing that simply supervising one cross attention head to focus both on word alignments and language labels reduces the bias towards translating into the wrong language , improving the zero - shot performance overall . Moreover , as an additional advantage , we find that our alignment supervision leads to more stable results across different training runs .", "entities": [[11, 13, "TaskName", "Machine Translation"], [105, 106, "MethodName", "Transformer"], [125, 127, "TaskName", "word alignment"]]}
{"text": "Multilingual Neural Machine Translation ( MNMT ) focuses on translation between multiple language pairs through a single optimized neural model , and has been explored from different angles witnessing a rapid progress in recent years ( Arivazhagan et al , 2019b ; Dabre et al , 2020 ; Lin et al , 2021 ) . Besides the great flexibility MNMT models offer , they are also highlighted by their so called zero - shot translation capabilities , i.e. , translating between all combinations of languages available in the training data , including those with no parallel data seen at training time ( Ha et al , 2016 ; Firat et al , 2016 ; Johnson et al , 2017 ) . Many studies have investigated this feature , focusing on the impact of both , the model architecture design ( Arivazhagan et al , 2019a ; and data pre - processing ( Lee et al , 2017 ; Wang et al , 2019 ; Rios et al , 2020 ; . Broadly speaking , MNMT architectures are categorized according to their degree of parameter sharing , from fully shared ( Johnson et al , 2017 ) to the use of language - specific components ( V\u00e1zquez et al , 2020 ; Escolano et al , 2021 ; Zhang et al , 2021 ) . The Johnson et al ( 2017 ) MNMT model is widely used , due to its simplicity and good translation quality . It uses the fully shared parameters setting , and relies on appending an artificial language label to each input sentence to indicate the target language . While this method allows for zeroshot translation , several works have highlighted two major flaws : i ) its failure to reliably generalize to unseen language pairs , ending up with the so called off - target issue , where the language label is ignored and the wrong target language is produced as a result , ii ) its lack of stability in translation results between different training runs ( Rios et al , 2020 ) . In this work , we investigate the role of guided alignment in the Johnson et al ( 2017 ) setting , by jointly training one cross attention head to explicitly focus on the target language label . We show that alignment supervision mitigates the off - target translation issue in the zero - shot case . Our method improves the zero - shot translation performance and results in more stable results across different training runs .", "entities": [[2, 4, "TaskName", "Machine Translation"]]}
{"text": "In this work we present an empirical comparative evaluation of integrating different alignment methods in Transformer - based models for highly multilingual English - centric MT setups . Our extensive evaluation over three alignment variants shows that adding alignment supervision between corresponding words and the language label consistently improves the stability of the models , resulting in stable performance across different runs and mitigating the off - target translation issue in the zero - shot scenario . We believe that our work will pave the way for designing new and better multilingual MT models to improve their generalization in zero - shot setups . As future work , we intend to analyze the quality of the learned alignments and their effect on the other attention weights in both supervised and zeroshot evaluation data ( Raganato and Tiedemann , 2018 ; Tang et al , 2018 ; Mare\u010dek and Rosa , 2019 ; Voita et al , 2019 ) . Finally , we plan to explore other mechanisms to inject prior knowledge to better handle zero - shot translations ( Deshpande and Narasimhan , 2020 ; Song et al , 2020 ) .", "entities": [[15, 16, "MethodName", "Transformer"]]}
{"text": "A.1 Data TED Talks ( Qi et al , 2018 ) . This parallel corpus includes 59 language pairs from and to English . It is a highly imbalanced benchmark , ranging from less than 4 K up to 215 K training sentences . We use the same languages as Aharoni et al ( 2019 ) for both supervised testing and zero - shot evaluation . As supervised test sets , we use { Azerbeijani , Belarusian , Galician , Slovak , Arabic , German , Hebrew , Italian } \u2194English . As zero - shot test sets , we use Arabic\u2194French , and Ukrainian\u2194Russian . WMT - 2018 ( Bojar et al , 2018 ) . We use training and testing data as provided by the WMT 2018 news translation task organizers . The benchmark contains a total of 14 language pairs : { Chinese , Czech , Estonian , Finnish , German , Russian , Turkish } \u2194English . For training , we use up to 5 M parallel sentences per language pair , with Turkish\u2194English , Estonian\u2194English , and Finnish\u2194English , having only 200 K , 1 M , and 2.7 M training sentences , respectively . For zeroshot test sets , we use the test data from Tiedemann ( 2020 ) , using the following 24 language directions : Czech \u2194 German , German \u2194 Russian , German \u2194 Chinese , Finnish \u2194 German , Finnish \u2194 Turkish , Russian \u2194 Finnish , Russian \u2194 Chinese , Turkish \u2194 Chinese , Czech \u2194 Russian , German \u2194 Turkish , Estonian \u2194 Russian , Russian \u2194 Turkish OPUS - 100 . OPUS - 100 is a recent benchmark consisting of 55 M Englishcentric sentence pairs covering 100 languages . The data is collected from movie subtitles , GNOME documentation , and the Bible . Out of 99 language pairs , 44 have 1 M sentences , 73 have at least 100 K sentences , and 95 at least 10K. It provides also zero - shot test sets , pairing the following languages : Arabic , Chinese , Dutch , French , German , and Russian .", "entities": [[127, 130, "DatasetName", "WMT 2018 news"], [134, 136, "DatasetName", "The benchmark"]]}
{"text": "Question Answering ( QA ) involves constructing an answer for a given question in either an extractive or an abstractive manner . QA systems are central to other Natural Language Processing ( NLP ) applications like search engines , and dialogue . Recently , QA based solutions have also been proposed to evaluate factuality ( Wang et al , 2020 ) and faithfulness ( Durmus et al , 2020 ) of abstractive summarization systems . In addition to popular QA benchmarks like SQuAD ( Rajpurkar et al , 2016 ) , and MRQA - 2019 ( Fisch et al , 2019 ) , we have seen QA challenges that require reasoning over human dialogue . Some notable examples being QuAC ( Choi et al , 2018 ) and CoQA ( Reddy et al , 2019 ) . These datasets require the model to attend to the entire dialogue context in the process of retrieving an answer . In this work , we are interesting in building a QA system to help with human dialogue . Feng et al ( 2020 ) introduced a new dataset of goal - oriented dialogues ( Doc2Dial ) that are grounded in the associated documents . Each sample in the dataset consists of an information - seeking conversation between a user and an agent where agent 's responses are grounded in FAQ - like webpages . DialDoc shared task derives its training data from the Doc2Dial dataset and proposes two subtasks which require the participants to ( 1 ) identify the grounding knowledge in form of document span for the next agent turn ; and ( 2 ) generate the next agent response in natural language . In this paper , we describe our solution to the subtask 1 . This subtask is formulated as a span selection problem . Therefore , we leverage a transformerbased extractive question - answering model Lan et al , 2019 ) to extract the relevant spans from the document . We pretrain our model on different QA datasets like SQuAD , different subsets of MRQA - 2019 training set , and conversational QA datasets like CoQA and QuAC . We find that models pretrained on out - of - domain QA datasets substantially outperform the baseline . Our experiments suggest that conversational QA datasets are more useful than MRQA - 2019 data or its subsets . In the following sections , we first present an overview of the DialDoc shared task ( 2 ) , followed by our system description ( 3 ) and a detailed account of our experimental results , and ablation studies ( 4 , 5 ) .", "entities": [[0, 2, "TaskName", "Question Answering"], [72, 73, "TaskName", "summarization"], [82, 83, "DatasetName", "SQuAD"], [92, 93, "DatasetName", "MRQA"], [119, 120, "DatasetName", "QuAC"], [128, 129, "DatasetName", "CoQA"], [191, 192, "DatasetName", "Doc2Dial"], [218, 219, "DatasetName", "agent"], [220, 221, "DatasetName", "agent"], [240, 241, "DatasetName", "Doc2Dial"], [266, 267, "DatasetName", "agent"], [276, 277, "DatasetName", "agent"], [340, 341, "DatasetName", "SQuAD"], [345, 346, "DatasetName", "MRQA"], [356, 357, "DatasetName", "CoQA"], [358, 359, "DatasetName", "QuAC"], [389, 390, "DatasetName", "MRQA"]]}
{"text": "Dataset used in the DialDoc shared - task is derived from Doc2Dial dataset ( Feng et al , 2020 ) , a new dataset with goal - oriented document - grounded dialogue . It includes a set of documents and conversations between a user and an agent grounded in the associated document . The authors provide annotations for dialogue acts for each utterance in the dialogue flow , along with the span in the document that acts as the reference of it . The dataset shared during the shared task was divided into train / validation / testdev / test splits . Train and validation splits were provided to the participants to facilitate model development . During phase 1 , the models were evaluated on testdev whereas , the final ranking was done on the performance on the test set . Pre - processing Using the pre - processing scripts provided by the task organizers , we converted the Doc2Dial dataset into SQuAD v2.0 format with questions containing the latest user utterance as well as all previous turns in the conversation . This is in line with previous work from ( Feng et al , 2020 ) which showed that including the entire conversational history performs better than just considering the current user utterance . Dialogue context is concatenated with the latest user utterance in the reverse time order . The output of this pre - processing step consisted of 20431 training , 3972 validation , 727 testdev , and 2824 test instances .", "entities": [[11, 12, "DatasetName", "Doc2Dial"], [46, 47, "DatasetName", "agent"], [159, 160, "DatasetName", "Doc2Dial"], [162, 163, "DatasetName", "SQuAD"]]}
{"text": "Recent work ( Gururangan et al , 2020 ) has shown that multi - phase domain adaptive pretraining of transformer - based encoders on related datasets ( and tasks ) benefits the overall performance of the model on the downstream task . Motivated by this , we experimented with further pretraining the QA model on different out - of - domain QA datasets to gauge its benefits on Doc2Dial ( Table 1 ) .", "entities": [[68, 69, "DatasetName", "Doc2Dial"]]}
{"text": "Our first set of results portray the differential benefits of different out - of - domain QA datasets when used to pretrain the transformer encoder . Experiments with bert - base - uncased on the validation set ( Table 2 ) portray that pretraining on different QA datasets is indeed beneficial . Datasets like SQuAD , NewsQA , and NaturalQuestions are more useful than SearchQA , and Trivi - aQA . However , pretraining on complete MRQA - 2019 training set does not outperform the individual datasets suggesting that merely introducing more pretraining data might not result in improved performance . Furthermore , conversational QA datasets like CoQA and QuAC , which are more similar in their setup to DialDoc , perform substantially better than any of the other MRQA - 2019 training datasets . We observe similar trends with larger transformers ( Table 3 ) . Models pretrained on QuAC or CoQA outperform those pretrained on SQuAD . However , combining CoQA and QuAC during pretraining does not seem to help with the performance on validation or testdev split . Analyzing Different Transformer Variants Table 3 also contains the results for experiments where albert - xl is used to encode the questioncontext pair . We find that albert - xl - based models outperform their bert counterparts on validation set . However , they do not generalize well to the Testdev set , which contains about 30 % of the test instances but is much smaller than validation set in size ( 727 samples in testdev vs 3972 in validation set ) .", "entities": [[54, 55, "DatasetName", "SQuAD"], [56, 57, "DatasetName", "NewsQA"], [64, 65, "DatasetName", "SearchQA"], [76, 77, "DatasetName", "MRQA"], [107, 108, "DatasetName", "CoQA"], [109, 110, "DatasetName", "QuAC"], [129, 130, "DatasetName", "MRQA"], [150, 151, "DatasetName", "QuAC"], [152, 153, "DatasetName", "CoQA"], [157, 158, "DatasetName", "SQuAD"], [162, 163, "DatasetName", "CoQA"], [164, 165, "DatasetName", "QuAC"], [183, 184, "MethodName", "Transformer"]]}
{"text": "In this work , we tackle the task of question answering ( QA ) for English language text . While we believe that the proposed methods can be effective in other languages , we leave this exploration for future work . We also acknowledge that QA systems suffer from bias ( Li et al , 2020 ) , which often lead to unintended real - world consequences . For the purpose of the shared task , we focused solely on the modeling techniques , but a study of model bias in our systems is necessary .", "entities": [[9, 11, "TaskName", "question answering"]]}
{"text": "P - Stance : A Large Dataset for Stance Detection in Political Domain", "entities": [[8, 10, "TaskName", "Stance Detection"]]}
{"text": "The most common stance detection task on social media is target - specific stance detection ( ALDayel and Magdy , 2021 ) which aims to identify the stance toward a set of figures or topics ( Hasan and Ng , 2014 ; Mohammad et al , 2016a ; Xu et al , 2016 ; Taul\u00e9 et al , 2017 ; Swami et al , 2018 ; Zotova et al , 2020 ; Conforti et al , 2020b ; Lai et al , 2020 ; Vamvas and Sennrich , 2020 ; Conforti et al , 2020a ) . Besides target - specific stance detection , multi - target stance detection Darwish et al , 2017 ; Li and Caragea , 2021a ) , and claimbased stance detection ( Qazvinian et al , 2011 ; Derczynski et al , 2015 ; Ferreira and Vlachos , 2016 ; Bar - Haim et al , 2017 ; Rao and Pomerleau , 2017 ; Derczynski et al , 2017 ; Gorrell et al , 2019 ) are other popular trends of stance detection . Multitarget stance detection aims to jointly identify the stance toward two or more targets in the same text . Unlike the target - specific stance detection and multi - target stance detection where the target is usually a prominent figure or topic , in claimbased stance detection the target is a claim , which could be an article headline or a rumor 's post . Interestingly , despite substantial progress on stance detection , large - scale annotated datasets are limited . We compare our P - STANCE dataset with some existing stance detection datasets in Table 2 . We can observe that the sizes of existing stance detection datasets are smaller than ours except for the WT - WT dataset ( Conforti et al , 2020b ) in the financial domain . However , the average tweet length of WT - WT is much shorter when compared with our P - STANCE . Moreover , more explicit mentions of targets and lexical cues of stance appear in the sentences of WT - WT dataset . In our work , we focus on the political domain and our P - STANCE , which contains much longer sentences and less surfacelevel lexical cues , can serve as a new challenging benchmark for stance detection tasks . Different from classifying the stance detection tasks by target type ( i.e. , one specific target , multiple targets , or a claim ) , we can also categorize the stance detection as in - target and cross - target stance detection by the training setting . Most previous works focused on the in - target stance detection where a classifier is trained and validated on the same target ( Mohammad et al , 2016b ; Zarrella and Marsh , 2016 ; Wei et al , 2016 ; Vijayaraghavan et al , 2016 ; Du et al , 2017 ; Sun et al , 2018 ; Wei et al , 2018 ; Caragea , 2019 , 2021b ) . However , sufficient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets , which motivates the studies of cross - target stance detection ( Augenstein et al , 2016 ; Xu et al , 2018 ; Wei and Mao , 2019 ; Zhang et al , 2020 ) . Most previous studies evaluated the cross - target models on the SemEval - 2016 dataset ( Mohammad et al , 2016a ) , which is a small dataset and thus may make the conclusions less convincing . In this paper , we show that our P - STANCE dataset can be also used to evaluate the model performance of cross - target stance detection and provides opportunities for exploring more crosstarget tasks by interacting with previous SemEval - 2016 ( Mohammad et al , 2016a ) and Multi - Target stance datasets . In addition , P - STANCE enables the exploration of largescale deep learning models including pre - trained language models , e.g. , BERT ( Devlin et al , 2019 ) and BERTweet ( Nguyen et al , 2020 ) . We fine - tune the BERT and BERTweet models on our dataset and compare them with other strong baselines .", "entities": [[3, 5, "TaskName", "stance detection"], [13, 15, "TaskName", "stance detection"], [101, 103, "TaskName", "stance detection"], [107, 109, "TaskName", "stance detection"], [124, 126, "TaskName", "stance detection"], [133, 134, "DatasetName", "Derczynski"], [159, 160, "DatasetName", "Derczynski"], [176, 178, "TaskName", "stance detection"], [180, 182, "TaskName", "stance detection"], [203, 205, "TaskName", "stance detection"], [209, 211, "TaskName", "stance detection"], [224, 226, "TaskName", "stance detection"], [250, 252, "TaskName", "stance detection"], [271, 273, "TaskName", "stance detection"], [286, 288, "TaskName", "stance detection"], [296, 299, "DatasetName", "WT - WT"], [319, 322, "DatasetName", "WT - WT"], [350, 353, "DatasetName", "WT - WT"], [390, 392, "TaskName", "stance detection"], [398, 400, "TaskName", "stance detection"], [424, 426, "TaskName", "stance detection"], [434, 436, "TaskName", "stance detection"], [450, 452, "TaskName", "stance detection"], [527, 529, "TaskName", "stance detection"], [548, 550, "TaskName", "stance detection"], [638, 640, "TaskName", "stance detection"], [692, 693, "MethodName", "BERT"], [715, 716, "MethodName", "BERT"]]}
{"text": "In this section , we detail the creation and the particularities of P - STANCE , our large political stance detection dataset composed of 21 , 574 tweets collected during the 2020 U.S. presidential election .", "entities": [[19, 21, "TaskName", "stance detection"]]}
{"text": "We collected tweets using the Twitter streaming API . Similar to prior works ( Mohammad et al , 2016a ; ) that target presidential candidates , we focus our attention on three political figures 2 in the presidential race of 2020 : \" Donald Trump , \" \" Joe Biden , \" and \" Bernie Sanders . \" We used a set of query hashtags as seeds to collect target - related tweets , which can be categorized as favor hashtags , against hashtags and neutral hashtags ( Mohammad et al , 2016a ) . We show examples of these query hashtags in Table 3 . In total , we gathered around 2.8 million tweets for all three targets combined .", "entities": [[66, 67, "DatasetName", "seeds"]]}
{"text": "To ensure the quality of this dataset , we performed several preprocessing steps : 1 ) We removed tweets with less than 10 , or more than 128 words . According to our observations , tweets with less than 10 words are either too easy for detecting the stance or too noisy , and tweets with more than 128 words usually contain duplicate expressions . 2 ) We removed duplicates and retweets . Twitter data are noisy not only due to the creative spellings , slang and URLs , but also because of the duplicate tweets . Since these duplicate data reduce our ability to build reliable models , we need to clean the dataset by removing duplicates . 3 ) We kept only the tweets in English because our goal in this work is to build an English stance detection dataset . We leave multilingual stance detection as future work . After data preprocessing , the size of our corpus reduces to around 2 million examples . In Table 4 , we show the number of tweets before and after preprocessing for each political figure . We will provide this large - scale repository of tweets ( which we call P - STANCE - EXT ) alongside P - STANCE , in hope that it will spur further research in the field of semisupervised learning for stance detection . Finally , we sampled 10 , 000 tweets for each political figure , obtaining 30 , 000 tweets for annotation in total .", "entities": [[139, 141, "TaskName", "stance detection"], [146, 148, "TaskName", "stance detection"], [227, 229, "TaskName", "stance detection"]]}
{"text": "Stance - exposing hashtags that may expose the stance directly , e.g. , # NeverBernie , can be observed in the data . A model can detect the stance from these hashtags without extracting effective representations for the meanings of sentences , which makes stance detection easier . To remove the stance - exposing hashtags and ensure the data quality , we performed the following steps after the data annotation : 1 ) We manually built a hashtag lexicon that contains stance - exposing hashtags for each target . Then we removed all hashtags that are appended at the end of a sentence if they are in the hashtag lexicon . The reason of only removing the appended hashtags is that a hashtag may serve as a constituent of a sentence , so it would introduce more noise if we simply remove all stance - exposing hashtags . 2 ) To address the stance - exposing hashtag that is a constituent of a sentence , we replaced stance - exposing hashtags that contain the target name with a neutral hashtag , e.g. , # NeverBernie # Bernie . These steps ensure the high quality of our P - STANCE dataset . In addition , P - STANCE is a challenging dataset for the following reasons : 1 ) Targets in P - STANCE are referred to in a more implicit way . Consider the second example in Table 1 , the target name only appears at the end of the sentence and it is hard to correctly identify the stance without any knowledge about the political figures mentioned in the content and background immigration policy . Similarly , for the third example , it is difficult to correctly identify the stance if the classifier fails to connect the target with relevant events , i.e. , climate change or medicare for all residents . 2 ) The average length of tweets in previous datasets is short , and there are more explicit mentions of targets and rich sentiment and emotion words that can easily reveal the stance toward the target . The average tweet length is 17 in Mohammad et al ( 2016a ) , 21 in and 16 in Conforti et al ( 2020b ) . However , our P - STANCE has a much longer average length of 30 and more implicit mentions of targets and context words , which indicates that our dataset is more difficult . In addition , P - STANCE covers more target - relevant events . These characteristics contribute to making P - STANCE a challenging dataset for stance detection .", "entities": [[44, 46, "TaskName", "stance detection"], [338, 339, "DatasetName", "emotion"], [434, 436, "TaskName", "stance detection"]]}
{"text": "In this section , we first introduce two benchmark datasets of stance detection in 4.1 . The union of these datasets and our P - STANCE dataset provides opportunities for studying the cross - target stance detection ( 5.2 ) and cross - topic stance detection ( 5.3 ) . Then we discuss the evaluation metrics in 4.2 and introduce the baseline methods in 4.3 .", "entities": [[11, 13, "TaskName", "stance detection"], [35, 37, "TaskName", "stance detection"], [44, 46, "TaskName", "stance detection"]]}
{"text": "SemEval - 2016 ( Mohammad et al , 2016a ) and Multi - Target stance datasets are two benchmark datasets in which political figures are chosen as the targets . SemEval - 2016 contains six targets : \" Atheism , \" \" Climate Change is a Real Concern , \" \" Feminist Movement , \" \" Hillary Clinton , \" \" Legalization of Abortion , \" and \" Donald Trump . \" The dataset is annotated for detecting the stance toward a given target . The data distribution of SemEval - 2016 is shown in Table 7 . Multi - Target stance dataset contains three sets of tweets corresponding to three target pairs : \" Donald Trump and Hillary Clinton , \" \" Donald Trump and Ted Cruz , \" \" Hillary Clinton and Bernie Sanders \" for 2016 U.S. presidential election . The task aims at detecting the stances toward two targets for each data . The data distribution of Multi - Target stance dataset is shown in Table 8 . In the next section , we show how to perform various stance detection tasks with the union of these datasets and our P - STANCE dataset .", "entities": [[183, 185, "TaskName", "stance detection"]]}
{"text": "In this section , we present the set of experiments performed on various stance detection tasks on our dataset and show the results obtained by using the aforementioned baselines . Each result is the average of seven runs with different initializations .", "entities": [[13, 15, "TaskName", "stance detection"]]}
{"text": "In - target stance detection is a stance detection task where a classifier is trained and validated on the same target . Most previous works adopt an \" Adhoc \" training strategy by training one model for each target and evaluate it on the test set of that target ( i.e. , we train three different models if there are three targets in the dataset ) . However , the model is more likely to predict the stance by following specific patterns without fully considering the target information and overfit . Therefore , to better evaluate the performance of baselines , we propose a \" Merged \" training strategy by training and validating a model on all targets and testing it on separate targets to be compared with the \" Ad - hoc \" setting . Experimental results of these two different settings are shown in Table 9 . First , we can observe that BERTweet performs best in both settings and significantly outperforms the second best results , demonstrating the effectiveness of this model . Second , performance drops can be observed on all models in the \" Merged \" setting and models ( BiL - STM and CNN ) that do not consider target information suffer the most severe drops , which means our proposed training strategy can serve as a better evaluation method to test whether the model learns target - specific representations . Moreover , we can observe that both BERTweet and BERT perform well and have the minimum performance drops compared with the other baselines , which demonstrates that self - attention mechanism can better capture target - specific representations .", "entities": [[3, 5, "TaskName", "stance detection"], [7, 9, "TaskName", "stance detection"], [246, 247, "MethodName", "BERT"]]}
{"text": "Despite substantial progress on the stance detection , sufficient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets , which motivates the studies of crosstarget stance detection . The model of cross - target stance detection is first trained and validated on a source target , and then tested on a destination target . In this subsection , we show that our P - STANCE dataset can be also used to evaluate the model performance of cross - target stance detection and provides opportunities for exploring more cross - target tasks by interacting with previous SemEval - 2016 and Multi - Target stance datasets . We use five targets for our experiments : \" Donald Trump \" ( DT ) , \" Joe Biden \" ( JB ) , \" Bernie Sanders \" ( BS ) , \" Hillary Clinton \" ( HC ) , and \" Ted Cruz \" ( TC ) . Experimental results of cross - target stance detection are shown in Table 10 . For the first half of Table 10 , only targets of P - STANCE dataset are used to evaluate the model performance . However , for the second half , targets of SemEval - 2016 and Multi - Target datasets also serve as destination targets , which makes it a more challenging task since the target - related topics in 2016 are quite different from the ones in 2020 . More specifically , we train and validate the model on a source target of P - STANCE dataset and test it on the data of a destination target , which is a combination of train , validation , and test sets of previous datasets . Note that we merge the data from SemEval - 2016 and Multi - Target datasets if these two datasets share the same target , e.g. , Hillary Clinton . For the cross - target tasks only on the P - STANCE dataset , first , we can observe from the Table 10 that BERTweet achieves the best performance on all target configurations , demonstrating its effectiveness . Moreover , BERTweet shows greater improvement over the best baseline when training on the data of two targets . The reason is that BERTweet learns more universal representations by leveraging the data from two targets . Second , we see that Cross - Net outperforms BiCE on almost all target configurations , which is consistent with the observations of previous studies ( Xu et al , 2018 ; Zhang et al , 2020 ) . Third , we find that models achieve better performance on JB BS and BS JB . One potential explanation is that targets \" Joe Biden \" and \" Bernie Sanders \" are from the same party and thus share more similar topics . For the second half of Table 10 , we observe a significant drop in performance on all models , which verifies that it is more challenging to transfer the knowledge to a destination target with more diverse topics in the past . BERTweet still achieves the best performance on almost all target configurations , making it a highly competitive model for crosstarget stance detection task . Interestingly , we can observe that both BiCE , CrossNet , and BERTweet show better performance on target \" Ted Cruz . \" A possible reason is that the data of \" Ted Cruz \" contain more universal expressions and topics .", "entities": [[5, 7, "TaskName", "stance detection"], [20, 22, "TaskName", "stance detection"], [39, 41, "TaskName", "stance detection"], [48, 50, "TaskName", "stance detection"], [93, 95, "TaskName", "stance detection"], [174, 176, "TaskName", "stance detection"], [544, 546, "TaskName", "stance detection"]]}
{"text": "Obtaining sufficient annotated data of specific target from most recent past is challenging . However , sometimes historical annotated data of the same target are available . Therefore , motivated by a desire to improve the models ' generalization ability to transfer knowledge from historical data , we come up with a new stance detection task , named cross - topic stance detection . Specifically , in this task , the model of cross - topic stance detection is first trained on the data of a target ( e.g. , Donald Trump ) in 2016 , and then validated and tested on the data of the same target in 2020 . Note that the annotated data of year 2016 are the same with the data used in 5.2 . The results are shown in Table 11 . Since target \" Joe Biden \" is absent from the previous stance detection datasets , we use targets \" Donald Trump \" and \" Bernie Sanders \" for evaluation . We can observe that BERTweet still performs best on this task and the overall model performance of cross - topic stance detection is better than that of cross - target stance detection due to the use of the same target in evaluation stage . Moreover , we see that models perform relatively poorly on target \" Bernie Sanders \" . One possible explanation is that some topics , e.g. healthcare and climate change , appear rarely in previous datasets .", "entities": [[53, 55, "TaskName", "stance detection"], [61, 63, "TaskName", "stance detection"], [76, 78, "TaskName", "stance detection"], [148, 150, "TaskName", "stance detection"], [187, 189, "TaskName", "stance detection"], [197, 199, "TaskName", "stance detection"]]}
{"text": "In this paper , we introduced P - STANCE , an English stance detection dataset in the political domain , which is larger and more challenging compared with previous datasets for stance detection . Composed of 21 , 574 tweets that were collected during the 2020 USA election , P - STANCE can serve as a new benchmark for stance detection and enable future research in other stance detection tasks , e.g. , cross - target stance detection and cross - topic stance detection . Experimental results show that the BERTweet model significantly outperforms other strong baselines not only on intarget stance detection , but also on cross - target and cross - topic stance detection . Moreover , the performance of BERTweet can be further improved by using semi - supervised learning . Future work includes constructing another large dataset for a more challenging task , i.e. , multi - target stance detection , and studying the multilingual stance detection with the union of P - STANCE and other multilingual datasets .", "entities": [[12, 14, "TaskName", "stance detection"], [31, 33, "TaskName", "stance detection"], [59, 61, "TaskName", "stance detection"], [67, 69, "TaskName", "stance detection"], [76, 78, "TaskName", "stance detection"], [82, 84, "TaskName", "stance detection"], [101, 103, "TaskName", "stance detection"], [114, 116, "TaskName", "stance detection"], [152, 154, "TaskName", "stance detection"], [159, 161, "TaskName", "stance detection"]]}
{"text": "An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction", "entities": [[8, 11, "TaskName", "Grammatical Error Correction"]]}
{"text": "The incorporation of pseudo data in the training of grammatical error correction models has been one of the main factors in improving the performance of such models . However , consensus is lacking on experimental configurations , namely , choosing how the pseudo data should be generated or used . In this study , these choices are investigated through extensive experiments , and state - of - the - art performance is achieved on the CoNLL - 2014 test set ( F 0.5 = 65.0 ) and the official test set of the BEA - 2019 shared task ( F 0.5 = 70.2 ) without making any modifications to the model architecture .", "entities": [[9, 12, "TaskName", "grammatical error correction"]]}
{"text": "To date , many studies have tackled grammatical error correction ( GEC ) as a machine translation ( MT ) task , in which ungrammatical sentences are regarded as the source language and grammatical sentences are regarded as the target language . This approach allows cutting - edge neural MT models to be adopted . For example , the encoder - decoder ( EncDec ) model ( Sutskever et al , 2014 ; Bahdanau et al , 2015 ) , which was originally proposed for MT , has been applied widely to GEC and has achieved remarkable results in the GEC research field ( Ji et al , 2017 ; Chollampatt and Ng , 2018 ; . However , a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data ( Koehn and Knowles , 2017 ) , but the largest set of publicly available parallel data in GEC has only two million sentence pairs ( Mizumoto et al , 2011 ) . Consequently , the method of augmenting the data by incorporating pseudo training data has been studied intensively ( Xie et al , 2018 ; Ge et al , 2018 ; Lichtarge et al , 2019 ; Zhao et al , 2019 ) . * Current affiliation : Future Corporation When incorporating pseudo data , several decisions must be made about the experimental configurations , namely , ( i ) the method of generating the pseudo data , ( ii ) the seed corpus for the pseudo data , and ( iii ) the optimization setting ( Section 2 ) . However , consensus on these decisions in the GEC research field is yet to be formulated . For example , Xie et al ( 2018 ) found that a variant of the backtranslation ( Sennrich et al , 2016b ) method ( BACKTRANS ( NOISY ) ) outperforms the generation of pseudo data from raw grammatical sentences ( DIRECTNOISE ) . By contrast , the current state of the art model ( Zhao et al , 2019 ) uses the DIRECTNOISE method . In this study , we investigate these decisions regarding pseudo data , our goal being to provide the research community with an improved understanding of the incorporation of pseudo data . Through extensive experiments , we determine suitable settings for GEC . We justify the reliability of the proposed settings by demonstrating their strong performance on benchmark datasets . Specifically , without any task - specific techniques or architecture , our model outperforms not only all previous single - model results but also all ensemble results except for the ensemble result by Grundkiewicz et al ( 2019 ) 1 . By applying task - specific techniques , we further improve the performance and achieve state - of - the - art performance on the CoNLL - 2014 test set and the official test set of the BEA - 2019 shared task .", "entities": [[7, 10, "TaskName", "grammatical error correction"], [15, 17, "TaskName", "machine translation"]]}
{"text": "The BEA - 2019 workshop official dataset 4 is the origin of the training and validation data of our experiments . Hereinafter , we refer to the training data as BEA - train . We create validation data ( BEA - valid ) by randomly sampling sentence pairs from the official validation split 5 . As a seed corpus T , we use SimpleWiki 6 , Wikipedia 7 or Gigaword 8 . We apply the noizing methods described in Section 3 to each corpus and generate pseudo data D p . The characteristics of each dataset are summarized in Table 1 . Evaluation We report results on BEA - valid , the official test set of the BEA - 2019 shared task ( BEA - test ) , the CoNLL - 2014 test set ( CoNLL - 2014 ) ( Ng et al , 2014 ) , and the JFLEG test set ( JFLEG ) ( Napoles et al , 2017 ) . All reported results ( except ensemble ) are the average of five distinct trials using five different random seeds . We report the scores measured by ERRANT ( Bryant et al , 2017 ; Felice et al , 2016 ) for BEA - valid , BEA - test , and CoNLL - 2014 . As the reference sentences of BEAtest are publicly unavailable , we evaluate the model outputs on CodaLab 9 for BEA - test . We also report results measured by the M 2 scorer ( Dahlmeier and Ng , 2012 ) on CoNLL - 2014 to compare them with those of previous studies . We use the GLEU metric ( Napoles et al , 2015 ( Napoles et al , , 2016 for JFLEG . Model We adopt the Transformer EncDec model ( Vaswani et al , 2017 ) using the fairseq toolkit ( Ott et al , 2019 ) and use the \" Transformer ( big ) \" settings of Vaswani et al ( 2017 ) . Optimization For the JOINT setting , we opti - mize the model with Adam ( Kingma and Ba , 2015 ) . For the PRETRAIN setting , we pretrain the model with Adam and then fine - tune it on BEA - train using Adafactor ( Shazeer and Stern , 2018 ) 10 .", "entities": [[149, 150, "DatasetName", "JFLEG"], [153, 154, "DatasetName", "JFLEG"], [181, 182, "DatasetName", "seeds"], [289, 290, "DatasetName", "JFLEG"], [295, 296, "MethodName", "Transformer"], [320, 321, "MethodName", "Transformer"], [347, 348, "MethodName", "Adam"], [366, 367, "MethodName", "Adam"], [378, 379, "MethodName", "Adafactor"]]}
{"text": "Adobe AMPS 's Submission for Very Low Resource Supervised Translation Task at WMT20", "entities": [[9, 10, "TaskName", "Translation"]]}
{"text": "This paper describes our submissions to the shared task on Very Low Resource Supervised Machine Translation at WMT 2020 . The task involved a single language pair : Upper Sorbian - German . We submit supervised neural machine translation ( NMT ) systems for both translation directions , Upper Sorbian German and German Upper Sorbian . NMT models ( Sutskever et al , 2014 ; Bahdanau et al , 2015 ; Cho et al , 2014a ) have achieved stateof - the - art performance on benchmark datasets for multiple language pairs . A big advantage of such systems over phrase - based statistical machine translation ( PBSMT ) ( Koehn et al , 2003 ) models is that they can be trained end - to - end . The bulk of the development , however , has been limited to a handful of high - resource language pairs . The primary reason is that training a well - performing NMT system requires a large amount of parallel training data , which means a lot of equivalent investment in terms of resources . Koehn and Knowles ( 2017 ) show that when compared to PBSMT approaches , NMT models need more training data to achieve the same level of performance . 1 One of the most popular ways to increase the amount of parallel training data for supervised training is backtranslation ( Sennrich et al , 2016a ) . We utilize this approach to improve upon the performance of our baseline models . All of our systems follow the Transformer architecture ( Vaswani et al , 2017 ) . Our primary system is a supervised NMT model trained on the original training bitext . We also report our results on experiments with backtranslation , which were completed post the shared task and hence not a part of our primary submissions . We use the backtranslated data in two distinct ways - as a standalone parallel corpus , and to create a combined parallel corpus by mixing in a 1:1 ratio with the provided training data . We also report the performance of fine - tuned models originally trained only on the backtranslated data . In the following sections , we begin by briefly describing the Transformer architecture and backtranslation . We then discuss our experimental setup as well as our experiments with backtranslation . We conclude with a discussion of our results and possible future work .", "entities": [[14, 16, "TaskName", "Machine Translation"], [17, 19, "DatasetName", "WMT 2020"], [37, 39, "TaskName", "machine translation"], [104, 106, "TaskName", "machine translation"], [259, 260, "MethodName", "Transformer"], [375, 376, "MethodName", "Transformer"]]}
{"text": "We used the complete parallel training corpus for our primary systems . In addition , we also made use of monolingual data from each language for 2 https://github.com/pytorch/fairseq two purposes - learning Byte Pair Encodings ( BPE ) ( Sennrich et al , 2016b ) and backtranslation . For Upper Sorbian ( hsb ) , we used the monolingual corpora provided by the Sorbian Institute and by the Witaj Sprachzentrum . To control the quality of the backtranslated data , we chose not to use the data scraped from the web . For the German ( de ) side , we made use of the News Crawl 3 2009 dataset , as it is large enough to satisfy the requirements for our experiments .", "entities": [[36, 37, "MethodName", "BPE"]]}
{"text": "No . of sentences hsb - de , bitext 58 , 389 hsb , monolingual 540 , 994 de , monolingual 2 , 000 , 000 Moses toolkit ( Koehn et al , 2007 ) was used for tokenization and punctuation normalization for all data . Before doing any additional preprocessing , we learned separate truecaser models using the toolkit . For this purpose , we took first 500 K sentences from each of the monolingual corpora and aggregated them with the corresponding portion from the training bitext . After tokenizing and truecasing , we joined the parallel training corpus with the same monolingual data . We learned joint BPE 4 with 32 K merge operations over this corpus and applied them to the parallel training data to get vocabularies for each language . Additionally , we used the clean - corpus - n.perl script within Moses to filter out sentences from the parallel corpus with more than 250 subwords as well as sentence length ratio over 1.5 in either direction . Final corpus statistics are presented in Table 1 .", "entities": [[109, 110, "MethodName", "BPE"]]}
{"text": "NNEMBs at SemEval - 2017 Task 4 : Neural Twitter Sentiment Classification : a Simple Ensemble Method with Different Embeddings", "entities": [[11, 12, "TaskName", "Classification"]]}
{"text": "Twitter sentiment classification has attracted a lot of attention ( Dong et al , 2015 ; Nakov et al , 2016 ; Rosenthal et al , 2017 ) , which aims to classify a tweet into three sentiment categories : negative , neutral , and positive . Tweet text has several features : written by the informal language , hash - tags and emoticons indicate sentiments , and sometimes is sarcasm , which make decisions of tweet sentiment hard for machines . With releases of annotated datasets , more researchers prefer to use the 1 https://github.com/zwjyyc/NNEMBs twitter sentiment classification as one testbed to evaluate their proposed models . Traditional methods ( Mohammad et al , 2013 ) for twitter sentiment classification use a variety of hand - crafted features including surface - form , semantic and sentiment lexicons . The performances of these methods often depend on the quality of feature engineering work , and building a state - ofthe - art system is difficult for novices . Moreover , these designed features are presented by the onehot representation which can not capture the semantic relativeness of different features and proposes a problem of feature sparsity . To address this , Tang et al ( 2014 ) induced sentiment - specific low - dimensional , real - valued embedding features for twitter classification , which encode both semantics and sentiments of words . In the experiments , the embedding features and hand - crafted features obtain similar results , and also they are complementary for each other in the system . With the developments of neural networks in natural language processing , neural sentiment classification ( Severyn and Moschitti , 2015 ; Deriu et al , 2016 ) has attracted a lot of attention recently and become the state - of - the - arts . These methods first learn word embeddings from large - scale twitter corpus , then tune neural networks by the tweets which have distant labels , and finally fine - tune the proposed models by the annotated datasets . Learning word embeddings using in - domain data is an effective way to boost model performances ( Mikolov et al , 2013 ; Yin et al , 2016 ) . However , collecting large - scale twitter corpus is often time - consuming . In this paper , we use the different word embedding sets to boost the performances of our neural networks , which only include released different word embeddings sets and the word embedding set derived from the released Yelp large - scale datasets by Skip - gram ( Mikolov et al , 2013 ) . A simple and effective ensemble method is proposed , which takes different word embedding sets as input to train neural networks and predicts labels of testing tweets by merging all output of neural models . Our ensemble method show its effectiveness in SemEval 2017 , though most of used word embedding sets are not learned from twitter corpus , which can be explained that different embedding sets has different vocabularies and encode different parts of sentiment knowledge . Moreover , we conduct additional experiments to analyze our model .", "entities": [[150, 152, "TaskName", "feature engineering"], [310, 312, "TaskName", "word embeddings"], [345, 347, "TaskName", "word embeddings"], [413, 415, "TaskName", "word embeddings"]]}
{"text": "We have many choices of neural networks ( e.g. , LSTM , RNN and GRU ) for our method , here we consider RCNN ( Lei et al , 2016 ) in our method . RCNN has non - consecutive convolution and adaptive gated decay , which aims to capture longerrange , non - consecutive patterns in a weighted manner . Given a sequence of words which are denoted as { x i } l i=1 , the corresponding word embeddings { x i } l i=1 are derived using the embedding matrix E. Then , RCNN obtains their corresponding hidden vectors { h i } l i=1 using the convolution operation and gating mechanism . After obtaining hidden vectors , RCNN uses a pooling operation to get fixed - sized vector presentation , which is fed into softmax layer to finish the prediction . The ngram convolution operation and gating decay are described as follows : \u03bbt = \u03c3 ( W \u03bb xt + U \u03bb ht\u22121 + b \u03bb ) , c ( 1 ) t = \u03bbt c ( 1 ) t\u22121 + ( 1 \u2212 \u03bbt ) ( W1xt ) , c ( 2 ) t = \u03bbt c ( 2 ) t\u22121 + ( 1 \u2212 \u03bbt ) ( c ( 1 ) t\u22121 + W2xt ) , , c ( n ) t = \u03bbt c ( n ) t\u22121 + ( 1 \u2212 \u03bbt ) ( c ( n\u22121 ) t\u22121 + Wnxt ) , ht = tanh ( c ( n ) t + b ) , where W \u03bb , U \u03bb , b \u03bb , b and W * are learnable parameters , \u03c3 is sigmoid function which rescales the value into ( 0 , 1 ) , is dot product , \u03bb t is gating value determining how much information of x t and previous patterns is added into the hidden vector , c ( i ) t refer to the vector for accumulated previous patterns which are ended with x t include i consecutive tokens . When \u03bb t = 0 , the convolution becomes a standard n - gram convolution . We also can build a deep RCNN by adding several convolution layer on top of hidden vectors derived from the bottom convolution layer . Here we consider the RCNN with d convolution layers , which outputs { h d i } l i=1 . Then , a last pooling operation is conducted on hidden vectors to obtain text representation r. Finally , text representation is fed into a softmax layer . The softmax layer outputs the probability distribution over | Y | categories for the distributed representation , which is defined as : p ( r ) = softmax ( W class k r ) . The cross - entropy objective function is used to optimize the RCNN model .", "entities": [[10, 11, "MethodName", "LSTM"], [14, 15, "MethodName", "GRU"], [40, 41, "MethodName", "convolution"], [79, 81, "TaskName", "word embeddings"], [110, 111, "MethodName", "convolution"], [138, 139, "MethodName", "softmax"], [147, 148, "MethodName", "convolution"], [293, 294, "DatasetName", "0"], [352, 353, "DatasetName", "0"], [355, 356, "MethodName", "convolution"], [362, 363, "MethodName", "convolution"], [374, 375, "MethodName", "convolution"], [385, 386, "MethodName", "convolution"], [395, 396, "MethodName", "convolution"], [432, 433, "MethodName", "softmax"], [436, 437, "MethodName", "softmax"], [462, 463, "MethodName", "softmax"]]}
{"text": "To improve the accuracy of predicateargument structure ( PAS ) analysis , large - scale training data and knowledge for PAS analysis are indispensable . We focus on a specific domain , specifically Japanese blogs on driving , and construct two wide - coverage datasets as a form of QA using crowdsourcing : a PAS - QA dataset and a reading comprehension QA ( RC - QA ) dataset . We train a machine comprehension ( MC ) model based on these datasets to perform PAS analysis . Our experiments show that a stepwise training method is the most effective , which pre - trains an MC model based on the RC - QA dataset to acquire domain knowledge and then fine - tunes based on the PAS - QA dataset .", "entities": [[3, 4, "MetricName", "accuracy"], [60, 62, "TaskName", "reading comprehension"]]}
{"text": "We construct a driving - domain RC - QA dataset in the same way as SQuAD 1.1 . We extract a document from the Driving Experience Corpus and ask three crowdworkers to write questions and their answers about the document . After that , we ask another five crowdworkers to answer a question to validate its answerability and adopt questions with three or more same answers . We randomly extracted 200 questions from the RC - QA dataset and judged the question types . The result is shown in Table 2 . A question was classified according to whether it is a question asking for any argument of nominative , accusative or dative , and if applicable , whether it is an omission or not . As shown in Table 2 , the RC - QA dataset contains nearly 40 % of questions asking arguments of nominative , accusative and dative , and a few questions asking for omitted arguments , which are similar to the PAS - QA dataset . There are various other questions asking for arguments other than nominative , accusative and dative , and questions using why and how .", "entities": [[15, 16, "DatasetName", "SQuAD"]]}
{"text": "We constructed driving - domain PAS - QA and RC - QA datasets using crowdsourcing 5 . We also proposed an MC - based PAS analysis method . In particular , the stepwise training method based on BERT was the most effective , which outperformed the previous state - of - the - art NN - PAS model . In the future , we will pre - train an MC model based on datasets other than the RC - QA dataset to acquire domain knowledge . A Details of PAS - QA Dataset Construction We construct the PAS - QA dataset asking for omitted nominative arguments using the following procedure : 1 . We extract four consecutive sentences that satisfy the following conditions from the Driving Experience Corpus constructed by Iwai et al ( 2019 ) . The Driving Experience extracting CRF tool ( Iwai et al , 2018 ) judges that three or more sentences out of four sentences are driving experience . Each sentence contains at least one PAS . The PAS analyzer , KNP , judges that there is a PAS whose nominative argument is omitted in the fourth sentence . Sentences include at least one \" Driving Characteristic Word \" ( Iwai et al , 2019 ) . 2 . We automatically make crowdsourcing tasks using an extracted document and a PAS whose nominative argument is omitted ( See Figure 5 and Figure 6 ) . Each task consists of a document , a question and answer choices . Answer choices consist of nouns extracted from the document and special symbols , \" author , \" \" other , \" and \" not sure . \" For nominative PAS - QA questions , the special symbol \" author \" can often be an answer , but it is not explicitly expressed in the document . So we add it to the choices . We add \" other \" so that it can be selected when there is an appropriate answer besides the choices . We add \" not sure \" so that workers can select it if they can not find an answer . We add more explanations to crowdsourcing answer screen ( See Figure 5 and Figure 6 ) . 3 . Using crowdsourcing , we ask five crowdworkers per question to select one or more appropriate answers from the choices . We asked five crowdworkers per question using Yahoo ! crowdsourcing . We adopted triplets with three or more votes if they are not \" not sure . \" If they are \" other , \" we handled them as described in the main paper . We finally record the answers as spans in a document or NULL .", "entities": [[37, 38, "MethodName", "BERT"], [141, 142, "MethodName", "CRF"]]}
{"text": "Image - text retrieval ( ITR ) has been widely studied as a staple benchmark task in both NLP and computer vision communities . Traditional ITR search engines typically deploy ranking - based models built upon visual - semantic embedding matching ( Faghri et al , 2017 ; Huang et al , 2018 ) or deep cross - modal fusion with attention mechanism ( Lee et al , 2018 ; Li et al , 2020a , b ) . Earliest works ( Kiros et al , 2014 ; Faghri et al , 2017 ( a ) Early work ( Faghri et al , 2017 ) using dot product to learn the similarity between global image features and global text features . ( b ) Later study ( Lee et al , 2018 ) applying cross - attention between the features of each region and each word . ( c ) Pre - trained V+L models with deep Transformer . ( d ) LightningDOT without cross - attention . CMR , SMRM and VMLM refer to different pre - training tasks , which will be introduced later in method section . employ separate image encoder ( e.g. , CNN ) and text encoder ( e.g. , RNN ) , the embeddings from which are then measured by doc product for similarity matching ( Figure 1 ( a ) ) . Later studies ( Lee et al , 2018Wang et al , 2019 ; improve this paradigm by employing advanced region - level visual encoder ( e.g. , Faster - RCNN ) and applying cross - attention between word features and region features for multimodal fusion ( Figure 1 ( b ) ) . With the advent of Transformer ( Vaswani et al , 2017 ) and BERT ( Devlin et al , 2019 ) , crossmodal retrieval tasks are more recently dominated by vision - and - language ( V+L ) pre - trained models , such as ViLBERT , UNITER , OSCAR ( Li et al , 2020b ) , and VILLA . Large - scale pre - trained models learned from massive corpus of image - text pairs can power heterogeneous downstream tasks that take diverse modalities as inputs ( e.g. , text , image , video , audio ) . These models benefit from the self - attention mechanism in Transformer architecture , learning joint image+text embeddings through pre - training objectives such as masked language modeling ( MLM ) and masked region modeling ( MRM ) ( Figure 1 ( c ) ) . However , the very ingredient that engenders the success of these pre - trained models , crossmodal attention between two modalities ( through self - attention ) , also destines the inevitable latency and huge computation cost in training and deploying such massive - scale models . For example , UNITER builds upon 12/24 Transformer layers , and trains over 10 million image+text pairs . The inference time of such large models with 110 million parameters is 48 seconds on average for text query from COCO dataset ( Chen et al , 2015 ) , not scalable in real - life applications serving millions of queries per second . To make real - time ITR possible with low latency , we ask a bold question : can we go back to the beginning , reverting to simple dot product for efficient cross - modal retrieval ? To make this retro experiment feasible , we rely on Transformer to pre - train high - quality image and text encoders , but use efficient dot product for multimodal fusion instead of computationally heavy self - attention . To still facilitate effective cross - modal embedding learning , we use a special [ CLS ] token on both encoders , which transfers the learned embedding from the other modality ( Figure 1 ( d ) ) . We name this new paradigm LightningDOT , for its lightening speed benefiting from dot product computation . By removing the time - consuming cross - attention between modalities , the model can learn visualsemantic embeddings without extensive matching between each image - text pair during inference , as used in existing pre - trained models Li et al , 2020b ; . Further , by eliminating the dependency on real - time computation over image - text pairs , we can compute all image and text embeddings independently offline just for once , and reuse these embeddings as cached indexes for new queries on the fly ( Figure 2 ) . For model training , we propose three learning objectives to jointly train two Transformer blocks : Image Encoder and Language Encoder . Specifically , Visual - embedding fused MLM ( namely VMLM ) and Semantic - embedding fused MRM ( namely SMRM ) ensure cross - modal information is harnessed even without cross - modality self - attention . A cross - modal retrieval objective ( namely CMR ) encourages the model to learn multimodal fusion through pre - training . To maintain competitive model performance , we further introduce a reranking mechanism to bring back the benefit of cross - attention methods . In summary , LightningDOT is designed with late fusion to learn visual - semantic embeddings . Experiments on popular ITR benchmarks show that LightningDOT is 600/1900 times faster than existing pre - trained models on Flickr30k / COCO , while achieving new state - of - the - art results . When retrieving from larger candidate pool ( > 120 K images ) , LightningDOT is 23 , 000 times faster . To the best of our knowledge , this is the first known effort on improving V+L model efficiency .", "entities": [[157, 158, "MethodName", "Transformer"], [286, 287, "MethodName", "Transformer"], [295, 296, "MethodName", "BERT"], [327, 328, "MethodName", "ViLBERT"], [329, 330, "MethodName", "UNITER"], [331, 332, "MethodName", "OSCAR"], [392, 393, "MethodName", "Transformer"], [406, 409, "TaskName", "masked language modeling"], [410, 411, "DatasetName", "MLM"], [477, 478, "MethodName", "UNITER"], [481, 482, "MethodName", "Transformer"], [512, 513, "DatasetName", "COCO"], [568, 572, "TaskName", "cross - modal retrieval"], [583, 584, "MethodName", "Transformer"], [775, 776, "MethodName", "Transformer"], [790, 791, "DatasetName", "MLM"], [822, 826, "TaskName", "cross - modal retrieval"], [901, 902, "DatasetName", "Flickr30k"], [903, 904, "DatasetName", "COCO"]]}
{"text": "For pre - training , we use pre - processed data provided by , including 4.2 million 8 The computation time of LightningDOT is negligible compared to that of UNITER . Therefore , the empirical speed is proportional to the number of pairs UNITER has to rank : constant M for LightningDOT + UNITER vs. the whole database ( index ) size for UNITER only . images with 9.5 million associated captions from COCO ( Chen et al , 2015 ) , VG ( Krishna et al , 2017 ) , Conceptual Captions ( Sharma et al , 2018 ) , and SBU captions ( Ordonez et al , 2011 ) . For evaluation , we use Flickr30k ( Plummer et al , 2015 ) and COCO ( Lin et al , 2014 ) datasets , which include 31K/123 K images , respectively , each associated with 5 human - written captions . Following ( Faghri et al , 2017 ) , we split COCO into 114K/5K/5 K and Flickr30 K into 29K/1k/1k images for train , validation and test . Downstream performance is measured by recall at K ( R@K ) for both image and text retrieval tasks . We also use an additional metric \" AR \" , the average of R@K for all K across both image and sentence retrieval tasks .", "entities": [[29, 30, "MethodName", "UNITER"], [43, 44, "MethodName", "UNITER"], [53, 54, "MethodName", "UNITER"], [63, 64, "MethodName", "UNITER"], [73, 74, "DatasetName", "COCO"], [91, 93, "DatasetName", "Conceptual Captions"], [117, 118, "DatasetName", "Flickr30k"], [126, 127, "DatasetName", "COCO"], [164, 165, "DatasetName", "COCO"]]}
{"text": "We conduct ablation studies on Flickr30 K ( Table 4 ) and compare LightningDOT ( L4 ) against 3 ablated instances : ( i ) \" R - CNN only \" ( L1 ) : image representations are extracted from Faster R - CNN directly , with no image encoder applied ; ( ii ) \" + Image Encoder \" ( L2 ) : regional features are encoded with a 12 - layer Transformer as the image encoder ; ( iii ) \" + PT \u2020 \" ( L3 ) : our model is pre - trained with MLM+MRM+CMR , then finetuned on Flickr30K. Note that the difference between MLM vs. VMLM and MRM vs. SMRM is whether the predictions of masked tokens ( regions ) rely on infused embeddings from the other modality . Multi30 K and COCO datasets . We compare with task - specific methods : S - LIWE ( Wehrmann et al , 2019 ) , MULE , SMALR ( Burns et al , 2020 ) , pre - trained method M 3 P ( Huang et al , 2020a ) Results show that \" R - CNN only \" is not sufficient in learning good image representations for ITR task , while image encoder with Transformer architecture can effectively learn contextualized image representations , hence achieving better performance . Pre - trained models ( L3 - 4 ) generally achieve better performance , compared to nonpretrained models ( L1 - 2 ) . Comparing \" + PT \u2020 \" to the full instance of LightningDOT , dependency on the other modality in VMLM and SMRM brings universal performance lift across all metrics . This indicates that these cross - modal dependencies introduced by VMLM and SMRM are effective in learning the association between image and text inputs . In addition , we investigate the effectiveness of each pre - training task in Table 5 . Comparing to baseline without pre - training , pre - training with CMR alone lifts +1.4 on AR . Pre - training with all three tasks achieves the best performance , indicating that the learning of contextualized word and region representations promotes better global alignment between image and text , and these three pre - training tasks work collaboratively to yield better visual - semantic embeddings .", "entities": [[26, 29, "MethodName", "R - CNN"], [40, 44, "MethodName", "Faster R - CNN"], [73, 74, "MethodName", "Transformer"], [109, 110, "DatasetName", "MLM"], [138, 139, "DatasetName", "COCO"], [189, 192, "MethodName", "R - CNN"], [210, 211, "MethodName", "Transformer"]]}
{"text": "We show an example of image retrieval results here at figure 4 for query as \" Sky view of a blue and yellow biplane flying near each other \" . In addition to the ground truth image in the red rectangle , all the 10 images retrieved by our model are valid retrieval since multiple keywords ( \" sky \" , \" blue \" , \" yellow \" , \" airplane \" , \" near \" ) are captured for each image . Please see the appendix A.4 for more examples .", "entities": [[5, 7, "TaskName", "image retrieval"]]}
{"text": "In this paper , we propose a pre - training framework that learns joint visual - semantic embedding without any cross - attention between modalities . Light - ningDOT outperforms previous state of the art , while significantly speeding up inference time by 600 - 2000\u00d7 on Flickr30 K and COCO image - text retrieval benchmarks . Future work includes extending the efficient training framework to other V+L tasks .", "entities": [[50, 51, "DatasetName", "COCO"]]}
{"text": "When evaluating on ITR under the multilingual setting , we consider two benchmarks : Multi30 K ( Elliott et al , 2016 ( Elliott et al , , 2017Barrault et al , 2018 ) and COCO Japanese ( Yoshikawa et al , 2017 ) and Chinese ( Li et al , 2019b ) . Multi30 K is constructed by manually translating English captions in Flickr30 K ( Plummer et al , 2015 ) to German , French , and Czech . Each image in Multi30 K is paired with 5 captions in German , 1 caption in French and Czech . We adopt the same train / val / test split as in Flickr30K. COCO Japanese ( Yoshikawa et al , 2017 ) collected 820 K Japanese captions for 165 K COCO images ( Lin et al , 2014 ) . We use the same train / dev / test splits for COCO Japanese as in Karpathy and Fei - Fei ( 2015 ) , and present results on the 1 K test set . Similarly , Li et al ( 2019b ) collected 1 - 2 Chinese captions per image for 20 K COCO images to build COCO Chinese . We follow the original split defined in Li et al ( 2019b ) .", "entities": [[35, 36, "DatasetName", "COCO"], [114, 115, "DatasetName", "COCO"], [131, 132, "DatasetName", "COCO"], [152, 153, "DatasetName", "COCO"], [194, 195, "DatasetName", "COCO"], [198, 199, "DatasetName", "COCO"]]}
{"text": "We present the detailed inference time of UNITERbase , SCAN the proposed LightningDOT and LightningDOT with UNITER - base re - ranker in Table 7 , measured by seconds / query . UNITER clearly is the slowest , as the 12 - layer Transformer model inference needs to be run between each query and all images . Comparing between Flickr30k - test and COCO - test , its inference time scales up linearly with the number of images . With the lightweight GRU ( Chung et al , 2014 ) , SCAN is \u223c1.9\u00d7 faster than UNITER . Across all settings , LightningDOT is significantly faster than both cross - attention methods ( UNITER - base and SCAN ) . When adding UNITER - base as the re - ranker , our method slows down by \u223c10 , but still achieves decent speedup .", "entities": [[9, 10, "DatasetName", "SCAN"], [16, 17, "MethodName", "UNITER"], [32, 33, "MethodName", "UNITER"], [43, 44, "MethodName", "Transformer"], [59, 60, "DatasetName", "Flickr30k"], [63, 64, "DatasetName", "COCO"], [82, 83, "MethodName", "GRU"], [91, 92, "DatasetName", "SCAN"], [96, 97, "MethodName", "UNITER"], [113, 114, "MethodName", "UNITER"], [117, 118, "DatasetName", "SCAN"], [122, 123, "MethodName", "UNITER"]]}
{"text": "We show several qualitative results of image retrieval ( top - 10 ) . All results are retrieved from COCO - Full dataset ( 123k images in total ) . Our model can well understand the underlying semantic meaning . For example , \" romantic \" only appears twice in the whole COCO dataset annotations , yet the top retrieved images are all topic - related ( Figure 5 ) . With multiple keywords , our model attempts to retrieve the combinations of them ( if not all ) . For example , for the query \" blue girl boy ball \" with four keywords , our model retrieves images Figure 5 : Retrieved top - 10 images for query \" romantic \" . Figure 6 : Retrieved top - 10 images for query \" blue girl boy ball \" that capture at least three keywords ( Figure 6 ) . We also present image retrieval results where the text query is sampled from COCO dataset . We randomly sample 3 queries and present the results as below ( ground truth on the top , retrieved top - 10 images at the bottom ) . Clearly , our model retrieves related images from the full dataset .", "entities": [[6, 8, "TaskName", "image retrieval"], [19, 20, "DatasetName", "COCO"], [52, 53, "DatasetName", "COCO"], [154, 156, "TaskName", "image retrieval"], [164, 165, "DatasetName", "COCO"]]}
{"text": "Back in 1997 , Hearst tried to detect the structure of text using patterns of lexical co - occurrence to identify paragraphs related to the same topic ( Hearst , 1997 ) . In this case , term repetition proved to be enough to detect subtopics in explanatory texts , but did not include consideration about other traits of the discourse ( e.g. syntactic constructions , verb tenses , number of adjectives in each region ) neither recovering more meaning further than topic identification , as could be the purpose intended on the paragraph ( s ) . Besides , the author remarked that the results had proved highly valuable when applied to explanatory text , but they would be less significant for other text types . From another point of view , Bachand ( Bachand et al , 2014 ) develops a research focused on the relations between text - type , discourse structures and rhetorical relations . Again , the experiments conducted are implemented on a single type of feature , this time rhetorical relations and markers . The good results obtained by the author indicate that our approach , which is grounded in similar intuitions , can reach comparable developments that we expect will enrich our capacity for generating accurate document plans . Regarding reviews , most of the work developed refers to sentiment analysis or polarity classifica - tion ( Cambria et al , 2013 ) . A few research works have been focused on the structure related to textual genres , relying on the Systemic Functional Theory ( Taboada , 2011 ) . The relations of different parts of the text with several purposes are revealed , focusing their analysis on the domain of movie reviews , and showing at the same time the variability of the ordering in such type of documents . Finally , a special mention must be done to the Systemic Functional Theory ( Halliday et al , 2014 ) . It provides a notion of genre that connects situation types with semantic / lexico - grammatic patterns from a conception of language highly related to its socio - semiotic origin . A textual typology is depicted on this terms , connected as well with the context of the discourse and the semantic choices to organise it . On the other hand , and as a more precise example , the typology of processes that Halliday and Mathiessen describe , directly influences the classification accomplished by ADESSE , one of the resources applied in our experiments over Spanish reviews , explained in the next section .", "entities": [[226, 228, "TaskName", "sentiment analysis"]]}
{"text": "We believe that , in order to become more meaningful , the quality of features could be improved by means of some resources rooted in Web Semantic technologies . There is some research related to genres that can be useful in our project . In the ADESSE verb senses Mental , material , relational , verbal , existential and modulation FREELING features PoS tagging : noun , adjective , pronoun , verb ( tense , aspect , ... ) , etc . realm of reviews , opinion and sentiment annotation , we can take advantage for example of MARL Ontology Specification 1 , a data schema that has been used in the EuroSentiment Project ( Buitelaar et al , 2013 ) or directly related to reviews from a Sentiment Analysis perspective ( Santosh and Vardhan , 2015 ) . Other genres have been targeted for similar developments . With regard to news genre , in order to obtain more significant annotation of the documents , BBC provides a set of ontologies related to their contents . DBPedia has been already proved useful for Wikipedia articles researchers . Drammar ( Lombardo and Damiano , 2012 ) and OntoMedia ( Jewell et al , 2005 ) are ontology - based models for annotating features of media and cultural narratives . All of them represent resources that may lead to different results in our clustering task and analysis .", "entities": [[99, 100, "MethodName", "Ontology"], [128, 130, "TaskName", "Sentiment Analysis"], [176, 177, "DatasetName", "DBPedia"], [205, 206, "MethodName", "ontology"]]}
{"text": "Pre - trained language models have been found to capture a surprisingly rich amount of lexical knowledge , ranging from commonsense properties of everyday concepts to detailed factual knowledge about named entities . Among others , this makes it possible to distill high - quality word vectors from pre - trained language models . However , it is currently unclear to what extent it is possible to distill relation embeddings , i.e. vectors that characterize the relationship between two words . Such relation embeddings are appealing because they can , in principle , encode relational knowledge in a more finegrained way than is possible with knowledge graphs . To obtain relation embeddings from a pre - trained language model , we encode word pairs using a ( manually or automatically generated ) prompt , and we fine - tune the language model such that relationally similar word pairs yield similar output vectors . We find that the resulting relation embeddings are highly competitive on analogy ( unsupervised ) and relation classification ( supervised ) benchmarks , even without any task - specific fine - tuning . 1", "entities": [[105, 107, "TaskName", "knowledge graphs"], [169, 171, "TaskName", "relation classification"]]}
{"text": "One of the most widely studied aspects of word embeddings is the fact that word vector differences capture lexical relations ( Mikolov et al , 2013a ) . While not being directly connected to downstream performance on NLP tasks , this ability of word embeddings is nonetheless important . For instance , understanding lexical relations is an important prerequisite for understanding the meaning of compound nouns . Moreover , the ability of word vectors to capture semantic relations has enabled a wide range of applications beyond NLP , including flexible querying of relational databases ( Bordawekar and Shmueli , 2017 ) , schema match - 1 Source code to reproduce our experimental results and the model checkpoints are available in the following repository : https://github.com/asahi417/relbert ing ( Fernandez et al , 2018 ) , completion and retrieval of Web tables ( Zhang et al , 2019 ) , ontology completion ( Bouraoui and Schockaert , 2019 ) and information retrieval in the medical domain ( Arguello Casteleiro et al , 2020 ) . More generally , relational similarity ( or analogy ) plays a central role in computational creativity ( Goel , 2019 ) , legal reasoning ( Ashley , 1988 ; Walton , 2010 ) , ontology alignment ( Raad and Evermann , 2015 ) and instance - based learning ( Miclet et al , 2008 ) . Given the recent success of pre - trained language models ( Devlin et al , 2019 ; Liu et al , 2019 ; Brown et al , 2020 ) , we may wonder whether such models are able to capture lexical relations in a more faithful or fine - grained way than traditional word embeddings . However , for language models ( LMs ) , there is no direct equivalent to the word vector difference . In this paper , we therefore propose a strategy for extracting relation embeddings from pre - trained LMs , i.e. vectors encoding the relationship between two words . On the one hand , this will allow us to gain a better understanding of how well lexical relations are captured by these models . On the other hand , this will also provide us with a practical method for obtaining relation embeddings in applications such as the ones mentioned above . Since it is unclear how LMs store relational knowledge , rather than directly extracting relation embeddings , we first fine - tune the LM , such that relation embeddings can be obtained from its output . To this end , we need a prompt , i.e. a template to convert a given word pair into a sentence , and some training data to fine - tune the model . To illustrate the process , consider the word pair Paris - France . As a possible input to the model , we could use a sentence such as \" The relation between Paris and France is < mask > \" . Note that our aim is to find a strategy that can be applied to any pair of words , hence the way in which the input is represented needs to be sufficiently generic . We then fine - tune the LM such that its output corresponds to a relation embedding . To this end , we use a crowdsourced dataset of relational similarity judgements that was collected in the context of SemEval 2012 Task 2 ( Jurgens et al , 2012 ) . Despite the relatively small size of this dataset , we show that the resulting fine - tuned LM allows us to produce high - quality relation embeddings , as confirmed in our extensive evaluation in analogy and relation classification tasks . Importantly , this also holds for relations that are of a different nature than those in the SemEval dataset , showing that this process allows us to distill relational knowledge that is encoded in the pre - trained LM , rather than merely generalising from the examples that were used for fine - tuning .", "entities": [[8, 10, "TaskName", "word embeddings"], [43, 45, "TaskName", "word embeddings"], [148, 149, "MethodName", "ontology"], [158, 160, "TaskName", "information retrieval"], [162, 164, "DatasetName", "medical domain"], [207, 208, "MethodName", "ontology"], [282, 284, "TaskName", "word embeddings"], [615, 617, "TaskName", "relation classification"]]}
{"text": "In this section , we present our main experimental results , testing the relation embeddings learned by RelBERT on analogy questions ( Section 5.1 ) and relation classification ( Section 5.2 ) .", "entities": [[26, 28, "TaskName", "relation classification"]]}
{"text": "In our main experiments , RelBERT is trained using the SemEval 2012 Task 2 dataset . This dataset contains a broad range of semantic relations , including hypernymy and meronymy relations . This raises an important question : Does RelBERT provide us with a way to extract relational knowledge from the parameters of the As a further analysis , Table 5 shows a breakdown of the Google and BATS analogy results , showing the average performance on each of the top - level categories from these datasets . 10 While RelBERT is outperformed by FastText on the morphological relations , it should be noted that the differences are small , while such relations are of a very different nature than those from the SemEval dataset . This confirms that RelBERT is able to model a broad range of relations , although it can be expected that better results would be possible by including task - specific training data into the fine - tuning process ( e.g. including morphological relations for tasks where such relations matter ) .", "entities": [[66, 67, "DatasetName", "Google"], [94, 95, "MethodName", "FastText"]]}
{"text": "Nearest Neighbors barista : coffee baker : bread , brewer : beer , bartender : cocktail , winemaker : wine , bartender : drink , baker : cake bag : plastic bottle : plastic , bag : leather , container : plastic , box : plastic , jug : glass , bottle : glass duck : duckling chicken : chick , pig : piglet , cat : kitten , ox : calf , butterfly : larvae , bear : cub cooked : raw raw : cooked , regulated : unregulated , sober : drunk , loaded : unloaded , armed : unarmed , published : unpublished chihuahua : dog dachshund : dog , poodle : dog , terrier : dog , chinchilla : rodent , macaque : monkey , dalmatian : dog dog : dogs cat : cats , horse : horses , pig : pigs , rat : rats , wolf : wolves , monkey : monkeys spy : espionage pirate : piracy , robber : robbery , lobbyist : lobbying , scout : scouting , terrorist : terrorism , witch : witchcraft", "entities": [[121, 122, "MethodName", "chinchilla"]]}
{"text": "Figure 3 compares the performance of RelBERT with that of the vanilla pre - trained RoBERTa model ( i.e. when only the prompt is optimized ) . As can be seen , the fine - tuning process is critical for achieving good results . In Figure 3 , we also compare the performance of our main RelBERT model , which is based on RoBERTa , with versions that were instead initialized with BERT ( Devlin et al , 2019 ) and ALBERT ( Lan et al , 2019 ) . 11 RoBERTa clearly outperforms the other two LMs , which is in accordance with findings from the literature suggesting that RoBERTa captures more semantic knowledge Warstadt et al , 2020 ) .", "entities": [[15, 16, "MethodName", "RoBERTa"], [63, 64, "MethodName", "RoBERTa"], [72, 73, "MethodName", "BERT"], [81, 82, "MethodName", "ALBERT"], [91, 92, "MethodName", "RoBERTa"], [110, 111, "MethodName", "RoBERTa"]]}
{"text": "To give further insight into the nature of RelBERT embeddings , Table 6 shows the nearest neighbors of some selected word pairs from the evaluation datasets . To this end , we computed RelBERT relation vectors for all pairs in the Wikipedia pretrained RELATIVE vocabulary ( over 1 M pairs ) . 12 The neighbors are those word pairs whose Rel - BERT embedding has the highest cosine similarity within the full pair vocabulary . As can be seen , the neighbors mostly represent word pairs that are relationally similar , even for morphological relations ( e.g. dog : dogs ) , which are not present in the SemEval dataset . A more extensive qualitative analysis , including a comparison with RELATIVE , is provided in the appendix .", "entities": [[62, 63, "MethodName", "BERT"]]}
{"text": "We have proposed a strategy for learning relation embeddings , i.e. vector representations of pairs of words which capture their relationship . The main idea is to fine - tune a pre - trained language model using the relational similarity dataset from SemEval 2012 Task 2 , which covers a broad range of semantic relations . In our experimental results , we found the resulting relation embeddings to be of high quality , outperforming state - of - the - art methods on several analogy and relation classification benchmarks . Among the models tested , we obtained the best results with RoBERTa , when using manually defined templates for encoding word pairs . Importantly , we found that high - quality relation embeddings can be obtained even for relations that are unlike those from the SemEval dataset , such as morphological and encyclopedic relations . This suggests that the knowledge captured by our relation embeddings is largely distilled from the pre - trained language model , rather than being acquired during training .", "entities": [[86, 88, "TaskName", "relation classification"], [101, 102, "MethodName", "RoBERTa"]]}
{"text": "Table 9 shows additional results of word embeddings on analogy test together with RelBERT results . We concatenate the RELATIVE and pair2vec vectors with the word vector difference . However , this does not lead to better results .", "entities": [[6, 8, "TaskName", "word embeddings"]]}
{"text": "Table 11 shows the best hyperparameters in the validation set of the MLPs for relation classification .", "entities": [[14, 16, "TaskName", "relation classification"]]}
{"text": "Towards Unsupervised Text Classification Leveraging Experts and Word Embeddings", "entities": [[1, 4, "TaskName", "Unsupervised Text Classification"], [7, 9, "TaskName", "Word Embeddings"]]}
{"text": "Document classification is a standard task in machine learning ( Joachims , 1999 ; Sebastiani , 2002 ) . Its applications span a variety of \" use cases and contexts , e.g. , email filtering , news article clustering , clinical document classification , expertquestion matching \" . The standard process for text categorization relies on supervised and semisupervised approaches . The motivation for the present effort comes from the banking sector , in particular the management of operational risks . This category of risks corresponds to the broad set of incidents that are neither credit nor market risk and includes issues related to internal and external fraud , cybersecurity , damages on physical assets , natural disasters , etc . The practical management of operational risk is partially based on the management of a dataset of historical operational risk incidents where each incident is described in details and that is shared on a regular basis with regulators . Historically , all incident reports have been mapped to about twenty categories of risk issued from the regulator . However , from an operational perspective , a higher number of risk categories is relevant to better capture the nuances around the incidents and enable relevant comparisons . This led to the creation of a new internal risk taxonomy of risk composed of 264 categories , each described by a label ( a few words ) . To make it operational , the stock of all internal and external incident reports had to be classified into categories from the new internal taxonomy . However , since it had never been used before , we had no labeled samples readily available . As hundreds of thousands of incidents had to be processed , text classification seemed a promising approach to assist in that mapping task . Indeed , given the specificity of the domain and the lack of availability of experts , it was not conceivable to obtain many labeled examples for each category as would be required for supervised approaches . This is the issue addressed in this paper where describe our work towards an unsupervised approach to classify documents into a set of categories described by a short sentence ( label ) . While the inspiration of this paper is the classification of incident reports in operational risk , our approach aims to be readily transferable to other domains . For that purpose , we tested it on standard text classification corpora . The underlying idea is altogether simple . We emulate the approach that a domain expert would follow to manually assign an input document ( incident report , client review , news article , etc . ) to a given category . Specifically this entails developing an understanding of the categories semantic fields and then , for each document , to classify it into the closest category . The novelty of our method hinges on the diversity of enrichment techniques of the categories label , including expert input that assists the semantic expansion and the use of word embeddings , both generic and domain specific . The remainder of this paper is organized as follows . In Section 2 , we provide an overview of the relevant literature . Section 3 contains a detailed description of our approach . Sections 4 and 5 describe the results of its application to standard corpora and operational risks incidents respectively . We conclude in Section 6 .", "entities": [[0, 2, "TaskName", "Document classification"], [41, 43, "TaskName", "document classification"], [52, 54, "TaskName", "text categorization"], [290, 292, "TaskName", "text classification"], [408, 410, "TaskName", "text classification"], [508, 510, "TaskName", "word embeddings"]]}
{"text": "Our approach for unsupervised text classification is based on the choice to model the task as a text similarity problem between two sets of words : One containing the most relevant words in the document and another containing keywords derived from the label of the target category . While the key advantage of this approach is its simplicity , its success hinges on the good definition of a dictionary of words for each category . Figure 1 provides an overview of the main steps included in our method . On the document side , we simply perform standard cleaning steps . On the category labels side , besides the same initial processing , we implement a series of enrichment steps so as to iteratively expand label dictionaries . Before proceeding to the comparison of documents and labels via a similarity metric , we have added a consolidation step which considers all expanded label dictionaries and makes adjustments so that they are as discriminating as possible . We compare documents and labels by computing a similarity metric between cleaned documents and dictionaries . We provide further details into each of these main steps in the following subsections . In terms of notation , we refer to the unlabeled corpus as C , its vocabulary as V and and assume that we have M text categories to which documents in C need to be mapped .", "entities": [[3, 6, "TaskName", "unsupervised text classification"], [17, 19, "TaskName", "text similarity"]]}
{"text": "Cleaning of either documents or category labels is done as follows : After tokenization , we start by replacing a list of common abbreviations , e.g. , Mgt , Mngt , IT , ATM provided by business with their associated expansions . Similarly we spell out negative contractions . We then remove uninformative tokens including ( i ) isolated and special characters such as i , a , o , op , @ , * , ( ii ) punctuation ( iii ) stopwords ( based on stopword lists from NLTK 's list of english stopwords , scikit - learn version 0.18.2 , spaCy version 1.8.2 ) ( iv ) common words across documents such as risky , dangerous , based on the highest Term Frequency ( top 3 % ) ( v ) uncommon words , i.e. , top 3 % in terms of Inverse Term Frequency ( vi ) specific tokens such as dates , nationalities , countries , regions , bank names . For instance , to extract dates , we use both regular expression and fuzzy matching to identify all sorts of date - like strings ( e.g. , February can also be written as Feb or Febr ) . Regarding nationalities and bank names , we combined different lists coming from Wikipedia , business experts and fuzzy matching ( e.g. , BNP Paribas could be found as BNP , BNPParibas , BNP Securities , BNP Trading , BNP Group , etc . ) . As the taxonomy is designed to be universal , such tokens are not relevant to the text classification task and are thus removed . To give a concrete example , the following snippet of operational incident \" On 18 June 2013 the US Commodity Futures Trading Commission ( CFTC ) fined ABN AMRO Clearing Chicago USD 1 million ( EUR 748 , 000 ) for failing to segregate or secure sufficient customer funds , failing to meet the minimum net capital requirements , failing to maintain accurate books and records , and failing to supervise its employees ... \" would have been transformed into \" fine fail segreg secur suffici custom fund fail meet minimum net capit requir fail maintain accur book record fail supervis employe .. \"", "entities": [[265, 267, "TaskName", "text classification"]]}
{"text": "As mentioned previously , once we have clean labels , we make a series of enrichment steps . First , we make use of Expert Knowledge , i.e. , a human expert is asked to provide 3 to 5 additional words for each label . While this constitutes a small amount of manual effort , there are multiple ways to approximate this task without human intervention , for example , by querying Wikipedia or the web with the category name and performing token counts over retrieved entries . Before proceeding to the next enrichment step , we also add to the label dictionaries all the spelling variants of the expert - provided words that can be found in the document corpus . We also remove any word whose stem is not in the document corpus . Second , we leverage WordNet ( Fellbaum , 1998 ) to obtain knowledge - based synonyms . For every word obtained in the previous step , we add to the label dictionary all the associated synonym sets ( English nouns , verbs , and adjectives ) . Again , once this step is completed , we remove all words where the stem is not in the vocabulary V. Third , we bootstrap the label dictionary obtained upon this point by making use of representative documents . A representative sentence for a given category is defined by Ko and Seo ( 2000 ) as a sentence in the document corpus that contains manually pre - defined keywords of the category in its content words . In this work , we extend this definition to apply to documents instead of sentences and to include all categories ' keywords obtained at this stage . Therefore we calculate a similarity score between each pair of input document - category label keywords using cosine distance and Latent Semantic Analysis . The text similarity metric will be details in section 3.4 . For this step , we use an empirically identified similarity threshold ( 70 % ) . Then , for each identified representative document , we add all its words to the label dictionary . Finally , we make use of word embeddings ( Bengio et al , 2003 ; Mikolov et al , 2013a , b ) to further capture semantically similar words to the ones belonging to each label dictionary . We first proceed with pre - trained models which enable to identify semantically similar words used in the general domain . In our case , we used Glove 1 ( Pennington et al , 2014 ) , The model is pre - trained on a corpus using Wikipedia2014 and Gigaword5 , with a 330 vocabulary of the top 400 , 000 most frequent words and a context window size of 10 . Furthermore , we also seek to obtain similar words as used in the specific domain of the corpus . Since the neighbors of each keyword are semantically related in embedding space ( Mikolov et al , 2013b ) , we train a Word2Vec model , trained on all input documents cleaned then joined together . In this work , we tested its two main architectures : 1 https://nlp.stanford.edu/projects/glove/ Continous Bag of words ( CBOW ) that predicts a word based on its context defined by a sliding window of words and Skip - Gram ( SG ) which predicts the context given the target word . Experimental settings will be detailed in section 4.3 .", "entities": [[312, 314, "TaskName", "text similarity"], [362, 364, "TaskName", "word embeddings"]]}
{"text": "Once all labels have been associated with dictionaries , we perform a final step in order to reduce keyword overlap among all dictionaries . In essence , we favor words that are representative ( salient ) for the category in the sense that they have the ability to distinguish the category label from the other categories . We adapt the Function - aware Component ( FAC ) originally used in supervised document classification ( Liu et al , 2018 ) . F AC ( w , c ) = T F ( w , c ) \u2212 1 M 1\u2264k\u2264M T F ( w , k ) var ( T F \u2212c ( w ) ) ( 1 ) where T F \u2212c ( w ) is the collection of term frequencies except the c - th category and var ( ) is the variance . The consolidation step consists in computing the above metric for every word in the label dictionaries and to filter out those whose associated metric is below a given threshold . This latter threshold depends on two main constraints : The maximum number of categories that contain a given word and the minimum word frequency in the label dictionaries . Regarding the first constraint , in our practical case of operational risk taxonomy , we have 264 target categories that could be grouped into 16 broad categories : cyber - security , fraud , compliance , human resources , etc . Thresholds are determined so as to tolerate overlap within each broad category and to minimize it outside . More generally , we start by identifying the maximum number of semantically similar categories , i.e. , where we would expect some overlap and we set the threshold consequently . By construction , keywords in a given dictionary occur at least one time . We decided not to set an additional constraint on word frequency per category label so as to keep highly specific words with a low frequency , generally captured by the Word2vec model trained on the input corpus .", "entities": [[71, 73, "TaskName", "document classification"]]}
{"text": "In order to evaluate our approach , we conduct experiments on five standard text classification corpora , described listed in Table 1 . As we use an unsupervised approach for text classification , we make use of the whole corpus of each dataset by aggregating training and test sets . We describe each corpus briefly : ( 1 ) The 20NewsGroup 2 dataset consists of 18 , 846 news articles divided almost evenly among 20 different UseNet discussion groups . Some of the newsgroups are closely related ( e.g. , comp.sys.ibm.pc.hardware and comp.sys.mac.hardware ) . While each document may discuss multiple topics , it needs to be assigned to a single category . ( 2 ) The AG 's Corpus of news articles 3 is a collection of more than 1 million news articles . We used the version created by Zhang et al ( 2015 ) who selected 4 largest classes from AG news corpus on the web with each instance containing class index , title and description fields . ( 3 ) The Yahoo - Answers 4 corpus contains 4 , 483 , 032 questions and their corresponding answers from Yahoo ! Answers service as of 10/25/2007 . We used the version constructed by Zhang et al ( 2015 ) using 10 largest main categories and the best answer content from all the answers . ( 4 ) The 5AbstractsGroup 5 dataset is a collection of academic papers from five different domains collected from Web of Science namely , business , artificial intelligence , sociology , transport and law . We extracted the abstract and title fields of each paper as a document . ( 5 ) The Google - Snippets 6 dataset contains the web search results related to 8 different domains such as business , computers and engineering .", "entities": [[13, 15, "TaskName", "text classification"], [30, 32, "TaskName", "text classification"], [153, 155, "DatasetName", "AG news"], [192, 195, "DatasetName", "Yahoo ! Answers"], [280, 281, "DatasetName", "Google"]]}
{"text": "We apply multiple variants of our method to each of the above corpora . Note first that using representative documents ( Section 3.2 ) to enrich label dictionaries is suitable for categories whose labels take the form of a structured sentence containing more than 10 words before cleaning . In the application to operational risk incidents ( Section 5 ) , it allowed to enrich 13 % of dictionaries . In the standard text classification datasets used in our experiments , category labels contain less than 5 words so representative documents were not relevant in the enrichment process . Thus none of the configurations discussed in this section include this step . Overall , in addition to the full pipeline , which we refer to as all keywords , we also investigated whether semantic expansion solely through word embeddings could improve performance . We thus tested with either generic embeddings ( pre - trained Glove ) or corpus - based embeddings ( Word2Vec ) . Finally , for each configuration , we tested with and without the function aware component ( FAC ) for consolidation of the label dictionaries . We also implemented simple baselines for comparison . On the unsupervised side , ( 1 ) we calculated a text similarity score between each docu - ment and the set of expert provided keywords ( 2 ) we enriched this list of initial keywords with their synonyms from WordNet . On the supervised side , we use Multinomial Na\u00efve Bayes as a basic baseline where we represented each document as TF - IDF vector ( bag of words ) , cleaned the input corpus in the same way as in our proposed approach and split each dataset into a training set ( 2/3 ) and a test set ( 1/3 ) .", "entities": [[73, 75, "TaskName", "text classification"], [137, 139, "TaskName", "word embeddings"], [209, 211, "TaskName", "text similarity"]]}
{"text": "In this paper , we present a method for unsupervised text classification based on computing the similarity between the documents to be classified and a rich description of the categories label . The category label enrichment starts with humanexpert provided keywords but is then expanded through the use of word embeddings . We also investigated whether a consolidation step that removes non discriminant words from the label dictionaries could have an effect on performance . We have not explored whether recent advances in word embeddings from instance ELMO ( Peters et al , 2018 ) and BERT ( Devlin et al , 2018 ) could add further benefits . This is certainly an avenue that we seek to explore . However , for our application domain , we expect that it may not lead to increased performance as words are used to a large extent with the same sense across the corpus .", "entities": [[9, 12, "TaskName", "unsupervised text classification"], [49, 51, "TaskName", "word embeddings"], [83, 85, "TaskName", "word embeddings"], [87, 88, "MethodName", "ELMO"], [96, 97, "MethodName", "BERT"]]}
{"text": "Human language encompasses more than just text ; it also conveys emotions through tone and gestures . We present a case study of three simple and efficient Transformer - based architectures for predicting sentiment and emotion in multimodal data . The Late Fusion model merges unimodal features to create a multimodal feature sequence , the Round Robin model iteratively combines bimodal features using cross - modal attention , and the Hybrid Fusion model combines trimodal and unimodal features together to form a final feature sequence for predicting sentiment . Our experiments show that our small models are effective and outperform the publicly released versions of much larger , state - of - the - art multimodal sentiment analysis systems .", "entities": [[27, 28, "MethodName", "Transformer"], [35, 36, "DatasetName", "emotion"], [115, 118, "TaskName", "multimodal sentiment analysis"]]}
{"text": "Language is composed of three different modalities : text , audio , and video . These three modalities together make it easier for humans to convey emotion and sentiment . Thus , a machine learning model for sentiment analysis needs to learn the features and interactions of all three modalities . For example , a frown in the video can alter the emotion expressed in the text transcript , or audio intensity can help determine if a speaker is getting agitated . Multimodal learning has recently received a good deal of attention from the natural language processing community [ Sun et al , 2016 , Chen et al , 2018 , Pham et al , 2019 . The Transformer network [ Vaswani et al , 2017 ] , with its self - attention modules , has achieved strong performance in multimodal learning ; attention provides a natural way to model the relationship between pairs of modalities . In this work we investigate three small , lightweight , Transformer - based architectures for multimodal sentiment analysis and emotion recog - nition . Our first model is an implementation of the Late Fusion model commonly used as a baseline system , which assigns individual Transformer blocks to each of the three modalities for feature extraction and then combines these unimodal features to learn cross - modal interactions . The second model is an implementation of the Round Robin approach ; the model generates bimodal features by using cross - modal attention to combine pairs of modalities , one pair at a time . Our last model is a Hybrid of the early and late fusion schemes . This model merges the features extracted using a late fusion pipeline , as well as those from an early fusion pipeline , where the three modalities are concatenated and passed through a single Transformer block for feature extraction ; . We present experiments using these three models on three multimodal datasets : IEMOCAP [ Busso et al , 2008 ] , an emotion recognition dataset , and CMU - MOSI [ Zadeh et al , 2016 ] and CMU - MOSEI [ Zadeh et al , 2018b ] , two multimodal sentiment analysis datasets . Our results show that our small models are competitive with state - of - the - art models that use much more complex architectures . Our main contributions are as follows : We present three lightweight architectures for multimodal sentiment analysis that achieve comparable results to much larger , state - ofthe - art models . We analyze the effect of removing or simplifying components of state - of - the - art multimodal architectures . We conduct experiments on small training sets , demonstrating the ability of our lightweight architectures to leverage limited training data and computational resources .", "entities": [[26, 27, "DatasetName", "emotion"], [37, 39, "TaskName", "sentiment analysis"], [62, 63, "DatasetName", "emotion"], [118, 119, "MethodName", "Transformer"], [167, 168, "MethodName", "Transformer"], [172, 175, "TaskName", "multimodal sentiment analysis"], [176, 177, "DatasetName", "emotion"], [202, 203, "MethodName", "Transformer"], [308, 309, "MethodName", "Transformer"], [327, 328, "DatasetName", "IEMOCAP"], [337, 339, "TaskName", "emotion recognition"], [344, 345, "DatasetName", "MOSI"], [353, 356, "DatasetName", "CMU - MOSEI"], [365, 368, "TaskName", "multimodal sentiment analysis"], [408, 411, "TaskName", "multimodal sentiment analysis"]]}
{"text": "We do not give an exhaustive list of prior work in multimodal sentiment analysis , but focus on recent neural approaches that achieved state - of - the - art performance at their times of publication .", "entities": [[11, 14, "TaskName", "multimodal sentiment analysis"]]}
{"text": "The Memory Fusion Network ( MFN ) of Zadeh et al [ 2018a ] uses a separate LSTM to encode each of the three modalities and then uses attention to model cross - modal interactions for different combinations of modalities . The Recurrent Attended Variation Embedding Network ( RAVEN ) of Wang et al [ 2019 ] encodes the audio and video features using two recurrent neural networks ; these features are combined with the textual input using cross - modal attention in a Gated Modality Mixing Network . The Multi - Attention Recurrent Network ( MARN ) of Zadeh et al [ 2018c ] is an LSTM - based architecture that stores representations of each of the three modalities , which are then combined using a multi - attention block . Finally , the Multimodal Cyclic Translation Network ( MCTN ) of Pham et al [ 2019 ] produces multimodal features by translating one modality into another , learning a joint encoding in that direction , and then back - translating to learn a joint encoding in the other direction .", "entities": [[17, 18, "MethodName", "LSTM"], [48, 49, "DatasetName", "RAVEN"], [107, 108, "MethodName", "LSTM"], [137, 138, "TaskName", "Translation"]]}
{"text": "The Transformer network [ Vaswani et al , 2017 ] has been used widely in neural machine translation [ Tubay and Costa - juss\u00e0 , 2018 , Edunov et al , 2018 , Xia et al , 2019 , Devlin et al , 2019 and has proven effective for sentiment analysis and emotion recognition . However , existing architectures are very dense compared to our three lightweight models . The Multimodal Transformer ( MuLT ) of Tsai et al [ 2019 ] modifies the Transformer block to compute cross - modal attention for two modalities at a time . It combines modalities in directed pairs , using a total of six Transformers , whose outputs are then merged into a single multimodal representation . Unlike other works , MuLT is able to handle cases where the three modalities are not aligned at the word level ; it learns soft alignments via the cross - modal attention weights for each pair of modalities . The model works well in the unaligned case , and in the aligned case , it gives state of the art performance the Happy emotion in IEMO - CAP . The Factorized Multimodal Transformer ( FMT ) of introduces Factorized Multimodal Self - Attention ( FSM ) modules , which compute self - attention over unimodal , bimodal , and trimodal inputs in parallel . FMT gives state of the art performance in the word - aligned case on CMU - MOSI and on the Sad , Angry , and Neutral emotions in IEMOCAP . We use FMT , along with the word - aligned version of MuLT , as baselines for comparison in our experiments .", "entities": [[1, 2, "MethodName", "Transformer"], [16, 18, "TaskName", "machine translation"], [49, 51, "TaskName", "sentiment analysis"], [52, 54, "TaskName", "emotion recognition"], [71, 72, "MethodName", "Transformer"], [84, 85, "MethodName", "Transformer"], [187, 188, "DatasetName", "emotion"], [191, 192, "DatasetName", "CAP"], [196, 197, "MethodName", "Transformer"], [244, 245, "DatasetName", "MOSI"], [256, 257, "DatasetName", "IEMOCAP"]]}
{"text": "Our three lightweight architectures are comprised of Transformer blocks [ Vaswani et al , 2017 ] , which are non - recurrent neural networks that can process sequential data . It consists of alternating attention and linear layers . The attention block of a Transformer uses multi - head attention , where each head computes scaled dot product attention : attn ( Q , K , V ) = softmax QK T \u221a d k V head i = attn QW Q i , KW K i , V W V i multi ( Q , K , V ) = [ head 1 ; . . . ; head h ] W O where Q , K , V represent the query , key and value ; d k is the key dimension size ; W Q i , W K i , W V i are learned projection matrices for head i ; and W O is a learned projection matrix for the attention block . In addition , Vaswani et al note that positional encodings must be added to Transformer input because there is no sequential information present in the Transformer itself : P E ( pos , 2i ) = sin ( pos/10000 2i / d model ) P E ( pos , 2i+1 ) = cos ( pos/10000 2i / d model ) X = X + P E", "entities": [[7, 8, "MethodName", "Transformer"], [44, 45, "MethodName", "Transformer"], [46, 50, "MethodName", "multi - head attention"], [69, 70, "MethodName", "softmax"], [182, 183, "MethodName", "Transformer"], [193, 194, "MethodName", "Transformer"]]}
{"text": "Figure 1 shows our Late Fusion architecture . Three unimodal Transformers learn high - level features from the low - level input features of each modality . The outputs of these unimodal Transformers are then merged together using a simple summation , rather than the merge layer used in previous work [ Tsai et al , 2019 ] , and passed to a residual network of linear layers [ Xie et al , 2017 ] for sentiment prediction . Figure 2 shows our Round Robin architecture , which is a simplification of MuTL [ Tsai et al , 2019 ] . Three cross - modal Transformers learn bimodal feaatures for ordered pairs of modalities , where the query is one modality and the key / value is the other . We use only three pairs - text query and audio key / value , audio query and video key / value , and video query and text key / valuewith bimodal information flowing in only one direction ; in contrast , MuLT uses six pairs of crossmodal Transformers , with information flowing in both directions . MuLT also uses three Transformers , one for each modality , to merge the two pairs sharing that modality as key / value ; our pairwise features are simply concatenated and passed to the output residual network . Figure 3 shows our Hybrid Fusion architecture , which uses both an early fusion approach that concatenates the inputs and passes them to a single Transformer to learn trimodal features , as well as a late fusion approach that passes each modality through a separate Transformer to learn unimodal features . The trimodal and unimodal features are concatenated together and merged using a layer of Gated Recurrent Units .", "entities": [[63, 65, "MethodName", "residual network"], [221, 223, "MethodName", "residual network"], [249, 250, "MethodName", "Transformer"], [269, 270, "MethodName", "Transformer"]]}
{"text": "Text Features : For word - level textual features we use the pretrained , 300 - dimensional , Common Crawl GloVe embeddings [ Pennington et al , 2014 ] . Audio features , including Mel - frequency cepstral coefficients and transformations thereof , as well as harmonic , percussive , and glottal source parameters . We also use COVERAP [ Degottex et al , 2014 ] to extract pitch tracking and voiced / unvoiced sloping parameters , peak slope parameters , and maximum dispersion quotients . Video Features : We extract 35 facial units using Facet [ iMotions , 2017 ] , as well as 35 facial action units and 30 facial landmark and gaze fea - tures using OpenFace [ Baltrusaitis et al , 2018 ] .", "entities": [[18, 20, "DatasetName", "Common Crawl"], [20, 22, "MethodName", "GloVe embeddings"]]}
{"text": "We compare our results with the state - of - the - art Multimodal Transformer ( MuLT ) 1 [ Tsai et al , 2019 ] and Factorized Multimodal Transformer ( FMT ) , as well as Memory Fusion Network ( MFN ) [ Zadeh et al , 2018a ] , Recurrent Attended Variation Embedding Network ( RAVEN ) [ Wang et al , 2019 ] , Multi - Attention Recurrent Network ( MARN ) [ Zadeh et al , 2018c ] , and Multimodal Cyclic Translation Network ( MCTN ) [ Pham et al , 2019 ] . These systems are described in Section 2 ; all attained state of the art on at least one of the evaluation datasets at their times of publication , and all use a similar feature set to our work .", "entities": [[14, 15, "MethodName", "Transformer"], [29, 30, "MethodName", "Transformer"], [57, 58, "DatasetName", "RAVEN"], [86, 87, "TaskName", "Translation"]]}
{"text": "We perform ablation experiments on our models using the IEMOCAP dataset ; ablation results for CMU - MOSI and CMU - MOSEI are omitted due to space constraints , but exhibit similar trends . Table 6 presents the results of modality ablation on the simplest Late Fusion model ; it clearly shows that unimodal and bimodal models are unable to match the performance of a full multimodal model . This demonstrates the importance of considering all modalities when analyzing spoken language , since some of the emotions or sentiment may be dependent more on the audio or the visual actions of the speaker , rather than the text . Examining the unimodal results , we see that the Text modality is the most informative for predicting Happy , Sad , and Neutral , while Audio is the most informative for Angry . However , the bimodal results do not always match the unimodal results . The best - performing bimodal model for Happy is [ V , A ] , despite Video being the worst - performing single modality , and [ T , A ] is the worst - performing bimodal model , despite both Text and Audio outperforming Video individually . Considering the other three emotions , we see that the best bimodal model varies between [ T , A ] and [ V , A ] , with [ T , V ] generally performing the worst . Table 7 shows the results of modality ablation on the Round Robin model ; as the architecture does not support unimodal experiments , only bimodal results are shown . Comparing Table 6 to Table 7 , we see that the cross - modal Transformers of the full Round Robin model are outperformed by the full Late Fusion model . However , the relative performance among modality pairs is consistent across Tables 6 and 7 . Finally , Table 8 shows the results of modality ablation on the Hybrid Fusion model , where we compare the relative contributions of the early fusion and late fusion halves of the architecture . The top of the table shows the results of reducing the early fusion half to only two modalities while retaining all three modalities in the late fusion half , and the bottom shows the results of reducing the late fusion half to two modalities while retaining all three in the early fusion half ; in both sets of experiments , the overall model has access to all three modalities , but only through either the early fusion path or the late fusion path . Surprisingly , although standalone early fusion models are outperformed by standalone late fusion models [ Tsai et al , 2019 ] , we find that a hybrid model containing a full , trimodal early fusion half is more robust to modality ablation in its late fusion half than a model with a full late fusion half is to an ablated early fusion half . Our results in this experiment also show greater variability among modality pairs . The [ T , A ] combination , which gave the best performance in the Late Fusion and Round Robin experiments , remains the strongest modality pair for the full early fusion , bimodal late fusion model . In contrast , for the bimodal early fusion , full late fusion model , [ T , A ] is outperformed by one of the two Video - based modality pairs , [ T , V ] or [ V , A ] , on each of the four emotions , suggesting that the performance gap of early versus late fusion differs across modalities .", "entities": [[9, 10, "DatasetName", "IEMOCAP"], [17, 18, "DatasetName", "MOSI"], [19, 22, "DatasetName", "CMU - MOSEI"]]}
{"text": "We have presented three lightweight architectures for multimodal sentiment analysis and emotion recognition . The Late Fusion model merges unimodal features , the Round Robin model iteratively combines bimodal features , and the Hybrid Early - Late Fusion model combines early - fusion trimodal and late - fusion unimodal features . Our proposed models are much smaller in size compared to existing state - of - the - art models ; they are able to attain new state - of - the - art scores on the CMU - MOSI and CMU - MOSEI datasets on two metrics , while remaining competitive on the others . Further , our experiments analyzing the relative contribution of modalities and architecture components in our models suggest new directions for developing multimodal systems . We hope that our simple architectures for sentiment and emotion detection , currently the fastest and best - performing publicly available system , as well as the insights revealed in our experimental results , can be useful for further research in the field .", "entities": [[7, 10, "TaskName", "multimodal sentiment analysis"], [11, 13, "TaskName", "emotion recognition"], [89, 90, "DatasetName", "MOSI"], [91, 94, "DatasetName", "CMU - MOSEI"], [139, 140, "DatasetName", "emotion"]]}
{"text": "Deep Multi - Task Learning for Aspect Term Extraction with Memory Interaction *", "entities": [[1, 5, "TaskName", "Multi - Task Learning"], [7, 9, "TaskName", "Term Extraction"]]}
{"text": "We propose a novel LSTM - based deep multi - task learning framework for aspect term extraction from user review sentences . Two LSTMs equipped with extended memories and neural memory operations are designed for jointly handling the extraction tasks of aspects and opinions via memory interactions . Sentimental sentence constraint is also added for more accurate prediction via another LSTM . Experiment results over two benchmark datasets demonstrate the effectiveness of our framework . * The work described in this paper is substantially supported by a grant from the Research Grant Council of the Hong Kong Special Administrative Region , China ( Project Code : 14203414 ) . We thank Lidong Bing and Piji Li for their helpful comments on this draft and the anonymous reviewers for their valuable feedback .", "entities": [[4, 5, "MethodName", "LSTM"], [8, 12, "TaskName", "multi - task learning"], [15, 17, "TaskName", "term extraction"], [60, 61, "MethodName", "LSTM"]]}
{"text": "The aspect - based sentiment analysis ( ABSA ) task is to identify opinions expressed towards specific entities such as laptop or attributes of entities such as price ( Liu , 2012a ) . This task involves three subtasks : Aspect Term Extraction ( ATE ) , Aspect Polarity Detection and Aspect Category Detection . As a fundamental subtask in ABSA , the goal of the ATE task is to identify opinionated aspect expressions . One of most important characteristics is that opinion words can provide indicative clues for aspect detection since opinion words should co - occur with aspect words . Most publicly available datasets contain the gold standard annotations for opinionated aspects , but the ground truth of the corresponding opinion words is not commonly provided . Some works tackling the ATE task ignore the consideration of opinion words and just focus on aspect term modeling and learning ( Jin et al , 2009 ; Jakob and Gurevych , 2010 ; Toh and Wang , 2014 ; Chernyshevich , 2014 ; Manek et al , 2017 ; San Vicente et al , 2015 ; Liu et al , 2015 ; Poria et al , 2016 ; Toh and Su , 2016 ; Yin et al , 2016 ) . They fail to leverage opinion information which is supposed to be useful clues . Some works tackling the ATE task consider opinion information ( Hu and Liu , 2004a , b ; Popescu and Etzioni , 2005 ; Zhuang et al , 2006 ; Qiu et al , 2011 ; Liu et al , 2012bLiu et al , , 2013aLiu et al , , b , 2014 in an unsupervised or partially supervised manner . Qiu et al ( 2011 ) proposed Double Propagation ( DP ) to collectively extract aspect terms and opinion words based on information propagation over a dependency graph . One drawback is that it heavily relies on the dependency parser , which is prone to generate mistakes when applying on informal online reviews . Liu et al ( 2014 ) modeled relation between aspects and opinions by constructing a bipartite heterogenous graph . It can not perform well without a high - quality phrase chunker and POS tagger reducing its flexibility . As unsupervised or partially supervised frameworks can not take the full advantages of aspect annotations commonly found in the training data , the above methods lead to deficiency in leveraging the data . Recently , Wang et al ( 2016 ) considered relation between opinion words and aspect words in a supervised model named RNCRF . However , RNCRF tends to suffer from parsing errors since the structure of the recursive network hinges on the dependency parse tree . CMLA ( Wang et al , 2017a ) used a multilayer neural model where each layer consists of aspect attention and opinion attention . However CMLA merely employs standard GRU without extended memories . We propose MIN ( Memory Interaction Network ) , a novel LSTM - based deep multi - task learning framework for the ATE task . Two LSTMs with extended memory are designed for handling the extraction tasks of aspects and opinions . The aspect - opinion relationship is established based on neural memory interactions between aspect extraction and opinion extraction where the global indicator score of opinion terms and local positional relevance between aspects and opinions are considered . To ensure that aspects are from sentimental sentences , MIN employs a third LSTM for sentimental sentence classification facilitating more accurate aspect term extraction . Experiment results over two benchmark datasets show that our framework achieves superior performance .", "entities": [[1, 6, "TaskName", "aspect - based sentiment analysis"], [41, 43, "TaskName", "Term Extraction"], [51, 54, "TaskName", "Aspect Category Detection"], [486, 487, "MethodName", "GRU"], [502, 503, "MethodName", "LSTM"], [506, 510, "TaskName", "multi - task learning"], [546, 548, "TaskName", "aspect extraction"], [583, 584, "MethodName", "LSTM"], [586, 588, "TaskName", "sentence classification"], [592, 594, "TaskName", "term extraction"]]}
{"text": "Let an input review sentence with T word tokens and the corresponding distributed representations be w = { w 1 , ... , w T } and x = { x 1 , ... , x T } respectively . The ATE task is treated as a sequence labeling task with BIO tagging scheme and the set of aspect tags for the word w t is y A t { B , I , O } , where B , I , O represent beginning of , inside and outside of the aspect span respectively . Commonly found training data contains gold annotations for aspect terms and opinionated sentences , but the gold standard of opinion words are usually not available . In our multi - task learning framework , three tasks are involved : ( 1 ) aspect term extraction ( ATE ) , ( 2 ) opinion word extraction and ( 3 ) sentimental sentence classification . We design a taskspecific LSTM , namely , A - LSTM , O - LSTM and S - LSTM , for tackling each of the above tasks respectively . The first component of our proposed framework consists of A - LSTM and O - LSTM where we equip LSTMs with extended operational memories and some operations are defined over the memories for task - level memory interactions . The second component is to determine if a review sentence is sentimental . This is achieved by employing a vanilla LSTM , namely , S - LSTM .", "entities": [[124, 128, "TaskName", "multi - task learning"], [139, 141, "TaskName", "term extraction"], [156, 158, "TaskName", "sentence classification"], [163, 164, "MethodName", "LSTM"], [169, 170, "MethodName", "LSTM"], [173, 174, "MethodName", "LSTM"], [177, 178, "MethodName", "LSTM"], [199, 200, "MethodName", "LSTM"], [203, 204, "MethodName", "LSTM"], [247, 248, "MethodName", "LSTM"], [253, 254, "MethodName", "LSTM"]]}
{"text": "We conduct experiments on two benchmark datasets from SemEval ABSA challenge ( Pontiki et al , 2014 ( Pontiki et al , , 2016 as shown in Table 1 . D 1 ( Se - mEval 2014 ) contains reviews from the laptop domain and D 2 ( SemEval 2016 ) contains reviews from the restaurant domain . In these datasets , aspect terms have been labeled and sentences containing at least one golden truth aspect are regarded as sentimental sentences . As gold standard annotations for opinion words are not provided , we select words with strong subjectivity from MPQA 1 as potential opinion words . Apart from the common opinion words in the sentiment lexicon , we also treat words , which directly depend on gold standard aspect terms through highprecision dependency rules , as opinion words .", "entities": [[100, 101, "DatasetName", "MPQA"]]}
{"text": "Table 2 depicts experiment results . Compared to the best systems in SemEval challenge , MIN achieves 3.0 % and 1.1 % absolute gains on D 1 and D 2 respectively . Besides , our MIN outperforms WDEmb , a strong CRF - based system benefiting from several kinds of useful word embeddings , by 2.1 % on D 1 . With memory interactions and consideration of sentimental sentence , our MIN boosts the performance of vanilla bi - directional LSTM ( +2.0 % and +1.7 % respectively ) . It validates the effectiveness of the manually designed memory operations and the proposed memory interaction mechanism . MIN also outperforms the state - of - the - art RNCRF on each dataset suggesting that memory interactions can be an alternative strategy instead of syntactic parsing . To further study the impact of each element in MIN , we conduct ablation experiments . As shown in Table 3 , removing bi - directionality decreases the extraction performances ( - 2.0 % and - 1.0 % ) . The soft sentimental constraint proves to be useful since MIN is 1.5 % and 1.0 % superior than the framework without S - LSTM on D 1 and D 2 respectively . O - LSTM brings in the largest performance gains on D 2 compared with ablated framework ( i.e. , MIN without O - LSTM ) , verifying our postulation that aspect - opinion \" interaction \" is more effective than only considering aspect terms . We also observe that the contribution of O - LSTM is less significant than that of bi - directionality on D 1 ( +1.6 % vs +2.0 % ) . This is reasonable since using opinion words as adjective modifiers placed after the aspects is common in English .", "entities": [[41, 42, "MethodName", "CRF"], [51, 53, "TaskName", "word embeddings"], [80, 81, "MethodName", "LSTM"], [199, 200, "MethodName", "LSTM"], [210, 211, "MethodName", "LSTM"], [231, 232, "MethodName", "LSTM"], [262, 263, "MethodName", "LSTM"]]}
{"text": "We propose Memory Interaction Network ( MIN ) , a multi - task learning framework , to detect aspect terms from the online user reviews . Compared with previous studies , our MIN has following features : Co - occurrence pattern between aspects and opinions is captured via memory interactions , where the neural memory operations are designed to summarize task - level information and perform interactions . A novel LSTM unit with extended memories is developed for memory interactions .", "entities": [[10, 14, "TaskName", "multi - task learning"], [70, 71, "MethodName", "LSTM"]]}
{"text": "Contextual and Non - Contextual Word Embeddings : an in - depth Linguistic Investigation", "entities": [[5, 7, "TaskName", "Word Embeddings"]]}
{"text": "In this paper we present a comparison between the linguistic knowledge encoded in the internal representations of a contextual Language Model ( BERT ) and a contextual - independent one ( Word2vec ) . We use a wide set of probing tasks , each of which corresponds to a distinct sentence - level feature extracted from different levels of linguistic annotation . We show that , although BERT is capable of understanding the full context of each word in an input sequence , the implicit knowledge encoded in its aggregated sentence representations is still comparable to that of a contextualindependent model . We also find that BERT is able to encode sentence - level properties even within single - word embeddings , obtaining comparable or even superior results than those obtained with sentence representations .", "entities": [[22, 23, "MethodName", "BERT"], [67, 68, "MethodName", "BERT"], [106, 107, "MethodName", "BERT"], [119, 121, "TaskName", "word embeddings"]]}
{"text": "Distributional word representations ( Mikolov et al , 2013 ) trained on large - scale corpora have rapidly become one of the most prominent component in modern NLP systems . In this context , the recent development of context - dependent embeddings ( Peters et al , 2018 ; Devlin et al , 2019 ) has shown that such representations are able to achieve state - ofthe - art performance in many complex NLP tasks . However , the introduction of such models made the interpretation of the syntactic and semantic properties learned by their inner representations more complex . Recent studies have begun to study these models in order to understand whether they encode linguistic phenomena even without being explicitly designed to learn such properties ( Marvin and Linzen , 2018 ; Goldberg , 2019 ; Warstadt et al , 2019 ) . Much of this work focused on the definition of probing models trained to predict simple linguistic properties from unsupervised representations . In particular , those work provided evidences that contextualized Neural Language Models ( NLMs ) are able to capture a wide range of linguistic phenomena ( Adi et al , 2016 ; Perone et al , 2018 ; Tenney et al , 2019b ) and even to organize this information in a hierarchical manner ( Belinkov et al , 2017 ; Lin et al , 2019 ; Jawahar et al , 2019 ) . Despite this , less study focused on the analysis and the comparison of contextual and non - contextual NLMs according to their ability to encode implicit linguistic properties in their representations . In this paper we perform a large number of probing experiments to analyze and compare the implicit knowledge stored by a contextual and a non - contextual model within their inner representations . In particular , we define two research questions , aimed at understanding : ( i ) which is the best method for combining BERT and Word2vec word representations into sentence embeddings and how they differently encode properties related to the linguistic structure of a sentence ; ( ii ) whether such sentence - level knowledge is preserved within BERT single - word representations . To answer our questions , we rely on a large suite of probing tasks , each of which codifies a particular propriety of a sentence , from very shallow features ( such as sentence length and average number of characters per token ) to more complex aspects of morphosyntactic and syntactic structure ( such as the depth of the whole syntactic tree ) , thus making them as suitable to assess the implicit knowledge encoded by a NLM at a deep level of granularity . The remainder of the paper is organized as follows . First we present related work ( Sec . 2 ) , then , after briefly presenting our approach ( Sec . 3 ) , we describe in more details the data ( Sec . 3.1 ) , our set of probing features ( Sec . 3.2 ) and the models used for the experiments ( Sec . 3.3 ) . Experiments and results are described in Sec . 4 and 5 . To conclude , in Sec . 6 we summarize the main findings of the study . Contributions In this paper : ( i ) we perform an in - depth study aimed at understanding the linguistic knowledge encoded in a contextual ( BERT ) and a contextual - independent ( Word2vec ) Neural Language Model ; ( ii ) we evaluate the best method for obtaining sentence - level representations from BERT and Word2vec according to a wide spectrum of probing tasks ; ( iii ) we compare the results obtained by BERT and Word2vec according to the different combining methods ; ( iv ) we study whether BERT is able to encode sentence - level properties within its single word representations .", "entities": [[327, 328, "MethodName", "BERT"], [333, 335, "TaskName", "sentence embeddings"], [362, 363, "MethodName", "BERT"], [577, 578, "MethodName", "BERT"], [606, 607, "MethodName", "BERT"], [627, 628, "MethodName", "BERT"], [643, 644, "MethodName", "BERT"]]}
{"text": "In the last few years , several methods have been devised to open the black box and understand the linguistic information encoded in NLMs ( Belinkov and Glass , 2019 ) . They range from techniques to examine the activations of individual neurons ( Karpathy et al , 2015 ; Li et al , 2016 ; K\u00e1d\u00e1r et al , 2017 ) to more domain specific approaches , such as interpreting attention mechanisms ( Raganato and Tiedemann , 2018 ; Kovaleva et al , 2019 ; Vig and Belinkov , 2019 ) or designing specific probing tasks that a model can solve only if it captures a precise linguistic phenomenon using the contextual word / sentence embeddings of a pre - trained model as training features ( Conneau et al , 2018 ; Zhang and Bowman , 2018 ; Hewitt and Liang , 2019 ) . These latter studies demonstrated that NLMs are able to encode a wide range of linguistic information in a hierarchical manner ( Belinkov et al , 2017 ; Blevins et al , 2018 ; Tenney et al , 2019b ) and even to support the extraction of dependency parse trees ( Hewitt and Manning , 2019 ) . Jawahar et al ( 2019 ) investigated the representations learned at different layers of BERT , showing that lower layer representations are usually better for capturing surface features , while embeddings from higher layers are better for syntactic and semantic properties . Using a suite of probing tasks , Tenney et al ( 2019a ) found that the linguistic knowledge encoded by BERT through its 12/24 layers follows the traditional NLP pipeline : POS tagging , parsing , NER , semantic roles and then coreference . , instead , quantified differences in the transferability of individual layers between different models , showing that higher layers of RNNs ( ELMo ) are more task - specific ( less general ) , while transformer layers ( BERT ) do not exhibit this increase in task - specificity . Closer to our study , Adi et al ( 2016 ) proposed a method for analyzing and comparing different sentence representations and different dimensions , exploring the effect of the dimensionality on the resulting representations . In particular , they showed that sentence representations based on averaged Word2vec embeddings are particularly effective and encode a wide amount of information regarding sentence length , while LSTM auto - encoders are very effective at capturing word order and word content . Similarly , but focused on the resolution of specific downstream tasks , Shen et al ( 2018 ) compared a Single Word Embedding - based model ( SWEM - based ) with existing recurrent and convolutional networks using a suite of 17 NLP datasets , demonstrating that simple pooling operations over SWEM - based representations exhibit comparable or even superior performance in the majority of cases considered . On the contrary , Joshi et al ( 2019 ) showed that , in the context of three different classification problems in health informatics , context - based representations are a better choice than word - based representations to create vectors . Focusing instead on the geometry of the representation space , Ethayarajh ( 2019 ) first showed that the contextualized word representations of ELMo , BERT and GPT - 2 produce more context specific representations in the upper layers and then proposed a method for creating a new type of static embedding that outperforms GloVe and FastText on many benchmarks , by simply taking the first principal component of contextualized representations in lower layers of BERT . Differently from those latter work , our aim is to investigate the implicit linguistic knowledge encoded in pre - trained contextual and contextualindependent models both at sentence and word levels .", "entities": [[115, 117, "TaskName", "sentence embeddings"], [217, 218, "MethodName", "BERT"], [265, 266, "MethodName", "BERT"], [281, 282, "TaskName", "NER"], [311, 312, "MethodName", "ELMo"], [327, 328, "MethodName", "BERT"], [403, 404, "MethodName", "LSTM"], [550, 551, "MethodName", "ELMo"], [552, 553, "MethodName", "BERT"], [554, 555, "MethodName", "GPT"], [581, 582, "MethodName", "GloVe"], [583, 584, "MethodName", "FastText"], [602, 603, "MethodName", "BERT"]]}
{"text": "We studied how layer - wise internal representations of BERT encode a wide spectrum of linguistic properties and how such implicit knowledge differs from that learned by a context - independent model such as Word2vec . Following the probing task approach as defined in Conneau et al ( 2018 ) , we proposed a suite of 68 probing tasks , each of which corresponds to a distinct linguistic feature capturing raw - text , lexical , morpho - syntactic and syntactic characteristics of a sentence . More specifically , we defined two sets of experiments . The first consists in evaluating which is the best method for generating sentence - level embeddings using BERT and Word2vec single - word representations . In particular , we defined a simple probing model that takes as input layer - wise BERT and Word2vec combined representations for each sentence of a gold standard Universal Dependencies ( UD ) ( Nivre et al , 2016 ) English dataset and predicts the actual value of a given probing feature . Moreover , we compared the results to understand which model performs better according to different levels of linguistic sophistication . In the second set of experiments , we measured how many sentence - level properties are encoded in single - word representations . To do so , we performed our set of probing tasks using the embeddings extracted from both BERT and Word2vec individual tokens . In particular , we considered the word representations corresponding to the first , last and two internal tokens for each sentence of the UD dataset .", "entities": [[9, 10, "MethodName", "BERT"], [113, 114, "MethodName", "BERT"], [137, 138, "MethodName", "BERT"], [149, 151, "DatasetName", "Universal Dependencies"], [152, 153, "DatasetName", "UD"], [234, 235, "MethodName", "BERT"], [263, 264, "DatasetName", "UD"]]}
{"text": "In order to perform the probing experiments on gold annotated sentences , we relied on the Universal Dependencies ( UD ) English dataset . The dataset includes three UD English treebanks : UD English - ParTUT , a conversion of a multilin - gual parallel treebank consisting of a variety of text genres , including talks , legal texts and Wikipedia articles ( Sanguinetti and Bosco , 2015 ) ; the Universal Dependencies version annotation from the GUM corpus ( Zeldes , 2017 ) ; the English Web Treebank ( EWT ) , a gold standard universal dependencies corpus for English ( Silveira et al , 2014 ) . Overall , the final dataset consists of 23 , 943 sentences .", "entities": [[16, 18, "DatasetName", "Universal Dependencies"], [19, 20, "DatasetName", "UD"], [28, 29, "DatasetName", "UD"], [32, 33, "DatasetName", "UD"], [71, 73, "DatasetName", "Universal Dependencies"], [77, 78, "DatasetName", "GUM"], [86, 89, "DatasetName", "English Web Treebank"], [96, 98, "DatasetName", "universal dependencies"]]}
{"text": "As previously mentioned , our method is in line with the probing tasks approach defined in Conneau et al ( 2018 ) , which aims to capture linguistic information from the representations learned by a NLM . Specifically , in our work , each probing task correspond to predict the value of a specific linguistic feature automatically extracted from the POS tagged and dependency parsed sentences in the English UD dataset . The set of features is based on the ones described in Brunato et al ( 2020 ) and it includes characteristics acquired from raw , morphosyntactic and syntactic levels of annotation . As described in Brunato et al ( 2020 ) , this set of features has been shown to have a highly predictive role when leveraged by traditional learning models on a variety of classification problems , covering different aspects of stylometric and complexity analysis . As shown in Table 1 , these features capture sev - eral linguistic phenomena ranging from the average length of words and sentence , to morpho - syntactic information both at the level of POS distribution and about the inflectional properties of verbs . More complex aspects of sentence structure are derived from syntactic annotation and model global and local properties of parsed tree structure , with a focus on subtrees of verbal heads , the order of subjects and objects with respect to the verb , the distribution of UD syntactic relations and features referring to the use of subordination .", "entities": [[69, 70, "DatasetName", "UD"], [239, 240, "DatasetName", "UD"]]}
{"text": "We relied on a pre - trained English version of BERT ( BERT - base uncased , 12 layers ) for the extraction of the contextual word embeddings . To obtain the representations for our sentence - level tasks we experimented the activation of the first input token ( [ CLS ] ) 1 and four different combining methods : Max - pooling , Min - pooling , Mean and Sum . Each of this four combining methods returns a single s vector , such that each s n is obtained by combining the n th components w 1n , w 2n , ... , w mn of the embedding of each word in the input sentence . In order to conduct a comparison of contextbased and word - based representations when solving our set of probing tasks , we performed all the probing experiments using also the embeddings extracted from a pre - trained version of Word2vec . In particular , we trained the model on the English Wikipedia dataset ( dump of March 2020 ) , resulting in 300 - dimensional vectors . In the same manner as BERT 's contextual representations , we experimented four combining methods : Max - pooling , Min - pooling , Mean and Sum . We used a linear Support Vector Regression model ( LinearSVR ) as probing model .", "entities": [[10, 11, "MethodName", "BERT"], [12, 13, "MethodName", "BERT"], [26, 28, "TaskName", "word embeddings"], [190, 191, "MethodName", "BERT"]]}
{"text": "Once we have probed the linguistic knowledge encoded by BERT and Word2vec using different strategies for computing sentence embeddings , we investigated how much information about the structure of a sentence is encoded within single - word contextual representations . For doing so , we performed our sentence - level probing tasks using a single BERT word embedding for each sentence in the UD dataset . We tested four different words , corresponding to the first , the last and two internal tokens for each sentence in the UD dataset . In particular , we extracted the embeddings from the output layer ( - 1 ) and from the layer that achieved best results in the previous experiments ( - 8 ) . We used probing scores obtained with Word2vec embeddings for the same tokens as baseline . In Table 5 we report average \u03c1 scores obtained by BERT ( BERT - * ) and Word2vec ( Word2vec - * ) according to word - level representations extracted from the four tokens mentioned above . Results were computed aggregating all probing results ( All ) and according to raw text ( Raw ) , morphosyntactic ( Morphosyntax ) and syntatic ( Syntax ) levels of annotation . For comparison , we also report average scores obtained with the [ CLS ] token . As a first remark , we can clearly notice that even with a single - word embedding BERT is able to encode a wide spectrum of sentence - level linguistic properties . This result allows us to highlight the main potential of contextual representations , i.e. the capability of capturing linguistic phenomena that refer to the entire input sequence within single - word representations . An interesting observation is that , except for the raw text features , for which the best scores are achieved using [ CLS ] , higher performance are obtained with the embeddings corresponding to BERT - 4 , i.e. the last token of each sentence . This result seems to indicate that [ CLS ] , although being used for classification predictions , does not necessarily correspond to the most linguistically informative token within each input sequence . Comparing the results with those achieved using Word2vec word embeddings , we notice that BERT scores greatly outperform Word2vec for all the probing tasks . This is a straightforward result and can be easily explained by the fact that the lack of contextual knowledge does not allow singleword representations to encode information that are related to the structure of the whole sentence . Since the latter results demonstrated that BERT is capable of encoding many sentence - level properties within its single word representations , as a last analysis , we decided to compare these results with the ones obtained using sentence embeddings . In particular , Figure 3 reports probing scores obtained by BERT single word ( tok * ) and Mean sentence representations ( sent ) extracted from the output layer ( - 1 ) and from the layer that achieved best results in average ( - 8 ) . As already mentioned , for many of these probing tasks , word embeddings performance is comparable to that obtained with the aggregated sentence representations . Nevertheless , there are several cases in which the difference between performance is particularly significant . Interestingly , we can notice that aggregated sentence representations are generally better for predicting properties belonging to the left heatmap , i.e. to the group of features more related to syntactic properties . This is particularly noticeable for the average number of tokens per clause ( avg token per clause ) or the distribution of subordinate chains by length ( subord dist ) , for which we observe an improvement from word - level to sentence - level representations of more than .10 \u03c1 points . On the contrary , probing features belonging to the right heatmap , therefore more close to raw text and morphosyntactic properties , are generally better predicted using single word embeddings , especially when considering the inner representations corresponding to the last token in each sentence ( tok 4 ) . The property most affected by the difference in scores between wordand sentence - level embeddings is the the distribution of periods ( xpos dist . ) . Focusing instead on differences in performance between the two considered layers , we can notice that regardless of the method used to predict each feature , the representations learned by BERT tend to lose their precision in encoding our set of linguistic properties , most likely because the model is storing task - specific information ( Masked Language Modeling task ) at the expense of its ability to encode general knowledge about the language .", "entities": [[9, 10, "MethodName", "BERT"], [17, 19, "TaskName", "sentence embeddings"], [55, 56, "MethodName", "BERT"], [63, 64, "DatasetName", "UD"], [88, 89, "DatasetName", "UD"], [148, 149, "MethodName", "BERT"], [150, 151, "MethodName", "BERT"], [240, 241, "MethodName", "BERT"], [322, 323, "MethodName", "BERT"], [374, 376, "TaskName", "word embeddings"], [380, 381, "MethodName", "BERT"], [435, 436, "MethodName", "BERT"], [467, 469, "TaskName", "sentence embeddings"], [480, 481, "MethodName", "BERT"], [529, 531, "TaskName", "word embeddings"], [578, 579, "MethodName", "heatmap"], [655, 656, "MethodName", "heatmap"], [673, 675, "TaskName", "word embeddings"], [752, 753, "MethodName", "BERT"], [778, 781, "TaskName", "Masked Language Modeling"], [791, 793, "TaskName", "general knowledge"]]}
{"text": "In this paper we studied the linguistic knowledge implicitly encoded in the internal representations of a contextual Language Model ( BERT ) and a contextual - independent one ( Word2vec ) . Using a suite of 68 probing tasks and testing different methods for combining word embeddings into sentence representations , we showed that BERT and Word2vec encode a wide set of sentence - level linguistic properties in a similar manner . Nevertheless , we found that for Word2vec the best method for obtaining sentence representations is the Sum , while BERT is more effective when averaging all the single - word representations ( Mean method ) . Moreover , we showed that BERT is able in storing features that are mainly related to raw text and syntactic properties , while Word2vec is good at predicting morphosyntactic characteristics . Finally , we showed that BERT is able to encode sentence - level linguistic phenomena even within single - word embeddings , exhibiting comparable or even superior performance than those obtained with aggregated sentence representations . Moreover , we found that , at least for morphosyntactic and syntactic characteristics , the most informative word representation is the one that correspond to the last token of each input sequence and not , as might be expected , to the [ CLS ] special token .", "entities": [[20, 21, "MethodName", "BERT"], [45, 47, "TaskName", "word embeddings"], [54, 55, "MethodName", "BERT"], [91, 92, "MethodName", "BERT"], [113, 114, "MethodName", "BERT"], [144, 145, "MethodName", "BERT"], [158, 160, "TaskName", "word embeddings"]]}
{"text": "Many works related to automatic detection of misogyny , hate , sexism on social media and web have been proposed . Abir Rahali ( Rahali et al , 2021 ) proposed a approach for automatic misogyny detection in social media using attention based bidirectional LSTM . Endang Wahyu Pamungkas ( Pamungkas et al , 2020 ) proposed a method for Automatic Identification of Misogyny in English and Italian Tweets at EVALITA 2018 with a Multilingual Hate Lexicon . Mario Anzovino , Elisabetta Fersini ( Anzovino et al , 2018 ) proposed a method for Automatic Identification and Classification of Misogynistic Language on Twitter . The main contribution of this paper is two - fold : ( 1 ) a corpus of misogynous tweets , labelled from different perspective and ( 2 ) an exploratory investigation on NLP features and ML models for detecting and classifying misogynistic language . Rachael Fulper ( Fulper et al , 2014 ) proposed a relation between misogynistic language in twitter and sexual Violence . In their paper they consider all 50 states in Washington DC . Lakes Goenaga , Aitziber ( Goenaga et al , 2018 ) Atutxa proposed a Automatic misogyny identification using neural networks . In this paper they focus on recurrent neural network ( RNN ) approach using a Bidirectional Long Short Term Memory ( Bi - LSTM ) .", "entities": [[43, 45, "MethodName", "bidirectional LSTM"], [97, 98, "TaskName", "Classification"], [225, 226, "MethodName", "LSTM"]]}
{"text": "First , we removed all the punctuations , numbers , links and stop words . We have used lemmatization for grouping together the different forms of a word into a single word . NLTK wordnet ( Loper and Bird , 2002 ) is used for lemmatization .", "entities": [[18, 19, "TaskName", "lemmatization"], [45, 46, "TaskName", "lemmatization"]]}
{"text": "TfidfVectorizer ( Kumar and Subba , 2020 ) is used for converting the text into numerical features . Pipeline 1 is used for doing TfidfVectorizer and classification in pipelined manner . Tokenizer by keras library is used for LSTM and Bert . For Logistic regression and SVM we have used TfidfVectorizer from scikit - learn library .", "entities": [[2, 3, "DatasetName", "Kumar"], [38, 39, "MethodName", "LSTM"], [43, 45, "MethodName", "Logistic regression"], [46, 47, "MethodName", "SVM"]]}
{"text": "Multivalent Entailment Graphs for Question Answering", "entities": [[4, 6, "TaskName", "Question Answering"]]}
{"text": "Drawing inferences between open - domain natural language predicates is a necessity for true language understanding . There has been much progress in unsupervised learning of entailment graphs for this purpose . We make three contributions : ( 1 ) we reinterpret the Distributional Inclusion Hypothesis to model entailment between predicates of different valencies , like DEFEAT ( Biden , Trump ) WIN ( Biden ) ; ( 2 ) we actualize this theory by learning unsupervised Multivalent Entailment Graphs of open - domain predicates ; and ( 3 ) we demonstrate the capabilities of these graphs on a novel question answering task . We show that directional entailment is more helpful for inference than non - directional similarity on questions of fine - grained semantics . We also show that drawing on evidence across valencies answers more questions than by using only the same valency evidence . * Now at Google Research . 1 The murder mystery board game Clue ( also known as Cluedo ) lends inspiration to this project .", "entities": [[100, 102, "TaskName", "question answering"], [151, 152, "DatasetName", "Google"]]}
{"text": "We are reading a mystery about a dark and foreboding manor and have one question : \" is Mr. Boddy dead ? \" 1 Our text might say \" Colonel Mustard killed Mr. Boddy , \" or \" Mr. Boddy was murdered in the kitchen with a candlestick , \" either of which answers the question , but only via natural language inference . An Entailment Graph ( EG ) is a structure of meaning postulates supporting these inferences such as \" if A kills B , then B is dead . \" Entailment Graphs contain vertices of opendomain natural language predicates and entailments between them are represented as directed edges . Previous models learn predicates of a single valency , the number and types of arguments controlled by the predicate . Commonly these are binary graphs , which can not model single - argument predicates like the entity states \" is dead \" or \" is an author . \" This means they miss a variety of entailments in text that could be used to answer questions such as our example . The Distributional Inclusion Hypothesis ( DIH ) ( Dagan et al , 1999 ; Kartsaklis and Sadrzadeh , 2016 ) is a theory which has been used effectively in unsupervised learning of these same - valency entailment graphs ( Geffet and Dagan , 2005 ; Berant et al , 2010 ; Hosseini , 2021 ) . In this work the DIH is reinterpreted in a way which supports learning entailments between predicates of different valencies such as KILL ( Mustard , Boddy ) DIE ( Boddy ) . We extend the work of Hosseini et al ( 2018 ) and develop a new Multivalent Entailment Graph ( MGraph ) where vertices may be predicates of different valencies . This results in new kinds of entailments that answer a broader range of questions including entity state . We further pose a true - false question answering task generated automatically from news text . Our model draws inferences across propositions of different valencies to answer more questions than using same - valence entailment graphs . We also compare with several baselines , including unsupervised pretrained language models , and show that our directional entailment graphs succeed over non - directional similarity measures in answering questions of fine - grained semantics . Advantageously , EGs are structures designed to be queried , so they are inherently explainable . This research is conducted in English , but as an unsupervised algorithm it may be applied to other languages given a parser and named entity linker .", "entities": [[60, 63, "TaskName", "natural language inference"], [326, 328, "TaskName", "question answering"], [365, 368, "TaskName", "pretrained language models"]]}
{"text": "The task of recognizing textual entailment ( Dagan et al , 2006 ) requires models to predict a relation between a text T and hypothesis H ; \" T entails H if , typically , a human reading T would infer that H is most likely true . \" From here , research has moved in several directions . We study predicates , including verbs and phrases that apply to arguments . Research in predicate entailment graphs has evolved from \" local \" learning of entailment rules ( Geffet and Dagan , 2005 ; Szpektor and Dagan , 2008 ) to later work on joint learning of \" globalized \" rules , overcoming sparsity in local graphs ( Berant et al , 2010 ; Hosseini et al , 2018 ) . These graphs frequently rely on the DIH for the local learning step to learn initial predicate entailments . The DIH states that for some predicates p and q , if the contextual features of p are included in those of q , then p entails q ( Geffet and Dagan , 2005 ) . In previous work predicate arguments are successfully used as these contextual features , but only predicates of the same valency are considered ( e.g. binary predicates entail binary ; unary entail unary ) , and further research computes additional edges in these same - valency graphs such as with link prediction ( Hosseini et al , 2019 ) . However , this leaves out crucial inferences that cross valencies such as the kill / die example , which are easy for humans . We generalize the DIH to learn entailments within and across valencies . Typing is very helpful for entailment graph learning ( Berant et al , 2010 ; Lewis and Steedman , 2013 ; Hosseini et al , 2018 ) . Inducing a type for each entity such as \" person , \" \" location , \" etc . enables generalized learning across instances and disambiguates word sense , e.g. \" running a company \" has different entailments than \" running code . \" We compare our model to several baselines , including strong pretrained language models in an unsupervised setting using similarity . BERT ( Devlin et al , 2019 ) generates impressive word representations , even unsupervised ( Petroni et al , 2019 ) , which we compare with on a task of predicate inference . We further test RoBERTa ( Liu et al , 2019 ) to show the impact of robust in - domain pretraining on the same architecture . These non - directional similarity models provide a strong baseline for evaluating directional entailment graphs .", "entities": [[234, 236, "TaskName", "link prediction"], [286, 288, "TaskName", "graph learning"], [361, 364, "TaskName", "pretrained language models"], [371, 372, "MethodName", "BERT"], [408, 409, "MethodName", "RoBERTa"]]}
{"text": "We define an Entailment Graph as a directed graph of predicates and their entailments , G = ( V , E ) . The vertices V are the set of predicates , where each argument has a type from the set of 49 FIGER base types T , e.g. TRAVEL.TO ( : person , : location ) V , and : person , : location T . The directed edges are E = { ( v 1 , v 2 ) | v 1 , v 2 V if v 1 v 2 } , or all entailments between vertices in V . In Multivalent Entailment Graphs we expand V to contain predicates of both 1 - and 2 - valency , and E to edges between these vertices , described as follows . Let b i , b j V be distinct binary predicates and u i , u j V be unary predicates . Define E as the set of all entities in the world , and some particular entities x , y E to illustrate argument slots . E contains these patterns of entailment : 1 . b i ( x , y ) b j ( x , y ) or b i ( x , y ) b j ( y , x ) Binary entails binary ( BB entailments ) 2 . b i ( x , y ) u i ( x ) or b i ( x , y ) u i ( y ) Binary entails unary of one argument ( BU ) 3 . u i ( x ) u j ( x ) Unary entails unary ( UU ) Predicates with valence > 2 are sparse in the text , but are also included in the MGraph by decomposing them into binary relations between pairs of entities . This is another application of our Multivalent DIH . We maintain argument roles , so each binary is a window into its higher - valency predicate , allowing higher - valency predicates to entail lower binaries and unaries . To learn these new kinds of connections we develop a method of local entailment rule learning using the MDIH . As in 2 , the local step learns the initial directed edges of the entailment graph , which are further improved with global learning . Our local step learns entailments by machine - reading the NewsSpike corpus ( 2.3 GB ) , which contains 550 K news articles , or over 20 M sentences ( Zhang and Weld , 2013 ) . NewsSpike consists of multi - source news articles collected within a fixed timeframe , and due to these properties the articles frequently discuss the same events but phrased in different ways , providing appropriate training evidence .", "entities": [[43, 44, "DatasetName", "FIGER"]]}
{"text": "Our pipeline processes raw article text into a list of propositions : predicates with associated typed arguments . We use the MoNTEE system ( Bijl de Vroe et al , 2021 ) to extract natural language relations between entities from raw text 2 . This system first parses sentences using the RotatingCCG parser ( Stanojevi\u0107 and Steedman , 2019 ) ( Combinatory Categorial Grammar ; Steedman , 2000 ) and then forms dependency graphs from the parses . Fi - nally , it traverses these graphs to extract the relations , each consisting of a predicate and its arguments . Figure 1 shows an example dependency graph and the relation extracted from it . Arguments may be either named entities 3 or general entities ( noun phrases ) . These entities are mapped to types by linking to their Freebase IDs ( Bollacker et al , 2008 ) using AIDA - Light ( Nguyen et al , 2014 ) , and mapping the IDs to the 49 base FIGER types ( Ling and Weld , 2012 ) . Both binary and unary relations are extracted from the corpus if they contain at least one named entity , which helps anchor to a real - world event . This poses a challenge as noted by Szpektor and Dagan ( 2008 ) . While binary predicates may be extracted from dependency paths between two entities , unary predicates only have one endpoint , so we must carefully apply linguistic knowledge to extract meaningful unary relations . We extract these neo - Davidsonian event cases : One - argument verbs including intransitives , e.g. \" Knowles sang \" \u21d2 SING.1 ( Knowles ) and passivized transitives , e.g. \" Bill H.R. 1 was passed \" \u21d2 PASS.2 ( Bill - HR1 ) Copular constructions , where copular \" be \" acts as the main verb , e.g. \" Chiang is an author \" \u21d2 BE.AUTHOR.1 ( Chiang ) and where it does not , e.g. \" Phelps seems to be the winner \" \u21d2 SEEM.TO.BE.WINNER.1 ( Phelps ) As with binaries in earlier work , unary predicates are lemmatized , and tense , aspect , modality , and other auxiliaries are stripped . The CCG argument position which corresponds to its case ( e.g. 1 for nominative , 2 for accusative ) , is appended to the predicate . Passive predicates are mapped to active ones . Modifiers such as negation and predicates like \" planned to \" as in \" Professor Plum planned to attend \" are also extracted in the predicate . We pay special attention to copular constructions , which always introduce stative predicates , rather than events ( Vendler , 1967 ) . These are interesting for modeling the properties of entities .", "entities": [[168, 169, "DatasetName", "FIGER"]]}
{"text": "We pose an automatically generated QA task to evaluate our model explicitly for directional inference between binary and unary predicates , as we are not aware of any standard datasets for this problem . Our task is to answer true - false questions about real events that are discussed in the news , for example , \" Was Biden elected ? \" These types of questions are surprisingly difficult and frequently require inference to answer ( Clark et al , 2019 ) . For this , entailment is especially useful : we must decide if the question ( hypothesis ) is true given a list of propositions from limited news text ( premises ) , which are all likely to be phrased differently . This task is designed independently of the MGraph as a challenge in information retrieval . Positive questions made from binary and unary predicates are selected directly from the news text using special criteria , and are then removed . From these positives we automatically generate false events to use as negatives , which are designed to mimic real , newsworthy events . The remaining news text is used to answer the questions . We at - tempt to make every question answerable , but since they are generated automatically there is no guarantee . However , the task is fair as all models are given the same information . The additive effects of multivalent entailment should be demonstrated : with more kinds of entailment , the MGraph should find more textual support and answer more questions . The task is presented on a text sample from NewsCrawl , a multi - source corpus of news articles , to be published separately . A test set is extracted which contains 700 K sentences from articles over a period of several months , and also a development set from a further 500 K sentences . We generate questions balanced to a ratio of 50 % binary questions / 50 % unary ; and within each 50 % positive / 50 % negative . Table 2 shows a sample from the dev set . We generate 34 , 394 questions on the test set : 17 , 256 unary questions and 17 , 138 binary .", "entities": [[136, 138, "TaskName", "information retrieval"]]}
{"text": "For realism , questions should be both interesting and answerable using the corpus . A multi - step process extracts questions from the news text itself . 1 . Partitioning . First , the articles are grouped by publication date such that each partition covers a timespan of up to 3 consecutive days of news ( 49 partitions in the test set ) . We ask yes - no questions about events drawn from the partition , and the news text within this 3 - day window is used as evidence to answer them . We ask questions as if happening presently in this time window to control for the variable of time , so we can ask ambiguous questions like \" Did the Patriots win the Superbowl ? \" which may be \" true \" or not depending on the date and timespan . The small 3 - day window size was chosen so multiple news stories about an event appear together , increasing the chances of finding question answers . Within each partition we do relation extraction in a process mirroring 4.1 . 2 . Selecting Positives . We adapt a selection process from Poon and Domingos ( 2009 ) to choose good questions which are interesting to a human and answerable from the partition text . First , we identify repeated entities that star in the events of the articles ; these will yield interesting questions as well as ample textual evidence for answering them . In each partition we count the mentions of each entity pair ( from binary propositions ) and single entities ( from unary and binary ones ) . The most frequent entities and pairs mentioned more than 5 times in the partition are selected . Predicates which are mentioned across the entire news corpus 10 times or fewer are filtered out ; we assume those remaining are popular to report in news and thus are interesting to a human questioner . We randomly select propositions featuring both a star entity and predicate to use as questions , and remove them from the partition . 3 . Generating Negatives . A simple strategy for producing negatives might seem to be substituting random predicates into the positive questions . However , this is unsatisfactory because modern techniques in NLP excel at detecting unrelated words . For example , a neural model will easily distinguish a random negative like DETONATE ( Google , YouTube ) from a news text discussing Google 's acquisition of YouTube , classifying it as a false event on grounds of dissimilarity alone . To be a meaningful test of inference this task requires that negatives be difficult to discriminate from positives : they should be semantically related but should not logically follow from what is stated in the text . To this end we derive negative questions from the selected positives using linguistic relations in WordNet ( Fellbaum , 1998 ) . We assume that news text follows the Gricean cooperative principle of communication ( Davis , 2019 ) , such that it will report what facts are known and nothing more . To this end , noun hyponyms and their verbal equivalent , troponyms , are mined from the first sense of each positive in WordNet . For example , we extract \" burn \" as a troponym of \" hurt \" and the phrase \" inherit from \" as a troponym of \" receive from . \" We therefore expect that these specific relations will be untrue of the argument tuple in question and may be used as negatives . We also considered antonyms and other WordNet relations , but these are much sparser in English and have low coverage . For fairness , generated negatives which actually occur in the current partition are screened out ( 0.1 % of proposed negatives ) , as well as negatives which never occur in the entire corpus ( 76.8 % of proposed negatives ) . Only challenging negatives are left , which actually do occur in real news text . See Table 2 for a sample of questions . In the error analysis we find these negatives to be of good quality : they are uncommonly inferable from the text , accounting for a small percentage of false positives .", "entities": [[177, 179, "TaskName", "relation extraction"], [406, 407, "DatasetName", "Google"], [415, 416, "DatasetName", "Google"]]}
{"text": "In each partition , models receive factual propositions extracted from 3 days of news text to use as evidence for answering true - false questions . A model scores how strongly it can infer the question proposition from each evidence proposition , and we take the maximum score as the model confidence of a \" true \" answer . Exact - Match . Our text is multi - source news articles , so world events are often discussed multiple times in the data , even with the same phrasing . We compute an \" exact - match \" baseline which shows how many questions can be answered from an exact string match in the text ; the rest require inference . Binary Entailment Graph . Our BB model is roughly equivalent to the state of the art binary - tobinary entailment graph ( Hosseini et al , 2018 ) , so it serves as a baseline for the overall model . 5 All graph models look for directed entailments from evidence propositions to the question proposition . For example , \" Was YouTube sold to Google ? \" can be answered affirmatively by reading \" Google bought YouTube \" using the graph edge BUY ( x , y ) SELL.TO ( y , x ) . BInc scores range from 0 to 1 ; if no entailments are found we assume it is false ( score of 0 ) . Multivalent Entailment Graph . The MGraph is made of 3 component models : ( 1 ) the BB model which uses binary evidence to answer binary questions ; ( 2 ) the UU model which uses unary evidence to answer unary questions ; and ( 3 ) the BU model which uses binary evidence to answer unary questions . The MGraph is able to answer questions using evidence across valencies , e.g. \" Is J.K. Rowling an author ? \" is affirmed by reading \" J.K. Rowling wrote The Sorcerer 's Stone \" using the graph edge WRITE ( x , y ) BE.AUTHOR ( x ) . Individually , each model answers only binary or unary qustions , not both . By combining them all kinds of questions can be answered using all available evidence . At each precision level if any component model predicts true , the overall model does too . In some test instances the entity typer may make an error , and so we fail to find the question predicate in the typed subgraph . Similarly to Hosseini et al ( 2018 ) , in these cases we back off , querying all subgraphs for the untyped predicate and averaging the entailment scores found . We find 5 % more unary questions and 18 % more binaries . Similarity Models . BERT and RoBERTa predicate embeddings ( Devlin et al , 2019 ; Liu et al , 2019 ) are used in an unsupervised manner to answer questions based on similarity to the evidence . We encode the question into a representation vector , and each evidence proposition with the same arguments . We compute the cosine similarity between the question and each evidence vector , adjusted to a scale of 0 to 1 : sim ( p , q ) = ( cos ( p , q ) + 1 ) /2 . To compute each vector encoding we construct a simple natural language sentence from the proposition using its predicate and arguments and encode it with the language model . Our representation includes only the encoding for the predicate in the context of its arguments , but not the arguments themselves to make this a true test of predicate similarity . We average all final hidden - state vectors from the model corresponding to the predicate , excluding those of the arguments . We test the basic BERT model and RoBERTa model , which has robustly pretrained on 160 GB of text ( 76 GB news ) . PPDB . Though supervised , PPDB 2.0 ( we use XXXL ) ( Pavlick et al , 2015 ) is a useful comparison as it is a large , well - understood resource for phrasal entailment . PPDB relations come from bilingual pivoting and are categorized using textbased features , which is very different from our argument - tracking method . We view PPDB as a kind of Entailment Graph with 9 M predicate phrases ( vertices ) and 33 M \" Equivalence \" and \" ForwardEntailment \" edges . We convert evidence and question propositions into a natural text format and extract a PPDB relation score from each evidence phrase to the question .", "entities": [[185, 186, "DatasetName", "Google"], [195, 196, "DatasetName", "Google"], [220, 221, "DatasetName", "0"], [237, 238, "DatasetName", "0"], [466, 467, "MethodName", "BERT"], [468, 469, "MethodName", "RoBERTa"], [536, 537, "DatasetName", "0"], [644, 645, "MethodName", "BERT"], [647, 648, "MethodName", "RoBERTa"]]}
{"text": "We sample 300 false positives ( 100 for each model ) and report analyses in Table 4 . In all models spurious entailments are the largest issue , and may occur due to normalization of predicates during learning , or incidental correlations in the data . The UU and BU models also suffer during relation extraction ( parsing ) . When we fail to parse a second argument for a predicate we assume it only has one and extract a malformed unary , which can interfere with question answering ( e.g. reporting verbs \" explain , \" \" announce , \" etc . which fail to parse with their quote ) . We also find relatively few poorly generated negatives , which are actually true given the text . In these cases the model finds an entailment which the authors judge to be correct .", "entities": [[54, 56, "TaskName", "relation extraction"], [87, 89, "TaskName", "question answering"]]}
{"text": "The MDIH is shown as an effective theory of unsupervised , open - domain predicate entailment , which crosses valencies by respecting argument roles . Our multivalent entailment graph 's performance has been demonstrated on a question answering task requiring fine - grained semantic understanding . Our method is able to answer a broader variety of questions than earlier entailment graphs , aided by drawing on evidence across valencies . We outperform baseline models including a strong similarity measure using unsupervised BERT and RoBERTa , while using far less training data . This shows that directional entailment is more helpful for inference on such a task than non - directional similarity , even with robust , in - domain pretraining . We also noted a complementarity between unsupervised methods . Our symbolic graph method achieves high precision for learned predicates , while sub - symbolic neural models achieve high recall by generalizing to unseen predicates . Future work may leverage our MDIH signal to train a directional neural classifier and combine benefits .", "entities": [[36, 38, "TaskName", "question answering"], [81, 82, "MethodName", "BERT"], [83, 84, "MethodName", "RoBERTa"]]}
{"text": "This work was supported in part by ERC H2020 Advanced Fellowship GA 742137 SEMANTAX ,", "entities": [[11, 12, "MethodName", "GA"]]}
{"text": "MLEC - QA : A Chinese Multi - Choice Biomedical Question Answering Dataset", "entities": [[10, 12, "TaskName", "Question Answering"]]}
{"text": "Question Answering ( QA ) has been successfully applied in scenarios of human - computer interaction such as chatbots and search engines . However , for the specific biomedical domain , QA systems are still immature due to expert - annotated datasets being limited by category and scale . In this paper , we present MLEC - QA , the largest - scale Chinese multi - choice biomedical QA dataset , collected from the National Medical Licensing Examination in China . The dataset is composed of five subsets with 136 , 236 biomedical multi - choice questions with extra materials ( images or tables ) annotated by human experts , and first covers the following biomedical sub - fields : Clinic , Stomatology , Public Health , Traditional Chinese Medicine , and Traditional Chinese Medicine Combined with Western Medicine . We implement eight representative control methods and open - domain QA methods as baselines . Experimental results demonstrate that even the current best model can only achieve accuracies between 40 % to 55 % on five subsets , especially performing poorly on questions that require sophisticated reasoning ability . We hope the release of the MLEC - QA dataset can serve as a valuable resource for research and evaluation in open - domain QA , and also make advances for biomedical QA systems . 1", "entities": [[0, 2, "TaskName", "Question Answering"]]}
{"text": "As a branch of the QA task , Biomedical Question Answering ( BQA ) enables effectively perceiving , accessing , and understanding complex biomedical knowledge by innovative applications , which makes BQA an important QA application in the biomedical domain ( Jin et al , 2021 ) . Such a task has recently attracted considerable attention from the NLP community ( Zweigenbaum , 2003 ; He et al , 2020b ; Jin et al , 2020 ) , but is still confronted with the following three key challenges : 1 https://github.com/Judenpech/MLEC - QA ( 1 ) Most work attempt to build BQA systems with deep learning and neural network techniques ( Ben Abacha et al , 2017 , 2019bPampari et al , 2018 ) and are thus data - hungry . However , annotating large - scale biomedical question - answer pairs with high quality is prohibitively expensive . As a result , current expert - annotated BQA datasets are small in size . ( 2 ) Multi - choice QA is a typical format type of BQA dataset . Most previous work focus on such format type of datasets in which contents are in the field of clinical medicine ( Zhang et al , 2018b ; Jin et al , 2020 ) and consumer health ( Zhang et al , 2017 ( Zhang et al , , 2018aHe et al , 2019 ; Tian et al , 2019 ) . However , there are many other specialized sub - fields in biomedicine that have not been studied before ( e.g. , Stomatology ) . ( 3 ) Ideal BQA systems should not only focus on raw text data , but also fully utilize various types of biomedical resources , such as images and tables . Unfortunately , most BQA datasets are either texts ( Tsatsaronis et al , 2015 ; Pampari et al , 2018 ; Jin et al , 2019 ) or images ( Lau et al , 2018 ; Ben Abacha et al , 2019a ; He et al , 2020a ) ; as a result , BQA datasets that are composed by fusing different biomedical resources are relatively limited . To push forward the variety of BQA datasets , we present MLEC - QA , the largest - scale Chinese multi - choice BQA dataset . Questions in MLEC - QA are collected from the National Medical Licensing Examination in China ( NMLEC ) 2 , which are carefully designed by human experts to evaluate professional knowledge and skills for those who want to be medical practitioners in China . The NMLEC has a total number of 24 categories of exams , but only five of them have the written exams in Chinese . Every year , only around 18 - 22 % of applicants can pass one of these exams , showing the complexity and difficulty of passing them even for skilled humans . There are three main properties of MLEC - QA : ( 1 ) MLEC - QA is the largest - scale Chinese multichoice BQA dataset , containing 136 , 236 questions with extra materials ( images or tables ) , Table 1 shows an example . ( 2 ) MLEC - QA first covers the following biomedical sub - fields : Clinic , Stomatology , Public Health , Traditional Chinese Medicine , and Traditional Chinese Medicine Combined with Western Medicine ( denoted as Chinese Western Medicine ) . Only one ( Clinic ) of them has been studied in previous research . ( 3 ) MLEC - QA provides extra labels of five question types ( A1 , A2 , A3 / A4 and B1 ) for each question , and an in - depth analysis of the most frequent reasoning types of the questions in MLEC - QA , such as lexical matching , multi - sentence reading and concept summary , etc . Detailed analysis can be found in Section 3.2 . Examples of sub - fields and question types are summarized in Table 2 . We set each example of five question types corresponding to one of the subfields due to page limits . As an attempt to solve MLEC - QA and provide strong baselines , we implement eight representative control methods and open - domain QA methods by a two - stage retriever - reader framework : ( 1 ) A retriever finding documents that ( might ) contain an answer from a large collection of documents . We adopt Chinese Wikipedia dumps 3 as our information sources , and use a distributed search and analytics engine , ElasticSearch 4 , as the document store and document retriever . ( 2 ) A reader finding the answer in given documents retrieved by the retriever . We fine - tune five pre - trained language models for machine reading comprehension as the reader . Experimental results show that even the current best model can only achieve accuracies of 53 % , 44 % , 40 % , 55 % , and 50 % on the five categories of subsets : Clinic , Stomatology , Public Health , Traditional Chinese Medicine , and Chinese Western Medicine , respectively . The models especially perform poorly on questions that require understanding comprehensive biomedical concepts and handling complex reasoning . In summary , the major contributions of this paper are threefold : We present MLEC - QA , the largest - scale Chinese multi - choice BQA dataset with extra materials , and it first covers five biomedical sub - fields , only one of which has been studied in previous research . We conduct an in - depth analysis on MLEC - QA , revealing that both comprehensive biomedical knowledge and sophisticated reasoning ability are required to answer questions . We implement eight representative methods as baselines and show the performance of existing methods on MLEC - QA , and provide an outlook for future research directions .", "entities": [[9, 11, "TaskName", "Question Answering"], [810, 813, "TaskName", "machine reading comprehension"]]}
{"text": "Open - Domain BQA The Text REtrieval Conference ( TREC ) ( Voorhees and Tice , 2000 ) has triggered the open - domain BQA research . At the time , most traditional BQA systems were employing complex pipelines with question processing , document / passage retrieval , and answer processing modules . Examples of such systems include EPoCare ( Niu et al , 2003 ) , MedQA ( Yu et al , 2007 ; Terol et al , 2007 ; Wang et al , 2007 ) and AskHERMES ( Cao et al , 2011 ) . With the introduction of various BQA datasets that are focused on specific biomedical topics , such as BioASQ ( Tsatsaronis et al , 2015 ) , emrQA ( Pampari et al , 2018 ) and PubMedQA ( Jin et al , 2019 ) , pioneered by Chen et al ( 2017 ) , the modern open - domain BQA systems largely simplified the traditional BQA pipeline to a two - stage retriever - reader framework by combining information retrieval and machine reading comprehension models ( Ben Abacha et al , 2017 , 2019b . Moreover , the extensive use of medical images ( e.g. , CT ) and tables ( e.g. , laboratory examination ) has improved results in real - world clinical scenarios , making the BQA a task lying at the intersection of Computer Vision ( CV ) and NLP . However , most BQA models focus on either texts or images ( Lau et al , 2018 ; Ben Abacha et al , 2019a ; He et al , 2020a ) ; as a result , BQA datasets that are composed by fusing different biomedical resources are relatively limited . Open - Domain Multi - Choice BQA Datasets With rapidly increasing numbers of consumers asking health - related questions on online medical consultation websites , cMedQA ( Zhang et al , 2017 ( Zhang et al , , 2018a , webMedQA ( He et al , 2019 ) and ChiMed ( Tian et al , 2019 ) exploit patient - doctor QA data to build consumer health QA datasets . However , the quality problems in such datasets are that the answers are written by online - doctors and the data itself has intrinsic noise . By contrast , medical licensing examinations , which are designed by human medical experts , often take the form of multi - choice questions , and contain a significant number of questions that require comprehensive biomedical knowledge and multiple reasoning ability . Such exams are the perfect data source to push the development of BQA systems . Several datasets have been released that exploit such naturally existing BQA data , which are summarized in Table 3 . Collecting from the Spain public healthcare specialization examination , HEAD - QA ( Vilares and G\u00f3mez - Rodr\u00edguez , 2019 ) contains multichoice questions from six biomedical categories , including Medicine , Pharmacology , Psychology , Nursing , Biology and Chemistry . NLPEC ( Li et al , 2020 ) collects 21.7k multi - choice questions with human - annotated answers from the National Licensed Pharmacist Examination in China , but only a small number of sample data is available for public use . Last but not least , clinical medicine , as one of the 24 categories in NMLEC , has been previously studied by MedQA ( Zhang et al , 2018b ) and MEDQA ( Jin et al , 2020 ) . However , the former did not release any data or code , and the latter only focused on clinical medicine with 34k questions in their cross - lingual studies , questions with images or tables were not included , and none of the remaining categories in MLEC - QA were studied . Basically , as shown in Table 2 and Table 4 , the questions in MLEC - QA are divided into five types including : A1 : single statement question ; B1 : similar to A1 , with a group of options shared in multiple questions ; A2 : questions accompanied by a clinical scenario ; A3 : similar to A2 , with information shared among multiple independent questions ; A4 : similar to A3 , with information shared among multiple questions , new information can be gradually added . We further classify these questions into Knowledge Questions ( KQ ) and Case Questions ( CQ ) , where KQ ( A1+B1 ) focus on the definition and comprehension of biomedical knowledge , while CQ ( A2+A3 / A4 ) require analysis and practical application for real - world medical scenarios . Both types of questions require multiple reasoning ability to answer . For the Train / Dev / Test split , randomly splitting may cause data imbalance because the number of the five question types are various from each other ( e.g. , A1 is far more than others ) . To ensure that the subsets have the same distribution of the question types , we split the data based on the question types , with 80 % training , 10 % development , and 10 % test . The overall statistics of the MLEC - QA dataset are summarized in Table 5 . We can see that the length of the questions and the vocabulary size in Clinic are larger than the rest of the subsets , explaining that clinical medicine may involve more medical subjects than other specialties .", "entities": [[9, 10, "DatasetName", "TREC"], [45, 47, "TaskName", "passage retrieval"], [114, 115, "DatasetName", "BioASQ"], [123, 124, "DatasetName", "emrQA"], [132, 133, "DatasetName", "PubMedQA"], [174, 176, "TaskName", "information retrieval"], [177, 180, "TaskName", "machine reading comprehension"]]}
{"text": "Both examination counseling books and Wikipedia have been used as the source of supporting materials in previous research ( Zhong et al , 2020 ; Jin et al , 2020 ; Vilares and G\u00f3mez - Rodr\u00edguez , 2019 ) . However , because examination counseling books are designed to help examinees pass the examination , knowledge is highly simplified and summarized ; even the easily confused knowledge points are compared . Using examination counseling books as information sources may make the retriever - reader more likely to exploit shallow text matching , and complex reasoning is seldom involved . Therefore , to help better understand the improvement coming from future models , we adopt Chinese Wikipedia dumps as our information sources , which contain a wealth of information ( over 1 million articles ) of real - world facts . Building upon the whole Chinese Wikipedia data , we use a distributed search and analytics engine , Elas - ticSearch , as the document store and document retriever , which supports very fast full - text searches . The similarity scoring function used in Elasticsearch is the BM25 algorithm ( Robertson and Zaragoza , 2009 ) , which measures the relevance of documents to a given search query . As defined in Appendix C , the larger this BM25 score , the stronger the relevance between document and query . Specifically , for each question Q i and each candidate option O ij where j { A , B , C , D , E } , we define Q i O ij = Q i + O ij as a search query to Elasticsearch and is repeated for all options . The document with the highest BM25 score returned by each query is selected as supporting materials for the next stage machine reading comprehension task .", "entities": [[89, 91, "TaskName", "text matching"], [157, 158, "DatasetName", "Elas"], [302, 305, "TaskName", "machine reading comprehension"]]}
{"text": "We apply an unified framework UER - py ( Zhao et al , 2019 ) to fine - tuning pre - trained language models on the machine reading comprehension task as our reader . We consider the following five pre - trained language models : Chinese BERT - Base ( denoted as BERT - Base ) and Multilingual Uncased BERT - Base ( denoted as BERT - Base - Multilingual ) ( Devlin et al , 2019 ) , Chinese BERT - Base with whole word masking and pre - trained over larger corpora ( denoted as BERT - wwm - ext ) ( Cui et al , 2019 ) , and the robustly optimized BERTs : Chinese RoBERTa - wwm - ext and Chinese RoBERTa - wwm - ext - large ( Cui et al , 2019 ) . Specifically , given the i th question Q i , retrieved question relevant documents D i , and a candidate option O ij , where j { A , B , C , D , E } . The input sequence for the framework is constructed by concatenating [ CLS ] , tokens in D i , [ SEP ] , tokens in Q i , [ SEP ] , tokens in an option O ij , and [ SEP ] , where [ CLS ] is the classifier token , and [ SEP ] is the sentence separator in pre - trained language models . We pass each of the five options in turn , and the model outputs the hidden state representation S ij R 1\u00d7H of the input sequence , then performs the classification and output an unnormalized log probability P ij R of each option O ij being correct by P ij = S ij W T , where W R 1\u00d7H is the weight matrix . Finally , we pass the unnormalized log probabilities of each option through a softmax layer and obtain the option with the highest probability as the predicted answer A \u2032 i .", "entities": [[26, 29, "TaskName", "machine reading comprehension"], [46, 47, "MethodName", "BERT"], [52, 53, "MethodName", "BERT"], [59, 60, "MethodName", "BERT"], [65, 66, "MethodName", "BERT"], [80, 81, "MethodName", "BERT"], [97, 98, "MethodName", "BERT"], [118, 119, "MethodName", "RoBERTa"], [125, 126, "MethodName", "RoBERTa"], [324, 325, "MethodName", "softmax"]]}
{"text": "Tables 8 and Figure 3 show the performance of baselines as well as the performance on KQ and CQ questions . As we can see , among control methods , the correct option has a slight tendency to appear in the middle ( C and D ) of candidate options , but the margins are small . The performance of the Mixed method is slightly better than a random guess , which indicates that the flexible use of guessing skills may add wings to the tiger as humans can exclude some certain wrong options , but if the cart before the horse is reversed , it is impossible to pass the exam only through opportunistic guessing . RoBERTa - wwm - ext - large and BERTwwm - ext perform better than other models on five subsets . However , even the best - performing model can only achieve accuracies between 40 % to 55 % on five subsets , so there is still a gap to pass the exams . Comparing the performance between KQ and CQ questions , most models achieve better performance on CQ , which is positively correlated with CQ 's better retrieval performance . Among different subsets , the subset TCM is the easiest ( 54.95 % ) one to answer across the board , while the subset PH is the hardest ( 40.04 % ) , which does not totally correspond to their retrieval performance as shown in Table 7 . The possible reason is that the diagnosis and treatment of diseases in traditional Chinese medicine are characterized by \" Homotherapy for Heteropathy \" , that is , treating different diseases with the same method , which may result in some patterns or mechanisms that can be used by the models to reach such results .", "entities": [[117, 118, "MethodName", "RoBERTa"]]}
{"text": "Since the data is designed by a team of anonymous human healthcare experts , we are not able to directly reach them for inclusion in this dataset and thus could not be asked for demographic information . It is expected that most of the speakers come from China with professionals working in the area of biomedicine , and speak Chinese as a native language . No direct information is available about age and gender distribution .", "entities": [[71, 74, "DatasetName", "age and gender"]]}
{"text": "For five subsets in MLEC - QA , we collect 2006 to 2020 Sprint Paper for the National Medical Licensing Examination - Tianjin Science and Technology Press in PDF format , and then converted them into digital format via Optical Character Recognition ( OCR ) . We manually checked and corrected the OCR results with confidence less than 0.99 to ensure the quality of our dataset . We also scraped practice exercises from offcn ( http://www.offcn.com/yixue/yszg/ ) , which are freely accessible online for public usage .", "entities": [[39, 42, "TaskName", "Optical Character Recognition"]]}
{"text": "NLP systems for machine translation , summarization , paraphrasing , and other tasks often fail to preserve the compositional semantics of sentences and documents because they model language as bags of words , or at best syntactic trees . To preserve semantics , they must model semantics . In pursuit of this goal , several datasets have been produced which pair natural language with compositional semantic representations in the form of directed acyclic graphs ( DAGs ) , including the Abstract Meaning Representation Bank ( AMR ; Banarescu et al 2013 ) , the Prague Czech - English Dependency Treebank ( Haji\u010d et al , 2012 ) , Deepbank ( Flickinger et al , 2012 ) , and the Universal Conceptual Cognitive Annotation ( Abend and Rappoport , 2013 ) . To make use of this data , we require models of graphs . Consider how we might use compositional semantic representations in machine translation ( Jones et al , 2012 ) . The edge labels identify ' cat ' as the object of the verb ' miss ' , ' Anna ' as the subject of ' miss ' and ' Anna ' as the possessor of ' cat ' . Edges whose head nodes are not attached to any other edge are interpreted as node labels . ( Figure 1 ) , a two - step process in which semantic analysis is followed by generation . Jones et al ( 2012 ) observe that this decomposition can be modeled with a pair of synchronous grammars , each defining a relation between strings and graphs . Necessarily , one projection of this synchronous grammar produces strings , while the other produces graphs , i.e. , is a graph grammar . A consequence of this representation is that the complete translation process can be realized by parsing : to analyze a sentence , we parse the input string with the string - generating projection of the synchronous grammar , and read off the synchronous graph from the resulting parse . To generate a sentence , we parse the graph , and read off the synchronous string from the resulting parse . In this paper , we focus on the latter problem : using graph grammars to parse input graphs . We call this graph recognition to avoid confusion with other parsing problems . Recent work in NLP has focused primarily on hyperedge replacement grammar ( HRG ; Drewes et al 1997 ) , a context - free graph grammar formalism that has been studied in an NLP context by several researchers ( Chiang et al , 2013 ; Peng et al , 2015 ; Bauer and Rambow , 2016 ) . In particular , Chiang et al ( 2013 ) propose that HRG could be used to represent semantic graphs , and precisely characterize the complexity of a CKY - style algorithm for graph recognition from Lautemann ( 1990 ) to be polynomial in the size of the input graph . HRGs are very expressive - they can generate graphs that simulate non - context - free string languages ( Engelfriet and Heyker , 1991 ; Bauer and Rambow , 2016 ) . This means they are likely more expressive than we need to represent the linguistic phenomena that appear in existing semantic datasets . In this paper , we propose the use of Regular Graph Grammars ( RGG ; Courcelle 1991 ) a subfamily of HRG that , like its regular counterparts among string and tree languages , is less expressive than context - free grammars but may admit more practical algorithms . By analogy to Chiang 's CKY - style algorithm for HRG . We develop an Earley - style recognition algorithm for RGLs that is linear in the size of the input graph .", "entities": [[3, 5, "TaskName", "machine translation"], [6, 7, "TaskName", "summarization"], [153, 155, "TaskName", "machine translation"]]}
{"text": "To recognize RGG , we exploit the property that every nonterminal including the start symbol has rank at least one ( Definition 5 ) , and we assume that the corresponding external node is identified in the input graph . This mild assumption may be reasonable for applications like AMR parsing , where grammars could be designed so that the external node is always the unique root . Later we relax this assumption . The availability of an identifiable external node suggests a top - down algorithm , and we take in - spiration from a top - down recognition algorithm for the predictive top - down parsable grammars , another subclass of HRG ( Drewes et al , 2015 ) . These grammars , the graph equivalent of LL ( 1 ) string grammars , are incomparable to RGG , but the algorithms are related in their use of top - down prediction and in that they both fix an order of the edges in the right - hand side of each production .", "entities": [[49, 51, "TaskName", "AMR parsing"]]}
{"text": "[ b ( G ) , p S : S * S G , \u03c6 p S ] can be proved from the axiom [ ext G , p S : S * S G , \u03c6 p S [ ext R ( p S ) = ext G ] ] if and only if G L ( G ) . Proof . We prove that for each X N G , [ b ( G ) , p X : X * X , \u03c6 p X ] can be proved from [ ext G , p X : X * X , \u03c6 p X [ ext R ( p X ) = ext G ] ] if and only if G L X ( G ) where the dummy nonterminal X * was added to the set of nonterminals and p X : X * X was added to the set of productions . We prove this by induction on the number of edges in G. We assume that each production in the grammar contains at least one terminal edge . If the HRG is not in this form , it can be converted into this form and in the case of RGGs they are already in this form by definition . Base Case : Let G consist of a single edge . If : Assume G L X ( G ) . Since G consists of one edge , there must be a production q : X G. Apply PREDICT to the axiom and p X : X * X to obtain the item [ \u03c6 p X ( X ) , q : X G , \u03c6 0 q [ ext G = \u03c6 p X ( X ) ] ] . Apply SCAN to the single terminal edge that makes up G to obtain [ b ( G ) , q : X G , \u03c6 q ] and finally apply COMPLETE to this and the axiom reach the goal [ b ( G ) , p X : X * X , \u03c6 p X ] . Only if : Assume the goal can be reached from the axiom and G = e. Then the item [ b ( e ) , q : X e , \u03c6 q ] must have been reached at some point for some q P G . Therefore q : X e is a production and so e = G L X ( G ) . Assumption : Assume that the proposition holds when G has fewer than k edges . Inductive Step : Assume G has k edges . If : Assume G L X ( G ) , then there is a production q : X H where H has nonterminals Y 1 , . . . , Y n and there are graphs H 1 , . . . , H n such that G = H [ Y 1 /H 1 ] . . . [ Y n /H n ] . Each graph H i for i [ n ] has fewer than k edges and so we apply the inductive hypothesis to show that we can prove the items [ b ( H i ) , r i : Y i J i , \u03c6 r i ] for each i [ n ] . By applying COMPLETE to each such item and applying SCAN to each terminal edge of H we reach the goal [ b ( G ) , p X : X * X , \u03c6 p X ] . Only If : Assume the goal can be proved from the axiom . Then we must have at some point reached an item of the form [ b ( G ) , q : X H , \u03c6 q ] and that H has nonterminals Y 1 , . . . , Y n . This means that there are graphs H 1 , . . . , H n such that [ b ( H i ) , p Y i : Y * i Y i , \u03c6 p Y i ] for each i [ n ] and G = H [ Y 1 /H 1 ] . . . [ Y n /H n ] . Since each H i has fewer than k edges , we apply the inductive hypothesis to get that H i L Y i ( G ) for each i [ n ] and therefore G L X ( G ) . Example 5 . Using the RGG in Table 1 , we show how to recognize the graph in Figure 7 , which can be derived by applying production s followed by production u , where the external nodes of Y are ( v 3 , v 2 ) . Assume the ordering of the edges in production s is arg1 , arg0 , Z ; the top node isv 1 ; the bottom node isv 2 ; and the node on the right isv 3 ; and that the marker node is not in this subgraphwe elide reference to it for simplicity . Letv 4 be the top node of R ( u ) andv 5 be the bottom node of R ( u ) . The external nodes of Y are determined top - down , so the recognize of this subgraph is triggered by this item : [ { v 3 , v 2 } , Y arg1 arg0 Z , \u03c6 0 s [ ext R ( s ) = ( v 3 , v 2 ) ] ] ( 2 ) where \u03c6 s ( arg1 ) = ( v 1 , v 3 ) , \u03c6 s ( arg0 ) = ( v 1 , v 2 ) , and \u03c6 s ( Z ) = ( v 1 ) . Table 3 shows how we can prove the item [ { v 3 , v 2 } , { e 3 , e 2 } , Y arg1arg0Z , \u03c6 ] The boundary representation { v 3 , v 2 } , { e 3 , e 2 } in this item represents the whole subgraph shown in Figure 7 . v 1 v 4 v 2 v 3 . . . . . . need ( e1 ) arg0 ( e2 ) arg1 ( e3 )", "entities": [[282, 283, "DatasetName", "0"], [298, 299, "DatasetName", "SCAN"], [573, 574, "DatasetName", "SCAN"], [927, 928, "DatasetName", "0"]]}
{"text": "Our algorithm requires a fixed ordering of the edges in the right - hand sides of each production . We will constrain this ordering to exploit the structure of RGG productions , allowing us to bound recognition complexity . If s = \u0113 1 . . .\u0113 n is an order , define s i : j = \u0113 i . . .\u0113 j . Definition 7 . Let s = \u0113 1 , . . . , \u0113 n be an edge order of a right - hand side of a production . Then s is normal if it has the following properties : 1.\u0113 1 is connected to an external node , 2 . s 1 : j is a connected graph for all j [ n ] 3 . if\u0113 i is nonterminal , each endpoint of\u0113 i must be incident with some terminal edge\u0113 j for which j < i. Example 6 . The ordering of the edges of production s in Example 5 is normal . Arbitrary HRGs do not necessarily admit a normal ordering . For example , the graph in Figure 8 can not satisfy Properties 2 and 3 simultaneously . However , RGGs do admit a normal ordering . Current Item Reason 1 . [ { v3 , v2 } , Y arg1arg0Z , \u03c6 0 s [ ext R ( s ) = ( v3 , v2 ) ] ] Equation 2 2 . [ { v3 , v2 , v1 } , { e3 } , Y arg1 arg0Z , \u03c6s [ att ( arg1 ) = ( v1 , v3 ) ] ] SCAN : 1 . and e3 = edg arg1 ( v1 , v3 ) 3 . [ { v3 , v2 , v1 } , { e3 , e2 } , Y arg1arg0 Z , \u03c6s [ att ( arg0 ) = ( v1 , v2 ) ] ] SCAN : 2 . and e2 = edg arg0 ( v1 , v2 ) ] 4 . [ ( v1 ) , Z need , \u03c6 0 u [ ext R ( u ) = ( v1 ) ] ] PREDICT : 3 . and Z need 5 . [ { v1 , v4 } , { e1 } , Z need , \u03c6u [ att ( need ) = ( v1 , v4 ) ] ] SCAN : 4 . and e1 = edg need ( v1 , v4 ) 6 . [ { v3 , v2 } , { e3 , e2 } , Y arg1arg0Z , \u03c6s [ att ( Z ) = ( v1 ) ] ] COMPLETE : 3 . and 5 . Table 3 : The steps of recognizing that the subgraph shown in Figure 7 is derived from productions r2 and u in the grammar in Table 1 . Proposition 2 . If G is an RGG , for every p P G , there is a normal ordering of the edges in R ( p ) . Proof . If R ( p ) contains a single node then it must be an external node and it must have a terminal edge attached to it since R ( p ) must contain at least one terminal edge . If R ( p ) contains multiple nodes then by C2 there must be terminal internal paths between all of them , so there must be a terminal edge attached to the external node , which we use to satisfy Property 1 . To produce a normal ordering , we next select terminal edges once one of their endpoints is connected to an ordered edge , and nonterminal edges once all endpoints are connected to ordered edges , possible by C2 . Therefore , Properties 2 and 3 are satisfied . A normal ordering tightly constrains the recognition of edges . Property 3 ensures that when we apply PREDICT , the external nodes of the predicted edge are all bound to specific nodes in the graph . Properties 1 and 2 ensure that when we apply SCAN , at least one endpoint of the edge is bound ( fixed ) .", "entities": [[223, 224, "DatasetName", "0"], [274, 275, "DatasetName", "SCAN"], [323, 324, "DatasetName", "SCAN"], [349, 350, "DatasetName", "0"], [400, 401, "DatasetName", "SCAN"], [685, 686, "DatasetName", "SCAN"]]}
{"text": "Assume a normally - ordered RGG . Let the maximum number of edges in the right - hand side of any production be m ; the maximum number of nodes in any right - hand side of a production k ; the maximum degree of any node in the input graph d ; and the number of nodes in the input graph n. As previously mentioned , Drewes et al ( 2015 ) also propose a HRG recognizer which can recognize a subclass of HRG ( incomparable to RGG ) called the predictive top - down parsable grammars . Their recognizer in this case runs in O ( n 2 ) time . A well - known bottom - up recognizing algorithm for HRG was first proposed by Lautemann ( 1990 ) . In this paper , the recognizer is shown to be polynomial in the size of the input graph . Later , Chiang et al ( 2013 ) formulate the same algorithm more precisely and show that the recognizing complexity is O ( ( 3 d \u00d7 n ) k+1 ) where k in their case is the treewidth of the grammar . Remark 1 . The maximum number of nodes in any right - hand side of a production ( k ) is also the maximum number of boundary nodes for any subgraph in the recognizer . COMPLETE combines subgraphs I and J only when the entire subgraph derived from Y has been recognized . Boundary nodes of J are also boundary nodes of I because they are nodes in the terminal subgraph of R ( p ) where Y connects . The boundary nodes of I \u222a J are also bounded by k since form a subset of the boundary nodes of I. Remark 2 . Given a boundary node , there are at most ( d m ) k\u22121 ways of identifying the remaining boundary nodes of a subgraph that is isomorphic to the terminal subgraph of the right - hand side of a production . The terminal subgraph of each production is connected by C2 , with a maximum path length of m. For each edge in the path , there are at most d subsequent edges . Hence for the k \u2212 1 remaining boundary nodes there are ( d m ) k\u22121 ways of choosing them . We count instantiations of COMPLETE for an upper bound on complexity ( McAllester , 2002 ) , using similar logic to ( Chiang et al , 2013 ) . The number of boundary nodes of I , J and I \u222a J is at most k. Therefore , if we choose an arbitrary node to be some boundary node of I \u222a J , there are at most ( d m ) k\u22121 ways of choosing its remaining boundary nodes . For each of these nodes , there are at most ( 3 d ) k states of their attached boundary edges : in I , in J , or in neither . The total number of instantiations is O ( n ( d m ) k\u22121 ( 3 d ) k ) , linear in the number of input nodes and exponential in the degree of the input graph . Note that in the case of the AMR dataset ( Banarescu et al 2013 ) , the maximum node degree is 17 and the average is 2.12 . We observe that RGGs could be relaxed to produce graphs with no external nodes by adding a dummy nonterminal S with rank 0 and a single production S S. To adapt the recognition algorithm , we would first need to guess where the graph starts . This would add a factor of n to the complexity as the graph could start at any node .", "entities": [[596, 597, "DatasetName", "0"]]}
{"text": "We have presented RGG as a formalism that could be useful for semantic representations and we have provided a top - down recognition algorithm for them . The constraints of RGG enable more efficient recognition than general HRG , and this tradeoff is reasonable since HRG is very expressive - when generating strings , it can express non - context - free languages ( Engelfriet and Heyker , 1991 ; Bauer and Rambow , 2016 ) , far more power than needed to express semantic graphs . On the other hand , RGG is so constrained that it may not be expressive enough : it would be more natural to derive the graph in Figure 4 from outermost to innermost predicate ; but constraint C2 makes it difficult to express this , and the grammar in Table 1 A possible alternative would be to consider Restricted DAG Grammars ( RDG ; Bj\u00f6rklund et al 2016 ) . Parsing for a fixed such grammar can be achieved in quadratic time with respect to the input graph . It is known that for a fixed HRG generating k - connected hypergraphs consisting of hyperedges of rank k only , parsing can be carried out in cubic time ( k - HRG ; ( Drewes , 1993 ) ) . More general than RDGs a is the class of graph languages recognized by DAG automata ( DA - GAL ; Blum and Drewes 2016 ) , for which the deterministic variant provides polynomial time parsing . Note that RGGs can generate graph languages of unbounded node degree . With respect to expressive power , RDGs and k - HRGs are incomparable to RGGs . Figure 9 shows the relationships between the context - free and regular languages for strings , trees and graphs . Monadic - second order logic ( MSOL ; Courcelle and Engelfriet 2011 ) is a form of logic which when restricted to strings gives us exactly the regular string languages and when restricted to trees gives us exactly the regular tree languages . RGLs lie in the intersection of HRG and MSOL on graphs but they do not make up this entire intersection . Courcelle ( 1991 ) defined ( non - constructively ) this intersection to be the strongly context - free languages ( SCFL ) . We believe that there may be other formalisms that are subfamilies of SCFL which may be useful for semantic representations . All inclusions shown in Figure 9 are strict . For instance , RGL can not produce \" star graphs \" ( one node that has edges to n other nodes ) , while DAGAL and HRL can produce such graphs . It is well - known that HRL and MSOL are incomparable . There is a language in RGL that is not in DAGAL , for instance , \" ladders \" ( two string graphs of n nodes each , with an edge between the ith node of each string ) . Another alternative formalism to RGG that is defined as a restriction of HRG are Tree - like Grammars ( TLG ; Matheja et al 2015 ) . They define a subclass of SCFL , i.e. , they are MSO definable . TLGs have been considered for program verification , where closure under intersection of the formalism is essential . Note that RGGs are also closed under intersection . While TLG and RDG are both incomparable to RGG , they share important characteristics , including the fact that the terminal subgraph of every production is connected . This means that our top - down recognition algorithm is applicable to both . In the future we would like to investigate larger , less restrictive ( and more linguistically expressive ) subfamilies of SCFL . We plan to implement and evaluate our algorithm experimentally .", "entities": [[145, 146, "DatasetName", "Restricted"]]}
{"text": "This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science , funded by the UK Engineering and Physical Sciences Research Council ( grant EP / L016427/1 ) and the University of Edinburgh ; and in part by a Google faculty research award ( to AL ) . We thank Clara Vania , Sameer Bansal , Ida Szubert , Federico Fancellu , Antonis Anastasopoulos , Marco Damonte , and the anonymous reviews for helpful discussion of this work and comments on previous drafts of the paper .", "entities": [[44, 45, "DatasetName", "Google"]]}
{"text": "Neural Machine Translation with the Transformer and Multi - Source Romance Languages for the Biomedical WMT 2018 task", "entities": [[1, 3, "TaskName", "Machine Translation"], [5, 6, "MethodName", "Transformer"], [15, 17, "DatasetName", "WMT 2018"]]}
{"text": "The Transformer model is the first NMT model relying entirely on self - attention to compute representations of its input and output without using recurrent neural networks ( RNN ) or convolutional neural networks ( CNN ) . RNNs read one word at a time , having to perform multiple steps before generating an output that depends on words that are far away . But it has been demonstrated that the more steps required , the harder it is to the network to learn how to make these decisions ( Bahdanau et al , 2015 ) . In addition , given the sequential nature of the RNNs , it is difficult to fully take advantage of modern computing devices such as Tensor Processing Units ( TPUs ) or Graphics Processing Units ( GPUs ) which rely on parallel processing . The Transformer is an encoder - decoder model that was conceived to solve these problems . The encoder is composed of three stages . In the first stage input words are projected into an embedded vector space . In order to capture the notion of token position within the sequence , a positional encoding is added to the embedded input vectors . Without positional encodings , the output of the multi - head attention network would be the same for the sentences \" I love you more than her \" and \" I love her more than you \" . The second stage is a multi - head self - attention . Instead of computing a single attention , this stage computes multiple attention blocks over the source , concatenates them and projects them linearly back onto a space with the initial dimensionality . The individual attention blocks compute the scaled dot - product attention with different linear projections . Finally a position - wise fully connected feed - forward network is used , which consists of two linear transformations with a ReLU activation ( Vinod Nair , 2010 ) The decoder operates similarly , but generates one word at a time , from left to right . It is composed of five stages . The first two are similar to the encoder : embedding and positional encoding and a masked multi - head self - attention , which unlike in the encoder , forces to attend only to past words . The third stage is a multi - head attention that not only attends to these past words , but also to the final representations generated by the encoder . The fourth stage is another position - wise feed - forward network . Finally , a softmax layer allows to map target word scores into target word probabilities . For more specific details about the architecture , refer to the original paper .", "entities": [[1, 2, "MethodName", "Transformer"], [141, 142, "MethodName", "Transformer"], [210, 214, "MethodName", "multi - head attention"], [290, 295, "MethodName", "scaled dot - product attention"], [322, 323, "MethodName", "ReLU"], [397, 401, "MethodName", "multi - head attention"], [437, 438, "MethodName", "softmax"]]}
{"text": "Multi - source translation consists in exploiting multiple text inputs to improve NMT ( Zoph and Knight , 2016 ) . In our case , we are using this approach in the Transformer architecture described above and using only inputs from the same language family .", "entities": [[32, 33, "MethodName", "Transformer"]]}
{"text": "The experimental framework is the Biomedical Translation Task ( WMT18 ) 2 . The corpus used to train the model are the one provided for the task for the selected languages pairs : Spanishto - English ( es2en ) , French - to - English ( fr2en ) and Portuguese - to - English ( pt2en ) . Sources are mainly from Scielo and Medline and detailed in Table 3 . Validation sets were taken from Khresmoi development data 3 , as recommended in the task description . Each validation dataset contains 500 sentence pairs . Test sets were the ones provides by the task for the previous year competition ( WMT17 4 ) . Preprocessing relied on three basic steps : tokenization , truecasing and limiting sentence length to 80 words . Words were segmented by means of Byte - Pair Encoding ( BPE ) ( Sennrich et al , 2015 ) .", "entities": [[6, 7, "TaskName", "Translation"], [89, 91, "DatasetName", "validation dataset"], [144, 145, "MethodName", "BPE"]]}
{"text": "Neural Relation Extraction for Knowledge Base Enrichment", "entities": [[1, 3, "TaskName", "Relation Extraction"]]}
{"text": "Inspired by the work of Brin ( 1998 ) , state - of - theart methods employ distant supervision by leveraging seed facts from an existing KG ( Mintz et al , 2009 ; Suchanek et al , 2009 ; Carlson et al , 2010 ) . These methods learn extraction patterns from seed facts , apply the patterns to extract new fact candidates , iterate this principle , and finally use statistical inference ( e.g. , a classifier ) for reducing the false positive rate . Some of these methods hinge on the assumption that the co - occurrence of a seed fact 's entities in the same sentence is an indicator of expressing a semantic relationship between the entities . This is a potential source of wrong labeling . Follow - up studies ( Hoffmann et al , 2010 ; Riedel et al , 2010Riedel et al , , 2013Surdeanu et al , 2012 ) overcome this limitation by various means , including the use of relation - specific lexicons and latent factor models . Still , these methods treat entities by their surface forms and disregard their mapping to existing entities in the KG . Suchanek et al ( 2009 ) and Sa et al ( 2017 ) used probabilistic - logical inference to eliminate false positives , based on constraint solving or Monte Carlo sampling over probabilistic graphical models , respectively . These methods integrate entity linking ( i.e. , NED ) into their models . However , both have high computational complexity and rely on modeling constraints and appropriate priors . Recent studies employ neural networks to learn the extraction of triples . Nguyen and Grish - None of these neural models is geared for KG enrichment , as the canonicalization of entities is out of their scope .", "entities": [[0, 1, "DatasetName", "Inspired"], [239, 241, "TaskName", "entity linking"]]}
{"text": "Figure 1 illustrates the overall solution framework . Our framework consists of three components : data collection module , embedding module , and neural relation extraction module . In the data collection module ( detailed in Section 3.2 ) , we align known triples in an existing KB with sentences that contain such triples from a text corpus . The aligned pairs of sentences and triples will later be used as the training data in our neural relation extraction module . This alignment is done by distant supervision . To obtain a large number of high - quality alignments , we augment the process with a co - reference resolution to extract sentences with implicit entity names , which enlarges the set of candidate sentences to be aligned . We further use dictionary based paraphrase detection to filter sentences that do not express any relationships between entities . In the embedding module ( detailed in Section 3.3 ) , we propose a joint learning of word and entity embeddings by combining skip - gram ( Mikolov et al , 2013 ) to compute the word embeddings and TransE ( Bordes et al , 2013 ) to compute the entity embeddings . The objective of the joint learning is to capture the similarity of words and entities that helps map the entity names into the related entity IDs . Moreover , the resulting entity embeddings are used to train a triple classifier that helps filter invalid triples generated by our neural relation extraction model . In the neural relation extraction module ( detailed in Section 3.4 ) , we propose an n - gram based attention model by expanding the attention mechanism to the n - gram token of a sentence . The ngram attention computes the n - gram combination of attention weight to capture the verbal or noun phrase context that complements the word level attention of the standard attention model . This expansion helps our model to better capture the multi - word context of entities and relationships . The output of the encoder - decoder model is a sequence of the entity and predicate IDs where every three IDs indicate a triple . To generate highquality triples , we propose two strategies . The first strategy uses a modified beam search that computes the lexical similarity of the extracted entities with the surface form of entity names in the input sentence to ensure the correct entity prediction . The second strategy uses a triple classifier that is trained using the entity embeddings from the joint learning to filter the invalid triples . The triple generation process is detailed in Section 3.5", "entities": [[24, 26, "TaskName", "relation extraction"], [77, 79, "TaskName", "relation extraction"], [167, 169, "TaskName", "entity embeddings"], [184, 186, "TaskName", "word embeddings"], [187, 188, "MethodName", "TransE"], [198, 200, "TaskName", "entity embeddings"], [232, 234, "TaskName", "entity embeddings"], [250, 252, "TaskName", "relation extraction"], [257, 259, "TaskName", "relation extraction"], [423, 425, "TaskName", "entity embeddings"]]}
{"text": "We aim to extract triples from a sentence for KB enrichment by proposing a supervised relation extraction model . To train such a model , we need a large volume of fully labeled training data in the form of sentence - triple pairs . Following Sorokin and Gurevych ( 2017 ) , we use distant supervision ( Mintz et al , 2009 ) to align sentences in Wikipedia 1 with triples in Wikidata 2 ( Vrandecic and Kr\u00f6tzsch , 2014 ) . We map an entity mention in a sentence to the corresponding entity entry ( i.e. , Wikidata ID ) in Wikidata via the hyperlink associated to the entity mention , which is recorded in Wikidata as the url property of the entity entry . Each pair may contain one sentence and multiple triples . We sort the order of the triples based on the order of the predicate paraphrases that indicate the relationships between entities in the sentence . We collect sentence - triple pairs by extracting sentences that contain both head and tail entities of Wikidata triples . To generate high - quality sentence - triple pairs , we propose two additional steps : ( 1 ) extracting sentences that contain implicit entity names using co - reference resolution , and ( 2 ) filtering sentences that do not express any relationships using paraphrase detection . We detail these steps below . Prior to aligning the sentences with triples , in Step ( 1 ) , we find the implicit entity names to increase the number of candidate sentences to be aligned . We apply co - reference resolution ( Clark and Manning , 2016 ) to each paragraph in a Wikipedia article and replace the extracted co - references with the proper entity name . We observe that the first sentence of a paragraph in a Wikipedia article may contain a pronoun that refers to the main entity . For example , there is a paragraph in the Barack Obama article that starts with a sentence \" He was reelected to the Illinois Senate in 1998 \" . This may cause the standard co - reference resolution to miss the implicit entity names for the rest of the paragraph . To address this problem , we heuristically replace the pronouns in the first sentence of a paragraph if the main entity name of the Wikipedia page is not mentioned . For the sentence in the previous example , we replace \" He \" with \" Barack Obama \" . The intuition is that a Wikipedia article contains content of a single entity of interest , and that the pronouns mentioned in the first sentence of a paragraph mostly relate to the main entity . In Step ( 2 ) , we use a dictionary based paraphrase detection to capture relationships between entities in a sentence . First , we create a dictionary by populating predicate paraphrases from three sources including PATTY ( Nakashole et al , 2012 ) , POLY ( Grycner and Weikum , 2016 ) , and PPDB ( Ganitkevitch et al , 2013 ) ship \" place of birth \" are { born in , was born in , ... } . Then , we use this dictionary to filter sentences that do not express any relationships between entities . We use exact string matching to find verbal or noun phrases in a sentence which is a paraphrases of a predicate of a triple . For example , for the triple Barack Obama , place of birth , Honolulu , the sentence \" Barack Obama was born in 1961 in Honolulu , Hawaii \" will be retained while the sentence \" Barack Obama visited Honolulu in 2010 \" will be removed ( the sentence may be retained if there is another valid triple Barack Obama , visited , Honolulu ) . This helps filter noises for the sentence - triple alignment . The collected dataset contains 255 , 654 sentence - triple pairs . For each pair , the maximum number of triples is four ( i.e. , a sentence can produce at most four triples ) . We split the dataset into train set ( 80 % ) , dev set ( 10 % ) and test set ( 10 % ) ( we call it the WIKI test dataset ) . For stress testing ( to test the proposed model on a different style of text than the training data ) , we also collect another test dataset outside Wikipedia . We apply the same procedure to the user reviews of a travel website . First , we collect user reviews on 100 popular landmarks in Australia . Then , we apply the adapted distant supervision to the reviews and collect 1 , 000 sentence - triple pairs ( we call it the GEO test dataset ) . Table 2 summarizes the statistics of our datasets .", "entities": [[15, 17, "TaskName", "relation extraction"]]}
{"text": "We compare our proposed model 3 with three existing models including CNN ( the state - of - theart supervised approach by Lin et al ( 2016 ) ) , MiniE ( the state - of - the - art unsupervised approach by Gashteovski et al ( 2017 ) ) , and ClausIE by Corro and Gemulla ( 2013 ) . To map the extracted entities by these models , we use two state - of - theart NED systems including AIDA ( Hoffart et al , 2011 ) and NeuralEL ( Kolitsas et al , 2018 ) . The precision ( tested on our test dataset ) of AIDA and NeuralEL are 70 % and 61 % respectively . To map the extracted predicates ( relationships ) of the unsupervised approaches output , we use the dictionary based paraphrase detection . We use the same dictionary that is used to collect the dataset ( i.e. , the combination of three paraphrase dictionaries including PATTY ( Nakashole et al , 2012 ) , POLY ( Grycner and Weikum , 2016 ) , and PPDB ( Ganitkevitch et al , 2013 ) ) . We replace the extracted predicate with the correct predicate ID if one of the paraphrases of the correct predicate ( i.e. , the gold standard ) appear in the extracted predicate . Otherwise , we replace the extracted predicate with \" NA \" to indicate an unrecognized predicate . We also compare our N - gram Attention model with two encoder - decoder based models including the Single Attention model ( Bahdanau et al , 2015 ) and Transformer model ( Vaswani et al , 2017 ) .", "entities": [[271, 272, "MethodName", "Transformer"]]}
{"text": "CODEX : A Comprehensive Knowledge Graph Completion Benchmark", "entities": [[4, 7, "TaskName", "Knowledge Graph Completion"]]}
{"text": "We present CODEX , a set of knowledge graph COmpletion Datasets EXtracted from Wikidata and Wikipedia that improve upon existing knowledge graph completion benchmarks in scope and level of difficulty . In terms of scope , CODEX comprises three knowledge graphs varying in size and structure , multilingual descriptions of entities and relations , and tens of thousands of hard negative triples that are plausible but verified to be false . To characterize CODEX , we contribute thorough empirical analyses and benchmarking experiments . First , we analyze each CODEX dataset in terms of logical relation patterns . Next , we report baseline link prediction and triple classification results on CODEX for five extensively tuned embedding models . Finally , we differentiate CODEX from the popular FB15 K - 237 knowledge graph completion dataset by showing that CODEX covers more diverse and interpretable content , and is a more difficult link prediction benchmark . Data , code , and pretrained models are available at https://bit.ly/2EPbrJs .", "entities": [[7, 10, "TaskName", "knowledge graph COmpletion"], [20, 23, "TaskName", "knowledge graph completion"], [39, 41, "TaskName", "knowledge graphs"], [103, 105, "TaskName", "link prediction"], [106, 108, "TaskName", "triple classification"], [130, 133, "TaskName", "knowledge graph completion"], [150, 152, "TaskName", "link prediction"]]}
{"text": "Knowledge graphs are multi - relational graphs that express facts about the world by connecting entities ( people , places , things , concepts ) via different types of relationships . The field of automatic knowledge graph completion ( KGC ) , which is motivated by the fact that knowledge graphs are usually incomplete , is an active research direction spanning several subfields of artificial intelligence ( Nickel et al , 2015 ; Ji et al , 2020 ) . As progress in artificial intelligence depends heavily on data , a relevant and high - quality benchmark is imperative to evaluating and advancing the state of the art in KGC . However , the field has largely remained static in this regard over the past decade . Outdated subsets of Freebase ( Bollacker et al , 2008 ) are most commonly used for evaluation in KGC , even though Freebase had known quality issues ( Tanon et al , 2016 ) and was eventually deprecated in favor of the more recent Wikidata knowledge base ( Vrande\u010di\u0107 and Kr\u00f6tzsch , 2014 ) . Indeed , KGC benchmarks extracted from Freebase like FB15 K and FB15 K - 237 ( Bordes et al , 2013 ; are questionable in quality . For example , FB15 K was shown to have train / test leakage . Later in this paper ( 6.2 ) , we will show that a relatively large proportion of relations in FB15 K - 237 can be covered by a trivial frequency rule . To address the need for a solid benchmark in KGC , we present CODEX , a set of knowledge graph COmpletion Datasets EXtracted from Wikidata and its sister project Wikipedia . Inasmuch as Wikidata is considered the successor of Freebase , CODEX improves upon existing Freebase - based KGC benchmarks in terms of scope and level of difficulty ( Table 1 ) . Our contributions include : Foundations We survey evaluation datasets in encyclopedic knowledge graph completion to motivate a new benchmark ( 2 and Appendix A ) . Data We introduce CODEX , a benchmark consisting of three knowledge graphs varying in size and structure , entity types , multilingual labels and descriptions , and - unique to CODEX - manually verified hard negative triples ( 3 ) . To better understand CODEX , we analyze the logical relation patterns in each of its datasets ( 4 ) . Benchmarking We conduct large - scale model selection and benchmarking experiments , reporting baseline link prediction and triple classification results on CODEX for five widely used embedding models from different architectural classes ( 5 ) . Comparative analysis Finally , to demonstrate the unique value of CODEX , we differentiate Multi - domain , with a strong focus on awards , entertainment , and sports ( 6.1 and Appendix E ) Multi - domain , with focuses on writing , entertainment , music , politics , journalism , academics , and science ( 6.1 and Appendix E ) Scope ( auxiliary data ) Various decentralized versions of FB15 K with , e.g. , entity types ( Xie et al , 2016 ) , sampled negatives ( Socher et al , 2013 ) , and more ( Table 8 ) Centralized repository of three datasets with entity types , multilingual text , and manually annotated hard negatives ( 3 ) Level of difficulty FB15 K has severe train / test leakage from inverse relations ; while removal of inverse relations makes FB15 K - 237 harder than FB15 K , FB15 K - 237 still has a high proportion of easy - to - predict relational patterns ( 6.2 ) Inverse relations removed from all datasets to avoid train / test leakage ( 3.2 ) ; manually annotated hard negatives for the task of triple classification ( 3.4 ) ; few trivial patterns for the task of link prediction ( 6.2 ) CODEX from FB15 K - 237 in terms of both content and difficulty ( 6 ) . We show that CODEX covers more diverse and interpretable content , and is a more challenging link prediction benchmark .", "entities": [[0, 2, "TaskName", "Knowledge graphs"], [35, 38, "TaskName", "knowledge graph completion"], [49, 51, "TaskName", "knowledge graphs"], [273, 276, "TaskName", "knowledge graph COmpletion"], [329, 332, "TaskName", "knowledge graph completion"], [354, 356, "TaskName", "knowledge graphs"], [411, 413, "TaskName", "model selection"], [419, 421, "TaskName", "link prediction"], [422, 424, "TaskName", "triple classification"], [638, 640, "TaskName", "triple classification"], [651, 653, "TaskName", "link prediction"], [689, 691, "TaskName", "link prediction"]]}
{"text": "In addition to large encyclopedic knowledge graphs , it is common to evaluate KGC methods on at least one smaller , domain - specific dataset , typically drawn from the WordNet semantic network ( Miller , 1998 ; Bordes et al , 2013 ) . Other choices include the Unified Medical Language System ( UMLS ) database ( McCray , 2003 ) , the Alyawarra kinship dataset ( Kemp et al , 2006 ) , the Countries dataset ( Bouchard et al , 2015 ) , and variants of a synthetic \" family tree \" ( Hinton , 1986 ) . As our focus in this paper is encyclopedic knowledge , we do not cover these datasets further . ( 3.4 ) . We compute multilingual coverage over all labels , descriptions , and entity Wikipedia extracts successfully retrieved for the respective dataset in Arabic ( ar ) , German ( de ) , English ( en ) , Spanish ( es ) , Russian ( ru ) , and Chinese ( zh ) .", "entities": [[5, 7, "TaskName", "knowledge graphs"], [54, 55, "DatasetName", "UMLS"]]}
{"text": "We collected an initial set of triples using a type of snowball sampling ( Goodman , 1961 ) . We first manually defined a broad seed set of entity and relation types common to 13 domains : Business , geography , literature , media and entertainment , medicine , music , news , politics , religion , science , sports , travel , and visual art . Examples of seed entity types include airline , journalist , and religious text ; corresponding seed relation types in each respective domain include airline alliance , notable works , and language of work or name . Table 9 in Appendix B gives all seed entity and relation types . Using these seeds , we retrieved an initial set of 380 , 038 entities , 75 relations , and 1 , 156 , 222 triples by querying Wikidata for statements of the form ( head entity of seed type , seed relation type , ? ) .", "entities": [[118, 119, "DatasetName", "seeds"]]}
{"text": "Knowledge graphs are unique in that they only contain positive statements , meaning that triples not observed in a given knowledge graph are not necessarily false , but merely unseen ; this is called the Open World Assumption ( Gal\u00e1rraga et al , 2013 ) . However , most machine learning tasks on knowledge graphs require negatives in some capacity . While different negative sampling strategies exist ( Cai and Wang , 2018 ) , the most common approach is to randomly perturb observed triples to generate negatives , following Bordes et al ( 2013 ) . While random negative sampling is beneficial and even necessary in the case where a large number of negatives is needed ( i.e. , training ) , it is not necessarily useful for evaluation . For example , in the task of triple classification , the goal is to discriminate between positive ( true ) and negative ( false ) triples . As we show in 5.5 , triple classification over randomly generated negatives is trivially easy for state - of - the - art models because random negatives are generally not meaningful or plausible . Therefore , we generate and manually evaluate hard negatives for KGC evaluation . Generation To generate hard negatives , we used each pre - trained embedding model from 5.2 to predict tail entities of triples in CODEX . For each model , we took as candidate negatives the triples ( h , r , t ) for which ( i ) the type of the predicted tail entityt matched the type of the true tail entity t ; ( ii ) t was ranked in the top - 10 predictions by that model ; and ( iii ) ( h , r , t ) was not observed in G.", "entities": [[0, 2, "TaskName", "Knowledge graphs"], [53, 55, "TaskName", "knowledge graphs"], [138, 140, "TaskName", "triple classification"], [164, 166, "TaskName", "triple classification"]]}
{"text": "To give an idea of the types of reasoning necessary for models to perform well on CODEX , we analyze the presence of learnable binary relation patterns within CODEX . The three main types of such patterns in knowledge graphs are symmetry , inversion , and compositionality ( Trouillon et al , 2019 ; . We address symmetry and compositionality here , and omit inversion because we specifically removed inverse relations to avoid train / test leakage ( 3.2 ) .", "entities": [[38, 40, "TaskName", "knowledge graphs"]]}
{"text": "Compositionality captures path rules of the form ( h , r 1 , x 1 ) , . . . , ( x n , r n , t ) ( h , r , t ) . To learn these rules , models must be capable of \" multi - hop \" reasoning on knowledge graphs ( Guu et al , 2015 ) . To identify compositional paths , we use the AMIE3 system ( Lajus et al , 2020 ) , which outputs rules with confidence scores that capture how many times those rules are seen versus violated , to identify paths of lengths two and three ; we omit longer paths as they are relatively costly to compute . We identify 26 , 44 , and 93 rules in CODEX - S , CODEX - M , and CODEX - L , respectively , with average confidence ( out of 1 ) of 0.630 , 0.556 , and 0.459 . Table 4 gives the percentage of triples per dataset participating in a discovered rule . Evidently , composition is especially prevalent in CODEX - L. An example rule in CODEX - L is \" if X was founded by Y , and Y 's country of citizenship is Z , then the country [ i.e. , of origin ] of X is Z \" ( confidence 0.709 ) . We release these rules as part of CODEX for further development of KGC methodologies that incorporate or learn rules .", "entities": [[55, 57, "TaskName", "knowledge graphs"]]}
{"text": "Next , we benchmark performance on CODEX for the tasks of link prediction and triple classification . To ensure that models are fairly and accurately compared , we follow Ruffinelli et al ( 2020 ) , who conducted what is ( to the best of our knowledge ) the largest - scale hyperparameter tuning study of knowledge graph embeddings to date . Note that CODEX can be used to evaluate any type of KGC method . However , we focus on embeddings in this section due to their widespread usage in modern NLP ( Ji et al , 2020 ) .", "entities": [[11, 13, "TaskName", "link prediction"], [14, 16, "TaskName", "triple classification"], [56, 59, "TaskName", "knowledge graph embeddings"]]}
{"text": "We compare the following embedding methods : RESCAL ( Nickel et al , 2011 ) , TransE ( Bordes et al , 2013 ) , ComplEx ( Trouillon et al , 2016 ) , ConvE ( Dettmers et al , 2018 ) , andTuckER ( Balazevic et al , 2019b ) . These models represent several classes of architecture , from linear ( RESCAL , TuckER , ComplEx ) to translational ( TransE ) to nonlinear / learned ( ConvE ) . Appendix D provides more specifics on each model .", "entities": [[7, 8, "MethodName", "RESCAL"], [16, 17, "MethodName", "TransE"], [63, 64, "MethodName", "RESCAL"], [65, 66, "MethodName", "TuckER"], [72, 73, "MethodName", "TransE"]]}
{"text": "We first compare the content in CODEX - M , which is extracted from Wikidata , with that of FB15 K - 237 , which is extracted from Freebase . For brevity , Figure 2 compares the top - 15 relations by mention count in the two datasets . Appendix E provides more content comparisons . Diversity The most common relation in CODEX - M is occupation , which is because most people on Wikidata have multiple occupations listed . By contrast , the frequent relations in FB15 K - 237 are mostly related to awards and film . In fact , over 25 % of all triples in FB15 K - 237 belong to the /award relation domain , suggesting that CODEX covers a more diverse selection of content . Interpretability The Freebase - style relations are also arguably less interpretable than those in Wikidata . Whereas Wikidata relations have concise natural language labels , the Freebase relation labels are hierarchical , often at five or six levels of hierarchy ( Figure 2 ) . Moreover , all relations in Wikidata are binary , whereas some Freebase relations are n - nary ( Tanon et al , 2016 ) , meaning that they connect more than two entities . The relations containing a dot ( \" . \" ) are such n - nary relations , and are difficult to reason about without understanding the structure of Freebase , which has been deprecated . We further discuss the impact of such n - nary relations for link prediction in the following section .", "entities": [[257, 259, "TaskName", "link prediction"]]}
{"text": "We present CODEX , a set of knowledge graph COmpletion Datasets EXtracted from Wikidata and Wikipedia , and show that CODEX is suitable for multiple KGC tasks . We release data , code , and pretrained models for use by the community at https://bit.ly/2EPbrJs . Some promising future directions on CODEX include : Better model understanding CODEX can be used to analyze the impact of hyperparameters , training strategies , and model architectures in KGC tasks . Revival of triple classification We encourage the use of triple classification on CODEX in addition to link prediction because it directly tests discriminative power . Fusing text and structure Including text in both the link prediction and triple classification tasks should substantially improve performance . Furthermore , text can be used for few - shot link prediction , an emerging research direction ( Xiong et al , 2017 ; Shi and Weninger , 2017 ) . Overall , we hope that CODEX will provide a boost to research in KGC , which will in turn impact many other fields of artificial intelligence .", "entities": [[7, 10, "TaskName", "knowledge graph COmpletion"], [79, 81, "TaskName", "triple classification"], [86, 88, "TaskName", "triple classification"], [93, 95, "TaskName", "link prediction"], [111, 113, "TaskName", "link prediction"], [114, 116, "TaskName", "triple classification"], [132, 134, "TaskName", "link prediction"]]}
{"text": "Table 8 provides an overview of knowledge graph embedding papers with respect to datasets and evaluation tasks . In our review , we only consider papers published between 2014 and 2020 in the main proceedings of conferences where KGC embedding papers are most likely to appear : Artificial intelligence ( AAAI , IJCAI ) , machine learning ( ICML , ICLR , NeurIPS ) , and natural language processing ( ACL , EMNLP , NAACL ) . The main evaluation benchmarks are FB15 K ( Bordes et al , 2013 ) , WN18 ( Bordes et al , 2013 ) , FB15 K - 237 ( Toutanova and Chen , 2015 ) , WN18RR ( Dettmers et al , 2018 ) , FB13 ( Socher et al , 2013 ) , WN11 ( Socher et al , 2013 ) , NELL - 995 ( Xiong et al , 2017 ) , YAGO3 - 10 ( Dettmers et al , 2018 , Countries ( Bouchard et al , 2015 ) . UMLS ( McCray , 2003 ) , Kinship ( Kemp et al , 2006 ) , Families ( Hinton , 1986 , and other versions of NELL ( Mitchell et al , 2018 ) .", "entities": [[6, 9, "TaskName", "knowledge graph embedding"], [92, 93, "DatasetName", "WN18"], [113, 114, "DatasetName", "WN18RR"], [140, 141, "DatasetName", "NELL"], [151, 154, "DatasetName", "YAGO3 - 10"], [170, 171, "DatasetName", "UMLS"], [177, 178, "DatasetName", "Kinship"], [196, 197, "DatasetName", "NELL"]]}
{"text": "We provide the annotation guidelines we used to label candidate negative triples ( 3.4 ) . Task You must label each triple as either true or false . To help you find the answer , we have provided you with Wikipedia and Wikidata links for the entities and relations in each triple . You may also search on Google for the answer , although most claims should be resolvable using Wikipedia and Wikidata alone . If you are not able to find any reliable , specific , clear information supporting the claim , choose false . You may explain your reasoning if need be or provide sources to back up your answer in the optional explanation column . Examples False triples may have problems with grammar , factual content , or both . Examples of grammatically incorrect triples are those whose entity or relation types do not make sense , for example : ( United States of America , continent , science fiction writer ) ( Mohandas Karamchand Gandhi , medical condition , British Raj ) ( Canada , foundational text , Vietnamese cuisine ) Examples of grammatically correct but factually false triples include : ( United States of America , continent , Europe ) ( Mohandas Karamchand Gandhi , country of citizenship , Argentina ) ( Canada , foundational text , Harry Potter and the Goblet of Fire ) ( Alexander Pushkin , influenced by , Leo Tolstoy ) - Pushkin died only a few years after Tolstoy was born , so this sentence is unlikely . Notice that in the latter examples , the entity types match up , but the statements are still false . Tips For triples about people 's occupation and genre , try to be as specific as possible . For example , if the triple says ( < person > , occupation , guitarist ) but that person is mainly known for their singing , choose false , even if that person plays the guitar . Likewise , if a triple says ( < person > , genre , classical ) but they are mostly known for jazz music , choose false even if , for example , that person had classical training in their childhood .", "entities": [[58, 59, "DatasetName", "Google"]]}
{"text": "We briefly overview the five models compared in our link prediction and triple classification tasks . RESCAL ( Nickel et al , 2011 ) was one of the first knowledge graph embedding models . Although it is not often used as a baseline , Ruffinelli et al ( 2020 ) showed that it is competitive when appropriately tuned . RESCAL treats relational learning as tensor decomposition , scoring entity embeddings h , r R de and relation embeddings R R de\u00d7de with the bilinear form h Rt . P35 ) , headquarters location ( P159 ) , health specialty ( P1995 ) , indigenous to ( P2341 ) , industry ( P452 ) , influenced by ( P737 ) , instance of ( P31 ) , instrument ( P1303 ) , language of work or name ( P407 ) , languages spoken , written , or signed ( P1412 ) , legal form ( P1454 ) , legislative body ( P194 ) , located in the administrative terroritorial entity ( P131 ) , location of formation ( P740 ) , medical condition ( P1050 ) , medical examinations ( P923 ) , member of ( P463 ) , member of political party ( P102 ) , member of sports team ( P54 ) , mountain range ( P4552 ) , movement ( P135 ) , named after ( P138 ) , narrative location ( P840 ) , notable works ( P800 ) , occupant ( P466 ) , occupation ( P106 ) , official language ( P37 ) , parent organization ( P749 ) , part of ( P361 ) , place of birth ( P19 ) , place of burial ( P119 ) , place of death ( P20 ) , practiced by ( P3095 ) , product or material produced ( P1056 ) , publisher ( P123 ) , record label ( P264 ) , regulated by ( P3719 ) , religion ( P140 ) , residence ( P551 ) , shares border with ( P47 ) , sibling ( P3373 ) , sport ( P641 ) , spouse ( P26 ) , studies ( P2578 ) , subclass of ( P279 ) , symptoms ( P780 ) , time period ( P2348 ) , tributary ( P974 ) , unmarried partner ( P451 ) , use ( P366 ) , uses ( P2283 ) TransE ( Bordes et al , 2013 ) treats relations as translations between entities , i.e. , h + r \u2248 t for h , r , t R de , and scores embeddings with negative Euclidean distance \u2212 h + r \u2212 t . TransE is likely the most popular baseline for KGC tasks and the most influential of all KGC embedding papers . ComplEx ( Trouillon et al , 2016 ) uses a bilinear function to score triples with a diagonal relation embedding matrix and complex - valued embeddings . Its scoring function is re h diag ( r ) t , where t is the complex conjugate of t and re denotes the real part of a complex number . ConvE ( Dettmers et al , 2018 ) is one of the first and most popular nonlinear models for KGC . It concatenates head and relation embeddings h and r into a two - dimensional \" image \" , applies a pointwise linearity over convolutional and fullyconnected layers , and multiplies the result with the tail embedding t to obtain a score . Formally , its scoring function is given as f ( vec ( f ( [ h ; r ] * \u03c9 ) ) W ) t , where f is a nonlinearity ( originally , ReLU ) , [ h ; r ] denotes a concatenation and twodimensional reshaping of the head and relation embeddings , \u03c9 denotes the filters of the convolutional layer , and vec denotes the flattening of a two - dimensional matrix . TuckER ( Balazevic et al , 2019b ) is a linear model based on the Tucker tensor decomposition , which factorizes a tensor into three lower - rank matrices and a core tensor . The TuckER scoring function for a single triple ( h , r , t ) is given as W \u00d7 1 h \u00d7 2 r \u00d7 3 t , where W is the mode - three core tensor that is shared among all entity and relation embeddings , and \u00d7 n denotes the tensor product along the nth mode of the tensor . TuckER can be seen as a generalized form of other linear KGC embedding models like RESCAL and ComplEx .", "entities": [[9, 11, "TaskName", "link prediction"], [12, 14, "TaskName", "triple classification"], [16, 17, "MethodName", "RESCAL"], [29, 32, "TaskName", "knowledge graph embedding"], [59, 60, "MethodName", "RESCAL"], [68, 70, "TaskName", "entity embeddings"], [397, 398, "MethodName", "TransE"], [442, 443, "MethodName", "TransE"], [618, 619, "MethodName", "ReLU"], [660, 661, "MethodName", "TuckER"], [695, 696, "MethodName", "TuckER"], [757, 758, "MethodName", "TuckER"], [772, 773, "MethodName", "RESCAL"]]}
{"text": "Traditional IE systems assume abundant human annotations for training high quality machine learning models , which is impractical when trying to deploy IE systems to a broad range of domains , settings and languages . In the first part of the tutorial , we introduce how to extract structured facts ( i.e. , entities and their relations of different types ) from text corpora to construct knowledge bases , with a focus on methods that are minimally - supervised and domain - independent for timely knowledge base construction across various application domains . In the second part , we introduce how to leverage other knowledge , such as the distributional statistics of characters and words , the annotations for other tasks and other domains , and the linguistics and problem structures , to combat the problem of inadequate supervision , and conduct low - resource information extraction . In the third part , we describe recent advances in knowledge base reasoning . We start with the gentle introduction to the literature , focusing on pathbased and embedding based methods . We then describe DeepPath , a recent attempt of using deep reinforcement learning to combine the best of both worlds for knowledge base reasoning .", "entities": [[174, 175, "DatasetName", "pathbased"]]}
{"text": "The success of data mining and artificial intelligence technology is largely attributed to the efficient and effective analysis of structured data . The construction of a well - structured , machine - actionable knowledge base ( KB ) from raw ( unstructured or loosely - structured ) data sources is often the premise of consequent applications . Although the majority of existing data generated in our society is unstructured , big data leads to big opportunities to uncover structures of real - world entities ( e.g. , person , product ) , attributes ( e.g. , age , weight ) , relations ( e.g. , employee of , manufacture ) from massive text corpora . By integrating these semantic structures , one can construct a powerful KB as a conceptual abstraction of the original corpus . The constructed knowledge base will facilitate browsing information and inferring knowledge that are otherwise widely scattered in the text corpora . Computational machines can effectively perform algorithmic analysis at a large scale over these KBs , and apply the new insights to improve human productivity in various downstream tasks . Our Focus . In this tutorial , we focus our discussion on two tightly related problems : automatic construction of knowledge bases from text , and knowledge reasoning for knowledge base completion . While traditional information extraction techniques have heavy reliance on human - annotated data , our tutorial will devote more time on introducing methods that can reduce human efforts in the process , by leveraging external knowledge sources ( e.g. , distant supervision ) and exploiting rich data redundancy in massive text corpora ( e.g. , weak supervision ) . We also discuss how data sources from various domains and languages could opens up tremendous opportunities to leverage and transfer existing knowledge about domains , tasks and language , and help knowledge extraction in low - resource settings with minimal supervision . In the reasoning part , we aim to leverage the existing background knowledge and design various algorithms to fill in the missing link between entities in the KB , given the extracted KBs are likely incomplete . More specifically , this part will introduce two lines of research for KB reasoning : path - based and embedding - based methods . Topics to be covered in this tutorial . The first 2/3 of this tutorial presents a comprehensive overview of the information extraction techniques developed in recent years for constructing knowledge bases ( see also Section 2 for a more detailed outline ) . We will discuss the following key issues : ( 1 ) data - driven approaches for mining quality phrases from massive , unstructured text corpora ; ( 2 ) entity recognition and typing : preliminaries , challenges , and methodologies ; and ( 3 ) relation extraction : previous efforts , limitations , recent progress , and a joint entity and relation extraction method using distant supervision ; ( 4 ) multi - task and multi - domain learning for lowresource information extraction ; ( 5 ) distill linguistic knowledge into neural models to help low - resource information extraction . The second half of the tutorial presents a comprehensive overview of KB reasoning techniques . For path - based methods , we will first describe the Path - Ranking Algorithm ( PRA ) ( Lao et al , 2011 ) and briefly describe extensions such as ProPPR . Our tutorial will also cover the recent integration of PRA with recurrent neural networks . For the embedding based method , we will briefly describe RESCAL ( Nickel et al , 2011 ) and TransE ( Bordes et al , 2013 ) . Finally , we discuss DeepPath ( Xiong et al , 2017 ) , a novel deep reinforcement learning model that combines the embedding and path - based approaches for the learning to reason problem . Research Impact . Our phrase mining tool , SegPhrase ( Liu et al , 2015 ) , won the grand prize of Yelp Dataset Challenge 1 and was used by TripAdvisor in productions 2 . Our entity recognition and typing system , ClusType , was shipped as part of the products in Microsoft Bing and U.S. Army Research Lab . We built the first named entity recognizer on Chinese social media Dredze , 2015 , 2016 ) and closed the gap between NER on English and Chinese social media . The same technique was applied to build the first relation extractor for cross - sentence , n - ary relation extraction between drug , gene , and mutation ( Peng et al , 2017 ) . Duration and Sessions . The duration of the tutorial is flexible : It is expected to be 3 hours , but it can be extended into 6 hours , based on the need of the conference . The outline presented here is for the 3 - hour tutorial . For longer duration of the tutorial , we plan to extend entity and relation extraction parts , and add in more case studies and applications . Relevance to ACL . Machine \" reading \" and \" reasoning \" of large text corpora have long been the interests to CL and NLP communities , especially when people now are exposed to an explosion of information in the form of free text . Extracting structured information is key to understanding messy and scattered raw data , and effective reasoning tools are critical for the use of KBs in downstream tasks like QA . This tutorial will present an organized picture of recent research on knowledge base construction and reasoning . We will show how exciting and surprising knowledge can be discovered from your own not so well - structured raw corpora , and such incomplete KBs can be further used to derive new insights and more complex knowledge with reasoning techniques .", "entities": [[215, 218, "TaskName", "knowledge base completion"], [469, 471, "TaskName", "relation extraction"], [482, 487, "TaskName", "joint entity and relation extraction"], [598, 599, "MethodName", "RESCAL"], [607, 608, "MethodName", "TransE"], [733, 734, "TaskName", "NER"], [760, 762, "TaskName", "relation extraction"], [839, 841, "TaskName", "relation extraction"]]}
{"text": "Most of the previous tutorials focused exclusively on the knowledge base construction aspect . In the proposed tutorial , we will give a systematic discussion on the problem of knowledge base reasoning , for which extensive studies have been conducted recently but systematic tutorials are lacking . This tutorial also presents recent advances in applying distant and weak supervision to the extraction of structured facts in knowledge base construction , in addition to the traditional supervised techniques and rule - based approaches . Target audience and prerequisites . Researchers and practitioners in the field of natural language processing , computational linguistic , text mining , information retrieval , semantic web and machine learning . While the audience with a good background in these areas would benefit most from this tutorial , we believe the material to be presented would give general audience and newcomers an introductory pointer to the current work and important research topics in this field , and inspire them to learn more . Only preliminary knowledge about NLP , algorithms and their applications are needed . We expect there will be around 70 people interested in our tutorial . Tutorial material and equipment . We will provide attendees a website and upload our tutorial materials ( slides , references , softwares ) . There is no copyright issue . Standard equipment will be enough for our tutorial .", "entities": [[105, 107, "TaskName", "information retrieval"]]}
