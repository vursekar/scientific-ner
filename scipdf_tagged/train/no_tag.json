{"text": "Total number train 16010 development 1999 test 2001 tion of computational complexity , shallow learning models generally show better performance than deep learning models . Therefore , some researchers have studied the design of shallow models in specific areas of data replacement . Deep learning consists of multiple hidden layers in a neural network ( Aroyehun and Gelbukh , 2018 ) , has higher complexity , and can be trained on unstructured data . The deep learning architecture can directly learn feature representations from the input without excessive manual intervention and prior knowledge . However , deep learning technology is a data - driven method that usually requires a lot of data to achieve high performance . And the self - attention - based model can bring some interword interpretability to DNN , but the comparison with the shallow model does not explain why and how it works .", "entities": []}
{"text": "This is a comment / post level classification task . Given a Youtube comment ( Chakravarthi et al , 2020bChakravarthi and Muralidaran , 2021 ) , the system has to classify it into one of the five categories mentioned in the Abstract section . For this task , the available sentences including 16010 training sentences , 1999 development sentences , and 2001 testing sentences . The label distribution is very uneven ( Not - offensive label accounts 88.4 % . The label with the second largest number is not - malayalam , which accounts for only 0.08 % of the total . And there are relatively fewer labels in other categories . ) The number of sentences for each domain is listed in Table 1 .", "entities": []}
{"text": "The output of the classification result is shown in is zero . N ot \u2212 Of f ensive labels account for the majority , accounting for 91.15 % of the total number of labels . The N ot \u2212 M alayalam labels account for the second most significant 7.5 % of the total . Offensive - Untargeted labels are the least , only about 1 % . This may be due to data imbalance ( N ot \u2212 Of f ensive labels in the training set account for about 88 % of the total ) resulting in only three categories being identified .", "entities": []}
{"text": "Fast and Accurate Entity Recognition with Iterated Dilated Convolutions", "entities": []}
{"text": "We present iterated dilated convolutional neural networks , fast token encoders that efficiently aggregate broad context without losing resolution . These provide impressive speed improvements for sequence labeling , particularly when processing entire documents at a time . In the future we hope to extend this work to NLP tasks with richer structured output , such as parsing .", "entities": []}
{"text": "Corrected Examples Starting from a DP - based solution to the [ traveling salesman problem ] Method , we present a novel technique ...", "entities": []}
{"text": "Based on the correction contributed by ( Wang et al , 2019 ) , we use the proposed method to justify label inconsistency though the label mistakes take \" only \" 5.38 % . It also validates the label consistency after recovery . Figure 3 ( a ) shows that starting with the wrong labels in the original test set makes the performance worse than starting with the training set or the good test subset . After label correction , this issue is fixed in Figure 3 ( b ) .", "entities": []}
{"text": "This work is supported by National Science Foundation IIS - 1849816 and CCF - 1901059 .", "entities": []}
{"text": "In this section , we compare our approach to various sentence simplification models using both automatic and manual evaluations . We show that our model achieves a new state - of - the - art and can adapt easily to different simplification styles , such as paraphrasing and splitting without deletion .", "entities": []}
{"text": "In this section , we analyze the contribution of each model component and examine the system errors .", "entities": []}
{"text": "The Seattle kids petitioned Washington state last year to adopt stricter science - based regulations to protect them against climate change .", "entities": []}
{"text": "The eight kids from Seattle who petitioned Washington state last year are fighting for the state to adopt stricter laws to protect their future against climate change .", "entities": []}
{"text": "The Seattle kids asked the Washington state last year to take tougher rules against climate change . Hallucinations ( 13 % ) Complex \" It 's more of a family than living outside , \" said Jessica Konczal , 33 , whose husband is Sgt . Matthew Konczal .", "entities": []}
{"text": "Jessica Konczal , age 33 , says the base is its own type of family . Our Model \" It 's more of a family than living outside , \" said Jessica Konczal , one of the protesters .", "entities": []}
{"text": "Parental feedback on the menu additions so far , from some of the early adopter markets , has been \" phenomenal , \" Leverton said .", "entities": []}
{"text": "Leverton said parents love the new food .", "entities": []}
{"text": "So far , parents parents have feedback on the menu changes has been a great deal , he added .", "entities": []}
{"text": "One of the device 's inventors explained to the president that the machine was a prototype .", "entities": []}
{"text": "One of the girls explained it was just a prototype .", "entities": []}
{"text": "One of the inventors told the president that the first design was a design . Anaphora Resolution / Entailment ( 12 % ) Complex Sea slugs dubbed sacoglossans are some of the most remarkable biological burglars on the planet .", "entities": []}
{"text": "The sea slugs known as sacoglossans are some of the most interesting burglars on the planet .", "entities": []}
{"text": "Scientists say these are some of the most interesting creatures on the planet . Human Reference Errors ( 4 % ) Complex They float in and out of formations that combine the underwater world with the stage .", "entities": []}
{"text": "In the dance on a round stage on the seabed , the dancers swim together and apart .", "entities": []}
{"text": "Experts say China 's air pollution exacts a tremendous toll on human health .", "entities": []}
{"text": "This year , the FAA has approved dozens of permits for agricultural drone businesses .", "entities": []}
{"text": "this year , the government has approved dozens of permits for drone businesses for no permission . Our Model ( cp = 0.6 ) this year , the faa has allowed many businesses to use drones . Our Model ( cp = 0.7 , 0.8 ) this year , the faa has approved dozens of permits for drones .", "entities": []}
{"text": "The room echoed with the sounds of song , the beat of drums , the voices of young men .", "entities": []}
{"text": "LearningToQuestion at SemEval 2017 Task 3 : Ranking Similar Questions by Learning to Rank Using Rich Features", "entities": []}
{"text": "These set of features represent the lexical similarity between question texts . The lexical used are the common n - gram ( n = 1 , 2 , 3 ) counts between the original question and a candidate question . Apart from these features , we compute a count vector and a tfidf vector for n - grams ( n = 1 , 2 , 3 ) for both the question and candidate question and compute the cosine similarity between them .", "entities": []}
{"text": "These features represent the syntactical similarity between the texts of questions . This is represented by cosine similarity of POS count vector for ngram ( n = 1 , 2 , 3 ) .", "entities": []}
{"text": "We compute the heuristic based on length in tokens of both the texts as f ( l 1 , l 2 ) = abs ( l 1 \u2212l 2 ) l 1 + l 2 .", "entities": []}
{"text": "We use count of 10 selected common question words also as a feature in the system . All of the above features are calculated for both the question subject and body separately .", "entities": []}
{"text": "A Appendix", "entities": []}
{"text": "Previous comprehension question datasets focused on either inferential or literal questions . Although these questions assess comprehension skills , they do not provide fine - grained evaluation of the reader comprehension . Thus , to build a more comprehensive list of question types , we started by reviewing curriculum documents available from Columbia University Teacher 's College Readers 5 and Writers Workshop Program 6 . Then , we compiled a list of SBRCS , which we then expanded to include additional skills based on school teachers ' recommendations . In Section A.1 , we present further details for each skill type . Also , in Appendix A.2 , we give further details on the skills list and on the educational theory behind the skills taxonomy . Our final list contains the following skills : 1 . Basic Story Elements ( BSE ) : Can the reader identify the story 's main characters and setting ? From the details in this passage , how many individuals were part of this investigation ? 2 . Character Traits ( CT ) : Can the reader identify the traits attributable to certain characters in the story ( e.g. character feelings , physical attributes ) ? How did the Rabbit feel in this passage ? Which word in the passage is a synonym for \" stubborn \" ? With our list of SBRCS as a guide , we wrote question - answer pairs for each story . Given the difficulty of the task , we needed a large number of trained content writers to build the required questions . Each written question should fall into one of the mentioned skills . For that , a total of 25 professionals contributed to the writing process ( 18 teachers , 7 graduate students ) . Each annotator was asked to write a question per skill for a given story . Not every skill is applicable to every story , so some skills were discarded for some stories . We chose not to use crowdworkers ( e.g. Amazon Mechanical Turk ) to ensure high - quality and educationally - appropriate questions . To verify the quality of the generated content , a second team member reviews each question - answer pair before adding them to the dataset . If the second team member found issues , a discussion took place . In the cases that the team members could not reach an agreement , a third team member is brought in to resolve the disagreement . In addition to annotating questions with a skills label , our content writers annotate each question as either Literal or Inferential question types . This information is important to measure the comprehension performance of the reader on each question type . Overall , we generate 4 K question - answer pairs , with an average of 5.5 pairs per story . Note that we did not ask multiple annotators to write questions per story in order to measure the annotators ' agreement . Different annotators often write the same question in different ways , or may choose a different question topic for a given skill , or even select a different skill . Thus , measuring inter - annotator agreement is not meaningful . Instead , we chose to ask one annotator to write questions and another to validate the questions grammatically and to check whether the question is correctly related to the chosen skill .", "entities": []}
{"text": "Decoding strategies are crucial and directly impact output quality . In general , Beam Search ( Reddy , 1977 ) is the most common algorithm , in addition to some other sampling techniques such as Nucleus sampling ( Top - p ) ( Holtzman et al , 2019 ) . In Beam Search , the output of a model is found by maximizing the model probability . On the other hand , Nucleus sampling selects the smallest possible set of tokens whose cumulative probability exceeds the probability p. Experimentally , we found that using the top - p ( p=0.9 ) algorithm yields the best results in terms of the used scoring metrics , thus we use it in all of our experiments .", "entities": []}
{"text": "Determining what are the main story elements is one of the comprehension skills to assess the reader understanding . Using this skill , we can understand whether the reader is able to identify the main characters and environment settings of the stories . 2 . Character Traits ( CT ) : Identifying permanent traits that can be assigned to characters or describe character development . For instance , knowing what most likely X character felt during the story , recognizing facts about X , identifying main adjectives that X has , etc .", "entities": []}
{"text": "Figurative language is common in stories as it makes ideas and concepts easier to visualize by the reader . Also , it is an effective way of conveying an idea that is not easily understood . With this skill , we examine the reader ability of recognizing the implicated meaning of a sentence or a type of figurative language . 5 . Inferring ( I ) : Writers sometimes jump into the action or skip forward in their stories . Good readers must infer what happened in between scenes if the time in - between is not explicitly detailed . In addition , readers must infer their characters ' emotions if their characters do not share those aloud .", "entities": []}
{"text": "Consolidating a text into a precise synopsis of only the most key information . Summarizing skill contains the main literary elements of the characters , the problem , and the solutions . Key events from the beginning , middle , and end are included in a summary . 8 . Visualizing ( V ) : This skill requires readers to visualize scenes in their heads to fully comprehend the story . It can assess readers ability of imagining specific events or elements in the stories .", "entities": []}
{"text": "Identifying the meaning of unfamiliar words in the text is a key skill for readers to fully comprehend the story . In this skill , the reader should identify the right meaning of a word within a context when the word has multiple possible definitions . Additionally , the reader should be able to identify vocabulary based questions related to identifying synonyms , antonyms , homophones , compound words , and word types ( e.g. noun , verb , etc . ) .", "entities": []}
{"text": "In Table 6 , we show the fined - grained results per skill name after the manual labeling experiment for the generated questions from both One - Step and HTA - WTA models .", "entities": []}
{"text": "In this section , we list some random examples from HTA - WTA model for inferential questions : Story : \" The Line 1 Toronto train was a subway like many others you 've seen . He rocketed down Yonge Street , around the Union loop , and rattled off towards Vaughn . At Vaughn he 'd let out a loud , hissing sigh and a clanking sort of grunt , then reverse and do the whole thing backwards all over again . He liked his transit union job well enough , but he could n't help thinking about the lights at the end of his tunnels . No matter how long he 'd been running , or how much he wished for anything else , that little hopeful point of light always turned out to be just one more dirty subway platform . \" Generated Figurative Language question : \" Reread this sentence : \" He rocketed down Yonge Street , around the Union loop , and rattled off towards Vaughn . \" Which figurative language technique is being used here ? \" Generated answer : \" Alliteration \" . Story : \" \" The map says left \" , said Bri . \" But my heart says right ! \" cried Rob . \" Is your heart full of hidden treasure ? \" asked Bri . \" Yes . \" Rob replied . \" At least , that 's what my mom says . \" \" Generated Inferring question : \" Why do you think Bri 's heart says \" But my heart says right ! \" ? \" Generated answer : \" Because she thinks she has found something \" . Story : \" Mary looked at it , not really knowing why the hole was there , and as she looked she saw something almost buried in the newlyturned soil . It was something like a ring of rusty iron or brass and when the robin flew up into a tree nearby she put out her hand and picked the ring up . It was more than a ring , however ; it was an old key which looked as if it had been buried a long time . Mistress Mary stood up and looked at it with an almost frightened face as it hung from her finger . \" Perhaps it has been buried for ten years , \" she said in a whisper . \" Perhaps it is the key to the garden ! \" \" Generated Vocabulary question : \" Reread this sentence : \" Perhaps it has been buried for ten years \" What is the correct definition of the word \" frightened \" as it is used here ? \" Generated answer : \" Scared \" .", "entities": []}
{"text": "ADVISER : A Toolkit for Developing Multi - modal , Multi - domain and Socially - engaged Conversational Agents", "entities": []}
{"text": "We extend and substantially modify our previous , text - based dialog system toolkit ( Ortega et al , 2019 ) while following the same design choices . This means that our toolkit is meant to optimize the following four criteria : Modularity , Flexibility , Transparency and User - friendliness at different levels . This is accomplished by decomposing the dialog system into independent modules ( services ) , which in turn are either rule - based , machine learning - based or both . These services can easily be combined in different orders / architectures , providing users with flexible options to design new dialog architectures .", "entities": []}
{"text": "ADVISER supports three options to access information from external information sources . In addition to being able to query information from SQL - based databases , we add two new options that includes querying information via APIs and from knowledge bases ( e.g. Wikidata ( Vrande\u010di\u0107 and Kr\u00f6tzsch , 2014 ) ) . For example , when a user asks a simple question - Where was Dirk Nowitzki born ? , our pretrained neural network predicts the topic entity - Dirk Nowitzki - and the relation - place of birth . Then , the answer is automatically looked up using Wikidata 's SPARQL endpoint .", "entities": []}
{"text": "To allow for maximum flexibility in combining and reusing components , we consider a dialog system as a group of services which communicate asynchronously by publishing / subscribing to certain topics . A service is called as soon as at least one message for all its subscribed topics is received and may additionally publish to one or more topics . Services can elect to receive the most recent message for a topic ( e.g. up - to - date belief state ) or a list of all messages for that topic since the last service call ( e.g. a list of video frames ) . Constructing a dialog system in this way allows us to break free from a pipeline architecture . Each step in the dialog process is represented by one or more services which can operate in parallel or sequentially . For example , tasks like video and speech capture may be performed and processed in parallel before being synchronized by a user state tracking module subscribing to input from both sources . Figure 2 illustrates the system architecture . For debugging purposes , we provide a utility to draw the dialog graph , showing the information flow between services , including remote services , and any inconsistencies in publish / subscribe connections .", "entities": []}
{"text": "Services are location - transparent and may thus be distributed across multiple machines . A central dialog system discovers local and remote services and provides synchronization guarantees for dialog initialization and termination . Distribution of services enables , for instance , a more powerful computer to handle tasks such as real - time text - to - speech generation ( see Figure 2 ) . This is particularly helpful when multiple resource - heavy tasks are combined into a single dialog system .", "entities": []}
{"text": "In addition to providing multi - modal support , the publish / subscribe framework also allows for multidomain support by providing a structure which enables arbitrary branching and rejoining of graph structures . When a service is created , users simply specify which domain ( s ) it should publish / subscribe to . This , in combination with a domain tracking service , allows for seamless integration of domain - agnostic services ( such as speech input / output ) and domain - specific services ( such as NLU / NLG for the lecturers domain ) .", "entities": []}
{"text": "We provide several example domains to demonstrate ADVISER 's functionalities . Databases for lecturers and courses at the Institute for Natural Language Processing ( IMS ) , which we used in the previous version of ADVISER , were adapted to the new system architecture . As example APIs , we implemented a weather domain that makes calls to the OpenWeatherMap API 3 and a mensa domain for gathering information from the dining hall at the university of Stuttgart . Note that affective templates were only added to the lecturers and mensa domain . All domains can be used within the same dialog , simply by switching the topic .", "entities": []}
{"text": "We introduce ADVISER - an open - source , multidomain dialog system toolkit that allows users to easily develop multi - modal and socially - engaged conversational agents . We provide a large variety of functionalities , ranging from speech processing to core dialog system capabilities and social signal processing . With this toolkit , we hope to provide a flexible platform for collaborative research in multi - domain , multi - modal , socially - engaged conversational agents .", "entities": []}
{"text": "Linked Open Treebanks . Interlinking Syntactically Annotated Corpora in the LiLa Knowledge Base of Linguistic Resources for Latin", "entities": []}
{"text": "In spite of the current availability of large collections of treebanks that can be used and queried from one common place on the web , we are still far from achieving a real interconnection , both between treebanks themselves and with other ( kinds of ) linguistic resources . However , making resources interoperable is a crucial requirement to maximize the contribution of each single resource , as well as to account for the linguistic complexity of the texts provided by ( annotated ) corpora and particularly by treebanks . This paper describes how dependency treebanks are interlinked in a Knowledge Base of linguistic resources for Latin based on Linked Open Data practices and standards . The Knowledge base is built to make linguistic resources interact by integrating all types of annotation applied to a particular word / text into a common representation .", "entities": []}
{"text": "In this section , we discuss how we integrated the Latin treebanks into the LiLa Knowledge Base and how the linked data obtained by connecting the treebank tokens to the other resources support complex queries crossing through different linguistic resources .", "entities": []}
{"text": "In this paper , we have described how we interlinked three dependency treebanks for Latin ( one available in two versions ) into a Knowledge Base of linguistic resources based on Linked Open Data practices and standards . Linking resources of different kind ( such as corpora and lexica ) makes it possible to exploit their potential to the best . Indeed , single resources tend to focus on a limited set of linguistic features ( e.g. morphology and syntax for treebanks ) , which are in most cases insufficient to provide a full analysis of the textual or lexical data . Making interoperable the still scattered and unconnected resources that are currently available for Latin ( as well as for many other languages ) is a way to approach the data from the various layers of annotation that such resources provide . Our work of interlinking the linguistic resources for Latin has just begun . In the near future , we plan to integrate into the LiLa Knowledge Base two other lexical resources , namely an etymological dictionary ( de Vaan , 2008 ) and the Latin WordNet . Interlinking these resources with the textual occurrences of their lemmas ( enriched with syntactic annotation in treebanks ) will enable the users of LiLa to run complex queries crossing different kinds of linguistic features . Given that the set of interlinked resources will grow in the coming years , the chain of connection can be continued indefinitely ; as long as new lexical resources are connected to the Knowledge Base , all the connections from any corpus token to their nodes will become explorable in the network .", "entities": []}
{"text": "This project has received funding from the European Research Council ( ERC ) under the European Union 's Horizon 2020 research and innovation programme - Grant Agreement No 769994 .", "entities": []}
{"text": "Original : all the employees are friendly and helpful . Transferred : all the employees are rude and unfriendly . Original : i ' m so lucky to have found this place ! Transferred : i ' m so embarrassed that i picked this place .", "entities": []}
{"text": "Compute Q t via ( 5 ) , and update \u03c0 \u03c6 with policy gradient via ( 8 ) .", "entities": []}
{"text": "We use the same initial state for both the generator and the guider networks . For conditional generation , the initial state is the encoded latent code of the conditional information for both training and testing . For unconditional generation , the initial state is the encoded latent code of a target sentence in training and random noise in testing .", "entities": []}
{"text": "Compute evaluation scores based on references .", "entities": []}
{"text": "Compute Q s t via ( 6 ) , and update \u03c0 \u03c6 with policy gradient via ( 8 ) . 7 : until GMST converges", "entities": []}
{"text": "Generated Examples Real Data What this group does is to take down various different websites it believes to be criminal and leading to terrorist acts . Over 1 , 600 a day have reached Greece this month , a higher rate than last July when the crisis was already in full swing . \" We ' re working through a legacy period , with legacy products that are 10 or 20 years old , \" he says . ' The first time anyone says you need help , I ' m on the defensive , but that ' s all that I know . Out of those who came last year , 69 per cent were men , 18 per cent were children and just 13 per cent were women . He has not played for Tottenham ' s first team since and it is now nearly two years since he completed a full Premier League match for the club . So you have this man who seems to represent this way to live and how to be a good citizen of the world . CNN : You made that promise , but it wasn ' t until 45 years later that you acted on it . This is a part of the population that is notorious for its lack of interest in actually showing up when the political process takes place . They picked him off three times and kept him out of the end zone in a 22 - 6 victory at Arizona in 2013 . The treatment was going to cost \u00a3 12 , 000 , but it was worth it for the chance to be a mum . But if black political power is so important , why hasn ' t it made more of a difference in the lives of poor black people in Baltimore such as Gray ? Local media reported the group were not looking to hurt anybody , but they would not rule out violence if police tried to remove them . The idea was that couples got six months ' leave per child with each parent entitled to half the days each . The 55 to 43 vote was largely split down party lines and fell short of the 60 votes needed for the bill to advance . Taiwan ' s Defence Ministry said it was \" aware of the information , \" and declined further immediate comment , Reuters reported . I ' m racing against a guy who I lost a medal to - but am I ever going to get that medal back ? Others pushed back their trips , meaning flights early this week are likely to be even more packed than usual . \" In theory there ' s a lot to like , \" Clinton said , \" but ' in theory ' isn ' t enough . If he makes it to the next election he ' ll lose , but the other three would have lost just as much .", "entities": []}
{"text": "A general trend in machine learning research is to mathematically model the input - output relationship from a dataset . This is carried out by quantitatively estimating the set of model parameters that best fit the data . The approach warrants prior ( to fitting ) examination of the following aspects : The sufficiency of the informative data to the estimate model parameters , i.e. , practical identifiability . Thus , the limitation comes from the dataset quality or quantity and may lead to ambiguous data interpretations ( Raue et al , 2009 ) . The possibility that the structure of the model allows its parameters to be uniquely estimated , irrespective of the quality or quantity of the available data . This aspect is called structural identifiability . A model is said to be structurally unidentifiable if a different set of parameters yield the same outcome . In this work , we focus on the structural identifiability ( Bellman and\u00c5str\u00f6m , 1970 ) . It is noteworthy that the goodness of the fit of a model on the data does not dictate its structural identifiability . Similar to Brunner et al ( 2019 ) , we focus our analysis on the identifiability of attention weights , which are not model parameters , yet demands meaningful interpretations and are crucial to the stability of representations learned by the model .", "entities": []}
{"text": "A.1 Span , Column space and Row space Given a set of vectors V : = { v 1 , v 2 , . . . , v n } , the span of V , span ( V ) , is defined as the set obtained from all the possible linear combination of vectors in V , i.e. , span ( V ) : = { n i=1 \u03bb i v i | \u03bb i R , i { 1 , 2 , . . . , n } } . The span ( V ) can also be seen as the smallest vector space that contains the set V. Given a matrix A R m\u00d7n , the column space of A , Cs ( A ) , is defined as space spanned by its column vectors . Similarly , the row space of A , Rs ( A ) , is the space spanned by the row vectors of A. Cs ( A ) and Rs ( A ) are the subspaces of the real spaces R m and R n , respectively . If the row vectors of A are linearly independent , the Rs ( A ) will span R m . A similar argument holds between Cs ( A ) and R n .", "entities": []}
{"text": "The rank of a matrix P ( denoted as rank ( P ) ) tells about the dimensions of the space spanned by the row vectors or column vectors . It can also be seen as the number of linearly independent rows or columns . The following properties hold rank P \u2264 min m p , n p rank P Q \u2264 min rank ( P ) , rank ( Q ) . Where , P and Q are m p \u00d7 n p and m q \u00d7 n q dimensional matrices , respectively .", "entities": []}
{"text": "This research is supported by A*STAR under its RIE 2020 Advanced Manufacturing and Engineering programmatic grant , Award No . - A19E2b0098 .", "entities": []}
{"text": "Existing large - scale QA data sets can be categorized based on their context passage length in two groups : short - context QA , i.e. , data sets with paragraph - level context , and long - context QA , i.e. , data sets with multiple - paragraph or documentlevel context . Long - context QA can potentially include questions demanding long answers . In this section , we only review QA datasets . However , it is worth noting that very recently , ( Tay et al , 2020a ) introduced a unified benchmark using different tasks for evaluating model quality under long - context scenarios .", "entities": []}
{"text": "NLQuAD consists of news articles as context documents , interrogative sub - headings in the articles as questions , and body paragraphs corresponding to the sub - headings as contiguous answers to the questions . We automatically extract target answers because annotating for non - factoid long QA is rather challenging and costly . To ensure the qual - ity of answers in addition to the initial investigations , we perform human evaluations ( Section 5.3 ) . We choose the BBC news website as the resource of our documents and the question - answer pairs , mainly because its articles contain a considerable amount of high - quality question - like sub - headings which are suitable for the QA task . NLQuAD 's characteristics make it an appealing and challenging data set for the non - factoid long QA task : Its context documents are long , and its questions are non - factoid in a way that can not be answered by single or multiple entities . The questions are addressed by more than seven sentences on average . Meanwhile , it covers a wide range of topics , making it an open - domain QA data set . The BBC news articles typically follow a specific template . They begin with an introductory section consisting of news summaries ( Narayan et al , 2018 ) and one or more sections accompanied by sub - headings . Each section contains multiple short to medium - length paragraphs . We remove the template and section break - lines to prevent revealing possible answer boundaries .", "entities": []}
{"text": "We exploit Wayback Machine , 2 a digital archive of the Web , and Wayback Machine Scraper 3 to scrape the article archives . Links in the scraped pages are used to collect additional pages from the original website . We scraped the English BBC news website from 2016 to 2020 as a limited number of questions can be found in articles before 2016 . Only textual information is kept and we strip away multimedia objects and hyperlinks outside of the body of the articles . Duplicate documents are removed and questions with bullet list answer types are discarded . We detect interrogative sub - headings by checking if they end with a question mark .", "entities": []}
{"text": "NLQuAD contains 31k non - factoid questions based on 13k supporting documents from news articles . Table 2 shows the data set statistics . We randomly partition the data set into training ( 80 % ) , development ( 10 % ) , and evaluation ( 10 % ) sets . While NLQuAD has long documents and longanswer QA pairs , the histograms in Figure 2 indicate the wide range of samples . Figure 3 We manually investigated 100 randomly sampled question - answer pairs from the NLQuAD training set and find that 87 % of the questions are not self - contained and require additional contextual information to be understood or disambiguated . Most of the answers consist of explanations , descriptions , or opinions , and only 2 % of the questions can be answered by a short span of text .", "entities": []}
{"text": "This research was partly supported by VIVAT . We thank the BBC for giving permission to publish our extracted data for non - commercial , research purposes . We also thank our volunteers for providing human assessments .", "entities": []}
{"text": "Interpreting Emoji with Emoji : \u21d2", "entities": []}
{"text": "We explain next our deployed tool for creating interpretable word - emoji embeddings : PO - LAR ( Mathew et al , 2020 ) ; and provide detail on a revised POLAR extension via projection .", "entities": []}
{"text": "\u2212 E v = ( dir T ) \u22121 \u2212 W a v yielding an interpretable subspace along the differentials \u2212 \u2212 dir i that carries over specific geometric semantics from the input embedding . I.e. , for each word v V within the resulting interpretable embedding E , its embedding vector \u2212 E v now carries a measure along each polar dimension 's semantics . Limitations . Polar opposites being very close in the original embedding space might tear apart . From a technical perspective , the used pseudo inverse for the base change becomes numerically ill - conditioned if d \u2248 N ( Mathew et al , 2020 ) .", "entities": []}
{"text": "While the base change approach seems natural , its given limitations lead us to propose a variant that comes with several benefits . Instead of creating a new interpretable vector space , we take measurements on the differentials dir defined as before ( Fig . 2a , red dashed vectors ) . However , we now project each embedding vector \u2212 W v for v orthogonally onto the differentials as shown in Fig . 2b ( blue dotted vectors ) . This leads to a smallest distance between both lines w.r.t . the differential , yet simultaneously allows for a direct scale measure on the differential vector as shown in Fig . 2b & Fig . 2c ( green vectors ) . Thereby , we also decouple the transformation matrix , which eases later add - ins to the interpretable embedding . Orthogonal projection ( blue dotted vectors ) of each input embedding vector \u2212 W a v onto a differential i provides us the adjacent leg vector as follows : oproj dir i ( \u2212 W a v ) = \u2212 W a v \u2212 \u2212 dir i | \u2212 \u2212 dir i | scalar \u2212 \u2212 dir i | \u2212 \u2212 dir i | direction As this adjacent leg ( green vectors ) 's direction naturally equals the differential , we focus only on the scalar part representing a direct scale measure . By normalizing the differential vector length\u015d dir = dir | dir | \u22121 , the projected scale value conveniently results in : oproj scalar dir i ( \u2212 W a v ) = \u2212 W a v \u2212 \u2212 dir i . This transformation allows to create a new interpretable embedding E R | V | \u00d7N for each embedding vector \u2212 W a v ( exemplified in Fig . 1 ) as follows : \u2212 E v = oproj scalar dir ( \u2212 W a v ) = d ir T \u2212 W a v R N Computationally it requires an inital matrix multiplication for each embedded term ; Dimension increments require a dot product on each term . Downstream Tasks . Other experiments indicate POLAR \u03c1 downstream task performance being on par with the input embedding , and an edge over base change POLAR if d \u2248 N ( not shown ) .", "entities": []}
{"text": "POLAR \u03c1 can create interpretable embeddings w.r.t . a - priori provided opposites . We next describe how we select these opposites to make POLAR \u03c1 applicable to our data . Most importantly , the approach requires being part of or locating desired opposites within the original embedding space . Words . As we extend the word embedding space with emoji , we still want to use words . We find common sources of polar opposites in antonym wordlists ( Shwartz et al , 2017 ) as used in the original POLAR work . To fit our German dataset , we translated and manually checked all pairs keeping 1275 items . From GermaNet ( Hamp and Feldweg , 1997 ) , we extracted 1732 word pairs via antonym relations leading to | P words | = 1832 word pairs . Emoji . Being not ideal , but due to lack of better alternatives , we ended up heuristically creating semantic opposites from emoji through qualitative surveys across friends and colleagues resulting in | P emoji | = 44 emoji pairs , cf . Tab . 3 . While we could use far more opposites especially of facial emoji , due to emoji clustering in the input embedding , spanned expressive space would arguably become redundant at similar EWSO scores for many directions . Effectively it may bias interpretability over proportionally towards facial emoji .", "entities": []}
{"text": "While we have now created a supposedly interpretable embedding , it remains to be seen how well it is perceived by humans . That is , we next evaluate our two key RQs , discuss significance , and provide further details : RQ1 ) How well does POLAR \u03c1 with EWSO perform in selecting most interpretable dimensions at varying expressiveness of words and emoji ? RQ2 ) How well do POLAR \u03c1 scalar values reflect directions on the differential scales ? i ) Do humans prefer emoji to words ? ii ) How well do human raters align w.r.t . interpretability ? iii ) What impact do demographic factors play in interpretability with or without emoji ?", "entities": []}
{"text": "To gather human judgement , we employ crowdsourcing on the Microworkers platform .", "entities": []}
{"text": "Our evaluation of the POLAR \u03c1 approach including emoji to the differentials bases on two main questions next to demographics . Selection test . Analogous to the original work , we want to find out whether humans agree on best interpretability of POLAR \u03c1 selected differentials with a word intrusion task . The question asks our coders to select five out of ten differentials that describe a given word best as shown in Fig . 3b . We select half of these dimensions according to the highest absolute projection scale values ( most extreme ) . The other half consists of a random selection from the bottom half of available differentials . I.e. , if the projection approach determines interpretable dimensions well , humans would choose all five out of five POLAR \u03c1 chosen differentials . As any user might choose differently , we count how often coders choose certain differentials . The resulting frequencies immediately translate in a ranking that we leverage for calculating the fraction of Top 1 .. 5 being POLAR \u03c1 chosen differentials . Preference test . Additionally , we introduce the preference test evaluating whether the direction on a given differential scale is in line with human judgement . That is , for the same words from the selection test , we display the same ten dimensions ( 5 top - POLAR \u03c1 , 5 random bottom ) where coders select their interpretation of the given word on scales as shown in Fig . 3c . Typical for semantic differential scales ( Tullis and Albert , 2008 ; Osgood et al , 1957 ) , we deliberately use a seven point scale representing - 3 to 3 , allowing more freedom than 3 or 5 points ( Simms et al , 2019 ) . Further , we specifically allow a center point - being equal - as it might indicate both being equally well or not good at all . Due to scale usage heterogeneity ( Rossi et al , 2001 ) , we normalize coder chosen directions ( shift+scale according to mean ) prohibiting disproportional influence of single coders . We evaluate the coder agreement by counting direction ( sign ) non - /alignment with the POLAR \u03c1 projection scale . Demographics . There is a multitude of other external factors that might have impact on coders ' choices . To better understand participant back - ground , we ask for their education , emoji usage ( familiarity ) , smartphone platform ( different emoji pictograms ) , and if they had used Jodel before .", "entities": []}
{"text": "Crowdworker Campaigns We run a campaign for each of the cross product between words only , emoji only , and mixed Tab . 3a and Fig . 2 . ( W / W ) word / word sets a baseline comparison to results from the original POLAR work , albeit now using the projection approach . ( W / M ) : word / mixed uses not only words , but includes emoji to the POLAR subspace . ( W / E ) : word / emoji uses only emoji to describe words . ( E / W ) : emoji / word provides another baseline as to how well emoji may be interpreted with words only . ( E / M ) : emoji / mixed uses both , emoji and words to interpret emoji . ( E / E ) : emoji / emoji may be the most interesting as we only use the expressiveness of emoji to describe emoji . For mixed cases ( emoji and words within the PO - LAR subspace ) , we create rankings from absolute scale values on both types ( words / emoji ) separately and then select them equally often to achieve similar amounts of word and emoji differentials . Used Words & Emoji . We selected 50 words and emoji to be described in each campaign . To ensure that i ) we only use common words that are very likely known to our coders , and ii ) these words are captured well within the underlying embedding , we pick them out of the upper 25 % quantile by occurrences in the corpus ( n \u22651.6k ) . I.e. , we chose emoji and words that appear frequently and should therefore be well - known . For words , we ensured that they are part of the German dictionary Duden . Tasks Setup . Within our six campagins , we now have each 50 emoji or 50 words to be interpreted . We bundled this into 5 tasks each consisting of 10 emoji / words - resulting in 30 different tasks . Each of these tasks contains the Selection test , Preference test , and demographics . Subjects . Human judgement and crowdsourced evaluations are noisy by nature . While it is usually sufficient to employ few trusted expert coders , it is suggested to use more in the non - expert case ( Snow et al , 2008 ) . Thus , we assign 5 different annotators to each of the 30 tasks . At estimated 10 - 15min duration , we provide 3 $ compensation for answering a single task , above minimum wage in our country . Quality Assurance . Any crowdsourcing task offers an incentive to rush tasks for the money , which requires us to employ means of quality assurance ( QA ) . As we have an uncontrolled environment and thus untrusted coders , we handcraft test questions for the selection and preference test . This task is non - trivial as we require unambiguity in correct answers ( we ensured this with multiple qualitative tests among friends and colleagues ) , while simultaneously not being too obvious . We place one test question for selection and one for preference randomly into each task ( ending up in 11 words or emoji per task ) . This also means that each coder can only participate in up to 5 different tasks within a single campaign before re - seeing a test question . We define acceptance thresholds of four out of five correct answers for both , the selection test and the correct direction for the preference test .", "entities": []}
{"text": "Within the crowdsourcing process , we rejected about 10 % of all tasks according to our QA measures , which then had to be re - taken . We ended up with 6 campaigns each having 50 words / emoji answered by 5 coders ; summing up to completed 150 tasks . In total , 16 different coders accomplished this series of which 4 completed \u03a3 \u2265 100 tasks .", "entities": []}
{"text": "First we focus on the describing emoji campaigns ( E/ * ) . We present our main evaluation results in Tab . 1 . Within columns , we show results for random , original POLAR , and our six campaigns . We split the rows into results from the selection test across Top1 .. 5 entries and the preference test . Selection Test . We find very good results along all emoji campaigns ( E/ * ) being consistently better than any campaign describing words ( W/ * ) . The best performance was achieved for explaining emoji with emoji ( E / E ) ; others are on par . We want to note however , that the small size of used emoji - differential set may ease selection . E.g. , facial expression emoji regularly achieve higher embedding scores than others , which thus may bias the bottom control half ( 4.1.1 ) . However , interpreting emoji or words with words only , ( E / W ) and ( W / W ) , achieve comparable performance . Preference Test . Here , we make the same observation ; The projected scales on the differentials are mostly well in line with human judgement .", "entities": []}
{"text": "Again , we refer to Tab . 1 , but now change our focus to describing words , campaigns ( W/ * ) . Selection Test . Albeit not being directly comparable , using POLAR \u03c1 in compaings : describing words with words ( W / W ) , or describing words with words and emoji ( W / M ) achieved performance well on par with POLAR . Noteworthy , describing words with emoji ( W / E ) yielded the worst results . The projection scale values for the emoji dimensions were mostly lower compared to words . I.e. , according to POLAR \u03c1 , for words only few emoji differentials would be among the top 5 opposites . Preference Test . As for the preference test , describing words yield the best results using word opposites only ( W / W ) . Explaining words with emoji ( W / E ) performs particularly worse . Task Random POLAR ( W / W ) ( W / M ) ( W / E ) ( E / W ) ( E / M ) ( E / E )", "entities": []}
{"text": "Significance . To test for differences within the coder alignment with POLAR \u03c1 , we model both , the selection and preference test . With our primary goal to understand the impact of including emoji to a POLAR \u03c1 interpretable word embedding , we anchor to the ( W / W ) campaign as a baseline . For the selection test , we count if coders aligned with POLAR \u03c1 or chose any of the random alternatives across the Top 1 .. 5 selection . For the preference test , we count whether coders aligned with POLAR \u03c1 's scale direction . We apply double - sided chi - square tests \u03c7 2 with p < 0.05 between the interpreting words with words ( W / W ) baseline and the remaining five campaigns . We identify significant differences in coder - POLAR \u03c1 alignment to the ( W / W ) baseline when describing words with emoji ( W / E ) over Top1 .. 5 selection and preference . Counts from explaining emoji with emoji ( E / E ) signal significance for preference and selection Top3 .. 5 . Coder - POLAR \u03c1 alignment in preferences is also significant for describing emoji with emoji and words ( E / M ) .", "entities": []}
{"text": "Though we are confident in applied QA measures , none of the demographics can be confirmed . The annotator sample - size is small and thus most likely not representative . Further , we find most workers providing contrasting answers across multiple tasks they participated in , rendering collected demographic information unusable .", "entities": []}
{"text": "In this section , we describe the techniques used in our system . Interested readers are encouraged to check out the original papers for further details .", "entities": []}
{"text": "Iterative back - translation ( Hoang et al , 2018 ) is an extension of back - translation ( Sennrich et al , 2016a ) . It can exploit both sides of monolingual data of a language pair , and produces translation models for both directions , which is suitable for this shared task . The initial models for generating synthetic parallel data are produced by using dual transfer with low resource authentic parallel data . In each iteration of iterative back - translation , we use the latest model to greedily decode a disjoint subset of 4 m monolingual sentences 1 to generate synthetic parallel data . Then a new model is trained on a mixture of authentic and synthetic parallel data . With the use of dual transfer , model training can start from [ A ] PLM emb . [ A ] PLM body A and B mono . ( 1 ) [ P ] PLM emb . [ A ] PLM body P and Q mono . ( 2 ) [ A ] NMT encoder emb . [ A ] NMT encoder body [ B ] NMT decoder emb .", "entities": []}
{"text": "( 3 ) [ P ] NMT encoder emb .", "entities": []}
{"text": "We only use selected finetuning for the chv - ru pair because parallel data for hsb - de is scarce . In order to test the effect of selected finetuning , we start from the models of Iteration 2 in Table 5 . Results in Table 6 indicate that selected finetuning gives modest improvements .", "entities": []}
{"text": "Ideally , the generated element e s after decoding is supposed to exactly belong to the vocabulary set it is meant to be . For example , the predicted aspect term should explicitly appear in the input sentence . However , this might not always hold since each element is generated from the vocabulary set containing all tokens instead of its specific vocabulary set . Thus , the predictions of a generation model may exhibit morphology shift from the ground - truths , e.g. , from single to plural nouns . We propose a prediction normalization strategy to refine the incorrect predictions resulting from such issue . For each sentiment type c denoting the type of the element e such as the aspect term or sentiment polarity , we first construct its corresponding vocabulary set V c . For aspect term and opinion term , V c contains all words in the current input sentence x ; for aspect category , V c is a collection of all categories in the dataset ; for sentiment polarity , V c contains all possible polarities . Then for a predicted element e of the sentiment type c , if it does not belong to the corresponding vocabulary set V c , we use\u0113 V c , which has the smallest Levenshtein distance ( Levenshtein , 1966 ) with e , to replace e.", "entities": []}
{"text": "Datasets We evaluate the proposed GAS framework on four popular benchmark datasets including Laptop14 , Rest14 , Rest15 , and Rest16 , originally provided by the SemEval shared challenges ( Pontiki et al , 2014 ( Pontiki et al , , 2015 ( Pontiki et al , , 2016 . For each ABSA task , we use the public datasets derived from them with more sentiment annotations . Specifically , we adopt the dataset provided by Fan et al ( 2019 ) , Li et al ( 2019a ) , , Wan et al ( 2020 ) for the AOPE , UABSA , ASTE , TASD task respectively . For a fair comparison , we use the same data split as previous works .", "entities": []}
{"text": "Annotation - style & Extraction - style As shown in result tables , the annotation - style method generally performs better than the extraction - style method on the AOPE and UASA task . However , the former one becomes inferior to the latter on the more complex ASTE and TASD tasks . One possible reason is that , on the ASTE and TASD tasks , the annotation - style method introduces too much content , such as the aspect category and sentiment polarity , into the target sentence , which increases the difficulty of sequence - to - sequence learning .", "entities": []}
{"text": "Program Committee", "entities": []}
{"text": "Am I Me or You ? State - of - the - Art Dialogue Models Can not Maintain an Identity", "entities": []}
{"text": "We also consider combining expanded attention with re - ranking methods , or with multi - objective training , to see if the combination can improve results . For the latter we use the automated grounding trainable mask method . 4 Experimental Results", "entities": []}
{"text": "Profile Grounding Expanding the decoder attention yields significant gains across all automated metrics , as seen in Table 6 for a 1024 - truncate model ( and in Table 15 in Appendix F for a 128truncate model ) . As a baseline we explore simply re - attending to the full context again ; this indeed improves metrics across the board for the shortcontext model , but the long - context model actually suffers . However , both models improve substantially over the baseline when including the full LIGHT context without the dialogue history , and attention over sub - components of the LIGHT context still yields strong improvements . To see how much this expanded attention matters , we explored varying the number of rounds r { 1 , 2 , 3 } of expanded attention , i.e. , how many times the model attends to this additional context . In Table 6 , we also see that a second expanded attention round yields even better results , but performance drops off after applying a third round .", "entities": []}
{"text": "We performed human evaluation on our models . For each model we collected 100 human - model conversations , set up similarly to the original LIGHT dataset conversations . During the conversation , crowdworkers were asked to annotate the model 's response for the following attributes : 1 ) Mistaken Identity : your partner says something that would imply they believe they 're someone other than who they 're noted to be ; 2 ) Contradiction : your partner says something that contradicts something they 've said before ; 3 ) Wrong Location : your partner says something that would imply they believe they are in a different location than the provided one ; 4 ) Unrelated : your partner says something that does n't follow the previous turns ; and 5 ) Repetitive : your partner says something they 've already said , or are driving the conversation in circles . Utterances that do not contain any of the negative attributes are denoted \" all good \" . Finally , we collect an engagingness score on a scale of 1 - 5 at the end of the conversation . More details in Appendix I. Results are given in Correlations between automatic metrics and human evaluations are measured in Appendix K , where we find that RPA and mistaken identity are indeed strongly correlated . 5 Qualitative Analysis", "entities": []}
{"text": "We note that the human dialogue data is classified as being \" in character \" only 92.8 % of the time on the validation set by the LTR RPA classifier . We examine the scenarios in which the classifier is incorrect , with example input / output pairs in Table 21 in the Appendix . First , there are instances where either character could have said the output response ( row 1 ) . Second , there are instances where there are not enough clues in the context to provide an estimation of who said the response , for example at the beginning of the conversation ( row 2 ) . And , there are still some small amount of instances that the classifier simply fails ( row 3 ) .", "entities": []}
{"text": "We analyze the results of turn annotation to understand what failure modes contribute to mistaken identity . A full list of such modes is in Table 16 ; the baseline model most often mistakes its partner for itself ( i.e. , the model thinks it is talking to itself ) . Other common failures include the model thinking that it is its partner 's character , or emulating irrelevant characteristics .", "entities": []}
{"text": "We consider the RPA of various models when evaluated across the turns of conversation . Intuitively , baseline models would suffer as the conversation goes on for a variety of reasons ( character roles are truncated out of context , more input yields noisier outputs , etc . ) . Appendix Figure 1 shows the perturn results for a set of representative models . The human outputs are most often correct on the first turn , with gradual RPA decay throughout the conversation . The 128 - truncate baseline , as expected , suffers a dramatic performance drop after the first couple of turns . Meanwhile , with the profile expanded attention , we see near - human performance , with better RPA in later turns . Including RPA reranking improves dramatically over all turns .", "entities": []}
{"text": "To gain some insight into what is happening with the expanded attention , we mapped out the attention between context and response tokens for both a baseline model with no expanded attention , and a model with profile expanded attention . Figures 4 and 5 in the Appendix display the heat maps for an example context and response , with details on heat map construction given in Appendix M. We find that the baseline model spreads its attention out across both the LIGHT context and the dialogue history , with the majority of the attention looking at overlapping words in the context and the response and almost no attention on the character names . The expanded attention model concentrates on the recent dialogue history heavily in the first level of attention , and then concentrates on pertinent words in the context related to the character information ( i.e. , the character names ) in the second round of attention .", "entities": []}
{"text": "In this work we explored the problem of maintaining one 's character in open dialogue , and showed that state - art - of - the - art models have a fundamental weakness in this regard . We provided a clear framing of the problem and showed one can build automatic metrics ( RPA ) that evaluate models using a classifier . We then explored a variety of methods throughout this paper . While a wide variety of well - known techniques , such as multi - objective or unlikelihood training , have little impact , we found that expanded attention and re - ranking are two approaches that can help to a degree , and their combination also improves results . Our introduced method PACER performs well and may be suitable for other tasks beyond the focus of this paper . Nevertheless , our best methods still lag behind human ( crowdworker ) performance in several regards , e.g. 1.34 % vs. 2.23 % in terms of mistaken identity per turn , or 5 % vs. 14.7 % per conversation . Therefore considerable progress still has to be made on this challenging problem .", "entities": []}
{"text": "Limitations We note in the conclusion that the problem is not solved ; our best models still lag behind human performance in maintaining character identity . All results are tested in the LIGHT environment , comprising open - domain dialogue within constrained settings with assigned characters . The application of these methods to other role - playing ( or otherwise ) settings is left for future work , though we believe that such methods could be beneficial outside of LIGHT .", "entities": []}
{"text": "We provide methods for mitigating mistaken identity in dialogue models . It follows that such methods yield models that are more convincingly role - playing as a given character . With more convincingly in - character models , someone with bad intentions could have a model imitate realworld people without consent , or worse , can say negative / harmful things while impersonating someone else . We note that our methods are orthogonal to improvements in dialogue safety ( Xu et al , 2020 ; Dinan et al , 2019a ) , and so can be used in tandem to mitigate these potential risks .", "entities": []}
{"text": "We make use of LIGHT in this work ( Urbanek et al , 2019 ) ( released under CC - BY license ) , an English - language crowdsourced dataset . We also plan to release the code and models ( will be released under MIT license ) , with the intended use being for others ( and ourselves ) to reproduce and build upon the research discussed in this paper .", "entities": []}
{"text": "We build the training data for the RPA classifiers from the LIGHT dataset . The input is a concate - 2 https://parl.ai", "entities": []}
{"text": "In Table 10 , we see how each RPA classifier performs on the various datasplits , varying the number of prior utterances used during training and evaluation . Each model performs best on the split on which it was trained ( the highlighted numbers ) .", "entities": []}
{"text": "We find that the left - to - right RPA classifiers are correctly sensitive to per - token perturbations in the input , and can accurately predict the speaker at the token level . In Table 11 , we give an example where the classifier changes its character prediction , depending on the candidate utterance .", "entities": []}
{"text": "In Table 14 , we see how , when using either the encoder+decoder or just the decoder outputs , we do not require additional multi - objective layers ( as we did in the non - automated - grounding case ) .", "entities": []}
{"text": "We provide results for both the 128 - truncate and 1024 - truncate models with profile grounding in Table 15 . Trends remain the same for both models .", "entities": []}
{"text": "Table 17 includes results on the LIGHT validation set for models in Table 4 .", "entities": []}
{"text": "We evaluated a Poly - encoder baseline model with an RPA re - ranker as well . The Poly - encoder scores utterances from the full training set as candidates , and the candidates for re - ranking are the top - k ranked utterances ; results are in Table 18 . Retrieval models benefit dramatically from the re - ranking , improving to almost 99 % RPA as measured by the LTR classifier . As the candidate responses for retrieval models come from the set of all training utterances , and due to overlap between the set of characters appearing in the train and valid sets , we can examine how often the model output was originally spoken by its partner 's character ; this can be seen as a proxy for mistaken identity . We find that the re - ranker reduces the amount of time that the model returns a message its partner said , indicating some viable and promising results .", "entities": []}
{"text": "In Table 19 , we display the full results of human evaluations across all dimensions . We note that the Poly - encoder model is best at not mistaking location or being repetitive , but this is expected given its retrieving over human - written utterances . In Setting : Turquoise Shore , Shore A beautiful turquoise color water by the shore . It is filled with many gems and gold . Figure 2 , we show a screenshot of the instructions for the evaluation task provided to crowdworkers on Amazon Mechanical Turk .", "entities": []}
{"text": "We provide qualitative analysis of the various generation methods below . No Re - ranking When examining the baseline with no re - ranking , we found that nucleus sampling can help when beam search does not work ; however , both can go out of character the farther one goes in conversation . Beam Search Re - rankers The beam outputs in standard beam search are at times too similar , in which case re - ranking does next to nothing , unless a viable response is available . Nucleus Sampling Re - rankers Using nucleus setting in a re - ranking setup yields more diverse choices to choose from ; however , sometimes the model simply does not address * any * character within the conversation . Delayed Beam Search Re - rankers This strikes a nice balance between sensible outputs from beam search and diversity from nucleus sampling . Mixed - Decoding Re - ranker Using mixed decoding ( re - ranking several decoding schemes ) can work quite well , as it is a nice blend of different generation methods .", "entities": []}
{"text": "Qualitative analysis of the turn annotation results are in Table 16 . We generally found that beam search fails the vast majority of the time when the model thinks that it is talking to itself ; i.e. , it confuses its partner for its own character . The rerankers can help shift the hallucination away from this regime .", "entities": []}
{"text": "seems to be most helpful in later turns , where other models generally drop off in efficacy .", "entities": []}
{"text": "To build the heat maps in Figures 4 and 5 , we look at the maximum attention applied per - head , and the maximum weight applied across the model decoder layers ; other combinations were considered ( mean per - head , mean over layers or last layer ) and yielded similar findings . The speaker is the mermaid , whose partner is a sea - witch . The last utterance from the sea - witch is , \" What are you doing on the turquoise shore ? \" . The mermaid responds , \" I 've been catching waves with the dolphins all morning . What kind of victims do you expect to find in a tranquil place like this ? \"", "entities": []}
{"text": "Exploring Statistical and Neural Models for Noun Ellipsis Detection and Resolution in English", "entities": []}
{"text": "Noun ellipsis is a linguistic phenomenon where the head noun of a noun phrase gets deleted , without making the sentence ungrammatical . For example in the sentence in ( 1 ) from ( Lobeck , 1995 ) , the noun presentation is elided at \" [ e ] \" . 1 . John 's presentation on urban development was virtually ignored because [ NP Mary 's [ e ] ] was so much more interesting . The elided information can be retrieved from the previous context as in ( 1 ) or with the knowledge of idiomatic usage of language as in I will be back in two [ e ] . where two means two minutes . It is also possible that the reference of the elided information comes from extra - linguistic , situational context . For example , consider a speaker pointing towards the roses in a shop and saying an utterance as in I will take two [ e ] . While human interlocutors resolve any such elided information by disambiguating from context , cognitive commonsense extension and reasoning ( Chen , 2016 ) , ellipsis resolution can be a hard task for Natural Language Processing ( NLP ) systems ( Hardt , 1999 ) . Resolution of ellipsis comprises two tasks - detection of the elided material and antecedent selection ( Liu et al , 2016b ; Nielsen , 2003 ) . Ellipses occur in the environment of certain syntactical structures or trigger words , also known as licensors or triggers of ellipses . They are useful syntactic cues for the detection of noun ellipsis . See Figure 1 for an example of the noun ellipsis resolution process .", "entities": []}
{"text": "We use POS tags of the licensor and modifier of the antecedent as our syntactic features and cosine similarity between their POS tags as our semantic features , following ( Khullar et al , 2019 ) . We concatenate these features to the embeddings to explore the efficacy of adding manual features on resolution .", "entities": []}
{"text": "Attending via both Fine - tuning and Compressing", "entities": []}
{"text": "Though being a primary trend for enhancing interpretability of neural networks , attention mechanism 's reliability and validity are still under debate . In this paper , we try to purify attention scores to obtain a more faithful explanation of downstream models . Specifically , we propose a framework consisting of a learner and a compressor , which performs finetuning and compressing iteratively to enhance the performance and interpretability of the attention mechanism . The learner focuses on learning better text representations to achieve good decisions by fine - tuning , while the compressor aims to perform compressions over the representations to retain the most useful clues for explanations with a Variational information bottleneck ATtention ( VAT ) mechanism . Extensive experiments on eight benchmark datasets show the great advantages of our proposed approach in terms of both performance and interpretability .", "entities": []}
{"text": "In this section , we introduce our framework consisting of a learner and a compressor with a Variational information bottleneck ATtenttion ( VAT ) mechanism . Given an attention - based neural network model , we formulate our idea within the framework of variational information bottleneck ( VIB ) ( Tishby et al , 1999 ) . Our framework aims to improve the attention 's interpretalility with better performance by restricting the attention to capture the crucial words while filter the useless information .", "entities": []}
{"text": "First , we perform our models and baselines on eight benchmark datasets and visualize the text representation to verify the effectiveness of VAT ( Section 5.1 ) . Second , to further investigate our VAT model , we adopt two popular explanation metrics for quantitative evaluation ( Section 5.2 ) . Third , we apply our models to semi - supervision sentiment detection task to evaluate the explanation of our model ( Section 5.3 ) . Fourth , we explore the influence of our iteration strategy in Section 5.4 and provide case studies in Section 5.5 . For the limitation of the space , we may only list the results on parts of the datasets in some cases since the conclusions are similar for other datasets . The complete results are presented in the supplementary materials .", "entities": []}
{"text": "The authors wish to thank the reviewers for their helpful comments and suggestions . This research is ( partially ) supported by NSFC ( 62076097 ) , STCSM ( 18ZR1411500 ) , the Fundamental Research Funds for the Central Universities . This research is also funded by the Science and Technology Commission of Shanghai Municipality ( 19511120200 & 20511105102 ) . The computation is performed in ECNU Multifunctional Platform for Innovation ( 001 ) . The corresponding authors are Yuanbin Wu and Liang He .", "entities": []}
{"text": "Pervasive Attention : 2D Convolutional Neural Networks for Sequence - to - Sequence Prediction", "entities": []}
{"text": "In this section , we present our experimental setup , followed by quantitative results , qualitative examples of implicit sentence alignments from our model , and a comparison to the state of the art .", "entities": []}
{"text": "The JPC2 patent task consisted of translation in the patent domain between English and Japanese , Korean and Japanese , and Chinese and Japanese . The training data consisted of parallel corpora provided by the Japan Patent Office ( JPO ) , with training sets containing one million sentence pairs for each language pair . The data are drawn from four domains , chemistry , electricity , mechanical engineering , and physics . 1", "entities": []}
{"text": "The overall architecture of our universal dependency parser is shown in Figure 1 . The whole system can be divided into two parts : Known Language Parser and Surprise Language Parser . The former deals with known languages , including rich resource treebanks and low resource treebanks , whose annotations as the training data are accessible , while the latter disposes of the ones without dependency annotations . When the text to be processed by the system is inputed , it is first discriminated as rich - resource or low - resource and then dispatched to the corresponding sub - systems , which will be described as follows . For the Known Language Parser , the related pipeline contains three steps as follow . ( 1 ) Tokenizer The raw texts are split into basic units for the latter processing of dependency anal - ysis , which is the main task of the tokenizer . For all rich resource languages , we train tokenziers using provided training data , including the languages which can be easily tokenized by specific delimiters . ( 2 ) Tagger The tokenized texts are labeled by taggers , which provides them with the tags which will be utilized in the later dependency analysis , such as POS and morphological features . Like the previous step , we train taggers for all the rich resource languages . ( 3 ) Dependency Parser Tokens and linguistic features generated by taggers are put into the dependency parser to generate the final dependency structures . For Surprise Language Parser , only Dependency Parser is needed . We directly take the provided CoNLL - U files which already include the tokens and features as inputs and predicts the results . Without annotated training data , we could not train the tokenizers and taggers for these languages ; Meanwhile for the parsing , we adopt a delexicalized and cross - lingual strategy , which will be described later in Section 3.3 .", "entities": []}
{"text": "In the final testing phase of the shared task , there are mainly three types of test data ( Nivre et al , 2017a )", "entities": []}
{"text": "In the Known Language Parser , the first step is to tokenize the input raw text , generating the basic units for later processing . We train tokenizers for all the languages using UDPipe , including those ones which are quite easy to separate using simple rules , like identifying the blank spaces in English . Considering there are some languages that could not be simply tokenized by blank spaces , we adopt this unified treatment for this step . The tokenizers are trained mainly using the SpaceAfter features provided in the CoNLL - U files and the parameters of UDPipe Tokenizer are shown in Table 1 .", "entities": []}
{"text": "Morphological features from the universal feature inventory or from a defined language - specific extension . These features will be used as inputs in the final parsing step for Rich Resource Languages .", "entities": []}
{"text": "Evaluation process of this shared task is deployed in TIRA 6 ( Potthast et al , 2014 ) . LAS is the main scoring metric and we show performances of our system in several types of treebanks in Table 5 using the same groups as the official results . What 's more , LAS of our system in Surprise Languages are shown in Table 6 . We show several official evaluation results such as LAS , UAS and other results and compared with best results in Table 7 .", "entities": []}
{"text": "Let a test document x be a sequence of words ( w 1 , , w j , ) , and a class topic description y be a sequence of words d y = ( w 1 , , w y , ) . All words are in vocabulary V . We propose a generative approach , where the predictive probabil - ity p ( y | x ) \u221d p ( x | y ) p ( y ) . Generative approaches tends to perform well when training data is scarce , which is the case in our setting . We assume there exists weak prior knowledge on which classes are popular and which are rare . We can then construct rough estimatesp ( y ) using simple heuristics as described in ( Schapire et al , 2002 ) . It distributes probability mass q evenly among majority classes , and 1 \u2212 q evenly among minority classes . We treat the most frequent class as the majority class , the rest as minority classes , and q = 0.7 in our experiments . By interpreting class topic description as words , we obtainp ( x | y ) = p ( x | d y ) . We assume that the d y expresses a noisy - OR relation of the words it contains ( Oni\u015bko et al , 2001 ) . Up to first - order approximation : p ( x | d y ) = 1 \u2212 wy dy ( 1 \u2212 p ( x | w y ) ) \u2248 wy dy p ( x | w y ) , ( 1 ) where each w y is a word in the class topic description d y . Further , we assume that words in document x are conditionally independent given a label word w y ( na\u00efve Bayes assumption ) : p ( x | w y ) = w j x p ( w j | w y ) . ( 2 ) Combining ( 1 ) and ( 2 ) , the document likelihood i\u015d p ( x | y ) = wy dy w j x p ( w j | w y ) . To this end , we need a word association model p ( w 1 | w 2 ) , \u2200w 1 , w 2 V . It can be efficiently learned by word embedding algorithms . The skipgram algorithm learns vector representations of words , such that for words w 1 , w 2 , their vectors u w 1 , v w 2 approximate the conditional probability 1 p ( w 1 | w 2 ) = exp u w 1 v w 2 w V exp ( u w v w 2 ) . ( 4 ) Combining ( 3 ) with ( 4 ) , the document likelihood become\u015d p ( x | y ) = wy dy exp \uf8eb \uf8ed w j x u w j v wy \u2212 C wy \uf8f6 \uf8f8 , where C wy = log w V exp u w v wy is independent of document x and only related to label word w y , therefore can be precomputed and stored to save computation . Finally , we construct an generative classifier a\u015d p ( y | x ) \u221dp ( x | y ) p ( y ) . We call this method word embedding na\u00efve Bayes ( WENB ) .", "entities": []}
{"text": "We thank the anonymous reviewers for their helpful comments . This work was in part supported by the National Library of Medicine under grant number 2R01LM010681 - 05 . Qiaozhu Mei 's work was supported in part by the National Science Foundation under grant numbers 1633370 and 1620319 . Yue Wang would like to thank the support of the Eleanor M. and Frederick G. Kilgour Research Grant Award by the UNC - CH School of Information and Library Science .", "entities": []}
{"text": "The author would like to thank Saleh Soltan , Gokhan Tur , Saab Mansour , and Batool Haider for reviewing this work and providing valuable feedback .", "entities": []}
{"text": "Automatic Compositor Attribution in the First Folio of Shakespeare", "entities": []}
{"text": "Within literary studies , the field of bibliography has an unusually long tradition of quantitative analysis . One particularly relevant area is that of compositor attribution - the clustering of pages in a historical printed document by the individual ( the compositor ) who set the type . Like stylometry , a long - standing area of NLP that has largely focused on attributing the authorship of text ( Holmes , 1994 ; Hope , 1994 ; Juola , 2006 ; Koppel et al , 2009 ; Jockers and Witten , 2010 ) , the analysis of orthographic patterns is fundamental to compositor attribution . Additionally , compositor attribution often makes use of visual features , such as whitespace layout , introducing new challenges . These analyses have traditionally been done by hand , but efforts are painstaking due to the difficulty of manually recording these features . In this paper , we present an unsupervised model specifically designed for compositor attribution that incorporates both textual and visual sources of evidence traditionally used by bibliographers ( Hinman , 1963 ; Taylor , 1981 ; Blayney , 1991 ) . spelling variation spacing variation medial comma Figure 1 : The compositor of the left page tended to use the spellings doe and deere , while the compositor for the right page used spellings do and deare , indicating these pages were likely set by different people . The varying width of the medial comma whitespace also distinguishes the typesetters . Our model jointly describes the patterns of variation both in orthography and in the whitespace between glyphs , allowing us to cluster pages by discovering patterns of similarity and difference . When applied to digital scans of historical printed documents , our approach learns orthographic and whitespace preferences of individual compositors and predicts groupings of pages set by the same compositor . 1 This is , to our knowledge , the first attempt to perform compositor attribution automatically . Prior work has proposed automatic approaches to authorship attributionwhich is typically viewed as the supervised problem of identifying a particular author given samples of their writing . In contrast , compositor attribution lacks supervision because compositors are unknown and , in addition , focuses on different linguistic patterns . We explain spellings of words conditioned on word choice , not the word choice itself . c i s ik m ij d ij dear deere K i J i I Compositor C ! ! ! ! ! ! Edit operation weights :", "entities": []}
{"text": "Word variant weights : Figure 2 : In our model , a compositor ci is generated for page i from a multinomial prior . Then , each diplomatic word , dij , is generated conditioned on ci and the corresponding modern word , mij , from a distribution parameterized by weight vector wc . Finally , each medial comma spacing width ( measured in pixels ) , s ik , is generated conditioned on ci from a distribution parameterized by \u03b8c . To evaluate our approach , we fit our model to digital scans of Shakespeare 's First Folio ( 1623 ) a document with well established manual judgements of compositor attribution . We find that even when relying on noisy OCR transcriptions of textual content , our model predicts compositor attributions that agree with manual annotations 87 % of the time , outperforming several simpler baselines . Our approach opens new possibilities for considering patterns across a larger vocabulary of words and at a higher visual resolution than has been possible historically . Such a tool may enable scalable first - pass analysis in understudied domains as a complement to humanistic studies of composition .", "entities": []}
{"text": "In this paper we focus on modeling the same types of observations made by scholars and demonstrate agreement with authoritative attributions . We use compositor studies of Shakespeare 's First Folio to inform our approach , drawing on the methods proposed by Hinman ( 1963 ) , Howard - Hill ( 1973 ) , andTaylor ( 1981 ) . Hinman 's landmark 1963 study clustered the pages of the First Folio according to five different compositors based on variations in spelling among three common words . Figure 1 , for example , shows portions of two pages of the First Folio with different spelling variants for the words dear and do : one compositor used deere and doe , while the other used deare and do . Hinman relied on the assumption that each compositor was consistent in their preferences for the sake of convenience in the typesetting process ( Blayney , 1991 ) . Subsequent studies looked at larger sets of words and more general orthographic preferences ( e.g. the preference to terminate words with - ie instead of - y ) , leading to modifications of Hinman 's original analysis ( Howard - Hill , 1973 ; Taylor , 1981 ) . In this paper we propose a probabilistic model designed to capture both word - specific preferences and general orthographic patterns . To separate the effect of the compositor from the choices made by the author or editor , we condition on a modernized ( collated ) version of Shakespeare 's text as was done by scholars . Visual features , including typeface usage and whitespace layout , also inform compositor attribution . For example , the highlighted spacing in Figure 1 shows different choices after medial commas ( commas that occur before the end of the line ) . Bibliographers produced new hypotheses about how many compositors were involved in production based on the analysis of the use of spaces before and after punctuation ( Howard - Hill , 1973 ; Taylor , 1981 ) . We additionally incorporate this source of evidence into to our automatic approach by modeling pixel - level whitespace distances . Bibliographers also use contextual information to inform their analyses , including copy text orthography , printing house records , collation , type case usage , and the use of type with cast - on spaces . In our model , we restrict our analysis to only those features that can be derived from the OCR output and simple visual analysis .", "entities": []}
{"text": "The results on OCR ( character error rate for most plays \u2248 10 \u2212 15 % ) transcripts are only marginally worse than those on manual transcripts , which shows that our approach can be generalized for the common case where manual diplomatic transcriptions are not available . For our experiments , we also chose a common modern edition of Shakespeare instead of more carefully produced modernized transcription of the facsimile - our goal being to again show that this approach can be generalized , perhaps to documents where careful modernizations of the facsimile are not available . Together , these results suggest that our model may be sufficiently robust to aid bibliographers in their analysis of less studied texts . Figure 3 shows an example of the feature weights and spacing parameters learned by the FEAT w/ ALL model . Our statistical approach is able to successfully explain some of the observations scholars made . For example , Taylor ( 1981 ) notices that compositors C and D prefer to omit u in young but A does not . Our model reflects this by giving u DEL high weight for D and low weight for A. However , the weight of a single feature is difficult to interpret in isolation . This might be the reason why our model only moderately agrees in case of compositor C. Another example can be seen in spacing patterns : according to Taylor ( 1981 ) , compositor C uses spaced medial commas unlike A and D. Our model learns the same behavior .", "entities": []}
{"text": "Our primary goal is to scale the methods of compositor attribution , including both textual and visual modes of evidence , for use across books and corpora . By using principled statistical techniques and considering evidence at a larger scale , we offer a more robust approach to compositor identification than has previously been possible . The fact that our system works well on OCR texts means that we are not restricted to only those documents for which we have manually produced transcriptions , opening up the possibility for bibliographic study on a much larger class of texts . Though we are unable to incorporate the kinds of world knowledge used by bibliographers , our ability to include more information and more finegrained information allows us to recreate their results . Having validated these techniques on the First Folio , where historical claims are well established , we hope future work can extend these methods and their application .", "entities": []}
{"text": "We thank the three anonymous reviewers for their valuable feedback . This project is funded in part by the NSF under grant 1618044 .", "entities": []}
{"text": "Detecting Polarized Topics Using Partisanship - aware Contextualized Topic Embeddings", "entities": []}
{"text": "Growing polarization of the news media has been blamed for fanning disagreement , controversy and even violence . Early identification of polarized topics is thus an urgent matter that can help mitigate conflict . However , accurate measurement of topic - wise polarization is still an open research challenge . To address this gap , we propose Partisanship - aware Contextualized Topic Embeddings ( PaCTE ) , a method to automatically detect polarized topics from partisan news sources . Specifically , utilizing a language model that has been finetuned on recognizing partisanship of the news articles , we represent the ideology of a news corpus on a topic by corpus - contextualized topic embedding and measure the polarization using cosine distance . We apply our method to a dataset of news articles about the COVID - 19 pandemic . Extensive experiments on different news sources and topics demonstrate the efficacy of our method to capture topical polarization , as indicated by its effectiveness of retrieving the most polarized topics . 1", "entities": []}
{"text": "We use the AYLIEN COVID - 19 dataset 3 consisting of~1.5 M news articles related to the pandemic spanning from Nov 2019 to July 2020 that are from 440 global sources . To discover the polarization between politically divided news media , we select six well - known US publishers evenly split between partisan leanings : CNN , Huffington Post ( Huff ) , New York Times ( NYT ) as liberal sources vs. Fox , Breitbart ( Breit ) and New York Post ( NYP ) as conservative sources . After filtering the publishers and remove duplicate articles , 66 , 368 articles are left spanning from Jan 2020 to July 2020 . The statistics of news articles are shown in Appendix A.", "entities": []}
{"text": "To quantitatively evaluate the effectiveness of PaCTE and the baselines in capturing topic polarization , we use the 10 manually labeled topics to create a ground truth ranking of polarized topics and score models on their ability to retrieve the most polarized topics on this ranked list . Evaluation protocol . Given a liberal news corpus D L , a conservative news corpus D R , and a list of 10 topics ranked by ground - truth polarization scores , l gt ( D L , D R , T labeled ) , as described in Section 4.3 , we define the top - 3 topics in the list as the target polarized topics that deserve more attention and that should be addressed when trying to prevent polarization from escalating . The target polarized topics between different pairs of news sources are shown in Table 2 . Then , given a ranked list of topics f pred ( D L , D R , T labeled ) predicted by a model , we evaluate how effectively the 3 target polarized topics are retrieved in this model predicted list using recall@3 . In other words , we check how much the overlap is between the top - 3 topics in the ground - truth ranking and the top - 3 topics in the predicted ranking , of the 10 labeled topics . We call this task polarized topics retrieval .", "entities": []}
{"text": "In Section 4.5 we quantitatively demonstrate the effectiveness of PaCTE in retrieving polarized topics when evaluating with the 10 labeled topics . We believe that such success generalizes to the case where the input to the model is the complete topic list T containing 30 topics . In this section , we conduct a case study and retrieve the top - 3 most polarized topics from T in CNN vs. Fox , Huff vs. Breit and NYT vs. NYP , by PaCTE . Since we do not have the ground - truth target polarized topics from T , for the retrieved topics , we conduct manual inspections on relevant documents and give explanations about the polarization . For the topics in T labeled , the polarization is formed due to the two political stances . Therefore in this section we only focus on the retrieved topics not in T labeled . CNN vs. Fox . The retrieved top - 3 topics are topic 28 , 6 , 10 , where topic 10 is in T labeled . The first retrieved topic is topic 28 , where CNN suggests the surge of new COVID cases every day but Fox suggests that the state should reopen . On topic 6 CNN reports the serious situation of coronavirus in the US , including the high number of cases and collapse of quarantine hotels , but Fox focuses more on worldwide coronavirus situation and suggests the high number of cases in Michigan is misleading . Huff vs. Breit . The retrieved top - 3 topics are topic 29 , 9 , 31 , where topic 9 is in T labeled . On topic 29 , Huff advocates Pelosi 's coronavirus bills while Breit criticizes them . On topic 31 , the articles talk about different court cases ; however , no clear polarization is discerned between the pair of news sources by manual inspections . We regard it as a failure case of PaCTE . Although the relevant articles are regarding the same topic , they have different subjects or events , and thus misleading PaCTE to perceive polarization between them . NYT vs. NYP . The retrieved top - 3 topics are topic 28 , 12 , 10 , where topic 12 and 10 are in T labeled . On topic 28 , just as in CNN vs. Fox , NYT takes the pandemic more seriously and NYP suggests reopening . As a result , despite a minor error , PaCTE manages to retrieve polarized topics from T on the three pairs of news sources . Although we are not able to verify if the retrieved topics are indeed the groundtruth top - 3 most polarized topics , we argue that if given the ground - truth ranking on T , PaCTE will retain its satisfactory quantitative performance in retrieving polarized topics .", "entities": []}
{"text": "In this paper , we propose a method to automatically discover topic - level polarization between partisan news sources by contextualized topic embeddings . For evaluation , we create annotations on topic polarization scores in different partisan news source pairs on a variety of topics . Compared to the leaveout estimator ( Demszky et al , 2019 ) that is purely based on statistical features , our method can more precisely and meaningfully capture topical polarization as indicated by the performance on polarized topics retrieval . We hope that more NLP and researchers and practitioners can contribute to this research area that is promising but receiving insufficient attention . Because detecting polarized topics between partisan news sources is a less established task in the research community , we articulate the data annotation and the model evaluation in great detail and make the method seemingly \" complicate \" . However , we believe that for public media watchdogs and social media platforms to flag the highly polarized topics , our method is simple to implement , because each of the five steps described in Section 3 is based on robust methods in NLP . For future work , we plan to perform our method on more datasets , such as the tweets with noisy texts ( Demszky et al , 2019 ) . In addition , we will study how to finetune the language model when when partisanship labels are not available .", "entities": []}
{"text": "The statistics of the dataset is in Table 5 . We use the summary of each news article to perform the textual analysis , because the summary contains sufficient information to understand the political stance of the article and the whole text is lengthy for the pretrained language model to handle . For a complete list of all documents , please check our public repository 4 .", "entities": []}
{"text": "On topic 10 , we show six examples of news articles , one from each news source . For a complete list of news articles , please refer to our public repository . CNN : There has been a concerted effort among aides and allies to get President Donald Trump to stop conducting the daily coronavirus briefings , multiple sources tell CNN . The briefing came a day after Trump had given a lengthy briefing to the media , at one point suggesting it might be possible to treat coronavirus by injecting people with sunlight or disinfectants . Trump asked White House coronavirus task force coordinator Dr. Deborah Birx during Thursday 's briefing . A source close to the coronavirus task force said Trump was upset about the \" flack \" he was taking after those comments and that appears to be part of the reason why the President cut Friday 's briefing short . During the earlier questioning from reporters on Friday , Trump said he was being \" sarcastic \" with his suggestion that people inject themselves with disinfectant , even though he was clearly being serious during Thursday 's briefing . Fox : White House press secretary Kayleigh McEnany , during her first official briefing , promised that she ' will never lie ' to the press in her new role . White House press secretary Kayleigh McEnany , during her first official briefing , promised that she \" will never lie \" to the press in her new role . McEnany took the podium for the first time Friday , after being tapped as White House press secretary from her post as national spokeswoman for President Trump 's re - election campaign earlier this month . TRUMP NAMES KAYLEIGH MCENANY AS NEW WHITE HOUSE PRESS SECRETARY \" I will never lie to you , \" McEnany told reporters . McEnany seemed to signal that the White House would scale back on their daily coronavirus task force briefings , which were regularly led by the president himself , and Vice President Pence , with appearances from Dr. Deborah Birx and Dr. Anthony Fauci to provide public health information . Huffington Post : President Donald Trump on Sunday tore into former President Barack Obama , calling him \" an incompetent president \" after Obama appeared to criticize his response to the coronavirus crisis during two commencement speeches a day earlier . Asked about Obama 's remarks , Trump told reporters on the White House lawn that he \" did n't hear it \" before proceeding to bash his predecessor as \" grossly incompetent . \" President Trump : \" [ President Obama ] was an incompetent president . But earlier this month , Obama reportedly bashed the Trump administration 's response to the pandemic as \" an absolute chaotic disaster \" during a phone call with some of his former White House aides . When a Washington Post reporter last week asked Trump to explain \" Obamagate , \" the president refused . Breibart : New York magazine Washington correspondent Olivia Nuzzi responded angrily to criticism from former White House press secretary Ari Fleischer on Monday evening , tweeting at him : \" Oh shut the f*ck up . \" Fleisher , who served under President George W. Bush , criticized Nuzzi after a Rose Garden press briefing on the coronavirus pandemic in which she asked President Donald Trump : \" If an American president loses more Americans over the course of six weeks than died in the entirety of the Vietnam War , does he deserve to be re - elected ? \" One example is a \" fake news \" viral photograph of President Lyndon B. Johnson , which was presented by many Trump critics as if Johnson had been expressing grief over the deaths in Vietnam . President Trump is said to be reconsidering post , twitter , video , facebook , tweet , social_media , share , write , call , make 10 trump , president , white_house , donald , administration , fauci , coronavirus , vice , briefing , task_force 11 covid , dr , coronavirus , health , disease , drug , expert , risk , treatment , director 12 mr , biden , campaign , election , party , democratic , voter , joe_biden , republican , primary 13 school , child , student , university , parent , high , kid , year , family , class 15 american , pandemic , crisis , america , nation , make , policy , job , people , economy 17 time , world , space , launch , turn , center , long , life , leave , moment 18 coronavirus , report , outbreak , accord , ship , official , quarantine , military , force , iran 19 city , york , de_blasio , mayor , resident , area , yorker , coronavirus , people , tuesday 20 mask , people , wear , face , service , social_distance , church , sunday , coronavirus , stay 21 people , time , thing , good , work , make , lot , add , give , feel 22 department , official , national , security , fire , investigation , report , threat , call , director 23 employee , worker , company , restaurant , food , store , work , customer , business , amazon 24 china , chinese , world , outbreak , virus , wuhan , organization , coronavirus , global , government 25 time , series , film , show , year , make , movie , live , race , set 27 year , company , market , stock , price , drop , month , business , global , sale 28 state , coronavirus , cuomo , florida , texas , york , governor , tuesday , week , monday 29 house , coronavirus , republican , member , bill , senate , democrat , wednesday , washington , thursday 30 country , lockdown , government , coronavirus , measure , people , italy , restriction , travel , border 31 claim , court , judge , law , federal , district , rule , chicago , legal , decision 32 health , public , people , work , community , include , protect , provide , group , pandemic 33 hospital , care , health , patient , medical , covid , center , facility , home , doctor 34 program , pay , money , fund , economic , job , business , relief , federal , receive 38 coronavirus , office , letter , pandemic , call , send , statement , issue , write , act his daily press briefings because journalists use them to grandstand and to score political points , rather than to pursue information . The contrast with press briefings for governors and mayors is stark : there , journalists tend to be more deferential and to ask questions aimed at eliciting information rather than assigning political fault . New York Times : WASHINGTON - After several days spent weathering attacks from White House officials , Dr. Anthony S. Fauci hit back on Wednesday , calling recent efforts to discredit him \" bizarre \" and a hindrance to the government 's ability to communicate information about the coronavirus pandemic . On Wednesday , Peter Navarro , Mr. Trump 's top trade adviser , published a brazen op - ed article in USA Today describing Dr. Fauci as \" wrong about everything . \" All the while , White House officials - including the president and the press secretary - assert in the face of the evidence that there is no concerted effort to attack Dr. Fauci . given \" opposition research \" to discredit Fauci , including his past remarks early on in the pandemic that the public did n't need to wear masks . \" We were asked a very specific question by the Washington Post , and that question was President Trump noted that Dr. Fauci had made some mistakes , and we provided a direct answer to what was a direct question . \"", "entities": []}
{"text": "We show the topic - 10 most relevant document indices on all 30 topics on each source . On some topics there are less than 10 relevant documents on some sources . Note that such topics are not in the 10 labeled topics and are only used for qualitative analysis ; in other words , for quantitative analysis , we ensure that on all the 10 labeled topics , there are 10 relevant documents on each source . Topic 1 . CNN : 22873 , 62724 , 62635 , 63979 ,", "entities": []}
{"text": "The famous \" laurel / yanny \" phenomenon references an audio clip that elicits dramatically different responses from different listeners . For the original clip , roughly half the population hears the word \" laurel , \" while the other half hears \" yanny . \" How common are such \" polyperceivable \" audio clips ? In this paper we apply ML techniques to study the prevalence of polyperceivability in spoken language . We devise a metric that correlates with polyperceivability of audio clips , use it to efficiently find new \" laurel / yanny \" - type examples , and validate these results with human experiments . Our results suggest that polyperceivable examples are surprisingly prevalent , existing for > 2 % of English words . 1", "entities": []}
{"text": "To investigate polyperceivability in everyday auditory input , we searched for audio clips of single spoken words that exhibit the desired effect . Our method consisted of two phases : ( 1 ) sample a large number of audio clips that are likely to be polyperceivable , and ( 2 ) collect human perception data on those clips using Amazon Mechanical Turk to identify perceptual modes and confirm polyperceivability .", "entities": []}
{"text": "Each Mechanical Turk worker was randomly assigned 25 clips from our importance - sampled set of 200 . Each clip was slowed to either 0.9x , 0.75x , or 0.6x the original rate . Workers responded with a perceived word and a confidence score for each clip . We collected responses from 574 workers , all of whom self - identified as US - based native English speakers . This yielded 14 , 370 responses ( \u2248 72 responses per clip ) . Next , we manually reviewed these responses and selected the most promising clips for a second round with only 11 of the 200 clips . Note that because these selections were made by manual review ( i.e. listening to clips ourselves ) , there is a chance we passed over some polyperceivable clips - this means that our computations in Section 3 are only a conservative lower bound . For this round , we also included clips of the 5 words identified by Guan and Valiant ( 2019 ) , 12 potentially - polyperceivable words we had found in earlier experiments , and \" laurel \" as controls . We collected an additional 3 , 950 responses among these 29 clips ( \u2248 136 responses per clip ) to validate that they were indeed polyperceivable . Finally , we took the words associated with these 29 clips and produced a new set of clips using each of the 16 voices , for a total of 464 clips . We collected 4 , 125 responses for this last set ( \u2248 3 responses for each word / voice / rate combination ) .", "entities": []}
{"text": "An enormous body of work from cognitive sciences communities explores the quirks of human / animal sensory systems ( Fahle et al , 2002 ) . These works often have the explicit goal of exploring isolated \" illusions \" that provide insights into our perceptual systems ( Davis and Johnsrude , 2007 ; Fritz et al , 2005 ) . However , there are few efforts to quantify the extent to which \" typical \" instances are polyperceivable or lie close to decision boundaries . Miller ( 1981 ) studies the effect of speaking rate on how listeners perceive phonemes . The perceptual shifts studied therein are between phonetically adjacent perceptions ( e.g. \" pip \" vs. \" peep \" ) rather than dramatically different perceptions ( e.g. \" laurel \" vs. \" yanny \" ) . The \" perturbation \" of increasing human speaking rate is much more complex than simply linearly scaling the playback rate of an audio clip . Speaking - rate induced shifts also seem to hold more universally across voices , as opposed to the polyperceivable instances we examine .", "entities": []}
{"text": "We would like to thank Melody Guan for early discussions on this project , and the anonymous reviewers for their thoughtful suggestions . This research was supported by a seed grant from Stanford 's HAI Institute , NSF award AF - 1813049 and ONR Young Investigator Award N00014 - 18 - 1 - 2295 .", "entities": []}
{"text": "SPDB Innovation Lab at SemEval - 2022 Task 3 : Recognize Appropriate Taxonomic Relations Between Two Nominal Arguments with ERNIE - M Model", "entities": []}
{"text": "Synonyms and antonym practices are the most common practices in our early childhood . It correlated our known words to a better place deep in our intuition . At the beginning of life for a machine , we would like to treat the machine as a baby and build a similar training for it as well to present a qualified performance . In this paper , we present an ensemble model for sentence logistics classification , which outperforms the state - of - art methods . Our approach essentially builds on two models including ERNIE - M and DeBERTaV3 . With crossvalidation and random seed tuning , we select the top performance models for the last soft ensemble and make them vote for the final answer , achieving the top 6 performance .", "entities": []}
{"text": "In this section , we first describe the dataset and our data preprocessing steps , and then we present the details of the experimental setup for subtask1 .", "entities": []}
{"text": "This section describes our problem definition , codeswitching algorithm , language families , and the training methodology .", "entities": []}
{"text": "Given a source ( S ) and a set of target ( T ) languages , the goal is to train a classifier using data only in the source language and predict examples from the completely unseen target languages . We assume the target language is unknown during training ( fine - tuning ) time , which makes direct translation to target infeasible . In this context , we use code - switching ( cs ) to augment the monolingual source data . Thus , the input , augmented input , and output of our problem can be defined as : lset = googletrans.languages \u2212 lT for i 1 .. k do for j 1 .. len ( X en ut ) do G cs , L cs chunks = slot_chunks ( X en ut [ j ] , y en sl [ j ] ) for c chunks do l random.choice ( lset ) t translate ( c , l ) G cs G cs \u222a t L cs L cs \u222a align_label ( c , t ) end X cs ut X cs ut \u222a G cs y cs y cs \u222a y cs [ j ] y cs sl y cs sl \u222a L cs end end Input : X S ut , y S , y S sl , l T Code - Switched Input : X cs ut , y cs , y cs sl Output : y T , y T sl predict ( X T ut ) where X ut represents sentences , y their ground truth intent classes , y sl the slot labels for the words in those sentences , and l T the set of target languages . An example sentence , its intent class , and slot labels are shown in Figure 2 .", "entities": []}
{"text": "A language family is defined as a group of related languages that likely share a common ancestor . For example , Portuguese , Spanish , French , Italian , and Romanian are all derived from Latin ( Rowe and Levine , 2017 ) . We use language families to study their impact on the target languages . We augment the source language with code - switching to a particular language family . For instance , codeswitching the English dataset with Turkic language family and testing on Japanese can reveal how closely the two are aligned in the vector space of a pre - trained multilingual model . We work with 6 language groups : Afro - Asiatic ( Voegelin and Voegelin , 1976 ) , Germanic ( Harbert , 2006 ) , Indo - Aryan ( Masica , 1993 ) , Romance ( Elcock and Green , 1960 ) , and Turkic ( Johanson and Johanson , 2015 ) , also grouping Sino - Tibetan , Koreanic and Japonic ( Shafer , 1955 ; Miller , 1967 ) . 2 Germanic , Romance , and Indo - Aryan are genera of the Indo - European family . Language groups and corresponding languages are shown in Table 1 . Each group is selected based on a target language in the dataset , and the Afro - Asiatic family is added as an extra group . In experiments , lset in Algorithm 1 will be assigned languages from a specific family .", "entities": []}
{"text": "The tweet dataset that we constructed for disaster NLU was originally released by Appen 6 , and we use it to construct slot labels in two languages : English ( en ) and Haitian Creole ( ht ) . Data statement that includes annotator guidelines for the labeling jobs and other dataset information will be provided with the implementation . From a broader impact perspective , our code and developed models are open - source and allows NLP technology to be accessible to information systems for emergency services and social scientists in quickly deploying model during disaster events .", "entities": []}
{"text": "This work was partially supported by U.S. National Science Foundation grants IIS - 1815459 , IIS - 1657379 , and 2040926 . This work was also supported in part by the grant H - 4Q21 - 009 from the Commonwealth Cyber Initiative , an investment in the advancement of cyber R&D , innovation , and workforce development ( for more information about CCI , visit www.cyberinitiative . org ) . The authors are thankful to Ming Sun and Alexis Conneau for giving valuable insights on multilingual model training , as well as to the anonymous reviewers for their constructive feedback . We also acknowledge ARGO , a research computing cluster provided by the Office of Research Computing at George Mason University , were most experiments were conducted .", "entities": []}
{"text": "The Agriculture ministers of El Salvador and Honduras ... to control the spread of disease affecting poultry , like the virus Newcastle [ Disease ] . Urrutia ... to study the Newcastle outbreak . The disease has killed close to half a million Honduran chickens [ Victims ] in recent weeks . Honduras [ Country ] said this week it would halt the importation of chickens and eggs from Guatemala [ Country ] , where the disease has been detected earlier , and .....", "entities": []}
{"text": "Similar to the work of Kummerfeld and Klein ( 2013 ) , our error analysis approach is systemagnostic , i.e. it only uses system output and does not consider intermediate system decisions . This allows for error analysis and comparison across different kinds of systems - end - to - end or pipeline ; neural or pattern - based . Given inputs consisting of the system - predicted templates and gold standard templates ( i.e. desired output ) for every document in the target dataset , our error analysis tool operates in three steps . For each document , 1 . Perform an optimized mapping of the associated predicted templates and gold templates . 2 . Apply a pre - defined set of transformations to convert each system - predicted template into the desired gold template , keeping track of the transformations applied . 3 . Map the changes made in the conversion process to an IE - based set of error types . We describe each step in detail in the subsections below .", "entities": []}
{"text": "The transformations in Section 4.2 are mapped onto a set of IE - specific error types as shown in Figure 3 . In some cases , a single transformation maps onto a single error , while in others a sequence of transformations is associated with a single error . Full details are in Appendix A.", "entities": []}
{"text": "As new models for information extraction continue to be developed , we find that their predicted error types contain insights regarding their shortcomings . Analyzing error patterns within model predictions in a more fine - grained manner beyond scores provided by commonly used metrics is important for the progress of the field . We introduce a framework for the automatic categorization of model prediction errors for document - level IE tasks . We used the tool to analyze the errors of two state - of - theart models on three datasets from varying domains and compared the error profiles of these models to four of the earliest systems in the field on a dataset from that era . We find that state - of - the - art models , when compared to the earlier manual feature - based models , perform better at span extraction but worse at template detection and role assignment . With a better balance between precision and recall , the best early model outperforms the relatively highprecision , low - recall modern models . Missing role fillers remain the main source of errors , and scientific corpora are the most difficult for all systems , suggesting that improvements in these areas should be a priority for future system development .", "entities": []}
{"text": "We also provide example error types with the ProMED dataset .", "entities": []}
{"text": "We did not run the DyGIE++ model on the MUC - 4 dataset as the model output was made available to us by Xinya Du .", "entities": []}
{"text": "All code is made publicly available . 13 Exhaustive reproducibility details , including how to access all datasets , are provided in Appendix B. We fully adhere to the EMNLP 2020 Reproducibility guidelines , addressing all relevant checklist items .", "entities": []}
{"text": "For compression , we found sentence - level compression to be a naturally motivated metric given that many extractive systems are constrained to extract sentence - length sequence . We also considered byte - level compression as an alternative to word - level compression ( as computational length constraints have sometimes been used in evaluation instead of word length constraints ) . We found the results to be highly correlated with word - level compression and to not be further revealing ( and bytes may be inherently less interpretable for NLP when compared with words ) . We also considered only considering content words , motivated by literature in topic modelling ( Schofield et al , 2017 ) that has considered removing stopwords and other such lexical categories . These results were also highly correlated with the original word - level compression results and we did not find any discerning trends in looking at individual examples .", "entities": []}
{"text": "Our general framework for quantifying abstractivity is derived from Grusky et al ( 2018 ) . We considered p { 1 , 2 , 3 , 4 } initially and found p = 1 to be the most informative regarding abstractivity . In particular , we find that for increasing p , useful conclusions about abstractivity are inherently masked by the dominance of the | S i | p denominator in the definition . We report the scores for ABS 2 in Table 6 . We also considered the natural extensions to ABS 3 and ABS 4 but we found that the normalization dominates any deviation in the scores and all datasets essentially receive a score of 1 . We also considered other forms of normalization ( i.e. normalizing ABS 2 in the style of the L 2 norm / the style of generalized p - norms ) in initial experiments but found no substantial differences .", "entities": []}
{"text": "We provide precise and comprehensive details discussing all data , preprocessing and modelling decisions . All code will be made publicly available as noted in the main paper .", "entities": []}
{"text": "Fragments ( Grusky et al , 2018 ) were computed using the scripts released in that work for the purposes of estimating abstractivity . In the case of the NWS dataset , the authors already provide fragment - related scores which we use without recomputing these values .", "entities": []}
{"text": "In the main paper , we briefly discuss how we discovered that several of our metrics can serve the dual purpose of detecting generally low quality examples for example that achieve extreme scores . Figures 1 through 9 are several examples we found to be representative of the general structure of low quality examples for a given metric . In some cases , the trends are highly dataset - specific whereas in others they are more general . To facilitate reproducibility efforts , we provide all examples IDs we studied for each ( dataset , metric ) in Table 8 . Original Text ( truncated ) : Let us , in the beginning , give a word of cordial praise to the American publishers of these splendid volumes . The undertaking , in the first place , was an intellectual compliment to the country . It was based on the faith that there is in this country enough of philosophy and scholarship to justify a new and complete edition of . . . Summary : Let us , in the beginning , give a word of cordial praise to the American publishers of these splendid volumes . The undertaking , in the first place , was an intellectual compliment to the country .", "entities": []}
{"text": "Figure 1 : Dataset : NWS . This summary simply is the lede and we do not find it to be a useful summary for readers not familiar with the full context of the article . We hypothesize that such a summary may have been useful for members of a newsroom communicating information about the article to the other ( given their intimate familiarity with the article ) but this likely is inappropriate as a summary in most settings .", "entities": []}
{"text": "The entropy of a random variable X is defined as : H ( X ) \u2212 x p ( x ) log 2 p ( x ) Original Text ( truncated ) : A FULL - SERVICE hotel and conference center is to go up in the Lafayette Yard area of Trenton , giving the city a hotel for the first time since the 1980 's and bringing to an end its unenviable distinction as the only state capital without lodging for visitors . . .", "entities": []}
{"text": "Detector : Extremely Low Abstraction Figure 2 : Dataset : NYT . This summary simply conveys no useful information to someone who has not also read the reference document and simply is a word copied from the source document . It appears to be a label rather than a summary . Original Text ( truncated ) : a l\u00f3gica\u00e9 o estudo dos princ\u00edpios e crit\u00e9iros de infer\u00eancias e demonstra\u00e7\u00f5es v\u00e1lidas . um sistema l\u00f3gico\u00e9 composto por tr\u00eas partes : a sintaxe ( ou nota\u00e7\u00e3o ) , . . .", "entities": []}
{"text": ": logic is the science of correct inferences and a logical system is a tool to prove assertions in a certain logic in a correct way . . .", "entities": []}
{"text": "Detector : Extremely High Abstraction NYT . This summary is unlikely to be informative to someone who has not read the reference document and is more of a categorization / label than a summary . This is similar to the previous NYT example given . The conditional entropy of X given Y is defined as : Original Text ( truncated ) : Brodie ( the dog ) was neglected , and ended up with serious anger and health issues concerning his skin and allergies . My boyfriend adopted him . . . Summary : Onions . H ( X | Y ) y p (", "entities": []}
{"text": "In the main paper , we report the average score for each metric on each dataset . To complement reporting the mean , we report the standard deviation for each metric on each dataset in Table 9 .", "entities": []}
{"text": "In this section , we describe the onlinization of the offline model and propose two ways to control the quality - latency trade - off .", "entities": []}
{"text": "Depending on the language pair , translation tasks may require reordering or a piece of information that might not be apparent until the source utterance ends . In the offline setting , the model processes the whole utterance at once , rendering the strategy most optimal in terms of quality . If applied in online mode , this ultimately leads to a large latency . One approach to reducing the latency is to break the source utterance into chunks and perform the translation on each chunk . In this paper , we follow the incremental decoding framework described by Liu et al ( 2020a ) . We break the input utterance into small fixed - size chunks and decode each time after we receive a new chunk . After each decoding step , we identify a stable part of the hypothesis using stable hypothesis detection . The stable part is sent to the user ( \" committed \" in the following ) and is no longer changed afterward ( i.e. , no retranslation ) . 2 Our current implementation assumes that the whole speech input fits into memory , in other words , we are only adding new chunks as they are arriving . This simplification is possible because the evaluation of the shared task is performed on segmented input , on individual utterances . With each newly arrived input chunk , the decoding starts with forced decoding of the already committed tokens and continues with beam search decoding .", "entities": []}
{"text": "The limited context of the early chunks might result in an unstable hypothesis and an emission of erroneous tokens . The autoregressive nature of the model might cause further performance degradation in later chunks . One possible solution is to use longer chunks , but it inevitably leads to a higher latency throughout the whole utterance . To mitigate this issue , we explore a lengthening of the first chunk . We call this strategy an initial wait .", "entities": []}
{"text": "In this section , we describe the onlinization experiments .", "entities": []}
{"text": "In this section , we describe the experiments and discuss the results .", "entities": []}
{"text": "We experiment with chunk sizes of 250 ms , 500 ms , 1s , and 2 s. We combine the sizes of the chunks with different partial hypothesis selection strategies . The results are shown in Figure 1 . The results document that the chunk size parameter has a stronger influence on the trade - off than different prefix strategies . Additionally , this enables constant trade - off strategies ( e.g. , LA - 2 ) to become flexible .", "entities": []}
{"text": "We experiment with three strategies : hold - n ( withholds last n tokens ) , shared prefix ( SP - n ; finds the longest common prefix of all beams in n consecutive chunks ) and local agreement ( LA - n ; finds the longest common prefix of the best hypothesis in n consecutive chunks ) . For hold - n , we select n = 3 , 6 , 12 ; for SP - n , we select n = 1 , 2 ( n = 1 corresponds to the strategy by Nguyen et al ( 2021 ) ) ; for LA - n we select n = 2 , 3 , 4 ( n = 2 corresponds to the strategy by Liu et al ( 2020a ) ) . The results are in Figures 2 and 3 . Hold - n The results suggest ( see Figure 2 ) that the hold - n strategy can use either n or chunk size to control the quality - latency trade - off with equal effect . The only exception seems to be too low n < = 3 , which slightly underperforms the options with higher n and shorter chunk size . Local agreement ( LA - n ) The local agreement seems to outperform all other strategies ( see Figure 3 ) . LA - n for all n follows the same qualitylatency trade - off line . The advantage of LA - 2 is in reduced computational complexity compared to the other LA - n strategies with n > 2 . Shared prefix ( SP - n ) SP - 1 strongly underperforms other strategies in quality ( see Figure 3 ) . While the SP - 1 strategy performs well in the ASR task ( Nguyen et al , 2021 ) , it is probably too lax for the speech translation task . The generalized and more conservative SP - 2 performs much better . Although , the more relaxed LA - 2 , which considers only the best item in the beam , has a better qualitylatency trade - off curve than the more conservative SP - 2 .", "entities": []}
{"text": "In this paper , we reviewed onlinization strategies for end - to - end speech translation models . We identified the optimal stable hypothesis detection strategy and proposed two separate ways of the qualitylatency trade - off parametrization . We showed that the onlinization of the offline models is easy and performs almost on par with the offline run . We demonstrated that an improvement in the offline model leads to improved online performance . We also showed that our method outperforms a dedicated simultaneous system . Finally , we proposed an improvement in the average latency metric .", "entities": []}
{"text": "Ensembling of Distilled Models from Multi - task Teachers for Constrained Resource Language Pairs", "entities": []}
{"text": "Following the constrained track , we use bitext data provided in WMT21 for the following pairs : Bengali \u2194 Hindi , English \u2194 Hausa , Xhosa \u2194 Zulu and English \u2194 German . Statistics of the parallel data used for the three pairs in addition to the German helper are shown in Table 1 . We also use monolingual data for all previously mentioned languages provided in WMT21 for techniques such as multi - task training and back - translation . Statistics of the monolingual data used for the 6 languages in addition to the German helper are shown in Table 2 . For very low resource languages , Hausa , Xhosa and Zulu , we use all the available monolingual data , e.g. NewsCrawl + CommonCrawl + Extended CommonCrawl for Hausa , and Extended Common - Crawl for both Xhosa and Zulu . For relatively high resource languages , Bengali , Hindi , English and German , we only use a subset of the provided data mostly from NewsCrawl due to its high - quality . In addition to the NewsCrawl monolingual subset , we add a sampled subset from CommonCrawl to", "entities": []}
{"text": "The final MT system in each direction is an ensemble of two NMT models comprising a bilingual model ( one for each of the six primary directions ) and a multilingual model trained to provide translations for 8 directions ( the six primary directions plus English \u2194 German ) . The multilingual system uses a recently proposed multitask framework for training ( Wang et al , 2020 ) . We describe the individual systems in Subsection 3.1 . This is followed by presenting our system combination techniques in Subsection 3.2 . Finally we present the architecture of the submitted system highlighting our design decisions in Subsection 3.3 .", "entities": []}
{"text": "Our overall system is depicted in Figure 2 1 following the temperature - based strategy in ( Arivazhagan et al , 2019 ) to balance the training data in different resource languages using T = 5 . We pick the best system and use it to back translate the selected monolingual data . For most pairs , as detailed in Section 4 , we find that M T + DAE and M T + M LM + DAE are quite close . Therefore , we use the M T + DAE to do back translation for all submitted 6 pairs . We use beam search with beam size = 5 when generating the synthetic back - translated data . Once we get the back - translated data ( called BT 1 ) we add it to our parallel and monolingual data and build a new multilingual model called M T + DAE + BT 1 . We tag the back - translated data with < BT > tag at beginning of each source sentence so the model can differentiate between the genuine parallel and backtranslated data quality . The resulting model is used to regenerate the back - translated data ( called BT 2 ) and to knowledge distill the bitext ( called KD ) . The latter two data sets are augmented and used to build a bilingual system ( called M T + KD + BT 2 ) . We upsample the KD data set and the upsampling ratio is selected based on parameter sweeping and validating the resulting improvement on the validation set . Finally , the latter bilingual model is combined with our final multilingual model using the method in Section 3.2 to create our submission .", "entities": []}
{"text": "Understanding and Improving the Exemplar - based Generation for Open - domain Conversation", "entities": []}
{"text": "Exemplar - based generative models for opendomain conversation produce responses based on the exemplars provided by the retriever , taking advantage of generative models and retrieval models . However , due to the oneto - many problem of the open - domain conversation , they often ignore the retrieved exemplars while generating responses or produce responses over - fitted to the retrieved exemplars . To address these advantages , we introduce a training method selecting exemplars that are semantically relevant to the gold response but lexically distanced from the gold response . In the training phase , our training method first uses the gold response instead of dialogue context as a query to select exemplars that are semantically relevant to the gold response . And then , it eliminates the exemplars that lexically resemble the gold responses to alleviate the dependency of the generative models on that exemplars . The remaining exemplars could be irrelevant to the given context since they are searched depending on the gold response . Thus , our training method further utilizes the relevance scores between the given context and the exemplars to penalize the irrelevant exemplars . Extensive experiments demonstrate that our proposed training method alleviates the drawbacks of the existing exemplar - based generative models and significantly improves the performance in terms of appropriateness and informativeness . \u2020 Equal contribution * Corresponding author Given context A : Do you ever feel like time is just going by way too fast ? Retrieved exemplar B : It 's hard to get anything done with coworkers around . Generated responses B : Do you have any hobbies that you like to do while you 're at work ? like gardening ? ( a ) RetNRef ( Weston et al 2018 ) B : It 's hard to get things done with coworkers when you 're busy all the time . ( c ) RetNRef w/ CORGE ( Ours ) B : OMG ! especially recently . a week seems like one day . A : Yes ! Time especially goes by fast when I 'm working at my job . I 'm constantly busy . B : It 's hard to get around . anything done with coworkers gets to get done with anything .", "entities": []}
{"text": "( 1 ) We analyze the shortcomings of existing exemplar - based generative models derived from the nature of the opendomain conversation , the one - to - many problem . ( 2 ) We introduce a training method ( CORGE ) to improve the quality of generated responses by selecting useful exemplars and weighting the exemplars by relevance scores assessed by the retriever . ( 3 ) Through the human evaluation , we demonstrate that CORGE significantly improves the performance of exemplar - based generative models in terms of appropriateness and informativeness .", "entities": []}
{"text": "While generative models have shown remarkable performance on the open - domain conversation , it is well - known that generative models tend to yield uninformative and bland responses ( Li et al , 2016 ; Liu et al , 2016 ; Serban et al , 2017 ; Li et al , 2020 ; Holtzman et al , 2019 ; Welleck et al , 2019 ) . Exemplar - based generative models are introduced to overcome the aforementioned problem generative models suffer . Wu et al ( 2019 ) introduce an exemplar - based generative model for open - domain conversation , which retrieves a context - exemplar pair conditioned by the input context and encodes the lexical difference between the input context and the retrieved context to the edit vector . The response is produced by feeding the exemplar and the edit vector to the generator . ; Roller et al ( 2021 ) also retrieve the exemplar using the given context as a query and concatenate the exemplar with the context , then feed the concatenated exemplar into the generator to produce the final response for the open - domain conversation . Cai et al ( 2019a , b ) propose a method that removes the irrelevant information from the exemplar , then uses the masked exemplar to inform the generator to produce the response . Gupta et al ( 2021 ) condition the generator with the retrieved exemplars and the extracted semantic frames of the exemplars , which improves the coherence of generated responses . We do not consider this model as a baseline because their model requires an additional semantic frame extractor , and it can be mutually complemented with our proposed training method .", "entities": []}
{"text": "Let D = { ( c i , r i ) | 1 \u2264 i \u2264 n } denote the dialogue dataset , which consists of n pairs of context c and response r. Exemplar - based generative models are composed of two components : a retriever R and a generator G. For a given context c i , the retriever finds the top - scoring exemplar based on the relevance score S R ( z , c i ) of the exemplar z R , where R is a pre - defined response set . The generator computes the probability of the response for the context c i while utilizing the exemplar z as P G ( r | c i , z ) .", "entities": []}
{"text": "We hypothesize that selecting semantically relevant but lexically distanced exemplars from the gold response could solve the drawbacks above . To validate this hypothesis , we introduce a training method of exemplar - based generative models , called CORGE . Our proposed training method is illustrated in Figure 3 , and the illustrative examples about the exemplars selected by CORGE are described in Table 1 .", "entities": []}
{"text": "To verify the effectiveness of each component in CORGE , we conduct the ablation study . In Table 5 A Implementation Details", "entities": []}
{"text": "We prepare dialogue cases that have three - turn input contexts and the gold response from the BST and evaluate them by human pair - wise comparison and automatic evaluation . There are 980 test cases , and we randomly choose 100 test cases for the human evaluation .", "entities": []}
{"text": "As we described in Section 5.3 , we use Amazon Mechanical Turk to collect the annotations . Each test case is rated by three annotators to improve the robustness of the evaluation result . We set a maximum number of annotations per worker in order to reduce the potential bias . To control the quality of the annotations , we only allowed annotators who satisfy the following requirements to evaluate our results : ( 1 ) HITs approval rate greater than 95 % , ( 2 ) Location is one of Australia , Canada , New Zealand , United Kingdom , and the United States , ( 3 ) Lifetime number of HITs approved greater than 1000 , following Li et al ( 2018 ) . Figure 5 shows the instructions and the interface for the human evaluation . To mitigate the bias from the annotator , we randomly shuffle the order of the model and the corresponding response .", "entities": []}
{"text": "We provide additional samples for the retrieved exemplar and the model response from the baselines and our models in Table 6 .", "entities": []}
{"text": "A : I crash around 230 to 300 . I do n't have much of a social life . B : Neither do I , you are not alone A : I am alone but I 'm not lonely . I have a dog who 's great company .", "entities": []}
{"text": "No we went to a haunted house and I got chased often .", "entities": []}
{"text": "No we never got to go to the haunted house we went to once and got chased by a ghost . MatToGen + CORGE No , I don ' t like to go to haunted houses . I ' m scared of them .", "entities": []}
{"text": "Yes , I went to a haunted house and watched all the horror movies . It was so scary !", "entities": []}
{"text": "A : Oh God no ! I ca n't stay tied down to one woman . Why would you want to do that ? B : I know right ? Most people consider marriage to be involving 2 people but in certain parts of the world that varies between cultures and religions , so it does n't seem so bad A : Marriage is very good .", "entities": []}
{"text": "Benchmarking Hierarchical Script Knowledge", "entities": []}
{"text": "Our approach situates script learning as a case of grounding . For simplicity of exposition , let us assume there are three levels of abstraction to grounding : abstract concrete motor control . Most prior work in grounding treats language monolithically 1 and ignores the issue of audience . In practice , this means the task formulation or exposed API may implicitly bias the language to be more concrete . By viewing the task as purely linguistic , we have no API or robot that constrains our language ; instead , we define our audience as children . By eliciting child - directed instructions , we collect concrete language capturing otherwise implicit world knowledge that a child would not know . Because annotators assume a smart and capable but uninformed listener , we posit this language corresponds closely to the most \" concrete \" form in which language naturally occurs .", "entities": []}
{"text": "To investigate what new knowledge is being introduced and whether a model has captured it , we construct a cloze - style slot - filling task ( Chambers , 2017 ; Hermann et al , 2015 ) . We drop key content words from the concrete realization of an abstract instruction and ask the model to predict them . Several examples from the validation set are shown in Table 2 . Correctly predicting the missing words requires knowledge of the manner of executing a task and the tools required . To choose candidate words to drop , we only allow words that occur primarily in the concrete instructions . Additionally , we do not drop stop words , numbers , or words occurring fewer than five times . We do , however , drop units of measure ( cup , minute , etc . ) . This ensures we create blanks whose answers are previously omitted concrete details . Relatedly , under this filter the answer to a blank is very rarely an ingredient , as our goal is not to memorize recipes , but to infer the tool knowledge necessary to execute them . In total , we whitelist \u223c1 , 000 words that can be dropped to create blanks . We prefer longer blanks when available to give preference to compound nouns ( e.g. wire whisk ) . Finally , we do not drop any words ABS chop garlic into small pieces . CON put garlic on cutting board . press on back of knife with hand , cutting into small pieces . ABS add some parmesan cheese into the bowl and mix them well . CON use a grater to grate some parmesan cheese into the bowl . use a wire whisk to stir the cheese in . ABS add the tofu to the wok . CON drain the water from the tofu using a strainer . add the tofu into the pan . use a spoon to stir the tofu in the mixture . from the concrete sentence if they occur in the abstract description . This restriction eliminates any benefits that might have been achieved via models with copy mechanisms . Examples that do not meet our criteria are removed from the corpus .", "entities": []}
{"text": "We visualize alignments of our transduction model over two partial sequences in Fig . 2 . This shows which hidden vector of the abstract sentence aligned to every region of the concrete sequence . Specifically , we see how tools like the big bowl , spoon , and tongs are introduced to facilitate the actions . There are also implications , e.g. that high indicates grill . For further analysis we extract alignments over the training corpus , linking each decoded phrase with the word from the encoding it used during generation . We then aggregate these tuples into a table which we can filter ( based on our whitelist ) and sort ( with PMI ) . This process is imprecise as it discards the context in which the alignment occurs , but it nonetheless extracts many Abs shape each dough ball into a circle and add tomato sauce . Pred flatten out your dough into a flat circle using your hands . take a knife to add tomato sauce to the center of your dough . use the back side of the knife to cut the sauce out . make sure you keep the sauce about an inch from the edges . Gold flatten out your dough into a flat circle using your hands . take a spoon to add tomato sauce to the center of your dough . use the back side of the spoon to spread the sauce out . make sure you keep the sauce about an inch from the edges . Abs place the kale cucumber bell peppers carrots and radishes on the wrapper . Pred put the cut on a cutting . put a cutting amount of kale on the cutting . add a cut amount of cucumber ... Gold put the wrap on a plate . put a small amount of kale on the wrap . add a small amount of cucumber ...", "entities": []}
{"text": "We introduce a new hierarchical script learning dataset and cloze task in which models must learn commonsense world knowledge about tools , procedures and even basic physics to perform well . Our aim is to begin a conversation about abstraction in language , how it is modeled , and what is implicitly hidden . Our abstract and concrete instructions are grounded in the same videos yet differ dramatically due to their assumed audiences . We show that a neural transduction model produces interpretable alignments for analyzing these otherwise latent correlations and phenomena .", "entities": []}
{"text": "We briefly describe the model of Yu , Buys , and Blunsom ( 2016 ) and our minor modifications thereto .", "entities": []}
{"text": "We would like to thank the reviewers for insightful comments . This research was supported in part by an Amazon Research Award to K. Gimpel .", "entities": []}
{"text": "The organizer provided a dataset containing 687 memes for the training set , 63 memes for the development set , and 200 memes for the test set . The dataset of subtask 1 provides the ID , text of the meme , and the corresponding propaganda techniques used , and the dataset of subtask 3 also contains the corresponding meme image . The dataset of subtask 2 provides the ID , text of the meme , and the corresponding propaganda techniques used in a certain text fragment , in which the scope covered by the propaganda technology in the text is marked as \" start , \" \" end , \" and \" text fragment , \" respectively . The datasets were preprocessed using the following procedures before model training : In subtasks 1 and 3 , we first used one - hot encoding to encode the label into a vector whose length is the total number of technology categories . In subtask 2 , we labeled each token in the text as \" I - technique \" and \" O - technique \" based on the 20 propaganda technology terms . \" Itechnique \" indicates that the publicity technique was used and \" O - technique \" indicates that it was not , e.g. , O - Smears and I - Smears . For 20 different propaganda techniques there are 40 different codes , and then add another padding code , so the label code length is 41 . In subtask 3 , we normalized the meme image size to 224 \u00d7 224 \u00d7 3 .", "entities": []}
{"text": "The official evaluation measure for all subtasks is the micro F 1 - score , which is defined as follows : F 1 \u2212 score = 2 * P rec * Rec P rec + Rec ( 1 ) where Prec and Rec denote the precision and recall scores of all samples , respectively . For subtask 2 , the standard micro F 1 - score was slightly modified to account for partial matching between spans ( Dimitrov et al , 2021 ) . In addition , the macro F 1 - score was also reported for each type of propaganda .", "entities": []}
{"text": "Learning", "entities": []}
{"text": "This work was supported by the National Natural Science Foundation of China ( NSFC ) under Grants Nos . 61702443 , 61966038 and 61762091 .", "entities": []}
{"text": "Deep Learning and Sociophonetics : Automatic Coding of Rhoticity Using Neural Networks", "entities": []}
{"text": "Automated extraction methods are widely available for vowels ( Rosenfelder et al , 2014 ) , but automated methods for coding rhoticity have lagged far behind . R - fulness versus rlessness ( in words like park , store , etc . ) is a classic and frequently cited variable ( Labov , 1966 ) , but it is still commonly coded by human analysts rather than automated methods . Human - coding requires extensive resources and lacks replicability , making it difficult to compare large datasets across research groups ( Yaeger - Dror et al , 2008 ; Heselwood et al , 2008 ) . Can reliable automated methods be developed to aid in coding rhoticity ? In this study , we use Neural Networks / Deep Learning , training our model on 208 Boston - area speakers .", "entities": []}
{"text": "Despite advances in automation for phonetic alignment and extraction of vowel formants , there is still no reliable automated method for classifying r - dropping , that is , whether a given word is pronounced with an /r/ in words like park ( pahk ) , start ( staht ) , and so on . R - dropping , also known as non - rhotic speech , is an important sociolinguistic variable in modern dialect research . But unfortunately most researchers continue to depend on human judgments ( Nagy and Irwin , 2010 ; Becker , 2009 ; Nagy and Roberts , 2004 ) , which is an inconsistent and time - consuming method that lacks replicability . Turning to the field of machine learning , our deep learning approach investigates a new way to distinguish rhotic versus non - rhotic pronunciations in recorded data . This is the first study to use neural networks to classify rhotic versus non - rhotic speech . Although human - coding requires extensive resources and lacks consistency and replicability ( Yaeger - Dror et al , 2008 ; Heselwood et al , 2008 ) , making it difficult to compare large datasets across different research groups , it is the only method we have right now . How soon will computers be able to quickly and reliably code rhoticity up to this standard ? In terms of other machine learning approaches , McLarty , Jones , and Hall work on this challenge using Support Vector Machines ( SVMs ) ( Mclarty et al , 2018 ) . The present study uses Neural Networks / Deep Learning , one of the most effective and fastest - growing approaches in machine - learning . To our knowledge , this is the first attempt to use neural networks for automatic coding of any sociophonetic variable . This new method was developed using audio recordings from over 200 New England speakers from Boston , Maine , and central New Hampshire ( Stanford , forthcoming ) , and is here compared to other work on rhoticity ( Heselwood et al , 2008 ; Mclarty et al , 2018 ) . In what ways can neural networks be effective tools in assisting the coding of rhoticity ? To what level can they perform compared to traditional coding methods and other approaches ?", "entities": []}
{"text": "The phoneme /r/ has been particularly difficult to pin down because it may be articulated in different ways , yet still produce the same acoustic signal . As most phoneticians have come to agree , F3 is one of the primary acoustic correlates of rhoticity ( Espy - Wilson et al , 2000 ; Hagiwara , 1995 ; Thomas , 2011 ) . The general consensus is that the F3 measurement for /r/ is lower than that of other non - rhotic vowels , but reliable standards for coding rhoticity are lacking . In this paper , rhoticity will refer to post - vocalic realizations of the phoneme /r/ which do not occur before other vowels . For example , rhotic tokens of interest would include park and father but not marry . British phonetician John Wells used the term \" rhotic \" , which has been subsequently considered in the field as one of the most defining traits of varieties of English ( Wells , 1982 ) . Rhotic and non - rhotic dialects have been widely studied as they relate to sociolinguistic features of location , age , gender , and socioeconomic status . However , we are still reliant on human analysts to make judgements of rhotic vs. non - rhotic speech , which can require a lot of time and money . Despite advances in many areas of computational linguistics , there is still not an accurate way to determine rhoticity based on acoustic components alone ; a human must judge for themselves whether or not an /r/ has been dropped . As expected , this is not highly replicable as different speakers may perceive things differently especially when it comes to dialects that are not so clear - cut ( Yaeger - Dror et al , 2008 ) . For this reason , an automated way to determine rhotic / non - rhotic tokens would be especially helpful in these contexts . 3 Other work 3.1 Heselwood , Plug , and Tickle Heselwood et al ( 2008 ) extracted formant data from the spectrograms on the Bark scale - usually , formant data F2 / F3 is reported on the Hertz scale . The Bark scale more closely correlates to human perception of sounds , that is , on a logarithmic scale rather than absolute . After conversion , F2 was labeled Z2 and F3 was labeled Z3 , and a series of perceptual experiments were performed to ascertain rhoticity thresholds . Note that it was conducted for the purposes of perceptual research rather than coding applications .", "entities": []}
{"text": "In this initial study , we used Boston - area field recordings of 208 speakers , 100 tokens per speaker ( 107 women/101 men , born 1915 - 1997 ) . These on - the - street interviews ( 15 - 20 minutes each ) are typical sociolinguistic recordings in terms of speech styles ( word - list , sentences , reading passage , free speech ) and occasional background noise . We chose to omit free speech because its token variability between speakers would present another challenging factor , leaving us with recordings where participants were reading ( word - list , sentences , passage ) . Given word transcriptions , we used the Montreal Forced Aligner ( McAuliffe et al , 2017 ) and modified Praat scripts ( DiCanio , 2014 ; Koops , 2013 ) to align and extract vowel+ ( r ) sequences , e.g. , park , short . However , note that because non - rhotic dialects are less common , and some of our recordings had background noise , it could be possible that alignments were not perfect for all of our tokens . Two human analysts listened to recordings and judged each vowel+ ( r ) token as r - ful or r - less . The human analysts agreed on 89.9 % of the tokens , similar to human agreement elsewhere ( Nagy and Irwin , 2010 ) . Like other studies , we omitted tokens when the human analysts disagreed ( 10 % ) . So overall , 1700 tokens were discarded because of speaker disagreement , and 6500 rhotic tokens and 5300 non - rhotic tokens remained for analysis .", "entities": []}
{"text": "We are grateful for the anonymous reviewers of EMNLP who gave us very valuable comments and suggestions .", "entities": []}
{"text": "A Yes / no / maybe Answerability Not all naturally occuring question titles from PubMed are answerable by yes / no / maybe . The first step of annotating PQA - L ( as shown in algorithm 1 ) from pre - PQA - U is to manually identify questions that can be answered using yes / no / maybe . We labeled 1091 ( about 50.2 % ) of 2173 question titles as unanswerable . For example , those questions can not be answered by yes / no / maybe : \" Critical Overview of HER2 Assessement in Bladder Cancer : What Is Missing for a Better Therapeutic Approach ? \" ( wh - question ) \" Otolaryngology externships and the match : Productive or futile ? \" ( multiple choices )", "entities": []}
{"text": "Strictly speaking , most yes / no / maybe research questions can be answered by \" maybe \" since there will always be some conditions where one statement is true and vice versa . However , the task will be trivial in this case . Instead , we annotate a question using \" yes \" if the experiments and results in the paper indicate it , so the answer is not universal but context - dependent . Given a question like \" Do patients benefit from drug X ? \" : certainly not all patients will benefit from it , but if there is a significant difference in an outcome between the experimental and control group , the answer will be \" yes \" . If there is not , the answer will be \" no \" . \" Maybe \" is annotated when ( 1 ) the paper discusses conditions where the answer is True and conditions where the answer is False or ( 2 ) more than one intervention / observation / etc . is asked , and the answer is True for some but False for the others ( e.g. : \" Do Disease A , Disease B and/or Disease C benefit from drug X ? \" ) . To model uncertainty of the answer , we do n't strictly follow the logic calculations where such questions can always be answered by either \" yes \" or \" no \" .", "entities": []}
{"text": "An Expert Annotated Dataset for the Detection of Online Misogyny", "entities": []}
{"text": "We developed a hierarchical taxonomy with three levels . First , we make a binary distinction between Misogynistic content and Non - misogynistic content , which are mutually exclusive . Second , we elaborated subtypes of Misogynistic and Nonmisogynistic content . For Misogynistic content we defined four categories : ( i ) Misogynistic Pejoratives , ( ii ) descriptions of Misogynistic Treatment , ( iii ) acts of Misogynistic Derogation and ( iv ) Gendered Personal attacks against women . For Nonmisogynistic content we defined three categories : ( i ) Counter speech against misogyny , ( ii ) Nonmisogynistic personal attacks and ( iii ) None of the categories . Third , we included additional flags for some of the second level categories . Within both Misogynistic and Non - misogynistic content , the second level categories are not mutually exclusive , thereby allowing for multiple labels per entry . For instance , a Misogynistic entry could be assigned labels for both a Pejorative and Treatment . This taxonomy draws on the typologies of abuse presented by Waseem et al ( 2017 ) and Vidgen et al ( 2019 ) as well as theoretical work in online misogyny research ( Filipovic , 2007 ; Mantilla , 2013 ; Jane , 2016 ; Ging , 2017 ; Ging and Siapera , 2019 ; Farrell et al , 2019 ) . It was developed by reviewing existing literature on online misogyny and then iterating over small samples of the dataset . This deductive - inductive process allowed us to ensure that conceptually distinct varieties of abuse are separated and that different types of misogyny can be unpicked . This is important given that they can have very different impacts on victims , different causes , and reflect different outlooks and interests on the part of the speaker .", "entities": []}
{"text": "Misogynistic content directs abuse at women or a closely related gendered group ( e.g. feminists ) . This content can fall in to four non - mutually exclusive categories .", "entities": []}
{"text": "Misogynistic pejoratives are terms which are used to disparage women . It includes terms which are explicitly insulting and derogatory , such as ' slut ' or ' whore ' , as well as terms which implicitly express negativity or animosity against women , such as ' Stacy ' or ' Becky ' . For example , ' Stacy ' is a term used in the incel community to describe women considered attractive and unattainable , in opposition to a more average and attainable ' Becky ' ( Jennings , 2018 ) .", "entities": []}
{"text": "Misogynistic treatment is content that discusses , advocates , incites or plans negative or harmful treatment of women . It includes expressing intent to take action against women , as well as expressing desires about how they should be treated . Misogynistic treatment contains third - level subcategories : Threatening language and Disrespectful actions . 1 . Threatening language : Content which expresses an intent / desire to inflict / cause women to suffer harm , or expresses support for , encourages , advocates or incites such harm . It is an ' explicit ' form of abuse . It falls in to three thematic groups : ( a ) Physical violence : non - sexual physical violence such as killing , maiming , beating , etc . e.g. ' Feminists deserve to be shot ' . ( b ) Sexual violence : explicit sexual violence such as rape , penetration , molestation , etc . e.g. ' Someone should rape her - that would put her in her place ' . ( c ) Privacy : an invasion of privacy such as the disclosure of personal information ( i.e. doxing ) or threats to visit them . e.g. ' I know where you live , bitch ' .", "entities": []}
{"text": "Content which treats / portrays women as either lacking or not deserving independence / autonomy . This includes more subtly abusive statements about how women should be treated and what they should be allowed to do . It is an ' implicit ' form of abuse . It falls in to four thematic groups : ( a ) Controlling : suggesting or stating that women should be controlled in some way , especially by a man or men . E.g. ' I would never let my girlfriend do that ' . ( b ) Manipulation : using or advocating the use of tactics such as lying and gaslighting to manipulate what women do or think . E.g. ' Told my last girlfriend she was hallucinating when she saw the texts from my side piece ' . ( c ) Seduction and conquest : discussing woman solely as sexual conquests or describing previous incidences of when they have been treated as such . E.g. ' Got her home and used her so hard ' . ( d ) Other : content that is not covered by the other subcategories .", "entities": []}
{"text": "Misogynistic derogation is content that demeans or belittles women . This content can be explicitly or implicitly abusive . It is separated into third - level subcategories : 1 . Intellectual inferiority : making negative judgements of women 's intellectual abilities , such as a lack of critical thinking or emotional control . This includes content which infantilizes women . An implicit example would be ' My gf cries at the stupidest shit - lol ! ' for suggesting irrational emotional responses . An explicit example is ' Typical stupid bitchtalking about things she does n't understand ' . 2 . Moral inferiority : making negative judgements of women 's moral worth , such as suggesting they are deficient or lesser to men in some way . This includes subjects such as superficiality ( e.g. only liking men who are rich or attractive ) , promiscuity , and untrustworthiness . An implicit example is ' Girls love your money more than you ' . An explicit example is ' My ex - girlfriend was a whore , she slept with every guy she saw ' . 3 . Sexual and/or physical limitations : making negative judgements of women 's physical and/or sexual ability . This includes perceived unattractiveness ( i.e. a lack of sexual desirability ) , ugliness ( i.e. a lack of beauty ) , frigidness ( i.e. a lack of sexual willingness ) , as well as belittling statements about feminine physical weakness . An implicit example is ' I gave it my A - game but she would not give in , so uptight ! ' An explicit example is ' Yikes , Dianne Abbott looks like a monkey ! ' 4 . Other : content that is not covered by the other subcategories but is derogatory towards women .", "entities": []}
{"text": "Gender personal attacks are highly gendered attacks and insults . This category is used only when the nature of the abuse is misogynistic , e.g. ' Hilary Clinton is such a stupid bitch , someone should give her a good fucking and put her in her place ' . The category has a level three flag for the gender of the recipient of the abuse . We include this flag as research has shown that men can also be targeted by misogynistic attacks ( Jane , 2014 ) . The gender can either be a woman ( e.g. ' That chick is dumb ' ) , a man ( e.g. ' This dude is a piece of shit ' ) or unknown ( e.g. ' You 're are an idiot , fuck off ' ) . If the content was replying to an entry which reveals the recipient 's gender we can infer it from this context . For example if ' You 're an idiot , fuck off ' was a response to ' I 'm a man and a feminist there 's nothing contradictory about that ' we know the abuse is targeted at a man .", "entities": []}
{"text": "Non - misogynistic content can fall in to three nonmutually exclusive categories , all of which are relevant for misogyny research .", "entities": []}
{"text": "Interpersonal abuse which is not misogynistic . We include this category to allow for a comparison of the nature of abuse directed at women and men ( Duggan , 2017 ) . It includes content which personally attacks a woman but is not misogynistic in nature , e.g. ' Hilary Clinton has no clue what she 's talking about , idiot ! ' . It uses the same level three flag for the gender of the recipient as Misogynistic personal attack . This allows us to compare the rates of personal attacks against women and men . Note that although it is possible for an entry to contain both Misogyny and a Non - misogynistic personal attack , this was very rare . In such cases , we chose to not annotate the Non - misogynistic personal attack in order to keep the first level as a binary distinction .", "entities": []}
{"text": "Counter speech is content which challenges , refutes , and puts into question previous misogynistic abuse in a thread . It could directly criticise previous abuse ( e.g. ' What you said is unacceptable ' ) , specifically accuse it of prejudice ( e.g. ' That 's incredibly sexist ' ) , or offer a different perspective which challenges the misogyny ( e.g ' That 's not how women act , you 're so wrong ' ) .", "entities": []}
{"text": "Fleiss Our taxonomy has seven partially overlapping categories , and as such annotation is considerably more difficult compared with most prior work , which tends to involve only binary labelling . As such , whilst slightly low , we believe that our agreement scores show the robustness of our annotation approach . Further , all disagreements were then discussed with an expert adjudicator , meaning that points of disagreement were addressed before the final labels were determined .", "entities": []}
{"text": "Of the 6 , 567 agreed labels in the final dataset 10.6 % are Misogynistic ( n=699 ) and 89.4 % are 868 ) . Tables 2 and 3 show the number of labels in the final dataset for each of the Misogynistic and Non - misogynistic categories , broken down by the level two categories . The vast majority of entries fall under None of the categories ( 88.6 % of all labels ) . The next most common category is Misogynistic Pejoratives followed by Misogynistic Derogation pejoratives ( 4.2 % ) . There are relatively few labels for Personal attacks with just 0.7 % in total for each of the Misogynistic and Nonmisogynistic categories , respectively . The least common category is Counter speech against misogyny , with only ten cases ( 0.2 % ) .", "entities": []}
{"text": "Number", "entities": []}
{"text": "Annotators identified at least one misogynistic pejorative in 4.2 % of all entries . The most common misogynistic term in the labels is ' bitch ' ( n=43 ) followed by ' stacy ' ( 24 ) and ' stacies ' ( 21 ) .", "entities": []}
{"text": "There are 103 labels of Treatment . Figure 1 shows the number of labels for each level three subcategory . There are almost five times as many labels for Disrespectful actions ( n=85 ) than Threatening language ( n=18 ) . Both level three subcategories were broken down into more specific misogynistic themes . Within Disrespectful actions , Seduction and conquest is the most common topic , with twice as many labels as the second most common , Controlling ( 43 vs 17 ) . And , within Threatening language , Physical violence was the most common theme ( 13 Sexual violence and Invasion of privacy only have a couple of labels each ( three and two , respectively ) .", "entities": []}
{"text": "The are 286 Derogation labels .", "entities": []}
{"text": "Table 5 shows the breakdown of both Misogynistic and Nonmisogynistic personal attacks . Slightly more than half ( 55 % ) of interpersonal abuse was not misogynistic . Of these women were still the target of the abuse almost four times as often as men ( n=32 vs n=8 ) . And women were as likely to receive misogynistic person attacks as non - misogynistic ones ( n=32 ) . The gender of the target was only unknown in 5 % of cases , one misogynistic and three not . There were two cases of misogynistic abuse against men . All other misogynistic personal attacks were towards women .", "entities": []}
{"text": "There are only 10 cases of Counter speech in the final dataset of agreed labels . Annotators originally identified far more counter speech ( 188 labels for 149 unique entries were initially made ) but few were accepted during the adjudication meetings . In Section 5.2 we showed that the category has one of the lowest Kappa values . Notably , 39 % of original Counter speech labels were made by one annotator , showing that the annotators had different understandings of the threshold for Counter speech . However , the number of original labels for Counter speech decreased over the first few weeks of the annotation process , as shown in Figure 2 . This reflects the complexity of the category ; it took annotators time to differentiate content that was pro - women from that which actually countered previous misogynistic speech .", "entities": []}
{"text": "Of the 86 entries misclassified as Misogynistic , three are labelled as Nonmisogynistic personal attacks . An example is \" Male feminist reacts to vile scumbag who murdered his kids by telling ALL men to , you know , stop killing women and kids ... \" . The use of aggressive language combined with gendered phrases ( such as \" stop killing women \" ) likely led to its misclassification . The remaining 83 false positives fall under None of the categories and all contain some reference to women . Some refer to topics often associated with misogyny but are not misogynistic in themselves . For example , a comment in r / seduction stated , \" the most manly thing is to find your dream woman , marry her , and live happily ever after . The constant sex with women is so overrated anyways \" . This entry , which suggests that other things than high levels of sexual activity should be prioritised , is thematically similar to misogynistic content in the dataset . Other false positives mention women indirectly . \" Because they are n't men , they are SIMPS \" . ' Simp ' is a pejorative term used in the manosphere for a man who cares too much about a woman . Under our taxonomy it did not count as a misogynistic pejorative but it is likely that the term appears in misogynistic entries in the dataset . Some false positives are critical of misogyny , though not actively enough to count as Counter speech . For example \" Does this moid even know the meaning of the term ' butterface ' ? If this woman is ugly , there is no hope for most of the female population . \" . This discussion of unrealistic beauty standards of women references misogyny but is not itself misogynistic .", "entities": []}
{"text": "Of the 51 Misogynistic entries the model misses , almost half ( n=24 ) contain Derogation . Implicit and explicit derogation are missed at roughly similar rates , as are each of the subcategories . Importantly this shows that the different forms of derogation are no more or less likely to be missed .", "entities": []}
{"text": "All annotators were based in the United Kingdom and worked remotely . They were paid \u00a3 14 per hour for all work including training . Five of the six annotators gave permission to share their basic demographic information . All were between 18 and 29 years old . Two had high school degrees , two had an undergraduate degree , and one had a postgraduate taught degree or equivalent . Four identified as women , one as a man . All were British nationals , native English speakers , and identified as ethnically white . All annotators used social media at least once per day . Two had never been personally targeted by online abuse , two had been targeted 2 - 3 times ( in separate instances more than a year ago ) , and one had been personally targeted more than 3 times within the previous month .", "entities": []}
{"text": "Table 9 lists the subreddits used for target sampling of data . The columns Num entries and Num threads state how many individual entries and threads from each subreddit are in the datasets . The column Selection shows whether the subreddit was identified from existing literature , which is cited , or using snowball sampling .", "entities": []}
{"text": "( 1 )", "entities": []}
{"text": "( 1.1 )", "entities": []}
{"text": "This research was funded by Wave 1 of The UKRI Strategic Priorities Fund under the EPSRC Grant EP / T001569/1 at The Alan Turing Institute , particularly the \" Tools , Practices and System \" and \" Criminal Justice \" themes .", "entities": []}
{"text": "Where Are We in Discourse Relation Recognition ?", "entities": []}
{"text": "We would like to thank the reviewers , Diane Litman and Matthew Stone for providing helpful feedback for this work .", "entities": []}
{"text": "In the standard paradigm of MNMT , all parameters are shared across languages and the model is jointly trained on multiple language pairs . We follow Johnson et al ( 2017 ) to reuse standard bilingual NMT models for multilingual translation by altering the source input with a language token lang , i.e. changing x to x = ( lang , x 1 , . . . , x J ) .", "entities": []}
{"text": "Our goal is to build a unified model , which can achieve good performance on all language pairs . The main idea of our method is that different neurons have different importance to the translation of different languages . Based on this , we divide them into general and language - specific ones and make general neurons participate in the translation of all the languages while language - specific neurons focus on some specific languages . Specifically , the proposed approach involves the following steps shown in Figure 1 . First , we pretrain the model on the combined data of all the language pairs following the normal paradigm in Johnson et al ( 2017 ) . Second , we evaluate the importance of different neurons on these language pairs and allocate them into general neurons and language - specific neurons . Last , we fine - tune the translation model on the combined data again . It should be noted that for a specific language pair only the general neurons and the language - specific neurons for this language pair will participate in the forward and backward computation when the model is trained on this language pair . Other neurons will be zeroed out during both training and inference .", "entities": []}
{"text": "In this step , we should determine which neurons are shared across all the language pairs and which neurons are shared only for some specific language pairs .", "entities": []}
{"text": "Except for the general neurons shared by all the language pairs , our method allocates other neurons to different language pairs based on their importance . These language - specific neurons are important for preserving the language - specific knowledge . To better understand the effectiveness of our method , we will show how these specific neurons are distributed in the model . To evaluate the proportion of language - specific neurons for different language pairs at each layer , we introduce a new metric , LScore , formulated as : LScore ( l , m ) = \u0128 m l I l , m { 1 , . . . , M } ( 10 ) where\u0128 m l denotes the number of neurons allocated to language pair m in the l - th layer , and\u0128 l denotes the total number of the language - specific neurons in the l - th layer . The larger the LScore , the more neurons allocated to the language pair m. We also introduce a metric to evaluate the average proportion of language - specific neurons of each language in different modules , which formulated as : MScore ( l , f ) = 1 M M m=0\u0128 m l , f I l , f , m { 1 , . . . , M } ( 11 ) where\u0128 m l , f denotes the number of specific neurons for language pair m of in the f module of the lth layer and M denotes the total number of the language pair . The larger the MScore is , the more specific neurons are allocated to different language pairs in this module . As shown in Figure 3 ( a ) and Figure 3 ( b ) , the language pairs have low LScores at the top and bottom layers and high LScores at the middle layers of both the encoder and decoder . The highest LScore appears at the third or fourth layers , which indicates that the neuron importance of different language pairs is similar and the neurons of the middle layers are shared by more languages . As a contrast , the bottom and top layers will be more specialized for different language pairs . Next , from Figure 3 ( c ) and Figure 3 ( d ) , we can see the MScores of the attention modules are almost near 1.0 , which means neurons in self attention and cross attention are almost shared across all language pairs . However , the MScores of Feed Forward Network ( FFN ) gradually decrease as layer depth increases and it shows that the higher layers in FFN are more essential for capturing the language - specific knowledge .", "entities": []}
{"text": "In the proposed method we allocate neurons based on importance of language pair . There are three varieties of our method : ( a ) Source - Specific , share all neurons according to the source language only ; ( b ) Target - Specific , share all neurons according to the target language only ; ( c ) Separate Enc - Dec , Encoder neurons are shared according to the source language and decoder neurons are shared according to the target language . Note that ( c ) is different from our method since ( c ) is separate neurons to two parts ( encoder and decoder ) and then connect specific neurons of the two parts to form a whole , while our method is directly based on language pairs . As shown in Figure 6 , we compare our Taylor Expansion method with the other three varieties . Our approach outperforms other varieties on almost all language pairs , and the performance of the language - pair based approach is undoubtedly the best . The second is based on the target language and the source language . Worst of all are the separated encoder - decoder , which may be due to the mismatch between the neurons of the encoder and decoder when they are reconnected .", "entities": []}
{"text": "We conducted several experiments on \u03c1 to determine the optimal hyper - parameter , so as to determine the proportion of universal neurons . As shown in Table 3 , when \u03c1 = 90 % the model gets the best translation result and reach best trade - off between general and language - specific neurons .", "entities": []}
{"text": "We thank all the anonymous reviewers for their insightful and valuable comments . This work was supported by National Key R&D Program of China ( NO . 2017YFE0192900 ) .", "entities": []}
{"text": "Neural Topic Modeling by Incorporating Document Relationship Graph", "entities": []}
{"text": "The authors would like to thank the anonymous reviewers for insightful comments and helpful suggestions . This work was funded in part by the National Key Research and Development Program of China ( 2016YFC1306704 ) and the National Natural Science Foundation of China ( 61772132 ) .", "entities": []}
{"text": "NeuInfer : Knowledge Inference on N - ary Facts", "entities": []}
{"text": "Knowledge inference on knowledge graph has attracted extensive attention , which aims to find out connotative valid facts in knowledge graph and is very helpful for improving the performance of many downstream applications . However , researchers have mainly poured attention to knowledge inference on binary facts . The studies on n - ary facts are relatively scarcer , although they are also ubiquitous in the real world . Therefore , this paper addresses knowledge inference on n - ary facts . We represent each n - ary fact as a primary triple coupled with a set of its auxiliary descriptive attribute - value pair ( s ) . We further propose a neural network model , NeuInfer , for knowledge inference on n - ary facts . Besides handling the common task to infer an unknown element in a whole fact , NeuInfer can cope with a new type of task , flexible knowledge inference . It aims to infer an unknown element in a partial fact consisting of the primary triple coupled with any number of its auxiliary description ( s ) . Experimental results demonstrate the remarkable superiority of NeuInfer .", "entities": []}
{"text": "Different from the studies that define n - ary relations first and then represent n - ary facts ( Wen et al , 2016 ; Zhang et al , 2018 ) , we represent each n - ary fact as a primary triple ( head entity , relation , tail entity ) coupled with a set of its auxiliary description ( s ) directly . Formally , given an n - ary fact F ct with the primary triple ( h , r , t ) , m attributes and attribute values , its representation is : ( h , r , t ) , { | \u2212\u2212 a1 : v1 , | \u2212\u2212 a2 : v2 , | \u2212\u2212 . . . , | \u2212\u2212 am : vm } , where each a i : v i ( i = 1 , 2 , . . . , m ) is an attributevalue pair , also called an auxiliary description to the primary triple . An element of F ct refers to h / r / t / a i /v i ; A F ct = { a 1 , a 2 , . . . , a m } is F ct 's attribute set and a i may be the same to a j ( i , j = 1 , 2 , . . . , m , i = j ) ; V F ct = { v 1 , v 2 , . . . , v m } is F ct 's attribute value set . For example , the representation of the 5 - ary fact , mentioned in Section 1 , is : Note that , in the real world , there is a type of complicated cases , say , where more than two entities participate in the same n - ary fact with the same primary attribute . We follow Wikidata ( Vrande\u010di\u0107 and Kr\u00f6tzsch , 2014 ) to view the cases from different aspects of different entities . Take the case that John Bardeen , W alter Houser Brattain , and W illiam Shockley received N obel P rize in P hysics in 1956 for example , besides the above 5 - ary fact from the view of John Bardeen , we get other two 5 - ary facts from the views of W alter Houser Brattain 2 and W illiam Shockley 3 , respectively : ( W alter Houser Brattain , award - received , N obel", "entities": []}
{"text": "In this paper , we handle both the common simple knowledge inference and the newly proposed flexible knowledge inference . Before giving their definitions under our representation form of n - ary facts , let us define whole fact and partial fact first . Definition 1 ( Whole fact and partial fact ) . For the fact F ct , assume its set of auxiliary description ( s ) as S d = { a i : v i | i = 1 , 2 , . . . , m } . Then a partial fact of F ct is : F ct = ( h , r , t ) , S d , where S d \u2282 S d , i.e. , S d is a subset of S d . And we call F ct the whole fact to differentiate it from F ct . Notably , whole fact and partial fact are relative concepts , and a whole fact is a relatively complete fact compared to its partial fact . In this paper , partial facts are introduced to imitate a typical openworld setting where different facts of the same type may have different numbers of attribute - value pair ( s ) . Definition 2 ( Simple knowledge inference ) . It aims to infer an unknown element in a whole fact . Definition 3 ( Flexible knowledge inference ) . It aims to infer an unknown element in a partial fact .", "entities": []}
{"text": "To conduct knowledge inference on n - ary facts , NeuInfer first models the validity of the n - ary facts and then casts inference as a classification task . In the above first n - ary fact , the primary triple is invalid . In the second one , some auxiliary description is incompatible with the primary triple . Therefore , we believe that a valid n - ary fact has two prerequisites . On the one hand , its primary triple should be valid . If the primary triple is invalid , attaching any number of attribute - value pairs to it does not make the resulting n - ary fact valid ; on the other hand , since each auxiliary description presents a qualifier to the primary triple , it should be compatible with the primary triple . Even if the primary triple is basically valid , any incompatible attribute - value pair makes the n - ary fact invalid . Therefore , NeuInfer is designed to characterize these two aspects and thus consists of two components corresponding to the validity evaluation of the primary triple and the compatibility evaluation of the n - ary fact , respectively .", "entities": []}
{"text": "We perform an ablation study to look deep into the framework of NeuInfer . If we remove the compatibility evaluation component , NeuInfer is reduced to a method for binary but not n - ary facts . Since we handle knowledge inference on n - ary facts , it is inappropriate to remove this component . Thus , as an ablation , we only deactivate the validity evaluation component , denoted as NeuInfer \u2212 . The experimental comparison between NeuInfer and NeuInfer \u2212 is illustrated in Figure 2 . It can be observed from the figure that NeuInfer outperforms NeuInfer \u2212 significantly . It suggests that the validity evaluation component plays a pivotal role in our method . Thus , each component of our method is necessary .", "entities": []}
{"text": "The newly proposed flexible knowledge inference focuses on n - ary facts of arities greater than 2 . It includes flexible entity inference and flexible relation inference . For an n - ary fact , they infer one of the entities / the relation in the primary triple given any number of its auxiliary description ( s ) or infer the attribute value / attribute in an auxiliary description given the primary triple and any number of other auxiliary description ( s ) . In existing knowledge inference methods on n - ary facts , each n - ary fact is represented as a group of peer attributes and attribute values . These methods have not poured attention to the above flexible knowledge inference . Thus , we conduct this new type of task only on NeuInfer . Before elaborating on the experimental results , let us look into the new test set used in this section first .", "entities": []}
{"text": "We generate the new test set as follows : Collect the n - ary facts of arities greater than 2 from the test set . For each collected n - ary fact , compute all the subsets of the auxiliary description ( s ) . The primary triple and each subset form a new n - ary fact , which is added to the candidate set . Remove the n - ary facts that also exist in the training / validation set from the candidate set and then remove the duplicate n - ary facts . The remaining n - ary facts form the new test set . The size of the resulting new test set on JF17 K is 34 , 784 , and that on WikiPeople is 13 , 833 .", "entities": []}
{"text": "The experimental results of flexible entity and relation inference on these new test sets are presented in Table 4 . It can be observed that NeuInfer well tackles flexible entity and relation inference on partial facts , and achieves excellent performance . We also attribute this to the reasonable modeling of n - ary facts . For each n - ary fact , NeuInfer distinguishes the primary triple from other auxiliary description ( s ) and models them properly . Thus , NeuInfer well handles various types of entity and relation inference concerning the primary triple coupled with any number of its auxiliary description ( s ) .", "entities": []}
{"text": "The work is supported by the National Key Research and Development Program of China under grant 2016YFB1000902 , the National Natural Science Foundation of China under grants U1911401 , 61772501 , U1836206 , 91646120 , and 61722211 , the GFKJ Innovation Program , Beijing Academy of Artificial Intelligence ( BAAI ) under grant BAAI2019ZD0306 , and the Lenovo - CAS Joint Lab Youth Scientist Project .", "entities": []}
{"text": "Question - Answering systems ( QAS ) aim at analyzing and processing user questions in order to provide relevant answers ( Hirschman and Gaizauskas , 2001 ) . The recent popularity of intelligent assistants has increased the interest in QAS which have become a key component of \" Human - Machine \" exchanges since they allow users to have instant answers to their questions in natural language using their own terminology without having to go through a long list of documents to find the appropriate answers . Most of the existing research work focuses on the major complexity of these systems residing in the processing and interpretation of the question that expresses the user 's need for information , without considering the representation of the answer itself . Usually , the answer is either represented by a short set of terms answering exactly the question ( case of QAS which extract answers from structured data ) , or by a text span extracted from a document which , besides the exact answer , can integrate other unnecessary information that are not relevant to the context of the question asked . The following presents two answers for Who is the thesis supervisor of Albert Einstein ? possibly generated by two systems :", "entities": []}
{"text": "We have put forward , in this paper , an approach for Natural Language Generation within the framework of the question - answering task that considers dependency analysis and probability distribution of words sequences . This approach takes part of a question / answering system in order to help generate a user - friendly answer rather than a short one . The results obtained through a human evaluation and standard metrics tested over French and English questions are very promising and shows a good correlation with human judgement . However , we intend to put more emphasis on the Language Model choice as reported by the human study and consider the generation of more than one missing word within the answer .", "entities": []}
{"text": "How Do We Answer Complex Questions : Discourse Structure of Long - form Answers", "entities": []}
{"text": "We have a two - stage annotation process : annotators first determine the validity of the QA pair , and proceed to discourse annotation only if they consider the QA pair valid . We define the QA pair as valid if ( 1 ) the question is interpretable , ( 2 ) the question does not have presuppositions rejected by the answer , ( 3 ) the question does not contain more than one sub - question , and ( 4 ) the proposed answer properly addresses the question . Examples of the invalid QA pair identified are in A.1 . 6 We collect the first stage annotation from USbased crowdsource workers on Amazon Mechanical Turk and second stage annotation from undergraduate students majoring in linguistics , who are native speakers in English . 7 A total of 29 crowdworker participated in our task , and six undergraduates annotated roles for a subset of QA pairs annotated as valid by crowdworkers . We first qualified and then provided training materials to both groups of annotators . The annotation guideline and interface can be found in A.4 . We paid crowd workers $ 0.5 per example , and our undergraduate annotators $ 13 / hour . More details of data collection can be found in our datasheet . ples and role annotations for about half of them .", "entities": []}
{"text": "As our tasks are complex and somewhat subjective , we collected three way annotations . We consider a QA pair valid if all annotated it as valid , and invalid if more than two annotated it as invalid . If two annotators considered valid , we collect one additional annotation and consider it valid if and only if the additional annotator marked it as valid . 8 We consider the majority role ( i.e. chosen by two or more than two annotators ) as the gold label . When all annotators chose different roles , they resolved the disagreement through adjudication . We report inter - annotator agreement before the adjudication . Inter - annotator Agreement We find modest to high agreement for both annotation tasks : For crowdworkers , Fleiss Kappa was 0.51 for validity annotation . For student annotators , Fleiss Kappa was 0.44 for role annotation . Figure 2 shows the confusion matrix between pairs of annotations , with the numbers normalized by row and averaged across pairs of annotators . We observe frequent confusion between roles denoting different levels of information salience - Answer vs. Answer - Summary , and Answer vs. Auxiliary Information , reflecting the nuance and subjectivity in judging what information is necessary to answer a complicated question . Examples can be found in A.2 . 8 The Fleiss kappa for agreement improves to 0.70 after this re - annotation process .", "entities": []}
{"text": "WebGPT", "entities": []}
{"text": "Having analyzed discourse roles of human - written long - form answers , we investigate the discourse structure of model - generated answers . This will allow us to quantitatively study the difference in terms of discourse structure across gold and generated answers , which we hope will cast insights to the linguistic quality of system outputs .", "entities": []}
{"text": "We study how models can identify the discourse role for each sentence in long - form answer in a valid QA pair . 10 Such a model can be beneficial for large - scale automatic analysis .", "entities": []}
{"text": "We present a linguistically motivated study of longform answers . We find humans employ various strategies - introducing sentences laying out the structure of the answer , proposing hypothetical and real examples , and summarizing main points - to organize information . Our discourse analysis characterizes three types of long - form answers and reveals deficient discourse structures of modelgenerated answers . Discourse analysis can be fruitful direction for evaluating long - form answers . For instance , highlighting summary sentence ( s ) or sentence - level discourse role could be helpful for human evaluators to dissect long - form answers , whose length has been found to be challenging for human evaluation ( Krishna et al , 2021 ) . Trained role classifier can also evaluate the discourse structure of model - generated answers . Future work can explore using sentences belonging to the summary role to design evaluation metrics that focuses on the core parts of the answer ( Nenkova and Passonneau , 2004 ) , for assessing the correctness of generated the answer . Exploring controllable generation , such as encouraging models to provide summaries or examples , would be another exciting avenue for future work .", "entities": []}
{"text": "If you are n't married you are not legally a part of that person 's life , so any legal or medical decisions would be up to the parents of that individual . Answer Auxiliary 3 That 's why marriage equality was important a few years ago . Auxiliary", "entities": []}
{"text": "If someone was with their partner for 15 years and then suddenly dropped dead , their partner had better hope their in - laws liked them or even supported the partnership in the first place . Example Auxiliary", "entities": []}
{"text": "Figure 5 , 6 , 7 , 8 9 , and 10 show the annotation guideline as well as interface presented to the annotators ( we present Step 1 for crowdworkers , Step 2 and Step 3 for student annotators ) . We did n't capture the extended example section as well as FAQ here due to space . Figure 10 : Screenshot of annotation interface for sentence - level role , as well as summary sentence selection .", "entities": []}
{"text": "This work was partially supported by NSF grants IIS - 1850153 , IIS - 2107524 . We thank Kalpesh Krishna and Mohit Iyyer for sharing the model predictions and human evaluation results . We would like to thank Tanya Goyal , Jiacheng Xu , Mohit Iyyer , anonymous reviewers and meta reviewer for providing constructive feedback to improve the draft . Lastly , we thank Maanasa V Darisi , Meona Khetrapal , Matthew Micyk , Misty Peng , Payton Wages , Sydney C Willett and crowd workers for their help with the complex data annotation .", "entities": []}
{"text": "A.1 Invalid QA We provide definitions , as well as examples of each invalid QA type . No valid answer The answer paragraph does n't provide a valid answer to the question . [ Q ] : How does drinking alcohol affect your ability to lose weight ? [ A ] : Alcohol itself is extremely calorically dense . Doesn't really matter whether you 're drinking a light beer or shots , alcohol itself has plenty of calories . Just think of every three shots as eating a mcdouble , with even less nutritional value .", "entities": []}
{"text": "The question is nonsensical and it is unclear what is asked . [ Q ] : asia vs rest of the world cricket match Multiple questions asked More than one question are asked in the question sentence . [ Q ] : what is a limpet and where does it live Assumptions in the question rejected The answer focuses on rejecting assumptions in the question , without answering the question . [ Q ] : Why is it that as we get older , we are able to handle eating hotter foods [ A ] : I 'm not sure I accept the premise . Children in cultures where spicy food is common , think nothing of it . My nephews had no problem eating hot peppers when they were very young because it was just a normal part of their diet . [ ... ]", "entities": []}
{"text": "We include example role annotations in Table 6 which demonstrate disagreement between Auxiliary Information and the Answer role . Sentence 2 in answer ( a ) was annotated as answer by most of the annotators as it elaborates on becoming a legal ' next of kin ' by providing a counterfactual scenario . One annotator annotated it as auxiliary as it touches upon how the decisions would be up to the parents , which goes beyond what is asked in the question . For answer ( b ) , while most annotators think that sentence 1 is of Answer role , one annotator annotated it as Auxiliary Information which only talks about the property of purple .", "entities": []}
{"text": "In Section 3.1 , we introduce two kinds of wordlevel sentiment annotation , i.e. , soft and hard sentiment annotation . We now compare two methods . The results are reported in Tables 5 and 6", "entities": []}
{"text": "In order to gain more insight of our model and observe the effectiveness of the sentiment lexicon , in", "entities": []}
{"text": "The work was supported by the Natural Science Foundation of China ( No . 61672288 ) , and the Natural Science Foundation of Jiangsu Province for Excellent Young Scholars ( No . BK20160085 ) .", "entities": []}
{"text": "Data set statistics are presented in Table 1 . This shows that all domains are represented with a substantial number of sentences , although the prevalence of named entities and their distribution across types varies , as expected from data sets collected from different sources and genres . We also see that the zero - shot domains are significantly different in entity type distribution and density than the training data , making them well - suited for this setting .", "entities": []}
{"text": "Runtime", "entities": []}
{"text": "A couple of challenges are found during the parallel annotation . First , subjects are obligatory in English for most sentence forms whereas Korean is a prodrop language so that entities in the subject position can be missing in Korean but not in English , which explains the greater number of entities in English . Second , certain inflectional morphemes in Korean can be dropped without violating the grammar , that often makes the labeling ambiguous . For instance , the literal translation of \" Korean Church \" would be \" \ud55c\uad6d ( Korea ) + \uc758 ( 's ) \uad50\ud68c ( Church ) \" , although it is the standard practice to drop \" \uc758 ( 's ) \" in this case such that it becomes \" \ud55c\uad6d ( Korea ) \uad50\ud68c ( Church ) \" . Given this translation , the annotator can be easily confused to annotate \" \ud55c\uad6d ( Korea ) \" as a geopolitical entity ( GPE ) instead of a nationality ( NORP ) , which may lead to annotation disagreement . Additional analytics by news sections and entity types are described in Appendix A.6 4 Zero - shot Crosslingual Learning", "entities": []}
{"text": "Since the number of named entity types that each model covers are different , named entity tags are mapped based on the definition of the tags . Our named entities are more fine - grained , which makes multiple tags ( Zero - shot side ) be mapped to one tag ( Existing side ) . Named entity tags that can not be mapped are discarded in both gold labels and predicted labels , thus not considered in the evaluation of the models .", "entities": []}
{"text": "Another virtual workshop . . . Oh well : maybe next year we will go back to the ( new ) normal . Looking at submission numbers , this year 's edition of the workshop was a huge success . We have received really unusually many submissions ( thanks , everyone ! ) . Out of those , we have accepted 22 papers for a 38 % acceptance rate . A round of applause for our wonderful program committee ! The workshop programme consists of brief eight - minute Q&A sessions for ten oral presentations ( which you will have watched by then ! ) , and a twelve - poster session during which you will be able to chat with any author you like . Thematically , the papers cover the entire range of \" Cultural Heritage , Social Sciences , Humanities and Literature \" . The programme shows that this area of applied language technology is mature and active . Last but not least , Sara Tonelli will give a live invited talk . We are grateful , and we look forward to it .", "entities": []}
{"text": "Learning CNF Blocking for Large - scale Author Name Disambiguation", "entities": []}
{"text": "Author name disambiguation ( AND ) algorithms identify a unique author entity record from all similar or same publication records in scholarly or similar databases . Typically , a clustering method is used that requires calculation of similarities between each possible record pair . However , the total number of pairs grows quadratically with the size of the author database making such clustering difficult for millions of records . One remedy is a blocking function that reduces the number of pairwise similarity calculations . Here , we introduce a new way of learning blocking schemes by using a conjunctive normal form ( CNF ) in contrast to the disjunctive normal form ( DNF ) . We demonstrate on PubMed author records that CNF blocking reduces more pairs while preserving high pairs completeness compared to the previous methods that use a DNF and that the computation time is significantly reduced . In addition , we also show how to ensure that the method produces disjoint blocks so that much of the AND algorithm can be efficiently paralleled . Our CNF blocking method is tested on the entire PubMed database of 80 million author mentions and efficiently removes 82.17 % of all author record pairs in 10 minutes .", "entities": []}
{"text": "Here , we first briefly review DNF blocking and then introduce our CNF blocking function . This section describes the gain functions that select an optimal predicate term for each step in the CNF learner . Finally , we discuss an extension that ensures the production of disjunctive blocks . Algorithm 1 DNF Blocking 1 : function LEARNCONJTERMS ( L , P , p , k )", "entities": []}
{"text": "Let P os be set of positive samples in L 3 : Let N eg be set of negative samples in L 4 : T erms { p } 5 : CurT erm p 6 : i 1 7 : while i < k do 8 : Find p i P that maximizes gain function CALCGAIN ( P os , N eg , CurT erm p i ) 9 : CurT erm CurT erm p i 10 : Add CurT erm to T erms 11 : Let P osCov be all l P os that satisfies T i i +", "entities": []}
{"text": "Let N egCov be all l N eg that satisfies T ( Bilenko et al , 2006 ; Michelson and Knoblock , 2006 ) . Given labeled pairs , these methods attempt to learn the blocking function in the form of a DNF , the disjunction ( logical OR ) of conjunction ( logical AN D ) terms . Learning DNFs is known to be a NP - hard problem ( Bilenko et al , 2006 ) . Thus , an approximation algorithm was used to learn k - DNF blocking by using a sequential covering algorithm . k - DNF means each conjunction term has , at most , k predicates . Algorithm 1 shows the process of DNF blocking . Function LEARNDNF in lines 16 - 38 is the main part of the algorithm . It has 3 inputs which are the L labeled sample pairs , P blocking predicates , and k parameters of maximum predicates considered for each conjunction term . First , the algorithm selects a set of candidate conjunction terms with at most k predicates . For each predicate p , it generates k candidate conjunction terms with the highest gain function . Using the candidate terms , the algorithm learns the blocking function by using a sequential covering algorithm . It sequentially selects a conjunction term , from the set of candidates , that has the maximum gain value on the remaining samples , and attaches it with logical OR to the DNF term . In each step , all samples covered by the selected conjunction term are removed . This process repeats until it covers the desired minimum amount of positive samples , or there is no candidate term that can further be improved .", "entities": []}
{"text": "The gain function estimates the benefit of adding a specific term to the learned formula . It is used in two different places in the algorithm - when choosing the conjunction candidates ( line 8 ) and when choosing a term from the candidates for each iteration ( line 27 - 28 ) . Previous methods have proposed different gain functions . Here we describe each and compare the results in the experiments . P , N is the total number of positive and negative samples , and p , n is the number of remaining positive and negative samples covered by the term .", "entities": []}
{"text": "Originally from Mooney 's CNF learner ( 1995 ) , it is the dual of the information gain of a DNF learner gain CN F = n\u00d7 log n n + p \u2212log N N + P . ( Conj LEARNCNF ( L , P disjoint , 1 )", "entities": []}
{"text": "Let L be set of l L satisfies Conj", "entities": []}
{"text": "CN F LEARNCNF ( L remain , P f ull , k )", "entities": []}
{"text": "Blocks Apply Conj to whole data 6 : for Block Blocks do 7 : Let L be l Block that satisfies CN F", "entities": []}
{"text": "We use the PubMed to evaluate these methods . PubMed is a public large - scale scholarly database maintained by the National Center for Biotechnology Information ( NCBI ) at the National Library of Medicine ( NLM ) . We use NIH principal investigator ( PI ) data for evaluation , which include PI IDs and corresponding publications . We randomly picked 10 names from the most frequent ones in the dataset and manually verified that all publications belong to each PI . The set of names include C * Lee , J * Chen , J * Smith , M * Johnson , M * Miller , R * Jones , S * Kim , X * Yang , Y * Li , Y * Wang , where C * means any name starts with C. Table 2 shows the statistics of the dataset . Experiments are done with 5 - fold cross validation .", "entities": []}
{"text": "We show how to learn an efficient blocking function with a conjunctive normal form ( CNF ) of blocking predicates . Using CNF as a negation of the corresponding disjunctive normal form ( DNF ) of predicates ( Mooney , 1995 ) , our method is a logical dual of existing DNF blocking methods ( Bilenko et al , 2006 ; Michelson and Knoblock , 2006 ) . We find that our method reduces more pairs for a large number of target pairs completeness and has a faster run time . We devise an extension that ensures that our CNF blocking produces disjoint blocks . Thus , the clustering process can be efficiently parallelized . Future work could use multiple levels of blocking functions for processing each block ( Das Sarma et al , 2012 ) and using linear programming to find an optimal CNF ( Su et al , 2016 ) .", "entities": []}
{"text": "We gratefully acknowledge partial support from the National Science Foundation and the National Bureau of Economic Research and useful discussions with Bruce Weinberg .", "entities": []}
{"text": "The W - NUT 2022 workshop focuses on a core set of natural language processing tasks on top of noisy user - generated text , such as that found on social media , web forums and online reviews . Recent years have seen a significant increase of interest in these areas . The internet has democratized content creation leading to an explosion of informal user - generated text , publicly available in electronic format , motivating the need for NLP on noisy text to enable new data analytics applications . We have received 39 main workshop submissions ( 22 long and 17 short papers ) . The workshop will be held in hybrid in - person and virtual modes . We have two invited speakers Yulia Tsvetkov ( University of Washington ) and David Jurgens ( University of Michigan ) who will talk about their work . We 're very thankful to have them in our workshop . We have the best paper award ( s ) sponsored by Megagon Labs this year , for which we are thankful . We would like to thank the Program Committee members who reviewed the papers . We would also like to thank the workshop participants .", "entities": []}
{"text": "This paper describes the CMU submission to shared task 1 of SIGMORPHON 2017 . The system is based on the multi - space variational encoder - decoder ( MSVED ) method of Zhou and Neubig ( 2017 ) , which employs both continuous and discrete latent variables for the variational encoder - decoder and is trained in a semi - supervised fashion . We discuss some language - specific errors and present result analysis .", "entities": []}
{"text": "In this section we will detail the multi - space variational encoder - decoder model .", "entities": []}
{"text": "In morphological reinflection , the source sequence x ( s ) consists of the characters in an inflected word ( e.g. , \" played \" ) , while the associated labels y ( t ) describe some linguistic features ( e.g. , y ( t ) pos = Verb , y ( t ) tense = Past ) that we hope to realize in the target . The target sequence x ( t ) is therefore the characters of the re - inflected form of the source word ( e.g. , \" played \" ) that satisfy the linguistic features specified by y ( t ) . For this task , each discrete variable y ( t ) k has a set of possible labels ( e.g. pos = V , pos = ADJ , etc ) and follows a multinomial distribution .", "entities": []}
{"text": "We process the Wikipedia corpus provided by the shared task organizer as our unsupervised training data together with words in the training data . For each language , we first get the character vocabulary of the corresponding training data and only keep words in the Wiki corpus for which characters are all in the character set we obtained . All words that occur less than 20 times are eliminated . We also limit the number of words used during training to be the 50000 most frequent words .", "entities": []}
{"text": "In this work , we further examine the method proposed in ( Zhou and Neubig , 2017 ) for the shared task of SIGMORPHON 2017 on 52 languages and demonstrate the effectiveness of this approach . We will further improve our model 's sophistication by investigating strategies for choosing appropriate semi - supervised data , and examining the model 's performance on languages with a high inflection level .", "entities": []}
{"text": "This work has been supported in part by an Amazon Academic Research Award . We thank Matthew Honnibal for pointing out that the data distribution of Wikipedia corpus might be biased .", "entities": []}
{"text": "Autoregressive generation ( AG ) models generate sequences based on a left - to - right factorization . As shown in Figure 1 , given the source sequence X , the target sequence Y with length T is generated via a chain of conditional probabilities based on the left - to - right sequential dependencies as : p ( Y | X ) = T i=1 p ( y i | y < i , X ) , ( 1 ) where y < i denotes the tokens before the i - th step . This property of autoregressive factorization makes the generation process hard to be parallelized as the result is generated token by token . Unlike AG models , non - autoregressive ( NAG ) models generate sequences without modelling the output - side dependencies . As shown in Figure 1 , given the prespecified output length T , the probability of the target sequence Y is then modelled as : p ( Y | X ) = T i=1 p ( y i | X , i , T ) . ( 2 ) With this conditional independence assumption , NAG models can fully parallelize their generation process , which significantly improves the inference speed . However , it has been shown that , the choice of the prespecified output length has a notable impact on the model 's generation quality ( Gu et al , 2018 ) . In addition , the removal of output - side sequential dependency also causes the generation quality of NAG models to be inferior to their autoregressive counterparts ( Wang et al , 2019b ) .", "entities": []}
{"text": "The authors wish to thank Jialu Xu , Guanlin Li , Xing Wang for their insightful discussions and support . Many thanks to our anonymous reviewers for their suggestions and comments .", "entities": []}
{"text": "Read , Revise , Repeat : A System Demonstration for Human - in - the - loop Iterative Text Revision", "entities": []}
{"text": "Previous works on modeling text revision Botha et al , 2018 ; Ito et al , 2019 ; Faltings et al , 2021 ) have ignored the iterative nature of the task , and simplified it into a one - shot \" original - to - final \" sentence - to - sentence generation task . However , in practice , at every revision step , multiple edits happen at the document - level which also play an important role in text revision . For instance , reordering and deleting sentences to improve the coherence . More importantly , performing multiple highquality edits at once is very challenging . Continuing the previous example , document readability can degrade after reordering sentences , and further adding transitional phrases is often required to make the document more coherent and readable . Therefore , one - shot sentence - to - sentence text revision formulation is not sufficient to deal with real - world challenges in text revision tasks . While some prior works on text revision ( Coenen et al , 2021 ; Padmakumar and He , 2021 ; Gero et al , 2021 ; Lee et al , 2022 ) have proposed humanmachine collaborative writing interfaces , they are mostly focused on collecting human - machine interaction data for training better neural models , rather than understanding the iterative nature of the text revision process , or the model 's ability to adjust editing suggestions according to human feedback . Another line of work by Sun et al ( 2021 ) ; Singh et al ( 2022 ) on creative writing designed humanmachine interaction interfaces to encourage new content generation . However , text revision focuses on improving the quality of existing writing and keeping the original content as much as possible . In this work , we provide a human - in - the - loop text revision system to make helpful editing suggestions by interacting with users in an iterative way .", "entities": []}
{"text": "Figure 1 shows the general pipeline of R3 humanin - the - loop iterative text revision system . In this section , we will describe the development details of the text revision models and demonstrate our user interfaces . We first formulate an iterative text revision process : given a source document 1 D t\u22121 , at each revision depth t , a text revision system will apply a set of edits to get the revised document D t . The system will continue iterating revision until the revised document D t satisfies a set of predefined stopping criteria , such as reaching a predefined maximum revision depth t max , or making no edits between D t\u22121 and D t .", "entities": []}
{"text": "We follow the prior work of Du et al ( 2022 ) to build our text revision system . The system is composed of edit intention identification models and a text revision generation model . We follow the same data collection procedure in Du et al ( 2022 ) to collect the iterative revision data . 2 Then , we train the three models on the collected revision dataset . Edit Intention Identification Models . Following Du et al ( 2022 ) , our edit intentions have four categories : FLUENCY , COHERENCE , CLARITY , and STYLE . We build our edit intention identification models at each sentence of the source document D t\u22121 to capture the more fine - grained edits . Specifically , given a source sentence , the system will make two - step predictions : ( 1 ) whether or not to edit , and ( 2 ) which edit intention to apply . The decision whether or not to edit is taken by an edit - prediction classifier that predicts a binary label of whether to edit a sentence or not . The second model , called the edit - intention classifier , predicts which edit intention to apply to the sentence . If the edit - prediction model predicts \" not to edit \" in the first step , the source sentence will be kept unchanged at the current revision depth . Text Revision Generation Model . We fine - tune a large pre - trained language model like PEGA - SUS ( Zhang et al , 2020 ) on our collected revision dataset to build the text revision generation model . Given a source sentence and its predicted edit intention , the model will generate a revised sentence , conditioned on the predicted edit intention . Then , we concatenate all un - revised and revised sentences to get the model - revised document D t , and extract all its edits using latexdiff 3 and difflib . 4 In summary , at each revision depth t , given a source document D t\u22121 , the text revision system first predicts the need for revising a sentence , and for the ones that need revision , it predicts the corresponding fine - grained edit intentions - thus , generating the revised document D t based on the source document and the predicted edit decisions and intentions .", "entities": []}
{"text": "In practice , not all model - generated edits are equally impactful towards improving the document quality ( Du et al , 2022 ) . Therefore , we enable user interaction in the iterative text revision process to achieve high quality of text revisions along with a productive writing experience . At each revision depth t , our system provides the user with suggested edits , and their corresponding edit intentions . The user can interact with the system by choosing to accept or reject the suggested edits . Figure 2 illustrates the details of R3 's user interface . First , a user enters their i d to login to the web interface as shown in Figure 2a . Then , the user is instructed with a few guidelines on how to operate the revision as demonstrated in Figure 2b . After getting familiar with the interface , the user can select a source document from the left dropdown menu in Figure 2c . By clicking the source document , all the edits predicted by the text re - vision model , as well as their corresponding edit intentions will show up in the main page as illustrated in Figure 2d ( left panel ) . The user is guided to go through each suggested edits , and choose to accept or reject the current edit by clicking the Confirm button in Figure 2d ( right panel ) . After going through all the suggested edits , the user is guided to click the Submit button to save their decisions on the edits . Then , the user is guided to click the Next Iteration ! button to proceed to the next revision depth and check the next round of edits suggested by the system . This interactive process continues until the system does not generate further edits or reaches the maximum revision depth t max .", "entities": []}
{"text": "We conduct experiments to answer the following research questions : RQ1 How likely are users to accept the editing suggestions predicted by our text revision system ? This question is designed to evaluate whether our text revision system can generate high quality edits . RQ2 Which types of edit intentions are more likely to be accepted by users ? This question is aimed to identify which types of edits are more favored by users . RQ3 Does user feedback in R3 help produce higher quality of revised documents ? This question is proposed to validate the effectiveness of human - in - the - loop component in R3 .", "entities": []}
{"text": "Iterative Revision Systems . We prepare three types of iterative revision systems to answer the above questions : 1 . HUMAN - HUMAN : We ask users to accept or reject text revisions made by human writers , which are directly sampled from our collected iterative revision dataset . This serves as the baseline to measure the gap between our text revision system and human writers . 2 . SYSTEM - HUMAN : We ask users to accept or reject text revisions made by our system . Then , we incorporate user accepted edits to the system to generate the next iteration of revision . This is the standard human - in - the - loop process of R3 .", "entities": []}
{"text": "When R3 generates revisions at deeper depths , we observe a decrease in the acceptance ratio by human users . It is crucial to create a text revision system that can learn different revision strategies at each iteration and generate high quality edits at deeper revision levels . Editing suggestions provided by our text revision generation models could be improved . Particularly , FLUENCY edits show a huge gap between human and system revisions ( 45.05 % and 82.02 % ) . Future work could focus on developing more powerful text revision generation models . In our human - machine interaction , we restrict the users ' role to accept or reject the model 's predictions . Even with minimal human interaction , our experiment shows comparable or even better revision quality as compared to human writers at early revision depths . A potential future direction for human - machine collaborative text revision would be to develop advanced human - machine interaction interfaces , such as asking users to re - write the machine - revised text . Also , a larger - scale user study could be carried out to derive more meaningful statistics ( e.g. optimal number of revision depths and edit suggestions ) and investigate if there is any intriguing user behavior in the iterative revision process . For example , as mentioned in the users ' feedback , it would be interesting to check if users behave differently when they are asked to accept / reject edit suggestions provided for their own texts as opposed to the texts written by a third party .", "entities": []}
{"text": "We thank all linguistic expert annotators at Grammarly for participating in the user study and providing us with valuable feedback during the process . We also thank Karin de Langis at University of Minnesota for narrating the video of our system demonstration . We would like to extend our gratitude to the anonymous reviewers for their helpful comments .", "entities": []}
{"text": "The authors gratefully acknowledge the financial support of the research reported here by the grant Modellierung lexikalisch - semantischer Beziehungen von Kollokationen awarded by the Deutsche Forschungsgemeinschaft ( DFG ) . We would also like to thank three anonymous reviewers for their constructive remarks on an earlier version of this paper .", "entities": []}
{"text": "Hate and Toxic Speech Detection in the Context of Covid - 19 Pandemic using XAI : Ongoing Applied Research", "entities": []}
{"text": "Welcome to the inaugural Workshop on Knowledge Extraction and Integration for Deep Learning Architectures ( DeeLIO ) ! The DeeLIO workshop aims to bring together the knowledge interpretation , extraction and integration lines of research in deep learning , and cover the area in between . We hope that the DeeLIO workshop will become a regular forum for the exchange of ideas and will contribute to foster collaboration within these research fields . This volume includes the 11 papers presented at the workshop . DeeLIO was co - located with the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP 2020 ) and was held on November 19 , 2020 as an online workshop , following the exceptional \" new normal \" circumstances of 2020 . For the first edition of the workshop , we received 21 paper submissions . We accepted 11 papers ( acceptance rate 52.4 % ) which were presented at the workshop . The accepted papers cover both thematic aspects of DeeLIO : the extraction of linguistic knowledge from deep neural models and the integration of knowledge from external resources , for different languages and applications . In addition to the regular workshop papers , the first edition of DeeLIO also included presentations of several papers from the EMNLP companion volume ' Findings of EMNLP ' which were thematically relevant to the workshop goals . There was no distinction between oral and poster presentations this year , and all presentations involved pre - recorded talks accompanied with individual live Q&A sessions . We take this opportunity to thank the DeeLIO program committee for their thorough reviews . We also thank the authors who presented their work at DeeLIO , and the workshop participants for the valuable feedback and discussions . Finally , we are honored to have two excellent invited talks from our invited speakers Ellie Pavlick and Eduard Hovy .", "entities": []}
{"text": "Learning Input Strictly Local Functions : Comparing Approaches with Catalan Adjectives", "entities": []}
{"text": "The FestCat project ( Bonafonte et al , 2008 ) provides broad transcriptions for more than 53 , 000 adjectival surface forms in two major dialects of Catalan . We considered the Central Catalan forms and restricted our data to the nearly 6 , 500 lemmas that are also attested in a subtitle lexicon ( Boada et al , 2020 ) . While our main focus was on learning , we also developed a hand - written ISL transducer for the mapping to masc.sg . forms that is highly accurate ( > 98 % correct ) , along with custom code to derive plausible underlying representations from masc.sg . \u223c fem.sg . pairs .", "entities": []}
{"text": "We evaluated all four models on the same training / validation / testing data , as summarized in Table 1 . ISLFLA and OSTIA were unable to learn accurate mappings except when the fem.sg . and masc.sg . forms were artificially trimmed to their final VC * ( V ) sequences - a strong , languagespecific bias to attend to changes at the end of the word that the other models did not require . Results for larger training splits , and for mapping from URs to SRs , were similar . The errors made by DNN - ISL mostly involved underapplication of deletion ( e.g. , * [ blaNk ] ) .", "entities": []}
{"text": "In summary , we evaluated four learning models on an ISL phonological mapping ( with a small number of exceptions ) found in a large , realistic body of natural language data . The models that have proofs of learnability and efficiency , ISLFLA and OSTIA , performed much worse than models that currently lack such theoretical guarantees but share the inductive bias for ISL patterns . The results highlight the need for further empirical and formal study of highperforming subsymbolic models such as DNN - ISL , and extension of our model to output - based patterns and learning of underlying representations . We plan to release our processed data , hand - written ISL transducer , and model implementations .", "entities": []}
{"text": "Thanks to Coleman Haley and Marina Bedny for helpful discussion of this research , which was supported by NSF grant BCS - 1941593 to CW .", "entities": []}
{"text": "In this section , we explain how we introduce type information into a neural CR system .", "entities": []}
{"text": "We thank the anonymous reviewers for their insightful comments . We are also grateful to the members of the TELEDIA group at LTI , CMU for the invaluable feedback . This work was funded in part by Dow Chemical , and Microsoft .", "entities": []}
{"text": "To have an intuitive understanding of our proposed model , we visualize the attention weights on the aspect term and sentence in Figure 2 . The color depth indicates the importance degree of the weight , the darker the more important . In Figure 2 , the sentence is \" This is one great place to eat pizza more out but not a good place for take - out pizza . \" , the polarities are positive and negative for pizza and take - out pizza respectively . From Figure 2 , we can find that our model is more inclined to consider the neighboring words of the aspect term . For example , when the current aspect term is pizza , obviously , its neighboring words such as \" great \" , \" place \" and \" more \" get more attention and play a great role for judging sentiment polarity of pizza . However , those words that are far from the current aspect term such as \" but \" , \" not \" and \" take - out \" obtain less attention , which demonstrates the effectiveness of the position information . For aspect term take - out pizza , it is obvious that the word \" take - out \" is more important to express the aspect term than the word \" pizza \" . From Figure 2 , it is worth noting that some words such as \" good \" and \" place \" get less attention even they are closer to the current aspect term than \" but \" and \" not \" . This is because different words in aspect term have different effect on a sentence , and we apply the bidirectional attention mechanism to choose more useful words . For instance , in this case , PBAN should pay more attention on the word \" take - out \" . Therefore , PBAN is capable of figuring out the important part in a sentence for judging the sentiment polarity by modeling the mutual relation between sentence and different words in aspect term .", "entities": []}
{"text": "Traditional machine learning approaches mainly involve text representation and feature extraction , such as bag - of - words models and sentiment lexicons features , then training a sentiment classifier ( Prez - Rosas et al , 2012 ) . Rao et al ( 2010 ) demonstrated the utility of graph - based semi - supervised learning framework for building sentiment lexicons . Kaji et al ( 2007 ) explored to use structural clues that could extract polar sentences from HTML documents , and built lexicon from the extracted polar sentences . However , these methods are labor - intensive , and usually results in high dimensional and high sparse phenomenon for the text representation .", "entities": []}
{"text": "This work is funded in part by the national key research and development program of China ( 2017YFE0111900 ) , the Key Project of Tianjin Natural Science Foundation ( 15JCZDJC31100 ) , the National Natural Science Foundation of China ( Key Program , U1636203 ) , the National Natural Science Foundation of China ( U1736103 ) and MSCA - ITN - ETN - European Training Networks Project ( QUARTZ ) .", "entities": []}
{"text": "In this section , we introduce the overall framework of our proposed AMNRE in detail . As shown in Figure 1 , for each entity pair , AMNRE encodes its corresponding sentences in different languages into several semantic spaces to grasp their individual language patterns . Meanwhile , a unified space is also set up to encode consistent features among languages . By explicitly encoding the consistency and diversity among languages , AMNRE can achieve better extraction results in the multi - lingual scenario . For each given entity pair , we define its corresponding sentences in n different languages as T = { S 1 , . . . , S n } , where S j = { x 1 j , . . . , x | S j | j } denotes the sentence set in the j - th language . All these sentences are labeled with the relation r R by heuristical labeling algorithms in distant supervision ( Mintz et al , 2009 ) . Our model aims to learn a relation extractor by maximizing the conditional probability p ( r | T ) with the following three components : Sentence Encoder . Given a sentence and its target entity pair , we employ neural networks to encode the sentence into a embedding . In this paper , we implement the sentence encoder with both convolutional ( CNN ) and recurrent ( RNN ) architectures . Specifically , we set the encoders E I j and E C j to encode each sentence in the j - th language into its individual and consistent embeddings respectively , and expect these embeddings to capture the diversity and consistency among languages . Multi - lingual Attention . Since not all sentences are labeled correctly in distant supervision , we adopt multi - lingual attention mechanisms to capture those informative sentences . In practice , we apply language - individual and language - consistent attentions to compute local and global textual relation representations respectively for final prediction . Adversarial Training . Under the framework of AMNRE , we encode the sentences in various languages into a unified consistent semantic space . We further adopt adversarial training to ensure these sentences are well fused in the unified space after encoding so that our model can effectively extract the language - consistent relation patterns . We will introduce the three components in detail as follows .", "entities": []}
{"text": "Given a sentence x = { w 1 , w 2 , . . . } containing two entities , we apply neural architectures including both CNN and RNN to encode the sentence into a continuous low - dimensional space to capture its implicit semantics .", "entities": []}
{"text": "For each given entity pair , AMNRE adopts multi - lingual selective attention mechanisms to exploit informative sentences in T . We explicitly encode languages ' consistency and diversity into individual and consistent representations , thus our attentions are more simple than those proposed in .", "entities": []}
{"text": "We thank Jiacheng Zhang for his help . This work is supported by the National Natural Science Foundation of China ( NSFC No . 61621136008 , 61772302 ) and Tsinghua University Initiative Scientific Research Program ( 20151080406 ) . This research is part of the NExT++ project , supported by the National Research Foundation , Prime Minister 's Office , Singapore under its IRC@Singapore Funding Initiative .", "entities": []}
{"text": "Interpreting Verbal Irony : Linguistic Strategies and the Connection to the Type of Semantic Incongruity", "entities": []}
{"text": "Human communication often involves the use of verbal irony or sarcasm , where the speakers usually mean the opposite of what they say . To better understand how verbal irony is expressed by the speaker and interpreted by the hearer we conduct a crowdsourcing task : given an utterance expressing verbal irony , users are asked to verbalize their interpretation of the speaker 's ironic message . We propose a typology of linguistic strategies for verbal irony interpretation and link it to various theoretical linguistic frameworks . We design computational models to capture these strategies and present empirical studies aimed to answer three questions : ( 1 ) what is the distribution of linguistic strategies used by hearers to interpret ironic messages ? ; ( 2 ) do hearers adopt similar strategies for interpreting the speaker 's ironic intent ? ; and ( 3 ) does the type of semantic incongruity in the ironic message ( explicit vs. implicit ) influence the choice of interpretation strategies by the hearers ? \u21e4 Part of the research was carried out while Debanjan was a Ph.D. candidate at Rutgers University .", "entities": []}
{"text": "Messages : Explicit vs. Implicit Attardo ( 2000 ) and later Burgers ( 2010 ) distinguish between two theoretical aspects of irony : irony markers and irony factors . Irony markers are meta - communicative signals , such as interjections or emoticons that alert the reader that an utterance might be ironic . In contrast , irony factors can not be removed without destroying the irony , such as the incongruity between the literal evaluation and its context ( \" semantic incongruity \" ) . Incongruity expresses the contrast between the conveyed sentiment ( usually , positive ) and the targeted situation ( usually , negative ) . This contrast can be explicitly or implicitly expressed in the ironic message . Following Karoui et al ( 2017 ) , we consider that semantic incongruity is explicit , when it is lexicalized in the utterance itself ( e.g. , both the positive sentiment word ( s ) and the negative situation are available to the reader explicitly ) . On Twitter , beside sentiment words , users often make use of hashtags ( e.g. , \" Studying 5 subjects . . . # worstsaturdaynight \" ) or an image ( e.g. , \" Encouraging how Police feel they 're above the law . URL \" ; the URL shows a police car not paying parking ) to express their sentiment . We consider these cases as explicit , since the incongruity is present in the utterance even if via hashtags or other media . For implicit incongruity , we consider cases where one of the two incongruent terms ( \" propositions \" in Karoui et al ( 2017 ) ) is not lexicalized and has to be reconstructed from the con - text ( either outside word knowledge or a larger conversational context ) . For example \" You are such a nice friend ! ! ! \" , or \" Driving in Detroit is fun ; ) \" are cases of ironic messages where the semantic incongruity is implicit . Based on these definitions of explicit and implicit incongruity , two expert annotators annotated the S i m - H int dataset ( 1000 ironic messages ) as containing explicit or implicit semantic incongruity . The inter - annotator agreement was \uf8ff=0.7 , which denotes good agreement similar to Karoui et al ( 2017 ) . The annotation showed that 38.7 % of the ironic messages are explicit , while 61.3 % are implicit . In the following section we propose a typology of linguistic strategies used in hearers ' interpretations of speakers ' ironic messages and in section 6.2 we discuss the correlation of linguistic strategies with the type of semantic incongruity .", "entities": []}
{"text": "Given the definition of verbal irony , we would expect that Turkers ' interpretation of speaker 's ironic message will contain some degree of opposite meaning with respect to what the speaker has said . However , it is unclear what linguistic strategies the Turkers will use to express that . To build our typology , from the total set of S i m - H int pairs obtained through crowdsourcing ( i.e. , 4 , 762 pairs ; see Section 2 ) we selected a dev set of 500 S i m - H int pairs . Our approach does not assume any specific theory or irony , but it is data - driven : a linguist expert in semantics and pragmatics analyzed the dev set to formulate the lexical and pragmatic phenomena attested in the data . The assembled typology is , thus , the result of a bottom - up procedure . A S i m - H int pair can be annotated with more than one strategy . The core linguistic strategies are explained below and synthesized in Table 2 .", "entities": []}
{"text": "Lexical and phrasal antonyms : This category contains lexical antonyms ( e.g. , \" love \" $ \" hate \" , \" great \" $ \" terrible \" ) as well as indirect antonyms ( Fellbaum , 1998 ) , where the opposite meaning can only be interpreted in context ( e.g. , \" passionate speaker \" ! \" boring speaker \" ; Table 1 ) . Although the typical antonym of \" passionate \" is \" unpassionate \" , \" boring \" works in this context as a lexical opposite since a speaker who is passionate entails that he is not boring . Besides lexical antonyms , Turkers sometimes use antonym phrases ( e.g. , \" I ca n't wait \" ! \" not looking forward \" , \" I like ( to visit ER ) \" ! \" I am upset ( to visit ER ) \" ) . Negation : Here , Turkers negate the main predicate . This strategy is used in the presence of copulative constructions where the predicative expression is an adjective / noun expressing sentiment ( e.g. , \" is great \" ! \" is not great \" ) and of verbs expressing sentiment ( e.g. , \" love \" ! \" do not love \" ) or propositional attitudes ( e.g. , \" I wonder \" ! \" I do n't wonder \" ) . Weakening the intensity of sentiment : The use of negation and antonyms is sometimes accompanied by two strategies that reflect a weakening of sentiment intensity . First , when S i m contains words expressing a high degree of positive sentiment , the hearer 's interpretation replaces them with more neutral ones ( e.g. , \" I love it \" ! \" I do n't like it \" ) . Second , when S i m contains an intensifier , it is eliminated in the Turkers ' interpretation . Intensifiers specify the degree of value / quality expressed by the words they modify ( M\u00e9ndez - Naya , 2008 ) ( e.g. , \" cake for breakfast . so healthy \" ! \" cake for breakfast . not healthy \" ) . Interrogative to Declarative Transformation ( + Antonym / Negation ) : This strategy , used mostly in conjunction with the negation or antonym strategies , consists in replacing the interrogative form with a declarative form , when S i m is a rhetorical question ( for brevity , RQ ) ( e.g. , \" do n't you love fighting ? \" ! \" I hate fighting \" ) .", "entities": []}
{"text": "When the ironic utterance expresses a positive / negative sentiment towards a past event ( e.g. , \" glad you relayed this news \" ) or an expressive speech act ( e.g. , \" thanks X that picture needed more copy \" ) the hearer 's interpretation of intended meaning is expressed through the counterfactual desiderative constructions I wish ( that ) p ( \" I wish you had n't relayed . . . \" , \" I wish X did n't copy . . . \" ) . Differently from antonymic phrases , this strategy stresses on the failure of the speaker 's expectation more than on their commitment to the opposite meaning . Pragmatic Inference : In addition to the above strategies , there are cases where the interpretation calls for an inferential process to be recognized . For instance , \" made 174 this month . . . I 'm gon na buy a yacht ! \" ! \" made 174 this month . . . I am so poor \" . The distribution of the strategies on the dev set is represented in Table 2 .", "entities": []}
{"text": "In linguistic literature many different approaches to irony have been provided . Here we focus on the three accounts ( w.r.t . examples from S i m - H int corpus ) that bear a different views on pragmatic factors . According to Grice ( 1975 ) , ironic messages are uttered to convey a meaning opposite to that literally expressed , flouting the conversational maxim of quality \" do not say what you believe to be false \" . In verbal irony , the violation of the maxim is frequently signaled by \" the opposite \" of what is said literally ( e.g. , intended meaning of \" carcasses are flattering \" is they are gross ; Table 1 ) . The linguistic strategies of antonyms ( e.g. \" worst day of my life \" ) and simple negation ( \" yeap we totally do nt drink alcohol every single day \" [ ... ] ) cover the majority of the S i m - H int corpus and seem to fit the Gricean ( Grice , 1975 ) account of irony , since the hearer seems to have primarily recognized the presence of semantic incongruity . However , as touched upon by Giora ( 1995 ) , antonyms and direct negation are not always semantically equivalent strategies , since the second sometimes allows a graded interpretation : if \" x is not encouraging \" , it is not nec - essarily bad , but simply \" x < encouraging \" . Such an implicature is available exclusively with items allowing mediated contraries , such as sentiment words ( Horn , 1989 ) . Direct negation with sentiment words implies that just one value in a set is negated , while the others are potentially affirmed . The spectrum of interpretations allowed by negation as a rephrasing strategy indicates that hearers recognize that the relevance of the ironic utterance in itself plays a role next to what the utterances refers to ( if the rephrased utterance is intended as \" x is not encouraging at all \" , the perceived irrelevance of the corresponding ironic utterance is more prominent than in \" x is not very encouraging \" ) . The fact that the interpretation of irony has a propositional scope is even clearer when the ironic sentence in interrogative form ( \" and they all lived happily ever after ? \" ) is rephrased as a declarative ( e.g. \" I doubt they all lived happily ever after \" ) : the hearers recognizes that the question has a rhetoric value since otherwise contextually irrelevant . The intentional falsehood of Gricean analysis is also not deemed by Sperber and Wilson ( 1986 ) ; Wilson and Sperber ( 2012 ) as a necessary and sufficient condition for irony . According to their theory of echoic mentioning , irony presupposes the mention to the inappropriateness of the entire sentence : in asserting \" awesome weather in Scotland today \" the speaker does not simply want to express that the weather was horrible but he signals that assuming that the weather would be nice was irrelevant and , thus , ridiculous . Kreuz and Glucksberg ( 1989 ) expand the Relevance Theory approach talking about echoic reminding to account for cases such as \" could you be just a little louder , please ? My baby is n't trying to sleep \" where the extreme politeness reminds the hearer that the question is indeed a request and that the mother bears a certain stance and has certain expectations towards the addressee . Similarly , the use of the pragmatic inference strategy can not be fully explained in Gricean terms : the rephrase \" made 174 this month . . . I am so poor \" for \" made 174 this month . . . I am gon na buy a yatch \" more than pointing to the presence of lexical incongruity , show that the hearers knows for background knowledge that the assertion of \" buying a yatch \" is completely irrelevant in the context of a low salary situation . Rephrasing strategies using counterfactual desiderative constructions ( e.g. \" I really wish my friends and fam - ily would check up on my after yesterday 's near death experience \" ) show , instead , that the interpretation of irony involves an echoic reminding to the speaker 's ( social ) expectations which failed to be fulfilled . Overall , using the results of our crowdsourcing experiment with main existing theories of irony , it turns out that the theories have a complementary explanatory power . In Section 6.2 we investigate weather this situation might relate to the presence of explicit / implicit irony .", "entities": []}
{"text": "Here our goal is to perform a comparative empirical analysis to understand how hearers interpret verbal irony . To accomplish this , we propose computational models to automatically detect these linguistic strategies in two datasets : ( 1 ) S i m - H int dataset and ( 2 ) the SIGN dataset . As stated in Section 2 , albeit for a different purpose , the task designed in Peled and Reichart ( 2017 ) is identical to ours : they used a set of 3 , 000 sarcastic tweets and collected five interpretation verbalization , including an option to just copy the original message if it was not deemed ironic . They used workers skilled in comedy writing and literature paraphrasing . SIGN contains 14 , 970 pairs . To evaluate our models , we asked two annotators to annotate two test sets of 500 pairs each from the S i m - H int and the SIGN dataset ( i.e. , denoted by SIGN test ) , respectively . Note , the test set for the S i m - H int has no overlap with the dev set of 500 S i m - H int pairs used to identify the strategies ( Section 4 ) . Agreement between the annotators for both sets is high with \uf8ff > 0.9 . In SIGN test , 79 instances were just copies of the original message , which we eliminated , thus the SIGN test contains only 421 instances .", "entities": []}
{"text": "Identifying phrasal antonyms and pragmatic inference is a complex task , and thus we propose a method of phrase matching based on phrase extraction via unsupervised alignment technique in SMT . We use IBM Model 4 with HMM ( Giza++ ; ( Och and Ney , 2000 ) ) , phrase extraction via Moses ( Koehn et al , 2007 ) and the IRST tool to build the required language models . As postprocessing , we first remove phrase pairs obtained from the S i m - H int bitext that are also present in the set of extracted phrases from the H int - H int bitext . This increases the likelihood of retaining semantically opposite phrases , since phrases extracted from the H int - H int bitext are more likely to be paraphrastic . Second , based on the translation probability scores , for phrase e if we have a set of aligned phrases f set we reject phrases that have scores less than 1 size ( fset ) . Finally , 11 , 200 phrases are extracted from the S i m - H int bitext . The low recall for this strategy is expected since there are too many ways that users can employ pragmatic inference or rephrase the utterance without directly using any antonym or negation . In future , we will explore neural MT ( Cho et al , 2014 ) and use external data to generate more phrases . Since we have not manually evaluated these phrase pairs , we only use this strategy after we have tried all the remaining strategies ( AntPhrase+PragInf in Table 3 and Table 4 ) .", "entities": []}
{"text": "We investigate how hearers adopt strategies for interpreting the speaker 's ironic intent . To implement this study , we selected three Turkers ( e.g. , H 1 , H 2 , and H 3 ; In Table 1 , H i int are generated by the correspondent Turker H i ) , from our crowdsourced data , who were able to rephrase at least five hundred identical S i m messages . Note , we can not carry this experiment on the SIGN dataset ( Peled and Reichart , 2017 ) because the annotators ' information is absent there . Although the three Turkers choose lexical antonym and simple negation as two top choices , there is some variation among them . H 1 and H 2 choose antonyms more frequently than negation while in contrary Turker H 3 choose negation more than antonyms , sometime combined with the weakening of sentiment strategy . As we mentioned in Section 4.2 , antonyms and direct negation are not semantically equivalent strategies since the latter , allows a graded interpretation : if \" x is not inspiring \" , it is not necessarily bad , but simply \" x < inspiring \" ( Giora , 1995 ) . In Table 1 , the S i m - H int pair \" passionate \" ! \" boring \" and \" flattering \" ! \" gross \" ( interpretation of H 1 ) have more contrast than the pair \" passionate \" ! \" not passionate \" and \" so flattering \" ! \" not flattering \" ( interpretation of H 3 ) . This suggests that H 1 perceive the intensity of negative sentiment towards the target of irony ( \" Ed Davey \" and \" picture of dead animals \" , respectively ) higher than Turker H 3 . All three Turkers have chosen the remaining strategies with similar frequencies .", "entities": []}
{"text": "Interpretation Strategies and the Type of Semantic Incongruity : We investigate whether the type of semantic incongruity in the ironic message ( explicit vs. implicit ; see Section 3 ) influences the choice of interpretation strategies by the hearers . To do this , we looked at S i m - level distribution of interpretation strategies used by the hearers for the same ironic message S i m . as interpretation strategy more when the semantic incongruity is explicit than implicit ( 48.5 % vs. 34.8 % ) : the presence of explicit sentiment triggered the use of the antonym strategy . In contrary they use simple negation more when the semantic incongruity is implicit than explicit . We also analyze the interpretation strategies w.r.t . to the presence ( + ) or absence ( ) of irony markers . We implement various morpho - syntactic as well as typographic markers ( similar to ) to identify the presence of markers . We observe that Lex ant strategy is used more in cases where the markers are absent . In S i m - H int , markers are present twice as much in the case of implicit ( 21 % ) than explicit incongruity ( 10 % ) . This finding validates ( Burgers et al , 2012 ) who argued speakers will likely use markers to signal their ironic intent in implicit incongruity .", "entities": []}
{"text": "In Figure 1 , the vertical columns ( purple : S i m - H int and grey : SIGN ) depict the distribution ( in % ) of tweets strategy - wise . In S i m - H int dataset , for 17 % of messages ( 124 S i m s ) all five Turkers use the same strategy to interpret the S i m s ( labeled as 5 on the X - axis ) , whereas for 26 % ( 188 S i m s ) , 4 Turkers used same strategy ( labeled as 4 , 1 on Xaxis ) and so on . We observe when the S i m s are marked by strong subjective words e.g. , \" great \" , \" best \" , etc . , they have been replaced in 90 % of cases as lexical antonyms ( e.g. , \" great \" ! \" terrible \" ) . In addition , the majority of adjectives are used in attributive position ( i.e. , \" lovely neighbor is vacuuming at night \" ) , thus blocking paraphrases involving predicate negation . However , not all strong subjective words guarantee the use of direct opposites in the H int s ( e.g. , \" flattering \" ! \" not flattering \" ; See Table 1 ) . The choice of strategies may also depend upon the target of ironic situation ( Ivanko and Pexman , 2003 ) . We implement the bootstrapping algorithm from Riloff et al ( 2013 ) to identify ironic situations in S i m s that are rephrased by Lexical antonym strategy . We find utterances containing stereotypical negative situations regarding health issues ( e.g. , \" having migraines \" , \" getting killed by chemicals \" ) and other undesirable negative states such as \" oversleeping \" , \" luggage lost \" , \" stress in life \" are almost always interpreted via lexical antonym strategy . Utterances where all five Turkers used simple negation , if negative particles are positioned in the ironic message with a sentential scope ( e.g. , \" not a biggie \" , \" not awkward \" ) then they are simply omitted in the interpretations . This trend can be explained according to the inter - subjective account of negation types ( Verhagen , 2005 ) . Sentential negation leads the addressee to open up an alternative mental space where an opposite predication is at stake .", "entities": []}
{"text": "We leveraged a crowdsourcing task to obtain a dataset of ironic utterances paired with the hearer 's verbalization of their interpretation . We proposed a typology of linguistic strategies for verbal irony interpretation and designed computational models to capture these strategies with good performance . Our study shows ( 1 ) Turkers mostly adopt lexical antonym and negation strategies to interpret speaker 's irony , ( 2 ) interpretations are correlated to stereotype ironic situations , and ( 3 ) irony expression ( explicit vs. implicit incongruity and absence or presence of markers ) influences the choice of interpretation strategies and match with different explanatory theories ( the Gricean approach links up better with explicit incongruity , while Relevance Theory with the implicit one ) . The latter can have an impact on irony detection by bringing out more discriminative semantic and pragmatic features .", "entities": []}
{"text": "The Future is not One - dimensional : Complex Event Schema Induction by Graph Modeling for Event Prediction", "entities": []}
{"text": "Subgraph of G containing events before ei and their arguments", "entities": []}
{"text": "From a set of documents describing a complex event , we construct an instance graph G which contains event nodes E and entity nodes ( argument nodes ) V . There are three types of edges in this graph : ( 1 ) event - event edges e i , e l connecting events that have direct temporal relations ; ( 2 ) evententity edges e i , a , v j connecting arguments to the event ; and ( 3 ) entity - entity edges v j , r , v k indicating relations between entities . We can construct instance graphs by applying Information Extraction ( IE ) techniques on an input text corpus . In these graphs , the relation edges do not have directions but temporal edges between events are directional , going from the event before to the event after . For each complex event type , given a set of instance graphs G , the goal of schema induction is to generate a schema library S. In each schema graph S , the nodes are abstracted to the types of events and entities . Figure 1 is an example of schema 2 for complex event type car - bombing . Schema graphs can be regarded as a summary abstraction of instance graphs , capturing the reoccurring structures . 3 Our Approach", "entities": []}
{"text": "Baseline 1 : Event Language Model ( Rudinger et al , 2015 ; Pichotta and Mooney , 2016 ) is the state - of - the - art event schema induction method . It learns the probability of temporal event sequences , and the event sequences generated from event language model are considered as schemas . Baseline 2 : Sequential Pattern Mining ( Pei et al , 2001 ) is a classic algorithm for discovering common sequences . We also attach arguments and their relations as extensions to the pattern . Considering that the event language model baseline can not handle multiple arguments and relations , we add sequential pattern mining for comparison . The frequent patterns mined are considered as schemas . Reference : Human Schema is added as a baseline in the extrinsic task of event prediction . Since human - created schemas are highly accurate but not probabilistic , we want to evaluate their limits at predicting events in the extrinsic task . We match schemas to instances and fill in the matched type . Ablation Study : Event Graph Model w/o Argument Generation is included as a variant of our model in which we remove argument generation ( 3.5 and 3.6 ) . It learns to generate a graph containing only event nodes with their temporal relations , aiming to verify whether incorporating argument information helps event modeling .", "entities": []}
{"text": "We propose a new task to induce temporal complex event schemas , which are capable of representing multiple temporal dependencies between events and their connected arguments . We induce such schemas by learning an event graph model , a deep auto - regressive model , from the automatically extracted instance graphs . Experiments demonstrate the model 's effectiveness on both intrinsic evaluation and the downstream task of schema - guided event prediction . These schemas can guide our understanding and ability to make predictions with respect to what might happen next , along with background knowledge including location - , and participant - specific and temporally ordered event information . In the future , we plan to extend our framework to hierarchical event schema induction , as well as event and argument instance prediction .", "entities": []}
{"text": "Evaluating the Evaluation of Diversity in Natural Language Generation", "entities": []}
{"text": "Despite growing interest in natural language generation ( NLG ) models that produce diverse outputs , there is currently no principled method for evaluating the diversity of an NLG system . In this work , we propose a framework for evaluating diversity metrics . The framework measures the correlation between a proposed diversity metric and a diversity parameter , a single parameter that controls some aspect of diversity in generated text . For example , a diversity parameter might be a binary variable used to instruct crowdsourcing workers to generate text with either low or high content diversity . We demonstrate the utility of our framework by : ( a ) establishing best practices for eliciting diversity judgments from humans , ( b ) showing that humans substantially outperform automatic metrics in estimating content diversity , and ( c ) demonstrating that existing methods for controlling diversity by tuning a \" decoding parameter \" mostly affect form but not meaning . Our framework can advance the understanding of different diversity metrics , an essential step on the road towards better NLG systems .", "entities": []}
{"text": "To conclude , increasing interest in diversity resulted in multiple proposed diversity metrics . However , there is no consensus on how to evaluate diversity and what each metric actually measures .", "entities": []}
{"text": "In the content test ( conTest ) , our goal is to evaluate how different diversity metrics capture the notion of content diversity . Measuring content diversity requires deep understanding of the semantics of responses in S c . To isolate content from form diversity , we aim to generate response sets with a similar level of form diversity , but where the level of content diversity is controlled by the diversity parameter d. In 6 , we will focus on whether automatic diversity metrics can perform as well as humans on the task of estimating content diversity .", "entities": []}
{"text": "One of the core questions we tackle is : Can humans evaluate diversity reliably ? Although a few papers ( Ghandeharioun et al , 2019 ; Zhang et al , 2019b ) asked humans to evaluate diversity , to the best of our knowledge no work thoroughly investigated this question . The importance of this question is clear when comparing to quality evaluation . There , human judgment is the gold standard , and automatic quality metrics are established by showing high correlation with human score . Thus , understanding if humans can judge diversity is important for improving diversity metrics . We use crowdsourcing workers 2 to compute a human diversity score : we show workers a context followed by a set of responses , and ask them to rate the diversity of the set . To establish best practices , we experiment with multiple variations of HDS ( detailed in 6.2 ) , asking humans to rate the diversity of a response set , and evaluating each practice with our framework . We focus on the following questions : Should humans rate diversity of a set or similarity between pairs in the set , from which diversity can be inferred ? ( tl ; dr : diversity ) Can humans evaluate different aspects of diversity well ? ( tl ; dr : not effectively ) Should humans rate the absolute diversity score of a set of sentences or rank whether one set is more diverse than another ? Here , we did not reach a conclusive result , and describe this experiment in the Appendix C. As a preliminary step , we conducted pilot experiments among a group of NLP graduate students . The main insights were : ( a ) humans are biased by quality : if a generated set has high diversity but low quality , humans will rate diversity low . To neutralize this , we explicitly ask workers to evaluate the quality of one of the responses in the set S c , and then instruct them to ignore quality in diversity questions ; ( b ) To make sure a worker reads the context c , we ask them to generate a sentence s before they rate diversity ; ( c ) It is difficult for workers to evaluate the diversity of a set with more than 10 responses . Our crowdsourcing tasks are provided in Appendix A.", "entities": []}
{"text": "For each task , we collected 200 sets of 5 responses each ( 100 sets per class ) . For high content diversity class , we asked workers to give 5 responses per context , with as different content and structure as possible . Then we asked the same workers to choose a single response they wrote , and rephrase it 5 times such that the original content will be preserved , while changing the form - this set is used for the low content diversity class . A sample from this data is in Figure 1 and more samples in Appendix B. For each HDS metric , we collected 10 ratings from crowdsourcing workers , different than the ones who composed the sets .", "entities": []}
{"text": "In this work , we focused on the two primary aspects of diversity : content diversity ( What to say ? ) and form diversity ( How to say it ? ) . In Figure 1 , Both sets are diverse , but Set B is only form diverse , as all answers deliver the same massage , whereas Set A is diverse in both form and content . Furthermore , we can observe aspects of diversity as having a tree - like structure , where both content and form diversity can be divided to subaspects : Content diversity ( e.g. answering the question \" How are you today ? \" ) can be expressed by using different sentiment ( \" I 'm doing good . \" vs. \" I 'm so glad you asked ! I 'm really doing good . \" ) , different relevance ( \" I 'm fine \" vs. \" Did you watch the game last night ? \" ) , and more . Form diversity can be divided into sub - aspects as well : syntactic diversity ( \" Someone took it from me . \" vs. \" It was taken from me . \" ) or lexical diversity ( \" I feel fine . \" vs. \" I feel very well . \" ) . Even those sub - aspects can be further divided . For example , a sub - aspect of lexical diversity is register diversity ( \" How are you ? \" vs. \" Sup bro ? \" ) . Another observation is that different aspects are not orthogonal , that is , changing one aspect may lead to changes in other aspects . Specifically , we observe that while it is relatively easy to produce high form diversity with low content diversity ( Set B in Figure 1 ) , it is almost impossible to diversify content without changing form . This observation was important during the design of conTest .", "entities": []}
{"text": "This work presents a framework for evaluating diversity metrics as a step toward standardized evaluation . We limit the scope of this work to differ - ences between form and content diversity , which are key towards understanding different aspects of diversity . Future work can explore other aspects of diversity , e.g. testing sentiment diversity , as proposed in 3 . We urge researchers to use this framework as a platform for developing new diversity metrics and establishing their efficiency .", "entities": []}
{"text": "All Human scores for HDS metrics were collected using Amazon Mechanical Turk ( AMT ) crowdsourcing platform by English native - speaking workers that were specifically qualified for this task . Figure 7 presents the warm - up part , common for all HDS questionnaires . Before asking workers to rate the diversity of each set , we first asked them to generate a response for the context themselves , to make sure they read it . To neutralize the effect of the responses ' quality on the workers , we also asked the workers to rate the quality of the first response in the set , then explicitly instructed them to ignore quality when rating diversity . Figures 8 to 11 present the diversity questions of absHDS , aspHDS , rnkHDS and simHDS as appeared in the AMT questionnaires . Costs For HDS metrics that require one query per response set ( i.e. absHDS , rnkHDS , aspDHS ) , the cost for a single rating was 0.18$. We collected 10 ratings per response set , and conduct each experiment with 200 sets , hence the total cost for an experiment was 360$. In the case of simHDS , the response set size was 5 , and the number of queries needed per set is 5 2 = 10 . The cost of a single rating for this task was 0.056 $ , and with the same multipliers , the total cost for an experiment was 1120 $ , three times more expensive .", "entities": []}
{"text": "Tables 20 to 22 present data samples from sto - ryGen , respGen and promptGen with the human testers of conTest , as detailed in 6 . Each table presents two contexts and two response sets per context - one for the low content diversity class and one for the high content diversity class .", "entities": []}
{"text": "Response set ( low content diversity ) Suppose there 's an escape plan we have n't thought of yet . Suppose there 's an omelet that is the most amazing ever . Suppose there 's an airplane ticket that 's even cheaper . Suppose there 's an actual deadline for this paper . Suppose there 's an event that we can go to this weekend . Suppose there 's an airline that costs less . Suppose there 's an flight that is n't as expensive . Suppose there 's an air travel fare , but does n't cost as much . Suppose there 's an way to fly there that is low cost . Suppose there 's an flight going there and it 's not a lot of money Nothing remotely like eating a big breakfast . Nothing remotely like dancing with your wife at the wedding . Nothing remotely like singing Justin Bieber 's greatest hits Nothing remotely like falling down a hill Nothing remotely like getting yelled at Nothing remotely like being super full and satisfied . Nothing remotely like getting to taste many different foods . Nothing remotely like starting the day off right . Nothing remotely like doing exactly what I want to do . Nothing remotely like feeding myself with great food .", "entities": []}
{"text": "We thank Aya Meltzer - Asscher for linguistic advice , and Or Nachmias , Ben Bogin , Mor Geva , Omer Goldman and Ohad Rubin for their useful suggestions and references . This research was partially supported by The Israel Science Foundation grant 942/16 , The Yandex Initiative for Machine Learning and the European Research Council ( ERC ) under the European Union Horizons 2020 research and innovation programme ( grant ERC DELPHI 802800 ) .", "entities": []}
{"text": "Response set ( \u03c4 = 0.25 ) Response set ( \u03c4 = 0.8 ) Response set ( \u03c4 = 1.1 ) Fire next door . John woke up smelling like something was burning . He went outside . He saw the fire next door . He called the authorities . It was a minor fire and they put it out . It was a fire . It was a fire . It was a fire . It was a fire . It was a fire . It was a minor fire . It was a fire . It was a fire . It was a minor fire and they put it out . They arrived and put out the fire . It was a fire . It was a fire . It turned out to be a fire . It was a minor fire night . They arrived and put it out . It was a scary but beautiful fire next time he went in . It was a fire . It was a put out and John was able to put it out . It was a scary place to live . It turned out to be a mechanic . Before the fire was put out it was a fire . It was a fire . They co - worker matter how bad the fire was . Several shells , the fire department came just in time . They arrived and put out the fire . It turned out to be a clear , fact . It was a fire . It was a scary meal for wet clothes . It was a scary stealing from him . Charles Shaves His Chest . Charles decides he no longer wants chest hair . He shaves his chest hair with a razor . Although his skin is irritated , he hair is now gone . Charles is satisfied . He is glad that he finally shaved . He is glad that he finally shaved . He is very happy to have a chest hair . He is glad that he finally shaved . He is glad that he finally shaved . He is glad that he finally shaved . He is glad that he finally shaved . He is glad that he finally shaved . He is glad that he finally shaved . He is glad that he finally shaved . He is very happy to have a chest hair . He is glad that he finally shaved . He is very happy that he no longer has chest hair . He is glad that he finally shaved . He is glad that he finally shaved . He is glad that his chest hair is no longer shaved . He is glad that he finally shaved He is glad that he finally shaved . He is glad that he finally of shaving . He is glad that he finally shashock his chest hair . He is glad that he finally peaceful his voice again . He is glad that he finally shaavoided his chest hair . He is glad that he finally downcome . He is ready to start better cooperation his fireplace . He is glad that he finally shaved . He is glad that he no longer has chest hair . He is happy to finally be shaved . He is very happy to have ster good ride of his life . He is glad that he finally shaved . He is glad that he finally has chest hair .", "entities": []}
{"text": "Kate was wearing big over the ear headphones . But they were tight and squeezing her head . She tried to adjust them to relieve the tension . But nothing really worked . Kate decided to go to the store and buy some ear phones . She decided to go to the store and buy some headphones instead . She decided to go to the store and buy some ear phones . She decided to go buy a pair of headphones instead . She decided to go to the store and buy some headphones instead . She decided to go to the store and buy some headphones instead . She decided to go to the doctor and have some rest . Kate decided to go to the store and buy some headphones instead . She decided to go to the store and buy some ear phones . She decided to go buy some ear plugs . She decided to go buy a hat so she could enjoy the the long lin Kate decided to go to the store to buy some candy since she was ti Kate decided to go outside to rest . Kate decided to go to the store and buy some ran phones . Kate decided to go to the store and buy some headphones instead . Kate decided to go back to her old dishes . She decided to go buy a big pair of headphones instead . Kate decided to go to the store and wearing some headphones . She was forced to go to the store to buy some cash . She decided to go to the store and buy some headphones instead . She decided to go to a ess instead . Kate decided to go to the store and buy some fake headphones . She decided to go to the store and buy some ear phones . She decided to go to the store and buy some headphones instead . She decided to go to the store and buy some headphones instead . She decided to go buy some ear phones . Kate decided to go to the gym and use some saw no more watching T Kate decided to go to the store and buy some cute phones . She decided to go buy a fake headphones instead . She decided to go to the store and buy some concert .", "entities": []}
{"text": "Response set ( high content diversity ) Response set ( low content diversity ) Sold Out Jane wanted to watch a big new action movie . She had been waiting a long time for it to come out . When tickets became available she was too busy . By the time she had a chance to buy some it was sold out . Jane cried over the fact that she could n't watch it and just gave up looking for a ticket . Jane decided to look for a scalper that would sell her the ticket for the movie that she really wanted to see . Jane thought it was okay since she can still have a chance to watch it once it gets uploaded in video and movie streaming applications . Jane posted a status on her social media accounts asking her friends for any spare ticket that she is willing to buy . Jane resorted to contacting her old friend who is working at a huge movie theater hoping she can help her get a ticket . Jane remembered that she has an old friend who is a manager at a big movie theater so she contacted that friend in the hopes that she can buy any spare ticket . Desperate to watch the movie , Jane called her friend , who works at a movie theater , asking for a ticket to that movie . Jane recalled that her friend works at a movie theater and hoped that she can help get a ticket for that movie . Jane decided to look for her friend who could possibly have access to tickets for that movie since that friend currently works at a movie theater . Jane realized that her friend might have spare tickets since she is a manager of a movie theater showing that film .", "entities": []}
{"text": "My friend has some beavers in his backyard . They come up from the creek by his house . He invites my over and we watch them . We take pictures of them and send them to our friends . They are fascinating animals . Our friends love getting the pictures . Sometimes his dogs chase them . They are building a dam on the creek . They wo n't let us get too close to them . They are busy gathering sticks to make a dam . The dam they are building is almost complete . It 's fascinating to see their workmanship building a dam . They are turning the creek into a pond by building a dam . They all work together with careful engineering to build a dam . I just got into this show and ca n't stop watching places apple slices in a bowl so they 'll stay fresh Oh boy , I love apples . I do n't need you telling me how to keep things fresh , take a hike . Girl , you 're the fresh one around here . This post might be better in the life hacks section . This is actually a useful bit of advice . I find merit in this input . That information will serve me well . Thanks , that 's really good to know ! Such knowledge is certainly beneficial . Wise words , I will heed them .", "entities": []}
{"text": "For each question type , we select an object in the image scene graph , and update the question by substituting the reference to this object by another object . When substituting one object by another , we need to adjust the question to keep it fluent . Table 10 shows the specific linguistic rules we verify when performing this substitution . A.4 Annotation Task for Verifying Generated Contrast Sets Fig . 3 shows the annotation task that is shown to Turkers to validate the QA pairs generated by our method .", "entities": []}
{"text": "Singular vs. plural If the noun is singular and countable : add \" a \" or \" an \" If needed , replace \" Are \" and \" Is \" \" a fence \" , \" men \" \" a boy \" , \" an elephant \" Definite vs. indefinite Do not change definite articles to indefinite articles , and vice versa \" is there any fence near the boy \" suggests that there is a boy in the scene graph , which is not always correct", "entities": []}
{"text": "Meaning can be changed When replacing to general or specific terms \" Cats in the image \" = > \" Animals in the image \" , \" Animals not in the image \" = > \" cats not in the image \" , The opposite directions not necessarily holds Countable vs. uncountable If the noun is uncountable , do not add \" a \" or \" an \" \" A cat \" , \" water \" Table 10 : Partial linguistic rules to notice using our method .", "entities": []}
{"text": "Designing Algorithms for Referring with Proper Names", "entities": []}
{"text": "Standard algorithms for attribute choice in the generation of referring expressions have little to say about the role of Proper Names in referring expressions . We discuss the implications of letting these algorithms produce Proper Names and expressions that have Proper Names as parts .", "entities": []}
{"text": "Reference - the production and comprehension of referring expressions - has been studied intensively throughout the cognitive sciences . Computational Linguists are no exception , often paying particular attention to the generation of referring expressions ( REs , ( Krahmer and Van Deemter , 2012 ) for a survey ) . This area of Natural Language Generation is known as Referring Expressions Generation ( REG ) . An important strand of REG focusses on \" one - shot \" REs , which do not rely on any linguistic context ( precluding anaphoric and other attenuated REs ) ; these are also the primary focus of this paper . 1 One of the classic algorithm coming out or REG is the Incremental Algorithm ( IA ) ( Dale and Reiter , 1996 ) . Simplifying slightly , the IA starts by ordering properties in a sequence known as the Preference Order . The algorithm starts with an empty RE , then examines the first property from the Preference Order . If this property is true of the referent r and rules out one or more distractors , it is added to the RE ; otherwise it is not added , and the next property in the Preference Order is examined . The algorithm terminates when properties P i 1 , .. , P i k have 1 See , however , section 2.1 on the use of salience . been selected that jointly identify the referent ( i.e. , [ [ P i 1 ] ] \u2229 ... \u2229 [ [ P i k ] ] = { r } ) . Different Preference Orders tend to generate different REs , so finding a good one is important . Proper Names ( PNs ) are among the most widely studied REs in cognitive science ( see e.g. , ( van Langendonck , 2007 ) , passim ; ( van Deemter , 2016 ) , chapters 2 and 7 ) , and a crucial area of applied work in Information Extraction ( e.g. , ( Jurafsky and Martin , 2009 ) chapter 22 on Named Entities ) . Yet REG 2 has neglected PNs , presumably because names could easily trivialise REG : suppose the KB contained a set of people . If only one of the people in the KB is named Obama , then it is easy to identify him , by referring to him by his name . Since PNs tend to make excellent REs , REG would become trivial - so the presumed argument goes . We argue that this line of reasoning misses some important points and that PNs deserve more attention from researchers in REG .", "entities": []}
{"text": "Observe that : - Name are often ambiguous . \" Obama \" , for instance ( not to mention \" Smith \" ) could refer to many different people . - A referent can have many names ( \" Barack \" , \" Obama \" , \" Barack Obama \" , etc . ) or none . - A name can combine with other properties and epithets , as in \" Mr Barack Obama , America 's current president \" . - A name can be part of an expression that refers to another referent . The process is recursive , e.g. , \" The height of the income of Obama 's Secretary of State \" . So how might PNs be given a place in REG ?", "entities": []}
{"text": "This approach works , but it puts a spotlight on some difficult issues , some of which affect the generation of descriptive REs as well : 1 . PNs are not always preferred . For example , if the Director of Taxes is Mrs X , this does not mean that \" Contact the Director of Taxes \" is always better worded as \" Contact Mrs X \" , since her job title may be relevant . The lack of a computational theory of relevance affects all of REG but becomes very noticeable in the choice between PNs and descriptions . 2 . There is no reason for limiting reification to PNs . Colours too could be reified , for example , to generate \" the colour of grass \" . The traditional dichotomy between objects and properties limits the range of REs that these algorithms can generate . 3 . REG algorithms are ignorant about social relations between speaker , hearer , and referent . Consider a couple with a son and a daughter . Speaking to his mother , the son could say \" my sister \" , \" your daughter \" , etc . , yet in most situations a PN would be better . Titles and epithets like \" Dr \" and \" Aunt ( y ) \" , complicate matters further . 4 . As elsewhere in REG , questions about overspecification need to be faced . When , for example , is it useful to add an appositive to a PN , as in \" Mr Barack Obama , America 's current president \" ? Furthermore , Linguistic Realisation will have to decide about the surface order of the PN and the appositive , perhaps depending on whether the PN and/or the appositive ( by itself ) refers uniquely . 5 . If PNs are properties of the referent , then this leaves room for expressing one and the same PN with a different string . ( For example , \" Doctor \" may be worded as \" Doctor \" , \" Dr. \" , or \" Dr \" . ) The desirability of this use of Linguistic Realisation would need to be investigated . 6 . It is often difficult for the speaker to assess whether the hearer knows who a given PN refers to . The hearer may never have heard of Joe Klein , for example , and this would cause the RE \" Joe Klein \" to mis - fire . Lack of shared knowledge is a problem for descriptive REs as well , but it is exacerbated in the case of PNs , because names are highly conventional : once I 've learned what \" red \" means , I can apply the word to any red object , but learning your name does not teach me to apply this name to anyone else . The last point has important implications . Imagine a programmer wanting to implement the algorithm of section 2.1 , aiming to mimic human language use . If she decides to implement an Incremental Algorithm , then how to choose its free pa - rameter , the Preference Order ? She could learn one via an elicitation experiment , but how does she find a generic REG algorithm that works for all PNs ? Consider a scene from an experiment where speakers referred to stimuli on a screen . Participants called the man in the top right \" the man with the white beard \" , etc . They might have said \" Samuel Eilenberg \" , yet noone did , because participants did n't know his name . Participants could have been trained to be familiar with every individual 's name , but this could easily have primed the use of names at the expense of descriptions ; the same happens when names are visible as captions , as was done in ( de Oliveira et al , 2015 ) using fictitious names of geographical areas ; see also ( Anderson et al , 1991 ) . Such an approach does not give reliable information on how REG algorithms should choose between PNs and descriptions . The problem is not just that PNs are conventional , but that their conventional meaning can be entrenched to different degrees , varying from shortlived \" conceptual pacts \" ( Brennan and Clark , 1996 ) to names that are very widely known and used .", "entities": []}
{"text": "Suppose someone asks \" Who is Joe Klein ? \" ( cf . , section 2.2 , point 6 ) . Would it make sense to respond \" ( He is ) the author of the bestselling political novel of the 1990s ? \" It depends on the importance of this fact and how widely it is known . To model answers to \" Who is ? \" questions ( see ( Bo\u00ebr and Lycan , 1986 ) for a theoretical study ) , ( Kutlak et al , 2013 ) designed a REG algorithm that employs the following Heuristic : Based on the frequency with which a name n co - occurs with a property P , the Heuristic estimates how likely the proposition P ( n ) is to be known by an arbitrarily chosen hearer . Evaluation studies suggest that this Heuristic goes a long way towards estimating how many people know a fact , and the complete REG algorithm ( which involves 2 other heuristics ) outperforms its competitors in terms of its ability to generate descriptions that allow hearers to guess correctly the name of the referent . Although the authors focussed on the WWW , the approach can use any corpus that represents the ideas of a community ( e.g. , a company 's intranet ) . This approach suggests a promising handle on the conventionality of PNs . It allows us to estimate , for example , the likelihood that a name like \" Joe Klein \" is known by hearers to refer to the commentator and novelist of that name , and this would allow us to limit the KB of section 2 to names that are well enough known . We hypothesise that PNs have a higher likelihood of being uttered as part of REs by members of a community ( e.g. , users of the WWW ) the more frequently these PNs occur as names of this referent in documents produced by that community . Further experiments could flesh out how the use of PNs depends on a number of factors , including the Knowledge Heuristic . Essentially , PNs would be treated as properties of a referent that may or may not be known to the hearer , analogous to the descriptive properties of ( Kutlak et al , 2013 ) .", "entities": []}
{"text": "We have shown how , given appropriate semantic representations , standard attribute algorithms are able to generate REs that contain PNs , thereby solving problems with the standard 2 - step perspective on REG that separates choosing the general syntactic type of RE from more fine - grained decisions about the content of the RE . However , our approach raises difficult questions about the choices that a REG algorithm needs to make between PNs and descriptive REs . We argue that some of the trickiest questions in this area may be solved if large corpora are employed as a source of insight into the degree to which a PN is likely to be known by the recipient of the RE .", "entities": []}
{"text": "We thank the anonymous reviewers for valuable feedback and helpful suggestions .", "entities": []}
{"text": "Here , we discuss our rationale behind reducing the student vocabulary size and its challenges , followed by our mixed - vocabulary distillation approach .", "entities": []}
{"text": "We propose a two - stage approach for implicit transfer of knowledge to the student via the student embeddings , as described below .", "entities": []}
{"text": "Dialogue - act - driven Conversation Model : An Experimental Study", "entities": []}
{"text": "In this section , we provide details of several existing models that we will use to validate our hypothesis . These models include generative models ( such as encoder - decoder model and its hierarchical version i.e. , hierarchical encoder - decoder ) and discriminative model ( Siamese - based model ) . Next , we provide details of the proposed model that adds the hierarchical structure to the Siamese model along with the dialogue act information . To set the notations , we are given a set D of N conversations , i.e. D = ( C 1 , C 2 , . . . C N ) , with each conversation C i being a sequence of R i utterances , C i = ( u 1 , u 2 , . . . u R i ) . Each utterance u j in turn is itself a sequence of S j words , i.e. u j = ( w 1 , w 2 , . . . w S j ) .", "entities": []}
{"text": "Generative models are the most widely used models for conversation modeling . These models include encoder - decoder model and hierarchical encoder - decoder model .", "entities": []}
{"text": "A decoder in the encoder - decoder model generates the next word given the context , and though it has several valid and reasonable choices , it is burdened with the task of generating exactly a particular choice that matches the ground truth . For example , for a context I am enjoying the day , it is warm and sunny , if decoder generates yes , it is . and the ground truth dictates yes , indeed , it is a lovely day , the decoder has failed , though it is a valid response . Due to these challenges with generative models , discriminative models are trained directly to discriminate between positive and negative utterances . A typical discriminative model , or in particular Siamese model , consists of two encoders , one encoder encoding the context , while another encoding the candidate utterance , i.e utterance K + 1 . These two representations are passed to a final layer that computes the probability of candidate being a valid response given the context . Let h ( 1 ) and h ( 2 ) be the representations obtained from the first encoder and second encoder , respectively , then the probability of their association can be computed using the following expression . p ( s | h ( 1 ) , h ( 2 ) ) = \u03c3 h ( 1 ) T Ah ( 2 ) + b ( 3 ) where , the bias b and matrix A are learned model parameters .", "entities": []}
{"text": "In this section , we describe the details of the experiments , i.e. dataset and its preparation , baseline models , experimental setup , and analysis of results .", "entities": []}
{"text": "Here we list the baseline models , their modified version enhanced with dialogue act information , and the proposed model .", "entities": []}
{"text": "Siamese - Also known as Dual - Encoder , it uses two encoders ( both utterance encoders ) with shared weights , to produce the representation for the K utterances and the ( K + 1 ) utterance . HSiamese - A Hierarchical version of the Siamese model that uses a hierarchical encoder to produce a representation for the K utterances , and a plain encoder ( utterance encoder ) to produce a representation for the ( K + 1 ) utterance . Siamese - DA - An extension of Siamese model that uses the additional dialogue act information obtained through DA - encoder . The representation obtained from the DA - encoder is linearly combined with the representation of the K utterances obtained from an utterance encoder . HSiamese - DA - The proposed model uses a Hierarchical Encoder and a DA - Encoder . The representation obtained from the DA - Encoder is linearly combined with the representation obtained from the hierarchical encoder .", "entities": []}
{"text": "In this section , we present results of our experimental study , followed by its analysis .", "entities": []}
{"text": "ZCU - NLP at MADAR 2019 : Recognizing Arabic Dialects", "entities": []}
{"text": "The Madar shared tasks ( Bouamor et al , 2019 ) are a follow - up to Salameh 's work with the synthetic corpus of Bouamor ( Bouamor et al , 2014 ) and Salameh 's work with tweets based on the corpus . Two corpora are provided , a six - city corpus of travel sentences rendered into the dialects of five cities and MSA 1 , and a 25 - city + MSA corpus using a smaller number of sentences . In the first task , test data is classified as one of the 25 cities or MSA . For the second task , the organizers chose training , development and test tweet - sets for download from Twitter . The tweets are from 21 Arabic countries , and the goal is to determine , for each tweet author , the country of origin . For S - 1 we did not use any external data , only data provided by the shared task organizers . 1", "entities": []}
{"text": "The organizers provided training and development data 2 consisting of sentences in different dialects with a label denoting the corresponding dialect . The training data contain 41 K sentences and development data contain 5.2 K sentences . Organizers also provided additional data with Arabic sentences in seven dialects . S - 2 uses a corpus of tweets . Twitter does not permit the organizers to distribute tweets , only the user ids and tweet ids . Every participant must arrange with Twitter to download the tweets themselves , and because tweets are subject to deletion over time , it is possible that each participant 's version of the corpus and test is unique .", "entities": []}
{"text": "The absence of audio was remedied for the 2017 and 2018 VarDial workshops , ( Zampieri et al , 2017 ( Zampieri et al , , 2018 However , the five dialects plus MSA targeted by the VarDial shared task comprise a small fraction of Arabic 's dialectical variation . Salameh et al ) use a corpus which differentiates between twentyfive different cities and MSA . This still does n't address urban rural divides , but it begins to reflect more realistic diversity .", "entities": []}
{"text": "In this section we describe our models 4 . We submitted results for the S - 1 from two systems - Tortuous Classifier and Neural Network Classifier .", "entities": []}
{"text": "word and char language model features for corpus - 6 and corpus - 26 features , tfid vectorized word 1 - 2grams , and tfid vectorized char 3 - 5grams . The classifier did better on the development data , suggesting that it is over - fitted , but the language model features , which are the most predictive , also did better on the development data .", "entities": []}
{"text": "For the Subtask - 1 we achieved 0.658 macro F 1score on the test data , sixth among nineteen submissions with the Tortuous Classifier . The Neural Network Classifier achieved a macro F 1 - score of 0.648 on the test data . For the Subtask - 2 we submitted a single entry . It ranked 7 th among 9 submissions with 0.475 macro F 1 - score . Figure 2 shows that many of the errors are geographically plausible . For example , ASWan ALXandria and CAIro are all in Egypt , and each has a sizeable chunk of mistaken identity for the others . Similarly , DAMascus , ALEppo , AMMan , BEIrut , JERusalem which are all ' Levantine ' and only a few hundred miles apart .", "entities": []}
{"text": "This paper presents an automatic approach for Arabic dialect detection in the MADAR Shared Task . Our proposed systems for the Subtask - 1 use language model features . Our experiments showed that simpler machine learning algorithms outperform RNN using language model features . Subtask - 2 turned out to be more challenging because Tweets , which are real - world wild data , are more difficult to process than systematically prepared texts .", "entities": []}
{"text": "This work has been partly supported by Grant No . SGS - 2019 - 018 Processing of heterogeneous data and its specialized applications , and was partly supported from ERDF \" Research and Development of Intelligent Components of Advanced Technologies for the Pilsen Metropolitan Area ( InteCom ) \" . Access to computing and storage facilities owned by parties and projects contributing to the National Grid Infrastructure MetaCentrum provided under the programme \" Projects of Large Research , Development , and Innovations Infrastructures \" ( CESNET LM2015042 ) , is greatly appreciated .", "entities": []}
{"text": "A FullyQT Visual Guide", "entities": []}
{"text": "Underspecification and interpretive parallelism in Dependent Type Semantics", "entities": []}
{"text": "The scope parallelism in the Geach sentence ( Every boy loves , and every girl detests , some saxophonist ) and the related parallel interpretation requirement in pronominal binding is a pervasive phenomenon found across different types of coordination and ellipsis phenomena . Previous accounts all resort to additional constraints of some sort that restrict the otherwise flexible syntax - semantics interface to avoid overgeneration . In this paper , we propose a novel approach to this long - standing problem . We show that , by taking a proof - theoretic perspective on natural language semantics and by viewing the ambiguity resolution for pronouns and indefinites as underspecification resolution that invokes extra proof search , a conceptually natural solution emerges for the problem of interpretive parallelism . The analysis is cast in Dependent Type Semantics , with Hybrid Type - Logical Categorial Grammar as the syntax - semantics interface backbone . For empirical illustration , we show how the proposed approach correctly accounts for the classical Geach paradigm and its pronominal variant .", "entities": []}
{"text": "1 Introduction : interpretive parallelism in coordination and ellipsis One of the long - standing problems in the analysis of coordination and ellipsis is the strong parallelism requirement imposed on the interpretations of the ' shared ' linguistic expression . For example , Geach ( 1972 ) famously noticed that , in the following type of examples involving right - node raising ( RNR ) , the object indefinite that is shared in the two conjuncts can either scope below the subject quantifier in each conjunct or scope over the entire coordination , but that mixed scope readings , in which the object indefinite scopes above the subject quantifier in one conjunct but not in the other , are unavailable . ( 1 ) Every boy loves , and every girl detests , some saxophonist . ( \u2200 > \u2200 > / > \u2200 > \u2200 ) Jacobson ( 1999 ) notes that this interpretive parallelism extends to the pronominal variable binding in examples such as the following ( on reading 2 , John is a salient male individual in the discourse ) : ( 2 ) Every Englishman admires , and every American loves , his mother . reading 1 : ' Every Englishman admires his own mother , and every American loves his own mother . ' reading 2 : ' Every Englishman admires John 's mother , and every American loves John 's mother . ' In general , pronouns can either be free or bound by a quantifier . Thus , there are four logically possible interpretations for ( 2 ) ( bound / free in first / second conjunct ) . And all these interpretations are indeed available in the non - RNR counterpart of ( 2 ) ( Every Englishman admires his mother and every American loves his mother ) . However , only two of these readings are attested for ( 2 ) , as indicated above . The parallel interpretation requirement is not limited to coordination but extends to ellipsis phenomena . For example , Hirschb\u00fchler ( 1982 ) notes that VP ellipsis imposes parallelism requirement for the scope of the quantifier inside the ' elided ' material , in a way essentially parallel to the RNR sentences : ( 3 ) An American flag was hanging in front of every window . A Canadian flag was , too . Like the RNR example in ( 1 ) , there are only the \u2200 > \u2200 > and > \u2200 > \u2200 parallel scope readings for this sentence . Mixed scope readings in which the universal scopes over the indefinite in the antecedent but not in the ellipsis site ( or vice versa ) are unavailable . Just as the RNR parallelism for quantifier scope in ( 1 ) is mirrored in the anaphora case in ( 2 ) , the scopal parallelism in the VP ellipsis data in ( 3 ) has an exact analogue in the anaphora example in ( 4 ) . ( 4 ) Every Englishman admires his mother , and every American does as well . As in ( 2 ) , the admiration relation holds either between every Englishman and every American male and his own respective mother or between every Englishman and every American male and the mother of some specific male individual in the antecedent context , with no mixed reading possible . The parallelism patterns in ( 1 ) and ( 2 ) recur in the case of Stripping ( see Puthawala ( 2018 ) for a recent formal analysis of Stripping , as well as a discussion of important properties of this construction ) . ( 5 ) a. Every boy admires a saxophonist , and every girl too . b. Every Englishman admires his mother , and every American as well . These examples exhibit only the parallel interpretations ( analogous to the relevant readings for the RNR and VP ellipsis examples given above ) for the quantifier or pronoun contained in the elided material . The interpretive parallelism in the data surveyed above has been noted by many authors ( see , e.g. , Jacobson 1999 ; Fox 2000 ; Asudeh and Crouch 2002 ; Steedman 2012 ) , but no uniform analysis currently exists which treats the binding and quantifier cases in a principled manner and which covers both the coordination and ellipsis cases . The present paper attempts to make a first step in such a unified analysis by focusing on the binding and scope data in RNR ( i.e. the Geach paradigm ) . The key claim of our proposal is that interpretive parallelism is a consequence of the underspecification involved in the interpretation of pronouns and indefinites ( in this sense , it is similar in spirit to Steedman 's ( 2012 ) approach ) . All the examples above have one property in common : the shared material contains an expression ( pronoun / quantifier ) that exhibits interpretive variability . Our proposal in a nutshell is that interpretive parallelism falls out from the way underspecification resolution happens in sentences that have this property , due to interactions of the following conceptually natural assumptions : ( i ) interpretive variability is resolved by underspecification resolution , formalized as type - checking ( ii ) for ' shared ' material , the syntax - semantics mapping requires the duplication of resource at some point in the mapping from the surface string to the final , fully resolved translation ( iii ) the formal language for the underspecified semantic representation imposes a certain restriction on the way multiple copies of an ( originally ) underspecified term are interpreted The third condition can be thought to arise from the requirement to keep the mechanism simple for ensuring proper identity of underspecified terms with respect to their interpretive possibilities . We show below that these simple assumptions suffice to ensure the right range of interpretations to be assigned to the examples discussed above , by taking the case of RNR as an example . We formulate our analysis in Dependent Type Semantics ( DTS ; Bekki 2014 ; Bekki and Mineshima 2017 ) , by proposing a novel treatment of indefinites involving underspecification . The proof - theoretic perspective of DTS provides a particularly natural setup to embody the assumptions outlined in ( i ) - ( iii ) above . For the sake of explicitness , we adopt Hybrid Type - Logical Categorial Grammar ( Hybrid TLCG ; Levine 2012 , 2015 ) for the syntax - semantics interface in spelling out the analyses of specific linguistic examples . The choice of Hybrid TLCG for syntax is not essential , but we believe that it helps illuminate the general nature of our solution , which is compatible with any suitably general theory of compositional semantics . 2 Anaphora and scope via underspecification in DTS Dependent Type Theory ( Martin - L\u00f6f 1984 ) is an extension of simply typed \u03bb - calculus . Dependent Type Semantics ( DTS ) is a proof - theoretic compositional dynamic semantics based on Dependent Type Theory . This framework allows us to use types depending on terms and to represent propositions ( corresponding to semantic representations of sentences ) as types . For instance , run ( x ) is a type depending on a term x. Under the Curry - Howard correspondence ( propositions - as - types principle ) , the type run ( x ) can be regarded as the proposition that x runs . If a term u has this type , we write u : run ( x ) , expressing that u is a proof of the proposition that run ( x ) . Such a term u is called a proof term and plays a key role in representing the dynamic notion of contexts for resolving anaphora in DTS .", "entities": []}
{"text": "In the following analysis , we mainly use two constructions , \u03a3 - types and \u03a0 - types . \u03a3 - type , written ( x : A ) \u00d7 B , is a generalization of product type A \u00d7 B. A term of type ( x : A ) \u00d7 B is a pair ( m , n ) such that m has type A and n has type B [ m / x ] . 1 The projection functions \u03c0 1 and \u03c0 2 are defined so that \u03c0 1 ( m , n ) = m and \u03c0 2 ( m , n ) = n. \u03a3 - types can be used to represent existential quantification . For instance , A man entered is given the translation ( 6 ) in DTS . 2 ( 6 ) ( u : ( x : e ) \u00d7 man ( x ) ) \u00d7 enter ( \u03c0 1 ( u ) ) Here u is a proof term of ( x : e ) \u00d7 man ( x ) , which is a pair of x having type e ( entity ) and a proof that x is a man . Thus , its first component ( the entity x ) can be picked up by the projection \u03c0 1 ( u ) . The entire translation means that there is an entity x such that x is a man and x enters . For notational simplicity , we often abbreviate \u03a3 - type of the form ( x : e ) \u00d7A ( x ) as A * , thus we write ( 6 ) as ( u : man * ) \u00d7enter ( \u03c0 1 ( u ) ) . \u03a0 - type , ( x : A ) B in our notation , is a generalization of function type A B. A term of type ( x : A ) B is a function f such that for any term m of type A , f ( m ) is of type B [ m / x ] . \u03a0 - type is used to represent universal quantification . Thus , Every man entered is given the translation in ( 7 ) . ( 7 ) ( u : man * ) enter ( \u03c0 1 ( u ) ) Note that when the variable x does not occur free in B , ( x : A ) \u00d7 B and ( x : A ) B can be written A \u00d7 B and A B , respectively . We illustrate how anaphora resolution works in DTS by the example A man entered and he smiled , which is given the following translation as an initial underspecified representation . ( 8 ) ( v : ( u : man * ) \u00d7 enter ( \u03c0 1 ( u ) ) ) \u00d7 smile ( @ 1 e ) In DTS , a pronoun is analyzed as an underspecified term @ , possibly annotated with its type A , which we write @A. We assume that in the initial underspecified representation , each occurrence of underspecified term @ is assigned a mutually distinct index . In the above example , the pronoun he corresponds to @ 1 e. This underspecified term searches for its antecedent of type e in the context represented as a proof term . The initial step to resolve anaphora is type checking , which is a process to ensure that a given expression is a type ( i.e. a well - formed proposition ) . This amounts to proving that it has type type , abbreviated as t. The formation rules ( see Figure 1 ) tell us when a given expression has type t. In the case of ( 6 ) , the goal is to prove that the representation in ( 6 ) has type t. In this case , no underspecified term appears , thus using the inference rules in Figure 1 , we have the following closed derivation . Here we assume that type assignments ( signatures ) such as e : t and enter : e t are in the initial context and can be used as an axiom . To simplify derivations , we usually omit axioms and use the name of the predicate applied ( possibly with its type ) as a rule label . If an initial representation contains an underspecified term @ , the process of type checking tells us in what context the antecedent of the @ - term can be found . For this purpose , we use the following rule : ( 10 ) A : t A true @ i A : A @ We use a judgement of the form A true to mean that there exists a term of type A ; in other words , type A is inhabited . Using this rule , the type checking for ( 8 ) gives an open derivation as follows . . . . . ( 9 ) ( u : man * ) \u00d7 enter ( \u03c0 1 ( u ) ) : t e : t Ax e true @ 1 e : e @ smile ( @ 1 e ) : t smile ( v : ( u : man * ) \u00d7 enter ( \u03c0 1 ( u ) ) ) \u00d7 smile ( @ 1 e ) : t \u03a3F , 1 Here the derivation starts from the open premise e true . Once we prove e true and find a witness for @ i , it becomes a closed derivation . To formalize this idea , we use the following rule for @ - elimination . ( 12 ) @ - elimination : Let A be a term in which no @ - term occurs . Then the derivation on the left can be transformed into the derivation on the right : . . . . A : t . . . . D 2 u : A A true @ i A : A @ . . . . D 1 . . . . D 2 u : A . . . . D 1 [ u/@ i A ] This rule allows us to replace the underspecified term @ i A with its witness u in the entire derivation . To find a witness for an underspecified term @ , we need to do proof search in a given local context . In the case of ( 11 ) , the application of \u03a3F rule at the final step allows us to use a proof term for the left - side proposition , ( u : man * ) \u00d7 enter ( \u03c0 1 ( u ) ) , to find a witness for @ 1 e. It can be easily seen that one such witness is \u03c0 1 ( \u03c0 1 ( v ) ) ; in this case , we say @ 1 e is bound to \u03c0 1 ( \u03c0 1 ( v ) ) . 3 Thus we have a closed derivation on the left below and it can be transformed to the derivation on the right by @ - elimination . . . . . ( 9 ) ( u : man * ) \u00d7 enter ( \u03c0 1 ( u ) ) : t e : t Ax [ v : ( u : man * ) \u00d7 enter ( \u03c0 1 ( u ) ) ] 1 \u03c0 1 ( v ) : man * \u03a3E \u03c0 1 ( \u03c0 1 ( v ) ) : e \u03a3E e true @ 1 e : e @ smile ( @ 1 e ) : t smile ( v : ( u : man * ) \u00d7 enter ( \u03c0 1 ( u ) ) ) \u00d7 smile ( @ 1 e ) : t \u03a3F , 1 . . . . ( 9 ) ( u : man * ) \u00d7 enter ( \u03c0 1 ( u ) ) : t [ v : ( u : man * ) \u00d7 enter ( \u03c0 1 ( u ) ) ] 1 \u03c0 1 ( v ) : man * \u03c0 1 ( \u03c0 1 ( v ) ) : e \u03a3E smile ( \u03c0 1 ( \u03c0 1 ( u ) ) ) : t smile ( v : ( u : man * ) \u00d7 enter ( \u03c0 1 ( u ) ) ) \u00d7 smile ( \u03c0 1 ( \u03c0 1 ( u ) ) ) : t \u03a3F , 1 The final representation can be read off from the bottom line of the derivation on the right . ( 13 ) ( v : ( u : man * ) \u00d7 enter ( \u03c0 1 ( u ) ) ) \u00d7 smile ( \u03c0 1 ( \u03c0 1 ( v ) ) ) This is equivalent to saying that there is an entity x such that it satisfies man ( x ) , enter ( x ) , and smile ( x ) . Our analysis naturally accounts for the bound reading of ( 14a ) , whose translation is given in ( 14b ) . ( 14 ) a. Every Englishman thinks he is a genius . b. ( u : eng * ) think ( genius ( @ 1 e ) ) ( \u03c0 1 ( u ) ) The derivation on the left shows the type checking with proof search to find a witness for @ 1 e in ( 14b ) . ( 15 ) [ u : eng * ] 1 \u03c0 1 ( u ) : e e true @ 1 e : e @ genius ( @ 1 e ) : t genius [ u : eng * ] 1 \u03c0 1 ( u ) : e think ( genius ( @ 1 e ) ) ( \u03c0 1 ( u ) ) : t think : t e t ( u : eng * ) think ( genius ( @ 1 e ) ) ( \u03c0 1 ( u ) ) : t \u03a3F , 1 [ u : eng * ] 1 \u03c0 1 ( u ) : e genius ( \u03c0 1 ( u ) ) : t genius [ u : eng * ] 1 \u03c0 1 ( u ) : e think ( genius ( \u03c0 1 ( u ) ) ) ( \u03c0 1 ( u ) ) : t think : t e t ( u : eng * ) think ( genius ( \u03c0 1 ( u ) ) ) ( \u03c0 1 ( u ) ) : t \u03a3F , 1 Here the premise e true follows from the hypothesis u : eng * licensed by the application of \u03a3F . Thus @ 1 e is bound to \u03c0 1 ( u ) and the @ - term can be eliminated as shown in the derivation on the right . This yields the bound reading ( u : eng * ) think ( genius ( \u03c0 1 ( u ) ) ) ( \u03c0 1 ( u ) ) for ( 14a ) , as desired . An alternative treatment of indefinites In the classical version of DTS , pronouns and definites are translated as underspecified terms , while indefinites are not . Here we propose an alternative analysis that translates an indefinite to an underspecified term of the form # A where A is a \u03a3 - type . This alternative analysis translates the sentence ( 6a ) as follows ( note that man * is an abbreviation for ( x : e ) \u00d7 man ( x ) ) : ( 16 ) enter ( \u03c0 1 ( # man * ) ) For underspecified terms # , we use the following rule . 4 ( 17 ) A : t # A : A", "entities": []}
{"text": "In this paper , we have proposed an analysis of the interpretive parallelism for anaphora and scope in the so - called Geach sentences involving right - node raising . In the proposed analysis , the parallel interpretation requirement on pronouns and indefinites in the shared right periphery is a consequence of the way underspecified terms are interpreted in the underspecification language that mediates the compositional semantic representation straightforwardly derived from the syntactic derivation and the fully resolved semantic representation that explicitly encodes all the relevant logical entailment relations . The natural next question is whether the present approach can be extended to the ellipsis cases . Preliminary results suggest a positive answer to this question , but a detailed analysis is a task for future research .", "entities": []}
{"text": "This work was supported by JSPS KAKENHI JP15K16732 , the NINJAL collaborative research project ' Cross - linguistic Studies of Japanese Prosody and Grammar ' and the OSU College of the Arts and Sciences Larger Grant .", "entities": []}
{"text": "This section discusses each dataset used in our taskoriented pre - training and how we process the data . Then we introduce the selected pre - training base model and its objective functions .", "entities": []}
{"text": "A Appendices", "entities": []}
{"text": "On the Computational Power of Transformers and its Implications in Sequence Modeling", "entities": []}
{"text": "In this section , we describe the specifics of our experimental setup . This includes details about the dataset , models , setup and some sample outputs .", "entities": []}
{"text": "We thank the anonymous reviewers for their constructive comments and suggestions . We would also like to thank our colleagues at Microsoft Research and Michael Hahn for their valuable feedback and helpful discussions .", "entities": []}
{"text": "We conduct the ablation experiments to distinguish the contribution of each part . There are several different variants of our model . SWRM is our proposed full model . SWRM w/o Position does not Table 2 shows the results of the variants of our model . After ablating the sentiment word position location module , SWRM w/o Position obtains worse results than SWRM , which indicates that finding the right word for refinement is very important . The comparison between SWRM w/o Attention and SWRM w/o Position further demonstrates this conclusion . SWRM w/o Attention first detects the right position and then incorporates the information of the special word [ MASK ] , which achieves better performance than SWRM w/o Position . But SWRM w/o Attention is still worse than SWRM , which shows using the attention network to incorporating extra information from the candidate words is useful for refinement . Comparing the SWRM w/o Multi - modal between SWRM , we can find that the model benefits from the visual and acoustic features . It is in line with our expectations since the sentiment information provided by the multimodal features can help the model detect the sentiment word and incorporate the sentiment - related information from the candidate words .", "entities": []}
{"text": "This work was supported by the following Grants : National Natural Science Foundation of China ( No . 62176078 ) , National Key R&D Program of China ( No . 2018YFB1005103 ) .", "entities": []}
{"text": "In this section we make a brief description of the system submitted for the different subtasks . We presented our submission for English restaurants dataset for subtask 1 , slots 1 , 2 and 3 , and subtask 2 , slots 1 and 3 . For English laptops dataset we sent a submission for subtasks 1 and 2 only in slot 3 . Then , the system was also developed for Spanish language and restaurants dataset in subtasks 1 , slots 1 and 2 and subtask 2 , slot 1 . In the next subsections we describe the different stages carried out for obtaining all the different results .", "entities": []}
{"text": "Subtask 2 is similar to subtask 1 , but instead of implementing aspect detection at sentence - level , it is performed at text - level . Participants are asked to implement slots 1 and 3 for this subtask . We participate in slot 1 for Spanish and English language , following the same procedure for both . Slot 3 is just implemented for English language for restaurants and laptops datasets .", "entities": []}
{"text": "Similarly to slot 1 , we use the output from subtask 1 slot 3 as input for this slot . All the polarities found are again grouped for all the sentences contained in the review and added them to text - level . If there are different polarities for the same category , some rules are applied : if polarities are negative and neutral , negative is finally assigned ; if there are positive and neutral opinions , positive polarity is assigned ; if there are positive and negative opinions for the same category , the tag \" conflict \" is assigned to that category at review - level . Moreover , as RESTAURANT#GENERAL is compulsory for every review , if no sentence has this category assigned , we take into account all the polarities of the other categories found and then assign the polarity for this category . Again , if there are different polarities containing positive and negative , \" conflict \" tag is assigned . The same process is followed for laptops dataset , with the LAPTOPS#GENERAL category .", "entities": []}
{"text": "This work was supported by the Spanish Government , co - financed by the European Regional Development Fund ( ERDF ) under project TACTICA .", "entities": []}
{"text": "Discretized Integrated Gradients for Explaining Language Models", "entities": []}
{"text": "In this section , we first describe our proposed Discretized integrated gradients ( DIG ) and the desirable explanation axioms satisfied by it . Then we describe an interpolation algorithm that leverages our DIG in discrete textual domains . Please refer to Appendix A for a brief introduction of the attribution - based explanation setup and the integrated gradients method .", "entities": []}
{"text": "Here , we describe our proposed interpolation algorithm that searches for intermediate interpolation points between the input word embedding and the baseline embedding . Once we have the desired interpolation points , we can use Equation 3 to compute the word attributions similar to the IG algorithm . Please refer to Section A.2 for more details about application of IG to text . Design Consideration . First , we discuss the key design considerations we need to consider of our interpolation algorithm . Clearly , our interpolation points need to satisfy the monotonicity constraints defined in Equation 2 so that we can use the Riemann sum approximation of DIG . Hence , we need to ensure that every intermediate point lies in a monotonic path . Also , the interpolation points should lie close to the original words in the embedding space to ensure that the model gradients faithfully define the model behavior . Now , we define the notion of closeness for our specific use - case of explaining textual models . To calculate how far the interpolated words are from some true word embedding in the vocabulary , we can compute the distance of the interpolated point from the nearest word in the vocabulary . We define this as the word - approximation error ( WAE ) . More specifically , if w k denotes the k th interpolation point for a word w , then its word - approximation error along the interpolated path is defined as : WAE w = 1 m m k=1 min x V dist ( w k \u2212 x ) , ( 4 ) where V is the embedding matrix of all the words in the vocabulary . WAE of a sentence is the average WAE of all words in the sentence . Intuitively , minimizing WAE will ensure that the interpolated points are close to some real word embedding in the vocabulary which in turn ensures that output gradients of F are not computed for some out - ofdistribution unseen embedding points . We observe that to minimize WAE without the monotonic constraints defined in Section 2.1 , one can define some heuristic to search for interpolation points that belong to the set V ( i.e. , select words from the vocabulary as interpolation points ) , leading to a zero WAE . Motivated by this , for a given input word embedding , we first search for an anchor word from the vocabulary that can be considered as the next interpolation point . Since the anchor point need not be monotonic w.r.t . the given input , we then optimally perturb the dimensions of the anchor word so that they satisfy the monotonicity constraints in Equation 2 . This perturbed point becomes our first interpolation . For subsequent interpolation points , we repeat the above steps using the previous anchor and perturbed points . Formally , we break our interpolation algorithm into two parts : ( i ) ANCHORSEARCH : In this step , given the initial word embedding w , we search for an anchor word embedding a V . ( ii ) MONOTONIZE : This step takes the anchor embedding a and modifies its dimensions to create a new embedding c such that all dimensions of c are monotonic between the input w and the baseline w . Overall , given an initial input word embedding w and a baseline embedding w , our interpolation algorithm interpolates points from w to w ( which is in decreasing order of k in Eq . 3 ) . It proceeds by calling ANCHORSEARCH on w to get an anchor word a. Then , it applies MONOTONIZE on a to get the monotonic embedding c. This is our first interpolated point ( in reverse order ) , i.e. , c = w m\u22121 . Now , the a becomes the new w for the next iteration and the process continues till m steps . Next , we describe in detail our specific formulations of the MONOTONIZE and ANCHORSEARCH algorithms .", "entities": []}
{"text": "In this paper , we proposed Discretized integrated gradients ( DIG ) which is effective in explaining models working with discrete text data . Further , we proposed two interpolation strategies - DIG - GREEDY and DIG - MAXCOUNT that generate non - linear interpolation paths for word embedding space . Finally , we established the effectiveness of DIG over integrated gradients and other gradientbased baselines through experiments on multiple language models and datasets . We also conduct human evaluations and find that DIG enhances human trust on model predictions .", "entities": []}
{"text": "Attribution - based explanations generate a scalar score for a given input feature that indicates the contribution ( or importance ) of that feature towards particular label ( Ancona et al , 2018 ) . Formally , let x = [ x 1 , . . . , x N ] R N be an input to a model which produces an output y = [ y 1 , . . . , y C ] , where C is the total number of labels . For a given label ( usually the label predicted by the model ) , attribution - based explanation methods compute the contribution R c = [ R c 1 , . . . , R c N ] R N of each feature .", "entities": []}
{"text": "In this section , we redefine the evaluation metrics and state the formulations for each of them . In this work , we use the following three automated metrics : Log - odds ( LO ) score ( Shrikumar et al , 2017 ) is defined as the average difference of the negative logarithmic probabilities on the predicted class before and after masking the top k% features with zero padding . Given the attribution scores generated by an explanation algorithm , we select the top k% words based on their attributions replace them with zero padding . More concretely , for a dataset with N sentences , it is defined as : log \u2212 odds ( k ) = 1 N N i=1 log p \u0177 | x ( k ) i p ( \u0177 | x i ) , where\u0177 is the predicted class , x i is the i th sentence , and x ( k ) i is the modified sentence with top k% words replaced with zero padding . Lower scores are better . Comprehensiveness ( Comp ) score ( DeYoung et al , 2020 ) is the average difference of the change in predicted class probability before and after removing the top k% features . Similar to Log - odds , this measures the influence of the top - attributed words on the model 's prediction . It is defined as : Comp ( k ) = 1 N N i=1 p ( \u0177 | x ( k ) i ) \u2212 p ( \u0177 | x i ) . Here x ( k ) i denotes the modified sentence with top k% words deleted from the sentence . Higher scores are better . Sufficiency ( Suff ) score ( DeYoung et al , 2020 ) is defined as the average difference of the change in predicted class probability before and after keeping only the top k% features . This measures the adequacy of the top k% attributions for model 's prediction . It is defined in a similar fashion as comprehensiveness , except the x ( k ) i is defined as the sentence containing only the top k% words . Lower scores are better .", "entities": []}
{"text": "Here , we report the detailed analysis of the effect of increasing m and f in Tables 7 and 8 respectively . In Table 7 , we report the Log - odds score along with Delta % . We do not note any consistent trend in Log - odds with increasing m for both IG and DIG . The results of IG suggest that , as long as the Delta % is sufficiently low , decreasing Delta % any further does n't impact the explanations very significantly . Further , in", "entities": []}
{"text": "In this section , we study the effect of increasing the neighborhood size in DIG . The results are shown in Table 9 . We observe a clear decreasing trend in Delta % with increasing neighborhood size , but there is no clear trend on Log - odds or WAE . Hence , we believe that the neighborhood size has little impact on the explanation quality , but we should still ensure sufficiently low Delta .", "entities": []}
{"text": "In this section , we briefly discuss the computational complexity of our proposed interpolation strategies . The algorithms for DIG - GREEDY and DIG - MAXCOUNT are presented in Algorithms 1 and 2 respectively . From there , we observe that both our algorithms have a running time complexity of O ( nmK ) , where n is the number of words , m is the number of interpolation points , and K is the KN N V neighborhood size . While it is computationally feasible to parallelize the loops corresponding to n and K , the same can not be said for the loop corresponding to m because we select the interpolation points iteratively . Although we empirically find in Section F.1 that a small number of interpolation points are sufficient to calculate the explanations , we believe this bottleneck can be further tackled through efficient design of noniterative search algorithms . We leave this for future works .", "entities": []}
{"text": "Discovering Black Lives Matter Events in the United States : Shared Task 3 , CASE 2021", "entities": []}
{"text": "As a usability analysis , no training data were provided for this Task . Namely , the event definition applied for coding the reference event data set is the same as the one adopted for Shared Task 1 ( H\u00fcrriyetoglu et al , 2021a ) and any data utilized for Task 1 and Task 2 , such as the one from H\u00fcrriyetoglu et al ( 2021 ) , or any additional data could be used to build a system / model run on the input data .", "entities": []}
{"text": "We provide two types of input data . The first is a generic , not topic filtered collection of all news items ( Title and Lead Paragraph ) from the New York Times for the target time range May 25th - June 30th . The second is a collection of Black Lives Matter related tweets ( Giorgi et al , 2020 ) . New York Times The New York Times ( NYT ) data sets consists of 5 , 347 articles published between May , 25 and June 30 , 2020 . The data associated with each article includes published date , print headline , lead paragraph , web URL , authors , and an abstract , among other meta - data . This is a general set of NYT articles ( i.e. , articles may or may not be related to BLM ) , unlike the Twitter data set which only contains tweets related to BLM or counter protests ( e.g. , All Lives Matter and Blue Lives Matter ) .", "entities": []}
{"text": "We used an open source data set of tweets containing keywords related to Black Lives Matter and the counter protests : All Lives Matter and Blue Lives Matter . While this data set contains tweets dating back to the origins of the Black Lives Matter movement , the tweets used in this task are limited to the date range : May 25 , 2020 ( the date of George Floyd 's murder ) to June 30 , 2020 . These tweets were pulled in real time using the Twitter API 's keyword matching with the following three keywords : BlackLivesMatter , Al - lLivesMatter , and BlueLivesMatter . This data set consists of 30 , 160 , 837 tweets . Participants were given full access to each tweet 's meta - data ( including the tweet 's text ) , which could include URLs , location information , and dates .", "entities": []}
{"text": "System performance is evaluated by computing correlation coefficients on event counts aggregated on cell - days , using uniform grid cells of approximately 55 kilometers sides from the PRIO - GRID data set ( Tollefsen et al , 2012 ) . We use these analytical measures as a proxy to the spatio - temporal pattern of the BLM protest movement .", "entities": []}
{"text": "In order to be joined with PRIO - GRID shapefiles , string - like location information of system output data had to be normalized to coordinate pairs . To do this we used the OpenStreetMap Nominatim search API 5 . For structured location name representations ( i.e. , city , state , country ) we used a parametric search , otherwise we used free - form query strings . We note that geographical coordinate conversion from Nominatim places the event at the geographical centroid of the polygon of the assigned administrative unit . In our evaluation , we discarded the system output event records with no source location information or whose string - like location attribute returned null results in Nominatim API .", "entities": []}
{"text": "We use the cell - days counts for two different analysis : the correlation with the total daily \" protest cell \" counts ( i.e. , time trends alone ) and the event counts for each cell - day ( i.e. , spatial and temporal trends together ) .", "entities": []}
{"text": "As a baseline , we used the output from NEXUS , a state - of - the - art engine for events detection from news ( Tanev et al , 2008 ) that has been used in the area of security and disaster management 6 . We denote this system as Baseline throughout . Nexus is based on a blend of rule - based cascaded grammars for detection event slots ( i.e. perpetrator , various types of affected people , infrastructure and vehicle targets and weapons used ) , and a combination of keyword - based and statistical classifiers for detection of event classes . The dictionaries underlying the extraction grammars of the system have been learned using weakly supervised lexical learning on generic news corpora . No learning was performed on domain corpora in protest movements or related themes . Details on Nexus full taxonomy of event categories can be found in Atkinson et al ( 2017 ) . For this task , we filter the events belonging to the following type set : Disorder / Protest / Mutiny , Boycott / Strike , Public Demonstration , Riot / Turmoil , Sabotage / Impede , Mutiny . NEXUS performs event geocoding by ( 1 ) matching populated place names from the GeoNames gazetteer 7 in the news item ; ( 2 ) resolving them into unique location entities via disambiguation heuristics ( Pouliquen et al , 2006 ) ; and ( 3 ) selecting a single main event location based on the text proximity with the matched event components ( see the slots above ) in the news article . In order to mitigate the lack of geographical context in the tweet body , when processing the Twitter data , we ran Nexus on an enriched text , which included the String value of the full name field in the Place child object of the tweet , whenever that was available 8 . This resulted in a small fraction of 32 , 085 tweets with geographical information ( out of the roughly 30 million tweets originally sampled ) . For the sake of comparison , we shared with participants this subset of tweets , together with the assigned location .", "entities": []}
{"text": "The author from Koc University was funded by the European Research Council ( ERC ) Starting Grant 714868 awarded to Dr. Erdem Y\u00f6r\u00fck for his project Emerging Welfare . The authors from the National Institute on Drug Abuse were supported in part by the Intramural Research Program of the NIH , National Institute on Drug Abuse ( NIDA ) .", "entities": []}
{"text": "This section describes the structural causal model ( SCM ) for FSED , illustrated in Figure 1 ( a ) . Note that , we omit the causal structure of the query for simplicity since it is the same as the support set . Concretely , the SCM formulates the data distribution of FSED : 1 ) Starting from an event E we want to describe ( in Figure 1 ( a ) is an Attack in Iraqi ) . 2 ) The path E T indicates the trigger decision process , i.e. , selecting words or phrases ( in Figure 1 ( a ) is fire ) which can almost clearly express the event occurrence ( Doddington et al , 2004 ) . 3 ) The path E C T indicates that a set of contexts are generated depending on both the event and the trigger , which provides background information and organizes this information depending on the trigger . For instance , the context \" They killed by hostile [ fire ] in Iraqi \" provides the place , the role and the consequences of the event , and this information is organized following the structure determined by fire . 4 ) an event instance is generated by combining one of the contexts in C and one of the triggers in T via the path C S T . 5 ) Finally , a matching between query and support set is generated through S Y Q. Conventional learning criteria for FSED directly optimize towards the conditional distribution P ( Y | S , Q ) . However , from the SCM , we found that the backdoor path C T Y pass on associations ( Pearl et al , 2016 ) and mislead the learning with spurious correlation . Consequently , the learning procedure towards P ( Y | S , Q ) will mistakenly regard the effects of triggers as the effects of contexts , and therefore overfit the trigger information .", "entities": []}
{"text": "To further demonstrate the effectiveness of the proposed method , we also conduct experiments under different FSED settings : 1 ) The primal episodebased settings ( Episode ) , which is the 5 + 1 - way 5 - shot settings in Lai et al ( 2020 ) . 2 ) Episode + ambiguous instances ( Ambiguity ) , which samples some additional negative query instances that include words same as triggers in support set to verify whether models overfit the triggers . The performance of different models with different settings is shown in Figure 2 . We can see that : 1 ) Generally speaking , all models can achieve better performance on Episode because correctly recognize high - frequent triggers can achieve good performance in this setting . Consequently , the performance under this setting can not well represent how FSED is influenced by trigger overfitting . 2 ) The performance of all models dropped on Ambiguity setting , which suggests that trigger overfitting has a significant impact on FSED . 3 ) Our method still maintains good performance on Ambiguity , which indicates that our method can alleviate the trigger curse problem by optimizing towards the underlying causality .", "entities": []}
{"text": "We select ambiguous cases ( in Table 2 ) to better illustrate the effectiveness of our method . For Query 1 , FS - Base wrongly detects the word run to be a trigger word . In Support set 1 , run means nomi - nating while run means managing in Query 1 . FS - Base fails to recognize such different sense of word under context . For Query 2 , FS - Base makes mistake again on the ambiguous word suspect . Even though suspect is the noun form of suspected in Support set 2 , it does not trigger a Suspicion event in Query 2 . In contract to FS - Base , our approach is able to handle both cases correctly , illustrating its effectiveness .", "entities": []}
{"text": "This paper proposes to revisit the trigger curse in FSED from a causal view . Specifically , we identify the cause of the trigger curse problem from a structural causal model , and then solve the problem through casual intervention via backdoor adjustment . Experimental results demonstrate the effectiveness and robustness of our methods .", "entities": []}
{"text": "We prove the backdoor adjustment for our SCM using the rules of do - calculus ( Pearl , 1995 ) . For a causal graph G , let G X denote the graph where all of the incoming edges to Node X are removed . let G X denote the graph where all of the outgoing edges from Node X are removed . \u22a5 \u22a5 G denotes d - separation in G. D - separation ( Pearl , 2014 ) : Two ( sets of ) nodes X and Y are d - separation by a set of nodes Z ( i.e. X \u22a5 \u22a5 G Y | Z ) if all of the paths between ( any node in ) X and ( any node in ) Y are blocked by Z. The rules of do - calculus are : Rule 1 P ( y | do ( t ) , z , w ) = P ( y | do ( t ) , w ) if Y \u22a5 \u22a5 G T Z | T , W Rule 2 P ( y | do ( t ) , do ( z ) , w ) = P ( y | do ( t ) , z , w ) if Y \u22a5 \u22a5 G T Z Z | T , W Rule 3 P ( y | do ( t ) , do ( z ) , w ) = P ( y | do ( t ) , w ) if Y \u22a5 \u22a5 G T Z ( W ) Z | T , W ( 5 ) where Z ( W ) denotes the set of nodes of Z that are n't ancestors of any node of W in G T . We can prove our interventional distribution P ( Y | do ( C = C ) , E = e ) : Step 1 Using the law of total probability : Step 2 Using the law of conditional probability : P ( Y | do ( C = C ) , E = e , Q = q ) = t T P ( Y | do ( C = C ) , E = e , Q = q ) = t T s S [ P ( Y | do ( C ) , e , t , s , q ) \u00d7 P ( s | do ( C ) , e , t , q ) P ( t | do ( C ) , e , q ) ] Step 3 Using the Rule 3 : P ( Y | do ( C = C ) , E = e , Q = q ) = t T s S [ P ( Y | e , t , s , q ) \u00d7 P ( s | do ( C ) , e , t , q ) P ( t | e , q ) ] Step 4 Using the Rule 1 : P ( Y | do ( C = C ) , E = e , Q = q ) = t T s S P ( Y | s , q ) P ( s | do ( C ) , t ) P ( t | e ) Step 5 Using the Rule 2 : P ( Y | do ( C = C ) , E = e , Q = q ) = t T s S P ( Y | s , q ) P ( s | C , t ) P ( t | e )", "entities": []}
{"text": "The hyperparameter is shown in Table 5 . During training , the support set and the query is sampled in training set , the query contains 2 positive instances and 10 negative instances ( 5 times of positive instances ) . During validating , the support set and the query is sampled in dev set , the query contains 10 positive instances and 100 negative instances ( 10 times of positive", "entities": []}
{"text": "We thank the reviewers for their insightful comments and helpful suggestions . This research work is supported by National Key R&D Program of China under Grant 2018YFB1005100 , the National Natural Science Foundation of China under Grants no . 62106251 and 62076233 , and in part by the Youth Innovation Promotion Association CAS ( 2018141 ) .", "entities": []}
{"text": "instances ) . The results of dev set are shown in Table 3 . For FS - Causal , we found that there is an impact on whether backdoor adjustment is applied separately to the support set and query , as shown in Table 4 . Based on the best results of the dev set , we evaluate it on the test set .", "entities": []}
{"text": "Context - aware Interactive Attention for Multi - modal Sentiment and Emotion Analysis", "entities": []}
{"text": "In this section , we present our experimental results along with necessary analysis . We also compare our obtained results with several state - of - the - art systems .", "entities": []}
{"text": "The research reported here is partially supported by SkyMap Global India Private Limited . Asif Ekbal acknowledges the Young Faculty Research Fellowship ( YFRF ) , supported by Visvesvaraya PhD scheme for Electronics and IT , Ministry of Electronics and Information Technology ( MeitY ) , Government of India , being implemented by Digital India Corporation ( formerly Media Lab Asia ) .", "entities": []}
{"text": "As an extension of SE - Graph , IRSE - Graph can be denoted as G= ( V , E , W ) , where V and E share the same definitions with those of SE - Graph . Particularly , in IRSE - Graph , each ss - edge e i , i is a directed one with a weight w i , i W indicating the probability of sentence s i occurring before sentence s i . Meanwhile , there must exist a corresponding ssedge e i , i with the weight w i , i = 1\u2212w i , i denoting the probability of s i appearing after s i . For example , in Figure 2 , for two linked sentence nodes v 1 and v 2 , there exist two ss - edges e 1 , 2 and e 2 , 1 with weights w 1 , 2 and w 2 , 1 respectively , both of which are iteratively updated during constructing IRSE - Graph .", "entities": []}
{"text": "Since pairwise ordering plays a crucial role in our proposed framework , we first compare the performance of different classifiers on various datasets . the predictions of pairwise orderings .", "entities": []}
{"text": "To provide more experimental results , we summarize the runtime on the validation sets and the numbers of parameters for our enhanced models and baseline SE - GRN in Table 6 .", "entities": []}
{"text": "The project was supported by National Key Research and Development Program of China ( No . 2020AAA0108004 ) , National Natural Science Foundation of China ( No . 61672440 ) , Natural Science Foundation of Fujian Province of China ( No . 2020J06001 ) , and Youth Innovation Fund of Xiamen ( No . 3502Z20206059 ) . We also thank the reviewers for their insightful comments .", "entities": []}
{"text": "Who wrote this book ? A challenge for e - commerce", "entities": []}
{"text": "Unlike brick - and - mortar stores , e - commerce websites can list hundreds of millions of products , with thousands of new products entering their catalogs every day . The availability and the reliability of the information on the products , or product data , is crucial for the products to be found by the users via textual or visual search , or using faceted navigation . Books constitute a prominent part of many large ecommerce catalogs . Relevant book properties include : title , author ( s ) , format , edition , and publication date , among others . In this work , we focus on the names of book authors , as they are found to be extremely relevant to the user and are commonly used in search queries on e - commerce websites , but suffer from considerable variability and noise . To the best of our knowledge , there is no large - scale public dataset for books that captures the variability arising on e - commerce marketplaces from user - generated input . Thus , in this work we use product data from Rakuten France ( RFR ) . 1 1 https://fr.shopping.rakuten.com The variability and noise is evident in the RFR dataset . For example , books written by F. Scott Fitzgerald are also listed with the following author 's names : \" Francis Scott Fitzgerald \" ( full name ) , \" Fitzgerald , F. Scott \" ( inversion of the first and last name ) , \" Fitzgerald \" ( last name only ) , \" F. Scott Fitgerald \" ( misspelling of the last name ) , \" F SCOTT FITZGERALD \" ( capitalization and different typological conventions ) , as well as several combinations of those variations . The variability of the possible spellings for an author 's name is very hard to capture using rules , even more so for names which are not primarily written in latin alphabet ( such as arabic or asian names ) , for names containing titles ( such as \" Dr. \" or \" Pr . \" ) , and for pen names which may not follow the usual conventions . This motivated us to explore automated techniques for normalizing the authors ' names to their best known ( \" canonical \" ) spellings . Fortunately , a wealth of open databases exist for books , making it possible to match a significant fraction of the books listed in e - commerce catalogs . While not always clean and unambiguous , this information is extremely valuable and enables us to build datasets of name variants , used to train machine learning systems to normalize authors ' names . To this end , in addition to the match with open databases , we will explore two different approaches : approximate match with known authors ' names using Siamese neural networks , and direct correction of the provided author 's name using sequenceto - sequence learning with neural networks . Then , an additional machine learning component is used to rank the results . The rest of the paper is organized as follows : we present the data from RFR and from the open databases in Section 2 , before turning to the experimental setup for the overall system and for each of its components in Section 3 . Finally , we give results in Section 4 , we present related works in Section 5 , and conclude in Section 6 .", "entities": []}
{"text": "The RFR dataset contains 12 million book references 2 . The most relevant product data for normalization is : ISBN 3 in 10 digit or 13 digit format ; F. S. product title , which includes the book title , often supplemented with extra information in free text ; author ( s ) of the book as the input catalog name provided by the seller . In particular , the ISBN is a worldwide unique identifier for books , which makes it a prime candidate for unambiguous matching with external sources . In this dataset , an ISBN is present for about 70 % of the books . Among the books with no ISBN , 30 % are ancient books which are not expected to be associated an ISBN .", "entities": []}
{"text": "There is no central authority providing consistent information on books associated with an ISBN . However , there is a wealth of bibliographic resources and open databases for books . In order to retrieve the author 's name ( s ) associated with the books in the RFR dataset , we perform ISBN matching using public APIs on eight of them , listed in Table 1 along with the fraction of found ISBNs from this dataset . We find the sources to be highly complementary and that 75 % of the books with an ISBN are matched with at least one source . The match via ISBN on external bibliographic resources is the first component of the system depicted in Fig . 1 .", "entities": []}
{"text": "In order to evaluate the overall system , we need product data from RFR for which the canonical author name has been carefully annotated and can be considered as the ground truth . To this end , we have considered a subset of 1000 books from the RFR dataset , discarding books written by more than one author for simplicity . 6 We find that 467 books have a canonical author name that differs from RFR 's original ( unnormalized ) author name . Also , 310 do not have an ISBN or do not match on any of the bibliographic resources listed in Section 2.2 . Among them , 208 books have a canonical name that differs from the input catalog name provided by the seller .", "entities": []}
{"text": "The overview of the system can be found in Fig . 1 . Its first component , the matching via ISBN against external databases , has already been presented in Section 2.2 . In the rest of this section , we will shed light on the three machine learning components of the system .", "entities": []}
{"text": "We provided a first attempt at solving the problem of author name normalization in the context of books sold on e - commerce websites . To this end , we used a composite system involving open data sources for books , approximate match with Siamese networks , name correction with sequence - to - sequence networks , and ranking of the proposals . We find that 72 % of the books have the author 's name normalized by the highest ranked proposal . In order to facilitate future research , we are releasing data from Rakuten France : a large dataset containing product information , and a subset of it with expert human annotation for the authors ' names . They are accessible at rit.rakuten.co.jp/data_release . Multiple challenges remain and are left for future research . First , the system should be extended to handle the case of books with multiple authors . In addition , the book title could be used to help disambiguate between authors and to query external bibliographic resources . This work can also be seen as an intermediate step towards a knowledge base for book author names with name variants , extending public ones such as BnF , using the ISNI 8 for easier record linkage whenever available .", "entities": []}
{"text": "We thank Rapha\u00ebl Ligier - Tirilly for his help with the deployment of the system as microservices , and Laurent Ach for his support .", "entities": []}
{"text": "Reevaluating Adversarial Examples in Natural Language", "entities": []}
{"text": "State - of - the - art attacks on NLP models lack a shared definition of what constitutes a successful attack . These differences make the attacks difficult to compare and hindered the use of adversarial examples to understand and improve NLP models . We distill ideas from past work into a unified framework : a successful natural language adversarial example is a perturbation that fools the model and follows four proposed linguistic constraints . We categorize previous attacks based on these constraints . For each constraint , we suggest options for human and automatic evaluation methods . We use these methods to evaluate two state - of - the - art synonym substitution attacks . We find that perturbations often do not preserve semantics , and 38 % introduce grammatical errors . Next , we conduct human studies to find a threshold for each evaluation method that aligns with human judgment . Human surveys reveal that to successfully preserve semantics , we need to significantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences . With constraints adjusted to better preserve semantics and grammaticality , the attack success rate drops by over 70 percentage points . 1", "entities": []}
{"text": "We define F : X ! Y as a predictive model , for example , a deep neural network classifier . X is the input space and Y is the output space . We focus on adversarial perturbations which perturb a correctly predicted input , x 2 X , into an input x adv . The boolean goal function G ( F , x adv ) represents whether the goal of the attack has been met . We define C 1 ... C n as a set of boolean functions indicating whether the perturbation satisfies a certain constraint . Adversarial attacks search for a perturbation from x to x adv which fools F by both achieving some goal , as represented by G ( F , x adv ) , and fulfilling each constraint C i ( x , x adv ) . The definition of the goal function G depends on the purpose of the attack . Attacks on classification frequently aim to either induce any incorrect classification ( untargeted ) or induce a particular classification ( targeted ) . Attacks on other types of models may have more sophisticated goals . For example , attacks on translation may attempt to change every word of a translation , or introduce targeted keywords into the translation ( Cheng et al , 2018 ) . In addition to defining the goal of the attack , the attacker must decide the constraints perturbations must meet . Different use cases require different Input , x : \" Shall I compare thee to a summer 's day ? \" - William Shakespeare , Sonnet XVIII", "entities": []}
{"text": "Perturbation , x adv Explanation Semantics Shall I compare thee to a winter 's day ? x adv has a different meaning than x. Grammaticality Shall I compares thee to a summer 's day ? x adv is less grammatically correct than x. Edit Distance Sha1l i conpp$haaare thee to a 5umm3r 's day ? x and x adv have a large edit distance . Non - suspicion Am I gon na compare thee to a summer 's day ? A human reader may suspect this sentence to have been modified . 1 1 Shakespeare never used the word \" gon na \" . Its first recorded usage was n't until 1806 , and it did n't become popular until the 20th century . constraints . We build on the categorization of attack spaces introduced by Gilmer et al ( 2018 ) to introduce a categorization of constraints for adversarial examples in natural language . In the following , we define four categories of constraints on adversarial perturbations in natural language : semantics , grammatically , overlap , and non - suspicion . Table 1 provides examples of adversarial perturbations that violate each constraint .", "entities": []}
{"text": "Semantics constraints require the semantics of the input to be preserved between x and x adv . Many attacks include constraints on semantics as a way to ensure the correct output is preserved ( Zhang et al , 2019 ) . As long as the semantics of an input do not change , the correct output will stay the same . There are exceptions : one could imagine tasks for which preserving semantics does not necessarily preserve the correct output . For example , consider the task of classifying passages as written in either Modern or Early Modern English . Perturbing \" why \" to \" wherefore \" may retain the semantics of the passage , but change the correct label from Modern to Early Modern English 5", "entities": []}
{"text": "Grammaticality constraints place restrictions on the grammaticality of x adv . For example , an adversary attempting to generate a plagiarised paper which fools a plagiarism checker would need to ensure that the paper remains grammatically correct . Grammatical errors do n't necessarily change semantics , as illustrated in Table 1 .", "entities": []}
{"text": "Non - suspicion constraints specify that x adv must appear to be unmodified . Consider the example in Table 1 . While the perturbation preserves semantics and grammar , it switches between Modern and Early Modern English and thus may seem suspicious to readers . Note that the definition of the non - suspicious constraint is context - dependent . A sentence that is non - suspicious in the context of a kindergartner 's homework assignment might be suspicious in the context of an academic paper . An attack scenario where non - suspicion constraints do not apply is illegal PDF distribution , similar to a case discussed by Gilmer et al ( 2018 ) . Consumers of an illegal PDF may tacitly collude with the person uploading it . They know the document has been altered , but do not care as long as semantics are preserved . Attacks by Synonym Substitution : Some works focus on an easier way to generate a subset of paraphrases : replacing words from the input with synonyms ( Alzantot et al , 2018 ; Jin et al , 2019 ; Kuleshov et al , 2018 ; Papernot et al , 2016 ; Ren et al , 2019 ) . Each attack applies a search algorithm to determine which words to replace with which synonyms . Like the general paraphrase case , they aim to create examples that preserve semantics , grammaticality , and non - suspicion . While not all have an explicit edit distance constraint , some limit the number of words perturbed . Attacks by Character Substitution : Some studies have proposed to attack natural language classification models by deliberately misspelling words ( Ebrahimi et al , 2017 ; Gao et al , 2018 ; Li et al , 2018 ) . These attacks use character replacements to change a word into one that the model does n't recognize . The replacements are designed to create character sequences that a human reader would easily correct into the original words . If there are n't many misspellings , non - suspicion may be preserved . Semantics are preserved as long as human readers can correct the misspellings . Attacks by Word Insertion or Removal : Liang et al ( 2017 ) and Samanta and Mehta ( 2017 ) devised a way to determine the most important words in the input and then used heuristics to generate perturbed inputs by adding or removing important words . In some cases , these strategies are combined with synonym substitution . These attacks aim to follow all constraints . Using constraints defined in Section 2 we categorize a sample of current attacks in Table 2 .", "entities": []}
{"text": "A few past studies of attacks have included human evaluation of semantic preservation ( Ribeiro et al , 2018 ; Iyyer et al , 2018 ; Alzantot et al , 2018 ; Jin et al , 2019 ) . However , studies often simply ask users to simply rate the \" similarity \" of x and x adv . We believe this phrasing does not generate an accurate measure of semantic preservation , as users may consider two sentences with different semantics \" similar \" if they only differ by a few words . Instead , users should be explicitly asked whether changes between x and x adv preserve the meaning of the original passage . We propose to ask human judges to rate if meaning is preserved on a Likert scale of 1 - 5 , where 1 is \" Strongly Disagree \" and 5 is \" Strongly Agree \" ( Likert , 1932 ) . A perturbation is semantics - preserving if the average score is at least \u270f sem . We propose", "entities": []}
{"text": "Input , x Perturbation , x adv Semantics Jagger , Stoppard and director Michael Apted deliver a riveting and surprisingly romantic ride . Jagger , Stoppard and director Michael Apted deliver a baffling and surprisingly sappy motorbike . Grammaticality A grating , emaciated flick . A grates , lanky flick . Non - suspicion Great character interaction . Gargantuan character interaction . \u270f sem = 4 as a general rule : on average , humans should at least \" Agree \" that x and x adv have the same meaning .", "entities": []}
{"text": "Both Jin et al ( 2019 ) and Iyyer et al ( 2018 ) reported a human evaluation of grammaticality , but neither study clearly asked if any errors were introduced by a perturbation . For human evaluation of the grammaticality constraint , we propose presenting x and x adv together and asking judges if grammatical errors were introduced by the changes made . However , due to the rule - based nature of grammar , automatic evaluation is preferred .", "entities": []}
{"text": "We ran each of the generated ( x , x adv ) pairs through LanguageTool to count grammatical errors . LanguageTool detected more grammatical errors in x adv than x for 50 % of perturbations generated by TEXTFOOLER , and 32 % of perturbations generated by GENETICATTACK . Additionally , perturbations often contain errors that humans rarely make . LanguageTool detected 6 categories for which errors in the perturbed samples appear at least 10 times more frequently than in the original content . Details regarding these error categories and examples of violations are shown in Table 4 .", "entities": []}
{"text": "TFADJUSTED generated better quality adversarial examples by constraining its search to exclude examples that fail to meet three constraints : word embedding distance , sentence encoder similarity , and grammaticality . We performed an ablation study to understand the relative impact of each on attack success rate . We reran three TFADJUSTED attacks ( one for each constraint removed ) on each dataset . Table 6 shows attack success rate after individually removing each constraint . The word embedding distance constraint was the greatest inhibitor of attack success rate , followed by the sentence encoder .", "entities": []}
{"text": "We showed that two state - of - the - art synonym substitution attacks , TEXTFOOLER and GENETI - CATTACK , frequently violate the constraints they claim to follow . We created TFADJUSTED , which applies constraints that produce adversarial examples judged to preserve semantics and grammaticality . Due to the lack of a shared vocabulary for discussing NLP attacks , the source of improvement in attack success rate between TEXTFOOLER and GENETICATTACK was unclear . Holding constraint application constant revealed that the source of TEXTFOOLER 's improvement was lenient constraint application ( rather than a better search method ) . With a shared framework for defining and applying constraints , future research can focus on developing better search methods and better constraint application techniques for preserving semantics and grammaticality .", "entities": []}
{"text": "Let V denote the set of entities , R denote the set of binary relations and G denote a KB or equivalently a Knowledge Graph ( KG ) . Formally , G = ( V , E , R ) is a directed labeled multigraph where V and E denote the vertices and edges of the graph respectively . Note that , E \u2286 V \u00d7 R \u00d7 V . Let ( e 1 , r , e 2 ) denote a fact in G where e 1 , e 2 V and r E. Also , following previous approaches ( Bordes et al , 2013 ) , we add the inverse relation of every edge , i.e. , for an fact ( e 1 , r , e 2 ) E , we add the edge ( e 2 , r \u22121 , e 1 ) to the graph . ( If the set of binary relations R does not contain the inverse relation r \u22121 , it is added to R as well ) . Task : We consider the task of query answering on KGs , i.e. , answering questions of the form ( e 1q , r q , ? ) , where answer is an entity in the KG .", "entities": []}
{"text": "A path in a KG between two entities e s , e t is defined as a sequence of alternating entity and relations that connect e s and e t . A length of a path is the number of relation ( edges ) in the path . Formally , let a path p = ( e 1 , r 1 , e 2 , . . . , r n , e n+1 ) with st ( p ) = e 1 , en ( p ) = e n+1 and len ( p ) = n. We also define a path type as the sequence of the relations in p , i.e. , type ( p ) = ( r 1 , r 2 , . . . , r n ) . Let P denote the set of all paths in G. Let P n \u2286 P = { p | len ( p ) \u2264 n } be the set of all paths of length up to n. Also , let P n denote the set of all path types with length up to n , i.e. P n = { type ( p ) | p P n } . Let P n ( e 1 , r ) \u2286 P n denote all path types of length up to n that originate at e 1 and end at the entities that are connected to e 1 by a direct edge of type r. In other words , if S e 1 r = { e 2 | ( e 1 , r , e 2 ) G } denotes the set of entities that are connected to e 1 via a direct edge r , then P n ( e 1 , r ) denotes the set of all path types of length up to n that start from e 1 and end at entities in S e 1 r . By definition , r P n ( e 1 , r ) . Similarly , we define P n ( e 1 , r ) which contain paths instead of path types .", "entities": []}
{"text": "Next we discuss how to estimate path prior and precision terms . There exists abundant modeling choices to estimate them . For example , following Chen et al ( 2018 ) , we could train a neural network model to estimate P ( p | c e 1q , r q ) . However , with our original goal of designing a simple and efficient non - parametric model , we estimate these parameters by simple count statistics from the KG . E.g. , the path prior P ( p | c , r q ) is estimated as \u2211 e c c \u2211 p P n ( e c , r q ) 1 [ type ( p ) = p ] \u2211 e c c \u2211 p P n ( e c , r q ) 1 ( 2 ) For each entity in cluster c , we consider the paths that connect e c to entities it is directly connected to via edge type r q ( P n ( e c , r q ) in 2.1 ) . The path prior for a path type p is computed as the proportion of times the type of paths in P n ( e c , r q ) is equal to p. Let P n ( e c ) denote the paths of up to length n starting from the entity e c . Note , unlike P n ( e c , r q ) , the paths in P n ( e c ) do not have to end at specific entities . Also from 2.1 , en ( p ) denotes the end entity for a path p and S e c r q denotes the set of entities that are connected to e c via a direct edge of type r q . Equation 3 , therefore , estimates the proportion of times the path p successfully ends at one of the answer entities when starting from e c , given r q . There are several advantages in estimating the parameters using simple count statistics . Firstly , they are extremely simple , and statistics for each entity in clusters can be computed in parallel making them extremely time efficient . Secondly once they are computed , our approach needs no further training . Lastly , when new data is added , it makes it easy to update the parameters without training from scratch . To summarize , given a query entity ( e 1q , r q ) , our method gathers reasoning paths from k similar entities to e 1q . These reasoning paths are then traversed in the KG starting from e 1q , leading to a set of candidate answer entities . The score of each answer entity candidate is computed as a weighted sum of the reasoning paths the lead to them ( Equation 1 ) . Each path is weighed with an estimate of its frequency ( Equation 2 ) and precision ( Equation 3 ) given the query relation . The next section describes how we extend our model for open - world setting where new entities and facts are added to the KB .", "entities": []}
{"text": "In this section , we evaluate our proposed approach on a wide array of knowledge - base completion ( KBC ) benchmarks ( 3.3 ) . To evaluate the nonparametric nature of our approach , we also evaluate on an ' open - world ' setting ( 2.3 ) in which new entities are added to the KG . We demonstrate our proposed approach is competitive to several stateof - the - art methods on benchmarks in the standard setting , but it greatly outperforms other methods in the online setting ( 3.4 ) . The best hyper - parameters for all experiments including the range of hyperparameter tried and results on validation set are noted in A.6 .", "entities": []}
{"text": "( athlete - led - sports - team , team - plays - in - league ) ( athlete - home - stadium , league - stadiums \u22121 )", "entities": []}
{"text": "We present a simple yet accurate approach for probabilistic case - based reasoning in knowledge bases . Our method is non - parametric , deriving reasoning rules dynamically from similar entities in the KB and is capable of handling new entities . We cluster similar entities together and estimate per - cluster parameters that measures the prior and precision of paths using simple count statistics . Our simple approach performs competitively to the best embeddings based models on several benchmarks and outperforms all models in the open - world setting . Algorithm 1 Select a flat clustering from a tree structure . 1 : input : V : Entities , root : Root of tree , \u03c4 : ( 5 )", "entities": []}
{"text": "A hierarchical clustering T over the entities V , encodes a large number of flat partitions of the entities , often referred to as tree consistent partitions in the clustering literature . We select one of these tree consistent partitions using a threshold on the linkage function , \u03c4 . The algorithm performs a breadth first search starting at the root node . The search stops at any node for which the linkage is above the given value \u03c4 . Pseudocode is given in Algorithm 1 .", "entities": []}
{"text": "We analyze the number of entities that need to be re - clustered and added in each round . We observe that it is significantly fewer than the number of entities in the KB . Note that an online method like the one proposed in this paper just needs to run on the new and modified entities while a batch algorithm would need to run on the entire KB .", "entities": []}
{"text": "Proposition : Let n denote the maximum length of a reasoning path considered by our model . For every new entity e i added to the KG , we need to recompute statistics for entities that lie within cycles of length up to ( n + 1 ) starting from e i . We see from Eq 2 , that the estimate for the prior for a path type p depends on P n ( e c , r q ) i.e. the set of paths that lead from e c to entities that are connected to e c via relation r q . WLOG , say e t is such an entity i.e. ( e c , r q , e t ) G. When a new entity / edge is added to the KG , this set of paths might increase . It is easy to see that the set P n ( e c , r q ) is updated iff a new path p new of length \u2264 n appears between e c and e t . In this case , the edges in p new would form a cycle with the edge ( e c , r q , e t ) . The length of the cycle would be at most len ( p new ) + 1 which in turn is at most of length n + 1 . This , to find entities for which the prior has changed after the addition of a new edge / entity , it is sufficient to find entities lying on cycles of length up to n + 1 starting from the new entity / edge . This mechanism for finding entities for recomputation is only approximate when computing the precision . We see from Eq 3 , that the numerator depends on paths that lead to the answer entity ( as with prior ) while denominator depends on all n length paths around e c . So , if the numerator is ever to be increased , we would catch that update by the proposed cycle finding method . However , even if an entity does not lie on a cycle with the new edge / entity , if there is a path of length n from e c to the new edge / entity , the denominator count would", "entities": []}
{"text": "be incremented . Thus , the precision estimates for some entities might be an over - estimate of the path precision ( had it been recomputed after new edges are added to the KB ) .", "entities": []}
{"text": "Table 7 shows some example of new entities arriving and getting assigned to their respective clusters by GRINCH .", "entities": []}
{"text": "Group , Extract and Aggregate : Summarizing a Large Amount of Finance News for Forex Movement Prediction", "entities": []}
{"text": "In this part , we introduce the three news grouping methods . The ideal division enables news groups to be high cohesion and low coupling , which means the semantic information of finance news should be highly related intra - group and less related inter - groups . We suppose that extracting news by groups can reduce the extraction difficulty compared to extracting from all news directly because news in the same group is close to each other and has less noise . Moreover , this method can help us analyze the contributions of different groups .", "entities": []}
{"text": "In this method , finance news is divided into groups according to the time when news happens . We set the time unit to 5 minutes and news released in the same time unit will be divided into the same group . This method supposes that news happened closely is highly correlated .", "entities": []}
{"text": "In this method , finance news is divided into groups by news topic . The news topics are generated by unsupervised news clustering . In this work , we choose the affinity propagation algorithm ( Frey and Dueck , 2007 ) to generate news clusters without setting the number of clusters subjectively . Moreover , we choose the tf - idf of 2 - gram features from news headlines . This method supposes that finance news focuses on several finance event topics at a particular time . News in the same topic describes this topic from different aspects and has a high correlation .", "entities": []}
{"text": "In this section , we analyze the influence of finance news 's attributes ( category and region ) on prediction results and summarize the influence patterns for different currency pairs . We conduct the experiments based on BHAM - Category .", "entities": []}
{"text": "The forex trading data 's attention weights over news categories are calculated by Equation 6 . We sum up all the attention weights of test samples and calculate the proportions each category contributes . As shown in Figure 5 , we display the influence patterns of news category for different currency pairs . We observe that there are obvious differences among currency pairs . USD - EUR trading pays more attention to the Business Sectors and Politics / International Affairs news . USD - JPY trading is mostly influenced by Business Sectors and Science / Technology news . Politics / International Affairs news has the most significant impact on USD - RMB trading and Business Commodities news effects USD - GBP trading most . The summarized influence patterns can serve as decision - making reference for forex traders when facing news from various categories .", "entities": []}
{"text": "The trading data 's attention weight for selected news att ij is calculated by the following formula : att ij = att i * s i j ( 11 ) Where att i is the trade data 's attention on the i - th category in Equation 6 and s i j in Equation 4 is the weight of selected news in group . We sum up all the selected news 's attention according to their regions and access the region influence weight . The results are shown in Figure 6 . For each currency pair , the news are divided into three classes : news related to region A only , news related to region B only and news related to both region A and B. And we observe that the news related to both region A and B has the least influence on all currency pairs . News related to the US has the largest influence weight on USD - JPY and USD - GBP trading . Yet news related to China / Europe has a larger influence weight than news related to US in USD - RMB / USD - EUP trading . We can intuitively observe the influence weights of different regions for forex trading , which is helpful for the analysis and forecast of forex movement .", "entities": []}
{"text": "The selection number in each group is an essential hyper - parameter to control the amount of extracted information . As shown in Table 2 , the BHAM - Category performs best when the selection number is 3 in all currency pairs . When the selection number is small ( 1 , 2 ) , the model is too strict so that some crucial information will be missed . When the selection number is large ( 4 , 5 ) , some less influential news will be selected and interfere model 's decision . When we keep all news in the group , the model 's performance declines by a large margin . This experiment demonstrates that the selection mechanism plays an important role in the proposed model .", "entities": []}
{"text": "This work is supported by a Research Grant from Mizuho Securities Co. , Ltd. Mizuho Securities also provide experiment data and valuable domain experts suggestions .", "entities": []}
{"text": "Problem Formulation Throughout the paper , we refer to target code representations as API components . In all cases , components will consist of formal representations of functions , or function signatures ( e.g. , long max ( int a , int b ) ) , which include a function name ( max ) , a sequence of arguments ( int a , int b ) , and other information such as a return value ( long ) and namespace ( for more details , see Richardson ( 2018 ) ) . For a given API dataset D = { ( x i , z i ) } n i=1 of size n , the goal is to learn a model that can generate exactly a correct component sequence z = ( z 1 , .. , z | z | ) , within a finite space C of signatures ( i.e. , the space of all defined functions ) , for each input text sequence x = ( x 1 , ... , x | x | ) . This involves learning a probability distribution p ( z | x ) . As such , one can think of this underlying problem as a constrained MT task . In this section , we describe the baseline approach of Richardson and Kuhn ( 2017b ) . Technically , their approach has two components : a simple word - based translation model and task specific decoder , which is used to generate a k - best list of candidate component representations for a given input x. They then use a discriminative model to rerank the translation output using additional nonworld level features . The goal in this section is to provide the technical details of their translation approach , which we improve in Section 4 .", "entities": []}
{"text": "The translation models investigated in Richardson and Kuhn ( 2017b ) use a noisy - channel formulation where p ( z | x ) \u221d p ( x | z ) p ( z ) via Bayes rule . By assuming a uniform prior on output components , p ( z ) , the model therefore involves estimating p ( x | z ) , which under a word - translation model is computed using the following formula : p ( x | z ) = a A p ( x , a | z ) , where the summation ranges over the set of all many - to - one word alignments A from x z , with | A | equal to ( | z | + 1 ) | x | . They investigate various types of sequence - based alignment models ( Och and Ney , 2003 ) , and find that the classic IBM Model 1 outperforms more complex word models . This model factors in the following way and assumes an inde - pendent word generation process : p ( x | z ) = 1 | A | | x | j=1 | z | i=0 p t ( x j | z i ) ( 1 ) where each p t defines a multinomial distribution over a given component term z for all words x. The decoding problem for the above translation model involves finding the most likely output\u1e91 , which requires solving an arg max z over Equation 1 . In the general case , this problem is known to be N P - complete for the models under consideration ( Knight , 1999 ) largely due to the large space of possible predictions z. Richardson and Kuhn ( 2017b ) avoid these issues by exploiting the finiteness of the target component search space ( an idea we also pursue here and discuss more below ) , and describe a constrained decoding algorithm that runs in time O ( | C | log | C | ) . While this works well for small APIs , it becomes less feasible when dealing with large sets of APIs , as in the polyglot case , or with more complex semantic languages typically used in SP ( Liang , 2013 ) .", "entities": []}
{"text": "Our framework facilitates both monolingual and polyglot decoding . In the first case , the decoder requires a graph associated with the output semantic language ( more details in next section ) and a trained translation model . The latter case requires taking the union of all datasets and graphs ( with artificial identifier tokens ) for a collection of target datasets and training a single model over this global dataset . In this setting , we can then decode to a particular language using the language identifiers or decode without specifying the output language . The main focus in this paper is investigating polyglot decoding , and in particular the effect of training models on multiple datasets when translating to individuals APIs or SP datasets . When evaluating our models and building QA applications , it is important to be able to generate the k best translations . This can easily be done in our framework by applying standard k SSSP algorithms ( Brander and Sinclair , 1995 ) . We use an implementation of the algorithm of Yen ( 1971 ) , which works on top of the SSSP algorithms introduced above by iteratively finding deviating or branching paths from an initial SSSP ( more details provided in supplementary materials ) .", "entities": []}
{"text": "We experimented with two main types of resources : 45 API documentation datasets and two multilingual benchmark SP datasets . In the former case , our main objective is to test whether training polyglot models ( shown as polyglot in Tables 1 - 2 ) on multiple datasets leads to an improvement when compared to training individual monolingual models ( shown as monolingual in Tables 1 - 2 ) . Experiments involving the latter datasets are meant to test the applicability of our general graph and polyglot method to related SP tasks , and are also used for comparison against our main technical documentation task . Figure 3 : Test Acc@1 for the best monolingual models ( in yellow / left ) compared with the best lexical polyglot model ( green / right ) across all 45 technical documentation datasets .", "entities": []}
{"text": "We use the Foma finite - state toolkit of Hulden ( 2009 ) to construct all graphs used in our experiments . We also use the Cython version of Dynet ( Neubig et al , 2017 ) to implement all the neural models ( see supp . materials for more details ) . In the results tables , we refer to the lexical and neural models introduced in Section 4 as Lexical Shortest Path and Neural Shortest Path , where models that use copying ( + copy ) and lexical biasing ( + bias ) are marked accordingly . We also experimented with adding a discriminative reranker to our lexical models ( + rerank ) , using the approach from Richardson and Kuhn ( 2017b ) , which uses additional lexical ( e.g. , word match and alignment ) features and other phrase - level and syntax features . The goal here is to see if these additional ( mostly non - word level ) features help improve on the baseline lexical models .", "entities": []}
{"text": "We look at learning from multiple API libraries and datasets in the context of learning to translate text to code representations and other SP tasks . To support polyglot modeling of this type , we developed a novel graph based decoding method and experimented with various SMT and neural MT models that work in this framework . We report a mixture of positive results specific to each task and set of models , some of which reveal interesting limitations of different approaches to SP . We also introduced new API and mixed language datasets to facilitate further work on polyglot SP .", "entities": []}
{"text": "This work was supported by the German Research Foundation ( DFG ) in project D2 of SFB 732 .", "entities": []}
{"text": "Nomen Omen . Enhancing the Latin Morphological Analyser Lemlat with an Onomasticon", "entities": []}
{"text": "Lemlat is a morphological analyser for Latin , which shows a remarkably wide coverage of the Latin lexicon . However , the performance of the tool is limited by the absence of proper names in its lexical basis . In this paper we present the extension of Lemlat with a large Onomasticon for Latin . First , we describe and motivate the automatic and manual procedures for including the proper names in Lemlat . Then , we compare the new version of Lemlat with the previous one , by evaluating their lexical coverage of four Latin texts of different era and genre .", "entities": []}
{"text": "We evaluated the quality of the rules for automatic enhancement by precision and recall ( Van Rijsbergen , 1979 ) . Measuring the precision of our rules is straightforward . As said , while writing the rules , we focused on inflectionally regular groups of lemmas . As a consequence , we never had to modify the output of rules neither in terms of removal of results ( i.e. wrong results due to overproduction ) nor in terms of completion of results ( i.e. wrong results due to underproduction ) . Thus , the precision of our rules is always 100 % . To calculate recall , we grouped all those rules that treat lemmas of the same inflectional class ( e.g. all rules for nouns of the first declension ) . We measured the recall of such groups of rules by comparing the number of lemmas automatically inserted into Lemlat by one group of rules with the total number of lemmas in the Onomasticon of Forcellini belonging to the inflectional class addressed by that group of rules . The most problematic inflectional class is that of third declension nouns . 2 As mentioned above , this is motivated by the fact that it is not always possible to match regularly an inflectional paradigm ( e.g. third declension imparisyllable nouns ) with one specific ending . Hence , given such a low recall , the amount of manual work required for enhancing Lemlat with third declension proper names was quite considerable . To provide an example , the number of third declension feminine nouns in the Onomasticon is 1 , 200 . Our rules covered only 542 out of them . Thus , 658 nouns had to be inserted into Lemlat manually ( 54.833 % of the total for that class ) . There are also entire inflectional classes for which writing a rule was not possible , like for instance Busa 's class of irregularly inflected nouns ( 146 wordforms ) . All these lemmas were inserted into the LES archive manually . In total , the number of lemmas transferred manually into Lemlat is 1 , 752 ( 6.632 % of all the lemmas of the Onomasticon ) .", "entities": []}
{"text": "In this paper we described the enhancing of the morphological analyser for Latin Lemlat with a large Onomasticon provided by a reference dictionary for Latin ( Forcellini ) . Although we have included most of the words of the Onomasticon into Lemlat , the work is far from being complete . Indeed , we have just started to enhance the analyser with graphical variants . Furthermore , around 2 , 000 words of the Onomasticon belonging to minor and irregular inflectional classes still have to be included into Lemlat . Although this promises to be a largely manual and time - consuming work , it is worth doing for achieving the lexicographically motivated completeness of the tool 's lexical basis . Once completed , the lexical look - up table of the Onomasticon will become part of the overall Lemlat suite , which will be shortly made available for free download and on - line use .", "entities": []}
{"text": "Text Editing by Command", "entities": []}
{"text": "We retrieve grounding snippets for the edits in our dataset by querying a commercial search engine . In order to formulate a query for a given edit , we combine the relevant page and section titles with keywords 5 from the target sentence . While the target sentence is not available at test time , we make the assumption that in a real user scenario the relevant grounding would be provided by the user . We retrieve the top 200 returned web page results and only keep the preview snippets returned by the search engine as the grounding corpus . 6 Because Wikipedia , as well as several clones , often appear in search engine results , we check for 4 - gram overlap between the target sentence and each grounding snippet , removing any snippet with more than 50 % overlap . Finally , we rerank 7 the retrieved snippets using an information extraction score , and merge the ranked snippets to take the first N = 512 tokens .", "entities": []}
{"text": "Percentiles Mean 25 % 50 % 75 %", "entities": []}
{"text": "Krishna attended Dartmouth College where she was a double major in government and French .", "entities": []}
{"text": "Krishna attended Dartmouth College where she was a double major in government and French and graduated in the class of ' 13 .", "entities": []}
{"text": "Mountain State is currently seeing alternative accreditation by the Commission on Collegiate Nursing Education .", "entities": []}
{"text": "Mountain State is currently seeking alternative accreditation by the Commission on Collegiate Nursing Education . Comment correct year of marriage ( did not fit NSW records )", "entities": []}
{"text": "He married Margaret Frances Prowse Shaw in Sydney in 1874 .", "entities": []}
{"text": "He married Margaret Frances Prowse Shaw in Sydney in 1871 .", "entities": []}
{"text": "Entitled \" It Feels Like Home ( Re Invented ) Tour 2011 \" , it contained his songs and remakes of Alliage hits .", "entities": []}
{"text": "Johnson married Group 1 Crew member Manwell Reyes in 2011 .", "entities": []}
{"text": "Johnson married Group 1 Crew member Manwell Reyes in 2011 . Johnson married Group 1 Crew member Manwell Reyes in 2011 in a ceremony at Half Moon Bay , California .", "entities": []}
{"text": "They are more frequent than primary brain tumors . They are more frequent than primary brain tumors , and are mainly a problem in adults , though children may also have secondary tumors .", "entities": []}
{"text": "Text Geoff Hinton is an English tennis player .", "entities": []}
{"text": "Geoffrey Hinton is a computer science professor at the University of Toronto .", "entities": []}
{"text": "Geoffrey Hinton is an English - Canadian computer science professor at the University of Toronto .", "entities": []}
{"text": "Geoffrey Hinton ( born 1946 ) is an English - Canadian computer science professor at the University of Toronto .", "entities": []}
{"text": "Geoffrey Hinton ( born 1946 ) is an English - Canadian computer science professor at the University of Toronto . Geoffrey Hinton is most famous for his work on artificial neural networks . Table 9 : An example of a multi - turn interaction with our model . At each turn , the edit was chosen among the top 3 outputs returned by beam - search . See table 12 in the appendix for the grounding used in this example . This paper focuses on the task of editing individual sentences , which we believe to be a challenging task for NLP , as it involves making nuanced changes to text according to natural language commands . We also believe this task has useful applications , particularly in speech - to - text scenarios , where it may be more convenient to speak out a command rather than edit the text directly . However , we also wish to emphasize that this task is a step towards a larger goal of interactive document generation , and that there are many interesting future directions to explore in this space . While this paper has focused on single interactions ( i.e. making isolated edits to text ) , it would be worth modeling multiple interactions between the user and model . One can imagine that there may be a natural order in which to make edits , such as adding information at the start , and fine - tuning the language at the end . It is an open question whether or not a model could learn this . For illustration , table 9 gives an example of using our model to make several edits in order to create a sentence . Ultimately , this may look more like a dialogue than a sequence of commands coming from the user . Additionally , it would also be interesting to look at other settings where a model must generate a complex , structured object for a user , such as code , or images . We hope that our text editing task , as a first step , can demonstrate the potential for interactive generation systems , and that it will encourage the community to pursue more ideas in this space .", "entities": []}
{"text": "For a given edit , we combine the relevant page and section titles with keywords from the target sentence to construct a query that we use to retrieve grounding from a commercial search engine . In order to identify keywords we look at document frequency df ( w ) = | { D D | w D } | | D | , where D is a sample of 500 , 000 Wikipedia articles taken from the Tensorflow Wikipedia dataset . 12 We consider words w with df ( w ) < 0.01 to be keywords .", "entities": []}
{"text": "Because the combined length of the grounding snippets we retrieve far exceeds the capacity of our model , we rerank the retrieved snippets using an information extraction score . We then merge the ranked snippets and take only the first N = 512 tokens . Following ( Liu et al , 2018a ) we use tf - idf scores to rerank . For a given edit s \u2212 s , with retrieved grounding documents G , the information extraction score of snippet G G is score ( G ) = w s tf - idf ( w , G ) , where the tf - idf score of word w is tf - idf ( w , G ) = N w ( G ) log N g N gw , where N w ( G ) is the number of occurrences of w in G , N gw is the number of documents in G that contain w , and N g is the number of documents in G. . % Edits gives the prevalence of each label in our data , while % Orig . gives the prevalence in the hand - labelled dataset presented in . The percentages do not total 100 because edits can have multiple labels .", "entities": []}
{"text": "Reword", "entities": []}
{"text": "ByteDance responded by adding a kids - only mode to TikTok which allows music videos to be recorded , but not posted and by removing some accounts and content from those determined to be underage .", "entities": []}
{"text": "ByteDance responded by adding a kids - only mode to TikTok which blocks the upload of videos , the building of user profiles , direct messaging , and commenting on other 's videos , while still allowing the viewing and recording of content .", "entities": []}
{"text": "The authors would like to thank Thomas Hofmann , as well as Sudha Rao , Matt Richardson , Zhang Li , Kosh Narayanan , and Chandra Chikkareddy for their helpful suggestions .", "entities": []}
{"text": "We are very grateful to the mentor of this paper for her meaningful feedback . Thanks three anonymous reviewers for their insightful comments and practical suggestions .", "entities": []}
{"text": "Cogent : A Generic Dialogue System Shell Based on a Collaborative Problem Solving Model", "entities": []}
{"text": "The bulk of current research in dialogue systems is focused on fairly simple task models , primarily state - based . Progress on developing dialogue systems for more complex tasks has been limited by the lack generic toolkits to build from . In this paper we report on our development from the ground up of a new dialogue model based on collaborative problem solving . We implemented the model in a dialogue system shell ( Cogent ) that allows developers to plug in problem - solving agents to create dialogue systems in new domains . The Cogent shell has now been used by several independent teams of researchers to develop dialogue systems in different domains , with varied lexicons and interaction style , each with their own problem - solving backend . We believe this to be the first practical demonstration of the feasibility of a CPSbased dialogue system shell .", "entities": []}
{"text": "This system uses a computational model of music cognition , as well as knowledge about existing pieces of music , to help a human composer create and edit a musical score ( Quick and Morrison , 2017 ) . SMILEE : This system acts as a partner for playing a cooperative game ( Kim et al , 2018 ) . The game involves placing pieces ( blocks ) on a board to create complex symmetrical configurations . Players alternate , but each player can hold their turn for multiple rounds . Each player has some freedom to be creative with respect to the configuration being pursued ( it is not set in advance ) . Thus , they have to negotiate turn taking , and they can ask for explanations to achieve a shared understanding about the properties of the configuration being created . Aesop : A system for building animated stories . The user acts as a director , and can choose scenes , props , characters , direct them what to do , etc . Essentially , the system provides a dialogue interface to a sophisticated system for creating visual narratives . Of note , these systems work in several application domains , with varying interaction styles . Musica and Aesop currently work mostly in fixedinitiative mode ( user tells the system what to do ) . All others involve varying degrees of mixed initiative . While Cabot is a more traditional planning domain , it is interesting to note that all others involve fairly open - ended collaborative tasks , for which the ultimate goal is learning or creating something new . BoB is notable for the fact that it is helping the user learn new knowledge , by helping to formulate and evaluate biological hypotheses ( which may even lead to new scientific discoveries ) . Importantly , with the exception of Cabot - L , which was developed by our team , all others were developed by independent teams ( the BAs for Cabot and BoB were developed by a single team , though the latter also involved collaboration with a large group of biologists and bioinformaticians ) . We helped those teams understand how our tools work and the meaning of the CPS acts ( especially to the early adopters , who did not have the benefit of much documentation ) , but we had no role in deciding what problemsolving behaviors they should or should not implement , how to implement them and so on . Two of the systems ( BoB and Musica ) required additions to our surface NLP components ( mainly add -", "entities": []}
{"text": "We present the first systematic study of negative interference in multilingual models and shed light on its causes . We further propose a method and show it can improve cross - lingual transferability by mitigating negative interference . While prior efforts focus on improving sharing and cross - lingual alignment , we provide new insights and a different perspective on unsharing and resolving language conflicts .", "entities": []}
{"text": "We want to thank Jaime Carbonell for his support on the early stage of this project . We also would like to thank Zihang Dai , Graham Neubig , Orhan Firat , Yuan Cao , Jiateng Xie , Xinyi Wang , Ruochen Xu and Yiheng Zhou for insightful discussions . Lastly , we thank anonymous reviewers for their valueable feedbacks .", "entities": []}
{"text": "Due to the varied nature of the input presented we perform various data cleaning operations . We start by expansion of common contractions ( e.g. \" is n't \" ) and informal contractions ( e.g. \" howz \" , \" could ve \" ) . We then perform a spell check and hyphen removal , which are conditional , in the sense that a word is not modified unless the modified form appears in the other sentence . All remaining hyphens are replaced by spaces , a method different from those that previously handled hyphens ( Han et al , 2013 ) . We also perform case correction , as has been done previously ( H\u00e4nig et al , 2015 ) , since we observe several instances wherein sentence capitalisation is not suitable for parsing ( e.g. headlines and forums ) .", "entities": []}
{"text": "We use two measures , which are boosted based on different parameters described in Section 4 .", "entities": []}
{"text": "The first measure makes use of the aligner developed by Sultan et al ( 2014a ) , which was used to achieve State of the Art results in ( Sultan et al , 2014bSultan et al , 2015 ) . Our use of the aligner disregards sequences thus making use of the aligner more as a synonym finder , with the additional power of the Paraphrase Database ( PPDB ) ( Ganitkevitch et al , 2013 ) .", "entities": []}
{"text": "In this section , we detail the variations used to generate different similarity measures . These variations are not used simultaneously , but are instead combined as described in Algorithm 1 ( Section 5 ) , which iterates through all possible variations to generate a different similarity score associated with each combination .", "entities": []}
{"text": "Consider the following sentence pairs with relations assigned by human annotators : \" A boy is playing a guitar . \" , \" A man is playing a guitar . \" , rel : 3.2 ; and \" A man is cutting up a potato . \" , \" A man is cutting up carrots . \" , rel : 2.4 . Although both pairs of sentences differ by exactly one noun , the first pair was considered to be more closely associated than the second . We associate this to what we call the \" Surprise \" and assign a value to this , which we call the \" Surprise Factor \" . Surprise is based on the work by Dunning ( 1993 ) , who observed that the assumption of normality of data is invalid as \" simple word counts made on a moderate - sized corpus show that words that have a frequency of less than one in 50 , 000 words make up about 20 - 30 % of typical English language newswire reports . This ' rare ' quarter of English includes many of the content - bearing words . . . \" We define the Surprise Factor of a noun or phrase to be proportional to the number of Web Search Hits for that phrase or term , while inversely proportional to the Search Hits in the case of proper nouns . Intuitively this makes sense , as words that are more common will generate less Surprise , carry less information , and will also be more widely used on the Internet . We incorporate this idea of Surprise by adding the option of additionally weighting nouns by the total number of Web Search Hits or Results 4 . We define , H i to be the the number of Web Search Hits for the noun i , HT the total number of hits for all nouns HT = N i=0 H i , N i the fraction of the Search Hits that noun i captures N i = Hn HT , and N T the normalised total of all nouns ( C ) in a given sentence N T = C i=0 N i . We define the Surprise of word i in terms of the above in Equation 1 . S i = N i N T ( 1 )", "entities": []}
{"text": "T t=0 score t \u00d7 w t \u00d7 2 T t=0 count t \u00d7 w t ) ( 2 ) 4 We", "entities": []}
{"text": "As described above , we use variations to generate thousands of Similarity Scores , each of which we call a \" Method \" . Each Method 's performance varies depending on the input . In this section , we detail the process for combining these Methods , which is performed using either Support Vector Regression ( SVR ) or Kernel Ridge Regression ( KRR ) .", "entities": []}
{"text": "In addition to using scores from the chosen Methods , we add the following features to some of our submitted runs : a ) a binary value to represent whether each of the sentences were case corrected , b ) the length of each of the sentences , c ) the number of contin - uous aligned or unaligned sequences , d ) the maximum and minimum lengths of continuous aligned or unaligned sequences , and e ) a binary value to represent alignments that are non - sequential . It should be noted that the specific Methods we choose for use in the SVR or KRR will depend on the training data picked . We found , by testing our system using several different combinations of training data , that the best results were achieved when our system was trained on the headlines data from the years 2015 , 2014 and 2013 . The method selection criterion , regression model and parameters used for each of the runs submitted are detailed in Table 1 . Although some of the settings are very similar ( e.g. run2 ) , we noticed that these minor changes translated to significant differences in performance .", "entities": []}
{"text": "This work was supported , in part , by the EPSRC , U.K. , under grant number 1576491 , and is also partially funded by the ENDEAVOUR Scholarships Scheme , which may be part - financed by the European Union - European Social Fund .", "entities": []}
{"text": "The only system that our team submitted for the SMG - CH subtask is an ensemble model based on the XGBoost meta - learner , as illustrated in Figure 1 . In this section , we describe the three machine learning techniques that provide their predictions as input for the meta - learner , as well as the gradient boosting method that combines the independent models .", "entities": []}
{"text": "The SMG - CH subtask ( Hovy and Purschke , 2018 ) offers , as support , a training set of 25 , 261 Jodel posts , provided in plain text format . Each textual input is associated with a pair of coordinates , i.e. latitude and longitude , representing the position on Earth from where the text was posted . The development set is provided in an identical format and it is composed of 2 , 416 samples that we use to perform hyperparameter tuning and validate the results of our models . The test set has 2 , 438 samples without the corresponding coordinates , in order to avoid cheating or overfitting . Additionally , the proposed evaluation metric is the median distance between the predicted and the reference coordinates . A baseline median distance of 53.13 km is also included in the specifications .", "entities": []}
{"text": "For our final submission , we trained all the individual models on both the provided training and development data sets . Then , we also retrained the submitted ensemble model , in a hope that this will give an even smaller median distance on the test set , compared to what we have obtained in the preliminary validation phase . Table 2 shows an improvement in terms of the median distance on the test set compared to the one obtain on the development data . We can not be sure that this improvement is solely due to the retraining that involves the development set . However , we conjecture that this endeavour played its part in the slightly better results . We outperform the baseline model by 29.53 km in terms of the median distance and by 21.75 km in terms of the mean distance , obtaining the third place in the competition . The constrained submission proposed by the organizers of the SMG - CH shared task surpasses our model by 2.9 km in terms of the median distance and by 0.13 km in terms of the mean distance . The unconstrained system on the first place , which was also proposed by the organizers of the SMG - CH shared task , distances itself by larger margins , with a difference of 6.05 km for the median distance and a difference of 3.91 km for the mean distance .", "entities": []}
{"text": "The authors thank reviewers for their useful remarks . This work was supported by a grant of the Romanian Ministry of Education and Research , CNCS - UEFISCDI , project number PN - III - P1 - 1.1 - TE - 2019 - 0235 , within PNCDI III . This article has also benefited from the support of the Romanian Young Academy , which is funded by Stiftung Mercator and the Alexander von Humboldt Foundation for the period 2020 - 2022 .", "entities": []}
{"text": "We presented a comparison between 12 transformers - based models , with the goal of \" prescribing \" the best option to the researchers working in the field . We also wanted to test whether the span - based objective of SpanBERT and in - domain language pretraining were useful for the task . We can positively answer to the first question , since SpanBERT turned out to be the best performing model on both datasets . As for the in - domain models , PubMedBERT came as a close second after SpanBERT , suggesting that pretraining from scratch with no general domain data is the best strategy , at least for this task . We have been the first , to our knowledge , to test these two models in a systematic comparison on ADE detection , and they delivered promising results for future research . For the next step , a possible direction would be to combine the strengths of their respective representations : the accurate modeling of text spans on the one side , and deep biomedical knowledge on the other one .", "entities": []}
{"text": "Some statistics for the texts of the two datasets have been extracted with the TEXTSTAT Python package and reported reported in Table A : we extracted the counts of syllables , lexicon ( how many different word types are being used ) , sentences and characters . Difficult words refers to the number of polysyllabic words with Syllable Count > 2 that are not included in the list of words of common usage in English .", "entities": []}
{"text": "We use the data sets provided by the SemEval - 2016 shared task 6 ( Mohammad et al , 2016 ) .", "entities": []}
{"text": "We mined additional tweets for each of the five targets in Nov. 2015 by searching for hashtags relevant to the targets . These tweets are not included in the final systems since they increased the class imbalance . We will investigate better options for including the data in the future . Hashtags for Abortion include # abortion , # abortionrights , and # prolife ; Atheism includes # atheism , # atheist , and # theist ; Climate includes # actionclimate and # climatechange ; Feminist includes # feminism , # feminist , # heforshe , and # womensrights ; and Hillary includes # HillaryClinton . Tweets were then annotated for stance , following the guidelines used for the annotation of the official shared task data 3 . Two annotators participated in the annotation process . The number of additional tweets ranged between 260 and 2 , 400 per target .", "entities": []}
{"text": "Since the ensemble classifier was not completed in time for submission , we had to decide which individual classifier to submit . The random forest model is selected based on a five - fold cross validation on the training set . This system reaches a score of 63.60 ( macro - averaged F ) , as shown in table 3 , the sixth best result out of 19 participating systems . This result is approximately 4 percent points lower than that of the highest performing system .", "entities": []}
{"text": "This work is based on research supported by the U.S. Office of Naval Research ( ONR ) via grant # N00014 - 10 - 1 - 0140 .", "entities": []}
{"text": "The tokens decoded as B - Begin or I - Inside were marked as toxic . The character spans corresponding to these toxic tokens were added to the predicted spans . Two consecutive spans were merged if separated by at most five characters , provided all of them are non - alphabetic .", "entities": []}
{"text": "Block Pruning For Faster Transformers", "entities": []}
{"text": "We expect the method presented here to contribute to the reduction of the compute resources and energy needed to perform natural language tasks , while preserving the original model performance . It will contribute additionally to alleviating privacy concerns : smaller models running on user devices instead of server - side allow more information to stay private . This is especially relevant when considering the large anticipated demand for such NLP applications in the near future .", "entities": []}
{"text": "The complete code to run the experiments , analyze the results and finally create the figures and tables in this paper is available on the Hugging Face nn_pruning repository , at https://github.com/huggingface/nn_pruning .", "entities": []}
{"text": "The authors would like to thank the anonymous reviewers , the Hugging Face team for the support , Nvidia for providing us some hardware for evaluation , and finally the open - source community for the numerous tools which made this research possible .", "entities": []}
{"text": "Visual Story Post - Editing", "entities": []}
{"text": "Orthographic tr\u00e8s pr\u00e8s 7.5 tr\u00e8s ors 5 Phononetic tr\u00e8s frais 6.67 tr\u00e8s tra\u00eenent 6.67", "entities": []}
{"text": "The training set for the CNN consists of 488 hours of French Broadcast News with manual transcriptions . This dataset is composed of data coming from the ESTER1 ( Galliano et al , 2005 ) , ES - TER2 ( Galliano et al , 2009 ) and EPAC ( Est\u00e8ve et al , 2010 ) corpora . It contains 52k unique words that have been seen at least twice each in the corpus . All of them corresponds to a total of 5.75 millions occurrences . In French language , many words have the same pronunciation without sharing the same spelling , and they can have different meanings ; e.g. the sound [ so ] corresponds to four homophones : sot ( fool ) , saut ( jump ) , sceau ( seal ) and seau ( bucket ) , and twice more by taking into account their plural forms that have the same pronunciation : sots , sauts , sceaux , and seaux . When a CNN is trained to predict a word given an acoustic sequence , these frequent homophones can introduce a bias to evaluate the recognition error . To avoid this , we merged all the homophones existing among the 52k unique words of the training corpus . As a result , we obtained a new reduced dictionary containing 45k words and classes of homophones . Acoustic features provided to the CNN are logfilterbanks , computed every 10ms over a 25ms window yielding a 23 - dimension vector for each frame . A forced alignment between manual transcriptions and speech signal was performed on the training set in order to detect word boundaries . The statistics computed from this alignment reveal that 99 % of words are shorter than 1 second . Hence we decided to represent each word by 100 frames , thus , by a vector of 2300 dimensions . When words are shorter they are padded with zero equally on both ends , while longer words are cut equally on both ends . The CNN and DNN deep architectures are trained on 90 % of the training set and the remaining 10 % are used for validation .", "entities": []}
{"text": "This work was partially funded by the European Commission through the EUMSSI project , under the contract number 611057 , in the framework of the FP7 - ICT - 2013 - 10 call , by the French National Research Agency ( ANR ) through the VERA project , under the contract number ANR - 12 - BS02 - 006 - 01 , and by the R\u00e9gion Pays de la Loire .", "entities": []}
{"text": "As discussed in ( Zhao et al , 2019 ) , the routing procedure is computational expensive for a large number of capsules . Compressing capsules into a smaller amount can not only relieve the computational complexity , but also merge similar capsules and remove outliers . Therefore , hyperbolic compression layer is introduced . Each compressed local hyperbolic capsule is calculated as a weighted M\u00f6bius summation over all the local hyperbolic capsules . For instance , u l = M u k { u 1 , ... , u L } r k \u2297 u k B d , ( 9 ) where r k is a learnable weight parameter . And likewise for compressing global hyperbolic capsules . Let set { u 1 , . . . , u P } denote the compressed local and global hyperbolic capsules together , which are then aggregated in a label - aware manner via HDR .", "entities": []}
{"text": "The purpose of Hyperbolic Dynamic Routing ( HDR ) is to iteratively aggregate local and global hyperbolic capsules into label - aware hyperbolic capsules , whose activations stand for probabilities of the labels .", "entities": []}
{"text": "The proposed HYPERCAPS is evaluated on four benchmark datasets with various label number from 54 to 4271 . We compare with the state - of - the - art methods in terms of widely used metrics . Performance on tail labels is also compared to demonstrate the superiority of HYPERCAPS for MLC . An ablation test is also carried out to analyse the contribution of each component of HYPERCAPS .", "entities": []}
{"text": "This work was supported in part by the National Natural Science Foundation of China under Grant 61822601 , 61773050 , and 61632004 ; the Beijing Natural Science Foundation under Grant Z180006 ; National Key Research and Development Program ( 2017YFC1703506 ) ; the Fundamental Research Funds for the Central Universities ( 2019JBZ110 ) . We thank the anonymous reviewers for their valuable feedback .", "entities": []}
{"text": "Table 1 ) , i.e. labels have less than average number of training instances are tail labels . We observe that this division generally follows the Pareto Principle , as nearly 80 % of labels are divided into the tail label set .", "entities": []}
{"text": "Organizing and Program Committees Organizing Committee", "entities": []}
{"text": "A Teacher - Student Framework for Maintainable Dialog Manager", "entities": []}
{"text": "Fig . 3 shows two kinds of strategies to extend the original system . The first strategy requires a new interaction environment . However , building a user simulator or hiring real users once the system needs to be extended is costly and impractical in real - world applications . By contrast , our method enhances the reuse of existing resources . The basic idea is to use the existing user logs , original dialog policy model and logic rules ( \" teacher \" ) to guide the learning process of a new dialog manager ( \" student \" ) . Without an expensive interaction environment , the developers can maintain RL - based dialog systems as efficiently and straightforwardly as in rule - based systems .", "entities": []}
{"text": "To evaluate our method , we conduct experiments on a dialog system extension task of restaurant domain .", "entities": []}
{"text": "The dialog system provides restaurant information in Beijing . The database we use includes 2988 restaurants . This domain consists of 8 slots ( name , area , price range , cuisine , rating , number of comments , address and phone number ) in which the first four slots ( inform slots ) can be used for searching the desirable restaurant and all of these slots ( request slots ) can be asked by users . In each dialog , the user has a goal containing a set of slots , indicating the constraints and requests from users . For example , an inform slot , such as \" inform ( cuisine = Sichuan cuisine ) \" , indicates the user finding a Sichuan restaurant , and a request slot , such as \" request ( area ) \" , indicates the user asking for information from the system ( Li et al , 2016 ( Li et al , , 2017bPeng et al , 2017 ) .", "entities": []}
{"text": "A main advantage of our approach is that the unconsidered user actions can be handled in the extended system . In addition to traditional measurements ( e.g. , success rate , average turns and average reward ) , we define an objective measurement called \" Satis . \" ( user satisfaction ) to verify this feature in the simulated evaluation . \" Satis . \" indicates the rate at which the system takes reasonable actions in unsupported dialog situations . It can be calculated as follows : Satis . = d D | d | t=1 L l=1 1 { ht = h l } 1 { a s t = a l } d D | d | t=1 L l=1 1 { ht = h l } ( 8 ) where h t and a s t are the dialog history and system action in the t - th turn , h l and a l are dialog context condition and corresponding system action defined in the l - th rules . Intuitively , an unreasonable system reply will frustrate users and low \" Satis . \" indicates a poor user experience .", "entities": []}
{"text": "Training RL - based dialog systems requires a large number of interactions with users . It 's common to use a user simulator to train RL - based dialog systems in an online fashion ( Pietquin and Dutoit , 2006 ; Scheffler and Young , 2002 ; Li et al , 2016 ) . As a consequence , we construct an agenda - based user simulator , which we refer to as Sim1 , to train the original RL - based system . The user action set of Sim1 is denoted as A u , which includes such intents 4 : \" hello \" , \" bye \" , \" inform \" , \" deny \" , \" negate \" , \" affirm \" , \" request \" , \" reqalts \" and \" null \" . The slots of Sim1 are shown in section 6.1 . In each turn , the user action consists of a intent and slots and we append the value of slots according to the user goal .", "entities": []}
{"text": "In any case , the developers ca n't guarantee all user actions are considered . Fortunately , our method makes no assumptions about the new user actions and new dialog model architecture . As a result , the system can be extended over multiple iterations . To evaluate this characteristic , we deploy the extended system 11 in section 6.5 to interact with real human users . Users are given a goal sampled from our corpus for reference . To elicit more complex situations , they are encouraged to interact with our system by new intents and slots related to the restaurant domain . At the end of each dialog , they are asked to give a subjective rating on the scale from 1 to 5 based on the naturalness of the system ( 1 is the worst , 5 is the best . ) . After filtering dialog sessions unrelated to our task , we collect 315 episodes in total . Table 3 shows the details of the user logs . As shown in Table 3 , after deployment , there are a few slots 11 The extended system in the simulated evaluation will be the original system in our human evaluation .", "entities": []}
{"text": "Extended System and intents unseen before . For example , users may ask for the discount information or take a taxi to the restaurant . To represent the new intents and slots , the dimension of extracted dialog features is extended to 236 . Meanwhile , the number of system actions is extended to 29 to handle new user actions . To deal with the newfound user actions , we define 14 rules in total . Table 4 shows the details of new defined logic rules . Then we distill the knowledge of the original system and logic rules into a new system . Fig . 5 shows the comparison in user ratings . The extended system significantly gets a higher subjective rating than the original one . It proves that the extended system can give reasonable responses in unseen dialog situations . Table 5 shows sample dialogs from the original system and extended system with real users . We can see that the extended system is much more coherent since it takes new user actions into account . It inspires us that a complicated RL - based dialog system can start from a simple one and be improved incrementally based on our framework . This design pattern will be much more efficient if the interaction environment is hard to access .", "entities": []}
{"text": "In this paper , we present a practical solution to maintain RL - based dialog systems without retraining from scratch . By reusing existing resources , developers can extend the RL - based dialog manager to deal with unpredictable user actions after deployment . Furthermore , our method holds no constraints on the architecture of the new system and retains the key benefit of data - driven methods . It means the new system can be maintained iteratively and further refined by RL once a new interaction environment is available . The sim -", "entities": []}
{"text": "Extended System User : I want to find a Sichuan restaurant . User : I want to find a Sichuan restaurant in System : Where do you want to go ? Zhongguancun area . User : Zhongguancun area . System : \" Hot Charge \" is a nice Sichuan restaurant in System : \" Hot Charge \" is a nice Sichuan restaurant in in Zhongguancun area . Zhongguancun area . User : What 's the discount of this restaurant ? User : Can you tell me the discount ? System : This restaurant is offering 10 % off . System : The average cost is 42 CNY . User : Is there a waiting line ? User : Tell me the discount ! System : Currently there are 5 parties waiting in line . System : \" Hot Charge \" is a nice Sichuan restaurant in User : Can you book a table for me ? Zhongguancun area . System : Ok . User : Bye . User : Thanks . System : Bye . System : You are welcome . Subjective Rating = 2 Subjective Rating = 5 Table 5 : Sample dialogs of the original and extended systems in the human evaluation . The original system will collapse when encountering unseen slots and intents . After extension , the new system can handle more user actions . ulation and human experiments show our proposed framework guarantees the maintainability and scalability in RL - based systems , which are necessary for any industrial application .", "entities": []}
{"text": "The research work described in this paper has been supported by the National Key Research and Development Program of China under Grant No . 2017YFB1002103 and also supported by the Natural Science Foundation of China under Grant No . 61333018 .", "entities": []}
{"text": "Neural NLG for Methodius : From RST Meaning Representations to Texts *", "entities": []}
{"text": "The textual output of Methodius is pseudo - English with some expressions replaced by canned text , the morpho - syntactic descriptions of which are not present in either the content plan or in the logical form . Instead the canned text is retrieved from the Methodius system 's database by looking up the reference given in the content plan . Such canned texts might occur infrequently in a relatively small corpus . To avoid data sparsity , we substitute canned texts by their labels , cf . ( 1b ) , ( 1a ) . Note that the textual output of Methodius does n't contain nonterminal symbols the sort used in Balakrishnan et al 's approach . We use only special terminal symbols , which appear both in content plans ( decorating terminal nodes in the tree ) and in texts ( representing the corresponding chunks of canned texts ) .", "entities": []}
{"text": "We anonymize exhibits by replacing them with entity0 , entity1 , etc in both the content plans and corresponding text . In each text , there is a single focal exhibit . The focal exhibit is compared to one or many exhibits and this is expressed in text using singular and plural forms respectively ( e.g. the other vessel , which originates from region1 VS the other coins , which were created in city0 ) . We use two substitution forms : entity1 ( for singular ) and entityplural . Content plans are augmented with relevant information concerning the types of exhibits that occur in a content plan . The type predicate relates an exhibit to the NP it corresponds to in the text . This information is encoded within the Methodius logical form and thus is available for the Methodius system when it comes to generating text . However , since we anonymize exhibits and we ignore the logical forms , we need to explicitly provide the type information of each exhibit . Methodius sometimes produces content plans in which the first FACT TYPE is missing arg2 . This missing position corresponds to the focal exhibit in the text . The modified corpus regiments the input by ensuring every FACT TYPE includes arg2 . For every exhibit in the the Methodius content plan not explicitly typed we add a new OPTIONAL TYPE branch to the tree which includes the type of the exhibit . ( 1 ) a. This is a marriage cauldron and it was created during the classical period in between 420 and 410 2 . The output of Methodius is limited with respect to both the homogeneity and lengths of the texts - Methodius only infrequently produces very short or long texts , e.g. one or six sentences respectively . One of the test sets , which is described below , is explicitly constructed to determine whether the model 's knowledge of discourse structure is limited by the length of the texts it sees .", "entities": []}
{"text": "In the training set , there are around 4300 examples harvested by using the Methodius system . The higher number of inputs with SIMILARITY ( 2911 ) is due to the Methodius system . This proportion of SIMILARITY persists into every split except the challenge test set , where the number of inputs of distinct RST types is more homogeneous .", "entities": []}
{"text": "We have two splits of data for our experiments . One we dub the ' challenge split ' , the other the ' standard split ' . The major difference between them is their average lengths . The average length of the challenge split items are roughly half the length of the training set items , while the average length of the standard split is roughly seventy five percent of the training set items .", "entities": []}
{"text": "In the standard split , the average length of items in the training and validation sets is roughly the same ; the distribution of lengths is similar in the training , valid , and test sets but the training set still includes slightly longer sequences on average . The proportion of items with distinct RST types is roughly the same between the train , valid , and standard test sets . This test set does n't identify possible effects of item length on correct discourse structure production . Challenge The challenge test set consists of items on average half the length of the the average lengths of items in the train and valid sets . Due to the lower frequency of short items produced by Methodius , the number of items in the challenge set is reduced . The distribution of items with CON - TRAST and SIMILARITY is homogeneous . With respect to distinguishing RST types , the challenge test set is no more difficult than the standard test set ; the item length is shorter but no less structured . Moreover , the set of lexemes - including delexicalized expressions - which occur in the test set are present in the training set . However , there are patterns in the test set which are uncommon or unseen in the training set , e.g. one content plan in the challenge set begins with CONTRAST but no such items are found in training . This distinguishes possible effects of length , e.g. ' RST type X occurs in the third sentence ' , from effect of RST tree structure in the input for correct discourse structure production , i.e. ' RST type X must correspond to lexeme / structure Y ' . These challenge test - specific content plans help to determine how well a model learns to associate certain strings with either CONTRAST or SIMILAR - ITY . If the model stumbles on shorter texts then its knowledge of RST structure might be ( erroneously ) conditioned on item length . 5 Evaluation Methods", "entities": []}
{"text": "Since the data we generate after preprocessing contains certain expressions which we dub ' special terminals ' , these expressions can be tracked between the target and the hypothesis . By obtaining metrics based on the correspondence between these special terminals , we get a picture how close the hypothesis is to the target . This measure enjoys some useful properties . Firstly , it 's cheap - it is defined solely in terms of expressions which occur both in the input ( content plan ) and in the output ( text ) . Second , the special terminals stand for important parts of the text - those ones that are explicitly provided as values to features in the content plan ( since they are terminals ) . Hence , having information about their presence gives us a good hint of the quality of a text . In addition to standard evaluation metrics scores such as BLEU4 , we report the following metrics for each test item : 2 Repetitions : A special terminal is present in the hypothesis n times but in the target text it occurs m times , where m < n. We calculate n\u2212m for every such special terminal and sum up . Omissions : How many times special terminals occurring in the target text are not generated at all in the hypothesis . Hallucinations : Number of occurrences of those special terminals in the hypothesis that have no occurrence in the target .", "entities": []}
{"text": "When the FACT - LBL model makes mistakes , such mistakes frequently correspond to the substitution of one lexeme marking a rhetorical relation for another marking a distinct ( sometimes opposite ) relation . The following hypothesis replaces the CON - TRAST in the target with a SIMILARITY , misidentifying the origin of some previous exhibit in the chain . T unlike the other exhibits you recently saw , which originate from region0 , this coin was originally from city0 . H like the other exhibits you recently saw , this coin originates from city0 . In the following hypothesis the erroneous substitution of SIMILARITY by CONTRAST leads to an outright contradiction : T like the other exhibits you recently saw , this marriage cauldron is currently in museum0 . H unlike the other exhibits you recently saw , which are located in museum0 , this marriage cauldron is located in museum0 . Less frequently the insertion of SIMILARITY or CONTRAST compares the topic of an exhibit to itself : T this is a statue and it was created during historical - period0 in entity0 - creation - time . H this is a statue and it was created during historical - period0 in entity0 - creation - time . like the statue , this statue was created during historical - period0 . The details of the number of errors and successes in generating discourse connectives are reported in Appendix A. Fig . 4 and Fig . 5 show Fisher 's Exact Test statistics for best performing RST ( SM and LG ) and FACT ( SM and LG ) models .", "entities": []}
{"text": "The overall conclusion is that including RST relations in the input content plans is necessary to achieve optimum performance in correctly and coherently expressing discourse relations in the neural reimplementation of Methodius . This is somewhat surprising since the FACT - only inputs actually have all the information necessary to infer that a SIMILARITY or CONTRAST relation should be expressed , but the models nevertheless struggle to learn the desired same / different generalization . Moreover , the errors are often jarring - they produce genuine incoherence in the text . We see the best performance from the RST model with small but clean self - training data ( RST - SM ) , as it comes from Methodius and thus follows the same general patterns as the ones in the test set . The large RST model ( RST - LG ) had similar , where an item produces an error if either there is an incorrectly generated discourse cue word , or there has been a cue word generated while the target has none , or no cue word is generated but the reference contains one . The dotted line links two models if there is a significant difference between their performance in terms of Fisher 's Exact Test statistics ( with significance threshold of 5 % ) . performance to the small one . FACT models , both small and large , show significant self - training improvements when reranking with reverse models . Because the RST baseline already performs relatively well , such an improvement is not observable with them . RST - SM with vanilla self - training already showed high performance . In the case of the FACT models , we saw that reranking with reverse models lowers repetitions , omissions and hallucinations in total . It was also beneficial for the RST - LG model . Despite the highly regular nature of the rulebased texts , even our best models do not get close to zero content errors , highlighting the importance of continued work on eliminating these errors , e.g. using pretrained models ( Kale , 2020 ; Kale and Rastogi , Forthcoming ) or constrained decoding ( Balakrishnan et al , 2019b ) .", "entities": []}
{"text": "This research was supported by a collaborative open science research agreement between Facebook and The Ohio State University .", "entities": []}
{"text": "A question that as important as designing a subtle method is \" what make the model fail \" . By answering this question we can gain an insight into our model performance and further improve it . The method used for analyzing is the bottom row in Table 1 . Firstly , we plot confusion matrix ( Figure 3 ) , observe that both True Positive and True Negative are evenly distributed with a small proportion of False Negative and False Positive , indicating our model did not bias towards any classes . Secondly , we randomly sample some failures the model made ( Table 2 ) . It seems like sentences containing more numbers are usually ( mis ) classified as INFORMA - TIVE while the ones containing less numbers are classified as UNINFORMATIVE . This can be explained that INFORMATIVE tweets provide information about recovered , suspected , confirmed and death cases . Therefore , numbers appearance is inevitable .", "entities": []}
{"text": "Lot of research on MT evaluation deals with classification and analysis of MT errors , for example ( Vilar et al , 2006 ; Farr\u00fas et al , 2010 ; Stymne and Ahrenberg , 2012 ; Lommel et al , 2014 ; Klubi\u010dka et al , 2018 ) . Few papers deal with human perception of these errors , but neither of them defines precisely which criterion is the translation quality based on . Kirchhoff et al ( 2014 ) uses conjoint analysis to investigate user preferences for error types of SMT systems . First , the errors in MT outputs were annotated , and then MT outputs with different error types were given to the crowd evaluators . They were asked to choose the MT output which they like best and to give the reason for their preference . One of the findings is that the frequencies of error types are not related to the user preferences . The most dispreferred error type was word order error , although it was the least frequent one . It was followed by word sense errors ( ambiguity ) , then morphological errors ( most frequent ones ) , whereas errors in function words were the most tolerable . A similar study on SMT outputs based on linear mixed - effects models is described in ( Federico et al , 2014 ) , aiming to estimate the impact of different translation errors to the overall translation quality . For each MT output , experts were asked to assign a score on a 5 - point scale while other experts annotated the errors . The results confirmed that the frequency of errors of a given type does not correlate with human preferences . Another finding is that omissions and mistranslations have the highest impact on the overall translation quality . In addition , it is observed that certain combinations of errors have less impact than each of those error types ocurring in isolation . In the last few years , with the emergence of NMT systems which generate much more fluent and readable outputs but still are prone to adequacy errors , some studies have concentrated on investigating adequacy and fluency errors . Martindale and Carpuat ( 2018 ) carried out a survey to determine how users respond to good translations compared to translations that are either adequate but not fluent , or fluent but not adequate . This study showed that users strongly disliked disfluent translations , but were much less bothered with adequacy errors . Therefore , it was concluded that fluent translations with adequacy errors can mislead the reader to trust an incorrect meaning . Automatic identification of these misleading \" fluently inadequate \" translations using source text , reference human translation and MT output was proposed in ( Martindale et al , 2019 ) , and the main finding was that NMT systems generate more misleading translations than SMT systems . However , the question about how many adequacy errors are actually hidden by fluency remained open . To the best of our knowledge , the relation be - tween adequacy and comprehensibility has not been investigated yet . Comprehensibility , similarly to fluency , has an immediate effect on the reader , while adequacy problems can be perceived only if the reader has access to the source text or to a correct translation to find out that the meaning is wrong . This means that comprehensibility may have the same misleading effect making the reader accept an incorrect information . On the other hand , because comprehensibility is different than fluency ( fluent sentences can be incomprehensible and vice versa ) , the effects might be different .", "entities": []}
{"text": "As mentioned in Introduction , comprehensibility reflects the degree to which a translated text can be understood , and adequacy reflects the degree to which the translation conveys the meaning of the original text in the source language . Comprehension should be assessed without access to the original text in the source language ( or a correct translation ) , while the original text ( or a correct translation ) is mandatory for adequacy . Therefore , each annotator first completed the annotation of comprehension issues while reading only the translation . After completing ( usually after about two weeks ) , they annotated adequacy issues by comparing the translation with the original source text . For each criterion , the annotators were asked to distinguish two levels of issues : major issues and minor issues . While for this particular study we are interested only in major issues , we did not want any errors to remain unannotated . The following guidelines were given to the annotators : Comprehensibility : mark all parts of the text ( single words , small or long phrases , or entire sentences ) which are not understandable ( it does not make sense , it is not clear what it is about , etc . ) as major issues ; mark all parts of the text ( again : words , phrases or sentences ) which seem understandable but contain grammatical or stylistic errors as minor issues ; if it seems that something is missing , add \" XXX \" to the corresponding position . Adequacy : mark all parts of the translation ( single words , small or long phrases , or entire sentences ) which have different meaning than the original English text as major issues ; mark all parts of the translation ( again : words , phrases or sentences ) which do not actually change the meaning of the source text , but contain sub - optimal lexical choices or grammar errors as minor issues ; if some parts of the original English text are missing in the translation , add \" XXX \" to the corresponding position in the translation ; if there are any errors in the source language 8 ( spelling or grammar errors , etc . ) , mark its translation as major or minor issue if it does not correspond to the intented English word even though it is a correct translation of the erroneous English word . The annotators were seeing the entire reviews during the process , not only isolated segments or blocks of 2 - 3 segments . In this way , it was ensured that the annotators were able to spot any contextdependent issues . We wanted the texts to be annotated by a reliable group of readers which is neither too homogeneous as a group of professional translators nor too heterogeneous as crowd evaluators . Therefore , the annotation was performed by computational linguistics researchers and students , fluent in the source language and native speakers of the target language . They had different backgrounds , coming from technical studies , translation studies as well as from humanities . Because the annotators were not asked to perform any fine - grained categorisation , the interannotator agreement was high - annotators assigned identical issue tags to more than 70 % of words . More details about the annotation process can be found in ( Popovi\u0107 , 2020 ) .", "entities": []}
{"text": "Table 1 presents overall percentages 9 of words perceived as issues , separately for each of the two translation criteria . In total ( including both target languages and all three MT systems ) , 9.5 % words in the text were perceived as incomprehensible , and the meaning of 9.9 % words was changed in the translation process . As for minor issues , 13.5 % words were perceived as slightly difficult to understand , and 12.8 % were not translated in the optimal way . It can be noted that the overall amounts of comprehensibility and adequacy issues are similar . However , it does not necessarily mean that the majority of words is perceived both as incomprehensible and inadequate . Therefore , we examined major comprehensibility and adequacy issues in depth . 9 raw counts divided by the total number of words in the text including those without issues and the omission marks \" XXX \"", "entities": []}
{"text": "In order to determine presence or absence of misleading translations , we explored the following cases of different relations between comprehensibility and adequacy errors : only major adequacy issue A maj ( comprehensible inadequate translation ) - incorrect information is accepted - The meaning of the original text is changed but the translation is readable and comprehensible . The reader feels comfortable with the text and does not notice any problem , thus accepting the incorrect meaning . A maj + C min - major adequacy and minor comprehension issues ( almost comprehensible inadequate translation ) - incorrect information can be accepted - The meaning of the original text is changed , and the reader finds this incorrect meanining slightly difficult to understand . The reader is therefore susceptible to accept this incorrect meaning . A maj + C maj - both major issues ( incomprehensible inadequate translation ) - incorrect information is discarded - The meaning of the original text is changed , and the reader is not able to understand this changed meaning . The reader clearly notices that there is something wrong with the text . Table 2 : Raw counts and percentages of words ( normalised by the total number of words , including those without issues and the omission marks \" XXX \" ) of all combinations of perceived issue types . For cases involving major adequacy issues , the percentages normalised by the total number of major adequacy issues are shown , too , in order to estimate the portion of hidden adequacy issues . For the sake of completenes , the numbers are presented for minor issues , too , although they were not further analysed in this work . optimal way , but the reader can not understand it . The reader is thus missing some correct information . only major comprehension issue C maj ( incomprehensible adequate translation ) - correct information is discarded - The meaning of the original text is correctly conveyed to the translation , but the reader can not understand it . The reader is therefore not able to get the fully correct information . Table 2 presents raw counts and percentages of words perceived in the described ways . For the sake of completeness , the numbers for minor issue types are shown as well . The numbers are generally in line with the findings of the previous work ( Kirchhoff et al , 2014 ; Federico et al , 2014 ) regarding lack of correlation between the error frequency and perception of severity - in our texts , the frequencies of words perceived only as minor issues are higher than the frequencies of words perceived as major issues . As already mentioned , minor issues were not further analysed in this work , because by definition they were not perceived as essential : either the meaning was preserved although not conveyed in the best way , or the translation was slightly difficult to understand , or both . 2 shows that about 3 % of words in the translated text are mis - leading , and 2.5 % are potentially misleading . This means that of every 100 words in the translation , 3 are fully accepted by the reader although their meaning is not correct , and 2 can be potentially accepted . Furthermore , from all major adequacy errors in the text , only 45.5 % are incomprehensible . About 30 % of adequacy errors are fully hidden so that the reader does not notice any problem , and about 25 % are partially hidden because the reader is not fully sure that s / he understands the text , but s / he is very susceptible to accept the meaning .", "entities": []}
{"text": "All in all , the portion of misleading translations is not negligible , so we continued our analysis by trying to identify error types associated with such translations . Also , we wanted to explore whether there are error types related ( almost ) exclusively to them .", "entities": []}
{"text": "For each ( group of ) word ( s ) perceived as compreensibility or adequacy major issue , we assigned an error type . The error types were not predefined by any particular error typology , but identified while looking into the text . It is worth noting that many error types were identified , but most of them are ocurring rarely in the text . Also , for some of the annotated words no particular error type could be defined , which is probably an effect of annotators ' personal preferences . The most frequent error types perceived as misleading translations can be defined as follows :", "entities": []}
{"text": "The obtained translation for the given word is in principle correct , but not in the given context ( word sense error ) .", "entities": []}
{"text": "The obtained translation for the given word is incorrect .", "entities": []}
{"text": "An English sequence consisting of a head noun and additional nouns and adjectives is incorrectly translated . Formation rules for Serbian and Croatian are rather different than for English and there is often no unique solution . The examples in the table below represent two English noun collocations and their reference translations into Serbian and Croatian together with the corresponding English glosses . This type of issue is relevant for many Slavic languages . spelling error in source A word in the original text in the source language has spelling errors which result in incorrect translation . This type of issue is especially relevant for user - generated content . subject - verb agreement A verb inflection in the translation denoting person does not correspond to the subject .", "entities": []}
{"text": "This work presents the results of a detailed analysis of translation errors perceived by readers as major comprehensibility and/or major adequacy issues . The main finding is that good comprehensibility , similarly to good fluency , can mask a number of adequacy errors . Of all major adequacy errors , 30 % were fully comprehensible , thus fully misleading the reader to accept the incorrect information . An - word - by - word 5.4 word - by - word 5.5 word - by - word 5.8 mistranslation 3.7 subject - verb 3.8 ( subject - verb 3.2 ) ( subject - verb 4.5 ) subject - verb 4.9 ( subject - verb 2.0 ) untranslated 3.8 ( source spelling 2.4 ) ( source spelling 3.5 ) { POS ambiguity 3.5 } ( source spelling 1.2 ) Table 3 : The most frequent error types perceived as a particular issue combination . The numbers represent percentages of the error type perceived as the issue combination - 24.0 % of all comprehensible inadequate translations ( accepted incorrect information ) are ambiguity errors , 6.0 % are mistranslations , etc . Parentheses indicate that the error type was not in the top list for the given issue combination , but it is presented for comparison because it is in the top list for misleading traslations . other 25 % of major adequacy errors were perceived as almost comprehensible , thus being potentially misleading . In addition , a vast majority of omissions ( about 70 % ) is hidden by comprehensibility . Further analysis of those misleading translations was carried out in order to find out which types of translation errors are perceived in this way . Ambiguous words , mistranslations , noun phrases , untranslated words , word - by - word translations , subject - verb agreement and spelling errors in the original text were identified as the most frequent error types in misleading translations . Although noun phrase problems are typical for Slavic languages and errors in the source text are typical for user generated content , the rest of the error types is rather general . However , none of these error types is exclusively related to misleading translations , but are also frequent in fully incorrect ( incomprehensible inadequate ) and discarded correct ( incomprehensible adequate ) translations . Deeper analysis is needed to potentially detect underlying phenomena specifically related to misleading translations . Apart from the obvious directions for future work such as analysing more texts and including more language pairs and domains , the presented analysis can be expanded in the following directions : including fluency in the analysis , including all minor issues in the analysis , further analysis of omissions , and investigating co - ocurrences of different error types . Another experiment could include monolingual annotators for comprehensibility in order to completely eliminate potential influence of knowlegde of the source language . source For the kind of shipping they want it would be reasonable to expect a better presentation . MT Za vrstu dostave XXX\u017eele da bi bilo razumno o\u010dekivati bolju prezentaciju . gloss For the kind of shipping { which } they want that would be reasonable to expect better presentation .", "entities": []}
{"text": "We found an issue during the prediction where some words are labeled with O , in between B - label and I - label tags . Our solution is to insert I - label tag if the tag is surrounded by B - label and I - label tags with the same entity category . Another problem we found that many I - label tags are paired with B - label in different categories . So , we replace B - label category tag with corresponding I - label category tag . This step improves the result of the pre -", "entities": []}
{"text": "GOT : Testing for Originality in Natural Language Generation", "entities": []}
{"text": "Our proposed generation originality test ( GOT ) determines whether : 1 . any fragment in a generated sentence equals an \" original \" fragment in the ground truth , in which case the generation may be in violation of a copyright law , if no citation of the original source is included ; or , 2 . the generated sentence is \" original \" , per Definition 1 , below . Definition 1 ( Original Sentence ) . A sentence , whether generated or in the ground truth , of n tokens is original if there exists an original k - gram within the sentence for some k\u2264n . The originality of k - grams is defined next . The definition of originality of a fragment ( or k - gram ) depends on whether we are referring to a generated fragment or to a fragment in the ground truth . Generated fragments are tested against the ground truth . If the generated fragment does not appear in the ground truth , then the generated fragment is considered original . If it appears once in the ground truth , then it is considered not original and so a citation may be needed . See Table 1 for a summary of the criterion for each type of fragment to be true . In Table 1 , C equals the number of times that fragment appears in the ground truth .", "entities": []}
{"text": "Generated Metaphor tears The arrested waters shone and danced . fathers Expectations are premeditated resentments . character Today is the companion of genius . friends Assumptions are the termites of relationships . writers The writer is the lengthened shadow of a man . world This world is the rainbow of us . truth The brain is the eden of a star . innocence The cure for silence is the salt of speech . imagination Success is the only deadline .", "entities": []}
{"text": "Our approach to originality testing includes two contributions : An automatic test , where no standard existed , for originality in generated language An automatic test , where no standard existed , for identifying where generators are in violation of copying an original use of language without attribution The first contribution tells us whether a generation is an original use of language . The second contribution tells us whether a generation is , at least , not at risk of committing plagiarism . For example , the sentence \" A bird built a nest \" is not an original use of language ; however , it is at least probably not in violation of plagiarism since it does not contain a fragment that is so rare that it should be protected as an original use of language .", "entities": []}
{"text": "On the application of Transformers for estimating the difficulty of Multiple - Choice Questions from text", "entities": []}
{"text": "ASSISTments 2 is an online tutoring system that provides instructional assistance while assessing students ( Feng et al , 2009 ) . In practice , this means that questions - called problems - can be broken down into steps : if the student does not get the original problem correctly , he has to answer a sequence of scaffolding questions that break the problem down into steps . In the current work , we consider both original and scaffolding problems for QDE from text . An example problem and the corresponding scaffolding questions are shown in Appendix B. We filter the dataset to keep only questions that are answered by at least 50 students , to improve the reliability of the estimation of ground truth latent traits with IRT ; on average , each item is answered by 151 students and each student answers to 64 different items . We also remove the questions that require external resources and the system messages ( e.g. \" Submit your answer from the textbook . \" , \" Sorry , that is incorrect . Let 's go to the next question ! \" ) . After removal of the unsuitable questions , the final dataset used for QDE from text contains 11 , 393 different items . A is publicly available for download 3 ; Q is publicly available under request 4 .", "entities": []}
{"text": "CloudAcademy 5 is an e - learning provider offering online courses about IT technologies . All the questions are MCQ and we have access to the text of the possible choices . An example question is shown in Appendix B. The dataset used in our experiments is a sub - sample of the CloudAcademy data collection and it was generated in order to have only questions answered by at least 50 students . A contains 7 , 323 , 502 interactions , involving 34 , 696 students and 13 , 603 unique questions ; on average , each item is answered by 304 students and each student answers to 115 different items . The overall correctness is 66 % . L contains the transcript of some of the online lectures offered by CloudAcademy about the same topics ( i.e. cloud technologies ) assessed by the questions . L contains a total of 159 , 563 sentences and 3 , 228 , 038 words .", "entities": []}
{"text": "The computer game Peter wants to buy will cost at least $ 50 and not more than $ 70 . He earns $ 3 an hour running errands for his grandmother . Which inequality shows the number of hours , n , he will have to work to pay for the game ? Original 326 What is the minimum cost of the game ? Scaffolding 327 What is the maximum cost of the game ? Scaffolding 328 Write an expression that represents the amount of money Peter earns in n hours . Scaffolding 329 Which inequality shows the number of hours , n , Peter will have to work to pay for the game ? Scaffolding", "entities": []}
{"text": "Question A user has launched an EBS backed EC2 instance in the US - East - 1 region . The user wants to implement a disaster recovery ( DR ) plan for that instance by creating another instance in a European region . How can the user accomplish this ? Correct choice Create an AMI of the instance and copy the AMI to the EU region . Then launch the instance from the EU AMI .", "entities": []}
{"text": "Use the \" Launch more like this \" option to copy the instance from one region to another .", "entities": []}
{"text": "Copy the instance from the US East region to the EU region .", "entities": []}
{"text": "An example problem from ASSISTments and the corresponding scaffolding questions are shown in Table 4 . An example question from CloudAcademy , with its correct answer and distractors , is given in Table 5 .", "entities": []}
{"text": "Tandem Anchoring : a Multiword Anchor Approach for Interactive Topic Modeling", "entities": []}
{"text": "Single word anchors can be opaque to users . For an example of bewildering anchor words , consider a camera bag topic from a collection of Amazon product reviews ( Table 1 ) . The anchor word \" backpack \" may seem strange . However , this dataset contains nothing about regular backpacks ; thus , \" backpack \" is unique to camera bags . Bizarre , low - to - mid frequency words are often anchors because anchor words must be unique to a topic ; intuitive or high - frequency words can not be anchors if they have probability in any other topic . The anchor selection strategy can mitigate this problem to some degree . For example , rather than selecting anchors using an approximate convex hull in high - dimensional space , we can find an exact convex hull in a low - dimensional embedding ( Lee and Mimno , 2014 ) . This strategy will produce more salient topics but still makes it difficult for users to manually choose unique anchor words for interactive topic modeling . If we instead ask users to give us representative words for this topic , we would expect combinations of words like \" camera \" and \" bag . \" However , with single word anchors we must choose a single word to anchor each topic . Unfortunately , because these words might appear in multiple topics , individually they are not suitable as anchor words . The anchor word \" camera \" generates a general camera topic instead of camera bags , and the topic anchored by \" bag \" includes bags for diaper pails ( Table 1 ) . Instead , we need to use sets of representative terms as an interpretable , parsimonious description of a topic . This section discusses strategies to build anchors from multiple words and the implications of using multiword anchors to recover topics . This extension not only makes anchors more interpretable but also enables users to manually construct effective anchors in interactive topic modeling settings .", "entities": []}
{"text": "We first need to turn words into an anchor . If we interpret the anchor algorithm geometrically , each row of Q represents a word as a point in V - dimensional space . We then model each point as a convex combination of anchor words to reconstruct the topic matrix A ( Equation 1 ) . Instead of individual anchor words ( one anchor word per topic ) , we use anchor facets , or sets of words that describe a topic . The facets for each anchor form a new pseudoword , or an invented point in V - dimensional space ( described in more detail in Section 2.2 ) . While these new points do not correspond to words in the vocabulary , we can express nonanchor words as convex combinations of pseudowords . To construct these pseudowords from their facets , we combine the co - occurrence profiles of the facets . These pseudowords then augment the original cooccurrence matrix Q with K additional rows corresponding to synthetic pseudowords forming each of K multiword anchors . We refer to this augmented matrix as S. The rest of the anchor algorithm proceeds unmodified . Our augmented matrix S is therefore a ( V + K ) \u00d7 V matrix . As before , V is the number of token types in the data and K is the number of topics . The first V rows of S correspond to the V token types observed in the data , while the additional K rows correspond to the pseudowords constructed from anchor facets . Each entry of S en - codes conditional probabilities so that S i , j is equal to p ( w i | w j ) . For the additional K rows , we invent a cooccurrence pattern that can effectively explain the other words ' conditional probabilities . This modification is similar in spirit to supervised anchor words ( Nguyen et al , 2015 ) . This supervised extension of the anchor words algorithm adds columns corresponding to conditional probabilities of metadata values after having seen a particular word . By extending the vector - space representation of each word , anchor words corresponding to metadata values can be found . In contrast , our extension does not add dimensions to the representation , but simply places additional points corresponding to pseudoword words in the vectorspace representation .", "entities": []}
{"text": "After constructing the pseudowords of S we then need to find the coefficients C i , k which describe each word in our vocabulary as a convex combination of the multiword anchors . Like standard anchor methods , we solve the following for each token type : C * i , = argmin C i , D KL S i , K k=1 C i , k S g k , . ( 6 ) Finally , we appeal to Bayes ' rule , we recover the topic - word matrix A from the coefficients of C. The correctness of the topic recovery algorithm hinges upon the assumption of separability . Separability means that the occurrence pattern across documents of the anchor words across the data mirrors that of the topics themselves . For single word anchors , this has been observed to hold for a wide variety of data ( Arora et al , 2012b ) . With our tandem anchor extension , we make similar assumptions as the vanilla algorithm , except with pseudowords constructed from anchor facets . So long as the occurrence pattern of our tandem anchors mirrors that of the underlying topics , we can use the same reasoning as Arora et al ( 2012a ) to assert that we can provably recover the topic - word matrix A with all of the same theoretical guarantees of complexity and robustness . Furthermore , we runtime analysis given by Arora et al ( 2013 ) applies to tandem anchors . If desired , we can also add further robustness and extensibility to tandem anchors by adding regularization to Equation 6 . Regularization allows us to add something which is mathematically similar to priors , and has been shown to improve the vanilla anchor word algorithm ( Nguyen et al , 2014 ) . We leave the question of the best regularization for tandem anchors as future work , and focus our efforts on solving the problem of interactive topic modeling .", "entities": []}
{"text": "Before addressing interactivity , we apply tandem anchors to real world data , but with anchors gleaned from metadata . Our purpose is twofold . First , we determine which combiner from Section 2.2 to use in our interactive experiments in Section 4 and second , we confirm that well - chosen tandem anchors can improve topics . In addition , we examine the runtime of tandem anchors and compare to traditional model - based interactive topic modeling techniques . We can not assume that we will have metadata available to build tandem anchors , but we use them here because they provide a high water mark without the variance introduced by study participants .", "entities": []}
{"text": "We examine the qualitative differences between how users select multiword anchor facets versus single word anchors . Table 2 gives examples of topics generated using different anchor strategies . In a follow - up survey with our users , 75 % find it easier to affect individual changes in the topics using tandem anchors compared to single word anchors . Users who prefer editing multiword anchors over single word anchors often report that In all cases higher is better . Multiword anchors produce topics which are more significant than single word anchors . multiword anchors make it easier to merge similar topics into a single focused topic by combining anchors . For example , by combining multiple words related to Christianity , users were able to create a topic which is highly specific , and differentiated from general religion themes which included terms about Atheism and Judaism . While users find that use tandem anchors is easier , only 55 % of our users say that they prefer the final topics produced by tandem anchors compared to single word anchors . This is in harmony with our quantitative measurements of topic coherence , and may be the result of our stopping criteria : when users judged the topics to be useful . However , 100 % of our users feel that the topics created through interaction were better than those generated from Gram - Schmidt anchors . This was true regardless of whether we used tandem anchors or single word anchors . Our participants also produce fewer topics when using multiword anchors . The mean difference between topics under single word anchors and multiple word anchors is 9.35 . In follow up interviews , participants indicate that the easiest way to resolve an ambiguous topic with single word anchors was to create a new anchor for each of the ambiguous terms , thus explaining the proliferation of topics for single word anchors . In contrast , fixing an ambiguous tandem anchor is simple : users just add more terms to the anchor facet .", "entities": []}
{"text": "This work was supported by the collaborative NSF Grant IIS - 1409287 ( UMD ) and IIS - 1409739 ( BYU ) . Boyd - Graber is also supported by NSF grants IIS - 1320538 and NCSE - 1422492 .", "entities": []}
{"text": "A second experiment to validate that retention is used as a heuristic in models ' predictions is to modify their input sentences in a controlled manner", "entities": []}
{"text": "We thank the anonymous reviewers for their thoughtful comments . Part of this work was performed while Bruno Taill\u00e9 was an employee of BNP Paribas and supported by the French Ministry of Higher Education , Research and Innovation under the CIFRE convention 2018/0327 .", "entities": []}
{"text": "In this section , we first compare our HacRED with existing datasets . Then we re - evaluate the SOTA RE models on HacRED and systematically analyze their abilities on different experiment settings . At last , we demonstrate the effectiveness of HacRED via a case study .", "entities": []}
{"text": "As shown in Figure 4 , the text mentions multiple organization entities and similar relations including graduate_from and affiliation_of . The incorrect triple ( Lu , graduate_from , Yanjing University ) extracted by CasRel represents that models struggle to capture fine - grained semantic information . The distractive phrases study for a doctorate could result in the incorrect extraction ( Wu , graduate_from , University of Chicago ) , which can be rectified by comprehending the context of before finishing his doctoral dissertation . Reasoning is needed to extract the triple ( Wu , af filiation_of , Yanjing University ) since he worked as a professor in the organization .", "entities": []}
{"text": "In order to effectively evaluate the RE models and accelerate the research of practical RE , we first analyze the performance gap between popular datasets and practical applications . Therefore , we construct a large - scale and high - quality Ha - cRED with reasonable data distribution and sufficient hard cases . To focus on the practical challenging cases , we propose a case - oriented construction framework . We also design a novel annotation method to guarantee the quality of Ha - cRED . Finally , we conduct extensive experiments and analyze the abilities of SOTA models from various aspects , which provides a deeper understanding of RE models and inspiration for further improvement .", "entities": []}
{"text": "In", "entities": []}
{"text": "We would like to thank the anonymous reviewers for their thoughtful and constructive comments . This", "entities": []}
{"text": "XL - NBT : A Cross - lingual Neural Belief Tracking Framework", "entities": []}
{"text": "The final component is a slot - value decoder , which predicts the score y of a given slot - value pair using the filtered information from the utterance representation r as : y ( c s , c v , u t , a t ) = W T y [ r ( u t ) g ( c s , c v , a t ) ] ( 3 ) where W y R H\u00d71 is the weight vector . The above expression computes the score for the slotvalue pair based on the information from the current turn . We combine it with the information from previous turns to get the final score : y ( cv | ut , at , cs ) = \u03bby ( cs , cv , ut , at ) + ( 1 \u2212 \u03bb ) \u0177 ( cs , cv , ut\u22121 , at\u22121 ) ( 4 ) here \u03bb is a combination weight . For each given slot c s , NBT selects the single highest value for informable slots and selects all values above a certain threshold for request slots . Here we replace the multi - layer perceptron in the orginal NBT by a linear output layer ( to be explained in section 5 ) .", "entities": []}
{"text": "We would like to thank the anonymous reviewers for their thoughtful comments and insightful feedback . This work was supported by the National Key Research and Development Program of China ( 2016YFB100101 ) .", "entities": []}
{"text": "We divide the 100 K SwissText dataset ( downloaded from SwissText 2019 website ) into three subsets : train , dev , and test in 90:5:5 ratio ( i. the test data ) . The experiments performed over these datasets are described in Section 4.3 ( denoted as S1 experimental setup ) .", "entities": []}
{"text": "The preprocess step involves preprocessing the dataset such that source and target are aligned and use the same dictionary . Additionally , we truncate the source length at 400 tokens and the target length at 100 tokens to expedite training ( See et al , 2017 ) .", "entities": []}
{"text": "In our first experiment we studied how irrelevant visual cues performed compared to relevant ones . We fine - tune the model with irrelevant cues defined as : S irrelevant : = ( 1 \u2212 S h ) , where , S h represents the human - based importance scores . As shown in the ' Grounding using irrelevant cues ' section of Table 1 , both HINT and SCR are within 0.3 % of the results obtained from looking at relevant regions , which indicates the gains for HINT and SCR are not necessarily from looking at relevant regions .", "entities": []}
{"text": "In order to truly assess if existing methods are using relevant regions to produce correct answers , we use our proposed metric : Correctly Predicted but Improperly Grounded ( CPIG ) . If the CPIG values are large , then it implies that large portion of correctly predicted samples were not properly grounded . Fig . A4 shows % CPIG for different variants of HINT trained on human attention - based cues , whereas Fig . A5 shows the metric for different variants of SCR trained on textual explanationbased cues . We observe that HINT and SCR trained on relevant regions have the lowest % CPIG values ( 70.24 % and 80.22 % respectively ) , indicating that they are better than other variants in finding relevant regions . However , only a small percentage of correctly predicted samples were properly grounded ( 29.76 % and 19.78 % for HINT and SCR respectively ) , even when trained on relevant cues .", "entities": []}
{"text": "Acknowledgement . This work was supported in part by AFOSR grant [ FA9550 - 18 - 1 - 0121 ] , NSF award # 1909696 , and a gift from Adobe Research . We thank NVIDIA for the GPU donation . The views and conclusions contained herein are those of the authors and should not be interpreted as representing the official policies or endorsements of any sponsor . We are grateful to Tyler Hayes for agreeing to review the paper at short notice and suggesting valuable edits and corrections for the paper .", "entities": []}
{"text": "Adhering to UDv2 guidelines provided an opportunity to make a consistent decision about topics under debate , and to generally revise inconsistencies in the system . Our revisions typically fall under one of the following three categories : predicate / argument types distinctions ( 3.3.1 ) , morphological vs. syntactic distinctions ( 3.3.2 ) , and Hebrew - specific vs. universal distinctions ( 3.3.3 ) .", "entities": []}
{"text": "Some inconsistencies in the treebank were spotted but not yet fixed as their automatic full retrieval and change is more complicated 12 . For example , it - extraposition construction is represented in UDv2 by a combination of nsubj and ccomp or advcl , but should be a combination of expl+csubj , as defined in the guidelines ( see example 9 in the supplements ) . In another case , lack of congruence was found between our treatment of participles and Adler et al ( 2008 ) . The feature of VerbForm = Part marks both deverbal nouns and present tense clauses , as in the following sentence . Hebrew makes various uses of the dative case , some of them fulfill purely discursive functionality ( Borer and Grodzinsky , 1986 ) . The current representation of the dative case marker in UDv2New does not give way to all possible meanings , including experiencer dative ( Berman , 1982 ) as opposed to ethical dative , the regular dative where the dative argument is subcategorized by the verb . The current UDv2 guidelines do not distinguish between the different types of dative , so an educated decision must be made locally as for how to tell them apart . 12 For reasons of brevity we do not discuss all of them in this work .", "entities": []}
{"text": "We thank the ONLP team at the Open University of Israel for fruitful discussions throughout the process . We further thank two anonymous reviewers for their detailed and insightful comments . This research is supported by the European Research Council , ERC - StG - 2015 scheme , Grant number 677352 , and by the Israel Science Foundation ( ISF ) , Grant number 1739/26 , for which we are grateful .", "entities": []}
{"text": "Enriching Language Models with Visually - grounded Word Vectors and the Lancaster Sensorimotor Norms", "entities": []}
{"text": "Acknowledgements Thanks to the anonymous reviewers whose comments really helped strengthen the paper . Also thanks to NVIDIA for donating that GPU that was used for the experiments . than baseline when frozen for 2 epochs , but the results of wac - aoa are significantly better .", "entities": []}
{"text": "Clause Final Verb Prediction in Hindi : Evidence for Noisy Channel Model of Communication", "entities": []}
{"text": "Research on sentence comprehension has conclusively established the widespread role of prediction during online processing ( e.g. , Marslen - Wilson , 1973 ; Altmann and Kamide , 1999 ; Staub and Clifton , 2006 ; Kutas and Hillyard , 1984 ) . It is known that comprehenders actively anticipate the upcoming linguistic material prior to receiving that information during listening or reading ( Luke and Christianson , 2016 ; Staub , 2015 ) . The role of active prediction during comprehension has particularly been emphasized for processing of SOV languages ( e.g. , Konieczny , 2000 ; Yamashita , 1997 ; * Equal contribution by KS and NB . Friederici and Frisch , 2000 ) . In particular , it has been argued that preverbal nominal features such as case - markers are effectively used to make precise prediction regarding the clause final verb . Indeed , the ADAPTABILITY HYPOTHESIS states that owing to the typological properties , the prediction system in SOV languages is particularly adapted to make effective use of preverbal linguistic material to make robust clause final verbal prediction ( Vasishth et al , 2010 ; Levy and Keller , 2013 ) . Evidence for the adaptability hypothesis come from various behavioral experiments that show effective use of case - markers to make clause final verbal prediction ( e.g. , Husain et al , 2014 ) , facilitation at the verb when the distance between the verb and its prior dependent increase ( e.g. , Konieczny , 2000 ) , and lack of structural forgetting in the face of complex linguistic environment ( e.g. , Vasishth et al , 2010 ) . On the other hand , the NOISY CHAN - NEL HYPOTHESIS assumes that prediction during comprehension is required to accommodate uncertainty in the input ( Gibson et al , 2013 ; Kurumada and Jaeger , 2015 ) . In other words , the hypothesis posits that comprehenders have the knowledge that speakers make mistakes during production , hence , comprehenders need to reconstruct the received input ( Ferreira and Patson , 2007 ) . The two hypotheses stated above make distinct assumptions regarding the utilization of pre - verbal context towards making clause final verbal predictions in SOV languages . One way to operationalize the predictions of the adaptability hypothesis is to assume that the preverbal linguistic material will be faithfully used to make verbal prediction , the noisy channel hypothesis on the other hand , assumes that the preverbal context is noisy and therefore subject to reconstruction . One consequence of this would be that the adaptability hypothesis would predict that verbal prediction should be robust while the noisy channel hypothesis would predict that verbal prediction should be susceptible to errors . In addition , the two hypotheses would make distinct prediction regarding the nature of errors that might occur during clause final verbal prediction . In order to probe the two hypotheses stated earlier , in this work , we investigate various incremental models that use local linguistic features to predict clause final verbal prediction in Hindi ( an SOV language ) . The distribution of these model predictions is compared with human data . In particular , we investigate to what extent the models are able to capture the nature of both grammatical as well as ungrammatical verbal predictions when compared to data collected from native speakers of Hindi . Further , in order to probe the assumptions of the noisy channel hypothesis more closely , we probe multiple noise functions to investigate the nature of preverbal context reconstruction during prediction . The paper is arranged as follow , in Section 2 we briefly describe the experimental results that we model . Section 3 provide the necessary details regarding methodology ( data / tools , model evaluation , etc . ) . In Sections 4 and 5 we respectively discuss the n - gram surprisal and the lossy - surprisal models . Section 6 presents the results . Section 7 discusses the current findings and its implications . We conclude the paper in Section 8 .", "entities": []}
{"text": "Given the abstract nominals and their case - marker , a model 's task is to complete the input string with an appropriate verb phrase . For example , if the model is given 3 noun tokens ( each with a unique case - marker ) with the lexical item replaced with a label A denoting animate , the task is to predict a verb phrase from this context . End of prediction is signalled as a punctuation . We note that , given a context , the model makes the prediction in an incremental fashion , rather than producing a one - shot phrase . This means that once a word is predicted , the model considers it as part of the context for the prediction of the next word . For example , given \" A - ne A - ko A - se \" , the model completes the sentence with w 1 w 2 w 3 in the following manner : A - ne A - ko A - se \u21d2 w 1 A - ne A - ko A - se w 1 \u21d2 w 2 A - ne A - ko A - se w 1 w 2 \u21d2 w 3 All implemented models discussed in Section 4 and Section 5 , use the 1/2/3 preverbal arguments as context . The rationale for use of local context is driven by the goal to model the role of argument structure in verbal prediction ( see Section 2 ) . Interestingly , the automatically parsed Hindi corpus ( Bojar et al , 2014 ) shows that arguments ( when compared to adjuncts ) tend to be closer to the verb 6 suggesting that the critical information needed to predict the verb should be accessible locally . In addition , we place an upper limit on the no . of predicted words - 2 words for 2 - NP conditions and 3 for 3 - NP . 7 Given the cognitive validity of limited beam - size ( e.g. , Boston et al , 2011 ) , we only pick the top 50 predictions for further analyses . Both human and model completions are manually annotated with verb classes based on the valency of the predicted verb . In addition , any nominal argument prediction was also annotated . Verb classes were labeled as IN ( intransitive ) , T ( transitive ) , DT ( ditransitive ) , CAUS ( causative ) , or combinations of the above in case a combination of non - finite and matrix verbs is predicted . For example , the following phrase contains a transitive verb preceded by its object noun : ( 2 ) khaana food khaaya eat - PT \u2212 N T Verb classes are used for comparing model output with human data as predictions are known to be graded rather than all - or - nothing lexical prediction ( Luke and Christianson , 2016 ; Staub , 2015 ) . Additionally , we do n't predict the verb classes directly to keep the model output consistent with the human data . These completions are then labelled for grammaticality automatically ; given the prompt condition and the verb class of the completion , we can infer the grammaticality of the sentence . 8 7 No significant change in the set of predictions was observed on increasing these numbers any further . 8 We use information from our human - annotated completion data as well as native speaker knowledge to construct an exhaustive list of valid completions per condition for this purpose .", "entities": []}
{"text": "Pred - Bias ) We first consider a noise distribution such that the context is reconstructed based on the predictability of a sub - context . This is driven by the idea that reconstruction of context given a noisy input will be influenced by prior linguistic exposure ( Futrell et al , 2020 ) . When the input is less frequent , its reconstruction will be influenced by frequent linguistic patterns in the language . Note , however , that a single word is obviously more frequent than two . Hence , we needed to control for the reduction in the size of the context that may arise due to this predictability bias . We do this by selecting sub - contexts based on their size with a preference to a larger size . Starting from the complete context , we thus iteratively reduce the size by 1 with a high probability ( d = 0.8 ) . 10 Thus , a sub - context of size m is considered with a probability d n\u2212m where m is the size of the corresponding context . Hence , p M ( r | c ) \u221d d n\u2212m p L ( r ) ( 3 )", "entities": []}
{"text": "We next consider a noise distribution which exploits both predictability bias as well as recency . It is well attested that recent input is easier to retrieve from memory compared to non - recent input ( e.g. , Lewis and Vasishth , 2005 ) . The function therefore is motivated by the fact that while previous linguistic exposure should influence context reconstruction ( Futrell et al , 2020 ) , this reconstruction should bias recent linguistic material . In a way , this model combines the properties of the Predictability bias noise model and the n - gram surprisal model . The conditional probability p ( r | c ) , here , thus can be seen as the multiplication of two parts - ( a ) predictability of r , p L ( r ) ; and ( b ) decaying erasure factor , p rec ( r | c ) . Let c = w 1 w 2 w n , r = w i 1 w i 2 w i k for some n , k , then p M ( r | c ) \u221d n\u2212k j=1 f n\u2212i j p L ( r ) , ( 4 ) where f is a constant fixed at 0.8 . 11 Thus , a context which is both predictable and can be formed from a recent subcontext is favored . The further a word is from the last uttered word , the lesser its likelihood of being a part of the reduced context r.", "entities": []}
{"text": "Table 4 compares the verb class results for the three models discussed above . The key finding is that the values of KLp for the LC - Surp Pred - Rec model is lower than the other models for most of the conditions . This suggests that the model performs better in capturing the verb class distribution found in the human data . 10 We also evaluated the model with d = 0.9 but the model with d = 0.8 gave better results . 11 Following the value fixed for d in Section 5.1 . In order to test if the improvement seen in the LC - Surp Pred - Rec model is indeed significant , we also performed the chi - square test to see if the categories of verb class predicted in the LC - Surp Pred - Rec model were significantly different from other models . Results showed that this was indeed true - categories of verb classes in the LC - Surp Pred - Rec model were significantly different ( p < 0.05 ) from both 4 - gram model and the LC - Surp Predbias model . 12 KLp provides a measure to quantify the divergence between the human and model prediction distributions . However , the nature of this divergence is still unclear . In order to understand the output of the models better , we evaluate them on some additional metrics . Finally , we report a qualitative analysis of the model output .", "entities": []}
{"text": "We implemented three models to predict clause final verbs in Hindi . Model outputs were compared with verb predictions of native speakers of Hindi using quantitative measures as well as qualitatively . Results show that the model that uses limited preverbal context with a predictability recency bias noise function captures the distribution of human data best . The success of this model is consistent with the idea that the reconstruction of the noisy context during prediction is influenced by prior linguistic exposure and that this process interacts with recency of input . These results support the noisy channel hypothesis to language comprehension .", "entities": []}
{"text": "Evaluating Attribution Methods using White - Box LSTMs", "entities": []}
{"text": "Formal languages are often used to evaluate the expressive power of RNNs . Here , we focus on formal languages that have been recently used to probe LSTMs ' ability to capture three kinds of dependencies : counting , long - distance , and hierarchical dependencies . We define a classification task based on each of these formal languages .", "entities": []}
{"text": "Strictly piecewise ( SP , Heinz , 2007 ) languages were used by Avcu et al ( 2017 ) and Kelleher ( 2018 , 2019a , b ) to test the propensity of LSTMs to learn long - distance dependencies , compared to Elman 's ( 1990 ) simple recurrent networks . SP languages are regular languages whose membership is defined by the presence or absence of certain subsequences , which may or may not be contiguous . For example , ad is a subsequence of abcde , since both letters of ad occur in abcde , in the same order . Based on these ideas , we define the SP task as follows . Task 3 ( SP Task ) . Given x { a , b , c , d } * , determine whether or not x contains at least one of the following subsequences : ab , bc , cd , dc . Example 4 . In the SP task , aab is classified as True , since it contains the subsequence ab . Similarly , acb is classified as True , since it contains ab non - contiguously . The string aaa is classified as False . The choice of SP languages as a test for longdistance dependencies is motivated by the fact that symbols in a non - contiguous subsequence may occur arbitrarily far from one another . The SP task yields a variant of the pointing game task in the sense that the input string may or may not contain an \" object \" ( one of the four subsequences ) that the network must identify . Therefore , we expect an input symbol to receive a nonzero attribution score if and only if it comprises a subsequence .", "entities": []}
{"text": "To evaluate attribution methods under our framework , we begin with a qualitative description of the heatmaps that are computed for our whitebox networks , based on the illustrative sample of heatmaps appearing in Table 3 .", "entities": []}
{"text": "The heatmaps for the PDA - based network also differ strikingly from those of the other networks , in that the gradient - based methods never assign nonzero scores . This is because equation ( 1 ) causes g ( t ) to be highly saturated , resulting in zero gradients . In the case of LRP , the matching bracket is highlighted when c \u0338 = None . When the matching bracket is not the last symbol of the input , the other unclosed brackets are also highlighted , with progressively smaller magnitudes , and with brackets of the opposite type from c receiving negative scores . This pattern reflects the mechanism of ( 1 ) , in which progressively larger powers of 2 are used to determine the content copied to c ( t ) k . When the relevance output class is c = None , LRP assigns opening brackets a negative score , revealing the fact that those input symbols set the bit c ( t ) 2k+1 to indicate that the stack is not empty . Although occlusion sometimes highlights the matching bracket , it does not appear to be consistent in doing so . For example , it fails to highlight the matching bracket ( [ [ ( [ ( [ [ ( [ ( [ [ ( [ ( [ [ ( [ 22 ) ) ( [ [ ( [ ] ( [ [ ( [ ] ( [ [ ( [ ] ( [ [ ( [ ] ( [ [ ( [ ] 23 None None ( [ [ ] ] ) ( [ [ ] ] ) ( [ [ ] ] ) ( [ [ ] ] ) ( [ [ ] ] ) 24 ] ] [ ( [ ] [ ( ) [ ( [ ] [ ( ) [ ( [ ] [ ( ) [ ( [ ] [ ( ) [ ( [ ] [ ( ) 25 ) ] [ ( [ ] [ ( ) [ ( [ ] [ ( ) [ ( [ ] [ ( ) [ ( [ ] [ ( ) [ ( [ ] [ ( )", "entities": []}
{"text": "We now turn to focused investigations of particular phenomena that attribution methods exhibit when applied to white - box networks . Subsection 7.1 begins by discussing the effect of network saturation on the gradient - based methods and LRP . In Subsection 7.2 we apply Bach et al 's ( 2015 ) ablation test to our attribution methods for the SP task .", "entities": []}
{"text": "So far , we have primarily compared attribution methods via visual inspection of individual examples . To compare the five methods quantitatively , we apply the ablation test of Bach et al ( 2015 ) to our two white - box networks for the SP task . 5 Given an input string classified as True , we iteratively remove the symbol with the highest relevance score , recomputing heatmaps at each iteration , until the string no longer contains any of the four subsequences . We apply the ablation test to 100 randomly generated input strings , and report the average percentage of each string that is ablated in Table 6 . A peculiar property of the SP task is that removing a symbol preserves the validity of input strings . This means that , unlike in NLP settings , our ablation test does not suffer from the issue that ablation produces invalid inputs . Saliency , G \u00d7 I , and LRP perform close to the random baseline on the FSA network ; this is unsurprising , since these methods only assign nonzero scores to the last input symbol . While Table 3 shows some variation in the IG heatmaps , IG also performs close to the random baseline . Only occlusion performs considerably better , since it is able to identify symbols whose ablation would destroy subsequences . On the counter - based SP network , IG performs remarkably close to the optimal benchmark , which represents the best possible performance on this task . Occlusion , G \u00d7 I , and LRP achieve a similar level of performance to one another , while saliency performs worse than the random baseline .", "entities": []}
{"text": "Of all the heatmaps considered in this paper , only those computed by G \u00d7 I and IG for the counting task fully matched our expectations . In other cases , all attribution methods fail to identify at least some of the input features that should be considered relevant , or assign relevance to input features that do not affect the model 's behavior . Among the five methods , saliency achieves the worst performance : it never assigns nonzero scores for the counting and bracket prediction tasks , and it does not identify the relevant symbols for either of the two SP networks . Saliency also achieves the worst performance on the ablation test for both the counterbased and the FSA - based SP networks . Among the four white - box networks , the two automatabased networks proved to be much more challenging for the attribution methods than the counterbased networks . While the LRP heatmaps for the PDA network correctly identify the matching bracket when available , no other method produces reasonable heatmaps for the PDA network , and all five methods fail to interpret the FSA network . Taken together , our results suggest that attribution heatmaps should be viewed with skepticism . This paper has identified cases in which heatmaps fail to highlight relevant features , as well as cases in which heatmaps incorrectly highlight irrelevant features . Although most of the methods perform better for the counter - based networks than the automaton - based networks , in practical settings we do not know what kinds of computations are implemented by a trained network , making it impossible to determine whether the network under analysis is compatible with the attribution method being used . In future work , we encourage the use of our four white - box models as qualitative benchmarks for evaluating interpretability methods . For example , the style of evaluation we have developed can be replicated for attribution methods not covered in this paper , including DeepLIFT ( Shrikumar et al , 2017 ) and contextual decomposition ( Murdoch et al , 2018 ) . We believe that insights gleaned from white - box analysis can help researchers choose between different attribution methods and identify areas of improvement in current techniques .", "entities": []}
{"text": "This appendix provides detailed descriptions of our four white - box networks .", "entities": []}
{"text": "I would like to thank Dana Angluin and Robert Frank for their advice and mentorship on this project . I would also like to thank Yoav Goldberg , John Lafferty , Tal Linzen , R. Thomas Mc - Coy , Aaron Mueller , Karl Mulligan , Shauli Ravfogel , Jason Shaw , and the reviewers for their helpful feedback and discussion .", "entities": []}
{"text": "EDTC : A Corpus for Discourse - Level Topic Chain Parsing", "entities": []}
{"text": "Discourse analysis has long been known to be fundamental in natural language processing . In this research , we present our insight on discourse - level topic chain ( DTC ) parsing which aims at discovering new topics and investigating how these topics evolve over time within an article . To address the lack of data , we contribute a new discourse corpus with DTC - style dependency graphs annotated upon news articles . In particular , we ensure the high reliability of the corpus by utilizing a two - step annotation strategy to build the data and filtering out the annotations with low confidence scores . Based on the annotated corpus , we introduce a simple yet robust system for automatic discourse - level topic chain parsing .", "entities": []}
{"text": "[ u1 ] ALBERTA ENERGY Co. , Calgary , said it filed a preliminary prospectus for an offering of common shares . [ u2 ] The natural resources development concern said proceeds will be used to repay long - term debt , which stood at 598 million Canadian dollars ( US$ 510.6 million ) at the end of 1988 . [ u3 ] The company plans to raise between C$ 75 million and C$ 100 million from the offering , according to a spokeswoman at Richardson Greenshields of Canada Ltd. , lead underwriter . [ u4 ] The shares will be priced in early November , she said . wsj_1183", "entities": []}
{"text": "The authors would like to thank Yuqing Xing , Jialong Xie , and the other annotators for their valuable discussion and advice on this research . This work was supported by the National Key R&D Program of China under Grant No . 2020AAA0108600 , Projects 61876118 and 61976146 under the National Natural Science Foundation of China and the Priority Academic Program Development of Jiangsu Higher Education Institutions .", "entities": []}
{"text": "Referring to our system outputs , we find that the automatically parsed DTC structures are highly consistent with human annotations . Here , we present some automatic DTC structures constructed by the Bert - large - based system for reference .", "entities": []}
{"text": "Moody 's Investors Service said it reduced its rating on $ 165 million of subordinated debt of this Beverly Hills , Calif. , thrift , citing turmoil in the market for low - grade , high - yield securities . [ u2 ] The agency said it reduced its rating on the thrift 's subordinated debt to B - 2 from Ba - 2 and will keep the debt under review for possible further downgrade . [ u3 ] Columbia Savings is a major holder of so - called junk bonds . [ u4 ] New federal legislation requires that all thrifts divest themselves of such speculative securities over a period of years . [ u5 ] Columbia Savings officials were n't available for comment on the downgrade . [ u6 ] FRANKLIN SAVINGS ASSOCIA - TION ( Ottawa , Kan. ) - Moody 's Investors Service", "entities": []}
{"text": "We apply back - translation ( Sennrich et al , 2016b ) method to use monolingual data . For English - German and Chinese - English translation , we sample monolingual data from the NewsCrawl2016 corpora . For English - Chinese translation , we sample monolingual data from the XinhuaNet2011 corpus .", "entities": []}
{"text": "For Chinese - English translation , we also use a target - bidirectional model ( Liu et al , 2016 ; Sennrich et al , 2016a ) to rescore the hypotheses . To train a target - bidirectional model , we reverse the target side of bilingual pairs from left - to - right ( L2R ) to right - to - left ( R2L ) . We first output 50 candidates from the ensemble of 4 L2R models . Then we rescore candidates by interpolating L2R score and R2L score with uniform weights . 3 https://github.com/rsennrich/ subword - nmt", "entities": []}
{"text": "Table 4 shows the ranking of our submitted systems at the WMT17 shared news translation task . Our submissions are ranked ( tied ) first for 2 out of 3 translation directions in which we participated : EN\u2194ZH .", "entities": []}
{"text": "Or bends with the remover to remove .", "entities": []}
{"text": "We can assign any format and rhyming symbols C to control the generation . Given C , we will obtain P and S automatically . And the model can conduct generation starting from the special token bos iteratively until meet the ending marker eos . Both beam - search algorithm ( Koehn , 2004 ) and truncated top - k sampling ( Fan et al , 2018 ; Radford et al , 2019 ) method are utilized to conduct the decoding . 4 Experimental Setup", "entities": []}
{"text": "We conduct ablation study on corpus SongCi and the experimental results are depicted in Table 4 . It should note that all the models are purely trained on SongCi corpus without any pre - training stages . From the results we can conclude that the introduced symbols C , P , and S indeed play crucial roles in improving the overall performance especially on the metrics of format , rhyme , and sentence integrity . Even though some of the components can not improve the performance simultaneously on all the metrics , the combination of them can obtain the best performance .", "entities": []}
{"text": "A Report on the 2020 VUA and TOEFL Metaphor Detection Shared Task", "entities": []}
{"text": "Metaphor use in everyday language is a way to relate our physical and familiar social experiences to a multitude of other subjects and contexts ( Lakoff and Johnson , 2008 ) ; it is a fundamental way to structure our understanding of the world even without our conscious realization of its presence as we speak and write . It highlights the unknown using the known , explains the complex using the simple , and helps us to emphasize the relevant aspects of meaning resulting in effective communication . Metaphor has been studied in the context of political communication , marketing , mental health , teaching , assessment of English proficiency , among others Gutierrez et al , 2017 ; Littlemore et al , 2013 ; Thibodeau and Boroditsky , 2011 ; Kaviani and Hamedi , 2011 ; Kathpalia and Carmel , 2011 ; Landau et al , 2009 ; Beigman Klebanov et al , 2008 ; Zaltman and Zaltman , 2008 ; Littlemore and Low , 2006 ; Cameron , 2003 ; Lakoff , 2010 ; Billow et al , 1997 ; Bosman , 1987 ) ; see chapter 7 in Veale et al ( 2016 ) for a recent review . We report on the second shared task on automatic metaphor detection , following up on the first shared task held in 2018 . We present the shared task and provide a brief description of each of the participating systems , a comparative evaluation of the systems , and our observations about trends in designs and performance of the systems that participated in the shared task .", "entities": []}
{"text": "We use the VU Amsterdam Metaphor Corpus ( VUA ) ( Steen et al , 2010 ) . The dataset consists of 117 fragments sampled across four genres from the British National Corpus : Academic , News , Conversation , and Fiction . The data is annotated using the MIPVU procedure with a strong interannotator reliability of \u03ba > 0.8 ( Steen et al , 2010 ) . The VUA dataset and annotations is the same as the one used in the first shared task on metaphor detection , where the reader is referred for further details .", "entities": []}
{"text": "This data labeled for metaphor was sampled from the publicly available ETS Corpus of Non - Native Written English 1 and was first introduced by ( Beigman . The annotated data comprises essay responses to eight persuasive / argumentative prompts , for three native languages of the writer ( Japanese , Italian , Arabic ) , and for two proficiency levels - medium and high . The data was annotated using the protocol in Beigman Klebanov and Flor ( 2013 ) , that emphasized argumentation - relevant metaphors : \" Argumentation - relevant metaphors are , briefly , those that help the author advance her argument . For example , if you are arguing against some action because it would drain resources , drain 1 https://catalog.ldc.upenn.edu/LDC2014T06 is a metaphor that helps you advance your argument , because it presents the expenditure in a very negative way , suggesting that resources would disappear very quickly and without control . \" Beigman Klebanov and Flor ( 2013 ) Average inter - annotator agreement was \u03ba = 0.56 - 0.62 , for multiple passes of the annotation ( see for more details ) . We use the data partition from Beigman , with 180 essays as training data and 60 essays as testing data . Tables 1 and 2 show some descriptive characteristics of the data : the number of texts , sentences , tokens , and class distribution information for Verbs and AllPOS tracks for the two datasets . To facilitate the use of the datasets and evaluation scripts beyond this shared task in future research , the complete set of task instructions and scripts are published on Github 2 . We also provide a set of features used to construct one of the baseline classification models for prediction of metaphor / non - metaphor classes at the word level , and instructions on how to replicate that baseline .", "entities": []}
{"text": "In this first phase , data is released for training and/or development of metaphor detection models . Participants can elect to perform crossvalidation on the training data , or partition the training data further to have a held - out set for preliminary evaluations , and/or set apart a subset of the data for development / tuning of hyperparameters . However the training data is used , the goal is to have N final systems ( or versions of a system ) ready for evaluation when the test data is released .", "entities": []}
{"text": "We first describe the baseline systems . Next , we briefly describe the general approach taken by every team . Interested readers can refer to the teams ' papers for more details .", "entities": []}
{"text": "Table 4 present the results for All POS and Verbs tracks for VUA data . Table 5 present the results for All POS and Verbs tracks for TOEFL data .", "entities": []}
{"text": "As organizers of the shared task , we would like to thank all the teams for their interest and participation . We would also like to thank Ton Veale , Eyal Sagi , Debanjan Ghosh , Xinhao Wang , and Keelan Evanini for their helpful comments on the paper , and Verna Dankers for pointing out an error in the original paper that has since been fixed .", "entities": []}
{"text": "We now consider the second subtask of INTSUMM : generating lists of suggested queries . The list is regenerated after every interaction , to yield queries that focus on sub - topics that were not yet explored . Reusing the notations of M Summ in 3 , we define a model , M Sugg , for suggested queries list generation , that receives an input tuple ( D , E in , m ) ( notice that a query is not needed here ) . Here , the jth phrase in D is denoted \u03c1 j , when the documents in D are concatenated , and accordingly , history E in is a list of phrases extracted from the session 's current accumulated summary . m is the number of suggested queries to output . The model outputs phrase sequence E out = { e out 1 , e out 2 , ... , e out m } from D , accounting for history E in . As in M Summ 's setting , D is paired with a set of generic reference summaries R.", "entities": []}
{"text": "We ran several experiments for the assessment of our M Summ and M Sugg models , applying the INTSUMM evaluation framework of Shapira et al ( 2021b ) . The goals of the experiments are to compare varying configurations of our models and to evaluate against an INTSUMM baseline system . The experiments include both simulations and interactive sessions with human users .", "entities": []}
{"text": "The M Summ model architecture ( 3.2 ) has several configurable components : encoding the query into sentences , considering the query in the MMR function ( both at train and inference time ) , and the dual reward mechanism . We compared several variations of these using simulations , presented in 5.2 . In addition , we compare , both via simulations ( 5.2 ) and real sessions ( 5.3 ) , against the ( betterperforming ) baseline system in ( Shapira et al , 2021b ) , named S 2 . S 2 's initial summary algorithm is TextRank , and the query - response generator extracts sentences via lexical+semantic similarity to the query , somewhat resembling QSIM in Equation 10 , fully neglecting the summary - so - far , in contrast to M Summ . S 2 's suggested queries list contains TextRank 's top salient topic phrases . Since these too do not account for the summary - so - far , they are computed at the session beginning and are not updated along the session , in contrast to M Sugg .", "entities": []}
{"text": "We analyzed the types of queries users submitted throughout their sessions , to assess the utility of updating suggested queries , with M Sugg , as opposed to a static list of suggestions , with S 2 . To that end , we tallied suggested query clicks and query submissions via other modes , binning the tallies to three sequential temporal segments within their respective sessions ( Appendix E.3 ) . We found that , on average , the usage of suggested query clicks increased by~13 % when nearing the end of a session with M Sugg , and conversely decreased by~24 % with S 2 . While the decrease in use of the static list is expected , since appealing queries are likely exhausted earlier in a session , it is encouraging to witness the usefulness of updated queries as the session progresses . This behavior suggests that the updated list contains suggested queries that are indeed engaging for learning more about the topic .", "entities": []}
{"text": "Systems that are made for interacting with humans must respond quickly in order to keep the user 's engagement . The exact amount of time does not affect the user experience as long as it does not surpass some limit , after which the user starts losing interest or feeling irritated ( Attig et al , 2017 ; Anderson , 2020 ) . As mentioned in Appendix D , M Summ generates summaries in under a second and M Sugg prepares the list in a few seconds . The baseline summarizer also responds in under a second . The difference between the systems is virtually unperceivable during interaction . There were no comments from the users in our experiments that stated any issue with execution time . Figure 3 : Averaged recall curves of our system and the S 2 baseline system in the experiment described in 5.3 and Table 2 ( using M Summ configuration v from Table 1 ) . The intersecting range is bounded by dashed lines ( between 106 and 250 tokens ) .", "entities": []}
{"text": "In this analysis , we assessed what modes of query submission users relied on over the course of a session . To that end , ( 1 ) we divided each session to three segments ( first , second and third part of the session ) , and counted the types of queries . The types are \" suggested query \" , \" free - text \" , \" highlight \" ( a span from the summary text ) and \" repeat \" ( repeating the last submitted query ) . ( 2 ) We then computed the percentage of each mode in each segment . ( 3 ) The percentages over all sessions and all topics were computed for each of the three segments . This process was conducted only for sessions between 4 and 20 interactions , as the few long and short sessions often show different behavior . For the first experiment , this left 43 sessions with avg . 8.63 ( std . 2.32 ) interactions for our system , and 50 sessions with 8.44 ( 2.48 ) interaction for S 2 . For the second experiment , it left 72 sessions with 10.24 ( 4.82 ) interactions for our system , and 74 sessions with 9.59 ( 4.42 ) interactions for S 2 . We focus here on the use of suggested queries versus all other query types . In the first experiment we observe a change of +9 % from the first to the third segment in our system , and - 20 % in S 2 . In the second experiment we see +18 % and - 28 % in S 2 . As discussed in 5.3 , this suggests the effectiveness of updated suggested queries , especially by the end of a session . Figure 4 : Averaged recall curves of our system and the S 2 baseline system in the experiment described here in Appendix E.1 and Table 3 ( using M Summ configuration i from Table 1 ) . The intersecting range is bounded by dashed lines ( between 106 and 250 tokens ) .", "entities": []}
{"text": "We show in Figure 5 an example of an INTSUMM system using the web application of Shapira et al ( 2021b ) and our our M Summ ( configuration i from Table 1 ) and M Sugg models in the backend . shows the result of clicking the \" carbon dioxide gas \" suggested query ( with the query response and updated suggested queries list ) . Sub - figure ( c ) shows the result of subsequently submitting the query \" water level \" . Query responses should be informative for the general topic , while also complying to the user queries . System summaries and expansions must be output fast in order to allow smooth interaction and human engagement .", "entities": []}
{"text": "We thank the anonymous reviewers for their constructive comments and suggestions . This work was supported in part by Intel Labs ; by the Israel Science Foundation ( grants no . 2827/21 and 2015/21 ) ; by a grant from the Israel Ministry of Science and Technology ; by the NSF - CAREER Award # 1846185 ; and by a Microsoft PhD Fellowship .", "entities": []}
{"text": "Interpreting the Robustness of Neural NLP Models to Textual Perturbations", "entities": []}
{"text": "Natural noise ( simulated by perturbations in this work ) usually co - occurs with latent features in an example . If we did not assign random labels and simply perturbed one of the original groups , there would be confounding latent features that would prevent us from estimating the causal effect of the perturbation . Figure 1a illustrates this scenario . Both perturbation P and latent feature T may affect the outcome Y , 2 while the latent feature is predictive of label L. Since we make the perturbation P on examples with the same label , P is decided by L. It therefore follows that T is a confounder of the effect of P on Y , resulting in non - causal association flowing along the path P L T Y . However , if we do randomize the labels , P no longer has any causal parents ( i.e. , incoming edges ) ( Figure 1b ) . This is because perturbation is purely 2 Y is later defined in Section 3.2 the language of causality , this is \" correlation is not causation \" . Causality provides insight on how to fully decouple the effect of perturbation and other latent features . We introduce the causal motivations for step 1 and 3 of learnability estimation in the following Section 3.1 and 3.2 respectively .", "entities": []}
{"text": "Natural noise ( simulated by perturbations in this work ) usually co - occurs with latent features in an example . If we did not assign random labels and simply perturbed one of the original groups , there would be confounding latent features that would prevent us from estimating the causal effect of the perturbation . Figure 4a illustrates this scenario . Both perturbation P and latent feature T may affect the outcome Y , 3 while the latent feature is predictive of label L. Since we make perturbation P on examples with the same label , P is decided by L. It therefore follows that T is a confounder of the effect of P on Y , resulting in non - causal association flowing along the path P L T Y . However , if we do randomize the labels , P no longer has any causal parents ( i.e. , incoming edges ) ( Figure 4b ) . This is because perturbation is purely random . Without the path represented by P L , all of the association that flows from P to Y is causal . As a result , we can directly calculate the causal effect from the observed outcomes ( Section 3.2 ) . Our randomization experiments allow us to dis - of the association that flows from P to Y is causal .", "entities": []}
{"text": "As a result , we can directly calculate the causal 315 effect from the observed outcomes ( Section 3.2 ) .", "entities": []}
{"text": "Our randomization experiments allow us to dis - Figure 1 : Causal graph explanation for decoupling perturbation and latent feature with randomization . P is the perturbation and T is the latent feature . L is the original label and Y is the correctness of the predicted label . random . Without the path represented by P L , all of the association that flows from P to Y is causal . As a result , we can directly calculate the causal effect from the observed outcomes .", "entities": []}
{"text": "Criteria for Perturbations . We select various character - level and word - level perturbation methods in existing literature that simulate different types of noise an NLP model may encounter in real - world situations . These perturbations are nonadversarial , label - consistent , and can be automatically generated at scale . We note that our perturbations do not require access to the model internal structure . We also assume that the feature of perturbation does not exist in the original data . Not all perturbations in the existing literature are suitable for our task . For example , a perturbation that swaps gender words ( i.e. , female male , male female ) is not suitable for our experiments since we can not distinguish the perturbed text from an unperturbed one . In other words , the perturbation function g ( \u22c5 ) should be asymmetric , such that g ( g ( x ) ) \u2260 x. Figure 2 shows an example sentence with different perturbations . Perturbation of \" dupli - cate_punctuation \" doubles the punctuation by appending a duplicate after each punctuation , e.g. , \" , \" \" \" \" ; \" butter_fingers_perturbation \" misspells some words with noise erupting from keyboard typos ; \" shuffle_word \" randomly changes the order of word in the text ( Moradi and Samwald , 2021 ) ; \" random_upper_transformation \" randomly adds upper cased letters ( Wei and Zou , 2019 ) ; \" in - sert_abbreviation \" implements a rule system that encodes word sequences associated with the replaced abbreviations ; \" whitespace_perturbation \" randomly removes or adds whitespaces to text ; \" vi - sual_attack_letters \" replaces letters with visually similar , but different , letters ( Eger et al , 2019 ) ; \" leet_letters \" replaces letters with leet , a common encoding used in gaming ( Eger et al , 2019 ) .", "entities": []}
{"text": "This work targets at an open question in NLP : why models are less robust to some textual perturbations than others ? We find that learnability , which causally quantifies how well a model learns to identify a perturbation , is predictive of the model robustness to the perturbation . In future work , we will investigate whether these findings can generalize to other domains , including computer vision .", "entities": []}
{"text": "Computing average learnability requires training a model for multiple times at different perturbation probabilities , which can be computationally intensive if the sizes of the datasets and models are large . This can be a non - trivial problem for NLP practitioners with limited computational resources . We hope that our benchmark results of typical perturbations for NLP models work as a reference for potential users . Collaboratively sharing the results of such metrics on popular models and perturbations in public fora can also help reduce duplicate investigation and coordinate efforts across teams . To alleviate the computational efficiency issue of average learnability estimation , using learnability at selected perturbation probabilities may help at the cost of reduced precision ( Appendix D ) . We are not alone in facing this issue : two similar metrics for interpreting model inductive bias , extractability and s - only error ) also require training the model repeatedly over the whole dataset . Therefore , finding an efficient proxy for average learnability is promising for more practical use of learnability in model interpretation .", "entities": []}
{"text": "This research is supported by the National Research Foundation , Singapore under its International Research Centres in Singapore Funding Initiative . Any opinions , findings and conclusions or recommendations expressed in this material are those of the author ( s ) and do not reflect the views of National Research Foundation , Singapore . We acknowledge the support of NVIDIA Corporation for their donation of the GeForce RTX 3090 GPU that facilitated this research .", "entities": []}
{"text": "In order to enable the model to learn from explicit hard negatives , we construct three diverse types of graphs - synthetically constructed structural negatives for learning graph constraints and synthetic and human - created semantic negatives to capture a fairly large space of semantically incorrect graphs . Below we discuss the construction of these graphs .", "entities": []}
{"text": "As shown previously , one common source of errors in the generated explanation graphs is the violation of structural constraints . To enable learning these constraints , we generate four types of negative graphs by performing the following perturbations on each ground - truth graph : ( 1 ) removing an edge at random such that the resultant graph becomes disconnected , ( 2 ) adding an edge between two randomly chosen nodes such that the resultant graph becomes cyclic , ( 3 ) adding and removing one edge at random such that the resultant graph becomes both disconnected and cyclic , ( 4 ) removing a node randomly such that the resultant graph contains less than two concepts from the belief or argument . Fig . 2 shows an example of a disconnected graph created as part of the structurally negative graphs .", "entities": []}
{"text": "Generate & Refine model ( Sec . 5.3 ) improves all metrics ; however the gains are small . Note that this model refines all graphs ( correct or not ) and can lead to already correct graphs becoming incorrect after refinement . In practice , we observe that most graphs do not change much after refinement which we believe stems from the model 's inability to distinguish between correct and incorrect graphs . graphs . This can potentially be improved by incorporating more structurally diverse graphs . Finally , our best SeCA is far from perfect and significant future work can be done in improving the graph semantics . Further ablations of negative graphs and human evaluation are done on the Max - Margin model , due to its slightly higher SeCA .", "entities": []}
{"text": "We create a total of 11k negative graphs . Table 6 shows the respective counts of the negative graphs belonging to synthetic structural ( SySt ) , synthetic semantic ( SySe ) and human - created semantic ( HuSe ) categories .", "entities": []}
{"text": "In Fig . 5 , we show the interface for human verification of commonsense explanation graphs on Amazon Mechanical Turk . We select crowdworkers who are located in the US with a HIT approval rate higher than 96 % and at least 1000 HITs approved . Since graph evaluation is a challenging task , we first explain how to read the graphs and also provide clear guidelines for comparing the quality of the two graphs . 9", "entities": []}
{"text": "We use all publicly available literature from PubMed and PubMed Central Open Access subset , which cover most of the relevant literature and are commonly used as the prime source of data in biomedical text mining knowledge bases . PubMed provides titles and abstracts in XML format in a collection of baseline release and subsequent updates . The former is available at the end of each year whereas the latter is updated daily . As this project was started during 2015 , we have first processed the baseline release from the end of 2014 and this data has then been extended with the new publications from the end of 2015 baseline release . The rest of the data up to date has been collected from the daily updates . The full articles in PMC Open Access subset ( PMCOA ) are retrieved via the PMC FTP service . Multiple types of data format are provided in PM - COA , including NXML and TXT formats which are suitable for text processing . We use the provided NXML format as it is compatible with our processing pipeline . This service does not provide distinct incremental updates , but a list of all indexed articles updated weekly .", "entities": []}
{"text": "We have introduced a new resource which provides the basic linguistic analyses , essential in the development of text mining knowledge bases , for the whole of PubMed and PubMed Central Open Access section , thus drastically reducing the amount of required preprocessing efforts . In addition , we provide named entity tagging for several biologically relevant entity types and show that the models we have used are comparable to the state - of - the - art approaches , although our focus has been on retaining the processing pipeline as simple as possible for easier maintenance . The resource is periodically updated with an automated pipeline , and currently includes over 26 M documents fully parsed with 526 M named entity mentions detected . The data is available for download in XML format .", "entities": []}
{"text": "Computational resources were provided by CSC - IT Center For Science Ltd. , Espoo , Finland . This work was supported by ATT Tieto k\u00e4ytt\u00f6\u00f6n grant .", "entities": []}
{"text": "Among the previous series ( 2011 , 2013 , 2016 ) of the BioNLP Shared Task , the Bacteria Biotope Task in 2013 is the first shared task that addressed the problem of normalization of the entities in the bacteria biotopes domain . In 2013 , the participant teams proposed rule - based methods and similarity - based methods . According to the official results of the Bacteria Biotope Task of 2013 , for the habitat mention normalization , the best precision was obtained by the BOUN system , which utilized syntactic rules and shallow linguistic knowledge ( Karadeniz and Ozg\u00fcr , 2013 ; Karadeniz and\u00d6zg\u00fcr , 2015 ) . In the following series of the Bacteria Biotopes task , the habitat mention normalization sub - task continued to attract the attention of the researchers . In the Bacteria Biotope task of the BioNLP Shared Task 2016 , the best precision for the habitat normalization task was obtained by the BOUN system , which utilized both approximate string matching and cosine similarity of word - vectors weighted with Term Frequency - Inverse Document Frequency ( TF - IDF ) ( Tiftikci et al , 2016 ) . After the Shared Tasks , the researchers continued to search for a solution for the problem of Bacteria Biotopes normalization ( Ferr\u00e9 et al , 2017 ; Mehryary et al , 2017 ; Karadeniz and\u00d6zg\u00fcr , 2019 ) . Although promising results have been obtained by these approaches , the results showed that there is still room for improvement for the normalization task of bacteria biotopes . Besides the bacteria biotopes , there exist a significant amount of prior work on biomedical named entity normalization for different types of biomedical entities including genes / proteins ( Morgan et al , 2008 ; Wermter et al , 2009 ; Lu et al , 2011 ; and diseases ( Leaman et al , 2013 ; Li et al , 2017 ) . However , the need for manually annotated training data makes the adaptation of such methods to new entities difficult .", "entities": []}
{"text": "( w * S S ( m head , c head ) ) + ( ( 1 - w ) * S S ( m , c ) ) ( 1 )", "entities": []}
{"text": "Similar to the extraction of localization relations , for the extraction of Exhibits relations , all the sentences are searched for whether there exist a Microorganism entity and a Phenotype entity . The same rules that are explained in the previous subsection are applied for the extraction of the Exhibits relations .", "entities": []}
{"text": "The official results obtained by our system and the other participants for the BB - rel task are demonstrated in Table 5 .", "entities": []}
{"text": "We would like to thank the BioNLP shared task organizers , especially , Robert Bossy for their help with the questions .", "entities": []}
{"text": "Our learner trace data comes from Duolingo : a free , award - winning , online language - learning platform . Since launching in 2012 , more than 200 million learners worldwide have enrolled in Duolingo 's game - like courses , either via the website 1 or mobile apps . Figure 1 ( a ) is a screen - shot of the home screen , which specifies the game - like curriculum . Each icon represents a skill , aimed at teaching thematically or grammatically grouped words or concepts . Learners can tap an icon to access lessons of new material , or to review material once all lessons are completed . Learners can also choose to get a personalized practice session that reviews previouslylearned material from anywhere in the course by tapping the \" practice weak skills \" button .", "entities": []}
{"text": "To create the SLA modeling corpus , we sampled from Duolingo users who registered for a course and reached at least the tenth row of skill icons within the month of November 2015 . By limiting the data to new users who reach this level of the course , we hope to better capture beginners ' broader language - learning process , including repeated interaction with vocabulary and grammar over time . Note that we excluded all learners who took a placement test to skip ahead in the course , since these learners are likely more advanced .", "entities": []}
{"text": "An important question for SLA modeling is : to what extent does an approach generalize across languages ? While the majority of Duolingo users learn English - which can significantly improve job prospects and quality of life ( Pinon and Haydon , 2010 ) - Spanish and French are the second and third most popular courses . To encourage researchers to explore language - agnostic features , or unified cross - lingual modeling approaches , we created three tracks : English learners ( who speak Spanish ) , Spanish learners ( who speak English ) , and French learners ( who speak English ) .", "entities": []}
{"text": "The goal of the task is as follows : given a history of token - level errors made by the learner in the learning language ( L2 ) , accurately predict the errors they will make in the future . In particular , we focus on three Duolingo exercise formats that require the learners to engage in active recall , that is , they must construct answers in the L2 through translation or transcription . Figure 1 ( b ) illustrates a reverse translate item , where learners are given a prompt in the language they know ( e.g. , their L1 or native language ) , and translate it into the L2 . Figure 1 ( c ) illustrates a reverse tap item , which is a simpler version of the same format : learners construct an answer using a bank of words and distractors . Figure 1 ( d ) is a listen item , where learners hear an utterance in the L2 they are learning , and must transcribe it . Duolingo does include many other exercise formats , but we focus on these three in the current work , since constructing L2 responses through translation or transcription is associated with deeper levels of processing , which in turn is more strongly associated with learning ( Craik and Tulving , 1975 ) . Since each exercise can have multiple correct answers ( due to synonyms , homophones , or ambiguities in tense , number , formality , etc . ) , Duolingo uses a finite - state machine to align the learner 's response to the most similar reference answer form a large set of acceptable responses , based on token string edit distance ( Levenshtein , 1966 ) . For example , Figure 1 ( b ) shows an example of corrective feedback based on such an alignment . Figure 2 shows how we use these alignments to generate labels for the SLA modeling task . In this case , an English ( from Spanish ) learner was asked to translate , \" \u00bf Cu\u00e1ndo puedo ayudar ? \" and wrote \" wen can help \" instead of \" When can I help ? \" This produces two errors ( a typo and a missing pronoun ) . We ignore capitalization , punctuation , and accents when matching tokens .", "entities": []}
{"text": "The data were released in two phases . In phase 1 ( 8 weeks ) , TRAIN and DEV partitions were released with labels , along with a baseline system and evaluation script , for system development . In phase 2 ( 10 days ) , the TEST partition was released without labels , and teams submitted predictions to CodaLab 3 for blind evaluation . To allow teams to compare different system parameters or features , they were allowed to submit up to 10 predictions total ( up to 2 per day ) during this phase . Table 1 reports summary statistics for each of the data partitions for all three tracks . We created TRAIN , DEV , and TEST partitions as follows . For each user , the first 80 % of their exercises were placed in the TRAIN set , the subsequent 10 % in DEV , and the final 10 % in TEST . Hence the three data partitions are sequential , and contain ordered observations for all users . Note that because the three data partitions are sequential , and the DEV set contains observations that are potentially valuable for making TEST set predictions , most teams opted to combine the TRAIN and DEV sets to train their systems in final phase 2 evaluations .", "entities": []}
{"text": "We would also like to get a sense of which features , if any , significantly affect system performance . Table 4 lists features provided with the SLA modeling data set , as well as several newlyengineered feature types that were employed by at least three teams ( note that the precise details may vary from team to team , but in our view aim to cap - 4 Interestingly , the only linear model to rank among the top 5 ( CECL ) relied on combinatorial feature conjunctionswhich effectively alter the decision surface to be non - linear with respect to the original features . The RNN hidden nodes and GBDT constituent trees from other top systems may in fact be learning to represent these same feature conjunctions .", "entities": []}
{"text": "The authors would like to acknowledge Bo\u017cena Paj\u0105k , Joseph Rollinson , and Hideki Shima for their help planning and co - organizing the shared task . Eleanor Avrunin and Natalie Glance made significant contributions to early versions of the SLA modeling data set , and Anastassia Loukina and Kristen K. Reyher provided helpful advice regarding mixed - effects modeling . Finally , we would like to thank the organizers of the NAACL - HLT 2018 Workshop on Innovative Use of NLP for Building Educational Applications ( BEA ) for providing a forum for this work .", "entities": []}
{"text": "English Spanish French by NYU ( +1.632 ) , whereas the top - performing team SanaLabs had a surprisingly lower weight ( +0.841 ) . This could be due to the fact that their system was itself an ensemble of an RNN and GBDT models , which were used ( in isolation ) by each of the other two teams . This seems to add further support for the effectiveness of combining these algorithms for the task .", "entities": []}
{"text": "Higher - order Comparisons of Sentence Encoder Representations", "entities": []}
{"text": "Representational Similarity Analysis ( RSA ) is a technique developed by neuroscientists for comparing activity patterns of different measurement modalities ( e.g. , fMRI , electrophysiology , behavior ) . As a framework , RSA has several advantages over existing approaches to interpretation of language encoders based on probing or diagnostic classification : namely , it does not require large training samples , is not prone to overfitting , and it enables a more transparent comparison between the representational geometries of different models and modalities . We demonstrate the utility of RSA by establishing a previously unknown correspondence between widely - employed pretrained language encoders and human processing difficulty via eye - tracking data , showcasing its potential in the interpretability toolbox for neural models .", "entities": []}
{"text": "We presented a framework for analyzing neural network representations ( RSA ) that allowed us to relate human sentence processing data with language encoder representations . In experiments conducted on two widely used encoders , our findings show that sentences which are difficult for humans to process have more divergent representations both intra - encoder and between different encoders . Furthermore , we lend modest support to the intuition that a model 's middle layers encode comparatively more syntax . Our framework offers insight that is complimentary to decoding or probing approaches , and is particularly useful to compare representations from across modalities .", "entities": []}
{"text": "We empirically analyse the OAAG model with dynamic fusion and CHIME model to answer the following research questions : RQ1 : Are the retrieved review snippets significantly utilized for generating the answers ? RQ2 : Is the model performing similarly for a heterogeneous group of questions ? RQ3 : Is the generative model biased towards more frequently occurring phrases ? RQ4 : Can ROUGE capture the correctness of generated answers ?", "entities": []}
{"text": "For answering RQ1 , at inference time , we replace these reviews with four sets of review snippets : ( i ) TrainA : We use BM25 to find the closest question to the test question in the train data , and we take the answer of it as the generated answer . ( ii ) Ran - domOD : We randomly choose the review snippets from any other product of that category except the product for which the question is asked . ( iii ) Ran - domID : We randomly select review snippets from the review sentences of that particular product . ( iv ) BM25QA : We retrieve the review snippets using the BM25 algorithm that uses the question and reference answer in the test dataset . OAAG uses the opinion along with the reviews . We also select the opinion of the corresponding review sentence while replacing the reviews . Both the models utilize the top 10 reviews for training and evaluation . Table 1 shows the result of this experiment . The TrainA does not utilize either of the models to generate the answer . It shows the answer from the most similar train question , and its performance is competitive with other methods , especially in Home . In both the categories , the performance of both the models is almost similar in RandomOD and RandomID . RandomID shows marginally better performance than RandomOD for OAAG . For CHIME , BM25Q performs the best in both categories . For OAAG , BM25QA performs the best in Home while in Sports , BM25QA performs the best in R1 , and BM25Q performs the best in RL , but the difference is minute . The results are quite surprising : the performance of the models is very similar when the answers are generated with random reviews vs. when the answers are generated with the reviews obtained from BM25 . Hence , it is not clear if the model is effectively utilizing the retrieved review snippets .", "entities": []}
{"text": "We observe that some phrases are frequently occurring in the reference answers as well as in the generated answers . We find that in the training data of both categories , \u223c2.4 % of the reference answers start with the phrase \" I do n't think so \" , but 12.29 % of responses in Sports and 35.64 % responses in Home begin with this phrase . This \u223c2.4 % repetition of the same phrase in the training data makes the generative model biased towards this phrase . Many of the reference answers in the training data contain \" I do n't know \" , \" I have no idea \" , \" I ca n't say \" . These kinds of answers do not give any meaningful information to the user . Together , we denote these phrases as IDK . On analysis of the dataset , we find that in Sports , there are 3.04 % , 2.9 % , and 6.9 % IDK phrases in train dataset answers , test dataset answers , and generated answers , respectively . In Home , the answers in the train and test dataset contain 3.64 % and 3.60 % IDK phrases , respectively , but 16.31 % of the answers are generated as IDK phrases . So , in the generated answers , the appearance of IDK phrases has increased by three to five folds which clearly shows that the model is biased towards these frequent phrases . To see the effect of these phrases on the models , we remove the questions from the training dataset which have IDK in their reference answer 4 and retrain the models . We denote this model as BM25Q - IDK . Table 3 shows the result of BM25Q , the model trained on the original training data , and BM25Q - IDK . Home had 16.31 % and Sports had only 6.9 % IDK phrases , and consequently , when the IDK phrases are removed , it has more impact on Home which drops the bias towards these phrases and improves the ROUGE score , whereas , in Sports , BM25Q and BM25Q - IDK have close ROUGE scores due to lesser IDK phrases in the generated answer .", "entities": []}
{"text": "For answering RQ4 , we look into the generated response with high R1 scores and check their correct - Question is this box made of polypropylene ? can photos and mementos be safely stored in it ? Ref . Ans . i ca n't comment on the material it is made of , but the top does not stay on tight . it is not a decent storage container for anything requiring a top or to be airtight . OAAG . it says \" made in china \" on the bottom of the box . it says \" made in china \" on the bottom of the box . hope that helps . ( 26.22 ) CHIME . yes , it is polypropylene and can be stored in the box if you want to store them in a safe environment that is not toxic to you and/or your pets or food they will be exposed ( 27.77 )", "entities": []}
{"text": "What is the width at the base ? Ref . Ans . Width across the bottom / base is approximately 3 inches . OAAG . The width of the top of the base is about 1.5 inches . the base of the pitcher is 9 inches . the top of the pitcher is about 1.5 inche . ' ( 25.00 ) CHIME . it\u015b about 12 \" wide at the base and about 10.5 inches deep ( 26.08 ) ness with respect to the reference answer . In OAAG model , 15.36 % predictions in Home and 13.34 % predictions in Sports have R1 score above 20 . We manually analyse the reference and generated answers of randomly chosen 100 question - answers with a high ROUGE score ( > 20 ) , and we find that 54 % are answered incorrectly . In CHIME model , 46.87 % predictions in Home and 46.15 % predictions in Sports have R1 score above 20 and 56 % of 100 randomly chosen question - answer pairs ( whose ROUGE score > 20 ) turn out to be incorrect . Table 4 shows two examples where the generated responses result in high R1 scores , but the answers are incorrect . Both models predict irrelevant answers in the first question , and the predicted dimension is incorrect in the second question . It shows that it is not possible to infer from ROUGE scores if the generated answer is accurate to the reference answer , i.e. , the word count overlap is not an indicator of an accurate answer . We show some more cases with high R1 scores in Tables A.2 and A.3 in the Appendix .", "entities": []}
{"text": "Similarly , in the case of Table A.3 , the second and the fifth examples have high R1 scores , but the generated answers are exactly opposite of the reference answers . In the first question , the model wrongly predicts that it would melt and bubble up a little in the microwave , and in the third question , it predicts an entirely different answer . The response to the fourth question is \" I do n't know , \" which is a frequently occurring phrase .", "entities": []}
{"text": "it 's about 3 oz . and it 's just as strong as any other clip i ' ve seen OAAG : it 's about 7.5 \u2032\u2032 long . it 's a great knife . it 's a great deal . it 's a great knife and i love it . it 's a great deal . CHIME : i 'm not sure about the exact weight but it is very light and is very easy to use with ease . Question : does this fit a 2 year old ? Ref . Ans . : it fit my two year old who has a pretty normal size head . the multiple thicknesses of pads that are included really make it a great long term helmet ! OAAG : yes it will , it 's a very tight fit . i do n't think it would be too big for a 2 \u2032\u2032 2 \u2032\u2032 . it 's a great helmet .", "entities": []}
{"text": "i bought this for my son and he loves it so much he bought another one for his 2 year - bean .", "entities": []}
{"text": "We present some examples of numerical questions with their answers in Table A.4 . In the first example , the generated answer is right , but none of the answers are correct for the rest of the questions .", "entities": []}
{"text": "The AFRL WMT19 Systems : Old Favorites and New Tricks", "entities": []}
{"text": "Six Attributes of Unhealthy Conversations", "entities": []}
{"text": "We present a new dataset of approximately 44000 comments labeled by crowdworkers . Each comment is labelled as either ' healthy ' or ' unhealthy ' , in addition to binary labels for the presence of six potentially ' unhealthy ' sub - attributes : ( 1 ) hostile ; ( 2 ) antagonistic , insulting , provocative or trolling ; ( 3 ) dismissive ; ( 4 ) condescending or patronising ; ( 5 ) sarcastic ; and/or ( 6 ) an unfair generalisation . Each label also has an associated confidence score . We argue that there is a need for datasets which enable research based on a broad notion of ' unhealthy online conversation ' . We build this typology to encompass a substantial proportion of the individual comments which contribute to unhealthy online conversation . For some of these attributes , this is the first publicly available dataset of this scale . We explore the quality of the dataset , present some summary statistics and initial models to illustrate the utility of this data , and highlight limitations and directions for further research .", "entities": []}
{"text": "In this paper , we broadly characterise a healthy online public conversation as one where posts and comments are made in good faith , are not overly hostile or destructive , and generally invite engagement . Such a conversation may include robust engagement and debate , and is generally ( though not always ) focused on substance and ideas . Importantly , though , healthy contributions to online conversations are not necessarily friendly , grammatically correct , well constructed , intellectual , substantive , or even free of any vulgarity . Some harmful contributions to conversations are obviously derogatory , threatening , violent , or insulting ( Anderson et al , 2018 ) , and these are the sorts of comments which have been the primary focus of research in algorithmic moderation assistance and related areas . However , many of those comments which deter people from engagement or create downward spirals in interactions can be more subtle ( Zhang et al , 2018 ) . This is especially the case with conversations online , many of which ( i ) take place in a ' public ' forum that is visible to thousands of others , and ( ii ) involve strangers who have never met and know little about one another ( Santana , 2014 ) . These two features of online conversations can sometimes enhance commenters ' sensitivity to subtler forms of toxicity like sarcasm , condescension , or dismissiveness , amplifying their negative impact on conversations despite the fact that these attributes may be less ( or not at all ) harmful in other specific contexts . Identifying subtle indicators of problematic online comments is a difficult task . There are at least three reasons for this . First , they are less extreme and therefore less likely to use clearly identifiable explicit or inflammatory language . Second , a substantive point might be made in an inflammatory way , or a remark may be perceived differently depending on the context , norms , and expectations of the reader . Third , there is an even greater risk of identifying ' false positives ' and ' false negatives ' , since many of the expressions used in subtle forms of toxicity can also be deployed for positive contributions . For example , sarcasm is often used in derisive or bullying ways , but it can also be used for humour or to express a substantive , inoffensive point ( Vidgen et al , 2019 ) . The challenge is to identify the subtle characteristics of harmful comments online despite their ambiguity , without falsely identifying healthy comments . We differentiate between two categories . The first , which is the most well studied to date , are those whose explicit intention is to insult , threaten , or abuse . The second category , are comments which engage with others , share an opinion , or contribute to the conversation , but are written in a way which is likely to antagonise , hurt , or deter others . We found these comments to be at least as prevalent in the sample data ( Table 1 ) . Our typology of unhealthy attributes aims to include this second category of comments , and determine whether annotators believe they belong in a healthy online conversation . Our hypothesis was that together these 6 attributes account for the majority of ' unhealthy ' comments online , but that there will still be some comments that are ' unhealthy ' but do not display any sub - attribute , and also some which are ' healthy ' despite representing one or more sub - attributes ( see Figure 1 ) . In general , whether the presence of these attributes indicates healthy or unhealthy conversation will also depend significantly on the nature of the forum and users . Nonetheless , the combination of an abstract ' health ' rating with the other 6 attributes provides a useful dataset for investigating nuanced comments , and could be used to help develop a broader range of models that are customised for specific production environments .", "entities": []}
{"text": "One further challenge which comes with annotating more subtle unhealthy attributes is the potential to encode unintended societal biases and value judgements in models trained on this data . For example ,", "entities": []}
{"text": "In this job , you will be asked to read a comment and to express an overall opinion about whether you think it has a place in a healthy conversation online . You will also be asked to identify whether it displays a range of characteristics that may lead to unhealthy conversations . These characteristics include : sarcasm , gross generalisations , hostility , aggression , dismissiveness , condescension and patronization . All of the comments you will see are real comments posted by users in online conversations . Most of them will have been posted in response to one or more comments made by others ( which you are not given ) . However , the questions are designed in such a way that you should be able to answer them without seeing these other comments . The data collected here will be used to help build tools which promote healthier conversations online .", "entities": []}
{"text": "Please bear in mind that the questions do not ask whether you agree or disagree with the substance of each comment . Do your best to ignore your own opinion on the substantive idea or claim made in the comment when answering the questions . Please be sure to read the full text of the comment before answering the questions . Sometimes the part of a comment which displays one or more of the attributes you will be asked about , appears close to the end of the comment .", "entities": []}
{"text": "What are the characteristics of a healthy conversation ? Posts and comments are made in good faith Posts and comments are not overly hostile , and are not destructive The comments in the conversation generally invite engagement The conversation may include robust engagement and debate The conversation is generally focused on substance and ideas A healthy conversation does not necessarily require all posts and comments to be : Is this comment sarcastic ? 3 . Does this comment make a generalisation about a specific group of people ? 4 . If yes , would a member of that group feel that the generalisation is unfair ? 5 . Is this comment needlessly hostile ? 6 . Is the intention of this comment to insult , antagonize , provoke , or troll other users ? 7 . A comment with a condescending or patronising tone will generally assume an attitude of superiority , and imply that the other commenter ( s ) is ignorant , child - like , naive , or unintelligent . Such comments will usually imply that the other commenter should n't be taken seriously . Is this comment condescending and/or patronising ? 8 . A comment is dismissive if it rejects or ridicules another comment without good reason , or tries to push another commenter and their ideas out of the conversations . Note : A comment which expresses disagreement is not necessarily dismissive . Is this comment dismissive ?", "entities": []}
{"text": "The LAIX Systems in the BEA - 2019 GEC Shared Task", "entities": []}
{"text": "We have implemented the following GEC rules . ( 1 ) ' a ' and ' an ' substitution . For this problem , we made rules based on the first phoneme of the following word . ( 2 ) Comma deletion . After a prepositional phrase at the beginning of a sentence , we add a comma . For example , \" Despite our differences we collaborate well . \" A comma should be added after Despite our differences . ( 3 ) Orthography mistakes . We obtain statistics of named entities that require initial capitalization and make a white list using the Wikipedia corpus . If a word is on the white list , we will force the conversion to the initial capitalization form . In addition , we use Pyenchant as our spell checker ( Kelly , 2006 ) . The top candidate is considered to be the correction .", "entities": []}
{"text": "We use one conflict solver to combine the outputs from all of the systems in this task . Parameters for this ensemble system are shown in Table 7 . sults on the test set . We can see that the base systems are not very strong , and the ensemble system significantly improves the performance . The difference between the development set and test set can still be observed in this task .", "entities": []}
{"text": "Unsupervised Detection of Argumentative Units though Topic Modeling Techniques", "entities": []}
{"text": "Given a document corpus , topic modeling techniques can be employed to discover the most representative topics throughout the corpus , and to provide an assignment of documents to topics , meaning that the higher is the assignment value of a document to a certain topic , the higher is the probability that the document is \" focused \" on that topic . The idea of A2 T is that an argumentative unit is a sentence highly focused on a specific topic , namely a sentence with high assignment value to a certain topic and low assignment value to the other topics . To this end , A2 T introduces the notion of attraction with the aim at recognizing the sentences highly focused on specific topics , that represent the recognized argumentative units . In the following , the A2 T approach and related techniques are described in detail .", "entities": []}
{"text": "The schema of the A2 T approach is shown in Figure 1 . Consider a corpus of texts C = { c 1 , . . . , c n } , where a text c i C is a sequence of sentences , like for example an essay , a web page / post , or a scientific paper . The ultimate goal of the A2 T approach is to derive a set of argumentative units U = { s 1 , c , l , . . . , s h , c , l } , where", "entities": []}
{"text": "Units ( U )", "entities": []}
{"text": "As a second experiment , we exploited probabilities associated with sentences to perform a ranked evaluation . In particular , we calculated two measures , namely P that is the area the under the precision - recall curve and R that is the area under the receiver operating characteristic ( ROC ) curve . In this experiments , we used different criteria for defining the true labels : in P CM , an annotated sentence in the corpus is considered a true argumentative unit if it is either a premise , a claim , or a major claim ; in CM only claims and major claims are taken as valid au ; in M only major claims are taken into account . Results are reported in Table 1 .", "entities": []}
{"text": "A first evidence emerging from the analysis of confusion matrices for both corpora C1 and C2 is that the role of sentences is strictly dependent on the type of documents . C1 contains structured essays of various topics , while C2 provides conversational texts extracted from blogs and chats . In the first case , the number of argumentative units is higher than in the second one . In particular , for C2 we overestimated the probability of sentences to be an argumentative unit . This is mainly due to the fact that those sentences contain words that are semantically related to the main topic of the conversation although they are not playing a role in the argumentation . An example is the following sentence , taken from a document associated with the topic \" school \" : \" why do some parents not think their kids can attain ? \" . The sentence is clearly part of a conversation and it has been annotated as a non argumentative unit because it is a question . However , since it contains words that are relevant for the topic ( i.e. , parents , kids , attain ) , A2 T associates the sentence with a good level of attraction , labeling it as a premise . In order to address this kind of false positives , we aim in our future work to study the dependency relations among sentences in text ( such as questionanswers ) to the goal of achieving a better insight of the sentences role . A second lesson learned from error analysis concerns the distinction between claims and premises . This confusion is evident especially when dealing with corpus C1 . An example is given by the following two sentences , taken from an essay about the role of sports in favor of peace . ( s1 ) for example , when Irak was hardly struck by the second gulf war , its citizens tried to catch any incoming news about the footballworld cup through their portable receivers . ( s2 ) thus , world sports events strongly participate in eventually pulling back people towards friendship and peace The sentence ( s1 ) has been annotated as a premise , while ( s2 ) as a claim . In our classification , they are both claims . The reason is that they both contain topic - related words and their position in text is similar . The main distinction is the presence of the expression \" for example \" in the first sentence which qualifies it as a premise . To this end , in our future work we aim at adding some special words ( such as \" for example \" , \" therefore \" ) in the background knowledge of the classifier , in order to improve the capability of discriminating premises and claims .", "entities": []}
{"text": "In this paper , we present the \" Attraction to Topics \" - A2 T unsupervised approach for detecting argumentative discourse units , at sentence - level granularity . Motivated by the observation that topic information is frequently employed as a sub - task in the process of manual annotation of arguments , we propose an approach that exploits topic modeling techniques in order to identify argumentative units . Since manual supervision is not required , A2 T has the potential to be applicable on documents of various genres and domains . Preliminary evaluation results on two different corpora are promising . First , A2 T performs significantly better than the baseline on argumentative sentence detection on both corpora . Second , A2 T exhibits good results for classifying argumentative sentences as major claims , claims , premises , and non - argumentative units , at least for the first corpus , which has a low rate of non - argumentative sentences ( 20 % ) . Regarding directions for further research , there are several axes that can be explored . Evaluation on a larger set of annotation corpora will provide enhanced insights about the performance of the proposed approach on different document types . Our preliminary results showed that despite good recall on multiple corpora , achieving also good precision can be a challenging task in documents where argumentative units are sparse , and false positives can be an issue . In this context , we would like to also exploit other types of relations , and extend our method with other kinds of similarities over sentences .", "entities": []}
{"text": "In this section , we further analyse UDapter to understand its impact on different languages , and the importance of its various components .", "entities": []}
{"text": "UDapter learns language embeddings from syntactic , phonological and phonetic inventory features . A natural alternative to this choice is to learn language embeddings from scratch . For a comparison , we train a model where , for each in - training language , a separate language embedding ( of the same size : 32 ) is initialized randomly and learned end - toend . For the zero - shot languages we use the average , or centroid , of all in - training language embeddings . As shown in Figure 4a , on the high - resource set , the models with and without typological features achieve very similar average LAS ( 87.3 and 87.1 respectively ) . On zero - shot languages , however , the use of centroid embedding performs very poorly : 9.0 vs 36.5 average LAS score over 30 languages . As already discussed in 4.1 ( Table 2 ) , using a proxy language embedding belonging to the same family as the test language , when available , also clearly underperforms UDapter . These results confirm our expectation that a model can learn reliable language embeddings for in - training languages , however typological signals are required to obtain a robust parsing quality on zero - shot languages .", "entities": []}
{"text": "Details of training and zero - shot languages such as language code , data size ( number of sentences ) , and family are given in Table 5 and Table 6 .", "entities": []}
{"text": "Arianna Bisazza was partly funded by the Netherlands Organization for Scientific Research ( NWO ) under project number 639.021.646 . We would like to thank the Center for Information Technology of the University of Groningen for providing access to the Peregrine HPC cluster and the anonymous reviewers for their helpful comments .", "entities": []}
{"text": "In Section [ 1 ] we provided a brief summary of each sub - task in which we participated . For each of them , participants were given access to a labeled training and validation set , as well as an unlabeled evaluation set that was used to determined the final performance of the submitted systems .", "entities": []}
{"text": "In this section , we give a brief description of the system we used to conduct our experiments , share our results and provide a brief discussion .", "entities": []}
{"text": "The model was developed using the PyTorch ( Paszke et al , 2019 ) machine with an Intel Core i9 - 9820X CPU @ 3.30GHz and a NVIDIA GeForce RTX 2080 Ti GPU with 11 GB of memory .", "entities": []}
{"text": "We predict complex SQL clause - wisely as described in Figure 1 . Each clause is predicted consecutively by at most three different types of modules ( sketch , column , operator ) . The same architecture recursively predicts nested queries with temporal predicted SQL as an additional input .", "entities": []}
{"text": "After the predictions of all the other clauses , we use a heuristic to generate the FROM clause . We first collect all the columns that appear in the predicted SQL , and then we JOIN tables that include these predicted columns .", "entities": []}
{"text": "To predict the presence of a sub - query , we train another module that has the same architecture as the operator prediction module . Instead of predicting corresponding operators for each column , it predicts whether each column is compared to a variable ( e.g. , WHERE age > 3 ) or to a sub - query ( e.g. , WHERE age > ( SELECT avg ( age ) .. ) ) . This input is encoded in the same way as question encoding described in Section 3.1 . Then , the rest of the SQL generation process is identical to that described in Section 3.2 - 3.4 . After the sub - query is predicted , it replaces the [ SUB QUERY ] token to form the final query .", "entities": []}
{"text": "We thank Yongsik Lee , Jaesik Yoon , and Donghun Lee ( SAP ) for their reviews and support . We also thank professor Sungroh Yoon , Jongyun Song , and Taeuk Kim ( Seoul National University ) for their insightful feedback and three anonymous reviewers for their helpful comments .", "entities": []}
{"text": "In Table 3 , we show some examples of predicted SQL queries from different models . We compare the result of our model with two of previous state - of - the - art models : SyntaxSQLNet ( Yu et al , 2018b ) and the modified version of SQLNet ( Xu et al , 2017 ) by Yu et al ( 2018c ) to support complex SQL queries .", "entities": []}
{"text": "Psycholinguistic Models of Sentence Processing Improve Sentence Readability Ranking", "entities": []}
{"text": "While previous research on readability has typically focused on document - level measures , recent work in areas such as natural language generation has pointed out the need of sentence - level readability measures . Much of psycholinguistics has focused for many years on processing measures that provide difficulty estimates on a word - by - word basis . However , these psycholinguistic measures have not yet been tested on sentence readability ranking tasks . In this paper , we use four psycholinguistic measures : idea density , surprisal , integration cost , and embedding depth to test whether these features are predictive of readability levels . We find that psycholinguistic features significantly improve performance by up to 3 percentage points over a standard document - level readability metric baseline .", "entities": []}
{"text": "We have already explained the experimental findings of Kintsch & Keenan ( 1973 ) with respect to idea density , but what behavioral evidence is there to suggest that the remaining theories are valid ? Demberg & Keller ( 2008 ) examined the relationship between both surprisal and integration cost and eye - tracking times in the Dundee corpus ( Kennedy and Pynte , 2005 ) Demberg & Keller found that increased surprisal significantly correlated with reading times . Although they found that integration cost did not significantly contribute to predicting eye - tracking reading times in general , its contribution was significant when restricted to nouns and verbs . They also found that surprisal and integration cost were uncorrelated , suggesting that they should be considered complementary factors in a model of reading times . Another eyetracking study divided surprisal into lexical and synactic components , finding that lexical surprisal was a significant factor but not syntactic surprisal ( Roark et al , 2009 ) . Wu et al ( 2010 ) examined surprisal , entropy reduction , and embedding depth in a study of psycholinguistic complexity metrics . Their study of the reading times of 23 native English speakers reading four narratives indicated that embedding difference was a significant predictor of reading times for closed class words . Moreover , this contribution was independent of the contribution of surprisal , indicating that the two measures are capturing different components of the variance in reading times . Since integration cost was a significant predictor of reading times for nouns and verbs ( i.e. not closed class words ) and embedding depth was a significant predictor of reading times for closed class words , integration cost and embedding depth should also be complementary to each other .", "entities": []}
{"text": "We examined features for the ranking of sentences by their complexity , training linear models on two corpora using features derived from psycholinguistic theories of online sentence processing : idea density , surprisal , integration cost , and embedding depth . Surprisal coupled with embedding depth and our baseline features ( average word length & sentence length ) performed as well as the full model across all subsets of the OSE corpus . Integration cost and idea density were less effective , suggesting that the gain in speed from running a faster dependency parser may not be worth it . Instead , it is necessary to use the slower ModelBlocks parser to extract the more useful features . Overall , our strongest model combined the baseline features and the online psycholinguistic features . Because these features are complementary to features which have been explored in other work ( Vajjala and Meurers , 2014 ; Ambati et al , 2016 ) , the next step in future work is to combine all of these features and conduct a more comparison between the features proposed here and those examined in earlier work . In the meantime , we have demonstrated that features derived from psycholinguistic theories of sentence processing can be used to improve models for ranking sentences by readability .", "entities": []}
{"text": "Thanks are due to Matthew Crocker , Michael White , Eric Fosler - Lussier , William Schuler , Detmar Meurers , Marten van Schijndel , and Sowmya Vajjala for discussions and guidance during the development of this work . We are supported by DFG collaborative research center SFB 1102 ' Information Density and Linguistic Encoding ' .", "entities": []}
{"text": "Dense Procedure Captioning in Narrated Instructional Videos", "entities": []}
{"text": "Understanding narrated instructional videos is important for both research and real - world web applications . Motivated by video dense captioning , we propose a model to generate procedure captions from narrated instructional videos which are a sequence of stepwise clips with description . Previous works on video dense captioning learn video segments and generate captions without considering transcripts . We argue that transcripts in narrated instructional videos can enhance video representation by providing fine - grained complimentary and semantic textual information . In this paper , we introduce a framework to ( 1 ) extract procedures by a cross - modality module , which fuses video content with the entire transcript ; and ( 2 ) generate captions by encoding video frames as well as a snippet of transcripts within each extracted procedure . Experiments show that our model can achieve state - of - the - art performance in procedure extraction and captioning , and the ablation studies demonstrate that both the video frames and the transcripts are important for the task .", "entities": []}
{"text": "In this section , we describe our framework and model details as shown in Figure 2 . First , we adopt a context - aware video - transcript fusion module to generate features by fusing video information and transcript embedding ; Then the procedure extraction module takes the embedded features and predicts procedures with various lengths ; Finally , the procedure captioning module generates captions for each procedure by an encoder - decoder based model .", "entities": []}
{"text": "We take the encoded T feature vectors F of each video as the elementary units to generate procedure proposals . We follow the idea in ( Zhou et al , 2018a ; Krishna et al , 2017 ) that ( 1 ) generate a lot of anchors , i.e. proposals , with different lengths and ( 2 ) use the frame features within a proposal span to predict plausible scores .", "entities": []}
{"text": "In order to generate different - sized procedure proposals , we adopt a 1D ( temporal ) convolutional layer with the setting of K different kernels ; three output channels and zero padding to generate procedure candidates . The layer takes F R T \u00d7f as input and outputs a list of M ( k ) R T \u00d73 for each k - th kernel . All these results are stacked as a tensor M R K\u00d7T \u00d73 . Next , the tensor M is divided into three matrices : M = M m , M l , M s whereM m , M l , M s R K\u00d7T , They are designed to represent the offset of the proposal 's midpoint ; the offset of the proposal 's length and the prediction score . We calculate the starting and ending timestamp of each proposal by the offset of midpoint and length . Finally , a non - linear projection is applied on each matrix : M m = tanh ( M m ) , M l = tanh ( M l ) , M s = \u03c3 ( M s ) where \u03c3 is the Sigmoid projection .", "entities": []}
{"text": "We conduct the ablation experiments to show the effectiveness of utilizing transcripts . Table 3 lists the results . The Video Only Model only relies on video information for all modules . The Captioning by Video Model fuses transcripts during the procedure extraction which shows the transcript is effective for the extracting procedure . The Caption by Transcript Model only uses transcripts for captioning . Compared with the Caption by Video Model , we find that only using transcripts for captioning decreases performance . The reason is that only using transcripts for captioning will miss several actions appearing in the video but not mentioned in the transcript . The full Model achieves state -", "entities": []}
{"text": "( 1.1 ) mix the eggs and mix in a bowl ( 1.2 ) mix the eggs in a bowl ( 1.3 ) cut the meat into pieces ( 1.4 ) mix some olive oil in a bowl ( 1.5 ) add salt and pepper and pepper to the bowl ( 1.6 ) mix the sauce and mix ( 1.7 ) pour the sauce in the pan and s \ufffd r ( 1.8 ) add the pasta and mix it with the sauce", "entities": []}
{"text": "( 2.1 ) add some oil in a pan and add some water ( 2.2 ) add a li \ufffd le of oil and add a pan and add some oil ( 2.3 ) add oil and add to a pan and add some oil ( 2.4 ) add salt and pepper to the pan and s \ufffd r ( 2.5 ) add the chicken to the pan and s \ufffd r ( 2.6 ) add the sauce to the pan and s \ufffd r ( 2.7 ) add the pasta and add the sauce and mix", "entities": []}
{"text": "( a ) cut the potatoes into a bowl and add some oil and pepper ( b ) cut a pan and add some oil and add the pan ( c ) cut the potatoes into a bowl and add them ( d ) heat some oil in a pan and add some chopped onions and add some chopped onions and pepper ( e ) add chopped garlic and garlic and garlic and add to the pot ( f ) add the sauce and cook in the pan and s \ufffd r ( g ) add the sauce and add the sauce and s \ufffd r", "entities": []}
{"text": "( 5.1 ) blend the pepper and a small pieces ( 5.2 ) mix cheese bread crumbs parmesan cheese egg yolks a bowl and whisk the mixture ( 5.3 ) add sugar cream ketchup and worcestershire sauce on a pan ( 5.4 ) add some tomato into a bowl ( 5.5 ) add salt and black pepper to the salad and mix ( 5.6 ) mix the cabbage and salt in a bowl of - the - art results on procedure extraction and captioning , while Caption by Video Model gets better results on captioning for the ground - truth procedure . To sum up , both video frame frames and transcripts are important for the task .", "entities": []}
{"text": "We also present a qualitative analysis based on the case study shown in Figures 3 and 4 ( best viewed in color ) . Figure 3 visualizes the ground - truth procedures and the predicted procedures . The horizontal axis is the time and the number on each small ribbon is the ID of the procedure . We have slightly shifted the overlapping procedures in order to show the results more clearly . It can be seen that the extracted procedures by our full model have the most similar trend with the ground - truth procedures . Figure 4 presents the generated captions on extracted procedures ( Fig . 4a ) and ground - truth procedures ( Fig . 4b ) separately . Each column shows captioning results from one model , and the first column is the ground - truth result . On one hand , only the full model can generate eggs in the procedure ( 1.1 ) and ( 1.2 ) , which is also an important ingredient entity in the ground - truth captions . On the other hand , the ingredient bacon in groundtruth caption ( c ) is ignored by all models . In fact , our Full Model predicts meat synonyms of bacon . Besides , the Full Model can also generate the action cut and the final state of ingredient pieces mentioned in transcript , while it is hard to recognize using only video signals .", "entities": []}
{"text": "In this paper , we propose a framework for procedure extraction and captioning modeling in instructional videos . Our model use narrated tran - scripts of each video as the supplementary information and can help to predict and caption procedures better . The extensive experiments demonstrate that our model achieves state - of - the - art results on the YouCookII dataset , and ablation studies indicate the effectiveness of utilizing transcripts .", "entities": []}
{"text": "We thank the reviewers for their carefully reading and suggestions . This work was supported by the National Natural Science Foundation of China ( No . 61370137 ) , the National Basic Research Program of China ( No.2012CB7207002 ) , the Ministry of Education - China Mobile Research Foundation Project ( 2016/2 - 7 ) .", "entities": []}
{"text": "The likelihood of the keyword in the target domain is l x , and l x is set as the value of IDF in the target domain of w : l x = log ( N d i ) + 1 Here N is the number of articles in the article collection in the target domain , and d i is the number of articles containing the word w in the article collection in the target domain .", "entities": []}
{"text": "In this section we describe ( 1 ) our baseline sequence - to - sequence model , ( 2 ) our pointergenerator model , and ( 3 ) our coverage mechanism that can be added to either of the first two models . The code for our models is available online . 1", "entities": []}
{"text": "In this work we presented a hybrid pointergenerator architecture with coverage , and showed that it reduces inaccuracies and repetition . We applied our model to a new and challenging longtext dataset , and significantly outperformed the abstractive state - of - the - art result . Our model exhibits many abstractive abilities , but attaining higher levels of abstraction remains an open research question .", "entities": []}
{"text": "There are two broad meta - evaluation strategies : summary - level and system - level . Setup : For each document d i , i { 1 . . . n } in a dataset D , we have J system outputs , where the outputs can come from ( 1 ) extractive systems ( Ext ) , ( 2 ) abstractive systems ( Abs ) or ( 3 ) a union of both ( Mix ) . Let s ij , j { 1 . . . J } be the j th summary of the i th document , m i be a specific metric and K be a correlation measure .", "entities": []}
{"text": "Summary - level correlation is calculated as follows : K sum m 1 m 2 = 1 n n i=1 K [ m 1 ( s i1 ) . . . m 1 ( s iJ ) ] , [ m 2 ( s i1 ) . . . m 2 ( s iJ ) ] . ( 1 ) Here , correlation is calculated for each document , among the different system outputs of that document , and the mean value is reported .", "entities": []}
{"text": "System - level correlation is calculated as follows : K sys m 1 m 2 = K 1 n n i=1 m1 ( si1 ) . . . 1 n n i=1 m1 ( siJ ) , 1 n n i=1 m2 ( si1 ) . . . 1 n n i=1 m2 ( siJ ) . ( 2 ) Additionally , the \" quality \" of a system sys j is defined as the mean human score received by it i.e. HScore sys j mean = 1 n n i=1 humanScore ( s ij ) . ( 3 )", "entities": []}
{"text": "We collect the system - generated summaries from 25 top - scoring systems , 9 covering 11 extractive and 14 abstractive systems ( Sec . 2.2 ) on the CNNDM dataset . We organize our collected generated summaries into three groups based on system type : CNNDM Abs denotes collected output summaries from abstractive systems . CNNDM Ext denotes collected output summaries from extractive systems . CNNDM Mix is the union of the two .", "entities": []}
{"text": "Since collecting human annotations is costly , we sample 100 documents from CNNDM test set ( 11 , 490 samples ) and evaluate system generated summaries of these 100 documents . We aim to include documents of varying difficulties in the representative sample . As a proxy to the difficulty of summarizing a document , we use the mean score received by the system generated summaries for the document . Based on this , we partition the CNNDM test set into 5 equal sized bins and sample 4 documents from each bin . We repeat this process for 5 metrics ( BERTScore , MoverScore , R - 1 , R - 2 , R - L ) obtaining a sample of 100 documents . This methodology is detailed in Alg . 1 in Sec . A.1 .", "entities": []}
{"text": "Most papers that propose a new state - of - the - art system often use automatic metrics as a proxy to human judgments to compare their proposed method against other top scoring systems . However , can metrics reliably quantify the improvements that one high quality system makes over other competitive systems ? To answer this , instead of focusing on all of the collected systems , we evaluate the correlation between automatic metrics and human judg - ments in comparing the top - k systems , where top - k are chosen based on a system 's mean human score ( Eqn . 3 ) . 14 Our observations are presented in Fig . 3 . We find that : ( 1 ) As k becomes smaller , metrics de - correlate with humans on the TAC - 2008 and CNNDM Mix datasets , even getting negative correlations for small values of k ( Fig . 8a , 8c ) . Interestingly , SMS , R - 1 , R - 2 and R - L improve in performance as k becomes smaller on CNNDM Ext . ( 2 ) R - 2 had negative correlations with human judgments on TAC - 2009 for k < 50 , however it remains highly correlated with human judgments on CNNDM Abs for all values of k. Takeaway : Metrics can not reliably quantify the improvements made by one system over others , especially for the top few systems across all datasets . Some metrics , however , are well suited for specific datasets , e.g. JS - 2 and R - 2 are reliable indicators of improvements on TAC - 2009 and CNNDM Abs respectively .", "entities": []}
{"text": "We sincerely thank all authors of the systems that we used in this work for sharing their systems ' outputs .", "entities": []}
{"text": "We are grateful to the DataLEASH project and Helse Nord for funding this research work .", "entities": []}
{"text": "The need to detect events that could lead to protests is of prime interest to sociologists and governments ( Danilova et al , 2016 ) . There are several active ongoing projects for socio - political event systems such as KEDS ( Kansas Event Data System ) ( Schrodt and Hall , 2006 ) , CAMEO ( Conflict and Mediation Event Observation ) ( Gerner et al , 2002 ) , and several other databases for protest de - tection systems ( Danilova , 2015 ) . These methods have focused on news data as they have traditionally been the most reliant source of events . Protest detection has been one of the major issues in the context of social and political ( Ettinger et al , 2017 ) . Papanikolaou and Papageorgiou ( 2020 ) presented a computational social science methodology to analyse protests in Greece . constructed a corpus of protest events comprising various language sources from various countries . Several systems were submitted to the CLEF ProtestNews Track that consisted of three shared tasks , primarily aimed at identifying and extracting event information spanning to multiple countries ( H\u00fcrriyetoglu et al , 2019b .", "entities": []}
{"text": "This dataset comprises 26 , 208 sentences in three languages , namely English , Spanish , and Portuguese . The dataset consists of two classes : Event : The sentence indicates an event of the past . Not - event : The sentence does not talk about any event . The volume of sequences indicating Not - event is higher in contrast to that of the Event label . Therefore , the dataset distribution is quite imbalanced . We can also notice that the number of English samples exceeds that of Spanish and Portuguese ones . Refer to", "entities": []}
{"text": "EmoNet : Fine - Grained Emotion Detection with Gated Recurrent Neural Networks", "entities": []}
{"text": "Learning and Evaluating Emotion Lexicons for 91 Languages", "entities": []}
{"text": "The exact design of the Source train - dev - test split is as follows : All entries ( words plus ratings ) from all splits are taken from Warriner et al ( 2013 ) . The data was then partitioned based on the overlap with the two precursory versions by Bradley and Lang ( 1999 ) ( the original ANEW ) and Bradley and Lang ( 2010 ) ( an early extended version of ANEW roughly twice as large ) . Source - test was built by intersecting the lexicon from Warriner et al ( 2013 ) with the original ANEW . A similar process was applied for Source - dev : we intersected the words from Warriner et al ( 2013 ) and Bradley and Lang ( 2010 ) and removed the ones present in Source - test . Lastly , Source - train is made up by all words from Warriner et al ( 2013 ) which are neither in Source - test nor in Source - dev . The reason why the ratings in Source are taken exclusively from Warriner et al ( 2013 ) is that these are distributed under a more permissive license compared to their precursors . We removed multi - token entries ( e.g. , boa constrictor ) and entries with upper case characters ( e.g. , Budweiser ) from all data splits of Source , thus restricting the lexicon to single - token , nonproper noun entries to make it more suitable for word embedding - based research . All splits combined have 13 , 791 entries ( train : 11 , 463 , dev : 1 , 296 , test : 1 , 032 ) , thus removing less than 1 % from the original lexicon . 5 Regarding the remaining gold standards , the only cases which needed additional preparation or cleansing steps were zh1 and zh2 ( Yao et al , 2017 ) . zh1 was created and is distributed using traditional Chinese characters , whereas the embedding model by Grave et al ( 2018 ) employs simplified ones . Therefore , we converted zh1 into simplified characters using GOOGLE TRANSLATE 6 prior to evaluation . While manually examining the zh2 lexicon , we noticed several cases where the ratings seemed rather counter - intuitive ( e.g. , seemingly positive words which received very negative ratings ) . We contacted the authors who confirmed the problem and sent us a corrected version . We did not find any such problems in the second version . We consulted with a Chinese native speaker for both of these procedures regarding the zh1 and zh2 lexicons .", "entities": []}
{"text": "We would like to thank the anonymous reviewers for their helpful suggestions and comments , and Tinghui Duan , JULIE LAB , for assisting us with the Chinese gold data . This work was partially funded by the German Federal Ministry for Economic Affairs and Energy ( funding line \" Big Data in der makro\u00f6konomischen Analyse \" [ Big data in macroeconomic analysis ] ; Fachlos 2 ; GZ 23305/003#002 ) .", "entities": []}
{"text": "Paraphrases How do I improve my English What is the best way to learn English", "entities": []}
{"text": "In this section , some traditional approaches without neural models will be introduced .", "entities": []}
{"text": "This approach usually generates paraphrases by substituting some words in the source sentences with their synonyms extracted from a thesaurus ( Bolshakov and Gelbukh , 2004 ; Kauchak and Barzilay , 2006 ) . Thesaurus - based approaches proceed by first extracting all synonyms from a thesaurus for the words to be replaced . Then the optimal candidate is selected according to the context in the source sentence . Although simple and effective , this approach is severely limited by the diversity of the generated paraphrases .", "entities": []}
{"text": "Restoring Hebrew Diacritics Without a Dictionary", "entities": []}
{"text": "The input to the dotting task consists of a sequence of characters . Each of the characters is assigned three values , from three separate diacritic categories : one category for the dot distinguishing shin ( \u202b ) \ufb2a\u202c from sin ( \u202b , ) \ufb2b\u202c two consonants sharing a base character \u202b ; \u05e9\u202c another for the presence of dagesh / mappiq , a central dot affecting pronunciation of some consonants , e.g. \u202b\ufb44\u202c /p/ from \u202b\u05e4\u202c /f/ , but also present elsewhere ; and one for all other diacritic marks , which mostly determine vocalization , e.g. \u202b\u05d3\u202c /da/ vs. \u202b\u05d3\u202c /de/. Diacritics of different categories may co - occur on single letters , e.g. , or may be absent altogether . Full script Hebrew script written without intention of dotting typically employs a compensatory variant known colloquially as full script ( ktiv male , \u202b\u05de\u05dc\u05d0\u202c \u202b , ) \u05db\u05ea\u05d9\u05d1\u202c which adds instances of the letters \u202b\u05d9\u202c and \u202b\u05d5\u202c in some places where they can aid pronunciation , but are incompatible with the rules for dotted script . In our formulation of dotting as a sequence tagging problem , and in collecting our test set from raw text , these added letters may conflict with the dotting standard . For the sake of input integrity , and unlike some other systems , we opt not to remove these characters , but instead employ a dotting policy consistent with full script . See Appendix A for further details .", "entities": []}
{"text": "In Table 4 we present examples of words dotted incorrectly , or correctly , only by NAKDIMON , compared with Morfix and Dicta . The largest category for NAKDIMON - only errors ( \u223c18 % of 90 sampled ) are ones where a fused preposition+determiner character is dotted to only include the preposition , perhaps due to its inability to detect the explicit determiner clitic \u202b\u05d4\u202c in neighboring words , on which the complex systems apply morphological segmentation . In other cases ( \u223c15 % ) , NAKDIMON creates 11 These are : the sin / shin dot , vowel distinctions across the a / e / i / o / u / null sets , and dagesh in the \u202b/\u05d1\u202c \u202b/\u05db\u202c \u202b\u05e4\u202c characters . We do not distinguish between kamatz gadol / kamatz katan , and schwa is assumed to always be null . unreadable vocalization sequences , as it has no lexical component and is decoded greedily . These types of errors are more friendly to the typical use cases of a dotting system , as they are likely to stand out to a reader . In contrast , a large portion of cases where only NAKDIMON was correct ( \u223c13 % of 152 ) are foreign names and terms . This may be the result of such words not yet appearing in dictionaries , or not being easily separable from an adjoining clitic , while character - level information can capture pronunciation patterns from similar words ( e.g. \u202b\u05b6\u05e4\ufb4b\u202c \u202b\u05b6\u05dc\u202c \u202b\u05d8\u202c ' telephone ' , for the example \u202b. ) \u05d4\u05d0\u05d9\u05d9\u05e4\u05d5\u202c OOVs To further quantify the strengths of NAKDIMON 's architecture and training abilities , we evaluate the systems ' results pertaining only to those words in the test set which do not appear in our training sets . We follow common practice by calling them OOVs ( \" out of vocabulary \" ) , but emphasize that NAKDIMON does not consult an explicit vocabulary , and the other systems are not evaluated against their own vocabularies ( which are unknown to us ) . We find that NAKDIMON 's performance on this subset is substantially worse compared with the other systems than on the full set : 15 percentage points below Dicta and seven below Morfix on the VOC metric ( see full results in Appendix C ) . These results might be counter - intuitive considering the proven utility of character - level models in OOV contexts ( e.g. , Plank et al , 2016 ) , and so we offer several possible explanations : First , many \" OOVs \" consist in fact of known words coupled with an unseen combination of prefix clitics and/or suffix possessive markers , which other systems explicitly remove using morphological analyzers before dotting . Second , mirroring the last finding from the overall analysis , some \" OOVs \" are proper names which appear in dictionaries but are absent from the training set , due to corpus effects such as time and domain , or simply chance .", "entities": []}
{"text": "We collected the data for our training set and test sets from open online sources , while making sure their terms allow research application and privacy is not impugned . NAKDIMON 's architecture does not encourage memorization of training data and the system is not trained for generating text . We consider a main use case for our system to be assisting Hebrew learners in reading . We therefore expect NAKDIMON to facilitate life in Israel for immigrants still struggling with Hebrew , among other underprivileged groups . Automatic dotting can increase inclusion in Hebrew - prominent societies for literacy - challenged individuals , and derivative improvements in text - to - speech applications can assist those with impaired vision . Lastly , dotting can help researchers with limited understanding of Hebrew access resources in the language . Hebrew is a gendered language . Orthographically , in many cases the lack of dots masks gender ambiguity , allowing both masculine and feminine readings for a given word ( e.g. \u05b0 \u202b\u05b0\ufb4a\u202c \u202b\u05b7\u05d7\u202c \u202b\u05dc\u202c \u202b\ufb2a\u202c / \u05b8 \u202b\u05b0\ufb4a\u202c \u202b\u05b7\u05d7\u202c \u202b\u05dc\u202c \u202b\ufb2a\u202c ' you.fem sent ' / ' you.masc sent ' ) . While wellperforming automatic dotting can help alleviate these ambiguities and reduce the amount of potentially prejudiced readings , we recognize the large body of work on gender bias in NLP ( Blodgett et al , 2020 ) , including in Hebrew NLP ( Moryossef et al , 2019 ) , and the findings that an imbalanced training set may result in an even more skewed distribution of gender bias in applications ( Zhao et al , 2017 ) . We believe our unlexicalized approach is more robust to such bias compared with other systems , and have already started quantifying and addressing these issues as we find them in ongoing work . In the meantime , we offer this paragraph as a disclaimer .", "entities": []}
{"text": "We apply the following resolution tactics for added letters in undotted text : ( a ) We almost never remove or add letters to the original text ( unless it is completely undiacritizable ) . ( b ) We keep dagesh in letters that follow a shuruk which replaces a kubuts , and similarly for yod ( hirik male replacing hirik haser ) . ( c ) When we have double vav or double yod , the second letter is usually left undotted , except when it is impossible to have the correct vocalization this way . Resolving ktiv haser discrepancies from Morfix outputs is done by adding missing vowel letters , or removing superfluous vowel letters , in such a way that would not count as an error if it is correct according to Academy regulations .", "entities": []}
{"text": "We would like to thank Avi Shmidman for details about Dicta 's Nakdan and other suggestions . We thank Sara Gershuni for lengthy and fruitful discussions , and for her linguistic insights and advice . We thank Yoav Goldberg , Reut Tsarfaty , Ian Stewart , Sarah Wiegreffe , Kyle Gorman and many anonymous reviewers for their comments and suggestions in discussions and on earlier drafts .", "entities": []}
{"text": "SUPERB - SG : Enhanced Speech processing Universal PERformance Benchmark for Semantic and Generative Capabilities", "entities": []}
{"text": "Upstream Model ( eg . FBANK , TERA , etc . ) The introduction of these new tasks of varying difficulty takes us closer to a more comprehensive unified standard speech benchmark . We hope that this will motivate the development of more powerful , generalizable , and reusable pre - trained models to democratize the advancement of speech research . To facilitate this , we released the codes 1 and integrated the tasks with the SUPERB benchmark .", "entities": []}
{"text": "This section introduces the tasks in SUPERB - SG , including why we choose these tasks and how we design the task - specific heads for fine - tuning . Following SUPERB 's methodology , we use a lightweight fine - tuning approach wherein we freeze the pre - trained model parameters and only keep the task - specific head 's parameters trainable . This setting serves the dual purpose of evaluating the robustness as well as the generalizability of the speech representations , and provides a resourceefficient way of fine - tuning the models that is inclusive of participants with constrained compute resources . We call the pre - trained model as upstream model and the task - specific heads as downstream model . We now discuss the newly added tasks in SUPERB - SG in the following sub - sections .", "entities": []}
{"text": "Following SUPERB , we fix upstream models parameters for all downstream tasks ' training . We extract the frame - level representations for each hidden layer of the upstream models from raw waveform , and use a trainable task - specific weightedsum mechanism to summarize all layers ' representations into a sequence of vectors . The summarized representations are then used as the downstream model 's input . An overview of the training procedure is demonstrated in Figure 1 . Each experiment is done by one single run with the same seed . This procedure is consistent for all experiments , offering a fair and simple evaluation strategy for all upstream models .", "entities": []}
{"text": "This work fully adheres to the ACL code of ethics . For more details , we provide a checklist in Appendix B.", "entities": []}
{"text": "Here we answer the ethics questions to show our ethics statement .", "entities": []}
{"text": "Yes , we discussed the constrains on the frozen upstreams and simple task specific heads in abstract and Section 3 . Yes , we used public datasets and pre - trained models mentioned in Section 3 .", "entities": []}
{"text": "UniConv : A Unified Conversational Neural Architecture for Multi - domain Task - oriented Dialogues", "entities": []}
{"text": "DST . For state tracking , the metrics are calculated for domain - specific slots of the corresponding domain at each dialogue turn . We also report the DST separately for multi - domain and single - domain dialogues to evaluate the challenges in multi - domain dialogues and our DST performance gap as compared to single - domain dialogues . From our DST performs consistently well in the 3 domains attraction , restaurant , and train domains . However , the performance drops in the taxi and hotel domain , significantly in the taxi domain . We note that dialogues with the taxi domain is usually not single - domain but typically entangled with other domains . Secondly , we observe that there is a significant performance gap of about 10 points absolute score between DST performances in singledomain and multi - domain dialogues . State tracking in multi - domain dialogues is , hence , could be further improved to boost the overall performance . Additionally , we report qualitative analysis and the insights can be seen in Appendix C.", "entities": []}
{"text": "We thank all reviewers for their insightful feedback to the manuscript of this paper . The first author of this paper is supported by the Agency for Science , Technology and Research ( A*STAR ) Computing and Information Science scholarship .", "entities": []}
{"text": "De - Biased Court 's View Generation with Causality", "entities": []}
{"text": "Court 's view generation is a novel but essential task for legal AI , aiming at improving the interpretability of judgment prediction results and enabling automatic legal document generation . While prior text - to - text natural language generation ( NLG ) approaches can be used to address this problem , neglecting the confounding bias from the data generation mechanism can limit the model performance , and the bias may pollute the learning outcomes . In this paper , we propose a novel Attentional and Counterfactual based Natural Language Generation ( AC - NLG ) method , consisting of an attentional encoder and a pair of innovative counterfactual decoders . The attentional encoder leverages the plaintiff 's claim and fact description as input to learn a claim - aware encoder from which the claim - related information in fact description can be emphasized . The counterfactual decoders are employed to eliminate the confounding bias in data and generate judgmentdiscriminative court 's views ( both supportive and non - supportive views ) by incorporating with a synergistic judgment predictive model . Comprehensive experiments show the effectiveness of our method under both quantitative and qualitative evaluation metrics .", "entities": []}
{"text": "Owing to the prosperity of machine learning , especially the natural language processing ( NLP ) techniques , many legal assistant systems have been proposed to improve the effectiveness and efficiency of a judge from different aspects , such as relevant case retrieval ( Chen et al , 2013 ) , applicable law articles recommendation ( Chen et al , 2019 ) , controversy focus mining ( Duan et al , 2019 ) , and judgment prediction ( Lin et al , 2012 ; Zhong et al , 2018 ; Hu et al , 2018 ; Jiang et al , 2018 ; Chalkidis et al , * Corresponding Authors .", "entities": []}
{"text": "In this section , we first introduce the effect of mechanism confounding bias on the court 's view generation and propose a backdoor - inspired method to eliminate that bias . Then , we describe our Attentional and Counterfactual based Natural Language Generation ( AC - NLG ) model in detail . Fig . 3 shows the overall framework .", "entities": []}
{"text": "As shown in Fig . 3 , to optimize Eq . 3 , we use a pair of counterfactual decoders to learn the likelihood P ( V | I , j ) for each j. At inference , we propose to use a predictor to approximate P ( j ) . Note that our implementation on backdoor - adjustment can be easily applied for multi - valued confounding with multiple counterfactual decoders .", "entities": []}
{"text": "We conduct a human evaluation to better analyze the quality of the generated court 's view . First , we randomly sample 500 test cases , where the ratio of the supported and nonsupported cases are 1:1 . For each case , we present the generated court 's views from each method 7 with the ground truth to 5 human annotators with legal backgrounds . The evaluation is conducted following three perspectives : ( 1 ) Judgment level . Annotators are asked to give a score ( 1 - 5 ) on the judgment in the generated court 's view . 1 for totally wrong and 5 for totally correct . ( 2 ) Rational level . Annotators are asked to give a score ( 1 - 5 ) on the rationals in the generated court 's view . 1 for the worst and 5 for the best . ( 3 ) Fluency level . Annotators are asked to give a score ( 1 - 5 ) on the fluency of the generated court 's view . 1 for the worst and 5 for the best .", "entities": []}
{"text": "The defendant B return the loan of $ 495 , 000 . The defendant C return the loan together .", "entities": []}
{"text": "After the hearing , the court held the facts as follows : From November 20 , 2010 to March 23 , 2011 , the defendant B successively borrowed a total of $ 495 , 000 from the plaintiff A and issued four separate borrowings . The defendant B has not repaid the above loan .", "entities": []}
{"text": "The court concluded that : The private lending relationship between Plaintiff A and Defendant B , where the subject was appropriate , the content was legitimate , and the meaning was true , should be confirmed to be legal and valid . The two sides did not agreed in writing on a loan period , so the defendant should return the plaintiff 's loan in a timely manner within a reasonable period after the plaintiff urged . The defendant 's failure to return the loan timely constituted a breach of contract and should bear corresponding civil liabilities . Therefore , the plaintiff 's claim was reasonable and legal , and the court supported it Acceptance .", "entities": []}
{"text": "The court concluded that the subject of the private lending relationship between Plaintiff A and Defendant B was qualified , the content was legal , and the meaning was true . It should be deemed valid . The two sides did not agreed in writing on a loan period , the defendant shall return the loan within a reasonable period after the plaintiff urged . The plaintiff 's claim requesting the defendant to return the loan of $ 495 , 000 was in compliance with the law and the court supported it Acceptance . However , the court did not support the claim requesting the defendant C to bear the guarantee liability because the evidence was insufficient", "entities": []}
{"text": ".", "entities": []}
{"text": "Figure 4 shows three court 's views for a certain case : the court 's view generated by PGN , by the proposed AC - NLG method , and the real court 's view . We find that the one generated by PGN accepts the claim for principal , but ignores other claims such as issue related to guarantee . Compared with the real court 's view , our model accu - rately responds to both claims and produces the correct judgment .", "entities": []}
{"text": "The defendant B should return to the defendant $ 20 , 000 and pay litigation costs of this case , and the defendant C shall undertake joint and several liability .", "entities": []}
{"text": "After the hearing , the court held the facts as follows : Plaintiff A supported the facts of his claim and provided the court with a receipt of the loan issued by the defendant B on September 22 , 2008 . Although the evidence was not cross - examined by the two defendants in court , it was considered by the court that the evidence was legal , true and relevant to the facts of this case , so the validity of the evidence was confirmed . The facts confirmed by the court are consistent with the facts claimed by the plaintiff A.", "entities": []}
{"text": "The court concluded that : The loan relationship between the plaintiff and the defendant is legal and effective , it should be protected by law . The defendant did not return the loan within the agreed time limit , which constituted a breach of contract and should bear the corresponding liability . The plaintiff is now claiming the defendant to return the loan of $ 20 , 000 , which complies with the law and the court will support it Acceptance . The defendant was legally summoned by the court and failed to appear in court without justifiable reasons .", "entities": []}
{"text": "The court concluded that : The loan relationship between the plaintiff and the defendant is legal and valid . The defendant still owes the plaintiff a loan of $ 20 , 000 and has not returned . The plaintiff 's request for the defendant to return the money complies with the law and the court supports it Acceptance . The defendant B was legally summoned by the court and failed to appear in court without justifiable reasons . It was deemed to have waived his right to defend the facts and claims by the plaintiff .", "entities": []}
{"text": "The court concluded that : The guaranteed loan relationship between the plaintiff and the defendant is legal and effective . The defendant B still owes the plaintiff a loan of $ 20 , 000 and has not returned . The plaintiff 's claim for the defendant B to return the loan complies with the law and the court supports it Acceptance . Defendant C voluntarily provided guarantee for this loan and did not stipulate the guarantee method and period . According to law , he should bear joint and several liability for the above debt within six months from the date of maturity of the main debt . The main contract in this case did not stipulate the time limit for the performance of the main debt , and the guarantee period should be calculated from the date when the plaintiff claimed the rights . The plaintiff 's claim that the defendant C bears joint and several liability for the settlement of the above debts complies with the law , and the court also supports it Acceptance . The two defendants were legally summoned by the court and failed to appear in court without justifiable reasons . It was deemed to have waived his right to defend the facts and claims by the plaintiff .", "entities": []}
{"text": "This work was supported by National Natural Science Foundation of China ( No . 62006207 , 61625107 ) , National Key R&D Program of China ( No . 2018AAA0101900 , 2018YFC0830200 , 2018YFC0830206 , 2020YFC0832500 ) , the Fundamental Research Funds for the Central Universities . Finally , we would like to thank the anonymous reviewers for their helpful feedback and suggestions .", "entities": []}
{"text": "All models are trained on 2 V100 GPU ( 16 GB ) .", "entities": []}
{"text": "1.The defendant B shall return the plaintiff 's loan of $ 30 , 000 and pay the overdue interest at the interest rate of 2.4 % per month from the date of prosecution to the date of repayment . 2 . The defendant B shall pay the litigation costs of this case .", "entities": []}
{"text": "After the hearing , the court held the facts as follows : On December 11 , 2013 , the defendant B borrowed $ 30 , 000 from the plaintiff A. The defendant B received the loan and gave a receipt of this loan . Note : I have borrowed $ 30 , 000 from A today .", "entities": []}
{"text": "The court concluded that : The facts that defendant B borrowed $ 30 , 000 from the plaintiff A are clear . The private lending relationship between Plaintiff A and Defendant B is legitimate and valid , it shall be protected by the law . The plaintiff now demands that the defendant repay the loan of $ 30 , 000 . This demand is justified and should be supported Acceptance . Defendant B refused to appear in court without justification after being legally summoned by the court .", "entities": []}
{"text": "The court concluded that : The legitimate private lending relationships are protected by law . The act of borrowing between the plaintiff A and the defendant B did not violate the prohibitive provisions of state laws and regulations , so it should be valid . The fact that the defendant B owed the plaintiff A a loan of $ 30 , 000 is clear , and the evidence is sufficient . The defendant was supposed to repay the loan in time , and his failure to repay in time constituted a breach of contract , and he shall assume corresponding civil liabilities according to law Acceptance . The receipt of this loan provided by the plaintiff A does not have agreed interest , as not to pay interest . So the court does not support the claim that the plaintiff A asked the defendant B to calculate the interest from the date of the loan", "entities": []}
{"text": ". The defendant was summoned by the court and refused to appear in court without justification .", "entities": []}
{"text": "The court concluded that : the lending agreement between the plaintiff A and the defendant B contains the true meaning and does not violate the prohibitive provisions of state laws and regulations , it is legal and valid . Although the plaintiff and the defendant did not specifically agree on the time for repayment , after the defendant received the loan , it shall be returned within a reasonable period after being appealed by the plaintiff . If the defendant fails to return it within a reasonable period after being called , the defendant shall be responsible to pay the overdue interest from the date of prosecution Acceptance . For the calculation standard for overdue interest , the plaintiff claimed that the monthly interest rate was 2.4 % , but it did not provide a corresponding evidence . Therefore , the court does not support this claim of overdue interest", "entities": []}
{"text": ". With reference to the loan interest rate announced by the People 's Bank of China for the same period , the court determined that overdue interest is calculated at an annual interest rate of 5.6 % . The fact that the defendant has not returned the loan of $ 30 , 000 is clear . So the court supports the reasonable part of the plaintiff 's claim requesting the defendant to return the loan and pay the overdue interest . Defendant B refused to appear in court without justification after being legally summoned by the court .", "entities": []}
{"text": "The two defendants B and C return the loan principal of $ 2 , 000 , 000 and interest ( The interest will be calculated as four times the interest rate of similar loans of the bank from February 28 , 2014 to the date when the judgment is confirmed , it is $ 40 , 000 temporarily calculated to the date of prosecution ) .", "entities": []}
{"text": "After the hearing , the court held the facts as follows : The two defendants B and C have spousal relationship . On February 28 , 2014 , the defendant B borrowed $ 2 , 000 , 000 from the plaintiff A and signed a loan contract , stipulating that the defendant borrowed 2 million ( $ 2 , 000 , 000.00 ) from the plaintiff , and the loan period is from the date of signing to March 27 , 2014 , the interest is calculated at four times the interest rate of similar loans of the People 's Bank of China over the same period . The loan period has expired and the defendant refused to return the loan . For this reason , the plaintiff A claimed in court .", "entities": []}
{"text": "The court concluded that : The legal loan relationship is protected by law . The fact that the defendant B borrowed $ 2 , 000 , 000 from the plaintiff A is clear , and the evidence is indeed sufficient . The defendant B did not return the loan in time according to the contract , which was a breach of contract and should assume the corresponding liabilities for breach of contract according to law . The plaintiff 's claim was accepted and the court supports it Acceptance . The defendants failed to appear in court after being legally summoned by the court . The court can judge the case in absentia according to law .", "entities": []}
{"text": "The court concluded that : The civil loan relationship formed by the defendant B borrowing money from the plaintiff A and the act of giving a receipt of this loan are based on the true intention of them , they did not violate the mandatory provisions of the laws and regulations , it is legal and valid , and it should be protected by law . The defendant B did not repay the plaintiff 's loan of $ 2 million , which constituted a breach of contract , and he should assume the civil liabilities for returning the loan and paying interest . The defendant B and C have spousal relationship . The debt in this case occurred during the marriage , so it should be treated as joint debts and paid by the two defendants together . In summary , the plaintiff 's claim is supported by law , and the court supports it Acceptance . The defendants B and C was legally summoned by the court and refused to appear in court without justifiable reasons .", "entities": []}
{"text": "The court concluded that : The defendant B has not repaid the loan of $ 2 million from the plaintiff A , and should return it in time and pay interest according to the agreed time limit . The debt occurred during the marriage of the defendants B and C , so it should be treated as joint debts , the two defendants should jointly take the responsibility for repayment . The plaintiff A 's claim is legal , and the court supports it Acceptance . The defendants B and C were legally summoned by the court and refused to appear in court without justifiable reasons .", "entities": []}
{"text": "After the hearing , the court held the facts as follows : On June 28 , 2013 , the defendant B gave a receipt of loan . The defendant needed a loan of $ 50 , 000 . The loan period was from June 28 , 2013 to July 27 , 2013 . There was no agreed interest on the loan . After the due date , the defendant agreed to calculate the interest on the unrefunded principal at four times the bank 's loan interest during the same period . On the same day , the plaintiff A made a payment of $ 50 , 000 from his bank account to the defendant B 's bank account . Then the defendant B issued a receipt confirming that the loan of $ 50 , 000 was received . However , the defendant B has not returned the loan principal and interest .", "entities": []}
{"text": "The court concluded that : The loan relationship between the plaintiff and the defendant does not violate the compulsory provisions of state laws and administrative regulations , and should be deemed as legal and effective . The defendant B failed to repay the interest according to the receipt , and the plaintiff 's claim to return the principal and pay the overdue interest should be supported according to law Acceptance . Defendant B refused to appear in court without justification after being legally summoned by the court , and is deemed to have waived his right to litigation . The court can judge the case in absentia according to law .", "entities": []}
{"text": "The court concluded that : The loan between the plaintiff A and the defendant B did not violate the legal provisions , and was based on the true intentions of them , and this case has the loan agreement , receipt and bank statement issued by the defendant . The loan relationship is legal and valid . The plaintiff requested the defendant to return the loan principal of $ 50 , 000 in compliance with the law , and the court will support it . Acceptance . In this case , the interest was not agreed during the loan period , the court does not support the interest during the loan period in the plaintiff 's claim", "entities": []}
{"text": ". But the court supports the calculation of the overdue interest from July 28 , 2013 to March 31 , 2014 based on the four times bank 's loan interest rate during the same period .", "entities": []}
{"text": "After the hearing , the court held the facts as follows : On November 3 , 2011 , the defendant B borrowed $ 20 , 000 from the plaintiff A , and the defendant B issued a receipt for this loan of $ 20 , 000 to the plaintiff . The loan receipt did not specify the loan interest and repayment date . The plaintiff stated in court that the defendant B paid about $ 500 but less than $ 1 , 000 . It was also found that the defendant B and the defendant C registered their marriage on September 1 , 2006 .", "entities": []}
{"text": "The court concluded that : The legal loan relationship is protected by law . The defendant B borrowed $ 20 , 000 from the plaintiff A. This case has the evidence of the loan receipt and the plaintiff 's statement in court . The facts were clear and the evidence was true and sufficient . The legal loan relationship is protected by law , and the loan principal and interest should be repaid Acceptance .", "entities": []}
{"text": "The court concluded that : The legal loan relationship is protected by law . The fact that the defendant B owed the plaintiff A a loan of $ 20 , 000 was based on a loan receipt and the plaintiff 's statement in court . The facts are clear and the evidence is true and sufficient . The loan repayment period does not stipulate the repayment period , the plaintiff can urge the defendant to repay within a reasonable period . Now the plaintiff claims that the defendants repay the loan principal of $ 20 , 000 , it complies with the law and the court supports it . The defendant B and the defendant C have spousal relationship . In this case , the debt in this case occurred during the marriage , so it should be treated as joint debts and paid by the two defendants together Acceptance .", "entities": []}
{"text": "The court concluded that : The legal loan relationship is protected by law . The fact that the defendant B borrowed money from the plaintiff A was based on a loan receipt and the plaintiff 's statement . The facts are clear , and the evidence is true and sufficient . The loan interest rate is not agreed on the receipt , it shall be deemed as non - payment of interest . The plaintiff 's opinion that the amount paid by the defendant B is interest has no factual basis and the court will not approve it . Because the plaintiff could not determine the specific amount paid by the defendant B , the court determined the amount paid by the defendant B as $ 500 at his discretion , and the $ 500 should be deducted from the loan principal . If the loan does not agree on the repayment period , the debtor shall return the loan if the creditor requests it to be returned according to trading habits . The defendant B and the defendant C registered their marriage on September 1 , 2006 . The debt in this case occurred during their marriage , so it is the joint debt of the two defendants and should be repaid together Acceptance . The defendants B and C were legally summoned by the court and refused to appear in court without justifiable reasons .", "entities": []}
{"text": "The two defendants B and C are required to return the loan of $ 180 , 000 and pay the overdue interest of the loan of $ 100 , 000 ( from March 9 , 2013 , the monthly interest rate is calculated at 1.87 % to the date that the judgment is confirmed )", "entities": []}
{"text": "After the hearing , the court held the facts as follows : In support of the facts claimed by the plaintiff , the following evidence materials were provided to this court within the proof period : 1 . Two loan agreements to prove the fact that the defendant B borrowed $ 180 , 000 from the plaintiff . 2 . One piece of marriage registration information , to prove that the loan in the case occurred during the marriage of the two defendants , and should be the joint debts of the two defendants . Although the evidence provided by the plaintiff has not been cross - examined by the two defendants , the court found that the content of the abovementioned evidence was objective and clear , the source form was legal , and was related to the facts of the case , so it was accepted . Based on the evidence adopted above and the court investigation , the facts confirmed by the court are consistent with the facts claimed by the plaintiff .", "entities": []}
{"text": "The court concluded that : The private lending relationship between the plaintiff and the defendant B is established and legally valid . As the borrower , the defendant B failed to repay the loan , which constituted a breach of contract and should bear corresponding civil liabilities . The plaintiff 's claim has sufficient evidence and complies with the law , and the court supports it Acceptance . The two defendants were legally summoned by the court and failed to appear in court without justifiable reasons . It was deemed to have waived his right to defend the facts and claims by the plaintiff .", "entities": []}
{"text": "The court concluded that : The private lending relationship between the plaintiff and the defendant B is established and legally valid . As the borrower , the defendant B failed to perform the repayment obligations in time and should bear corresponding civil liabilities . The plaintiff 's changed claim has sufficient evidence and complies with the law , and the court will support it Acceptance . The two defendants were legally summon by the court and failed to appear in court without justifiable reasons . It was deemed to have waived his right to defend the facts and claims by the plaintiff .", "entities": []}
{"text": "The court concluded that : The civil lending relationship between the plaintiff A and the defendant B was established and legally valid . As the borrower , the defendant B failed to fully perform the repayment obligations as agreed , which constituted a breach of contract and should bear corresponding civil liabilities . Because the loan in this case was formed during the marriage of the two defendants , in view of the fact that the defendant C did not respond to the claim and did not appear in court to participate in the litigation , the debt owned by the defendant B personally should be regarded as the joint debts of the defendant B and C. The plaintiff 's changed claim has sufficient evidence and complies with the law , and the court will support it Acceptance . The two defendants were legally summon by the court and failed to appear in court without justifiable reasons . It was deemed to have waived his right to defend the facts and claims by the plaintiff .", "entities": []}
{"text": "After the hearing , the court held the facts as follows : On September 30 , 2013 and August 25 , 2014 , the defendant B borrowed $ 10 , 000 each time from the plaintiff A. The defendant issued a loan receipt to the plaintiff for each of the two loans . There was no written agreement on the interest and loan period . Later , the defendant did not return the loan , then it caused a dispute . The above facts are proved by two receipts of the loan provided by the plaintiff and the plaintiff 's statement in the court .", "entities": []}
{"text": "The court concluded that : The private lending relationship between the plaintiff and the defendant is established according to law , and is effective from the date the plaintiff provides the defendant with the loan . After the plaintiff provided the loan to the defendant , the defendant failed to return the loan as agreed , it was obviously a breach of contract . Therefore , the plaintiff 's claim requesting the defendant to return the loan principal of $ 28 , 000 was justified , and the court supports it Acceptance . The defendant was legally summoned by the court and refused to appear in court without justifiable reasons to participate in the proceedings .", "entities": []}
{"text": "The court concluded that : The private lending relationship between the plaintiff and defendant is established according to law and should be protected by law . The defendant borrowed $ 10 , 000 from the plaintiff . The facts were clear and the evidence was sufficient . The plaintiff now requires the defendant to repay the loan of $ 10 , 000 . The reasons are justified , and the court supports it Acceptance . But the court does not support the plaintiff 's claim requesting the defendant to pay interest on the loan because the plaintiff failed to provide evidence to prove the fact that both of them agreed on the interest of the loan", "entities": []}
{"text": ". The defendant was legally summoned by this court and refused to appear in court without justifiable reasons to participate in the proceedings .", "entities": []}
{"text": "Target - specified Sequence Labeling with Multi - head Self - attention for Target - oriented Opinion Words Extraction", "entities": []}
{"text": "Given a sentence s = { w 1 , w 2 , ... , w n } consisting of n words , an aspect ( opinion target ) a = { w i , w i+1 , ... , w i+k } , and an opinion term o = { w j , w j+1 , ... , w j+m } ( a and o are substrings of s ) , the probabilities of target - oriented opinion terms are defined as p ( o | s , a ) in the TOWE task and the probabilities of aspect - opinion pairs are defined as p ( a , o | s ) = p ( a | s ) \u00d7 p ( o | s , a ) in the AOPE task . The BIO tagging scheme ( Ramshaw and Marcus , 1995 ) and a special symbol \" [ SEP ] \" are applied to this task , where each word w i in the sentence s is tagged as y i { B , I , O , [ SEP ] } ( B : Beginning , I : Inside , O : Others , [ SEP ] : the tag of an aspect ) .", "entities": []}
{"text": "To evaluate the performance of our model 2 , we conduct experiments on two public datasets from laptop and restaurant domains . These two datasets were respectively built by Fan et al ( 2019 ) for TOWE and Chen et al ( 2020 ) for AOPE based on SemEval Challenge 2014 Task 4 , SemEval Challenge 2015 Task 12 , and SemEval Challenge 2016 Task 5 ( Pontiki et al , 2014 ( Pontiki et al , , 2015 ( Pontiki et al , , 2016 . For the first dataset , every sentence was annotated by two people , and the conflicts were checked and eliminated manually . The second dataset was developed by extending the first one . The statistics of these benchmark datasets are shown in Table 2 , from which we can observe that the second dataset includes many negative samples for AOPE ( i.e. , the sentences only contain aspects and opinion terms , without any aspect - opinion pairs ) . Note that these negative samples will also be considered when testing our model on AOPE .", "entities": []}
{"text": "In this paper , we propose a target - specified sequence labeling method based on multi - head selfattention ( TSMSA ) and a multi - task version ( MT - TSMSA ) to deal with TOWE and AOPE , respectively . In our methods , the encoder is capable of capturing the information of the specific aspect which is labeled by a special symbol \" [ SEP ] \" . Experimental results demonstrate that TSMSA and MT - TSMSA achieve quite competitive performance in most cases . When combining aspect and opinion words extraction with TOWE , our MT - TSMSA can slightly improve the performance as compared with TSMSA . In the future , we plan to extend our approaches to sentiment classification of pairs and explore an efficient model with a one - stage inference process to reduce the time complexity on AOPE .", "entities": []}
{"text": "We are grateful to the reviewers for their constructive comments and suggestions on this study . This work has been supported by the National Natural Science Foundation of China ( 61972426 ) and Guangdong Basic and Applied Basic Research Foundation ( 2020A1515010536 ) .", "entities": []}
{"text": "Modeling Framing in Immigration Discourse on Social Media", "entities": []}
{"text": "Framing selects particular aspects of an issue and makes them salient in communicating a message ( Entman , 1993 ) . Framing can impact how people understand issues , attribute responsibility ( Iyengar , 1991 ) , and endorse possible solutions , thus having major implications for public opinion and policy decisions ( Chong and Druckman , 2007 ) . While past work has studied framing by the news media and the political elite , little is known about how ordinary people frame political issues . Yet , framing by ordinary people can influence others ' perspectives and may even shape elites ' rhetoric ( Russell Neuman et al , 2014 ) . To shed light on this important topic , we focus on one issue - immigration - and develop a new methodology to computationally analyze its framing on Twitter . Our work highlights unique insights that social media data offers . The massive amount of available social media content enables us to compare framing strategies across countries and political ideologies . Furthermore , social media provides unique insights into how messages resonate with audiences through interactive signals such as retweets and favorites . By jointly analyzing the production and reception of frames on Twitter , we provide an in - depth analysis of immigration framing by and on the public . Political communications research has identified numerous typologies of frames , such as issuegeneric policy , immigration - specific , and narrative . Each of these frame types can significantly shape the audience 's perceptions of an issue ( Iyengar , 1991 ; Chong and Druckman , 2007 ; Lecheler et al , 2015 ) , but prior NLP work seeking to detect frames in mass media ( e.g. Card et al , 2016 ; Field et al , 2018 ; Kwak et al , 2020 ) has largely been limited to a single issue - generic policy typology . Multiple dimensions of framing must be considered in order to better understand the structure of immigration discourse and its effect on public opinion and attitudes . We thus create a novel dataset of immigration - related tweets containing labels for each typology to facilitate more nuanced computational analyses of framing . This work combines political communication theory with NLP to model multiple framing strategies and analyze how the public on Twitter frames immigration . Our contributions are as follows : ( 1 ) We create a novel dataset of immigrationrelated tweets labeled for issue - generic policy , immigration - specific , and narrative frames . ( 2 ) We develop and evaluate multiple methods to detect each type of frame . ( 3 ) We illustrate how a message 's framing is influenced by its author 's ideology and country . ( 4 ) We show how a message 's framing affects its audience by analyzing favoriting and retweeting behaviors . Finally , our work highlights the need to consider multiple framing typologies and their effects .", "entities": []}
{"text": "Framing serves four functions : ( i ) defining problems , ( ii ) diagnosing causes , ( ii ) making evaluative judgments , and ( iv ) suggesting solutions ( Entman , 1993 ) . Framing impacts what people notice about an issue , making it a key mechanism by which a text influences its audience . Framing Typologies We draw upon distinct typologies of frames that can be applied to the issue of immigration : ( 1 ) issue - specific , which identify aspects of a particular issue , or ( 2 ) issue - generic , which appear across a variety of issues and facilitate cross - issue comparison ( de Vreese , 2005 ) . Issue - generic frames include policy frames that focus on aspects of issues important for policymaking , such as economic consequences or fairness and equality ( Boydstun et al , 2013 ) . Other generic frames focus on a text 's narrative ; news articles use both episodic frames , which highlight specific events or individuals , and thematic frames , which place issues within a broader social context . The use of episodic versus thematic frames can influence the audience 's attitudes . For example , episodic frames lead audiences to attribute responsibility for issues such as poverty to individual citizens while thematic frames lead them to hold the government responsible ( Iyengar , 1991 ) . Issue - specific frames for immigration focus on the portrayal of immigrants . Our analysis uses Benson ( 2013 ) 's set of issue - specific frames , which represent immigrants as heroes ( cultural diversity , integration , good workers ) , victims ( humanitarian , global economy , discrimination ) , and threats ( to jobs , public order , taxpayers , cultural values ) . Both issue - specific and generic frames provide unique insights but present advantages and drawbacks . While issue - specific frames analysis are specific and detailed , they are hard to generalize and replicate across studies , which is a key advantage for generic frames ( de Vreese , 2005 ) . Framing effects Studies of framing typically focus on either frame - building or framesetting ( Scheufele , 1999 ; de Vreese , 2005 ) . Frame - building is the process by which external factors , such as a journalist 's ideology or economic pressures , influence what frames are used ; frame - building studies thus treat framing as the dependent variable . Frame - setting studies treat frames as independent variables that impact how an audience interprets and evaluates issues . Prior analyses of frame - building in immigration news highlight region and ideology as particularly important factors . Right - leaning media from conservative regions are more likely to frame immigrants as intruders ( van Gorp , 2005 ) , and as threats to the economy and public safety ( Fryberg et al , 2012 ) . Framing also differs across countries ; while the US press emphasizes public order , discrimination , and humanitarian concerns , the French press more frequently frames immigrants as victims of global inequality ( Benson , 2013 ) . Frame - setting has also been studied in the context of immigration . For example , experimental work has shown that frames eliciting angry or enthusiastic emotions impact participants ' opinions on immigration ( Lecheler et al , 2015 ) . While past work has analyzed linguistic framing in Twitter immigration discourse ( e.g. , de Saint Laurent et al , 2020 ) , little is known about how such framing affects users ' interactive behaviors such as resharing content , which is a key objective of frame setting .", "entities": []}
{"text": "These instances highlight the challenges of annotation ; there are convincing arguments that model 's predicted frames can be appropriate labels . Interestingly , the criteria to which immigrants would be held would not be met by a large number of the ' British ' people either .", "entities": []}
{"text": "Model predicts frames that may capture an author 's intention but without sufficient evidence from the text", "entities": []}
{"text": "Model erroneously predicted Threat : Public Order Missing necessary contextual knowledge Some frames are directly cued by lexical items ( e.g. politicians ' names cue Political frame ) , but model lacks real - world knowledge required to identify these frames @EricTrump Eric I have been alive longer than your immigrant mother in law and you . I paid more in taxes than you did and your immigrant mother in law combined ...", "entities": []}
{"text": "In writing about an issue , individuals are known to select particular frames - a process known as frame - building - based on numerous factors , such as exposure to politicians ' rhetoric or their own identity ( Scheufele , 1999 ) . Here , we focus on two specific identity attributes affecting frame building : ( i ) political ideology and ( ii ) country / region . The political , social , and historical contexts of an one 's nation - state can impact how they frame immigration ( Helbling , 2014 ) . Immigration has a long history in the USA relative to Europe , and former European colonial powers ( e.g. the UK ) have longer immigration histories than other countries ( e.g. Norway ) ( Thorbj\u00f8rnsrud , 2015 ; Eberl et al , 2018 ) . Cross - country variation in news framing also arise from differences in immigration policies ( Helbling , 2014 ; Lawlor , 2015 ) , media systems ( Thorbj\u00f8rnsrud , 2015 ) , journalis - tic norms ( Papacharissi and De Fatima Oliveira , 2008 ) , geographic proximity to immigrant populations or points of entry ( Grimm and Andsager , 2011 ; Fryberg et al , 2012 ) , and immigrants ' race / ethnicity ( Grimm and Andsager , 2011 ) . At the same time , increased globalization may result in a uniform transnational immigration discourse ( Helbling , 2014 ) . Framing variations across countries has implications for government policies and initiatives , particularly in determining what solutions could be applied internationally or tailored to each country ( Caviedes , 2015 ) . Prior studies on the role of ideology in framebuilding have focused on the newspapers or political movements , showing patterns in frames like morality and security by political affiliation in European immigration discourse ( Helbling , 2014 ; Hogan and Haltinner , 2015 ) or in use of economic frames by American newspapers ( Fryberg et al , 2012 ; Abrajano et al , 2017 ) . However , it remains unclear whether these patterns observed for elite groups can generalize to the effect of individual people 's political dispositions .", "entities": []}
{"text": "Chong and Druckman ( 2007 , p. 116 ) assert that a \" challenge for future work concerns the identification of factors that make a frame strong . \" Studies of frame - setting - i.e. , how a message 's framing affects its audience 's emotions , beliefs , and opinions - have largely been restricted to smallscale experimental studies because responses to news media framing can not be directly observed ( Eberl et al , 2018 ) . However , Twitter provides insight into the frame - setting process via interactive signals : favorites and retweets . While related , these two actions can have distinct underlying motivations : favoriting often indicates positive alignment between the author and the reader ; in contrast , retweeting may also be driven by other motivations , such as the desire to inform or entertain others ( boyd et al , 2010 tweets with detected author ideology . The presence of a frame is treated as a binary fixed effect . We control for all temporal , user - level and tweet - level features as in the prior section , as well as ideology . Results The framing of immigration has a significant impact on how users engage with the content via retweets and favorites ( Figure 4 ) . Many issuespecific frames have a stronger effect on audience responses than either of the other typologies . As recent NLP approaches have adopted issue - generic frames for analysis ( e.g. , Kwak et al , 2020 ) , the strength of issue - specific frames highlights the importance of expanding computational analyses beyond issue - generic frames , as other frames may have larger consequences for public opinion . Most frames impact favorites and retweets differently , suggesting that the strength of a frame 's effects is tied to the specific engagement behavior . Cultural frames ( e.g. hero : integration ) and frames oriented around human interest ( e.g. morality , victim : discrimination ) are particularly associated with more endorsements ( favorites ) , perhaps due to their increased emotional appeal to readers ( Semetko and Valkenburg , 2000 ) . On the other hand , political factors & implications is most highly associated with increased retweets . As the political frame emphasizes competition and strategy ( Boydstun et al , 2013 ) , this result mirrors similar links between the \" horse - race \" frame in news reports and engagement ( Iyengar et al , 2004 ) ; users may prefer amplifying political messages via retweeting to help their side win . Similarly , frames about security and safety ( e.g. crime & punishment , victim : humanitarian ) are highly associated with more retweets , but not necessarily favorites . While security and safety frames may not lead audience members to endorse such messages , perhaps they are more likely to amplify these messages due to perceived urgency or the desire to persuade others of such concerns . Finally , Figure 4 shows how a message 's narrative framing impacts audience response , even after controlling for all other frames . Both episodic and thematic frames are significantly associated with increased engagement ( retweets ) , but less strongly than issue frames . Having a clear narrative is important for messages to spread , but the underlying mechanisms driving engagement behaviors may differ for episodic and thematic frames ; prior work on mainstream media has found that news stories using episodic frames tend to be more emotionally engaging , while thematic frames can be more persuasive ( Iyengar , 1991 ; Gross , 2008 ) .", "entities": []}
{"text": "Users ' exposure to political information on social media can have immense consequences . By leveraging multiple theory - informed typologies , our computational analysis of framing enables us to better understand public discourses surrounding immigration . We furthermore show that framing on Twitter affects how audience interactions with messages via favoriting and retweeting behaviors . This work has implications for social media platforms , who may wish to improve users ' experiences by enabling them to discover content with a diversity of frames . By exposing users to a wide range of perspectives , this work can help lay foundations for more cooperative and effective online discussions . All code , data , annotation guidelines , and pretrained models are available at https : //github.com / juliamendelsohn / framing .", "entities": []}
{"text": "Figure 5 shows the distribution of frames as a fraction of total tweets in the annotated data .", "entities": []}
{"text": "Tables 9 - 35 show independent variable coefficients for logit regressions predicting frames from region .", "entities": []}
{"text": "Tables 36 - 62 show independent variable coefficients for logit regressions predicting frames from ideology .", "entities": []}
{"text": "We thank Anoop Kotha , Shiqi Sheng , Guoxin Yin , and Hongting Zhu for their contributions to the data annotation effort . We also thank Libby Hemphill and Stuart Soroka for their valuable comments and feedback . This work was supported in part through funding from the Volkswagen Foundation .", "entities": []}
{"text": "Aggressive language in an online hacking forum", "entities": []}
{"text": "We have an inter - corpus experimental design , in which a document classifier is trained on one dataset and tested on other datasets . Our training data come from the Wikipedia Comments Corpus ( WikiComments ) ( Wulczyn et al , 2017 ) , which contains 115 , 864 discussion posts extracted from an English Wikipedia dump , judged as personal attacks or harassment by crowdworkers . Ten judgements were collected for each post ; hence we have an attack score from zero to ten for every post 2 , and we assume that the higher the attack score the greater the linguistic aggression shown in writing . This assumption may be challenged , as we accept that there are many reasons why a text may not be unanimously judged to be an attack or ha - rassment - properties of the text such as poor grammar which obfuscates meaning , use of slang insults which are not universally known , or sarcastic phrasing which is not interpreted as an attack by all annotators . On the other hand , properties of the annotator , such as fatigue or inattention , inexperience with English or the terminology used , or idiosyncratic linguistic thresholds for attacks and harassment , could all play a part in judgement variation as well . However , over such a large dataset we assume that in terms of aggressive language the texts will be broadly well ordered by their attack scores . Table 1 shows examples randomly drawn from each attack score , zero to ten , along with the number of posts in each class , and the cumulative size of the corpus in reverse order from attack score ten to zero . The curators of WikiComments used these annotated discussion posts to train a classifier and further label unseen posts in a larger collection of 63 million discussion posts , with a view to largescale analyses of attacks by unregistered users , moderator actions in response to attacks , and more ( Wulczyn et al , 2017 ) . They experimented with different thresholds t where attack scores at or above t would be labelled as attacks , and those below t would not be attacks . They found that the optimal value for t balancing precision and recall was 4.25 . Our intention is to take the texts and attack scores from WikiComments to train a binary aggression classifier for use with other corpora . The question with such a classifier is how to partition the training data for true / false aggression labels : the cut - off could be any attack score value from one to ten . In the following sections we report on classification experiments with each attack score cut - off value and a test corpus sourced from Internet forums . Our test data come from the CrimeBB Corpus 3 , a dataset harvested from several hackingrelated websites including HackForums , Antichat and Greysec ( Pastrana et al , 2018 ( Fleiss , 1971 ; Landis and Koch , 1977 ) - i.e. \u03ba = 0.4 to 0.6 . We did not attempt to settle on single annotations for each post , but instead treated all judgements equally , allowing multiple labels both by individual annotators and across different annotators . A single annotator further labelled the remaining 1923 posts . Posts with aggressive intent are uncommon on HackForums , with only 100 aggressive posts judged to be aggressive by at least one annotator in the total corpus of 4123 posts ( 2.4 % ) . Note that profane language is more commonly found - which is unsurprising given the casual linguistic register - with 201 posts in this dataset featuring at least one of ' fuck , shit , cunt , jerk , crap , dick ' ( or derived forms ) . However , the profanity is often used for humorous purposes , or to defuse potentially confrontational conversations , or simply in a casual way for no purpose at all ; it is not always used aggressively ( hence the need for manual annotation ) . This observation underlines the distinction between offensive and aggressive language . Table 2 shows the size of the CrimeBB Corpus , the HackForums subset , and the annotated posts , along with examples of aggressive and nonaggressive posts from HackForums .", "entities": []}
{"text": "We trained a binary aggression classifier on the WikiComments Corpus setting the true / false threshold t at each attack score from 1 to 10 and testing the classifier on our annotated set of 4123 HackForums posts from the CrimeBB Corpus .", "entities": []}
{"text": "Refers to disability 7", "entities": []}
{"text": "Alludes to violence 2", "entities": []}
{"text": "Includes racism 1 ability ' label always involves the words ' retard ' and ' retarded ' in this 100 post sample . Finally , direct threats of violence are very rare , with only two examples found in this subcorpus .", "entities": []}
{"text": "This work was supported by The Alan Turing Institute 's Defence & Security Programme , and the U.K. Engineering & Physical Sciences Research Council . We thank Emma Lenton , Dr Alastair Beresford , and the anonymous reviewers for their support and advice .", "entities": []}
{"text": "Our work builds upon the notion of a noisy knowledge graph ( NKG ) , which consists of a directed graph G = ( V , E ) where V is a set of concepts and E the set of labelled binary semantic relations - e.g. , those found between synsets like , for instance , hypernymy or meronymy within a semantic network like WordNet . In a NKG we assume both V and E to have been acquired automatically , e.g. , in order to induce a domain - aware or a general purpose knowledge base . Additionally , we consider for our purposes the hypernymy graph T = ( T V , T E ) of G , the subgraph made up of the hypernymy ( i.e. , isa - labeled ) edges of E. Since T is a subgraph of G , we can expect that the former inherits a certain amount of noise from the latter . Noise within hypernymy graphs can be further classified into : i ) noisy nodes , the concepts that do not belong to a specific target vocabulary , e.g. , domain concepts for domain - specific KBs , such as Jaguar Cars within a zoological taxonomy ; ii ) noisy edges , the wrongly - acquired relations between unrelated concepts or out - of - domain relations , e.g. , Jaguar Cars isa Feline ; iii ) cycles of hypernymy relations , such as those derived from counts over very large corpora ( Seitner et al , 2016 ) , e.g. , jaguar ( Panthera onca ) feline animal jaguar ( Panthera onca ) . We accordingly define the task of extracting a clean taxonomy from a NKG as that of pruning the cycles , as well as the noisy edges and nodes , from the hypernymy subgraph T of G.", "entities": []}
{"text": "In order to enable end - to - end taxonomy induction from scratch , we combine our general approach with existing KBs that have been automatically induced from text and linked to reference lexical knowledge bases on the basis of unsuper - vised methods . To this end , we use the linked disambiguated distributional KBs from 1 , which are built in three steps : 1 ) Learning a JoBimText model . Initially , a sense inventory is created from a large text collection using the pipeline of the JoBimText project ( Biemann and Riedl , 2013 ) . 2 The resulting structure contains disambiguated protoconcepts ( i.e. , senses ) , their similar and related terms , as well as aggregated contextual clues per proto - concept . 2 ) Disambiguation of related terms . Similar terms and hypernyms associated with a protoconcept are fully disambiguated based on the partial disambiguation from step ( 1 ) . The result is a proto - conceptualization ( PCZ ) , where all terms have a sense identifier . 3 ) Linking to a lexical resource . The PCZ is automatically aligned with an existing lexical resource ( LR ) such as WordNet or BabelNet . For example , bridge : NN:3 is linked to the Babel synset bn:00013077n ( the ' infrastructure ' sense ) . That is , a mapping between the two sense inventories is created to combine them into a new extended sense inventory , a hybrid aligned resource . Table 1 shows the proto - conceptualization entries for the polysemous terms bridge and link , namely their figurative ( \" bridge : NN:2 \" and \" link : NN:1 \" ) and concrete ' infrastructure ' ( \" bridge : NN:3 \" and \" link : NN:0 \" ) senses , respectively . JoBimText models provide sense distinctions that are only partially disambiguated : the list of similar and hypernyms terms of each sense , in fact , does not carry sense information . Consequently , a semantic closure procedure is applied in order to obtain a PCZ and arrive at sense representation in which all terms get assigned a unique , best - fitting sense identifier ( see for details ) . PCZs consist of a rich , yet noisy , disambiguated semantic network automatically induced from large amounts of text : links to existing lexical resources provide us a source of external supervision that can be leveraged to clean them and turn them into full - fledged taxonomies . Steps 1 - 3 are unsupervised by nature . Consequently , when combined with our algorithm they provide a complete framework for fully unsupervised taxonomy induction from scratch . Note , however , that our approach offers a general solution to the problem of taxonomy cleaning . In an additional set of experiments , we apply it to different automatically generated taxonomies from a SemEval task in a more controlled setting where we rely on a few manually created KB links only .", "entities": []}
{"text": "We next evaluate the overall impact of our approach within an existing benchmark for the taxonomy induction task . Intuitively , most of the benefits from our method derive from the \" gold standard \" information of the companion KB , and its linking to the NKG , which act as a source of supervision . Consequently , we address the research question of how much ( pseudo - ) supervision our method needs in terms of KB links , and whether it can be used to improve the state - of - the - art on the task of taxonomy induction .", "entities": []}
{"text": "In Table 3 , we report the performance on the Sem - Eval task for the two selected input taxonomies . Results on the structural similarities of the pruned taxonomies with the gold standard ones , computed using the CF&M measure , indicate that , thanks to ContrastMedium and with a minimal human effort - the creation of just a few KB links ( up to 10 ) , which are needed only when automatic linking is not available - it is possible to boost the quality of taxonomies using state - of - art methods by a large margin . For instance , in the case of the Equipments taxonomy , we improve up to 7 points . The baseline , which only breaks cycles , is not able to reassess the graph structure and only provides very small improvements to the submitted NKGs . Overall , the results show that ContrastMedium leads to competitive performance on a hard , realistic benchmark such as TExEval , achieving the best overall results for both taxonomies . That is , our algorithm is able to improve the state - of - theart on taxonomy induction by additionally boosting the quality of existing top - performing systems for this task : this is achieved on the basis of a minimally supervised approach that only requires a few links to a reference KB , which is used to provide ground - truth taxonomic relations and guide the cleaning process .", "entities": []}
{"text": "We acknowledge the support of the Deutsche Forschungsgemeinschaft ( DFG ) under the JOIN - T project .", "entities": []}
{"text": "Computational Argumentation Synthesis as a Language Modeling Task", "entities": []}
{"text": "Synthesis approaches in computational argumentation so far are restricted to generating claim - like argument units or short summaries of debates . Ultimately , however , we expect computers to generate whole new arguments for a given stance towards some topic , backing up claims following argumentative and rhetorical considerations . In this paper , we approach such an argumentation synthesis as a language modeling task . In our language model , argumentative discourse units are the \" words \" , and arguments represent the \" sentences \" . Given a pool of units for any unseen topic - stance pair , the model selects a set of unit types according to a basic rhetorical strategy ( logos vs. pathos ) , arranges the structure of the types based on the units ' argumentative roles , and finally \" phrases \" an argument by instantiating the structure with semantically coherent units from the pool . Our evaluation suggests that the model can , to some extent , mimic the human synthesis of strategy - specific arguments .", "entities": []}
{"text": "Existing research on computational argumentation largely focuses on the analysis side . Various analysis tasks are widely studied including identifying the claims along with their supporting premises ( Stab and Gurevych , 2014 ) , finding the relation between argumentative units ( Cocarascu and Toni , 2017 ) , and assessing the persuasiveness of arguments ( Habernal and Gurevych , 2016 ) . Diverse downstream applications , however , necessitate the development of argumentation synthesis technologies . For example , synthesis is needed to produce a summary of arguments for a given topic ( Wang and Ling , 2016 ) or to build a debating system where new arguments are exchanged between the users and the system ( Le et al , 2018 ) . As a result , a number of recent studies addresses the argumentation synthesis task . These studies have proposed different approaches to generating claims or reasons for a given topic , partly with a particular stance towards the topic ( Bilu and Slonim , 2016 ; Hua and Wang , 2018 ) . However , the next important synthesis step is still missing in the literature , namely , to generate complete texts including both argumentative and rhetorical considerations . With the latter , we refer to Aristotle 's three means of persuasion : logos ( providing logical arguments ) , ethos ( demonstrating credibility ) , and pathos ( evoking emotions ) . As discussed by Wachsmuth et al ( 2018 ) , following a rhetorical strategy is key to achieving persuasion with argumentative texts . This paper proposes a new computational approach that synthesizes argumentative texts following a rhetorical strategy . We do not tackle this task immediately \" in the wild \" , i.e. , generating an entirely new argumentative text for a freely - chosen topic and a possibly complex strategy . Rather , we consider a \" controlled \" synthesis setting , with the goal of successively creating models that are able to deal with more complex settings later on . In particular , given a pool of argumentative discourse units ( ADUs ) , our approach generates arguments for any unseen pair of topic and stance ( e.g. , \" con abortion \" ) as well as a basic rhetorical strategy ( i.e. , logos - oriented vs. pathos - oriented ) . 1 To abstract from the arguments ' topics during training , we first identify different ADU types using clustering . Our approach then learns to select unit types matching the given strategy and to arrange them according to their argumentative roles . Both steps are realized as a language model where ADUs represent words and arguments are sentences . Finally , our approach \" phrases \" an argument by predicting the best set of semantically related ADUs for the arranged structure using supervised regression . Thereby , we ensure that the synthesized texts are composed of meaningful units , a property that neural generation methods barely achieve so far . In our evaluation , we utilize the dataset of Wachsmuth et al ( 2018 ) . This dataset contains 260 argumentative texts on 10 topic - stance pairs , where each text composes five ADUs in a logos - oriented or pathos - oriented manner . In our experiments , we train our approach on nine topic - stance pairs and then generate an argument for the tenth . The results demonstrate that our approach successfully manages to combine pairs of ADUs , but its performance on longer sequences of ADUs is limited . Altogether , our contribution is three - fold : 1 . A new view of argumentation synthesis that represents argumentative and rhetorical considerations with language modeling . 2 . A novel approach that selects , arranges , and phrases ADUs to synthesize strategy - specific arguments for any topic and stance . 3 . First experimental evidence that arguments with basic rhetorical strategies can be synthesized computationally . 2", "entities": []}
{"text": "To develop our model for argumentation synthesis , we exploit the dataset recently developed by Wachsmuth et al ( 2018 ) . The dataset comprises 260 manually generated argumentative texts . The generation of each text , for one topic - stance pair , has been conducted in a systematic fashion following the three canons of rhetoric ( Aristotle , 2007 ) : 1 . Inventio \u223c Selecting a subset of argumentative discourse units ( ADUs ) from a pool of given ADUs for a topic - stance pair . 2 . Dispositio \u223c Arranging the selected ADUs in a sequential order . 3 . Elocutio \u223c Phrasing the arranged ADUs by adding connectives at unit - initial or unit - final positions . Specifically , Wachsmuth et al ( 2018 ) selected a pool of 200 ADUs for 10 pairs of controversial topic and stance from the English version of the arg - microtexts corpus ( Peldszus and Stede , 2016 ) . As a preprocessing step , they \" decontextualized \" these ADUs manually by removing connectives , resolving pronouns , and similar . Each topic - stance pair comes with 20 such ADUs : four theses , four con units , and 12 pro units . Table 1 shows the ADU list for one topic - stance pair . 26 participants were asked by Wachsmuth et al ( 2018 ) to create short argumentative texts for each topic - stance pair following one of two basic rhetorical strategies : ( 1 ) logos - oriented , i.e. , arguing logically , and ( 2 ) pathos - oriented , i.e. , arguing based on emotional appeals . For each topic - stance pair they created an argument by selecting one thesis , one con and three pro units that they thought could best form a persuasive argument following the given strategies . Table 2 shows two samples of generated arguments in the dataset . The dataset contains 130 logos - oriented and 130", "entities": []}
{"text": "This section presents our computational approach to synthesize arguments for any pair of topic and stance , following one of two basic rhetorical strategies : arguing logically ( logos - oriented ) or arguing emotionally ( pathos - oriented ) . A black - box view of the approach is shown in Figure 1 . As input , our approach takes a strategy as well as a pool of argumentative discourse units ( ADUs ) for any specific topic - stance pair x. Each ADU has the role of a thesis ( in terms of claim with a stance on the topic ) , a con point ( objecting the thesis ) , or a pro point ( supporting the thesis ) . The approach then imitates the human selection , arrangement , and \" phrasing \" of a sequence of n ADUs , in order to synthesize an argument . Phrasing is done only in terms of picking semantically coherent ADUs for the arranged sequence ; the addition of connectives between ADUs is left to future work . Below , we detail how we realize each step ( selection , arrangement , and phrasing ) with a topicindependent model . For each step , we explain how it is trained ( illustrated in Figure 2 ) and how it is applied to an unseen topic - stance pair ( Figure 3 ) .", "entities": []}
{"text": "This model handles the selection of a set of n ADUs for a topic - stance pair x and a rhetorical strategy . We approach the selection as a language modeling task where each ADU is a \" word \" of our language model and each argument a \" sentence \" . To abstract from topic , the model actually selects ADU types , as explained in the following .", "entities": []}
{"text": "Output Strategy - specific arg '", "entities": []}
{"text": "Figure 1 : Black - box view of our argumentation synthesis approach . The input is a rhetorical strategy as well as a pool of thesis , con , and pro ADUs for some topicstance pair x. The approach outputs a strategy - specific sequence of n ADUs as an argument for x ( here , n = 5 ) .", "entities": []}
{"text": "We use the NRC lexicon of Mohammad and Turney ( 2013 ) . The lexicon has been compiled manually using crowdsourcing and contains a set of English words and their associations with ( 1 ) sentiment , i.e. , negative and positive polarities , and ( 2 ) emotions , i.e. , the eight basic emotions defined by Plutchik ( 1980 ) : anger , anticipation , disgust , fear , joy , surprise , sadness , and trust . These features are represented as the count of words associated with each category ( e.g. , the count of sad words in an ADU ) . Somasundaran et al ( 2007 ) constructed a lexicon that includes the following arguing patterns : assessments , doubt , authority , emphasis , necessity , causation , generalization , structure , conditionals , inconsistency , possibility , wants , contrast , priority , difficulty , inyour - From the type sequence , a set of candidate arguments is decoded . ( 2 ) The arrangement filters out candidates not matching the most probable ADU role sequence , ( T hesis , Con , P ro , P ro , P ro ) . ( 3 ) Phrasing scores each remaining argument and outputs the top argument .", "entities": []}
{"text": "shoes , rhetorical question . We use the count of each arguing pattern in text as one feature ( e.g. , number of assessments patterns in an ADU ) .", "entities": []}
{"text": "As shown in Figure 3 , the selection language model takes the ADUs of an unseen topic - stance x as input . It then outputs a set of candidate arguments , in terms of sequences of ADUs . Each ADU is encoded into a cluster label ( representing an ADU type ) . For example , one might have the following mappings , given the six labels A - F from Figure 2 : A { T he x , 1 , T he x , 2 , Con x , 3 } B { Con x , 2 , P ro x , 1 } C { T he x , 3 , Con x , c , P ro x , 2 , P ro x , 3 } D { P ro x , p , Con x , 1 } E { T he x , t } F { T he x , 4 , Con x , 4 , P ro x , 4 } The language model for either of the two rhetorical strategies generates a set of arguments where each argument is composed of n cluster labels , e.g. , ( A , B , C , D , C ) for n = 5 in Figure 3 . This set is ranked by probability of the associated sequence . For example , assume that ( A , B , C , D , C ) is most probable . Then we decode all possible ADU sequences for topic - stance x from ( A , B , C , D , C ) to a set of candidate arguments : ( A , B , C , D , C ) { T he x , 1 , T he x , 2 , Con x , 3 } \u00d7 { Con x , 2 , P ro x , 1 } \u00d7 { T he x , 3 , Con x , c , P ro x , 2 , P ro x , 3 } \u00d7 { P ro x , p , Con x , 1 } \u00d7 { T he x , 3 , Con x , c , P ro x , 2 , P ro x , 3 } The output of the model is a set of candidate arguments , which becomes the input of the arrangement language model .", "entities": []}
{"text": "In the arrangement process , we aim to imitate the human behavior of arranging ADUs for a specific topic - stance following a rhetorical strategy ( here , logos or pathos ) . Again , we approach this problem as a language modeling task . Each ADU role ( thesis , pro , or con ) is a word of the language model and each argument a sentence .", "entities": []}
{"text": "As sketched in Figure 2 , we first convert the humangenerated arguments from a sequence of ADUs to a sequence of ADU roles . Then , we use these sequences to train a language model for each strategy .", "entities": []}
{"text": "As shown in Figure 3 , the arrangement language model takes as input the candidate arguments that we get from the selection language model and outputs a set of filtered candidate arguments . The language model for a specific strategy generates a set of argument structures where each such structure is a sequence of n ADU roles , e.g. , ( T hesis , Con , P ro , P ro , P ro ) for n = 5 in Figure 3 . This set is ranked by the probability of the sequences . For example , assume that the most frequent sequence is ( T hesis , Con , P ro , P ro , P ro ) . Using the output from the selection language model , we filter out all candidate arguments that do not match ( T hesis , Con , P ro , P ro , P ro ) , ending up with the following filtered arguments : { T he x , 1 , T he x , 2 } \u00d7 { Con x , 2 } \u00d7 { P ro x , 2 , P ro x , 3 } \u00d7 { P ro x , p } \u00d7 { P ro x , 2 , P ro x , 3 } The output of the model is a filtered set of candidate arguments , which becomes the input of the phrasing regression model .", "entities": []}
{"text": "The set of arguments resulting from the selection and arrangement language models are based on topic - independent features . The missing step is to entail the topical relationship between the ADUs in each generated argument . We approach this task with supervised regression . As indicated above , our model does not really phrase an argument . Rather , it aims to choose the best among the given set of candidates in terms of semantic coherence .", "entities": []}
{"text": "At this point , the phrasing model is provided by the filtered arguments from the arrangement model . For each filtered argument , we extract the bigram features ( semantic similarities ) . Next , using the phrasing model , we predict the score of each sequence . The sequence with the highest score is the generated argument . In Figure 3 , this is : ( T he x , 2 , Con x , 2 , P ro x , 2 , P ro x , p , P ro x , 3 )", "entities": []}
{"text": "In this section , we report the results of evaluating the introduced approach to argumentation synthesis based on the dataset described in Section 3 .", "entities": []}
{"text": "This paper has presented a topic - independent computational approach to imitate the process of selecting , arranging , and phrasing argumentative discourse units ( ADUs ) - so to speak , to synthesize arguments . We have proposed to operationalize the necessary synthesis knowledge in the form of a combined language and regression model that predicts ADU sequences . So far , we have evaluated our approach on a small dataset only that contains 260 argumentative texts following either of two rhetorical strategies . For a controlled experiment setting based on this data , we have reported preliminary results of medium effectiveness regarding the imitation of human - generated arguments . A big challenge for the future is to move from such a controlled setting to a real - world scenario , where arguments have to be formed for a freelychosen topic from material that is mined from the web . Still , our topic - independent approach defines a first substantial step in this direction .", "entities": []}
{"text": "IMPLI : Investigating NLI Models ' Performance on Figurative Language", "entities": []}
{"text": "Jamie was pissed off this afternoon . Jamie was irritated this afternoon There 's a marina down in the docks . There 's a marina down under scrutiny .", "entities": []}
{"text": "The hearts of men were softened . The men were made kindler and gentler . The gun kicked into my shoulder . The mule kicked into my shoulder . Metaphors involve linking conceptual properties of two or more domains , and are known to be pervasive in everyday language ( Lakoff and Johnson , 1980 ; Stefanowitsch and Gries , 2008 ; Steen et al , 2010 ) . Recent work has shown that these types of figurative language are impactful across a broad array of NLP tasks ( see 2.1 ) . Large - scale pre - training and transformer - based architectures have yielded increasingly powerful language models ( Vaswani et al , 2017 ; Devlin et al , 2019 ; Liu et al , 2019 ) . However , relatively little work has explored these models ' representations of figurative and creative language . NLI datasets have widely been used for evaluating the performance of language models ( Dagan et al , 2006 ; Bowman et al , 2015a ; Williams et al , 2018 ) , but there are insufficient figurative language datasets in which a literal sentence is linked to a corresponding figurative counterpart that are large enough to be suitable for evaluating NLI . Due to the creative nature of human language , creating a dataset of diverse , high - quality literal / figurative pairs is time - consuming and difficult . To address this gap , we build a new English dataset of paired expressions designed to be leveraged to explore model performance via NLI . Our dataset , IMPLI ( Idiomatic / Metaphoric Paired Language Inference ) , is comprised of both silver pairs , which are built using semi - automated methods ( 3.1 ) , as well as hand - written gold pairs ( 3.4 ) , crafted to reflect both entailment and nonentailment scenarios . Each pair consists of a sentence containing a figurative expression ( idioms / metaphors ) and a literal counterpart , designed to be either entailed or non - entailed by the figurative expression ( Table 1 shows some examples ) . Our contribution thus consists of three key parts : We create a new IMPLI dataset consisting of 24 , 029 silver and 1 , 831 gold sentence pairs consisting of idiomatic and metaphoric phrases that result in both entailment and nonentailment relationship ( see Table 2 ) . We evaluate language models in an NLI setup , showing that metaphoric language is surprisingly easy , while non - entailing idiomatic relationships remain extremely difficult . We evaluate model performance in a number of experiments , showing that incorporating idiomatic expressions into the training data is less helpful than expected , and that idioms that can occur more in more flexible syntactic contexts tend to be easier to classify .", "entities": []}
{"text": "Our IMPLI dataset is built from idiomatic and metaphoric sentences paired with entailing and nonentailing counterparts , from both silver pairs ( 3.1 ) and manually written sentences ( 3.4 ) . For our purposes , we follow McCoy et al ( 2019 ) in conflating the neutral and contradiction categories into a nonentailment label . We then label every pair as either entailment ( ) or non - entailment ( ) . Due to the difficult nature of the task and to avoid issues with crowdsourcing ( Bowman et al , 2020 ) , we employed expert annotators . We used two fluent English speakers , both graduate students in linguistics with strong knowledge in figurative language , paid at a rate of $ 20 / hr . For each method below , we ran pilot studies , incorporated annotator feedback and iteratively assessed the viability of identifying and generating appropriate expressions . As the annotators were working on generating new expressions , agreement was not calculated : we instead assessed the quality of the resulting expressions ( see Section 3.3 ) . Table 2 contains an overview of the different entailment and non - entailment types collected ( Detail examples are also provided in Appendix D ) .", "entities": []}
{"text": "First , we explore a method for generating silver pairs using annotators to create phrase definitions which can be inserted automatically into relevant contexts , yielding a large number of possible entailment and non - entailment pairs that differ only with regard to the relevant phrase . Our procedure hinges on a key assumption : for any given figurative phrase , we can generate a contextually independent literal paraphrase . We then replace the original expression with the literal paraphrase , following the assumption that the figurative expression necessarily entails its literal paraphrase : He 's stuck in bed , which is his hard cheese . He 's stuck in bed , which is his bad luck . Conversely , in contexts where the original phrase is used literally , replacing it with the literal paraphrase should yield a non - entailment relation . Switzerland is famous for six cheeses , sometimes referred to as hard cheeses . The sailors all worked under scrutiny . The sailors all worked in the docks Switzerland is famous for six cheeses , sometimes referred to as bad luck .", "entities": []}
{"text": "As a second method for generating non - entailment pairs , we asked annotators to write novel , adversarial definitions for IEs . Given a particular phrase , they were instructed to invent a new meaning for the IE that was not entailed by the true meaning , but which seemed reasonable presuming they had never heard the original IE . Some examples of this process are shown in Table 3 . We then replace these adversarial definitions into figurative sentences from the corpora . This yields pairs where the premise is an idiom used figuratively , and the hypothesis is a sentence that attempts to rephrase the idiom literally , but does so incorrectly , thus yielding non - entailments ( Figure 2 ) .", "entities": []}
{"text": "Metaphors are handled in a similar way : we start with a collection of minimal metaphoric expressions ( MEs ) . These are subject - verb - object and adjective - noun constructions from Tsvetkov et al ( 2014 ) . Each is annotated as being either literal or metaphoric , along with an example sentence . We passed these MEs directly to annotators , who were then instructed to replace a word in the ME so that it would be considered literal in a neutral context . They ran through the airport to board their flight .", "entities": []}
{"text": "In implementing and analyzing this procedure , we noted a number of practical issues . First , a large number of the MEs provided are actually idiomatic or proverbial : the focus word does not actually contribute to the metaphor , but rather the entire expression is necessary . Similarly , we found that replacing individual parts of MEs is often insufficient to fully remove the metaphoric meaning . We iterated over possible solutions to circumvent these issues and found that it is best to simply skip instances for which a replacement does not yield a feasible literal interpretation .", "entities": []}
{"text": "To create gold pairs , annotators were given a figurative sentence along with the focus of the figurative expression : for idioms , this is the IE ; for metaphors , the focus word of the metaphor . For idioms , we used the MAGPIE dataset to collect contextually figurative expressions . For metaphors , we collected metaphoric sentences from the VUA Metaphor Corpus ( Steen et al , 2010 ) , the metaphor dataset of ( Mohammad et al , 2016 ) , and instances from the Gutenberg poetry corpus ( Jacobs , 2018 ) annotated for metaphoricity ( Chakrabarty et al , 2021b ; Stowe et al , 2021 ) . Annotators were instructed to rewrite the sentence literally . This was done by removing or rephrasing the figurative component of the sentence . This yields gold standard paraphrases for idiomatic and metaphoric contexts . We then asked annotators to write non - entailed hypotheses for each premise . They were encour - aged to keep as much of the original utterance as possible , ensuring high lexical overlap , while removing the main figurative element of the sentence . For idioms , this comes from adding or adjusting words to force a literal reading of the idiom : The old girl finally kicked the bucket . The girl kicked the bucket on the right . For metaphors , this typically involves keeping the same phrasing while adapting the sentence to have a different , non - metaphoric meaning . You must adhere to the rules . You must adhere the rules to the wall .", "entities": []}
{"text": "Previous work in NLI has employed the technique of replacing words in the literal sentences with their antonyms to yield non - entailing pairs ( Chakrabarty et al , 2021a ) . We replicate this process for idioms : for the manually elicited definitions , we replace key words as determined by annotators with their antonyms . This yields sentences which negate the original figurative meaning and are thus suitable non - entailment pairs . Previous work found this antonym replacement for figurative language remains relatively easy for NLI systems , which we can additionally explore with regard to idioms . These manual annotations provide a number of concrete benefits . First , they are not restricted to individual words or phrases ( excluding antonyms ) : the figurative components can be rewritten freely , allowing for diverse , interesting pairs . Second , they are written by experts , ensuring higher quality than the automatic annotations , which may be noisy .", "entities": []}
{"text": "Figure 6 shows correlations between ICE scores ( determined by frequency of occurences of a given IE outside of its normal form ) and roberta - base model performance on that IE .", "entities": []}
{"text": "It would be good to roll in a difficult situation all over . Pour in the soup . Pour in trouble . There 's a marina down in the docks . There 's a marina down under scrutiny . ( The mule kicked back into my shoulder . This was conveniently encapsulated on the first try . This was conveniently encapsulated in the first battle . On their tracks his eyes were fastened . On their tracks his hands were fastened .", "entities": []}
{"text": "In this section , we first describe representing a query , sentence and document using local and distributed representation schemes . We further describe enhanced query - document ( query - title and query - content ) and query - sentence interactions to compute query - aware document or sentence representations for Task - 1 and Task - 2 , respectively . Finally , we discuss the application of supervised neural topic modeling in ranking documents for task 1 and introduce unsupervised and supervised sentence rankers for Task - 2 .", "entities": []}
{"text": "The RDoC Task - 2 aims at extracting the most relevant sentence from each of the PubMed abstract for the corresponding RDoC construct . Each abstract consists of title t and sentences s with an RDoC construct q. To address RDoc Task - 2 , we first compute multi - view representation : BoW , TF - IDF and QAR ( i.e. , \u03a6 q ( s j ) ) for each sentence s j in an abstract d. On other hand , we compute ESR representation for RDoC construct ( query q ) and title t of the abstract d to obtain q and t , respectively . Figure 2 and section 3.1 describe the computation of these representations . We then use the representations ( \u03a6 q ( s j ) , t and q ) to compute a relevance scores of a sentence s j relative to q and/or t via unsupervised and supervised ranking schemes , discussed in the following section .", "entities": []}
{"text": "As shown in Figure 2 , we first extract representations : \u03a6 q ( s j ) , t and q for the sentence s j query q and title t. During ranking sentences within an abstract for the given RDoC construct q , we also consider title t in computing the relevance score for each sentence relative to q and t. It is inspired from the fact that the title often contains relevant terms ( or words ) appearing in sentence ( s ) of the document ( or abstract ) . On top , we observe that q is a very short text and non - descriptive , leading to minimal text overlap with s. We compute two relevance scores : r q and r t for a sentence s j with respect to a query q and title t , respectively . r q = sim ( q , \u03a6 q ( s j ) ) and r t = sim ( t , \u03a6 q ( s j ) ) Now , we devise two ways to combine the rele - vance scores r q and r t in unsupervised paradigm : version1 : r unsup 1 = r q r q + r t r t Observe that the relevance scores are weighted by itself . However , the task - 2 expects a higher importance to the relevance score r q over q t . Therefore , we coin the following weighting scheme to give higher importance to r q only if it is higher than r t otherwise we compute a weight factor r t for r t . version2 : r unsup 2 = r q r q + r t r t where r t is compute as : r t = ( r t > r q ) | r t \u2212 r q | The relevance score r unsup 2 is effective in ranking sentences when a query and sentence does not overlap . In such a scenario , a sentence is scored by title , penalized by a factor of | r t \u2212 r q | . At the end , we obtain a final relevance score r unsup f for a sentence s j by summing the relevance scores of BM25 - Extra and r unsup 1 or r unsup 2 .", "entities": []}
{"text": "Table 7 : RDoC Task - 2 analysis : This table shows that the most relevant sentence predicted using reRank ( BM25 - Extra ) is actually not a relevant sentence , but Ensemble { # 1 , # 2 , # 4 } ( Table 5 ) predicts the correct sentence as the most relevant . els ( except [ # 3 ] ) outperform tranditional ranking models , e.g. , reRank ( BM25 - Extra ) in terms of query - document relevance score .", "entities": []}
{"text": "Table 7 shows that the most relevant sentence predicted by reRank ( BM25 - Extra ) is actually a non - relevant sentence . But an ensemble of predictions from both unsupervised and supervised ranker models correctly predicts the relevant sentence . This suggests that complementary knowledge of different models is able to capture the relevance of sentences on different scales and majority voting among them is , evidently , a robust sentence ranking technique .", "entities": []}
{"text": "This research was supported by Bundeswirtschaftsministerium ( bmwi.de ) , grant 01MD19003E ( PLASS ( plass.io ) ) at Siemens AG - CT Machine Intelligence , Munich Germany .", "entities": []}
{"text": "Invited Speaker", "entities": []}
{"text": "This special theme considers the marking of information quality in discourse , i.e. , annotations that mark how the speaker / writer expresses assessments . These assessments may be explicit and/or implicit in discourse , and may reflect positions , beliefs , opinions , appraisals and/or assessments about written or spoken propositions , for example , how a politician shows in discourse the degree of truthfulness in one of his / her electoral promises , or how a reporter shows his / her degree of belief in what the politician stated . This might include the annotation of devices such as hedges ( \" Donald claims that the crowd size , if you can really trust him to measure it , was enormous . \" ) , committed belief ( \" The winners of the contest will be announced tomorrow . \" ) or attitudes ( \" It is with great sadness that we have learnt about the death of 6 people in the accident . \" ) .", "entities": []}
{"text": "Workshop chairs", "entities": []}
{"text": "Hybrid Code Networks : practical and efficient end - to - end dialog control with supervised and reinforcement learning", "entities": []}
{"text": "End - to - end learning of recurrent neural networks ( RNNs ) is an attractive solution for dialog systems ; however , current techniques are data - intensive and require thousands of dialogs to learn simple behaviors . We introduce Hybrid Code Networks ( HCNs ) , which combine an RNN with domain - specific knowledge encoded as software and system action templates . Compared to existing end - toend approaches , HCNs considerably reduce the amount of training data required , while retaining the key benefit of inferring a latent representation of dialog state . In addition , HCNs can be optimized with supervised learning , reinforcement learning , or a mixture of both . HCNs attain stateof - the - art performance on the bAbI dialog dataset ( Bordes and Weston , 2016 ) , and outperform two commercially deployed customer - facing dialog systems .", "entities": []}
{"text": "Task - oriented dialog systems help a user to accomplish some goal using natural language , such as making a restaurant reservation , getting technical support , or placing a phonecall . Historically , these dialog systems have been built as a pipeline , with modules for language understanding , state tracking , action selection , and language generation . However , dependencies between modules introduce considerable complexity - for example , it is often unclear how to define the dialog state and what history to maintain , yet action selection relies exclusively on the state for input . Moreover , training each module requires specialized labels . * Currently at JPMorgan Chase Recently , end - to - end approaches have trained recurrent neural networks ( RNNs ) directly on text transcripts of dialogs . A key benefit is that the RNN infers a latent representation of state , obviating the need for state labels . However , end - to - end methods lack a general mechanism for injecting domain knowledge and constraints . For example , simple operations like sorting a list of database results or updating a dictionary of entities can expressed in a few lines of software , yet may take thousands of dialogs to learn . Moreover , in some practical settings , programmed constraints are essential - for example , a banking dialog system would require that a user is logged in before they can retrieve account information . This paper presents a model for end - to - end learning , called Hybrid Code Networks ( HCNs ) which addresses these problems . In addition to learning an RNN , HCNs also allow a developer to express domain knowledge via software and action templates . Experiments show that , compared to existing recurrent end - to - end techniques , HCNs achieve the same performance with considerably less training data , while retaining the key benefit of end - to - end trainability . Moreover , the neural network can be trained with supervised learning or reinforcement learning , by changing the gradient update applied . This paper is organized as follows . Section 2 describes the model , and Section 3 compares the model to related work . Section 4 applies HCNs to the bAbI dialog dataset ( Bordes and Weston , 2016 ) . Section 5 then applies the method to real customer support domains at our company . Section 6 illustrates how HCNs can be optimized with reinforcement learning , and Section 7 concludes .", "entities": []}
{"text": "Broadly there are two lines of work applying machine learning to dialog control . The first decomposes a dialog system into a pipeline , typically including language understanding , dialog state tracking , action selection policy , and language generation ( Levin et al , 2000 ; Singh et al , 2002 ; Williams and Young , 2007 ; Williams , 2008 ; Hori et al , 2009 ; Lee et al , 2009 ; Griol et al , 2008 ; Young et al , 2013 ; Li et al , 2014 ) . Specifically related to HCNs , past work has implemented the policy as feed - forward neural networks , trained with supervised learning followed by reinforcement learning . In these works , the policy has not been recurrent - i.e. , the policy depends on the state tracker to summarize observable dialog history into state features , which requires design and specialized labeling . By contrast , HCNs use an RNN which automatically infers a representation of state . For learning efficiency , HCNs use an external lightweight process for tracking entity values , but the policy is not strictly dependent on it : as an illustration , in Section 5 below , we demonstrate an HCNbased dialog system which has no external state tracker . If there is context which is not apparent in the text in the dialog , such as database status , this can be encoded as a context feature to the RNN . The second , more recent line of work applies recurrent neural networks ( RNNs ) to learn \" endto - end \" models , which map from an observable dialog history directly to a sequence of output words ( Sordoni et al , 2015 ; Shang et al , 2015 ; Vinyals and Le , 2015 ; Yao et al , 2015 ; Serban et al , 2016 ; Li et al , 2016a , c ; Luan et al , 2016 ; Xu et al , 2016 ; Li et al , 2016b ; Mei et al , 2016 ; . These systems can be applied to task - oriented domains by adding special \" API call \" actions , enumerating database output as a sequence of tokens ( Bordes and Weston , 2016 ) , then learning an RNN using Memory Networks ( Sukhbaatar et al , 2015 ) , gated memory networks ( Liu and Perez , 2016 ) , query reduction networks ( Seo et al , 2016 ) , and copyaugmented networks ( Eric and Manning , 2017 ) . In each of these architectures , the RNN learns to manipulate entity values , for example by saving them in a memory . Output is produced by generating a sequence of tokens ( or ranking all possible surface forms ) , which can also draw from this memory . HCNs also use an RNN to accumulate dialog state and choose actions . However , HCNs differ in that they use developer - provided action templates , which can contain entity references , such as \" < city > , right ? \" . This design reduce learning complexity , and also enable the software to limit which actions are available via an action mask , at the expense of developer effort . To further reduce learning complexity in a practical system , entities are tracked separately , outside the the RNN , which also allows them to be substituted into action templates . Also , past end - to - end recurrent models have been trained using supervised learning , whereas we show how HCNs can also be trained with reinforcement learning .", "entities": []}
{"text": "The RNN was specified using Keras version 0.3.3 , with back - end computation in Theano version 0.8.0.dev0 ( Theano Development Team , 2016 ; Chollet , 2015 ) . The Keras model specification is given below . The input variable obs includes all features from Figure 1 step 6 except for the previous action ( step 18 ) and the action mask ( step 6 , top - most vector ) . Model sizes are given in Table 3 . Example dialogs are given below for each of the 5 dialog systems . For space and readability , the entity tags that appear in the user and system sides of the dialogs have been removed - for example , Call < name > Joan</name > is shown as Call Joan .", "entities": []}
{"text": "This paper proposes to adapt self - attention to discourse level for modeling discourse elements in argumentative student essays . Specifically , we focus on two issues . First , we propose structural sentence positional encodings to explicitly represent sentence positions . Second , we propose to use inter - sentence attentions to capture sentence interactions and enhance sentence representation . We conduct experiments on two datasets : a Chinese dataset and an English dataset . We find that ( i ) sentence positional encodings can lead to a large improvement for identifying discourse elements ; ( ii ) a structural relative positional encoding of sentences shows to be most effective ; ( iii ) inter - sentence attention vectors are useful as a kind of sentence representation for identifying discourse elements .", "entities": []}
{"text": "Attention mechanism was first introduced by ( Bahdanau et al , 2015 ) in the encoder - decoder framework . Attention has the ability to learn important regions within a context and has been widely adopted in deep learning . Liu and Lapata ( 2018 ) proposed a structured attention mechanism to derive a tree over a text , akin to an RST discourse tree . Ferracane et al ( 2019 ) evaluated the model , however , found multiple negative results . Attention mechanism has also been applied for RST parsing and its applications ( Li et al , 2016 ; Ji and Smith , 2017 ; Huber and Carenini , 2019 ) but it is mostly used for capturing local semantic interactions .", "entities": []}
{"text": "The construction of the Chinese Dataset mainly follows the definition and taxonomy of discourse elements proposed by Burstein et al ( 2003 ) . Specifically , we consider the following discourse elements : Introduction The role of introduction is to introduce background or attract readers ' attention before making claims .", "entities": []}
{"text": "We presented a method DiSA to identify discourse elements in argumentative student essays by explicitly modeling structural positions and inter - sentence relations . The structural positional encoding considers relative positions of the sentence and its paragraph . Moreover , we use inter - sentence attention vectors to capture sentence relations in content and function . Experiments on a Chinese dataset and an English dataset show that ( i ) although it is simple , the positional encoding largely improves the performance . This indicates that modeling structural positions is feasible and important to analyze the role of sentences ; ( ii ) discourse elements could be better identified with the help of inter - sentence attention vectors , especially the minority ones and the ones that have distinct relation patterns to other sentences . In future , we plan to evaluate DiSA on other discourse analysis tasks .", "entities": []}
{"text": "This work is supported by the National Natural Science Foundation of China ( Nos . 61876113 , 61876112 ) , Beijing Natural Science Foundation ( No . 4192017 ) , Support Project of Highlevel Teachers in Beijing Municipal Universities in the Period of 13th Five - year Plan ( CIT&TCD20170322 ) and Capital Building for Sci - Tech Innovation - Fundamental Scientific Research Funds . Lizhen Liu is the corresponding author .", "entities": []}
{"text": "Instead of treating attention as a by - product of model training , the following work explored how machine / human can consume attention for model improvement or explanation , respectively . Machine / human may also provide supervision . We thus categorize existing work by machine / human consumption and supervision . Our work falls into human providing supervision ( with machine augmenting supervision ) for machine consumption .", "entities": []}
{"text": "Overview of Open - Source Morphology Development for the Komi - Zyrian Language : Past and Future", "entities": []}
{"text": "This study describes the on - going development of the finite - state description for an endangered minority language , Komi - Zyrian . This work is located in the context where large written and spoken language corpora are available , which creates a set of unique challenges that have to be , and can be , addressed . We describe how we have designed the transducer so that it can benefit from existing open - source infrastructures and therefore be as reusable as possible . \u0422\u0430\u0439\u04e7 \u0433\u0438\u0436\u04e7\u0434\u044b\u043d \u0441\u0451\u0440\u043d\u0438 \u043c\u0443\u043d\u04e7 \u043a\u0430\u043d\u043c\u0443 \u043a\u043e\u043c\u0438 \u043a\u044b\u0432 \u0442\u0435\u0445\u043d\u043e\u043b\u043e\u0433\u0438\u044f \u0439\u044b\u043b\u044b\u0441\u044c , \u043a\u04e7\u043d\u0456 \u0441\u0435\u0442\u04e7\u043c\u0430\u04e7\u0441\u044c \u043a\u043e\u043c\u0438 \u043c\u043e\u0440\u0444\u043e\u043b\u043e\u0433\u0438\u044f\u043b\u044b \u043f\u043e\u043c\u044b\u0441\u044c - \u043f\u043e\u043c\u04e7\u0434\u0437 \u0430\u0432\u0442\u043e\u043c\u0430\u0442 . \u0423\u0434\u0436\u044b\u0441 \u0441\u044d\u0442\u0448\u04e7\u043c \u043a\u043e\u043d\u0442\u0435\u043a\u0441\u0442\u044b\u043d , \u043a\u04e7\u043d\u0456 \u044b\u0434\u0436\u044b\u0434 \u0433\u0438\u0436\u0430\u043d \u0434\u0430 \u0441\u0451\u0440\u043d\u0438\u0441\u0438\u043a\u0430\u0441 \u043a\u043e\u0440\u043f\u0443\u0441\u044a\u044f\u0441 \u0431\u043e\u0441\u044c\u0442\u0430\u043d\u043d\u043e\u0433 . \u0422\u0430 \u0432\u04e7\u0441\u043d\u0430 \u0447\u0443\u0436\u04e7\u043d\u044b \u0442\u043e\u0440\u0439\u04e7\u043d \u044e\u0430\u043b\u04e7\u043c\u044a\u044f\u0441 , \u043a\u043e\u0434\u043b\u044b \u0432\u044b\u043b\u044c \u0432\u043e\u0447\u0430 \u043a\u044b\u0432\u044a\u044f\u0441 \u043a\u043e\u043b\u0430\u043d\u0430\u04e7\u0441\u044c . \u041f\u0435\u0442\u043a\u04e7\u0434\u043b\u0430\u043c , \u043c\u044b\u0439 \u044d\u043c \u043a\u044b\u0434\u0437\u0438 \u0430\u0434\u0434\u0437\u044b\u043d\u044b \u043a\u043e\u043b\u0430\u043d\u0430 \u0432\u043e\u0447\u0430 \u043a\u044b\u0432\u044a\u044f\u0441 . \u0421\u0435\u0440\u043f\u0430\u0441\u0430\u043b\u0430\u043c \u0430\u043d\u0430\u043b\u0438\u0437\u0430\u0442\u043e\u0440\u0430\u0432\u0442\u043e\u043c\u0430\u0442\u043b\u044b\u0441\u044c \u0441\u04e7\u0432\u043c\u04e7\u0434\u04e7\u043c \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u0434\u0430 \u0432\u04e7\u0437\u0439\u04e7\u043c\u043d\u044b\u043c \u04e7\u0442\u043b\u0430\u04e7\u0434\u043d\u044b \u0430\u043d\u0430\u043b\u0438\u0437\u0430\u0442\u043e\u0440\u0441\u04e7 \u043f\u0430\u0441\u044c\u043a\u044b\u0434\u0434\u0436\u044b\u043a \u0432\u043e\u0441\u0441\u044f \u043a\u043e\u0434\u044a\u044f\u0441\u0430 \u04e7\u0442\u0443\u0432\u0442\u0435\u0447\u0430\u0441\u04e7\u0438\u043d\u0444\u0440\u0430\u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0430\u04e7 , \u043c\u0435\u0434\u044b\u043c \u0443\u0434\u0436\u044b\u0441 \u0443\u043d\u0430\u043b\u0430\u0437\u0434\u043e\u0440\u044a\u044f\u0441\u044b\u043d \u0432\u04e7\u0434\u0438\u0442\u0447\u044b\u043d\u044b .", "entities": []}
{"text": "This study discusses open - source morphology development , which has greatly benefited from opensource projects most notably achievements attributed to the GiellaLT infrastructure ( Moshagen et al , 2014 ) , i.e. Giellatekno & Divvun at the Norwegian Arctic University in Troms\u00f8 , Norway . Specifically we discuss the infrastructure for the Komi - Zyrian language . We describe the work done up until now , and delineate some of the tasks we deem necessary in the future . There are features of Komi morphosyntax that need special attention , in regard to both of their linguistic and computational descriptions . This contribution aims to bring that discussion forward , and delineate the current status of the work . Rueter ( 2000 ) describes the initial creation of the transducer , and the work discussed here continues that same undertaking , essentially providing an update of the changes done in the last decade , and a plan for the future . The transducer is available on GitHub for Komi - Zyrian.\u00b9 The nightly builds are available through a Python library called Ural - icNLP\u00b2 ( H\u00e4m\u00e4l\u00e4inen , 2019 ) . Easy and efficient access to the traducers and their lexical materials has been the main designing principle , and we consider current approach very successful . Komi - Zyrian has a growing representation in online corpora . There is a large written corpus that is accessible online\u00b3 ; it has been created by FU - Lab in Syktyvkar . The Giellatekno infrastructure provides a Korp implementation ( Ahlberg et al , 2013 ) hosting numerous Uralic Wikipedia corpora , among which Komi can also be found\u2074. At the Language Bank of Finland , parallel Bible corpora are available with possibilities for comparing different translations ( Rueter and Axelson , 2020 ) . While literary language often reflects astute professional language users , social media provides written language that may be more closely related to the vernacular , this type of Komi is found with minority languages of the adjacent Volga - Kama region\u2075 and as described in Arkhangelskiy ( 2019 ) . In a simi - lar vein , a Spoken Komi corpus containing mainly Izhma dialect has been created in a Kone Foundation funded research project ( Blokland et al , 2014 ( Blokland et al , - 2016 , and it is also available online for community and research access.\u2076 Written and spoken language corpora are different in many ways , but together they form a large and representative description of the Komi language . Thereby they both need to be accounted for when the transducer is further developed . Electronic corpora have an important role in the research of Komi in general , and their significance most certainly will only grow when access and practices improve ( for discussion about the use of electronic corpora , see \u0424\u0435\u0434\u0438\u043d\u0430 , 2019 ; \u0427\u0443\u043f\u0440\u043e\u0432 , 2018 ; \u0411\u043b\u043e\u043a\u043b\u0430\u043d\u0434 et al , 2014 ) . There are also numerous dialect materials in Komi , and their progressing digitization gives us access to an increasing number of materials hitherto unavailable in digital format . When this process advances and we inevitably encounter more dialectal texts , we must also consider wider dialectal features of Komi when we develop the transducer . Additionally , as there are two main Komi varieties with written standards and their dialects , Zyrian and Permyak , we must acknowledge that infrastructures for these languages can not be developed in isolation , but rather that both language variants must be taken into consideration in different ways ( Rueter et al , 2020c ) . At the same time , the respective written standards have needs for their own tools and resources that are still independent , so the whole question of how to best handle pluricentric language varieties such as Komi still needs additional planning . The study is structured so that we first describe the work that has been done for modeling the morphosyntax of the Komi - Zyrian language . Then we discuss individual features and their role in the description , and aim to illustrate the types of challenges they present . As we believe that computational modeling of the language is directly connected to the linguistic description itself , we also discuss different phenomena and the ways our description is directly connected to the grammar .", "entities": []}
{"text": "Komi regular morphology affects word forms in several parts of speech . In addition to verbal conjugation and nominal declension , there is an abundance of regular morpheme - sememe alignment in derivation . Whereas verbal conjugation is , indeed , limited to the indicative ( in four synthetic tenses ) and imperative moods , the complex noun - phrase head is associated with the categories of number ( singular and plural ) , possessive marking for three persons and two numbers as well as nearly thirty syntactic entity markers or cases . Regular derivation can be observed in aspect , mediopassive and causative marking of verbs , as well as comparative and diminutive marking of nominals . There is a plethora of single - syllable nouns and derivational suffixes , and , at times , the boundary between compounding and derivation becomes obscure .", "entities": []}
{"text": "As mentioned above , there are nearly thirty syntactic entity markers or cases associated with complex noun phrases . The distinction drawn here of cases versus derivations lies in the complexity of the noun phrase , i.e. compatibility with the category of number or presence of modifiers has been underlined as a possible boundary ( see Rueter , 2010 , 74 - 75 ; cf . Ylikoski ( 2020 ) ) . If a denominal adverbial derivation does not take adjectival or determiner modifiers , there is no syntactic need to distinguish it from other opaque adverbials . On the contrary , it may be noted , syntactic elements that can take this kind of modifiers should be classified according to their syntactic merits . ( The term CASE should not be reguarded as a title of estate but as a useful indication of syntactic class membership . ) Here , we will further note that according to the SIL Glossary of Linguistic Terms\u2078 case is defined as a grammatical category determined by the syntactic or semantic function of a noun or pronoun . If we apply this to a regular morphological description of the Komi languages , we may choose to distinguish between derivational endings applied to simple NP heads and inflectional endings applied to complex NP heads . By distinguishing these two varieties of inflection , we can arrive at a syntactic criterion for classifying different types of inflection , whereas the complex NP , which also takes marking for number , might be readily integrated into the enumeration of nominal modifiers , i.e. cases . For nearly one and a half centuries , the 16 and 1 dependent cases as defined by Castr\u00e9n ( 1844 ) have represented the canonical cases addressed in grammars of the Komi - Zyrian language . The seventeenth case , the comitative , is addressed as a postposition , but all examples of it show it as integrated morphology in the noun . \u041d\u0435\u043a\u0440\u0430\u0441\u043e\u0432\u0430 ( 2000 ) ( ' The Modern Komi Language ' , \u00d6KK ) , published in 2000 broke with this tradition by including a set of compounded cases ( seven ) . The 26 cases shown by the latest Komi grammar , may be further augmented to 29 by introducing the PROPRIETIVE , ABESSIVE and LOCATIVE cases , in - a , \u0442\u04e7\u043c and - \u0441\u0430 , respectively . The TEMPORAL in - \u0441\u044f might , as a function , be simply attributed to the already existing COMPARATIVE case . Similar questions of case definition have been treated by one of the authors , Rueter ( 2010 ) , where he regards syntactic entity complexity as sufficient grounds for casehood , ( see also Ylikoski , 2020 ) . Tauli ( 1956 ) , it should be noted , provides numerous references to researchers dealing with affixes , inclusive derivation and case , there does not seem to be any standards for distinction between case and derivation . The Komi - Zyrian PROPRIETIVE refered to also as a nomen possessoris suffix \u0430 , which occurs as a \" comitative \" ( Tauli , 1956 ) , provides a challenge for the those wishing to distinguish Kom proprietive - \u0430 , comitative - \u043a\u04e7\u0434 and instrumental - \u04e7\u043d . Not unlike the PROPRIETIVE , the ABESSIVE , LOCA - TIVE and even the temporal function of the COMPAR - ATIVE case are almost entirely limited in use to the adnominal range . The ABESSIVE has a predicative counterpart in the CARATIVE - \u0442\u04e7\u0433 , while the LOCA - TIVE has a predicative counterpart in the INESSIVE\u044b\u043d . Perhaps this range distinction has also played \u2078https://glossary.sil.org/term/case a part so - called case classification . The adnominal TEMPORAL marker , however , seems to have no morphological counterpart for use in the predicative .", "entities": []}
{"text": "One of the dilemmas in Komi morphosyntax is where to introduce the object of a sentence . Actual non - ambiguous accusative forms are attested for pronouns and other NP heads , but the accusative is not the only case used for indicating the object , the ZERO marker strategy is also used for this purpose . Hence , one might readily speak of object marking with the nominative . Canonic practice in the Komi grammaticography has been to include the nominative , ZERO form , as an additional accusative case form . If we introduce ZERO as an accusative case marker as well , we , essentially , be introducing ambiguity on the text on the analysis level . Komi is known for its use of singular possessive suffixes in the accusative for marking different degrees of identifiability ; zero , i.e. nominative marking , is also a possibility . When we also have the full syntactic dependency tree , the ambiguity between nominatives and unmarked accusative is resolved , as the object relation is unambiguously marked and connected to the root verb . The current solution in the morphological modeling has been to resolve all unmarked wordforms as nominatives , and to leave the nominative - accusative distinction into a later step of the analysis . None the less , we recognize this is only one of the various ways this can be analysed , and when the full analysis comes , we essentially have all the information to transform the material to match various existing traditions .", "entities": []}
{"text": "This section will investigate the ordering of morphological constituents typically associated with nominals and convey meaning associated with the categories of number , possession and case . In initial collaboration with FU - Lab , a singular set of morpheme ordering was adopted for each individual combination of possessor & case marking . Hence , it was determined that the word form \u0431\u0430\u0442\u044c\u04e7\u0439\u043b\u04e7\u043d \u00ab \u0431\u0430\u0442\u044c - \u04e7\u0439 - \u043b\u04e7\u043d ' father . N - PxSg1 - Gen ' featuring the \u04e7\u0439 marking for the first person singular possessor could be distinguished from the possessive suffix \u044b\u043c in \u0433\u043e\u0440\u0442\u04e7\u0434\u0437\u044b\u043c \u00ab \u0433\u043e\u0440\u0442 - \u04e7\u0434\u0437 - \u044b\u043c ' home . N - Ter - PxSg1 ' on the basis of complementary distribution , i.e. there was no need to label the possessive suffixes as separate entities . In later development , however , a different issue was observed in which case and possessive formatives might show varied ordering . Although this phenomenon is not as prevalent as in the Meadow and Eastern Mari language ( cf . Luutonen ( 1997 ) ) , it did merit recognition and distinction for the facilitation of further resource . The distinguishing tags strategy implemented for Meadow Mari and Hill Mari has been adapted for use with Komi - Zyrian with two tags . One tag indicates segment ordering where the possessive marker precedes the case marker ( + So / PC ) , and the other indicates the case marker precedes the possessive marker ( + So / CP ) , e.g. \u043a\u04e7\u0437\u044f\u0438\u043d\u044b\u0441\u043b\u0430\u043d\u044c \u043a\u04e7\u0437\u044f\u0438\u043d\u044b\u0441 - \u043b\u0430\u043d\u044c ' owner . N - PxSg3 - Apr ' \u043a\u04e7\u0437\u044f\u0438\u043d\u043b\u0430\u043d\u044c\u044b\u0441 \u043a\u04e7\u0437\u044f\u0438\u043d - \u043b\u0430\u043d\u044c - \u044b\u0441 ' owner . N - Apr - PxSg3 ' . In addition to this relatively infrequent type of ordering variation of cases versus possessive suffixes , there also appears to be use of the accusative possessive suffix markers for second - \u0442\u04e7 and third - \u0441\u04e7 person on noun and adjective phrase heads , where the accusative case would not be syntactically compatible . In fact , these same endings are found in connection with other parts of speech as well . It has been maintained that these morphological constituents convey discourse meaning , but there is still much to investigate and establishing tagging practices for these features will contribute to better research materials in the future .", "entities": []}
{"text": "In Komi , numerals are regularly derived to form subgroups in cardinals ZERO , ordinals - \u04e7\u0434 , distributives - \u04e7\u043d , iteratives - \u044b\u0441\u044c , ordinal iteratives - \u04e7\u0434\u044b\u0441\u044c and distributional iteratives - \u044b\u0441\u044c\u04e7\u043d ( Rueter et al , 2020c ) . As such , it is often novel or even confounding that we find the syntactic adverbial role found across languages is attributed to a regularly derived adverb \u043a\u044b\u043a\u044b\u0441\u044c ' twice ' on the Komi side , on the one hand , and a noun phrase fifty times ' \u0432\u0435\u0442\u044b\u043c\u044b\u043d\u044b\u0441\u044c ' , on the other . Like other adnominal modifiers , it should be noted , numerals may also be promoted to NP head position in instances of contextually motivated ellipsis .", "entities": []}
{"text": "We have recently moved into primarily data - driven development practice for Komi , where new lexicon and morphology is described primarily based on gaps we find through analysed language materi - als . At the same time we have developed further tests to check the validity of the output , and in the long term these approaches naturally will live on in parallel . Needless to say , using more natural texts has also forced us to take into account more spoken language and dialect phenomena , which moves the work into quite new directions , which we have already discussed partially above . After reporting our experiments with the written corpus data , we discuss our plan to integrate the dialectal materials and tags better to the currently discussed Komi analyser .", "entities": []}
{"text": "From a corpus of 1 , 415 , 210 unique word forms ( 2020 - 11 - 11 ) 520 , 180 were not recognized by the analyzer . Aside from the Russian words , apparently from quoted text , and words written entirely in upper case , the most frequent words not to be recognized by the FST seem to all involve hyphens . The use of hyphenation is best illiustrated by \u0420\u044b\u0442\u044b\u0432 - \u0412\u043e\u0439\u0432\u044b\u0432 the preposed modifier for direction ' north northwest ' ( 1377 times ) , a drawn out pronunciation \u041d\u043e - \u043e ' Well - l ' ( 1177 times ) , and the orthographic practice of adding - \u043c\u04e7\u0434 ' another ' in \u0437\u0434\u0443\u043a - \u043c\u04e7\u0434 ' yet another moment ' ( 942 times ) . Since over a third of the unique word forms had gone unrecognized , a strategy was developed for improving the model . This would be carried out for nominals initially and subsequently verbs . As described below , a very large portion of unrecognized forms involved various plurals . How they were dealt with is described below , as it illustrates well the challenges we have encountered and their possible solutions . In the Komi - Zyrian morphology there are two separate plural markers associated with nominal declension . One is the NP plural marker \u044f\u0441 and the other is the copula complement plural marker \u04e7\u0441\u044c . 20604 unrecognized word forms ended in \u044f\u0441 , and in 11441 of these the plural marker was preceded by a Cyrillic hard sign \u044a . This number was was further delimited by removing all instances of hyphenation and v followed by Cyrillic hard sign and wordfinal \u044f\u0441 . Where the hyphen may have meant compound words for simple hyphenation in the text , the removal of v meant we could automatically avoid the problem of determining whether the word stem contained the notorious l / v variation or not . Our resulting figure was 8766 . After entering 15 , 101 new stems the number of unrecognized unique word forms dropped to 422 , 227 , which was nearly a nineteen per cent improvement over the previous 520 , 180 . In the future we plan to go further through the frequency list of unknown word forms and improve the analyzer so that individual yet frequent phenomena is adequately described and addressed .", "entities": []}
{"text": "Currently the FST is designed so that dialectal elements are recognized , but they come with an additional error or dialect tag which prevents them being suggested in tools such as spellcheckers . We have also experimented with approaches where Zyrian , Permyak and Russian analysers are run on top of one another , so that unknown forms may be captured by one of the systems with appropriate language tags returned . Since some Zyrian dialectal phenomena is also present in Permyak standard language , already this solution helps to improve the coverage . Eventually , however , we consider it important that the analyser could capture nuances of individual dialects . In principle this could be accompanied with dialect specific tags , but this approach is also problematic . Many of the features are not strictly found in singular dialects , but cover larger regions . At the same time the speech of any individual is not necessarily limited to any specific variety . Moreover , we believe that further research in Komi dialect isoglosses may be necessary to exactly point for each feature where they definitely occur . Some rough areal boundaries , however , are well known and clear cut , which would make some areal tags potentially useful . Features that currently are not included are especially those found from southern and eastern Zyrian dialects , mainly because nobody has attempted to use an FST with those varieties yet . We must also recognize that Permyak and Zyrian dialects overlap in their features in various ways , and especially the creation of infrastructure that handless all Komi varieties and both standards remains a challenge .", "entities": []}
{"text": "As described in the study above , this work on Komi has been funded by Alfred Kordelin Foundation and Kone Foundation . Matias Grioni , Lo\u00efc Grobol , Normunds Gr\u016bz\u012btis , Bruno Guillaume , C\u00e9line Guillot - Barbance , Tunga G\u00fcng\u00f6r , Nizar Habash , Hinrik Hafsteinsson , Jan Haji\u010d , Jan Haji\u010d jr . , Mika H\u00e4m\u00e4l\u00e4inen , Linh H\u00e0 M\u1ef9 , Na - Rae Han , Muhammad Yudistira Hanifmuti , Sam Hardwick , Kim Harris , Dag Haug , Johannes Heinecke , Oliver Hellwig , Felix Hennig , Barbora Hladk\u00e1 , Jaroslava Hlav\u00e1\u010dov\u00e1 , Florinel Hociung", "entities": []}
{"text": "SMARTies : Sentiment Models for Arabic Target entities", "entities": []}
{"text": "We use the Arabic Opinion Target dataset developed by Farra et al ( 2015 ) , which is publicly available 1 . The data consists of 1177 online comments posted in response to Aljazeera Arabic newspaper articles and is part of the Qatar Arabic Language Bank ( QALB ) corpus ( Habash et al , 2013 ; Zaghouani et al , 2014 ) . The comments are 1 - 3 sentences long with an average length of 51 words . They were selected such that they included topics from three domains : politics , culture , and sports . Targets are always noun phrases and they are either labeled positive if a positive opinion is expressed about them and negative if a negative opinion is expressed ( as shown in Figure 1 ) . Targets were identified using an incremental process where first important entities were identified , and then entities agreed to be neutral were discarded ( the annotation does not distinguish between neutral and subjective neutral ) . The data also contains ambiguous or ' undeter - 1 www.cs.columbia.edu/~noura/Resources.html The dictator is destroying his country T T O O O O N N", "entities": []}
{"text": "This model predicts a sequence of labels S for the sequence x , S i { P ( pos ) , N ( neg ) , ( neutral ) } and each token x i is represented by a feature vector : ( f is , E i ) ; E i { T , O } Additionally , this model has the constraint : if E i = T , S i { P , N } and otherwise S i = The last constraint indicating that sentiment is either positive or negative is ensured by the training data , where we have no examples of target tokens having neutral sentiment . The two models are trained independently . Thus , if target words are already available for the data , the sentiment model can be run without training or running the target model . Otherwise , the sentiment model can be run on the output of the target predictor . The sentiment model uses knowledge of whether a word is a target and utilizes context from neighboring words whereby the entire sequence is optimized to predict sentiment polarities for the targets . An example sequence is shown in Table 1 , where the dictator is an entity target towards which the writer implicitly expresses negative sentiment . 5 Arabic Morphology and Linguistics", "entities": []}
{"text": "We experiment with the following classifiers : Maximum Entropy Classifier ( Berger et al , 1996 ) , Support Vector Machines Classifier ( Cortes and Vapnik , 1995 ) , Multilayer perceptron and Voted perceptron neural networks ( Freund and Schapire , 1999 ) and with Decision / regression tree learning ( Breiman et al , 1984 ) . We employ the following two frameworks : Brainy ( Konkol , 2014 ) and Weka ( Hall et al , 2009 ) .", "entities": []}
{"text": "The alignment of chunks is generated by the binary classification of all possible chunk pairs . If one chunk is aligned with multiple chunks in the other sentence , these chunks should be merged into one chunk . Also , impossible multiple chunks to multiple chunks alignments are generated in some cases ( e.g. two chunks from the first sentence belong a chunk in the second sentence but one of the two chunks from the first sentence belong also to a different chunk in the second sentence ) . These cases are resolved with few hand crafted rules .", "entities": []}
{"text": "This publication was supported by the project LO1506 of the Czech Ministry of Education , Youth and Sports and by Grant No . SGS - 2016 - 018 Data and Software Engineering for Advanced Applications . Computational resources were provided by the CESNET LM2015042 and the CERIT Scientific Cloud LM2015085 , provided under the programme \" Projects of Large Research , Development , and Innovations Infrastructures \" .", "entities": []}
{"text": "We investigate a new commonsense inference task : given an event described in a short free - form text ( \" X drinks coffee in the morning \" ) , a system reasons about the likely intents ( \" X wants to stay awake \" ) and reactions ( \" X feels alert \" ) of the event 's participants . To support this study , we construct a new crowdsourced corpus of 25 , 000 event phrases covering a diverse range of everyday events and situations . We report baseline performance on this task , demonstrating that neural encoder - decoder models can successfully compose embedding representations of previously unseen events and reason about the likely intents and reactions of the event participants . In addition , we demonstrate how commonsense inference on people 's intents and reactions can help unveil the implicit gender inequality prevalent in modern movie scripts .", "entities": []}
{"text": "Understanding a narrative requires commonsense reasoning about the mental states of people in relation to events . For example , if \" Alex is dragging his feet at work \" , pragmatic implications about Alex 's intent are that \" Alex wants to avoid doing things \" ( Figure 1 ) . We can also infer that Alex 's emotional reaction might be feeling \" lazy \" or \" bored \" . Furthermore , while not explicitly mentioned , we can infer that people other than Alex are affected by the situation , and these people are likely to feel \" frustrated \" or \" impatient \" . This type of pragmatic inference can potentially be useful for a wide range of NLP applications \u21e4 These two authors contributed equally .", "entities": []}
{"text": "PersonX reads PersonY 's diary to avoid doing things lazy , bored frustrated , impatient to impress their family tired , a sense of belonging impressed to be nosey , know secrets guilty , curious angry , violated , betrayed X 's intent X 's reaction Y 's reaction X 's intent X 's reaction Y 's reaction X 's intent X 's reaction Y 's reaction Figure 1 : Examples of commonsense inference on mental states of event participants . In the third example event , common sense tells us that Y is likely to feel betrayed as a result of X reading their diary . that require accurate anticipation of people 's intents and emotional reactions , even when they are not explicitly mentioned . For example , an ideal dialogue system should react in empathetic ways by reasoning about the human user 's mental state based on the events the user has experienced , without the user explicitly stating how they are feeling . Similarly , advertisement systems on social media should be able to reason about the emotional reactions of people after events such as mass shootings and remove ads for guns which might increase social distress ( Goel and Isaac , 2016 ) . Also , pragmatic inference is a necessary step toward automatic narrative understanding and generation ( Tomai and Forbus , 2010 ; Ding and Riloff , 2016 ; Ding et al , 2017 ) . However , this type of social commonsense reasoning goes far beyond the widely studied entailment tasks ( Bowman et al , 2015 ; Dagan et al , 2006 ) and thus falls outside the scope of existing benchmarks . In this paper , we introduce a new task , corpus , and model , supporting commonsense inference on events with a specific focus on modeling stereotypical intents and reactions of people , described in short free - form text . Our study is in a similar spirit to recent efforts of Ding and Riloff ( 2016 ) and Zhang et al ( 2017 ) , in that we aim to model aspects of commonsense inference via natural language descriptions . Our new contributions are : ( 1 ) a new corpus that supports commonsense inference about people 's intents and reactions over a diverse range of everyday events and situations , ( 2 ) inference about even those people who are not directly mentioned by the event phrase , and ( 3 ) a task formulation that aims to generate the textual descriptions of intents and reactions , instead of classifying their polarities or classifying the inference relations between two given textual descriptions . Our work establishes baseline performance on this new task , demonstrating that , given the phrase - level inference dataset , neural encoderdecoder models can successfully compose phrasal embeddings for previously unseen events and reason about the mental states of their participants . Furthermore , in order to showcase the practical implications of commonsense inference on events and people 's mental states , we apply our model to modern movie scripts , which provide a new insight into the gender bias in modern films beyond what previous studies have offered ( England et al , 2011 ; Agarwal et al , 2015 ; Ramakrishna et al , 2017 ; Sap et al , 2017 ) . The resulting corpus includes around 25 , 000 event phrases , which combine automatically extracted phrases from stories and blogs with all idiomatic verb phrases listed in the Wiktionary . Our corpus is publicly available . 1", "entities": []}
{"text": "One goal of our investigation is to probe whether it is feasible to build computational models that can perform limited , but well - scoped commonsense inference on short free - form text , which we refer to as event phrases . While there has been much prior research on phrase - level paraphrases ( Pavlick et al , 2015 ) and phrase - level entailment ( Dagan et al , 2006 ) , relatively little prior work focused on phrase - level inference that requires prag - matic or commonsense interpretation . We scope our study to two distinct types of inference : given a phrase that describes an event , we want to reason about the likely intents and emotional reactions of people who caused or affected by the event . This complements prior work on more general commonsense inference ( Speer and Havasi , 2012 ; Li et al , 2016 ; Zhang et al , 2017 ) , by focusing on the causal relations between events and people 's mental states , which are not well covered by most existing resources . We collect a wide range of phrasal event descriptions from stories , blogs , and Wiktionary idioms . Compared to prior work on phrasal embeddings ( Wieting et al , 2015 ; Pavlick et al , 2015 ) , our work generalizes the phrases by introducing ( typed ) variables . In particular , we replace words that correspond to entity mentions or pronouns with typed variables such as PersonX or PersonY , as shown in examples in Table 1 . More formally , the phrases we extract are a combination of a verb predicate with partially instantiated arguments . We keep specific arguments together with the predicate , if they appear frequently enough ( e.g. , PersonX eats pasta for dinner ) . Otherwise , the arguments are replaced with an untyped blank ( e.g. , PersonX eats for dinner ) . In our work , only person mentions are replaced with typed variables , leaving other types to future research .", "entities": []}
{"text": "To prune the set of events that will be annotated for intent and reaction , we ran a preliminary annotation to filter out candidate events that have implausible coreferences . In this preliminary task , annotators were shown a combinatorial list of coreferences for an event ( e.g. , PersonX punches PersonX 's lights out , PersonX punches PersonY 's lights out ) and were asked to select only the plausible ones ( e.g. , PersonX punches PersonY 's lights out ) . Each set of coreferences was annotated by 3 workers , yielding an overall agreement of \uf8ff = 0.4 . This annotation excluded 8 , 406 events with implausible coreference from our set ( out of 17 , 806 events ) .", "entities": []}
{"text": "Table 3 summarizes the performance of different encoding models on the dev and test set in terms of cross - entropy and recall at 10 predicted intents and reactions . As expected , we see a moderate improvement in recall and cross - entropy when using the more compositional encoder models ( Con - vNet and BiRNN ; both n - gram and sequence de - Table 3 : Average cross - entropy ( lower is better ) and recall @10 ( percentage of times the gold falls within the top 10 decoded ; higher is better ) on development and test sets for different modeling variations . We show recall values for PersonX 's intent , PersonX 's reaction and others ' reaction ( denoted as \" Intent \" , \" XReact \" , and \" OReact \" ) . Note that because of two different decoding setups , cross - entropy between n - gram and sequence decoding are not directly comparable . coding setups ) . Additionally , BiRNN models outperform ConvNets on cross - entropy in both decoding setups . Looking at the recall split across intent vs. reaction labels ( \" Intent \" , \" XReact \" and \" OReact \" columns ) , we see that much of the improvement in using these two models is within the prediction of PersonX 's intents . Note that recall for \" OReact \" is much higher , since a majority of events do not involve other people . Human evaluation To further assess the quality of our models , we randomly select 100 events from our test set and ask crowd - workers to rate generated intents and reactions . We present 5 workers with an event 's top 10 most likely intents and reactions according to our model and ask them to select all those that make sense to them . We evaluate each model 's precision @10 by computing the average number of generated responses that make sense to annotators . Figure 4 summarizes the results of this evaluation . In most cases , the performance is higher for the sequential decoder than the corresponding n - gram decoder . The biggest gain from using sequence decoders is in intent prediction , possibly because intent explanations are more likely to be longer . The BiRNN and ConvNet encoders consistently have higher precision than the mean - pooling with the BiRNN - seq setup slightly outperforming other models . Unless otherwise specified , this is the model we employ in further sections . ilar for all three sets of events , it is 10 % behind intent prediction on the full development set . Additionally , predicting other people 's reactions is more difficult for the model when other people are explicitly mentioned . Unsurprisingly , idioms are particularly difficult for commonsense inference , perhaps due to the difficulty in composing meaning over nonliteral or noncompositional event descriptions . To further evaluate the geometry of the embedding space , we analyze interpolations between pairs of event phrases ( from outside the train set ) , similar to the homotopic analysis of Bowman et al ( 2016 ) . For a handful of event pairs , we decode intents , reactions for PersonX , and reactions for other people from points sampled at equal inter - vals on the interpolated line between two event phrases . We show examples in Figure 5 . The embedding space distinguishes changes from generally positive to generally negative words and is also able to capture small differences between event phrases ( such as \" washes \" versus \" cuts \" ) .", "entities": []}
{"text": "Our approach decodes nuanced implications into more explicit statements , helping to identify and explain gender bias that is prevalent in modern literature and media . Specifically , our results indicate that modern movies have the bias to portray female characters as having pro - social attitudes , whereas male characters are portrayed as being competitive or pro - achievement . This is consistent with gender stereotypes that have been studied in movies in both NLP and psychology literature ( Agarwal et al , 2015 ; Madaan et al , 2017 ; Prentice and Carranza , 2002 ; England et al , 2011 ) .", "entities": []}
{"text": "We introduced a new corpus , task , and model for performing commonsense inference on textuallydescribed everyday events , focusing on stereotypical intents and reactions of people involved in the events . Our corpus supports learning representations over a diverse range of events and reasoning about the likely intents and reactions of previously unseen events . We also demonstrate that such inference can help reveal implicit gender bias in movie scripts .", "entities": []}
{"text": "We would like to thank all of the anonymous reviewers ( during ARR Oct. and ARR Dec. ) for the helpful comments . We also thank Baosong Yang and Dayiheng Liu for their instructive suggestions and invaluable help .", "entities": []}
{"text": "Semantic Content Prediction for Generating Interviewing Dialogues to Elicit Users ' Food Preferences", "entities": []}
{"text": "This study aims to generate interview dialogues that elicit information about users ' food preferences . For this purpose , we collected role - play conversations between an interviewer and a customer and constructed a corpus from the collected conversations .", "entities": []}
{"text": "Subject pairs were created with participants recruited by crowdsourcing . One subject was assigned the role of an interviewer and the other , the role of a customer . They conducted a text - based chat session in Japanese on the web . After typing an utterance and pressing the send button , the message was added to the chat screen . They were also instructed to take turns sending the messages . The participants playing as interviewers were requested to engage in conversations to elicit food preferences from customers . The participants playing as customers were asked to indicate their food preferences . We allowed the customers to respond to their real preferences or to pretend to be someone else . After the dialogue , each participant answered a questionnaire . The interviewers were asked to describe the client 's food preferences obtained from the conversation , and the dishes they would like to recommend to the customer . The customers were asked to describe the food preferences they expressed in the dialogue . They were also asked to describe the dishes they would like the interviewer to recommend to them . To create a dialogue model capable of generating responses that considered the interviewer 's dialogue strategy and dialogue history , we requested the participants to input at least 20 turns from each party and 40 turns in total . This was a task completion requirement .", "entities": []}
{"text": "Structured semantic labels were assigned to classify the interviewees ' utterances and understand their semantic content . Following the idea of structured semantic labels discussed in the Dialogue Act annotation ( Bunt et al , 2012 ) , we represented each utterance as a combination of communicative function and semantic content . More specifically , a dialog consists of messages sent by the user in the chat , and one message may include multiple sentences . We annotated each sentence in interviewer 's message . To annotate sentences in the interviewer 's message in our corpus collected in Section 3.1 , we first defined labels for communicative function and semantic content . Communicative Function : We defined 32 labels for the communicative functions based on those for SWBD - DAMSL ( Jurafsky , 1997 ) and Meguro et al ( 2014 ) . We used SWBD - DAMSL to label backward utterances , including understanding , answer , and agreement ( Appendix A ) . For self - disclosure ( SD ) and questions ( Q ) , we used labels defined in the Meguro et al ( 2014 ) as references and added new labels such as preferences , experiences , and habits . For the preference labels , we added the polarity : positive , negative , and neutral .", "entities": []}
{"text": "The semantic content expresses the meaning of a sentence , whereas the communicative function specifies the intention of a sentence , as discussed above . In our corpus , many of the interviewer 's questions referred to the name of the dish and its ingredients , tastes , recipes , and how to eat . Based on this observation , we defined semantic content as a combination of utterance objects ( e.g. , dishes and ingredients ) and their attributes ( e.g. , tastes and cooking methods ) . Figure 2 shows the structure of the semantic content and list of values for < verb > , < ObjectType > , and < ObjectAttribute > . Two examples of semantic content were assigned to an interviewer sentence . In Example A \" I ate hot curry \" in Figure 2 , the verb is \" eat \" and its object is \" hot curry \" . The object is the first argument ( argument_1 ) of the verb : eat , and the relationship between this verb and the object is expressed as a verb frame . verb frame : < verb > : We defined five verbs that are frequently used in conversations regarding food . They consider direct objects as arguments . We also defined negative forms for them by adding \" ! \" . For example , the negative form for \" like \" is \" ! like . \" In addition to these 10 verbs , \" think \" and \" other \" were added , and 12 verbs were defined in total . object - features : We defined four types of features for an object . These are ObjectType , ObjectName , Ob - jectAttribute , and AttributeValue . These are called the object features . The \" hot curry \" is an object of the verb ' eat ' . It contains a set of features : ObjectType='Dish ' , Object - Name='curry ' , ObjectAttribute='taste ' , and At - tributeValue='hot ' . We simply expressed this set as ( Dish , curry , taste , hot ) . Details of the object features are presented below . < ObjectType > : We defined 10 object types : Dish , Ingredient , and Drink . Each name begins with a capital letter . For example , \" Dish \" is assigned as the ObjectType value for curry , \" Ingredient \" for carrot , and \" Genre+Cuisine \" for Indian food . < ObjectName > : This feature indicates the name of the target object in an interviewer 's sentence . < ObjectAttribute > : As shown in Example - A in Figure 2 , there are many detailed questions and utterances about the target object , such as the taste of the food , its recipe , and how to eat it . We believe that such information is important for food preferences . To include it in the semantic content , we defined the attributes of objects with a specific ObjectType . The values of these attributes are described later in this study . < AttributeValue > : The value for the ObjectAttribute is specified in this section . A set of possible values is not defined , and the value is freely specified , as in ObjectName . For example , the ObjectType of \" hot curry \" is a ' Dish ' , and ObjectType='Dish ' can take an Objec - tAttribute ( see Figure 2 , Allowed to take < Objec - tAttribute > ? : Yes ) . Then , \" hot \" belongs to \" taste \" , which is defined as an ObjectAttribute . As a result , \" hot curry \" is interpreted as an object feature . ObjectType='Dish ' , ObjectName='curry ' , Objec - tAttribute='taste'\uff0cAttributeValue='hot ' . When the interviewer 's utterance is a question , such as a Yes / No question or WH question , the object of the question is indicated as a ' ? ' . For example , in the WH question , \" What taste of curry do you like ? \" , the AttributeValue for ObjectAt - tribute='taste ' is the target of this question . In this case , the semantic content is described as [ like , [ ( Dish , curry , taste , ? ) ] ] . For a Yes / No question , where ( default ) values are already assigned , the features are described as ObjectName+ ? and AttributeValue+ ? . For example , the semantic content for \" Do you like curry hot ? \" is described as [ like , [ ( Dish , curry , taste , hot ? ) ] ] Some sentences , such as \" Steak is good \" ( Example - B in Figure 2 ) , express an evaluation of the target object . In such a case , \" think \" is assigned to ( < verb > ) , and two arguments are used ; the object information is described in argument_1 and the evaluation in ( argument_2 ) . In this example , ar - gument_2 describes a pair of values : \" Evaluation \" and the ( < EvaluationValue > ) denoting the value of the evaluation . Thus , ( argument_2 ) is [ Evaluation , good ] .", "entities": []}
{"text": "With the goal of building a dialogue system that generates the interviewer 's appropriate questions to acquire the customer 's food preferences , we present two machine learning models in this section for communicative function prediction and semantic content generation .", "entities": []}
{"text": "Table 1 ( top ) lists the details of the corpus collected in Section 3 . Table 1 ( bottom ) shows the number of instances 4 that was used to train the CFP and SCG models . The dataset was divided into train / valid / test sets at a ratio of 7:1:2 . Although we defined 32 communication function labels in the original dataset , many of them were not frequently observed . Thus , we merged the labels whose frequency was lower than 20 % of all samples and used the seven labels listed in Table 2 in this experiment . We calculated the inter - coder reliability using three dialogues annotated by two coders . For the seven labels of communicative function , Cohen 's kappa was 0.75 , which indicated substantial agreement . For semantic content , which is a combination of verb and object - features , the percentage of agreement was 0.72 . Because we achieved a sufficient agreement level , the remaining data were annotated by either coder .", "entities": []}
{"text": "In this section , we present examples of the responses generated by our interview system . We first describe the template - based responsegeneration mechanism and then discuss examples of interview generation .", "entities": []}
{"text": "As shown in Figure 1 , the system receives outputs from the SCG and CFP models and generates the interviewer 's responses using the templatebased generation method . Suppose that the outputs from the two prediction models are as follows : communicative function label : Q - Preference - Positive semantic content : like [ SEP ] Dish [ SEP ] pasta [ SEP ] type - of [ SEP ] ? By referring to this information : communicative function='Q - Preference - Positive ' , verb='like ' , ObjectAttribute='type - of ' , and AttributeValue= ' ? ' , the system selects a template : \" { ObjectName } no Shurui de Nani ga Sukidesuka ? \" ( in English , \" What kind of { ObjectName } do you like ? \" ) . Then , a response sentence is generated by replacing { Object - Name } with the value ' pasta ' .", "entities": []}
{"text": "This work was supported by JST Moonshot R&D Grant Number JPMJMS2011 and JST AIP Trilateral AI Research ( PANORAMA project , grant no . JPMJCR20G6 ) and JSPS KAKENHI ( grant numbers JP19H01120 and JP19H04159 ) .", "entities": []}
{"text": "Provide own information and opinions about food . SD - Fact&Experience e.g. , I ate pasta yesterday . We defined the labels with reference SWBD - DAMSL ( Jurafsky , 1997 ) and Meguro et al ( 2014 ) 's dialogue acts .", "entities": []}
{"text": "Multitask Learning for Emotionally Analyzing Sexual Abuse Disclosures", "entities": []}
{"text": "Analyzing social media data of individuals discussing sexual harassment disclosures and exploitation in public spheres necessitates the need to safeguard the ethics and privacy of individuals ( Tusinski Berg , 2019 ) . We address these : Generalization We acknowledge that the limitations of the experiments might get amplified due to the highly subjective nature of this challenging problem . Therefore it would not be fair to conduct a population - centric analysis based on inferences from this work . Confidentiality Individual consent was not sought from social media users as the data was publicly available . Disclosure of sexual harassment information on public forums may have been met with public backlash and apathy . Therefore the social reputation of the accuser and the accused would be at a peril ( McDonald , 2019 ) . Hence , the authors were aware not to make any automated interventions , as any attempts to contact individuals could be seen as personally intrusive and might also repeal their social information ( Fiesler and Proferes , 2018 ) . Bias & Discrimination Social support discussions on social media platforms gave victims the liberty to describe their instances of sexual exploitation and abuse ( Manikonda et al , 2018a ) . The authors are aware of the potential inevitable sampling biases that may be present in the data . Importance has to be placed on mitigating the bias against certain minority groups , which might get amplified due to the sensitive nature of social discussions ( Hellwig and Sinno , 2017 ) .", "entities": []}
{"text": "Rajiv Ratn Shah is partly supported by the Infosys Center for AI and Center for Design and New Media at IIIT Delhi .", "entities": []}
{"text": "Overnight uses a context - free synchronous grammar to generate canonical representations for the logical forms . As can be seen in Fig . 2 , these canonical representations resemble natural language .", "entities": []}
{"text": "We provide training details and hyperparameters for all models in Appendix A. Below , we briefly explain the prompt - tuning methodology .", "entities": []}
{"text": "There are two main limitations of this work . The first is the limited analysis of the learned prompts . While concurrent work has shown that interpreting prompts is a difficult task , it is still an important consideration and left for future work ( Khashabi et al , 2021 ) . Secondly , training prompts on meaning representations requires substantially more compute than fine - tuning . This may exacerbate inequalities in regions where access to data and compute are similarly limited ( Ahia et al , 2021 ) .", "entities": []}
{"text": "For completeness , we provide all Overnight results in Table 5 .", "entities": []}
{"text": "Sentiment Tagging with Partial Labels using Modular Architectures", "entities": []}
{"text": "In the previous section we described a way of infusing information from other modules naively by simply concatenating them . But intuitively , the hidden representation from the decision module plays an important role as it is directly related to the final task we are interested in . To effectively use the information from other modules forming sub - tasks , we design a gating mechanism to dynamically control the amount of information flowing from other modules by infusing the expedient part while excluding the irrelevant part , as shown in Figure 2c . This gating mechanism uses the information from the decision module to guide the information from other modules , thus we name it as guided gating infusion , which we describe formally as follows : I seg t = \u03c3 ( W 1 h t + b 1 ) \u2297 ( W seg h seg t + b seg ) , I typ t = \u03c3 ( W 2 h t + b 2 ) \u2297 ( W typ h typ t + b typ ) , S t = W [ h t ; I seg t ; I typ t ] + b , where \u03c3 is the logistic sigmoid function and \u2297 is the element - wise multiplication . The { W 1 , W 2 , b 1 , b 2 } are the parameters of these guided gating , which are updated during the training to maximize the overall sequence labeling performance .", "entities": []}
{"text": "We used the Restaurants dataset provided by Se - mEval 2016 Task 5 subtask 1 , consisting of opinion target ( aspect ) expression segmentation , aspect classification and matching sentiment prediction . In the original task definition , the three tasks were designed as a pipeline , and assumed gold aspect labels when predicting the matching sentiment labels . Instead , our model deals with the challenging end - to - end setting by casting the problem as a sequence labeling task , labeling each aspect segment with the aspect label and sentiment polarity 2 .", "entities": []}
{"text": "Our modular architecture is a natural fit for learning with partial labels . Since the modular architecture decomposes the final task into sub - tasks , the absence of certain partial labels is permitted . In this case , only the module corresponding to the available partial labels will be updated while the other parts of the model stay fixed . This property can be exploited to reduce the supervision effort by defining semi - supervised learning protocols that use partial - labels when the full labels are not available , or too costly to annotate . E.g. , in the target sentiment task , segmentation labels are significantly easier to annotate . To demonstrate this property we conducted two sets of experiments . The first investigates how the decision module can effectively integrate the knowledge independently learned by sub - tasks modules using different partial labels . We quantify this ability by providing varying amounts of full labels to support the integration process . The second set studies the traditional semi - supervised settings , where we have a handful of full labels , but we have a larger amount of partial labels . Modular Knowledge Integration The modular architecture allows us to train each model using data obtained separately for each task , and only use a handful of examples annotated for the final task in order to integrate the knowledge learned by each module into a unified decision . We simulated these settings by dividing the training data into three folds . We associated each one of the first two folds with the two sub - task modules . Each one of the these folds only included the partial labels relevant for that sub - task . We then used gradually increasing amounts of the third fold , consisting of the full labels , for training the decision module . Fig . 3 describes the outcome for targetsentiment , comparing a non - modular model using only the full labels , with the modular approach , which uses the full labels for knowledge integration . Results show that even when very little full data is available results significantly improve . Additional results show the same pattern for subjective phrase identification and classification are included in the Appendix .", "entities": []}
{"text": "Partially - labeled data can be cheaper and easier to obtain , especially for low - resource languages . In this set of experiments , we model these settings over the target - sentiment task . The results are summarized in Fig . 4 . We fixed the amount of full labels to 20 % of the training set , and gradually increased the amount of partially labeled data . We studied adding segmentation and type separately . After the model is trained in this routine , it was tested on predicting the full labels jointly on the test set .", "entities": []}
{"text": "In our final analysis we considered a novel domain - adaptation settings , where we have a small amount of fully labeled in - domain data from aspect sentiment and more out - of - domain data from target sentiment . However unlike the traditional domain - adaptation settings , the out - ofdomain data is labeled for a different task , and only shares one module with the original task . In our experiments we fixed 20 % of the fully labeled data for the aspect sentiment task , and gradually added out - of - domain data , consisting of partial sentiment labels from the target sentiment task . Our model successfully utilized the out - ofdomain data and improved performance on the indomain task . The results are shown on Fig 5 .", "entities": []}
{"text": "For all our experiments , to show efficacy of our approach we kept the preprocessing as minimal as possible . Apart from word lowerization , tokenization , and punctuation removal we did n't perform any other activity .", "entities": []}
{"text": "Convolutional Neural Networks are known to perform well on short texts ( Yin et al , 2017 ) , in ( Conneau et al , 2017a ) authors proposed to concatenate representation at different levels of input sentence . The model was claimed to capture hierarchical abstractions of the input sentence . For our experiments , we fixed 128 kernels of size 2 , 3 , 4 , 4 at 4 different levels . These values were decided after the experiments with different number of kernels and their sizes .", "entities": []}
{"text": "We", "entities": []}
{"text": "Bing Liu 's work was partially supported by the National Science Foundation ( NSF IIS 1838770 ) and by a research gift from Huawei .", "entities": []}
{"text": "A Modular Architecture for Unsupervised Sarcasm Generation", "entities": []}
{"text": "Generation of sarcasm , unlike other language generation tasks , is highly nuanced . If we reconsider the example in the introductory section , the output sentence is sarcastic as it presents an unusual situation where the opinion holder has liked the rather boring act of waiting for a bus . The unusualness ( and hence , the sarcasm ) arises from two implicitly opposing ( incongruous ) contexts : love and waiting for the bus . Such a form of sarcasm , based on the context incongruity theory ( Campbell and Katz , 2012 ) , is more common in text than other forms such as prepositional , embedded or illocutionary sarcasm ( Camp , 2012 ) . For any textual sarcasm generator , figuring out contextually incongruous phrases will be as difficult as generating a fluent sentence . Moreover , most of the existing language generators are known to work on large scale literal / non - sarcastic texts ( e.g. , language models trained on Wikipedia articles ) , and are agnostic of the possible collocations of contextually incongruous phrases ( Joshi et al , 2017a ) . We try to overcome these challenges through our modular system design , discussed as follows .", "entities": []}
{"text": "The overall system architecture is presented in Figure 1 . For development of the modules three corpora are needed : ( a ) a corpus of positive sentiment sentences ( P ) , ( b ) a corpus of negative sentiment sentences ( N ) , and ( c ) a corpus of sarcastic sentences ( S ) . The framework performs transformation of literal text into sarcastic ones in four stages as given below :", "entities": []}
{"text": "This is a one - time process and is done using the unsupervised bootstrapping technique similar to Riloff et al ( 2013 ) . For each sentence in the sarcasm corpus S , a candidate negative situation phrase is extracted . A candidate negative situation phrase is a word n - gram ( n \u2264 5 ) that follows a positive sentiment phrase in a sarcastic sentence 3 . After the candidates for a positive phrase are obtained , their Part of Speech tags are extracted with the help of a POS tagger . Specific patterns of n - gram are then obtained using the POS tags . This ensures that the phrases extracted are mostly verb phrases , noun - phrases , and to - infinitive verb phrases that describe situations . In our setting we use 30 predefined POS n - gram patterns following Riloff et al ( 2013 ) . Once the candidate negative situation phrases are extracted , they are filtered based on a scoring function as given below : score i = # ns i in S # ns i in S , P , N ( 2 ) where ns i is the i th negative situation extracted for a certain positive phrase . The scoring function returns a real value indicating the exclusiveness of the negative situation w.r.t the sarcastic sentences . If the score exceeds 3 the word \" love \" is considered as the seed positive sentiment phrase to begin the bootstrapping procedure a threshold ( i.e. , p > 0.5 ) , the candidate phrase is added to the gazetteer . Once all the possible negative situation phrases are extracted , each phrase is used to extract more positive sentiment phrases similarly as above . This process of positive phrase and negative situation extraction is repeated until no new phrases are found . Table 1 shows some example negative situation phrases extracted from our dataset .", "entities": []}
{"text": "Since no dataset containing paired examples of literal and sarcastic utterances are available , we created a small test - set for evaluating our system . From the test split of the sarcasm corpus S , 250 sentences on diverse topics are selected and are manually translated into literal versions by two linguists . From this , only 203 sentences could be selected by the linguists who mutually decided whether the sentences were sarcastic enough to keep in the test dataset or not .", "entities": []}
{"text": "by Artetxe et al ( 2017 ) , which can be extended to any translation task . In our setting , the source and target side are literal and sarcastic utterances , i.e. the direction of translation is non - sarcastic to sarcastic .", "entities": []}
{"text": "Tables 2 and 3 present evaluation results . While it was expected that the automatic metrics may not be able to capture the subtleties of sarcasm , the WL measure indicates that a carefully designed modular approach like ours often generates longer sentences with more context . This is also corroborated by the human evaluation where annotators have judged that the output generated from our system are more sarcastic than the comparison systems . SarcasmBot , being a heuristic driven sarcasm generator produces sarcastic responses but is not related to the input topic . Moreover , it ends up generating only 20 different responses for our entire test dataset making its output redundant and unrelated to the input . Other existing systems such as UNMT and Monoses converge to autoencoding and end up replicating the input as output . FLIP , performs transformations at lexical level , hence achieves better fluency but certainly fails to induce sarcasm in most of the cases . Table 4 presents example generations from different systems . It is quite interesting to note that due to the RL , the model tends to produce longer sentences and brings additional context necessary for sarcasm . The fluency is however compromised . A close inspection of the outputs from each module suggests that the overall error committed by the system is due to accumulation of different types of errors , mainly ( a ) error during neutralization due to inappropriate assignment of weights to the words in the input , ( b ) dropping of words and/or insertion of spurious words during positive sentiment induction , and ( c ) error in scoring the sarcasm content in the RL setting . These can be addressed through better hyper - parameter tuning , gathering more training data for training the individual modules ( especially the sarcasm synthe - Input : worrying because did not finish my homework . Reference : did not finish any homework & i still need to shower ! lol ! love stressing out . SarcasmBot : How exciting ! rolls eyes . Monoses : whining because actually finish my homework . UNMT : worrying because i did not finish my homework . ST : if do not work my mom hurts . FLIP : reassuring because did not finish my homework . SG ( NORMAL ) : i am worrying about the worrying of homework . SG ( RL ) : worrying about finish homework . ALL ( NORMAL ) : no worrying , i finish doing homework great . ALL ( RL ) : worrying about finish homework is great . Input : swimming lessons are very costly in nyc . Reference : you have to sacrifice your first born child for swimming lessons in nyc . SarcasmBot : That is a very useful piece of information ! Yawn ! Monoses : Dry lessons are very costly financially . UNMT : swimming lessons are very costly in nyc . ST : a will was in bed . FLIP : swimming lessons are very costly in nyc . SG ( NORMAL ) : this is so costly to me swimming lessons in nyc . SG ( RL ) : swimming lessons is so costly in nyc . ALL ( NORMAL ) : loving the swimming lessons in nyc ch . ALL ( RL ) : i am loving the swimming lessons . going to be a very costly in nyc ch .", "entities": []}
{"text": "Twitter is an ever - growing store of daily generated data . Given the huge number of tweets talking about drug - related issues , social media mining is applicable to areas such as pharmacovigilance ( Lee et al , 2017 ; Nikfarjam et al , 2015 ; Ginn et al , 2014 ; Freifeld et al , 2014 ; Bian et al , 2012 ) . Tasks 1 and 2 focuses on detecting tweets with ADR and identifying location of mentions . We are provided with 25 , 672 tweets ( 2 , 374 positive and 23 , 298 negative ) and approximately 5 , 000 unlabeled tweets as a validation set . For the second task , a subset of 2 , 367 tweets from the first task was provided ( 1 , 212 positive and 1 , 155 negative ) . The evaluation data comprises 1 , 000 tweets ( ~500 positive , ~500 negative ) .", "entities": []}
{"text": "Stop words and punctuations were removed from tweets and all drug names found in the FDA 's Approved Drug Products list 1 were replaced by the word \" drug \" . Word stemming and tokenization were performed using nltk python library .", "entities": []}
{"text": "To identify the text spans of ADR mentions , first the model developed for task 1 was used to determine whether each tweet mentions an ADR . Then the similarity between each tweet and 3 different lexicon sets ( Nikfarjam et al 3 , MedDRA ( Medical Dictionary for Regulatory Activities ) 4 , and CHV ( Consumer Health Vocabulary ) 5 ) was measured . To calculate similarity , each tweet and lexicon was converted to a set of word stems . Since similarity measures such as cosine or Jaccard are highly affected by other non - ADR words , we defined similarity as the percent of word stems of a lexicon that exist in a tweet . For each tweet , only lexicons with a 100 % match were kept .", "entities": []}
{"text": "The sentiment polarity of price is positive ( scoring : 0.1 ) The sentiment polarity of price is neutral ( scoring : 0.2 ) The sentiment polarity of price is negative ( scoring : 0.7 )", "entities": []}
{"text": "given the crowd , but for the price I was disappointed . < miscellaneous : neutral > < incorrect output : negative > The kids really enjoyed their food and the value on the kids menu is good . < menu : neutral > < incorrect output : positive > The decor could be a bit better , and if there was a small bar the overall atmosphere would be a bit more inviting . < place : negative > < incorrect output : neutral >", "entities": []}
{"text": "A.1 Sentence - Level Datasets Rest14 ( Pontiki et al , 2014a ) Following previous work ( Cheng et al , 2017 ; Tay et al , 2018 ; Hu et al , 2019 ) , we remove samples with conflict polarities . Since there is no official development set for Rest14 , we use the split offered by Tay et al ( 2018", "entities": []}
{"text": "Zhiyang Teng is the corresponding author . We would like to thank the anonymous reviewers for their insightful comments . We gratefully acknowledge funding from the National Natural Science Foundation of China ( NSFC No.61976180 ) .", "entities": []}
{"text": "Seen2Unseen at PARSEME Shared Task 2020 : All Roads do not Lead to Unseen Verb - Noun VMWEs", "entities": []}
{"text": "We describe the Seen2Unseen system that participated in edition 1.2 of the PARSEME shared task on automatic identification of verbal multiword expressions ( VMWEs ) . The identification of VMWEs that do not appear in the provided training corpora ( called unseen VMWEs ) - with a focus here on verb - noun VMWEs - is based on mutual information and lexical substitution or translation of seen VMWEs . We present the architecture of the system , report results for 14 languages , and propose an error analysis .", "entities": []}
{"text": "The identification of multiword expressions ( MWEs ) such as spill the beans is a challenging problem ( Baldwin and Kim , 2010 ; Constant et al , 2017 ) , all the more so for verbal MWEs ( VMWEs ) subject to morphological ( spill the bean ) and syntactic variability ( the beans were spilled ) . The PARSEME shared task ( PST ) provided training , development and test corpora ( hereafter Train , Dev , and Test ) manually annotated for VMWEs . 1 Our system aimed at identifying every VMWE in Test which also appears in Train or Dev , including possible morphological or syntactic variants ( henceforth seen VMWEs ) or not present in Train / Dev ( unseen VMWEs ) . Unseen VMWE identification , the main focus of this PST edition , is harder than seen VMWE identification , as shown by previous results ( Ramisch et al , 2018 ) . We submitted two systems : Seen2Seen ( closed track ) and Seen2Unseen ( open track ) . Seen2Unseen relies on Seen2Seen for the identification of seen VMWEs and has an additional module for unseen ones . Its best global unseen F - score ( i.e. not only for verb - noun constructions ) was obtained for Hindi ( 42.66 ) and it reached 25.36 in French , which was our main focus . Despite the lower global MWE - based F1score of Seen2Unseen ( 63.02 ) compared to Seen2Seen ( 66.23 ) , we describe the former ( Sec . 2 ) , analyse its interesting negative results ( Sec . 3 ) , and conclude with ideas for future work ( Sec . 4 ) .", "entities": []}
{"text": "This work was funded by the French PARSEME - FR grant ( ANR - 14 - CERA - 0001 ) . We are grateful to Guillaume Vidal for his prototype , and to the anonymous reviewers for their useful comments .", "entities": []}
{"text": "Do n't sweat the small stuff , classify the rest : Sample Shielding to protect text classifiers against adversarial attacks", "entities": []}
{"text": "The typical attack strategy perturbing texts with word synonyms or character substitutions assumes to have query access to the target web site 's classifier ( W ) ( Yoo and Qi , 2021 ; Li et al , 2021a ; Ren et al , 2019 ; Li et al , 2020 ; Garg and Ramakrishnan , 2020 ; Jia et al , 2019 ; Li et al , 2019 ) . The text is modified by querying W hundreds or thousands of times , each time with a text version differing only slightly from the previouseven by just a single word ( Li et al , 2020 ; . Such a querying pattern can be easily identified as adversarial by the website and countered . Thus , practically the only way in which such an attack can take place is when the attacker owns a local classifier W \u2032 which is either an exact copy of W or a close enough approximation . We adopt this more realistic threat model , shown in Figure 1 . In our threat model the attacker uses feedback from its local W \u2032 to generate a final perturbed version that defeats W \u2032 or is close enough to do so . The attacker submits only this final version to the website , expecting W to make the same error . However , the website defends W using Sample Shielding : sample based pre - processing on the input text , prior to applying W . The attacker may or may not be aware of this fact . Keeping W = W \u2032 which is consistent with other defenses , we evaluate our defense under two conditions : 1 ) The attacker does not know that the website employs Sample Shielding pre - processing when classifying text using W . 2 ) The Sample Shielding step is leaked and the attacker incorporates it locally when using W \u2032 to generate the final perturbed text . We present results from experiments exploring both of these attack conditions .", "entities": []}
{"text": "Intuition . Current adversarial attackers have two goals : fool the classifier and maintain the original meaning . Since they make minimal changes , the extent of perturbation is in fact one of the reported statistics . For example , ( Li et al , 2020 ) note that their 10 % perturbation rate is far less than in previous attacks . ( Li et al , 2019 ) also focus on minimal changes ( 4 % ) needed in support of their attack success rate . Our defense approach capitalizes on this drive to make minimal changes . Specifically , in Sample Shielding , we take k samples each composed of p% of the text . We choose a p which minimizes the chance of a sample including attacked ( modified ) words , while maximizing the content available for the classifier to make a correct classification . We choose a k which is large enough to cover key information but small enough to reduce redundancy . We classify each sample and combine > 0.5 k sample predictions \u2026 I enjoyed this movie more than I thought I would . From multiple viewings it becomes especially clear how much time and energy the director put into this film . The choice for lead actor had me worried but it worked well . The twist was what really had me hooked . ... their decisions for the final classification . We explore two sampling and three decision combining methods .", "entities": []}
{"text": "Random Sampling . We randomly sample p portions of the text . We explore both sentences and words as sampled units . A visualization of random sampling is in Figure 2 . Shifting Sampling . We sample the text using a moving window of length p \u00d7 length_of _ text . The first starts at the beginning of the text . The next window starts right after the previous window ends . If there is insufficient text for the last window , then it wraps back to include the beginning text .", "entities": []}
{"text": "Majority voting . This is a simple majority vote across the k samples ( Figure 2 ) . Classifier trained on sample scores from original texts ( NN ) . We train a neural network summarizer to make a final class prediction based on the k sample probabilities . Since sample ID does not carry any information , the input to the neural network is a sorted list of sample probabilities . The intent is to see if the neural network picks up on latent patterns in the probabilities that are not captured by majority voting ( see Figure 2 ) . It should be emphasized that the neural network summarizer is trained only on probabilities generated from original texts and does not consider probabilities from attacker modified texts . We use a simple feed forward neural net composed of 2 linear layers ( size 500 and 300 ) as classification summarizer . Classifier trained on sample scores from original and attacked texts ( NN - BB ) . This is similar to the previous strategy except that the training data includes scores from original texts and texts that have been modified by the attacker . Because this assumes more knowledge of the attacker we expect NN - BB to perform better than NN . The ground truth label for these modified texts is the original correct class label . 3 Experimental Setup", "entities": []}
{"text": "We submit constrained systems to both German to French and French to German translations , with the same techniques .", "entities": []}
{"text": "Acquisition , Representation and Usage of Conceptual Hierarchies", "entities": []}
{"text": "Roadblocks in Gender Bias Measurement for Diachronic Corpora", "entities": []}
{"text": "Building on both Bolukbasi et al ( 2016 ) and Chen et al ( 2021 ) , we consider how the sets of profession words required by the Bolukbasi et al 's method would need to change over time in Arabic . We begin by describing two diachronic datasets that we used and how we processed these datasets , then we describe the changes in the profession word usage over time .", "entities": []}
{"text": "In the religion of Islam , some professions are forbidden , for example , all types of usury , and serving , selling , or drinking alcohol . We examined a set of illegal / religiously forbidden profession words in Islam across the 11 ages of the Arabic poems , such as male usurer ( ) , female usurer ( ) , male bartender ( ) , and female bartender ( ) . Specifically , we closely focused on the diachronic semantic meaning change of the bartending profession words in the parallel eras of the APCD dataset . Interestingly , we found that bartending profession words in the early ages of the Arabic poems like Pre - Islamic , Islamic , and Umayyad only point to providing water to people but not serving wine even though the wine does exist . Those bartending profession words are polysemous and could carry other meanings like the male bartender ( ) could have a meaning of the phrase ' my leg ' ( ) , while the female bartender ( ) could have as well the ) across the 11 ages of the Arabic poems in the APCD dataset , showing the related meanings like serving water , wine , or could be entirely meaning something that entirely unrelated to the profession word 's meaning of serving drinks . meaning of ' a water creek or an aqueduct ' ( ) . To thoroughly investigate the occurrence of those profession words regarding their correlation with water - the allowed / halal drink , and the wine - - the forbidden / haram drink in Islam , we manually analyzed the Arabic poems of each age and decided whether that word occurrence is a waterrelated meaning , wine - related meaning , or other unrelated meanings to both of the drinks . Figure 2a shows that the male bartender ( ) profession word started to appear in the Arabic poems as a profession of serving alcohol generally , wine exclusively , as a symbol of love , passion , and adoration for women from the age of between Umayyad and Abbasid until the Modern age . Similarly , in Figure 2b , the female bartender ( ) started to appear as a profession of serving wine from the age of between Umayyad and Abbasid until the Modern age as same as the male bartender ( ) profession word , except they did not appear in the two ages of Ayyubid and Ottoman . While the female and male bartender ( ) surprisingly appeared in correlation with wine in the Arabic poems despite its religious forbiddance , both of the two profession words also refer to water - related words . For example , the female bartender ( ) refers to the ' water creek or aqueduct . ' One example to show that is when the Modern Arabic poet , Rashid Ayoub ( ) , said in his poem : \" I sat in the meadow alone at the water creek , in which the water echoed the sound of my melodies \" ,", "entities": []}
{"text": "We 'd like to thank the Clarkson Open Source Institute for their help and support with infrastructure and hosting of our experiments . We 'd like to thank Abigail Matthews and Thomas Middleton for their help and support in writing and reviewing the manuscript .", "entities": []}
{"text": "Specializing Multilingual Language Models : An Empirical Study", "entities": []}
{"text": "Research in natural language processing is increasingly carried out in languages beyond English . This includes high - resource languages with abundant data , as well as low - resource languages , for which labeled ( and unlabeled ) data is scarce . In fact , many of the world 's languages fall into the latter category , even some with a high number of speakers . This presents unique challenges compared to high - resource languages : effectively modeling low - resource languages involves both accurately tokenizing text in such languages and maximally leveraging the limited available data . One common approach to low - resource NLP is the multilingual paradigm , in which methods that have shown success in English are applied to the union of many languages ' data , 1 enabling transfer between languages . For instance , multilingual contextual word representations ( CWRs ) from language models ( Devlin et al , 2019 ; Huang et al , 2019 ; Lample and Conneau , 2019 , inter alia ) are conventionally \" pretrained \" on large multilingual 1 Within the multilingual paradigm , a distinction is sometimes made between massively multilingual methods , which consider tens or hundreds of languages ; and polyglot methods , which use only a handful . In this paper , all mentions of \" multilingual \" refer to the former . corpora before being \" finetuned \" directly on supervised tasks ; this pretraining - finetuning approach is derived from analogous monolingual models ( Devlin et al , 2019 ; Liu et al , 2019 ; . However , considering the diversity of the world 's languages and the great data imbalance among them , it is natural to question whether the current multilingual paradigm can be improved upon for low - resource languages . Indeed , past work has demonstrated that it can . For instance , Wu and Dredze ( 2020 ) find that multilingual models often lag behind non - contextualized baselines for the lowest - resource languages in their training data , drawing into question their utility in such settings . Conneau et al ( 2020a ) posit that this phenomenon is a result of limited model capacity , which proves to be a bottleneck for sufficient transfer to low - resource languages . In fact , with multilingual models only being pretrained on a limited set of languages , most of the world 's languages are unseen by the model . For such languages , the performance of such models is even worse ( Chau et al , 2020 ) , due in part to the diversity of scripts across the world 's languages ( Muller et al , 2021 ; Pfeiffer et al , 2021b ; Rust et al , 2021 ) as compared to the models ' Latin - centricity ( \u00c1cs , 2019 ) . Nonetheless , there have been multiple attempts to remedy this discrepancy by specializing 2 a multilingual model to a given target low - resource language , from which we take inspiration . Among them , Chau et al ( 2020 ) augment the model 's vocabulary to more effectively tokenize text , then pretrain on a small amount of data in the target language ; they report significant performance improvements on a small set of low - resource languages . In a similar vein , Muller et al ( 2021 ) propose to transliterate text in the target language to Latin script to be better tokenized by the existing model , followed by additional pretraining ; they observe mixed results and note that transliteration quality may be a confounding factor . We hypothesize that these two methods can serve as the basis for improvements in modeling a broad set of low - resource languages . In this work , we study the effectiveness , extensibility , and interaction of these two approaches to specialization : the vocabulary augmentation technique of Chau et al ( 2020 ) and the script transliteration method of Muller et al ( 2021 ) . We verify the performance of vocabulary augmentation on three tasks in a diverse set of nine low - resource languages across three different scripts , especially on non - Latin scripts ( 2 ) and find that these gains are associated with improved vocabulary coverage of the target language . We further observe a negative interaction between vocabulary augmentation and transliteration in light of a broader framework for specializing multilingual models , while noting that vocabulary augmentation offers an appealing balance of performance and cost ( 3 ) . Overall , our results highlight several possible directions for future study in the low - resource setting . Our code , data , and hyperparameters are publicly available . 3", "entities": []}
{"text": "We begin by revisiting the Vocabulary Augmentation method of Chau et al ( 2020 ) , which we recast more generally in light of recent work ( 2.1 ) . We evaluate their claims on three different tasks , using a diverse set of languages in multiple scripts ( 2.2 ) , and find that the results hold to an even more pronounced degree in unseen low - resource languages with non - Latin scripts ( 2.3 ) .", "entities": []}
{"text": "To test this research question , we apply transliteration and VA in succession and evaluate their compatibility . Given unlabeled data in the target language , we first transliterate it into Latin script , which decreases but does not fully eliminate the issue of unseen wordpieces . We then perform VA , generating the vocabulary for augmentation based on the transliterated data . We evaluate on Meadow Mari and Uyghur , which are Type 2 languages where transliteration was successfully applied by Muller et al ( 2021 ) . To transliterate the data , we use the same methods as Muller et al ( 2021 ) : Meadow Mari uses the transliterate 8 package , while Uyghur uses a linguistically - motivated transliteration scheme 9 aimed at associating Uyghur with Turkish . We use the same training scheme , model architectures , and baselines as in 2.2 , the only difference being the use of transliterated data . This includes directly pretraining on the unlabeled data ( LAPT ) , which is comparable to the highest - performing transliteration models of Muller et al ( 2021 ) . Although our initial investigation of VA in 2 also included non - Type 2 languages of other scripts , we omit them from our investigation based on the finding of Muller et al ( 2021 ) that transliterating higherresource languages into Latin scripts is not beneficial .", "entities": []}
{"text": "We thank Jungo Kasai , Phoebe Mulcaire , and members of UW NLP for their helpful comments on preliminary versions of this paper . We also thank Benjamin Muller for insightful discussions and providing details about transliteration methods and baselines . Finally , we thank the anonymous reviewers for their helpful remarks .", "entities": []}
{"text": "Learning to Predict Denotational Probabilities For Modeling Entailment", "entities": []}
{"text": "In order to bridge the gap between vector - based distributional approaches to lexical semantics that are intended to capture which words occur in similar contexts , and logic - based approaches to compositional semantics that are intended to capture the truth conditions under which statements hold , Young et al ( 2014 ) introduced the concept of \" denotational similarity . \" Denotational similarity is intended to measure the similarity of simple , declarative statements in terms of the similarity of their truth conditions . From classical truth - conditional semantics , Young et al borrowed the notion of the denotation of a declarative sentence s , s , as the set of possible worlds in which the sentence is true . Young et al apply this concept to the domain of image descriptions by defining the visual denotation of a sentence s as the set of images that s describes . The denotational probability of s , P ( s ) , is the number of images in the visual denotation of s over the size of the corpus . Two sentences are denotationally similar if the sets of images ( possible worlds ) they describe have a large overlap . For example , \" A woman is jogging on a beach \" and \" A woman is running on a sandy shore \" can often be used to describe the same scenario , so they will have a large image overlap that corresponds to high denotational similarity . Given the above definitions , Young et al estimate the denotational probabilities of phrases from FLICKR30 K , a corpus of 30 , 000 images , each paired with five descriptive captions . Young et al ( 2014 ) and showed that these similarities are complementary to standard distributional similarities , and potentially more useful for semantic tasks that involve entailment . However , the systems presented in these papers were restricted to looking up the denotational similarities of frequent phrases in the training data . In this paper , we go beyond this prior work and define a model that can predict the denotational probabilities of novel phrases and sentences . Our experimental results indicate that these predicted denotational probabilities are useful for several textual entailment datasets .", "entities": []}
{"text": "Section 7 has demonstrated that we can successfully learn to predict denotational probabilities for phrases that we have not encountered during training and for longer sentences . We examine examples of predicted conditional probabilities for phrase and sentence pairs to analyze our model 's strengths and weaknesses . Table 4 has example predictions from the denotation phrase development data . Our model correctly predicts high conditional probability for entailed phrase pairs even when there is no direct hypernym involved , as in example 2 , and for closely related phrases that are not strictly entailing , as in example 3 . Our model also predicts reasonable probabilities for events that frequently co - occur but are not required to do so , such as example 7 . A person is on a beach . 0.88 2 Two women having drinks and smoking cigarettes at the bar . Two women are at a bar . 0.86 3 A senior is waiting at the window of a restaurant that serves sandwiches . A person waits to be served his food . 0.61 John Deere equipment is being worked on by two farmers .", "entities": []}
{"text": "We have presented a framework for representing denotational probabilities in a vector space , and demonstrated that we can successfully train a neural network model to predict these probabilities for new phrases . We have shown that when also trained on longer sentences with approximate probabilities , our model can learn reasonable representations for these longer sentences . We have also shown that our model 's predicted probabilities are useful for textual entailment , and provide additional gains in performance when added to existing competitive textual entailment classifiers . Future work will examine whether the embeddings our model learns can be used directly by these classifiers , and explore how to incorporate negation into our model .", "entities": []}
{"text": "Learning and Reasoning for Robot Dialog and Navigation Tasks", "entities": []}
{"text": "Reinforcement learning and probabilistic reasoning algorithms aim at learning from interaction experiences and reasoning with probabilistic contextual knowledge respectively . In this research , we develop algorithms for robot task completions , while looking into the complementary strengths of reinforcement learning and probabilistic reasoning techniques . The robots learn from trial - and - error experiences to augment their declarative knowledge base , and the augmented knowledge can be used for speeding up the learning process in potentially different tasks . We have implemented and evaluated the developed algorithms using mobile robots conducting dialog and navigation tasks . From the results , we see that our robot 's performance can be improved by both reasoning with human knowledge and learning from task - completion experience . More interestingly , the robot was able to learn from navigation tasks to improve its dialog strategies .", "entities": []}
{"text": "Knowledge representation and reasoning ( KRR ) and reinforcement learning ( RL ) are two important research areas in artificial intelligence ( AI ) and have been applied to a variety of problems in robotics . On the one hand , KRR research aims to concisely represent knowledge , and robustly draw conclusions with the knowledge ( or generate new knowledge ) . Knowledge in KRR is typically provided by human experts in the form of declarative rules . Although KRR paradigms are strong in representing and reasoning with knowledge in a variety of forms , they are not designed for ( and hence not good at ) learning from experiences of accomplishing the tasks . On the other hand , RL algorithms enable agents to learn by interacting with an environment , and RL agents are good at learning action policies from trial - and - error experiences toward maximizing long - term rewards un - der uncertainty , but they are ill - equipped to utilize declarative knowledge from human experts . Motivated by the complementary features of KRR and RL , we aim at a framework that integrates both paradigms to enable agents ( robots in our case ) to simultaneously reason with declarative knowledge and learn by interacting with an environment . Most KRR paradigms support the representation and reasoning of knowledge in logical form , e.g. , Prolog - style . More recently , researchers have developed hybrid KRR paradigms that support both logical and probabilistic knowledge ( Richardson and Domingos , 2006 ; Bach et al , 2017 ; Wang et al , 2019 ) . Such logical - probabilistic KRR paradigms can be used for a variety of reasoning tasks . We use P - log ( Baral et al , 2009 ) in this work to represent and reason with both human knowledge and the knowledge from RL . The reasoning results are then used by our robot to compute action policies at runtime . Reinforcement learning ( RL ) algorithms can be used to help robots learn action policies from the experience of interacting with the real world ( Sutton and Barto , 2018 ) . We use model - based RL in this work , because the learned world model can be used to update the robot 's declarative knowledge base and combined with human knowledge . Theoretical Contribution : In this paper , we develop a learning and reasoning framework ( called KRR - RL ) that integrates logical - probabilistic KRR and model - based RL . The KRR component reasons with the qualitative knowledge from humans ( e.g. , it is difficult for a robot to navigate through a busy area ) and the quantitative knowledge from modelbased RL ( e.g. , a navigation action 's success rate in the form of a probability ) . The hybrid knowledge is then used for computing action policies at runtime by planning with task - oriented partial world models . KRR - RL enables a robot to : i ) represent the probabilistic knowledge ( i.e. , world dynamics ) learned from RL in declarative form ; ii ) unify and reason with both human knowledge and the knowledge from RL ; and iii ) compute policies at runtime by dynamically constructing task - oriented partial world models . Application Domain : We use a robot delivery domain for demonstration and evaluation purposes , where the robot needs to dialog with people to figure out the delivery task 's goal location , and then physically take navigation actions to complete the delivery task Veloso , 2018 ) . A delivery is deemed successful only if both the dialog and navigation subtasks are successfully conducted . We have conducted experiments using a simulated mobile robot , as well as demonstrated the system using a real mobile robot . Results show that the robot is able to learn world dynamics from navigation tasks through model - based RL , and apply the learned knowledge to both navigation tasks ( with different goals ) and delivery tasks ( that require subtasks of navigation and dialog ) through logical - probabilistic reasoning . In particular , we observed that the robot is able to adjust its dialog strategy through learning from navigation behaviors .", "entities": []}
{"text": "We briefly describe the two most important building blocks of this research , namely model - based RL and hybrid KRR .", "entities": []}
{"text": "Cold start is a common problem in recommender systems ( Schein et al , 2002 ; Zhang et al , 2014 ; . This also applies to dialogue systems , as the partner personas are commonly missing in early turns . We conduct an analysis on the baselines and our framework when N turns are available where N= { 1 , 2 , 3 } , using PERSONACHAT - ORI . As demonstrated in Figure 3 , all the methods attain a better PPL when N increases , which indicates the existence of the cold start . This is also the case for the baseline with ground truth personas , and we postulate that it fails to learn how to use partner personas during cold start due to the lack of clues . Our framework effectively mitigates the cold start problem and attains the best among them for all N. our framework successfully recognizes that the partner is asking specifically for metallica . It then conditions on the generated personas to generate a much more entailed response than the baseline . The human response expresses negatively and thus seems less engaging . In the second case , our framework recognizes that the partner has a garden . It then talks about the garden rather than the irrelevant response from the baseline that we postulate is misled by the ' large ' adjective in the dialogue context . The human response is potentially sarcastic if the partner is not joking , while our generation does not have such issue . For the third case , the baseline produces a response that could be potentially offensive , which could be biased by the word ' violent ' in the dialogue context . In contrast , our framework recognizes the identity of the partner to generate a response without such an issue . The human response tends to raise a new topic and is less relevant . For the fourth case , we observe that the annotator sometimes converses based on the partner profile rather than his own traits . In this case , the annotator ( Dialogue Context ) said that he has many pets , which is not in his own traits ( Gold Partner ) . Rather , his conversation partner expressed his passion for animals in previous dialogue contexts . We postulate that the annotator attempted to engage the conversation by conditioning his partner personas and telling a relevant joke . Our PPG can recognize this , which further tweaks the model output to talk about dogs and cats rather than the dog only . These cases validate that leveraging partner personas is beneficial , and our framework can generate reasonable partner personas , which is not even in the ground truth .", "entities": []}
{"text": "Table 3 presents generated partner personas using PERSONACHAT - ORI . As depicted , our PPG can generate reasonable partner personas which are relevant to the ground truth partner personas . It sometimes gives a reasonable generation which is even not in the ground truth partner personas . In the first case , the generator successfully identifies the partner as being an army ranger . It then becomes rather positive than a violent person as given in the ground truth personas . Conditioning on such positive contents can give a positive response . In the second case , it recognizes the partner as a gym person , and imagines that the partner drinks protein and life weights , which is not in the ground truth personas . In the third case , the generator generates coherent personas , saying that the partner would drink beer and eat food while watching football , which is also not in the ground truth . We postulate that personas could be semantically closer to each other when they frequently co - occur in the training set . Our PPG then tends to generate more coherent personas by learning such semantical relationship . Since our generated personas are relevant and coherent , we postulate it as the underlying reason why our method gives a better generalization to DRG . In contrast , as demonstrated by Table 3 , ground truth personas tend to be more like discrete collections of traits . This could be the reason why some of our generated partner personas could beat the ground truth , which is also supported by our human evaluation in Section 5.6 . This is a potential benefit of our approach compared to sentence - level user profile extraction ( Li et al , 2014 ; Wu et al , 2020b ; that is upper bounded by the discrete ground truth . We present more examples in Table 8 in the Appendix .", "entities": []}
{"text": "The PERSONACHAT dataset used in this work is well - known and widely used . In our view , there is no known ethical issue with its usage . Large - scale pre - trained models are also employed , but they are widely known to be subject to potential problems such as generating offensiveness context . With its use , our partner personas generator could generate unseen personas , which are also subject to potential offensive generation . An offensiveness check can be incorporated to alleviate this problem for actual usage ( Baheti et al , 2021 ) .", "entities": []}
{"text": "This research / paper was supported by the Center for Perceptual and Interactive Intelligence ( CPII ) Ltd under the Innovation and Technology Commission 's InnoHK scheme .", "entities": []}
{"text": "To assure a seamless annotation procedure , the supply of new instances has to be reasonably fast . The generation and selection of the next instance is dependant on the label of the previous instance . Because of this , there is no way to pre - fetch the next instance in the background and the annotator has to wait for the selection / generation process to finish before the next instance is presented for annotation . However , the runtime for pool - based AL methods is increasing with the pool 's size . In contrast , the generation method presented in this work does not have this limitation . The least confidence baseline has a complexity of O ( n ) where n is the number of instances in the pool . The complexity of nearest neighbor search without any approximation techniques like preclustering is also O ( n ) . Query generation from an exact point with the decoder has a complexity of O ( m ) where m is the length of the sentence and n > > m. Because sentences have a natural length limit and in this work are capped to 15 words , one could argue that the complexity is O ( 1 ) . prototypical positive and negative instances . Example 7 is ambiguous , caused by the decoder generating an unknown ( UNK ) token at the position where one would normally expect an evaluative adjective . We see this as an indicator that the point is positioned close to the hyperplane and thus the sentiment of the latent variable is ambiguous . We also observe instances with UNK token which still express a sentiment , as seen in Example 8 an 9 . This can be interpreted as a placeholder for a named entity or , in other cases , a specifier like movie genre and does not impact the annotation process .", "entities": []}
{"text": "Part of this research has been conducted within the Leibniz Science Campus \" Empirical Linguistics and Computational Modeling \" , funded by the Leibniz Association under grant no . SAS - 2015 - IDS - LWC and by the Ministry of Science , Research , and Art ( MWK ) of the state of Baden - W\u00fcrttemberg .", "entities": []}
{"text": "In this section , we use language codes 8 to represent languages , and use MULTI and MIX to represent multilingual and code - mixed tracks respectively 9 .", "entities": []}
{"text": "We compare with some variants of our system that we designed but did not use in the test phase .", "entities": []}
{"text": "In Table 8 , we show the effectiveness of multistage fine - tuning on the development set for our baseline system . The result shows that multi - stage fine - tuning can significantly improve the model performance for all the tracks .", "entities": []}
{"text": "Performance ? In the multilingual test set , we can find 304 , 905 sentences in the other monolingual test sets while there are 167 , 006 sentences that can not be found . For these sentences , we can either search on the whole KB of all languages or first detect the language of the input sentence and then search in the specific language KB 14 . Moreover , as we discussed in Section 5.4 , using different kinds of retrieved knowledge affects the model performance . As a result , we train two types of multilingual models . One is only using the PARA contexts for all language and another is using the best option for each language based on Table 3 . From the results in Table 13 , we can observe that : 1 ) searching over the language specific KB performs better than searching the whole KB , 2 ) using the language specific context option can not improve the model performance . Therefore , we ensemble both types of the model for the final submission .", "entities": []}
{"text": "This work was supported by Alibaba Group through Alibaba Innovative Research Program .", "entities": []}
{"text": "It should be noted that we have detected a few flaws in the provided data , namely several sentences incorrectly considered as parallel , as well as the existence of many spelling errors , not only in the training data , but also in the testing documents . We believe that many of the typos result from PDF extraction and/or OCR processes , which are never perfect , having found and corrected a total of 127 , 198 misspellings . Yet , it should be noted that some misspelling errors are easy to correct , but errors which still produce correct words require sentence analysis which was not carried out . Some of the parallel problems are illustrated , for instance , by having the first Portuguese line from medline - pubmed \" ERRATA . \" aligned with the first English line \" Inequalities in self - rated health : an analysis of the Brazilian and Portuguese populations . \" , which should be \" ER - RATA . \" instead . Filtering wrong translation units as the one above , as well as translation units which the language was not Portuguese , reduced this corpora by almost 2 , 000 translation units . Some errors were simply detected by chance , like first and last entries of medline - pubmed , while other errors were detected by looking at the untranslated terms in the initial testing 3 and realizing that some terms were misspellings , as well as spelling and vocabulary differences between European and Brazilian Portuguese . Table 3 shows the differences between the original version medline - pubmed and its revised version medline - pubmed - rev . The reduction in size towards the revised version is mainly due to the removal of non - parallel sentences . However , efforts to correct such situations were only made over the mentioned medline - pubmed parallel document set , since the other sets were significantly larger , as shown in Table 1 . Also , no corrections were applied to the testing documents because we assumed they were not supposed to be edited . Yet , another \" noise \" element was the already mentioned difference in spelling and vocabulary between European Portuguese ( which has been our main focus of attention throughout our research experience ) and Brazilian Portuguese ( the version of the provided biomedical data ) , which can also impact results negatively .", "entities": []}
{"text": "Text tokenization ensures that words are properly separated by a single blank space , while normalization ensures that they are represented by a \" standard \" version . In English , this means that cases like \" was n't \" or \" is n't \" are going to be replaced by \" was not \" and \" is not \" , respectively . In Portuguese , this means that cases like \" do \" ( of the ) or \" nas \" ( in the ) are going to be replaced by \" de o \" ( of the ) and \" em as \" ( in the ) , respectively . These tokenization and normalization changes are reverted when presenting the final translation results . Whipple disease and central nervous system . Doen\u00e7a de Whipple e sistema nervoso central .", "entities": []}
{"text": "Phrase - level alignment was obtained with a modified version of the lexicon - based aligner proposed by Gomes ( 2009 ) . The aligner matches bilingual phrase pairs provided in an input lexicon ( described ahead in 2.3.2 ) and selects a maximalcoverage 1 subset of coherent alignments . While the original method imposed a monotonicity constraint , i.e. it selected a maximal - coverage chain of phrase alignments without allowing phrase reorderings , the new method applied has a more relaxed coherency criteria : it only requires that a source - language phrase is not simultaneously aligned with two distinct target - language phrases . Therefore , it allows phrase reordering as shown in the example in Figure 1 .", "entities": []}
{"text": "Similar to the ILP ( Integer Linear Programming ) solution proposed by ( DeNero and Klein , 2008 ) , we treat the alignment problem as an optimization problem , but we employ a greedy optimization algorithm which allows us to align longer sentences with reasonable time and memory . The algorithm 1 Maximal - coverage means that the selected phrase alignments cover as much text as possible from both sentences constructs a solution ( a set of coherent alignments ) incrementally . It starts by settling alignments of longer phrases , which tend to be more reliable , and progresses towards shorter phrases or words , which are allowed to align only if they are coherent with previously settled alignments .", "entities": []}
{"text": "Our lexicon covers 59.5 % of the EN corpus tokens and 55.4 % of the PT corpus tokens . There were 143 , 317 unique phrasal translations matched out of 931 , 568 in our lexicon . The cognaticity - based matching was responsible for aligning 8 % of the EN corpus and 7.2 % of the PT corpus 3 . The remainder 32.5 % of the EN corpus and 37.4 % of the PT corpus were left unaligned . These unaligned tokens are handled as gaps by the phrase table extraction algorithm described in ( Aires et al , 2009 ) .", "entities": []}
{"text": "The language model used is supported by the indexation of the texts in each language of the provided corpora . Such indexation will support determining the likelihood of the occurrence of phrases in the target language for the several adjacent translation fragments in decoding , a process based on the structures presented in ( Aires et al , 2008 ) .", "entities": []}
{"text": "The translation model depends on the alignment to determine phrase translation equivalents by establishing phrase relations between source and target languages , as well as to determine a degree of likelihood of those same relations , to be used in decoding to produce new translations , a process based on the methodology presented in ( Aires et al , 2009 ) .", "entities": []}
{"text": "The decoding stage is the one that will finally produce the actual translations . First , an original text is fragmented into smaller pieces of text , which will then be used to retrieve their corresponding translations . The several combinations of the translations of those smaller pieces will represent many possible translations and the purpose of decoding is to find the most likely one , according to the provided scores from the language and the translation models . As mentioned before , separate models can be obtained from separate corpora and be assigned with different relevances or weights , according to their importance to the translation in question . As such , and as explained in , decoding is carried out as a best path finding in a directed acyclic graph , where its edges are weighed by : the translation model score between source and target phrases ; and the language model scores between adjacent target phrases . Each complete path will represent a possible translation in which the final score is a composition of the scores of the several edges that compose the given path . An additional penalty is introduced to provide lower scores to larger paths , which are known to produce worse results .", "entities": []}
{"text": "Considering that the test documents to be translated , provided by the shared task organization , share their domain with the training data , we decided to propose for submission the three possible translation runs for each document according to the criteria described in each of the following subsections .", "entities": []}
{"text": "This run uses the medline - pubmed , biological and health training corpora with the same relevance to translate every translation test document . These can be considered our simplest set of tests since the possible model relevance difference is not explored and no additional sources are included . In this case we achieved a total of 7228 unique untranslated terms 5 .", "entities": []}
{"text": "This run also uses the medline - pubmed , biological and health training corpora , but assigns a higher 4 http://www.statmt.org/europarl/ 5 Terms can have one or more words relevance to the biological corpora to translate the biological test documents and then assigns a higher relevance to the health corpora to translate the health test documents . Because the changes introduced in this set of tests only concerned the relevance of the models , the total of 7228 unique untranslated terms did not change .", "entities": []}
{"text": "UID / CEC/04516/2013 ) . We would also like to thank Hugo Delgado for his support .", "entities": []}
{"text": "Table 3 shows the performance of CLSC with onestep transition ( L 1\u2212step ) and with Markov Chains ( L clsc ) as described in Section 3.2 . Results show that the use of Markov Chains does bring improvement to the overall performance , which is consistent with the model intuition .", "entities": []}
{"text": "Given a question q and a mixture set of paragraphs P = P + \u222a P \u2212 with some paragraphs p P + relevant to q and some p P \u2212 irrelevant . Our goal is to select a small subset of paragraphs P sel \u2282 P , such that every p P sel satisfies p P + ( relevancy ) , and all p P sel can jointly cover all the information asked by q ( complementary ) . The off - the - shelf models select relevant paragraphs independently , thus usually can not deal with the complementary property . The inner dependency among the selected P sel needs to be considered , which will be modeled in the remaining of the section .", "entities": []}
{"text": "For efficient inference when L = 2 , we start to select the top - N ( N K ) most relevant passages . Then we score the combinations between each passage pair in the top - N set and another top - M set . This reduces the complexity from O ( K 2 ) to O ( M N ) . M is a hyperparameter corresponding to the beam size . In a more general setting with L \u2265 2 , we have an algorithm with the complexity of O ( ( L \u2212 1 ) M N ) instead of O ( K L ) , which is shown in algorithm 1 .", "entities": []}
{"text": "Research reported in this publication was supported by the National Library Of Medicine of the National Institutes of Health under Award Numbers R01LM012918 and R01LM012973 . The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health .", "entities": []}
{"text": "We collect two corpora , Small Corpus and Large Corpus , with different sizes for cross - lingual pretraining . Table 1 lists the data statistics .", "entities": []}
{"text": "For tasks QADSM , WPR , QAM and QG , we label the data on an Microsoft internal crowdsourcing platform . Each labeler must learn the guideline and pass the labeling test . Each sample is labeled by three labeler . We only keep the samples with two or three labeler have same label . For tasks NC and NTG , we directly use the category label on MSN website . All the category label on MSN is review by human .", "entities": []}
{"text": "We investigate the impacts of different text noising strategies ( Section 4.1 ) in Unicoder xDAE SC , and list comparison results in Table 10 , where ( 1 ) + ( 2 ) + ( 3 ) denotes the result of using the first three strategies in pre - training , ( 4 ) denotes the result of using the last strategy in pre - training , ( 1 ) + ( 2 ) + ( 3 ) + ( 4 ) denotes the result of using all strategies in pretraining . To reduce experiment cost , we set max sequence length to 256 and only train 60 K steps . We find that ( 4 ) can achieve the best average result on NTG . So all results of Unicoder xDAE SC reported in this paper is pre - trained using ( 4 ) only .", "entities": []}
{"text": "In this section we first present some properties of our dataset , and then describe the process that we used to create it .", "entities": []}
{"text": "We follow three principles when designing the dataset construction process : 1 . The text part of the data should be directly comparable in complexity to the capability of state - of - the - art text generative models . 2 . The graph part of the data should be constructed in an automatic and scalable way . 3 . The graph part of the data should be relevant for the paired text data . Note that our process is general , and can be applied to any set of Wikipedia articles . We have tried to pair a full dump of English Wikipedia with Freebase and managed to get over 3 million graphtext pairs . Here we restrict the process to the set of articles from the WikiText - 103 dataset . We try to map each Wikipedia article to a relevant subgraph of the existing large scale KG Freebase ( Bollacker et al , 2008 ) . We used the last public dump of Freebase 2 , which contains 1.9B triples and a total of 250 GB of data . We filtered the data by keeping only the entities with at least 4 string attributes ( otherwise the entities are less interpretable ) , and keeping only the top 1024 most frequent relation types and restricting the relations to only those among the retained entities and between the entities and string attributes . We also simplified the entity and relation names by stripping off the irrelevant \" http://rdf.freebase.com/ \" and further removed duplicates . This gives us a significantly cleaner and smaller backbone graph for Freebase , with about 20 M nodes . Finding the relevant subgraph for an article in such a cleaned up but still large KG remains nontrivial . Our process for this contains 3 stages : mapping , expansion , and filtering . Mapping In the first stage of the process , we map each article into an entity in our processed Freebase KG . This is made possible through triples from Freebase like the following : ns / g.11b6jbqpt4 key / wikipedia.en \" Madunnella \" where ns / g.11b6jbqpt4 refers to an entity in the KG , key / wikipedia.en is the type of the edge , which indicates that this entity is linked to a Wikipedia article and \" Madunnella \" is the title of that article . We normalize the title string ( and in general any string literals ) from Freebase by replacing \" _ \" with white space and handle unicode characters properly . We extract the titles from the Wikipedia article through string matching , where titles are enclosed in a \" = [ title ] = \" pattern . In this step we managed to map 24 , 345 out of 28 , 475 ( 85.5 % ) article titles from WikiText - 103 to an entity in our KG . Expansion We treat each of the mapped entities as the center node of a subgraph , and expand 1 hop out in the entire filtered Freebase graph to include all the neighboring entities that are the most relevant to the center entity . We then expand further from this 1 - hop graph out to include all the relations that connect the selected entities to string attributes as well as between these entities themselves . Note that because of these edges between the 1 - hop neighbor entities the graphs are typically not star structured . This gives us a relevant but compact graph for each article . We have also investigated the possibility of a 2 - hop neighborhood from the center node , and found that 2 - hop neighborhoods are significantly larger than 1 - hop and through some \" hub \" nodes like \" Male \" or \" Female \" a 2 - hop neighborhood from an entity can easily include many other irrelevant entities . Based on such observations we decided to use the 1 - hop neighborhood to keep the relevance of the subgraph high . Filtering The last stage of the process involves more filtering and cleaning up of the data . We noticed that in Freebase it is common for one entity to have multiple relations of the same type pointing to different string attributes , like the following : ns / m.07c72 key / wikipedia.en \" The SImpsons \" ns / m.07c72 key / wikipedia.en \" The Simpson \" ns / m.07c72 key / wikipedia.en \" The simsons \" ns / m.07c72 key / wikipedia.en \" Thr Simpsons \" ns / m.07c72 key / wikipedia.en \" The Simpson 's \" It is clear that there is a lot of redundancy in this data . We reduced all such edges ( from the same entity with the same edge type to string attributes ) to a single edge by picking the most \" canonical \" one . This was done by fitting a unigram model to the characters in the collection of strings and using that model to pick the most likely string . We also filtered the graphs based on size and created three versions of the data with maximum graph size capped at 256 , 512 , and 1024 nodes , respectively . All the statistics and results in the rest of the paper are based on graphs with a maximum size of 256 , but all versions of the data are made available online .", "entities": []}
{"text": "In this last task , we evaluate the performance of graph retrieval given a text query . We use exactly the same setting and scores as Section 4.3 , but instead rank the graphs for each text article using the likelihood scores . The results are shown in Table 6 . Note that this task is quite easy with our data and setup , potentially because the graphs are much more distinguishable than the text articles . All the graph - conditioned models perform almost perfectly , with the GNN model again outperforming the others .", "entities": []}
{"text": "A.1 Graph visualization Some example visualizations of the KG structures are shown in Figure 7 and Figure 8 . The corresponding graph truth texts are shown in Table 7 .", "entities": []}
{"text": "The generated texts based on the graph shown in Figure 7 and Figure 8 are listed in Table 8 and Table 9 , respectively .", "entities": []}
{"text": "The order of merging these results is decided by the evaluation scores from these modules for training data . The same order is applied to the test data .", "entities": []}
{"text": "Once the UMBC corpus is pre - processed and the three required corpora and an embedding matrix are derived , candidate hypernyms are acquired by applying the below processes . Co - occurrence frequency from Normalized Corpus : With this module , we hypothesized that a hyponym and its possible hypernyms are more likely to co - occur within a context - window . The context window of a term is its own paragraph . We start by creating a map for all the input terms . If a normalized paragraph 2.2.1 contains any of the input terms , then all the words in the context are added to the map of this particular term which considers them to be hypernyms for this input hyponym term . Every time a hypernym - hyponym pair co - occurs in one line , their co - occurrence count is increased by one . Finally , the candidate hypernyms are ranked in descending order of their co - occurrence frequencies .", "entities": []}
{"text": "In the pre - processing step 2.2.1 , we extracted possible hypernym - hyponym mapping data using Hearst Patterns . Each line of the data is of the form hypernym : hyponym - 1 , hyponym - 2 , , hyponymn . In this module , we created a map where each hyponym is a key mapped to hypernyms occurring with that hyponym and their co - occurrence frequencies . For example , values for keys hyponym - 1 , hyponym - 2 , and hyponym - n are updated with hypernym and the frequencies are increased by 1 . Finally the top 15 hyponyms ( based on frequencies ) for each key are reported as the result hypernyms .", "entities": []}
{"text": "We need a general distance vector which represents a hypernym - hyponym distance in the UMBC Embedding . We use training data input term ( x ) and the gold data hypernyms ( y ) to calculate this distance ( \u03a6 * ) which is calculated by : \u03a6 * = argmin \u03a6 1 N ( x , y ) \u03a6 x \u2212 y 2 ( 1 ) \u03a6 is used to get candidate hypernyms from the UMBC word embedding matrix for the input terms ( test data ) .", "entities": []}
{"text": "We define large - scale sense induction as deriving sense clusters for all words in a large vocabulary and assigning a sense cluster to each corpus occurrence of these words . 2", "entities": []}
{"text": "We evaluate our method on large corpora by randomly sampling 2000 instances from the senseinduced Wikipedia , focusing on frequent words with many senses . We manually annotate the samples ' senses without access to the automatically induced senses , and then compare our annotations to the system 's sense assignments . We publicly release our manual sense annotations .", "entities": []}
{"text": "Due to limit of space we provide additional examples in the appendix . We start with the senses found for the word face : The face senses refer to meeting / confronting , the body part , turn / look and side , respectively . Here we present two senses of the word orange , corresponding to the color and fruit : For each of the target words you labeled , you will now receive a short list of indirect wordmeaning definitions . Indirect word - meanings are composed of : ( a ) A list of 10 words that may appear instead of the target word in specific contexts ( b ) A list of 5 sentences in which the target word has this specific word - meaning . For example , this is a possible indirect wordmeaning for the target word \" Apple \" , representing the fruit , as opposed to the tech company : Alternatives : orange , olive , cherry , lime , banana , emerald , lemon , tomato , oak , arrow , Sentences in which Apple appears in this word - meaning : \" He and his new bride planted apple trees to celebrate their marriage . \" \" While visiting , Luther offers Alice an apple . \" \" When she picks the apple up , it is revealed that Luther has stolen a swipe card and given it to Alice to help her escape . \" You will be asked to label the indirect wordmeanings with one of the labels you used in step 1 . If no label matches the indirect word - meaning you are allowed to propose a new label or define it to be \" Unknown \" . Additionally , if you find several indirect word - meanings too close in meaning , label them the same .", "entities": []}
{"text": "Finally we present the senses for Jordan : Josh0 Tyropoeon Here the clusters correspond to Jordan the surname , the country , first name and the Jordan River , respectively .", "entities": []}
{"text": "The objective of this task is to annotate wordmeanings of 20 ambiguous words in a total of 2000 different contexts . What is word - meaning ? Words have different meanings in different contexts , for example , in the sentence : \" there is a light that never goes out \" , the word \" light \" refers to any device serving as a source of illumination . While \" light \" in the sentence \" light as a feather \" refers to the comparatively little physical weight or density of an object . Step 1 : In this dataset we examine 20 ambiguous words as targets . For each of these words we collected 100 sentences in which the target word appears . For every sentence in the 100 set per target word , you will be asked to write a short label expressing the meaning of the target word in that particular context . For example , here are three sentences with the target word \" light \" , each with its possible annotation . 1 . \" there is a light that never goes out \" visible light . 2 . \" light as a feather \" light as in weight . 3 . \" magnesium is a light metal \" light as in weight . Note that in this example the annotator found the second and third meanings of the word \" light \" to be the same and therefore labeled them with the same label . 13 While some annotations are indeed intuitive , labeling word - meanings when the target word is part of a name can be challenging . Here are a few guidelines for such use case : Whenever a target word appeared as part of a name ( Person , Organization etc . ) , one of three heuristics should be used 14 : 1 . If the target word is the surname of a person , the example should be tagged surname . 15 2 . If the entity ( as a whole ) refers to one of the word - meanings , it should be labeled as such . For example , Quitobaquito Springs label should refer to a natural source of water . 3 . If the target word is part of a name different from the original word - meaning , it should be tagged as Part of Name . This includes song names , companies ( Cold Spring Ice ) , restaurants etc . Possible exceptions for this case are when a specific named entity is significantly frequent . Step", "entities": []}
{"text": "This research was supported in part by the Canada First Research Excellence Fund and the Natural Sciences and Engineering Research Council ( NSERC ) of Canada .", "entities": []}
{"text": "Morphotactics as Tier - Based Strictly Local Dependencies", "entities": []}
{"text": "It is commonly accepted that morphological dependencies are finite - state in nature . We argue that the upper bound on morphological expressivity is much lower . Drawing on technical results from computational phonology , we show that a variety of morphotactic phenomena are tierbased strictly local and do not fall into weaker subclasses such as the strictly local or strictly piecewise languages . Since the tier - based strictly local languages are learnable in the limit from positive texts , this marks a first important step towards general machine learning algorithms for morphology . Furthermore , the limitation to tier - based strictly local languages explains typological gaps that are puzzling from a purely linguistic perspective .", "entities": []}
{"text": "Different aspects of language have different levels of complexity . A lot of recent work in phonology ( see Graf ( 2010 ) , Heinz ( 2011a ; 2011b ; , Chandlee ( 2014 ) , Jardine ( 2015 ) and references therein ) argues that phonological well - formedness conditions are subregular and hence do not require the full power of finite - state automata . This is particularly noteworthy because computational phonology still relies heavily on finite - state methods ( Kaplan and Kay , 1994 ; Frank and Satta , 1998 ; Riggle , 2004 ) . A similar trend can be observed in computational syntax , where the original characterization as mildly context - sensitive string languages ( Huybregts , 1984 ; Shieber , 1985 ) is now being reinterpreted in terms of subregular tree languages ( Graf , 2012 ; Graf and Heinz , 2015 ) . Curiously missing from these investigations is morphology . While linguistic theories sometimes consider morphology a part of syntax , computational morphology recognizes that the weak generative capacity of morphology is much closer to phonology than syntax . Consequently , computational morphology involves largely the same finite - state methods as computational phonology ( Koskenniemi , 1983 ; Karttunen et al , 1992 ) . This raises the question whether morphology , just like phonology , uses only a fraction of the power furnished by these tools . A positive answer would have important repercussions for linguistics as well as natural language processing . The subregular classes identified in computational phonology are learnable in the limit from positive text ( Heinz et al , 2012 ) , so a subregular theory of morphology would greatly simplify machine learning while also explaining how morphological dependencies can be acquired by the child from very little input . A subregular model of morphology would also be much more restricted with respect to what processes are predicted to arise in natural languages . It thus provides a much tighter typological fit than the regular languages . In this paper , we argue that the subregular view of morphology is indeed correct , at least for morphotactics . Morphotactics describes the syntax of morphemes , that is to say , their linear order in the word and the conditions that license their presence or enforce their absence . One can distinguish surface morphotactics from underlying morphotactics . The former regulates the shape of the pronounced surface strings , whereas the latter is only concerned with the arrangements of the morphemes in the initial representation rather than how said morphemes are realized in specific environments . We only consider underlying morphotactics in this paper . The following example may clarify the distinction . In German , the past participle of a verb is formed via a circumfix . The first part of the circumfix is always the prefix ge - , whereas the second part may be the suffix - en or - t depending on the verb stem . In addition , the suffixes can also occur on their own , e.g. on infinitives or the third person singular form of the verb . Surface morphotactics thus has to ensure that ge - always appears with one of these two suffixes , and that the form of the suffix matches the stem . At the same time , it does not need to worry about matching - en or - t with ge - since these forms can occur independently as realizations of different morphemes . Underlying morphotactics , on the other hand , is unaware of the surface realizations and only knows that some abstract prefix GE - must always occur with the abstract suffix - EN , and the other way round . The fact that - EN has a surface realization that is indistinguishable from the infinitival marker , which does not require the prefix GE - , is irrelevant for underlying morphotactics . More succinctly : underlying morphotactics regulates the distribution of morphemes , surface morphotactics the distribution of allomorphs . This paper considers a variety of phenomenacircumfixation , variable affix ordering , unbounded prefixation - and concludes that they all belong to the class of tier - based strictly local languages . We first show that even though many morphotactic dependencies are strictly local , that is not the case for all of them ( Sec . 2.1 ) . While some of these outliers are strictly piecewise ( Sec . 2.2 ) , tier - based strictly local grammars are needed to handle the full range of data points ( Sec . 2.3 ) . This prompts our conjecture that all dependencies that are part of underlying morphotactics stay within the class of tier - based strictly local languages . We then use this hypothesis in Sec . 3 to explain two typological gaps with respect to compounding markers and circumfixation .", "entities": []}
{"text": "The regular languages are one of the best understood language classes , with many attractive properties . Yet it is often forgotten that this class properly includes many weaker ones ( McNaughton and Pappert , 1971 ) , some of which have recently attracted much interest in computational phonology . At the very bottom of the hierarchy one finds strictly local and strictly piecewise languages ( Rogers et al , 2010 ) , and a little bit higher up the tier - based strictly local languages ( Heinz et al , Regular Star - Free LTT LT SL PT SP TSL Figure 1 : The subregular hierarchy as given in Heinz et al ( 2011 ) ; language classes in dashed boxes are studied in this paper 2011 ) . The subregular hierarchy includes many other classes ( see Fig . 1 ) , but the previous three are noteworthy because they are conceptually simple and efficiently learnable in the limit from positive data ( Heinz et al , 2012 ; Jardine and Heinz , 2016 ) while also furnishing sufficient power for a wide range of phonological phenomena ( Heinz , 2015 ; Jardine , 2015 ) . In this section , we investigate the role of strictly local , strictly piecewise and tier - based strictly local patterns in morphotactics . We show that some but not all patterns are strictly local or strictly piecewise , whereas all typologically instantiated patterns seem to fit in the class of tier - based strictly local languages .", "entities": []}
{"text": "While SL covers a wide range of phenomena , it is n't just circumfixes that require more power . Problems arise whenever a dependency involves both the domain of prefixes and the domain of suffixes - because they can be separated by arbitrarily many symbols - and such configurations are not limited to circumfixes . In most languages the ordering of affixes tends to be fixed , but there are languages in which affixes are ordered relatively freely and do not follow a strict template , thereby creating non - local dependencies . Let us consider the following data from Swahili : ( 5 ) a. a - This data is taken from Stump ( 2016 ) . Based on his discussion of vyo , the following forms are ungrammatical . Different generalizations can be drawn from these data sets , some of which are more complex than others . The first generalization states that vyo is only licensed if it follows either a segment that is part of a stem or the prefix si . This is a strictly 2 - local pattern , and it explains both ( 6a ) and ( 6b ) . Alternatively , one may conclude that ( 6b ) is ill - formed because there is more than one occurrence of vyo . Such a ban against two instances of vyo is also supported by the ill - formedness of ( 6c ) , which is unexpected under the first generalization . Preventing the presence of two instances of vyo is beyond the power of any SL grammar G : if uvx + c \u2282 L ( G ) and uwcvx + \u2282 L ( G ) , then L ( G ) must also contain strings of the form uwcvx + c ( due to suffix substitution closure ) . The second generalization is similar to the phonological requirement that no word may contain more than one primary stress , which is strictly piecewise ( SP ; Heinz ( 2010 ) , Rogers et al ( 2010 ) ) . SP grammars work exactly the same as SL grammar except that instead of illicit substrings they list illicit subsequences . Given a string w , its set of k - sequences is k - seqs ( w ) : = s | s is a subsequence of\u0175 k\u22121 of length k . A strictly k - piecewise grammar G is a finite set of k - grams over \u03a3 \u222a { , } , and the language generated by G is L : = { w | k - seqs ( w ) \u2229 G = } . The ban against two occurrences of vyo is strictly 2 - piecewise - the grammar only need to contain the bigram vyo vyo . The intersection of the strictly 2 - local and strictly 2 - piecewise languages does not contain ( 6a ) - ( 6c ) , as desired . But it does contain ( 6d ) . Both generalizations miss that even though vyo can occur as a prefix and as a suffix , it is a prefix if and only if si is present . This kind of conditional positioning can not be captured by SL grammars , and the culprit is once again suffix substitution closure . But SP grammars by themselves are not sufficient , either . Suppose we increase the locality rank from 2 to 3 and include si x vyo as an illicit subsequence in our SP grammar . This forces vyo to be a prefix in the presence of si . However , it still incorrectly allows for vyo to be a prefix in the absence of si . No SP grammar can prevent this outcome . The problem is that any word of the form u vyo v x contains only subsequences that also occur in the well - formed u si vyo v x. Consequently , any SP grammar that generates the latter also generates the former . It is only in combination with the SL grammar that we can correctly rule out prefix vyo without a preceding si . Swahili 's inflectional morphology thus provides evidence that SL is not enough to handle all aspects of morphotactics and must be supplemented by some mechanism to handle long - distance dependencies , with SP being one option . But even the combination of SL and SP can not capture all non - local dependencies . In Swahili , the inability of SP mechanisms to enforce the presence of si with prefix vyo could be remedied by the strictly local requirement that vyo may only occur after si or a stem . This elegant interaction of SL and SP is not always possible , however . The most noteworthy case are circumfixes . Consider some arbitrary circumfix u - - v. Clearly all subsequences of ux + are subsequences of ux + v , so if the latter is generated by some SP grammar then by definition the former must be , too . The underlying problem is that SP grammars can only enforce the absence of an affix , not its presence . Circumfixes where the presence of one affix entails the presence of the other affix thus are not SP . It seems that we must move higher up the subregular hierarchy in order to accommodate circumfixes , which will also have the welcome side - effect of providing a simpler account for the distribution of Swahili vyo .", "entities": []}
{"text": "If the subregular hypothesis is correct , then no morphological pattern may exceed the computational power furnished by tier - based strictly local grammars . In particular , whenever the combination of two attested TSL patterns is not TSL , that combination should not be attested . The subreg - ular hypothesis thus provides a principled explanation for typological gaps . In this section we consider two such cases related to compounding markers and the limits of circumfixation .", "entities": []}
{"text": "Compounding describes the combination of two or more stems to form a compound lexeme , where the stems may belong to different categories . Languages differ with respect to whether compounding is ( at least sometimes ) explicitly marked . In the following we exhibit two TSL compounding patterns from Turkish and Russian whose combination is not typologically attested . We then explain why this combined pattern is not TSL , deriving the otherwise puzzling typological gap . Turkish possessive compounds ( see Aslan and Altan ( 1998 ) for a detailed description ) obey the general pattern stem - stem + - o , where o stands for the compounding marker - sI. Assuming once again the presence of the special symbol # - which marked the edges of stems in the previous section - we can show this pattern to also be tier - based strictly 2 - local . In this case , the illicit bigrams are # # , oo , o , and o . Observe that we can remove the first one of these bigrams to allow for cases where the compounding marker is optional . One may wonder now whether it is possible for natural languages to display a combination of the compounding patterns seen with Russian and Turkish . From a linguistic perspective , the expected answer is yes . If compounding can be marked by a suffix as in Turkish , and compounding can introduce a marker with each step as in Russian , then it should be possible to introduce a suffix with each compounding step . But to the best of our knowledge , no language instantiates this system . From a computational perspective , on the other hand , this typological gap is expected because the described system is not TSL - as a matter of fact , it is n't even regular . A language L that suffixes a marker to a compound with each compounding step would produce compounds where the number of compound markers is proportional to the number of stems . Let h be a map that replaces all stems by s , all compound markers by o , and all other material by some other symbol . Intersecting h ( L ) with the regular language s + o + yields the language s m o n , m > n. This string set is easily shown to be context - free ( e.g. via the Myhill - Nerode theorem ) , and since regular languages are closed under homomorphisms and intersection , it follows that L can not be regular . But every TSL language is regular , so the combination of Russian and Turkish outlined above is not TSL . The absence of this compounding pattern in the typology of natural languages thus lends further support to our conjecture that natural language morphotactics is limited to TSL dependencies .", "entities": []}
{"text": "Circumfixation already played an important role in motivating TSL as a reasonable lower bound on how much power is required for natural lan - guage morphotactics . We now show that just like compounding markers , circumfixation also suggests that TSL marks the upper bound on required expressivity . In particular , unbounded affixation is widely attested across languages , whereas unbounded circumfixation is not . A number of languages allow some of their affixes to occur multiple times . For instance , the Russian temporal prefix posle - can appear iteratively in the beginning of a word like zavtra ' tomorrow ' . The very same pattern is also found in German , with morgen ' tomorrow ' , \u00fcber - morgen ' the day after tomorrow ' , \u00fcber - \u00fcber - morgen ' the day after the day after tomorrow ' , and so on . German also has the pattern ur - gro\u00df - vater , ur - ur - gro\u00df - vater , which is the analog of English great grandfather , great great grandfather and its iterations ( various linguistic diagnostics show that these are morphological words rather than phrases ) . Note that in all those cases it is impossible to insert other prefixes between the iterated prefix : * ur - gro\u00df - ur - ur - gro\u00dfvater . In sum , some affixes can be freely iterated as long as no other affixes intervene . These patterns are all TSL by virtue of being strictly 2 - local . We illustrate this claim with German . We ignore the problem of how gro\u00df can be restricted to occur only with specific stems ( if stems are atomic symbols , this is trivial , otherwise it requires a strictly k - local grammar over the string of phonological segments where k is large enough to contain both gro\u00df and the relevant stems ) . We also assume , as before , that there is some marker # that marks the edges of stems . Then to ensure that the relevant strings of prefixes follow the pattern ur * gro\u00df # , the sequences gro\u00dfur , gro\u00dfgro\u00df , and ur # are marked as illicit . Unbounded prefixation thus stays within the class of TSL dependencies . An interesting counterpart to Russian and German is Ilocano ( Galvez Rubino , 1998 ) , which uses the circumfix ka - - an with a function similar to posle and\u00fcber . As before , there is little linguistic reason why unbounded circumfixation should be blocked . If affixation can be unbounded to construct more and more complex versions of day after tomorrow , and day after tomorrow can be derived via circumfixation , then one would expect unbounded circumfixation to be a viable option . But once again there is a clear computational reason as to why this does not happen : the corresponding morphotactic system would no longer be TSL . Let L be some minor variant of Russian where posle - is instead a circumfix pos - - le . As before we let h be a homomorphism that replaces all stems by s , the two parts of the circumfix by o , and all other material by some distinct symbol . The intersection of h ( L ) with the regular language o + so + is the context - free string set o n so n . Therefore L is supra - regular and can not be tier - based strictly local . Unbounded circumfixation simply can not be reconciled with the subregular hypothesis .", "entities": []}
{"text": "The received view is that all of morphology is easily modeled with finite - state machines ( Koskenniemi , 1983 ; Karttunen et al , 1992 ) . We contend that this view is overly generous and that tighter bounds can be established , at least for specific subparts of morphology . Morphotactics defines the restrictions on the possible orderings of morphological units , and we argued based on data from typologically diverse languages that the power of natural language morphotactics is severely restricted : Subregular Morphotactics All morphotactic dependencies are tier - based strictly local . In contrast to regular languages , tier - based strictly local languages are efficiently learnable in the limit from positive text ( Heinz et al , 2012 ; Jardine and Heinz , 2016 ) . Our result thus marks a first step towards provably correct machine learning algorithms for natural language morphology . Admittedly , morphotactics is just one of several parts of morphology . We put aside allomorphy and only considered the distribution of morphemes in the underlying forms . Even within that narrow area we did not thoroughly explore all facets , for instance infixation and incorporation . Nonetheless our results show that the success of the subregular perspective need not be limited to phonology . At least morphotactics can be insightfully studied through this lens , too . In addition , there has been a lot of progress in extending the subregular hierarchy from languages to transductions ( see Chandlee ( 2014 ) and references therein ) , and we are confident that these results will allow us to expand the focus of investigation from morphotactics to morphology at large . It will also be interesting to see how uniform the complexity bounds are across different modules of morphology . In phonology , suprasegmental dependencies tend to be more complex than segmental ones ( Jardine , 2015 ) . Most constructions in this paper involve derivational morphology , but the affix vyo in Swahili is related to inflectional morphology and turned out to have a distribution that is neither SL nor SP ( although it can be captured with a combination of the two ) . So both derivational and inflectional morphotactics occupy points in TSL \\ ( SL \u222a SP ) . In this regard it is also worth noting that some phonological processes such as tone plateauing belong to SP \\ TSL , whereas no morphological dependencies seem to be part of this subclass . We hope to address these and related issues in future work .", "entities": []}
{"text": "There is a long history of pre - training general language representations , and we briefly review the most widely - used approaches in this section .", "entities": []}
{"text": "NovelPerspective : Identifying Point of View Characters", "entities": []}
{"text": "We present NovelPerspective : a tool to allow consumers to subset their digital literature , based on point of view ( POV ) character . Many novels have multiple main characters each with their own storyline running in parallel . A well - known example is George R. R. Martin 's novel : \" A Game of Thrones \" , and others from that series . Our tool detects the main character that each section is from the POV of , and allows the user to generate a new ebook with only those sections . This gives consumers new options in how they consume their media ; allowing them to pursue the storylines sequentially , or skip chapters about characters they find boring . We present two heuristic - based baselines , and two machine learning based methods for the detection of the main character .", "entities": []}
{"text": "To the best of our knowledge no systems have been developed for this task before . As such , we have developed two deterministic baseline character classifiers . These are both potentially useful to the end - user in our deployed system ( Section 5 ) , and used to gauge the performance of the more complicated systems in the evaluations presented in Section 4 . It should be noted that the baseline systems , while not using machine learning for the character classification steps , do make extensive use of machine learning - based systems during the preprocessing stages .", "entities": []}
{"text": "An obvious way to determine the main character of the section is to select the first named entity . We use this to define the \" First Mentioned \" baseline In this system , the Feature Extraction step is simply retrieving the position of the first use of each name ; and the Character Scoring step assigns each a score such that earlier is higher . This works for many examples : \" One dark and stormy night , Bill heard a knock at the door . \" ; however it fails for many others : \" ' Is that Tom ? ' called out Bill , after hearing a knock . '' . Sometimes a section may go several paragraphs describing events before it even mentions the character who is perceiving them . This is a varying element of style .", "entities": []}
{"text": "A more robust method to determine the main character , is to use the occurrence counts . We call this the \" Most Mentioned \" baseline . The Feature Extraction step is to count how often the name is used . The Character Scoring step assigns each a score based what proportional of all names used were for this entity . This works well for many books . The more important a character is , the more often their name occurs . However , it is fooled , for example , by book chapters that are about the POV character 's relationship with a secondary character . In such cases the secondary character may be mentioned more often .", "entities": []}
{"text": "In the evaluation , the systems are given the body text and asked to predict the character names . During evaluation , we sum the scores of the characters alternative aliases / nick - names used in the books . For example merging Ned into Eddard in ASOIAF . This roughly corresponds to the case that a normal user can enter multiple aliases into our application when selecting sections to keep . We do not use these aliases during training , though that is an option that could be investigated in a future work .", "entities": []}
{"text": "The full source code is available on GitHub . 3 Scikit - Learn ( Pedregosa et al , 2011 ) is used for the machine learning and evaluations , and NLTK ( Bird and Loper , 2004 ) is used for textual preprocessing . The text is tokenised , and tagged with POS and named entities using NLTK 's default methods . Specifically , these are the Punkt sentence tokenizer , the regex - based improved Tree - Bank word tokenizer , greedy averaged perceptron POS tagger , and the max - entropy binary named entity chunker . The use of a binary , rather than a multi - class , named entity chunker is significant . Fantasy novels often use \" exotic \" names for characters , we found that this often resulted in character named entities being misclassified as organisations or places . Note that this is particularly disadvantageous to the First Mentioned baseline , as any kind of named entity will steal the place . Nevertheless , it is required to ensure that all character names are a possibility to be selected .", "entities": []}
{"text": "The demonstration system is deployed online at https://white.ucc.asn.au/tools/np . A video demonstrating its use can be found at https://youtu.be/iu41pUF4wTY . This web - app , made using the CherryPy framework , 4 allows the user to apply any of the model discussed to their own novels . The web - app functions as shown in Figure 1 . The user uploads an ebook , and selects one of the character classification systems that we have discussed above . They are then presented with a page displaying a list of sections , with the predicted main character ( /s ) paired with an excerpt from the beginning of the section . The user can adjust to show the top - k most - likely characters on this screen , to allow for additional recall . The user can select sections to retain . They can use a regular expression to match the character names ( /s ) they are interested in . The sections with matching predicted character names will be selected . As none of the models is perfect , some mistakes are likely . The user can manually correct the selection before downloading the book .", "entities": []}
{"text": "Acknowledgements This research was partially funded by Australian Research Council grants DP150102405 and LP110100050 .", "entities": []}
{"text": "We classified the schemes according to the annotation level they address , either on the sentence , entity or relation - level . We present a summary of all schemes that we found , but give a more detailed description for ( selected ) schemes for which an annotated corpus is available ( cf . Table 1 ) .", "entities": []}
{"text": "We would like to thank Arnaldo Candido Junior and Sandra Maria Alu\u00edsio from the MAZEA tool for kindly processing our documents . We also would like to thank Animesh Prasad and Min - Yen Kan for their support when using their tool .", "entities": []}
{"text": "IST - Unbabel 2021 Submission for the Quality Estimation Shared Task", "entities": []}
{"text": "We present the joint contribution of IST and Unbabel to the WMT 2021 Shared Task on Quality Estimation . Our team participated on two tasks : Direct Assessment and Post - Editing Effort , encompassing a total of 35 submissions . For all submissions , our efforts focused on training multilingual models on top of OpenKiwi predictor - estimator architecture , using pre - trained multilingual encoders combined with adapters . We further experiment with and uncertainty - related objectives and features as well as training on out - ofdomain direct assessment data .", "entities": []}
{"text": "Quality estimation ( QE ) is the task of evaluating a translation system 's quality without access to reference translations ( Blatz et al , 2004 ; Specia et al , 2018 ) . This paper describes the joint contribution of Instituto Superior T\u00e9cnico ( IST ) and Unbabel to the WMT21 Quality Estimation shared task ( Specia et al , 2021 ) , where systems were submitted to two tasks : 1 ) sentence - level direct assessment ; 2 ) word - and sentence - level post - editing effort . This year 's submission combines several ideas built on top of the OpenKiwi framework . Motivated by the mixture of blind and seen language pairs in the test sets , we experimented with extensions that would allow us to train multilingual models that maintain good generalization ability and are robust to the presence of epistemic and aleatoric uncertainty . For both tasks we trained and submitted an ensemble of multilingual models . All submitted models follow the predictor - estimator architecture ( Kim and Lee , 2016 ; Kim et al , 2017 ) and use pretrained models for feature extraction . Also , we fine - tune all models on the provided QE data using stacked adapter layers ( Pfeiffer et al , 2020 ) . * The first three authors have equal contribution . We show that we can thus achieve comparable performance across language pairs while minimising the number of trainable parameters ( see Table 1 ) . Furthermore , we experimented with different types of uncertainty - related information to leverage it 's benefits , improving performance and robustness of the submitted systems ( see 3.1.1 ) . All related code extensions will be publicly available . Our main contributions are : We build on our OpenKiwi architecture by exploring adapter layers ( Houlsby et al , 2019 ; Pfeiffer et al , 2020 ) for quality estimation as these demonstrated to be less amenable to overfitting while presenting the same or superior quality performance than fine - tuning the whole base pre - trained model for different NLP tasks ( He et al , 2021 ) . We incorporate different types of uncertainty into our architectures . We make use of the glass - box features ( Fomicheva et al , 2020 ) extracted from the NMT models , the aleatoric ( data ) uncertainty derived from the human annotations and the epistemic ( model ) uncertainty ( Hora , 1996 ; Kiureghian and Ditlevsen , 2009 ; Huellermeier and Waegeman , 2021 ) that originates from the QE model . We show that training the QE models on additional out - of - domain direct assessment ( DA ) data gives considerable gains in performance for the new language pairs from the blind test sets .", "entities": []}
{"text": "In this year 's shared task edition we submitted models for the first two tasks : 1 . Task 1 : sentence - level direct assessment 2 . Task 2 : word - and sentence level post - editing effort , comprising of two subtasks : a ) predicting the HTER score of the translated sentence ( hypothesis ) ; and b ) predicting OK / BAD tags for the words and gaps ( both in source and translation ) We note that this year , both tasks 1 and 2 provided additional blind test sets with language pairs that were not included in the data made available for training / development , providing an interesting challenge and motivating multilingual and generalisable approaches . 3 Implemented Systems", "entities": []}
{"text": "We present the performance of the implemented models on the test - 20 dataset .", "entities": []}
{"text": "We presented a joint contribution of IST and Unbabel to the WMT 2021 QE shared task . Our submissions are ensembles of multilingual checkpoints extending the OpenKiwi framework . We found adapter - tuning to be suitable for fine - tuning OpenKiwi on the QE tasks data and less prone to overfitting . We showed that pre - training on large , out - of - domain annotated data can prove beneficial both for the direct assessment and the postediting QE tasks . We also demonstrated that handling uncertainty - related sources of information improves the performance when integrated into the QE system . For Task 2 we do multi - task training based on the models from the previous task and use multiple checkpoints to create the submitted ensemble .", "entities": []}
{"text": "In", "entities": []}
{"text": "We present the performance of the submitted ensembles on the TEST - 21 dataset as calculated in the official QE results 6 for each task and sub - task . We also provide the comparison with the organisers ' baseline . The results for Task1 on TEST - 21 are presented in Table 7 .", "entities": []}
{"text": "The results for Task2 on TEST - 21TEST - 21 are presented in Table 8 , showing the performance for the sentence level , HTER score predictions .", "entities": []}
{"text": "The results for Task2 on TEST - 21 are presented in Table 9 , showing the performance for the word tag predictions .", "entities": []}
{"text": "We present below ( Tables 10 and 11 ) the statistics on the Metrics data used to train the M1 M model on direct assessments .", "entities": []}
{"text": "To generate abstractive and factual summaries from unstructured conversations , we propose to model structural signals in conversations by first constructing discourse relation graphs and action graphs ( Section 3.1 ) , and then encoding the graphs together with conversations ( Section 3.2 ) as well as incorporating these different levels of information in the decoding stage through a multi - granularity decoder ( Section 3.3 ) to summarize given conversations . The overall architecture is shown in Figure 2 .", "entities": []}
{"text": "Given a conversation and its corresponding discourse relation graph and action graph , we utilize an utterance encoder and two graph encoders , to obtain its hidden representations shown in Figure 2 ( a ) .", "entities": []}
{"text": "To fine - tune on sequence labeling tasks , a dropout layer ( p = 0.1 ) and a linear ( token - level ) classification layer is built upon the pre - trained model .", "entities": []}
{"text": "Table 4 to Table 6 shows the model performance on the validation set . The data usage in these tables", "entities": []}
{"text": "We implement our system on Ubuntu 18.04.3 LTS system . We run our experiments on an Intel ( R ) Xeon ( R ) CPU @ 2.30GHz and NVIDIA Tesla P100 - PCIe with 16 GB HBM2 memory . The NVIDIA - SMI version is 418.67 and the CUDA version is 10.1 .", "entities": []}
{"text": "Semi - Supervised Iterative Approach for Domain - Specific Complaint Detection in Social Media", "entities": []}
{"text": "In this paper , we present a semi - supervised bootstrapping approach to detect product or service related complaints in social media . Our approach begins with a small collection of annotated samples which are used to identify a preliminary set of linguistic indicators pertinent to complaints . These indicators are then used to expand the dataset . The expanded dataset is again used to extract more indicators . This process is applied for several iterations until we can no longer find any new indicators . We evaluated this approach on a Twitter corpus specifically to detect complaints about transportation services . We started with an annotated set of 326 samples of transportation complaints , and after four iterations of the approach , we collected 2 , 840 indicators and over 3 , 700 tweets . We annotated a random sample of 700 tweets from the final dataset and observed that nearly half the samples were actual transportation complaints . Lastly , we also studied how different features based on semantics , orthographic properties , and sentiment contribute towards the prediction of complaints .", "entities": []}
{"text": "Our proposed approach begins with a large corpus of transport - related tweets and a small set of annotated complaints . We use this labeled data to create a set of seed indicators that drive the rest of our iterative complaint detection process .", "entities": []}
{"text": "We focused our experimentation over the period of November 2018 to December 2018 . Our first step towards creating a corpus of transportrelated tweets is to identify linguistic markers related to the transport domain . To this end , we scraped random posts from transport - related web forums 2 . These forums involve users discussing their grievances and raising awareness about a wide array of transportation - related issues . We then processed this data to extract words and phrases ( unigrams , bigrams , and trigrams ) with high tf - idf scores . We then had human annotators prune them further to remove duplicates and irrelevant items . This resulted in a lexicon of 75 unique phrases . Some examples include cabs , discount , tickets , underground , luggage , transit , parking , neighborhood , downtown , traffic , Uber . We used Twitter 's public streaming API to query for tweets that contained any of the 75 phrases over the chosen time range . We then excluded non - English tweets and any tweets with less than two tokens . This resulted in a collection of 19 , 300 tweets . We will refer to this collection as corpus C. We chose a random sample of 1 , 500 tweets from this collection for human annotation . We employed two human annotators to identify traffic - related complaints from these 1 , 500 tweets . Following are some high - level details of the annotation task . We instructed the annotators to identify any tweets that contain first - hand accounts of a complaint or a grievance related to a public / private mode of transport . Following is a sample tweet from this instruction : \" @ [ UserHandle ] can you please make sure that compartment A - 6 is at least clean before public use . \" We also instructed them to identify tweets that provide verifiable sources of information ( news ) about transport - related services . Sample tweet : \" 4 hour jam in [ place ] area due to rain and poor management of traffic police . \" . Lastly , we also explicitly asked them to exclude tweets that contain announcements or advertisements about transportation services . Sample tweet : \" Please use [ name ] cabs , you will get 60 % discount on your first 3 rides . \" The two annotators worked independently , and when we finally tallied their responses , we observed that they had an inter - annotator agreement rate of \u03ba = 0.81 ( Cohen kappa ) . In cases where the annotators disagreed , the labels were resolved through a discussion . After the disagreements were resolved , the final seed dataset had 326 samples of traffic - related complaints . We will refer to this as T s . Table 1 shows some examples of tweets that were annotated as complaints .", "entities": []}
{"text": "Our proposed iterative approach is summarized in Algorithm 1 . First , we use the seed data T s to build a set of linguistic indicators I for complaints . We then use these indicators to get potential new complaints T l from the corpus C. We merge T s and T l to build our new dataset . We then use this new dataset to extract a new set of indicators I l . The indicators are combined with the original indicators I to extract the next version of T l . This process is repeated until we can no longer find any new indicators .", "entities": []}
{"text": "As shown in Algorithm 1 , extracting linguistic indicators ( n - grams ) is one of the most important steps in the process . These indicators are critical to identifying tweets that are most likely domainspecific complaints . We employ two different approaches for extracting these indicators . For seed data , T s , which is annotated , we just select n - grams with the highest tf - idf scores . In our experimental work , T s had 326 annotated tweets . We identified 50 n - grams with the highest tf - idf scores to initialize I. Some examples include : problem , station , services , toll - fee , reply , fault , provide information , driver , district , passenger . In subsequent iterations , when we are handling unannotated samples , we use a more advanced domain relevance criterion for extracting the indicators . When extracting indicators from T l , which is not annotated , it is possible that there could be frequently occurring phrases that are not necessarily indicative of complaints . These phrases could lead to a concept drift in subsequent iterations . To avoid these digressions , we use a measure of domain relevance when selecting indicators . This is defined as the ratio of the frequency of an n - gram in T l to that of in T r . T r is a collection of randomly chosen tweets that do not intersect with C. In our experimental work , we defined T r as a random sample of 5 , 000 tweets from a different time range than that of C. We also wanted to quantitatively en - Samples of transport - related complaints . 1 . No metro fares will be reduced , but proper fare structure needs to be introduced .... right ? . 2 . It takes [ name ] govt . longer to refund charges , but it took them a few mins to remove that bus stop . You ca n't erase the problem [ name ] . 3 . I tried to lodge a complaint on [ url ] but see the results . Sir if 8 A.C 's are not working in this coach , why have you attached that coach .", "entities": []}
{"text": "Given a set of indicators I , the process of selecting tweets from corpus C is fairly straightforward . It only requires to identify all the tweet that contains any of the indicators . The only caveat here is to reduce the redundancy in the dataset . For this , we just filtered out tweets that have a cosine similarity of more than 0.85 with any other tweet in the tf - idf space ( Albakour et al , 2013 ) . This process also helped remove tweets , which are exact matches , sub - strings , or differing by some punctuation . Removal of these redundant tweets also helps in diversifying the lexicon for subsequent iterations .", "entities": []}
{"text": "Our iterative approach converged in four rounds , after which it did not extract any new indicators . Figure 1 shows the counts of indicators and the number of tweets after each iteration . After four iterations , this approach chose 3 , 732 tweets and generated 2 , 840 unique indicators . We also manually inspected the indicators chosen during the process . We observed that only indicators with a domain relevance score of \u2265 2.5 were chosen for subsequent iterations . Table 2 provides a few examples of strong and weak indicators acquired after the first iteration . In this figure , strong indicators are those with a domain relevance score \u2265 2.5 . We chose a random set of 700 tweets from the final complaints dataset T and annotated them manually to help understand the quality . We used the same guidelines as discussed in section 3.1 and also employed the same annotators as before . The anno - tators once again obtained a high agreement score of \u03ba = 0.83 . After resolving the disagreements , we observed that 332 tweets were labeled as complaints . This accounts for 47.4 % of the sampled 700 tweets . This demonstrates that nearly half the tweets selected by our semi - supervised approach were traffic - related complaints . This is a significantly higher proportion in the original seed data T s , where only 21.7 % were actual complaints .", "entities": []}
{"text": "We conducted a series of experiments to understand if we can automatically build simple machine learning models to detect complaints . These experiments also helped us evaluate the quality of the final dataset . Additionally , this experimental work also studies how different types of linguistic features contribute to the detection of social media complaints . For these experiments , we used the annotated sample of 700 posts as a test dataset . We built our training dataset by selecting another 2 , 000 posts from the original corpus C , and anno - tated them once again per guidelines discussed in section 3.1 . In this sample , we observed that the annotators had similar agreements scores of \u03ba = 0.79 , and there were 702 instances of complaints .", "entities": []}
{"text": "We experimented with four different semantic features : Unigrams : Each tweet ( Wallach , 2006 ) is represented as sparse vector of tf - idf values correspond - ing to the constituent tokens . Word2Vec Clusters : We follow the same approach as in ( Preo\u0163iuc - Pietro et al , 2015 ) , where words are clustered using pair - wise similarities in Word2Vec space ( Mikolov et al , 2013 ) . Each tweet is then represented as a distribution over these clusters ; the values are proportional to the number of tokens belonging to a cluster . These clusters have previously been demonstrated to have great interpretability ( Preo\u0163iuc - Pietro et al , 2015Zou et al , 2016 ) . POS Tags : We used the Stanford POS Tagger ( Manning et al , 2014 ) to represent tweets as a dense frequency vector over five main POS tags : nouns , adjectives , adverbs , verbs , pronouns . Pronoun Types : Pronouns are often used in complaints and suggestions to reveal personal involvement or to add intensity to an opinion ( Claridge , 2007 ; Meinl , 2013 ) . We identify various pronoun types ( first person , second person , third person , demonstrative , indefinite ) using dictionaries and use their counts as features .", "entities": []}
{"text": "Our first set of orthographic feature uses counts of URLs , hashtags , user mentions , and special symbols used in the post . The second set of orthographic features try to identify potential intensifiers such as capitalization and repeated use of exclamation or question marks . These types of intensifiers are often used to express anger or strong opinions ( Meinl , 2013 ) .", "entities": []}
{"text": "A request is a speech act very closely related to complaints . Often , the main motivation behind a complaint on a social media platform is to get a correction or reparation from the service providers ( Blum - Kulka and Olshtain , 1984 ) . We use the model presented in ( Danescu - Niculescu - Mizil et al , 2013 ) to detect if a given tweet is a request . Requests might also often include polite phrases in expectation of better service . They are coded using various dictionaries e.g , downgraders ( little ) , down - toners ( just ) , hedges ( somewhat ) . Apology markers have the same effect as politeness markers , they may include greetings at the start ( Good Morning ) , direct start ( e.g so ) , subjunctive phrases ( could you ) ( \u0160v\u00e1rov\u00e1 , 2008 ) . We utilize pre - defined dictionaries to determine the presence of politeness identifiers along with the politeness score of the tweet based on the model in ( Danescu - Niculescu - Mizil et al , 2013 ) .", "entities": []}
{"text": "In this paper , we presented a semi - supervised iterative approach for the detection of complaints in social media platforms . The process begins with a small sample of annotated examples , and then iteratively builds more linguistic identifiers to expand the dataset . We evaluated this approach on the domain of transportation on Twitter , starting with a sample of 326 annotated tweets . After four iterations , we were able to construct a corpus with over 3 , 700 tweets . Annotation of random samples established that nearly half the tweets were actual complaints . We evaluated the predictive power based on semantic , orthographic , and sentiment features . We observed that complaint is a complex speech act , which is related to many other linguistic properties . Automatic detection of complaints is not only useful to service providers as feedback ; it could also prove helpful in improving service providers ' operations and in downstream applications such as developing chat - bots . Additionally , it could also be of interest to linguists in understanding how humans express grievances and criticism . This proposed methodology could be applied to many other products or services to detect complaints . This would only additionally require some lexicons and a small annotated dataset . We also expect it would be fairly straightforward to adapt this technique to many other types of speech acts . Further investigation is necessary to understand how this method compares against supervised or completely unsupervised techniques .", "entities": []}
{"text": "nlpUP at SemEval - 2019 Task 6 : A Deep Neural Language Model for Offensive Language Detection", "entities": []}
{"text": "Tweets are first tokenized and converted to lowercase . We constrain repeated character sequences to length 3 and replace all longer character sequences . HTML character encodings are replaced by their corresponding literal or token representation ( e.g. ' & amp ; ' translates to ' and ' ) . Tokens are further split if they enclose a set of special characters ( ' \\ ' , ' / ' , ' & ' , ' - ' ) . Since hashtags are often used to replace contextually important words mid - sentence , we split hashtags in the actual hashsymbol and the following string to keep the semantic information of a hashtag ( e.g. ' Brainless # Liberal Stooge Ocasio - Cortez ' ) .", "entities": []}
{"text": "Quantifying the Effects of COVID - 19 on Mental Health Support Forums", "entities": []}
{"text": "There is a considerable body of research that examines the relationship between language use and mental health , including work dating back several decades . For example , Bucci and Freedman ( 1981 ) and Weintraub ( 1981 ) observed an increased usage of first person singular pronouns in individuals with depression . Oxman et al ( 1982 ) showed that they could distinguish between paranoia and depression by applying linguistic analysis to speech . Since then , advances in tools for text analytics have led to increased research in this area . Notably , the Linguistic Inquiry and Word Count ( LIWC ) ) is a widely used computerized text analysis tool that has been validated for psycholinguistic analysis . Some of the earliest studies using LIWC analyzed written text . For instance , researchers have used LIWC to study linguistic patterns in essays written by college students with and without depression ( Rude et al , 2004 ) or in poems written by suicidal vs non - suicidal poets ( Stirman and Pennebaker , 2001 ) . More recently , there has been a proliferation of studies applying LIWC to online text , including social media data . LIWC has been used to study language patterns on social media for a variety of mental health disorders , including depression , anxiety , suicidality , and bipolar disorder ( De Choudhury et al , 2013 ; Shen and Rudzicz , 2017 ; Coppersmith et al , 2014 . In addition to LIWC , other methods used to study the linguistic patterns of mental illness include character and word models ( Coppersmith et al , 2014 ; Tsugawa et al , 2013 ) and topic modeling ( Resnik et al , 2015 ; Preo\u0163iuc - Pietro et al , 2015 ) .", "entities": []}
{"text": "We report the daily number of unique users who posted in each subreddit in Figure 2 . We observe an increase in the number of users who posted in the r / Anxiety subreddit in the post - COVID period . Meanwhile , in both r / depression and r / SuicideWatch , we find significant decreases in the number of users who posted . In r / depression , we observe a substantial drop in posting rates around mid - March . Activity in this subreddit remains abnormally low into late - April , when it starts to revert back towards the forecasted values . In r / SuicideWatch , the drop in user activity is less extreme , and we see that the activity levels eventually return to their predicted values .", "entities": []}
{"text": "The increase in users posting on r / Anxiety is consistent with prior work that has found that epidemics often lead to increased rates of anxiety ( Torales et al , 2020 ) . One explanation for the reduction of activity within r / depression could be that fewer users are depressed and do n't feel the need to post on the support forum . If this is the case , our findings contrast with prior work that found that depressive symptoms are commonly observed during pandemics ( Chew et al , 2020 ) . However , there are multiple possible alternatives ; for example , depression can also cause people to socially withdraw ( Mayo Clinic , 2018 ) , so an increase in depression rates could lead to a reduction in posting activity . Another finding from prior work is that delayed depression is common following disaster events ( Pennebaker and Harber , 1993 ; Nandi et al , 2009 ) . Our analysis covers only the beginning of the pandemic , so it likely would n't capture this phenomenon . Additional analysis focused on the causes driving the reduction in activity and how this pattern changes in the long - term is needed to make a more conclusive statement about the effects of COVID - 19 on depression .", "entities": []}
{"text": "In Table 2 , we show the ten LIWC categories with the most outliers ( outside of the 95 % prediction interval ) from March to May of 2020 . We observe a lack of consistency between the subreddits , both in the number and direction of the outliers . We see decreases in r / Anxiety and r / depression in the MOTION category . Categories such as BIO and BODY tend to increase in r / Anxiety ; however , this pattern is not present in other subreddits . We see consistent changes in time orientation ( e.g. , FOCUSPAST , FOCUSFUTURE ) across subreddits ; a higher focus on the past in r / depression , and a lower focus on the future in r / SuicideWatch . While it is not among the categories with the most outliers , there is a statistically significant drop in FO - CUSFUTURE on r / Anxiety and r / depression . We also see changes in pronoun usage ; the most notable and consistent change across the subreddits is that the usage of WE increases significantly , especially in the early period of COVID - 19 ( Figure 3a ) . While there is a significant decrease in I words in r / Anxiety , there is in fact an increase in r / depression ( Figure 3b ) . Finally , we see a notable drop in the WORK category ( Figure 3c ) .", "entities": []}
{"text": "Finding Good Conversations Online : The Yahoo News Annotated Comments Corpus", "entities": []}
{"text": "This work presents a dataset and annotation scheme for the new task of identifying \" good \" conversations that occur online , which we call ERICs : Engaging , Respectful , and/or Informative Conversations . We develop a taxonomy to reflect features of entire threads and individual comments which we believe contribute to identifying ERICs ; code a novel dataset of Yahoo News comment threads ( 2.4k threads and 10k comments ) and 1k threads from the Internet Argument Corpus ; and analyze the features characteristic of ERICs . This is one of the largest annotated corpora of online human dialogues , with the most detailed set of annotations . It will be valuable for identifying ERICs and other aspects of argumentation , dialogue , and discourse .", "entities": []}
{"text": "Automatically curating online comments has been a large focus in recent NLP and social media work , as popular news outlets can receive millions of comments on their articles each month ( Warzel , 2012 ) . Comment threads often range from vacuous to hateful , but good discussions do occur online , with people expressing different viewpoints and attempting to inform , convince , or better understand the other side , but they can get lost among the multitude of unconstructive comments . We hypothesize that identifying and promoting these types of conversations ( ERICs ) will cultivate a more civil and constructive atmosphere in online communities and potentially encourage participation from more users . ERICs are characterized by : A respectful exchange of ideas , opinions , and/or information in response to a given topic ( s ) . Opinions expressed as an attempt to elicit a dialogue or persuade . Comments that seek to contribute some new information or perspective on the relevant topic . ERICs have no single identifying attribute : for instance , an exchange where communicants are in total agreement throughout can be an ERIC , as can an exchange with heated disagreement . Figures 1 and 2 contain two threads that are characterized by continual disagreement , but one is an ERIC and the other is not . We have developed a new coding scheme to label ERICs and identify six dimensions of comments and three dimensions of threads that are frequently seen in the comments section . Many of these labels are for characteristics of online conversations not captured by traditional argumentation or dialogue features . Some of the labels we collect have been annotated in previous work ( 2 ) , but this is the first time they are aggregated in a single corpus at the dialogue level . In this paper , we present the Yahoo News Annotated Comments Corpus ( YNACC ) , which contains 2.4k threads and 10k comments from the comments sections of Yahoo News articles . We additionally collect annotations on 1k threads from the Internet Argument Corpus ( Abbott et al , 2016 ) , representing another domain of online debates . We contrast annotations of Yahoo and IAC threads , explore ways in which threads perceived to be ERICs differ in this two venues , and identify some unanticipated characteristics of ERICs . This is the first exploration of how characteristics of individual comments contribute to the dialogue - level classification of an exchange . YNACC will facilitate research to understand ERICS and other aspects of dialogue . The corpus and annotations will be available at https : //github.com / cnap / ynacc . . Controversial Controversial Sarcastic Sarcastic $ ' \u2639 % ' \u2639 \" % & ) \u270b % & ) \u270b % ' \u2639 \u2639 \" % & \u2639 \u2639 \" when a country has to use force to keep it 's businesses behind a wall . . . something is very wrong . will the next step be forcing the talented and wealthy to remain ? this strategy did not work well for the soviet union . A your solution is ? B @B , lower the govt imposed costs and businesses will stay voluntarily .", "entities": []}
{"text": "just because a company was started in us , given large tax breakes in the us and makes most of its profits in the us does not mean it owes loyalty right ? they have to appease the shareholders who want more value so lower your cost of business by lowering taxes while still getting all the perks is one way of doing it .", "entities": []}
{"text": "@D - in your world who eventually pays the taxes that our gov't charges business ?", "entities": []}
{"text": "@C lowering corporate taxes does not equate to more jobs , its only equates to corporations making more money . did you think they take their profits and make high paying jobs with them ? lol wake up !", "entities": []}
{"text": "Headline : Allergan CEO : Feds blindsided us on Pfizer deal Figure 1 : An ERIC that is labeled argumentative , positive / respectful , and having continual disagreement . Controversial Mean ! \" \u2639 Controversial $ % & ' $ \" \u2639 ' Controversial Informative $ % ( ' Informative $ % ( ' Controversial $ % \u2639 ' $ \" ( ) ' $ \" ( ) ' $ \" ( ) ' $ \" ( ' ) quit your whining you are in america assimilate into american society . or go back where you came from .", "entities": []}
{"text": "american society is that of immigrants and the freedom to practice whatever religion you wish . you anti american ? F @F , you may be an immigrant , but i 'm not G the only reason you are an american is because of immigrants .", "entities": []}
{"text": "that can be said of all humans . humans migrated from africa . everyone in germany is an immigrant .", "entities": []}
{"text": "then any statement about they need to \" go back \" is irrelevant and wrong . thanks for proving my point .", "entities": []}
{"text": "floridians tell new yorkers to go back . you have no point .", "entities": []}
{"text": "just because someone says something does nt make it valid . your point has no point .", "entities": []}
{"text": "just because someone says something does nt make it valid . nothing you say is valid .", "entities": []}
{"text": "that 's your opinion . but it 's not valid . my factual statement is .", "entities": []}
{"text": "Headline : ' The Daily Show ' Nailed How Islamophobia Hurts the Sikh Community Too", "entities": []}
{"text": "This section outlines our coding scheme for identifying ERICs , with labels for comment threads and each comment contained therein . Starting with the annotation categories from the IAC and the curation criteria of Diakopoulos ( 2015 ) , we have adapted these schemes and identified new characteristics that have broad coverage over 100 comment threads ( 4 ) that we manually examined . Annotations are made at the thread - level and the comment - level . Thread - level annotations capture the qualities of a thread on the whole , while comment - level annotations reflect the characteristics of each comment . The labels for each dimension are described below . Only one label per dimension is allowed unless otherwise specified .", "entities": []}
{"text": "Agreement The overall agreement present in a thread . Agreement throughout Continual disagreement Agreement disagreement : Begins with agreement which turns into disagreement . Disagreement agreement : Starts with disagreement that converges into agreement . Constructiveness A binary label indicating when a conversation is an ERIC , or has a clear exchange of ideas , opinions , and/or information done so somewhat respectfully . 1 Constructive Not constructive Type The overall type or tone of the conversation , describing the majority of comments . Two labels can be chosen if conversations exhibit more than one dominant feature . Argumentative : Contains a lot of \" back and forth \" between participants that does not necessarily reach a conclusion . Flamewar : Contains insults , users \" yelling \" at each other , and no information exchanged . Off - Topic / digression : Comments are completely irrelevant to the article or each other , or the conversation starts on topic but veers off into another direction . Personal stories : Participants exchange personal anecdotes . Positive / respectful : Consists primarily of comments expressing opinions in a respectful , potentially empathetic manner . Snarky / humorous : Participants engage with each other using humor rather than argue or sympathize . May be on - or off - topic .", "entities": []}
{"text": "The corpus was coded by two groups of annotators : professional trained editors and untrained crowdsourced workers . Three separate annotators coded each thread . The trained editors were paid contractors who received two 30 - 45 - minute training sessions , editorial guidelines ( 2 , 000 - word document ) , and two sample annotated threads . The training sessions were recorded and available to the annotators during annotation , as were the guidelines . They could communicate their questions to the trainers , who were two authors of this paper , and receive feedback during the training and annotation phases . Because training is expensive and time consuming , we also collected annotations from untrained coders on Amazon Mechanical Turk ( AMT ) . To simplify the task for AMT , we only solicited thread - level labels , paying $ 0.75 per thread . For quality assurance , only workers located in the United States or Canada with a minimum HIT acceptance rate of 95 % could participate , and the annotations were spot - checked by the authors . Trained annotators coded 1 , 300 Yahoo threads and the 100 - thread test set on the comment - and thread - levels ; untrained annotators coded threadlevel labels of 1 , 300 Yahoo threads ( 300 of which overlapped with the trained annotations ) and 1 , 000 IAC threads ( Table 1 ) . In total , 26 trained and 495 untrained annotators worked on this task .", "entities": []}
{"text": "To assess the difficulty of the task , we also collected a rating for each thread from the trained annotators describing how confident they were with their judgments of each thread and the comments it comprises . Ratings were made on a 5 - level Likert scale , with 1 being not at all confident and 5 fully confident . The levels of confidence were high ( 3.9 \u00b1 0.7 ) , indicating that coders were able to distinguish the thread and comment codes with relative ease .", "entities": []}
{"text": "To understand what makes a thread constructive , we explore the following research questions : 1 . How does the overall thread categorization differ between ERICs and non - ERICs ? ( 5.1 ) 2 . What types of comments make up ERICs compared to non - ERICs ? ( 5.2 ) 3 . Are social signals related to whether a thread is an ERIC ? ( 5.3 )", "entities": []}
{"text": "Before examining what types of threads are ERICs , we first compare the threads coded by different sets of annotators ( trained or untrained ) and from different sources ( IAC or Yahoo ) . We measure the significance of annotation group for each label with a test of equal proportions for binary categories ( constructiveness and each thread type ) and a chi - squared test of independence for the agreement label . Overall , annotations by the trained and untrained annotators on Yahoo threads are very similar , with significant differences only between some of the thread type labels ( Figure 3 ) . We posit that the discrepancies between the trained and untrained annotators is due to the former 's training sessions and ability to communicate with the authors , which could have swayed annotators to make inferences into the coding scheme that were not overtly stated in the instructions . The differences between Yahoo and IAC threads are more pronounced . The only label for which there is no significant difference is personal stories ( p = 0.41 , between the IAC and trained Yahoo labels ) . All other IAC labels are significantly different from both trained and untrained Yahoo labels ( p < 0.001 ) . ERICs are more prevalent in the IAC , with 70 % of threads labeled constructive , compared to roughly half of Yahoo threads . On the whole , threads from the IAC are more concordant and positive than from Yahoo : they have more agreement and less disagreement , more than twice as many positive / respectful threads , and fewer than half the flamewars . For Yahoo threads , there is no significant difference between trained and untrained coders for constructiveness ( p = 0.11 ) and the argumentative thread type ( p = 0.07 ; all other thread types are significant with p < 10 \u22125 ) . There is no significant difference between the agreement labels , either ( p = 1.00 ) . Untrained coders are more likely than trained to classify threads using emotional labels like snarky , flamewar , and positive / respectful , while trained annotators more frequently recognize off - topic threads . These differences should be taken into consideration for evaluating the IAC codes , and for future efforts collecting subjective annotations through crowdsourcing . We measure the strength of relationships between labels with the phi coefficient ( Figure 4 ) . There is a positive association between ERICs and all agreement labels in both Yahoo ( trained ) and IAC threads , which indicates that concord is not necessary for threads to be constructive . The example in Figure 1 is a constructive thread that is argumentative and contains disagreement . Thread types associated with non - ERICs are flamewars , off - topic digressions , and snarky / humorous exchanges , which is consistent across data sources . The labels from untrained annotators show a stronger correlation between flamewars and not constructive compared to the trained annotators , but the former also identified more flamewars . Some correlations are expected : across all annotating groups , there is a positive correlation between threads labeled with agreement throughout and positive / respectful , and disagreement throughout is correlated with argumentative ( Figures 1 and 2 ) and , to a lesser degree , flamewar . The greatest difference between the IAC and Yahoo are the thread types associated with ERICs . In the IAC , the positive / respectful label has a much stronger positive relationship with constructive than the trained Yahoo labels , but this could be due to the difference between trained and untrained coders . Argumentative has a positive correlation with constructive in the Yahoo threads , but a weak negative relationship is found in the IAC . In both domains , threads characterized as offtopic , snarky , or flamewars are more likely to be non - ERICs . Threads with some level of agreement characterized as positive / respectful are commonly ERICs . A two - tailed z - test shows a significant difference between the number of ERICs and non - ERICs in Yahoo articles in the Arts & Entertainment , Finance , and Lifestyle categories ( p < 0.005 ; Figure 5 ) .", "entities": []}
{"text": "We next consider the codes assigned by trained annotators to Yahoo comments ( Figure 6 ) . The majority of comments are not persuasive , reply to a previous comment , express disagreement , or have negative sentiment . More than three times as many comments express disagreement than agreement , and comments are labeled negative seven times as frequently as positive . Approximately half of the comments express disagreement or a negative sentiment . Very few comments are funny , positive , sympathetic , or contain a personal story ( < 10 % ) . Encouragingly , only 6 % of comments are off - topic with the conversation , suggesting that participants are attuned to and respectful of the topic . Only 20 % of comments are informative , indicating that participants infrequently introduce new information to complement the article or discussion . The only strong correlations are between the binary labels , but the moderate correlations provide insight into the Yahoo threads ( Figure 7 ) . Some relationships accord with intuition . For instance , participants tend to go off - topic with the article when they are responding to others and not during broadcast messages ; comments expressing disagreement with a commenter are frequently posted in a reply to a commenter ; comments expressing agreement tend to be sympathetic and have positive sentiment ; and mean comments correlate with negative sentiment . Commenters in this domain also express disagreement without particular nastiness , since there is no correlation between disagreement and mean or sarcastic comments . The informative label is moderately correlated with persuasiveness , suggesting that comments containing facts and new information are more convincing than those without . The correlation between comment and thread labels is shown in Figure 7 . Many of the relationships are unsurprising , like off - topic threads tend to have off - topic comments , personal - story threads have personal - story comments ; thread agreement levels correlate with comment - level In accord with our definition of ERICs , constructiveness is positively correlated with informative and persuasive comments and negatively correlated with negative and mean comments . From these correlations one can infer that argumenta - tive threads are generally respectful because , while they are strongly correlated with comments that are controversial or express disagreement or a mixed sentiment , there is no correlation with mean and very little with negative sentiment . More surprising is the positive correlation between controversial comments and constructive threads . Controversial comments are more associated with ERICs , not non - ERICs , even though the controversial label also positively correlates with flamewars , which are negatively correlated with constructiveness . The examples in Figures 1 - 2 both have controversial comments expressing disagreement , but comments in the second half of the non - ERIC veer off - topic and are not persuasive , where the ERIC stays on - topic and persuasive .", "entities": []}
{"text": "Previous work has taken social signals to be a proxy for thread quality , using some function of the total number of votes received by comments within a thread ( e.g. , Lee et al ( 2014 ) ) . Because earlier research has indicated that user votes are not completely independent or objective ( Sipos et al , 2014 ; Danescu - Niculescu - Mizil et al , 2009 ) , we take the use of votes as a proxy for quality skeptically ad perform our own exploration of the relationship between social signals and the presence of ERICs . On Yahoo , users reacted to comments with a thumbs up or thumbs down and we collected the total number of such reactions for each comment in our corpus . First , we compare the total number of thumbs up ( TU ) and thumbs down ( TD ) received by comments in a thread to the coded labels to determine whether there are any relationships between social signals and threads qualities . We calculate the relationship between labels in each category with TU and TD with Pearson 's coefficient for the binary labels and a one - way ANOVA for the agreement category . The strongest correlation is between TD and untrained annotators ' perception of flamewars ( r = 0.21 ) , and there is a very weak to no correlation ( positive or negative ) between the other labels and TU , TD , or TU\u2212TD . There is moderate correlation between TU and TD ( r = 0.46 ) , suggesting that threads that elicit reactions tend to receive both thumbs up and down . The correlation between TU and TD received by each comment is weaker ( r = 0.23 ) . Comparing the comment labels to the TU and TD received by each comment also show little correlation . Comments that reply to a specific commenter are negatively correlated with TU , TD , and TU\u2212TD ( r = 0.30 , - 0.25 , and - 0.22 , respectively ) . The only other label with a non - negligible correlation is disagreement with a commenter , which negatively correlates with TU ( r = \u22120.21 ) . There is no correlation between social signal and the presence of ERICs or non - ERICs . These results support the findings of previous work and indicate that thumbs up or thumbs down alone ( and , presumably , up / down votes ) are inappropriate proxies for quality measurements of comments or threads in this domain .", "entities": []}
{"text": "We have developed a coding scheme for labeling \" good \" online conversations ( ERICs ) and created the Yahoo News Annotated Comments Corpus , a new corpus of 2.4k coded comment threads posted in response to Yahoo News articles . Additionally , we have annotated 1k debate threads from the IAC . These annotations reflect several different characteristics of comments and threads , and we have explored their relationships with each other . ERICs are characterized by argumentative , respectful exchanges containing persuasive , informative , and/or sympathetic comments . They tend to stay on topic with the original article and not to contain funny , mean , or sarcastic comments . We found differences between the distribution of annotations made by trained and untrained annotators , but high levels of agreement within each group , suggesting that crowdsourcing annotations for this task is reliable . YNACC will be a valuable resource for researchers in multiple areas of discourse analysis .", "entities": []}
{"text": "We are grateful to Danielle Lottridge , Smaranda Muresan , and Amanda Stent for their valuable input . We also wish to thank the anonymous reviewers for their feedback .", "entities": []}
{"text": "Linguistic Reflexes of Well - Being and Happiness in Echo", "entities": []}
{"text": "Different theories posit different sources for feelings of well - being and happiness . Appraisal theory grounds our emotional responses in our goals and desires and their fulfillment , or lack of fulfillment . Self - Determination theory posits that the basis for well - being rests on our assessments of our competence , autonomy and social connection . And surveys that measure happiness empirically note that people require their basic needs to be met for food and shelter , but beyond that tend to be happiest when socializing , eating or having sex . We analyze a corpus of private micro - blogs from a well - being application called ECHO , where users label each written post about daily events with a happiness score between 1 and 9 . Our goal is to ground the linguistic descriptions of events that users experience in theories of well - being and happiness , and then examine the extent to which different theoretical accounts can explain the variance in the happiness scores . We show that recurrent event types , such as OBLIGATION and IN - COMPETENCE , which affect people 's feelings of well - being are not captured in current lexical or semantic resources .", "entities": []}
{"text": "This research was partially supported by NSF Robust Intelligence # IIS - 1302668 - 002 and NSF HCC # IIS - 1321102 .", "entities": []}
{"text": "After preprocessing and extracting the vocabulary from our training documents , each word type is converted to its embedding representation ( averaging all of its tokens for contextualized embeddings ; details in 5.3 ) . Following this we apply the various clustering algorithms on the entire training corpus vocabulary to obtain k clusters , using weighted ( 3.2 ) or unweighted word types . After the clustering algorithm has converged , we obtain the top J words ( 3.1 ) from each cluster for evaluation . Note that one potential shortcoming of our approach is the possibility of outliers forming their own cluster , which we leave to future work . Figure 1 : The figure on the left shows the cluster center ( ) without weighting , while the figure on the right shows that after weighting ( larger points have higher weight ) a hopefully more representative cluster center is found . Note that top words based on distance from the cluster center could still very well be low frequency word types , motivating reranking ( 3.3 ) .", "entities": []}
{"text": "When obtaining the top - J words that make up a cluster 's topic , we also consider reranking terms , as there is no guarantee that words closest to cluster centers are important word types . We will show in Table 2 that without reranking , clustering yields \" sensible \" topics but low NPMI scores .", "entities": []}
{"text": "To incorporate corpus statistics into the clustering algorithm , we examine three different schemes 2 to assign weights to word types , where n t is the count of word type t in corpus D , and d is a document : tf = n t t n t ( 1 ) tf - df = tf | { d D | t d } | | D | ( 2 ) tf - idf = tf log | D | | { d D | t d } | + 1 ( 3 ) These scores can now be used for weighting word types when clustering ( w ) , reranking top 100 words ( r ) after , both ( w r ) , or neither ( simply ) . We find that simply using tf outperforms the other weighting schemes ( App . C ) . Our results and subsequent analysis in 6 uses tf for weighting and reranking .", "entities": []}
{"text": "Our implementation is freely available online . 4", "entities": []}
{"text": "We use the 20 newsgroup dataset ( 20NG ) which contains around 18000 documents and 20 categories , 5 and a subset of Reuters21578 6 which contains around 10000 documents .", "entities": []}
{"text": "Our main results are shown in Table 1 .", "entities": []}
{"text": "To further understand the effect of other centroid based algorithms on topic coherence , we also applied the k - medoids ( KD ) clustering algorithm . KD is a hard clustering algorithm similar to KM but less sensitive to outliers . As we can see in Table 3 , in all cases KD usually did as well or worse than KM . KD also did relatively poorly after frequency reranking . Where KD did do better than KM , the difference is not very striking and the NPMI scores were still quite below the other top performing models .", "entities": []}
{"text": "Mises - Fisher Mixture", "entities": []}
{"text": "Self - Alignment Pretraining for Biomedical Entity Representations", "entities": []}
{"text": "We divide our experimental datasets into two categories ( 1 ) scientific language datasests where the data is extracted from scientific papers and ( 2 ) social media language datasets where the data is coming from social media forums like Reddit.com . For an overview of the key statistics , see Tab . 3 .", "entities": []}
{"text": "All our experiments are conducted on a server with specifications listed in Tab . 8 . C Other Details", "entities": []}
{"text": "The full table of supervised baseline models is provided in Tab . 4 .", "entities": []}
{"text": "Tab . 9 lists hyper - parameter search space for obtaining the set of used numbers . Note that the chosen hyper - parameters yield the overall best performance but might be sub - optimal on any single dataset . Also , we balanced the memory limit and model performance . C.3 A High - Resolution Version of Fig . 1 We show a clearer version of t - SNE embedding visualisation in Fig . 3 .", "entities": []}
{"text": "PUDMEDBERT Figure 3 : Same as Fig . 1 in the main text , but generated with a higher resolution .", "entities": []}
{"text": "Evaluating Multiple System Summary Lengths : A Case Study", "entities": []}
{"text": "We would like to thank the anonymous reviewers for their constructive comments . We thank Yinfei Yang for his assistance in producing the IC - SISumm summaries that we utilized in our analysis . This work was supported in part by grants from the MAGNET program of the Israel Innovation Authority ; by the German Research Foundation through the German - Israeli Project Cooperation ( DIP , grant DA 1600/1 - 1 ) ; by the BIU Center for Research in Applied Cryptography and Cyber Security in conjunction with the Israel National Cyber Bureau in the Prime Ministers Office ; and by the Israel Science Foundation ( grants 1157/16 and 1951/17 ) .", "entities": []}
{"text": "MovieChats : Chat like Humans in a Closed Domain", "entities": []}
{"text": "[ context ] dialogue context ation . Unlike in traditional task - oriented systems where subtasks are decomposed separately , we opt for a simple and unified approach by casting all subtasks into sequence prediction . A special token is injected in the beginning to indicate which subtask to perform ( Hosseini - Asl et al , 2020 ; Peng et al , 2020 ) . Table 3 shows the schema representation for different components . The condition and the target are concatenated into a single sequence and then fed into the language model to train . For example , the task of predicting the intent given the dialogue context will be transformed into \"", "entities": []}
{"text": "We tokenize the Text in the unit of Chinese characters and keep all unique non - Chinese unique tokens appearing for more than 5 times . The whole vocabulary contains 13 , 317 words . We train our model on 24 Nvidia V100 GPUs ( 32 GB ) with three different model sizes as shown in PyTorch ( Paszke et al , 2019 ) .", "entities": []}
{"text": "A : \u5a01\u5c14\u53f2\u5bc6\u65af\u6f14\u6280\u771f\u7684\u5f88\u68d2 ( Will Smith 's acting skill is really good ) . B : \u4ed6\u7684\u5f53\u5e78\u798f\u6765\u6572\u95e8\u592a\u7ecf\u5178\u4e86 ( His The Pursuit of Happyness is a classic ) . A : \u4e00\u76f4\u90fd\u6302\u5728\u7535\u5f71\u6392\u884c\u699c\u9760\u524d\u7684\u4f4d\u7f6e ( That ' always among top ranked movies ) . B : \u55ef\u55ef\uff0c\u8fd9\u90e8\u7535\u5f71\u771f\u7684\u5f88\u52b1\u5fd7\u554a ( Yes , it 's really motivational ) . A : \u5a01\u5c14\u53f2\u5bc6\u65af\u4e5f\u6f14\u51fa\u4e86\u5f88\u60e8\u7684\u611f\u89c9\u4e86 ( Will Smith plays like he is a real tragedy ) . B : \u6f14\u6280\u7279\u522b\u597d ( Yes , he acts pretty well ) .", "entities": []}
{"text": "Table 10 shows examples comparing our dataset and the others . As observed , forum conversations are mostly single - turn QA or comments . Current crowd - sourced datasets are either collected on constrained scenarios ( the scenario in ( Zhou et al , 2018b ) fixed the roles in a conversation as one introducer and one listener ) , or unconstrained but prompting people to deliberately connect knowledge . Our dataset simulates real - life conversations to the largest extent . We classify the utterances into one of 15 aspects .", "entities": []}
{"text": "As for the four human evaluation metrics . The first two will focus only on the conversational backbones without considering domain knowledge . The second two will check if the responses can provide informative and correct responses powered by domain knowledge . The detailed definitions of them are :", "entities": []}
{"text": "All the three metrics are evaluated by three crow - workers each except for factuality . As evaluating factuality requires in - depth knowledge about one movie , it is quite difficult for random human evaluators to judge them . Even if we filter to only keep people who have watched a movie , it is hard to guarantee they can recall all the scenes in the movie . Therefore , the factuality check is only done by the person who performed this dialogue . In the static evaluation , it is evaluated by the annotator who produced the reference response . In the interactive evaluation , it is evaluated by the person who chat with the bot . It is nevertheless not accurate though . However , if the bot can cheat the human into believing its false information , it can also somehow be considered a \" success \" . We provide examples for guiding the human evaluators in Table 12 . Table 13 shows some interactive examples with humans . We observe Mitsuku can XiaoIce perform decently in single - turn exchanges but strongly struggle at understanding multi - turn user intents . Most conversations stop at turn 4 and will not move on .", "entities": []}
{"text": "Determining a Person 's Suicide Risk by Voting on the Short - Term History of Tweets for the CLPsych 2021 Shared Task", "entities": []}
{"text": "In this shared task , we accept the challenge of constructing models to identify Twitter users who attempted suicide based on their tweets 30 and 182 days before the adverse event 's occurrence . We explore multiple machine learning and deep learning methods to identify a person 's suicide risk based on the short - term history of their tweets . Taking the real - life applicability of the model into account , we make the design choice of classifying on the tweet level . By voting the tweet - level suicide risk scores through an ensemble of classifiers , we predict the suicidal users 30 - days before the event with an 81.8 % true - positives rate . Meanwhile , the tweet - level voting falls short on the six - month - long data as the number of tweets with weak suicidal ideation levels weakens the overall suicidal signals in the long term .", "entities": []}
{"text": "Pre - processing : We clean the tweets by removing user mentions , URLs , punctuation , and non - ASCII characters , then normalize hashtags into words using a probabilistic splitting tool based on English Wikipedia unigram frequencies ( Anderson , 2019 ) . We maintain stopwords and emojis , as they might provide clues regarding the suicidal ideation of the users . Experimentation Framework : Before designing the experiments , we face a critical choice : Should we merge all tweets per user , or should we perform the assessment per tweet and then aggregate the scores ? To answer this , we consider a real - life risk assessment system . The system should provide a score every time someone posts a tweet . Some social media domains already implement these systems ( Ji et al , 2020 ) . Hence , we select to train the models to classify tweets , then apply majority voting ( MV ) per user to compute a risk score based on the tweet scores . Our framework is described in Figure 1 .", "entities": []}
{"text": "Secure access to the shared task dataset was provided with IRB approval under University of Maryland , College Park protocol 1642625 .", "entities": []}
{"text": "The organizers are particularly grateful to the users who donated data to the OurDataHelps project without whom this work would not be possible , to Qntfy for supporting the OurDataHelps project and making the data available , to NORC for creating and administering the secure infrastructure , and to Amazon for supporting this research with computational resources on AWS . The authors are thankful to the anonymous reviewers for their constructive comments and valuable suggestions .", "entities": []}
{"text": "Welcome to ACL 2017 in Vancouver , Canada ! This is the 55th annual meeting of the Association for Computational Linguistics . A tremendous amount of knowledge has been presented at more than half a century 's worth of our conferences . Hopefully , some of it is still relevant now that deep learning has solved language . We are anticipating one of the largest ACL conferences ever . We had a record number of papers submitted to the conference , and a record number of industry partners joining us as sponsors of the conference . We are on track to be one of the best attended ACL conferences to date . I hope that this year 's conference is intellectually stimulating and that you take home many new ideas and techniques that will help extend your own research .", "entities": []}
{"text": "Revisiting Pretraining with Adapters", "entities": []}
{"text": "Details of hyperparameter setting including the learning rates for the best performing results are provided in Table 4 , 5 , and 6 .", "entities": []}
{"text": "We present validation performance in Table 7 and Figure 3 and 8 .", "entities": []}
{"text": "We would like to thank Zsolt Kira , Mandeep Baines , Shruti Bhosale , and Siddharth Goyal for helpful feedback and suggestions . We also would like to thank anonymous reviewers for their insightful comments on the earlier version of the paper .", "entities": []}
{"text": "The work presented in this paper has been funded by the Slovenian Research Agency national basic research project \" Resources , methods and tools for the understanding , identification and classification of various forms of socially unacceptable discourse in the information society \" ( ARRS J7 - 8280 , 2017 - 2020 ) , and by the Slovenian research infrastructure CLARIN.SI .", "entities": []}
{"text": "This work is part of the FoTran project , funded by the European Research Council ( ERC ) under the European Union 's Horizon 2020 research and innovation programme ( grant agreement No 771113 ) . The authors gratefully acknowledge the support of the CSC - IT Center for Science , Finland , for computational resources . Finally , We would also like to acknowledge NVIDIA and their GPU grant .", "entities": []}
{"text": "Team JARS : DialDoc Subtask 1 - Improved Knowledge Identification with Supervised Out - of - Domain Pretraining", "entities": []}
{"text": "As discussed earlier , subtask 1 of DialDoc shared task is formulated as a span selection problem . Therefore , in order to learn to predict the correct span , we use an extractive question - answering setup .", "entities": []}
{"text": "In this section , we discuss our experimental setup in detail .", "entities": []}
{"text": "We use default parameters set by the subtask baseline provided by the authors . 4 However , we reduce the training per - device batch - size to 2 to accommodate the large models on an Nvidia Geforce GTX 1080 Ti 12 GB GPU . We stop the continual out - ofdomain supervised pretraining after 2 epochs .", "entities": []}
{"text": "We now present the results for different experimental setups we tried for DialDoc subtask 1 .", "entities": []}
{"text": "The final dataset contains 7 , 953 annotated tweets for \" Donald Trump \" , 7 , 296 for \" Joe Biden \" and 6 , 325 for \" Bernie Sanders \" , respectively . The label distribution of each target is shown in Table 6 . Each tweet is annotated with a stance label \" Favor \" or \" Against \" . We created the training , validation and testing sets following an 80/10/10 split . We note that P - STANCE is more than 3 times larger than the previous benchmark ( Mohammad et al , 2016a ) .", "entities": []}
{"text": "We thank the National Science Foundation and Amazon Web Services for support from grants IIS - 1912887 and IIS - 1903963 which supported the research and the computation in this study . We also thank our reviewers for their insightful comments .", "entities": []}
{"text": "In this section , we describe three methods for generating pseudo data . In Section 4 , we experimentally compare these methods .", "entities": []}
{"text": "The goal of our experiments is to investigate aspect ( i ) - ( iii ) introduced in Section 2 . To ensure that the experimental findings are applicable to GEC in general , we design our experiments by using the following two strategies : ( i ) we use an off - the - shelf EncDec model without any task - specific architecture or techniques ; ( ii ) we conduct hyper - parameter tuning , evaluation and comparison of each method or setting on the validation set . At the end of experiments ( Section 4.5 ) , we summarize our findings and propose suitable settings . We then perform a single - shot evaluation of their performance on the test set .", "entities": []}
{"text": "We investigate the effectiveness of the seed corpus T for generating pseudo data D p . The three corpora ( Wikipedia , SimpleWiki and Gigaword ) are compared in Table 3 . We set | D p | = 1.4M. The difference in F 0.5 is small , which implies that the seed corpus T has only a minor effect on the model performance . Nevertheless , Gigaword consistently outperforms the other two corpora . In particular , DIRECTNOISE with Gigaword achieves the best value of F 0.5 among all the configurations .", "entities": []}
{"text": "We compare the JOINT and PRETRAIN optimization settings . We are interested in how each setting performs when the scale of the pseudo data D p compared with that of the genuine parallel data D g is ( i ) approximately the same ( | D p | = 1.4 M ) and ( ii ) substantially bigger ( | D p | = 14 M ) . Here , we use Wikipedia as the seed corpus T instead of SimpleWiki or Gigaword for two reasons . First , SimpleWiki is too small for the experiment ( b ) ( see Table 1 ) . Second , the fact that Gigaword is not freely available makes it difficult for other researchers to replicate our results . ( a ) Joint Training or Pretraining Table 4 presents the results . The most notable result here is that PRETRAIN demonstrates the properties of more pseudo data and better performance , whereas JOINT does not . For example , in BACKTRANS ( NOISY ) , increasing | D p | ( 1.4 M 14 M ) improves F 0.5 on PRETRAIN ( 41.1 44.5 ) . By contrast , F 0.5 does not improve on JOINT ( 40.4 40.3 ) . An intuitive explanation for this case is that when pseudo data D p are substantially more than genuine data D g , the teaching signal from D p becomes dominant in JOINT . PRETRAIN alleviates this problem because the model is trained with only D g during fine - tuning . We therefore suppose that PRETRAIN is crucial for utilizing extensive pseudo data . F 0.5 = 45.9 .", "entities": []}
{"text": "In this study , we investigated several aspects of incorporating pseudo data for GEC . Through extensive experiments , we found the following to be effective : ( i ) utilizing Gigaword as the seed corpus , and ( ii ) pretraining the model with BACKTRANS ( NOISY ) data . Based on these findings , we proposed suitable settings for GEC . We demonstrated the effectiveness of our proposal by achieving stateof - the - art performance on the CoNLL - 2014 test set and the BEA - 2019 test set .", "entities": []}
{"text": "We thank the three anonymous reviewers for their insightful comments . We are deeply grateful to Takumi Ito and Tatsuki Kuribayashi for kindly sharing the re - implementation of BACKTRANS ( NOISY ) . The work of Jun Suzuki was supported in part by JSPS KAKENHI Grant Number JP19104418 and AIRPF Grant Number 30AI036 - 8 .", "entities": []}
{"text": "The author would like to thank his manager for supporting this project , and the anonymous reviewers for their thoughtful comments which helped improve the presentation of this work .", "entities": []}
{"text": "In this section , we describe the details of our method , which is illustrated in Figure 1 . We feed different word embedding sets into neural networks and train these neural networks separately . When predicting the labels of tweets in testing set , we sum label probabilities of all neural network to make final decisions .", "entities": []}
{"text": "We learn different RCNN models with different embedding sets as input . Formally , we have s embedding sets which are denoted as { E 1 , E 2 , , E s } , p 1 = RCNN 1 ( { x i } l i=1 , E 1 ) , p 2 = RCNN 2 ( { x i } l i=1 , E 2 ) , . . . , p s = RCNN s ( { x i } l i=1 , E s ) , p = 1\u2264i\u2264s p i . y = argmax 1\u2264i\u2264 | Y | p i , where y is the predicted label .", "entities": []}
{"text": "In this section , we analyze the incorrect predictions of our system in SemEval 2017 . We summarize four kinds of errors in our system . The first one is that some decisions need domain knowledge , which our method only can learn from the labeled datasets . The instances are as follows : Messi 's 100 international goals for Barcelona # fcblive https://t.co/fMkglvusL1 [ via @thereisagenius ] . Predicted label : neutral , golden label : positive # Trudeau gives your cash to # Terrorist # Hamas - influenced group - # UNRWA - @Can - diceMalcolm https://t.co/5i5o2qwRWl Predicted label : neutral , golden label : negative Messis 9 goals in CL are more than 20 of the 32 teams in the competition have scored in total , and he s tied with five other sides # fcblive Predicted label : neutral , golden label : positive The second one is emoticons in tweet , as most of word embedding sets do not include emoticon embeddings and emoticons are always with senti - ments . The instances are described in Figure 2 The third one is that sentiments are not consistent in sentences . For example , the first half part is positive , while the second half part is negative , in this case , our system would predict ' positive ' or ' negative ' , the golden label is neutral . @jimmyfallon 1 . Emily 2 . Michel 3 . Kirk 4 . TJ . Love the quirky ones and Emily coz she 's such a BIATCH ! # gilmoregirlstop4 . predicted label positive , golden label : neutral The fourth one is the sarcasm , such as : # Hamas leader : # Trump may be a # Jew https://t.co/jGFZTvj2pF. predicted label positive , golden label : negative", "entities": []}
{"text": "We propose a simple and effective ensemble method to boost the neural twitter sentiment classification . By using different embedding sets , the system can cover more words and encode more sentiment information . The results on datasets of previous SemEval and SemEval 2017 show the effectiveness of our method . Moreover , error analysis is conducted to propose the main challenges for our method . We release our code for system duplicability .", "entities": []}
{"text": "Machine Comprehension Improves Domain - Specific Japanese Predicate - Argument Structure Analysis", "entities": []}
{"text": "We construct PAS - QA and RC - QA datasets in the driving domain . Both the QA datasets consist of triplets of a document , a question and its answer as in existing RC - QA datasets . We employ crowdsourcing to create large - scale datasets in a short time . Figure 1 and Figure 2 show examples of our PAS - QA and RC - QA datasets .", "entities": []}
{"text": "We construct a PAS - QA dataset in which a question asks an omitted argument for a predicate . We focus on the ga case ( nominative ) , the wo case ( accusative ) , and the ni case ( dative ) , which are targeted in the Japanese PAS analysis literature ( Shibata et al , 2016 ; Shibata and Kurohashi , 2018 ; Kurita et al , 2018 ; Ouchi et al , 2017 ) . As a source corpus , we use blog articles included in the Driving Experience Corpus ( Iwai et al , 2019 ) . We first detect a predicate that has an omitted argument of either of the target three cases by applying the existing PAS analyzer KNP 1 to the corpus . KNP tends to overgenerate such predicates , but most erroneous ones are filtered out by the following crowdsourcing step . We extract the sentence that contains the predicate and preceding three sentences as a document . Then , we automatically generate a question using the following template for nominative . \" author , \" \" other , \" and \" not sure . \" The details of this procedure are described in the appendix . We generated questions from 2 , 146 blog articles . We asked five crowdworkers per question using Yahoo ! crowdsourcing 2 . We adopted triplets with three or more votes if they are not \" not sure . \" For accusative and dative PAS - QA questions , we adopted triplets if they are \" other . \" In this case , there is not any antecedent of a zero pronoun in a document , and the answer is \" NULL . \" For nominative PAS - QA questions , we did not adopt triplets if they are \" other \" because a nominative always exists as a noun in a document or \" author . \" In addition , because \" author \" is not explicitly expressed in the document , we add a sentence \" \" ( The author wrote the following document . ) to the beginning of the document to deal with \" author \" in MC models . We record the answers as spans in a document or NULL . We randomly extracted 100 questions for each case from the PAS - QA dataset and judged whether we can answer them . As a result , 97 % nominative , 87 % accusative and 68 % dative questions were answerable . For accusative and dative , we checked all the questions and chose answerable ones . Finally , we created 12 , 468 nominative , 3 , 151 accusative and 1 , 069 dative triplets including 476 accusative and 126 dative questions whose answers are NULL . It took approximately 32 hours and approximately 210 , 000 JPY to create this dataset .", "entities": []}
{"text": "We", "entities": []}
{"text": "\" ( the slope ) , looked correct although \" \" ( the slope ) was the only answer from crowdsourcing . Supplying multiple answers considering coreference relations is our future work . From these results , we think that it is important to use an RC - QA dataset to acquire domain knowledge , and suggest that it is better to construct both PAS - QA and RC - QA datasets to develop a PAS analyzer for a new domain .", "entities": []}
{"text": "LightningDOT : Pre - training Visual - Semantic Embeddings for Real - Time Image - Text Retrieval", "entities": []}
{"text": "In this section , we present the proposed Light - ningDOT framework , which consists of two deep Transformers as image and language encoders . We first introduce three tasks designed to pre - train the model , then present our inference pipeline from offline feature extraction to online instant retrieval .", "entities": []}
{"text": "This section discusses our experiments on pretraining and evaluating LightningDOT on downstream ITR benchmarks .", "entities": []}
{"text": "Analysing the Integration of Semantic Web Features for Document Planning across Genres", "entities": []}
{"text": "Language is usually studied and analysed from different disciplines generally on the premise that it constitutes a form of communication which pursues a specific objective . The discourse , in that sense , can be understood as a text which is constructed to express such objective . When a discourse is created , its production is related to some textual genre , usually connected with some pragmatic features , like the intention of the writer or the audience to whom is addressed , both conditioning the use of language . But genres can be considered as well as compounds of different pieces of text with a certain degree of order , each one seeking for more concrete objectives . This paper presents a proposal to learn such features as a way to generate richer document plans , applying clustering techniques over annotated documents .", "entities": []}
{"text": "The current research is carried out from a conception of Natural Language Generation ( NLG ) for which the creation of a text requires an intermediate output called a document plan . It is by the macroplanning stage that the system provides this plan of selected and ordered content . At present , our work is focused on how to elaborate that plan in order to meet some requisites regarding flexibility of the system : it should be able to produce different outcomes conditioned by the communicative goal , the audience , ... the context , on the whole . Henceforth , the main aim of our current research is to enrich the pragmatic facet of the NLG process . The expected outcome is a scheme or ordering of the ideas that should be realised in a set of cohesive and coherent sentences and paragraphs . According to some theories of the discourse ( Bakhtin , 2010 ; Halliday et al , 2014 ) , genres can be understood as social constructions that settle a connection between the discourse and the situation in which it is produced , reflected both in its structure and its content . According to Swavels ( 1990 ) : \" A genre comprises a class of communicative events , the members of which share some set of communicative purposes . These purposes are recognised by the expert members of the parent discourse community , and thereby constitute the rationale for the genre . This rationale shapes the schematic structure of the discourse and influences and constraints choice of content and style . \" Besides , genres become interesting because they are related to communicative purposes in different manners , from a global viewpoint to fine - grained levels . As an example , we can think on the case of a person who is looking for recommendation in review pages . Recommending would be the main , global purpose of the text he consults when it was created . But it is possible that the writer also wanted to explain the motivation of the journeynarrative , personal experience - or to describe the facilities in order to complete his review . Narration , description , recommendation , ... they represent low - level functions of the text related to the intention of the writer and , in some cases , they can be identified as different sets of sentences . This lead us to the possibility of learning the structure of the text and its features , which differs from one genre to another . In reviews , the presence and order of the parts is not strict . Maybe one traveller does not share his personal story , but also he describes the room and recom - mends the brand , while another one first evaluates and then describes . An example to illustrate this can be found in table 1 . Conversely , it would make no sense to write a scientific article that reports the results before explaining the methodology or not explaining it at all , for example .", "entities": []}
{"text": "Personal Experience : On our last trip to Hawaii my husband and I ... As an added bonus , we were given ... We decided to take advantage of ...", "entities": []}
{"text": "The lobby is adorned with lush gardens ... Alongside the gardens are tropical birds ... The rooms are spacious .", "entities": []}
{"text": "If you are ever fortunate enough to visit the beautiful island of Kauai , try to stay at the H Regency , you wo n't be disappointed .", "entities": []}
{"text": "The W New York is on Lexington right ... The rooms are just as small as before ... The lobby of the hotel is also ...", "entities": []}
{"text": "Being a corporate lawyer I travel ... The first time I was in a small room ... The second time I could not believe ...", "entities": []}
{"text": "Having pointed out the expanse of the related work , our approach wants to overcome its limitations . On the one hand , in the sense of being suitable for any genre , not a particular one . On the other hand , focusing on several types of features at the same time , in order to propose a more comprehensive description of the parts of a discourse . With regard to accomplish such a project , the selection and design of the proper features becomes a challenging task itself , strongly related to the aim of the investigation . Specifically , we try to detect the features that may reveal links with the functionality or purpose of the paragraph that includes them . We have begun annotating several aspects by means of linguistic tools and resources : Freeling ( Padr\u00f3 and Stanilovsky , 2012 ) for PoS annotation and Entity Recognition and ADESSE ( Garc\u00eda - Miguel et al , 2010 ) as a source of verb senses from a semantic perspective .", "entities": []}
{"text": "Until now , some experiments have been performed over a corpus of Spanish reviews extracted from Tripadvisor . The reviews were segmented into sentences , and some figures regarding semantic and morphological features were computed after dividing each document in regions ( sets of sentences ) , increasing their number from one block up to four blocks of sentences . Table 3 shows some statistics of the corpus employed . In order to strengthen the results , corpora of other genres with different degree of flexibility in their structure are being analysed : tales , news and Wikipedia articles are to be compared with the for - mer outcomes . The length of the blocks is the result of a proportional division of the length of the document for now . As the research advances , new experiments will be developed to determine a more accurate size for the pseudo - paragraphs . With the ideas introduced in the section 4 , our next step and proposal , includes improving the significance of the features with which the clustering algorithms have to work , trying to reveal an inner structure of the text related to its genre and purposes . The better our features are , the more precise the descriptions we can do of the discourse areas .", "entities": []}
{"text": "This research work has been supported by the Generalitat Valenciana by the grant ACIF/2016/501.It has also funded by the University of Alicante , Spanish Government and the European Commission through the projects , \" Explotaci\u00f3n y tratamiento de la informaci\u00f3n disponible en Internet para la anotaci\u00f3n y generaci\u00f3n de textos adaptados al usuario \" ( GRE13 - 15 ) and \" DIIM2.0 : Desarrollo de t\u00e9cnicas Inteligentes e Interactivas de Miner\u00eda y generaci\u00f3n de informaci\u00f3n sobre la web 2.0 \" ( PROMETEOII/2014/001 ) , TIN2015 - 65100 - R , TIN2015 - 65136 - C2 - 2 - R , and SAM ( FP7 - 611312 ) , respectively .", "entities": []}
{"text": "Distilling Relation Embeddings from Pre - trained Language Models", "entities": []}
{"text": "In this section , we describe our proposed relation embedding model ( RelBERT henceforth ) . To obtain a relation embedding for given a word pair ( h , t ) , we first convert it into a sentence s , called the prompt . We then feed the prompt through the LM and average the contextualized embeddings ( i.e. the output vectors ) to get the relation embedding of ( h , t ) . These steps are illustrated in Figure 1 and explained in more detail in the following .", "entities": []}
{"text": "To better understand how relation embeddings are learned , in this section we analyze the model 's performance in more detail .", "entities": []}
{"text": "In this section , we show additional experimental results that complement the main results of the paper .", "entities": []}
{"text": "In this section , we explain models ' configuration in the experiments , and details on RelBERT 's training time .", "entities": []}
{"text": "Training a single RelBERT model with a custom prompt takes about half a day on two V100 GPUs . Additionally , to achieve prompt by AutoPrompt technique takes about a week on a single V100 , while P - tuning takes 3 to 4 hours , also on a single V100 .", "entities": []}
{"text": "In this section , we analyze our experimental results based on prediction breakdown and provide an extended qualitative analysis .", "entities": []}
{"text": "Tables 13 shows the nearest neighbors of a number of selected word pairs , in terms of their RelBERT and RELATIVE embeddings . In both cases cosine similarity is used to compare the embeddings and the pair vocabulary of the RELATIVE model is used to determine the universe of candidate neighbors . The results for the RelBERT embeddings show their ability to capture a wide range of relations . In most cases the neighbors make sense , despite the fact that many of these relations are quite different from those in the SemEval dataset that was used for training RelBERT . The results for RELATIVE are in general much noisier , suggesting that REL - ATIVE embeddings fail to capture many types of relations . This is in particular the case for the morphological examples , although various issues can be observed for the other relations as well .", "entities": []}
{"text": "Jose Camacho - Collados acknowledges support from the UKRI Future Leaders Fellowship scheme .", "entities": []}
{"text": "Once documents and labels have been processed as described previously , we assign a label to a document by identifying the label to which it is most similar . Our evaluation of similarity is based on Latent Semantic Analysis ( to avoid the pitfalls of literal term matching ) and cosine similarity on the output LSA vectors . Before applying LSA , we start by stemming all the words using Porter stemmer . We feel that similarities between documents and labels can be more reliably estimated in the reduced latent space representation than in the original representation . The rationale is that documents which share frequently co - occurring terms will have a similar representation in the latent space , even if they have no terms in common . LSA thus performs some sort of noise reduction and has the potential benefit to detect synonyms as well as words that refer to the same topic .", "entities": []}
{"text": "As we described previously , the proposed method stemmed from a specific need in the banking industry where a large number of incidents had to be mapped to a newly defined taxonomy of operational risks . Specifically , it was designed to avoid the tedious and time consuming effort of asking experts to manually review thousands of incidents . An automated - or more precisely assisted - approach also presented the additional benefit of ensuring a higher degree of consistency in the mapping than would have been achieved a team of annotators . In this section , we provide some additional context into this specific task , report the observed performance of our method and discuss some of the specificities of the context .", "entities": []}
{"text": "For the purpose of experiment , operational teams ( not experts ) were asked to provide manual tags for a sample of 989 operational incidents . Table 4 provide the classification results of our approach when compared to those manual annotations , considering all three levels of the taxonomy . In a second step in the evaluation , an expert was given the difficult task to challenge each time they disagreed the computer and human annotation and determine which was ultimately correct . This exercise indicated that in 32 cases out of 989 operational incidents under consideration for the Level 1 classification , the machine generated category were more relevant ( hence correct ) than those identified by the operational team .", "entities": []}
{"text": "Given the number of categories , we were satisfied with the level of performance that we observed , especially for Level 1 and Level 2 of the taxonomy . More importantly , as we progress with the follow up exercise of mapping internal incident descriptions , we have evolved from a point where users always mistrust the outcome of the automated classification to a point where users see the suggested mapping from our algorithm as a rele - vant recommendation . Our perspective on the success of this method in this particular context is that operational risk is a textbook case where domain specific labels and vocabulary prevail . For instance , technical words such as forge , fictitious , bogus , ersatz , or counterfeit indicate almost surely that a Fraudulent Account Opening operation happened . Most of operational incidents must contain a combination of technical keywords due to their highly operational nature . What the method brings is the ability to combine human expertise through seed words with the strength of the machine which can process and memorize large corpus and derive distributional semantics from it . In this way , the cognitive burden of being exhaustive is lifted from the experts shoulders .", "entities": []}
{"text": "ICE : Idiom and Collocation Extractor for Research and Education", "entities": []}
{"text": "Collocation and idiom extraction are wellknown challenges with many potential applications in Natural Language Processing ( NLP ) . Our experimental , open - source software system , called ICE , is a python package for flexibly extracting collocations and idioms , currently in English . It also has a competitive POS tagger that can be used alone or as part of collocation / idiom extraction . ICE is available free of cost for research and educational uses in two user - friendly formats . This paper gives an overview of ICE and its performance , and briefly describes the research underlying the extraction algorithms .", "entities": []}
{"text": "ICE is a tool for extracting idioms and collocations , but it also has functions for part of speech", "entities": []}
{"text": "Lightweight Models for Multimodal Sequential Data", "entities": []}
{"text": "The Interaction Canonical Correlation Network ( ICCN ) [ Sun et al , 2020 ] implements Deep Canonical Correlation Analysis ( DCCA ) [ Andrew et al , 2013 ] to extract bimodal features from the outer product matrix of a pair of modalities . 3 Models", "entities": []}
{"text": "We use T , A , and V , to represent the three modalities : text , audio , and video , respectively . Following the notation in [ Tsai et al , 2019 ] and , we denote the input as X T , A , V = { x T , x A , x V } where x i = [ x t , i ] for i [ T , A , V ] and t [ 1 , \u03c4 ] and \u03c4 is the length of the input sentence . Each of the three modalities has its own lowlevel features , such as the Mel spectrogram for audio or facial landmarks for video . These features are extracted at different sampling rates - one set of features per word or character for text , per millisecond for audio , and per frame for video - and thus the input sequences for the three modalities are often different . A five - thousand - millisecond audio sequence , for example , may be only a three - word sequence from a textual perspective and a 50 - frame sequence from a video perspective . We align the audio and video to the text using the timestamps provided in the text transcripts . The set of audio or video samples that correspond to a word in the transcript are combined using a series of 1D convolutional layers : X { T , A , V } = conv1D X { T , A , V } R d where d is a common feature dimension size . This procedure ensures that the input sequence length is the same across modalities .", "entities": []}
{"text": "The effect of direction on our Round Robin model is shown in Table 9 ; this experiment shows the impact of the direction of information flow across modalities within the model . Comparing our results to those of MuLT and MuLT * , we see that capturing information flow in one direction , text to audio to video and back to text , is enough for a model to give good predictions , without requiring the additional overhead of handling both directions . We can also see that the direction does matter ; the performance of the Round Robin model with information flowing in the opposite direction , from video to audio to text and back to video , is relatively poor . These results suggest that the interactions between pairs of modalities are directed .", "entities": []}
{"text": "IIT DHANBAD CODECHAMPS at SemEval - 2022 Task 5 : MAMI - Multimedia Automatic Misogyny Identification", "entities": []}
{"text": "With the growth of the internet , the use of social media based on images has drastically increased like Twitter , Instagram , etc . In these social media , women have a very high contribution as of 75 % women use social media multiple times compared to men which is only 65 % of men uses social media multiple times a day . However , with this much contribution , it also increases systematic inequality and discrimination offline is replicated in online spaces in the form of MEMEs . A meme is essentially an image characterized by pictorial content with an overlaying text a posteriori introduced by humans , with the main goal of being funny and/or ironic . Although most of them are created with the intent of making funny jokes , in a short time people started to use them as a form of hate and prejudice against women , landing to sexist and aggressive messages in online environments that subsequently amplify the sexual stereotyping and gender inequality of the offline world . This leads to the need for automatic detection of Misogyny MEMEs . Specifically , I described the model submitted for the shared task on Multimedia Automatic Misogyny Identification ( MAMI ( Fersini et al , 2022 ) ) and my team name is IIT DHANBAD CODECHAMPS .", "entities": []}
{"text": "Here we have described the dataset and task provided by Multimedia Automatic Misogyny Identification ( MAMI ( Fersini et al , 2022 ) ) challenge . Multimedia Automatic Misogyny Identification ( MAMI ) task is divided into two sub task . Sub - task A : a basic task about misogynous meme identification , where a meme should be categorized either as misogynous or not misogynous ( shown in Table 1 ) . Sub - task B : an advanced task , where the type of misogyny should be recognized among potential overlapping categories such as stereotype , shaming , objectification and violence . e.g. umn represent Text Transcription of the meme .", "entities": []}
{"text": "We pose a new , multivalent interpretation of the DIH ( the MDIH ) which models the entailment of predicates across valencies . The intuition comes from observing eventualities ( Vendler , 1967 ) which occur in the world . Neo - Davidsonian semantics ( Davidson , 1967 ; Maienborn , 2011 ) explains that a textual predicate , its arguments , and adjuncts , are all properties of an underlying event variable . En - tailments about one or more of the arguments arise from their roles in this eventuality . We may infer that \" Mr. Boddy died \" due to his role as a direct object in the killing / murdering event . No other information is needed , including who murdered Mr. Boddy , where , or with what instrument . Boddy is dead simply because he was murdered . We build on this insight to develop the MDIH . Here , a predicate is represented ( as in 2 ) by features which are the argument tuples it appears with . We recognize a tuple as a proxy for a world event , e.g. VISIT ( Obama , Hawaii ) identifies one instance of a real VISIT event . Our method learns by tracking entity tuples across events in the world . The MDIH signals an entailment from a premise p to hypothesis h if , distributionally , subtuples of p are always found amongst tuples of h. Crucially , we allow h to drop in valency so that we learn entailments about subsets of p 's arguments . We now formalize the MDIH and then illustrate with an example . We define the argument tuple structures for a premise and hypothesis predicate : P = { ( a k , 1 , . . . , a k , I ) | k { 1 , . . . , M } } H = { ( b k , 1 , . . . , b k , J ) | k { 1 , . . . , N } } P is a set of M argument tuples ( each of size I ) which correspond to instances of a premise predicate p. H is a set of N argument tuples ( each of size J ) representing the same for hypothesis h. We limit J \u2264 I , e.g. we learn about relations on realized entity subsets . We do not learn entailments to higher valencies ( such as a unary entailing a binary ) because additional arguments must be existential , not real . We leave this to future work . To select argument subtuples from tuples in P , we define a vector of indices j with length J , which selects arguments by position . For example , with j = [ 2 , 3 ] , perform P [ : , j ] . For each argument tuple in P , select just the 2nd and 3rd arguments , forming a new set of 2 - tuples . We define the Multivalent Distributional Inclusion Hypothesis : If P [ : , j ] \u2286 H [ : , m ( j ) ] , then p h Here m : N J N J is a simple bijective mapping from argument indices of p to h. An example where m is needed for argument swapping is \" x bought y \" entails \" y sold to x. \" We illustrate by working the kill / die example on a hypothetical corpus . We might find that KILL ( x , y ) DIE ( y ) by trying j = [ 2 ] and m ( [ 2 ] ) = [ 1 ] . We start with P , all 2 - tuples of killings , and H , all 1 - tuples of dyings and apply j and m. We may find that selecting arg 2 from all tuples in P forms a subset of the selection of arg 1 from tuples in H. Though dyings may happen in many ways , we observe that arg 2 of a killing often occurs elsewhere in the corpus with a dying , and thus we infer the entailment between predicates . Intuitively this is true for arbitrarily large valencies : MURDER ( Mustard , Boddy , kitchen , candlestick ) entails KILL ( Mustard , Boddy ) and both entail DIE ( Boddy ) . Though arguments may be dropped from the premise , they still influence entailments . This is because the MDIH tracks eventualities . \" Person writing a book \" is a different kind of event than \" person writing software \" with a different distribution of argument tuples , so we learn that the former entails being an author , while the latter entails being a programmer .", "entities": []}
{"text": "Local learning of entailments suffers from sparsity issues which can be improved by further learning of \" global \" graphs . We use the soft constraint method of Hosseini et al ( 2018 ) which has two optimizations . The paraphrase resolution constraint encourages predicates within the same typed graphs that entail each other to have similar entailment patterns . The cross - graph constraint additionally encourages compatible predicates across different typed graphs to share entailment patterns . We apply global learning to bivalent graphs and separately to univalent graphs . Globalization is valency - agnostic , using just the common structures between predicates , so bivalent graphs can use BB and BU edges to optimize binary predicate entailments . Final graph size statistics are in Table 1 .", "entities": []}
{"text": "Vertices Edges Bivalent 938 K Binary 94 M BB / 30 M BU Univalent 36 K Unary 3.6 M UU", "entities": []}
{"text": "Since the annual examination papers are designed by a team of healthcare experts who try to follow the similar reasoning types distribution . To better understand our dataset , we manually inspected 10 sets of examination papers ( 2 sets for each subfield ) , and summarize the most frequent reasoning types of the questions from MLEC - QA and previous works ( Lai et al , 2017 ; Zhong et al , 2020 ) . The examples are shown in Table 6 . Notably , the \" Evidence \" is well - organized by us to show how models need to handle these reasoning issues to achieve promising performance in MLEC - QA . The definition of reasoning types of the questions are as follows : Lexical Matching This type of question is common and the simplest . The retrieved documents are highly matched with the question , the correct answer exactly matches a span in the document . As shown in the example , the model only needs to check which option is matched with . Multi - Sentence Reading Unlike lexical matching , where questions and correct answers can be found within a single sentence , multi - sentence reading requires models reading multiple sentences to gather enough information to generate answers .", "entities": []}
{"text": "The correct options for this type of question do not appear directly in the documents . It requires the model to understand and summarize the question relevant concepts after reading the documents . As shown in the example , the model needs to understand and summarize the relevant mechanism of \" Thermoregulation \" , and infer that when an obstacle arises in thermoregulation , the body temperature will not be able to maintain a relatively constant level , that is , it will rise with the increase of ambient temperature . Numerical Calculation This type of question involves logical reasoning and arithmetic operations related to mathematics . As shown in the example , the model first needs to judge the approximate age of month according to the height of the infant , and then reverse calculate the age of months according to the height formula of infants 7~12 months old to obtain the age in months : ( 68 - 65 ) / 1.5 + 6 = 8 . Multi - Hop Reasoning This type of question requires several steps of logical reasoning over mul - tiple documents to answer . As shown in the example , the patient 's hemoglobin ( HB ) value is low , indicating that the patient has anemia , and the supply of iron should be increased in their diet . The model needs to compare the iron content of each option : the iron content of C , D and E is low and that of A , B is high , but B is not easily absorbed , so the best answer is A. Reasoning Type Example ( * represents the correct answer )", "entities": []}
{"text": "The main hallmark of peritonitis is : A. Significant abdominal distension B. Abdominal mobility dullness C. Bowel sounds were reduced or absent D. Severe abdominal cramping * E. Peritoneal irritation signs Evidence : The hallmark signs of peritonitis are peritoneal irritation signs , i.e. , tenderness , muscle tension , and rebound tenderness .", "entities": []}
{"text": "Which is wrong in the following narrative relating to the appendix : The purpose of thermoregulation is to maintain body temperature in the normal range . In hyperthermic environments , the thermoregulatory center is dysfunctional and can not maintain the body 's balance of heat production and heat dissipation , so the body temperature is increased by the influence of ambient temperature . A.", "entities": []}
{"text": "A normal infant , weighing 7.5 kg and measuring 68 cm in length . Bregma 1.0 cm , head circumference 44 cm . Teething 4 . Can sit alone and can pick up pellets with a hallux and forefinger . The most likely age of the infant is : * A. 8 months B. 24 months C. 18 months D. 12 months E. 5 months Evidence : A normal infant measured 65 cm at 6 months and 75 cm at 1 year of age . The infant 's 7 to 12 month length is calculated as : length = 65 + ( months of age - 6 ) x 1.5 .", "entities": []}
{"text": "6 - month - old female infant , artificial feeding mainly , physical examination revealed a low hemoglobin ( HB ) value , the dietary supplement that should be mainly added is : * A. Liver paste B. Egg yolk paste C. Tomato paste D. Rice paste E. Apple puree Evidence : ( 1 ) Low HB value indicates anemia tendency . Iron deficiency anemia is the most important and common type of anemia in China . ( 2 ) Iron supply should be increased in diet . ( 3 ) Liver paste is rich in iron . ( 4 ) The iron content of egg yolk paste is lower than that of liver paste , and it is not easy to be absorbed . ( 5 ) The iron content of tomato paste , rice paste and apple puree is lower than that of liver paste .", "entities": []}
{"text": "Given that we use the Chinese Wikipedia database as our information sources and apply a two - stage retriever - reader framework , the reason for such poor baseline performance could come from both our information sources and the retriever - reader framework .", "entities": []}
{"text": "The data is represented in simplified Chinese ( zh - Hans - CN ) , and collect from the 2006 to 2020 NM - LEC , as well as practice exercises from the Internet .", "entities": []}
{"text": "The experiments involve annotations from 5 medical experts with at least have a master 's degree and have passed the NMLEC . They ranged in age from 28 45 years , included 3 men and 2 women , all come from China and speak Chinese as a native language .", "entities": []}
{"text": "All questions in MLEC - QA are collected from the National Medical Licensing Examination in China ( NMLEC ) , which are carefully designed by human experts to evaluate professional knowledge and skills for those who want to be medical practitioners in China .", "entities": []}
{"text": "We thank the anonymous reviewers for their insightful comments and suggestions . This work is supported by the National Natural Science Foundation of China ( NSFC No . 61972187 ) .", "entities": []}
{"text": "The data structure below describe the JSON file representation in MLEC - QA .", "entities": []}
{"text": "\" qid\":The question ID , \" qtype \" : [ \" A1 \" , \" B1 \" , \" A2 \" , \" A3 / A4 \" ] , \" qtext\":Description of the question , \" qimage\":Image or table path ( if any ) , \" options \" : { \" A\":Description of the option A , \" B\":Description of the option B , \" C\":Description of the option C , \" D\":Description of the option D , \" E\":Description of the option E } , \" answer \" : [ \" A \" , \" B \" , \" C \" , \" D \" , \" E \" ] }", "entities": []}
{"text": "The BM25 algorithm is defined as : where q i is the i th query term of a query Q , f ( q i , D ) is q i 's term frequency in the document D , | D | is the length of the document D in words , and avgdl is the average document length in the text collection from which documents are drawn . b determines the effects of the length of the document on the average length . k1 is a variable which helps determine term frequency saturation characteristics . By default , b , k1 has a value of 0.75 , 1.2 in Elasticsearch , respectively . IDF ( q i ) is the Inverse Document Frequency ( IDF ) weight of the query term q i . It is usually computed as : where N is the total number of documents in the collection , and n ( q i ) is the number of documents containing q i .", "entities": []}
{"text": "Parsing Graphs with Regular Graph Grammars", "entities": []}
{"text": "Recently , several datasets have become available which represent natural language phenomena as graphs . Hyperedge Replacement Languages ( HRL ) have been the focus of much attention as a formalism to represent the graphs in these datasets . Chiang et al ( 2013 ) prove that HRL graphs can be parsed in polynomial time with respect to the size of the input graph . We believe that HRL are more expressive than is necessary to represent semantic graphs and we propose the use of Regular Graph Languages ( RGL ; Courcelle 1991 ) , which is a subfamily of HRL , as a possible alternative . We provide a topdown parsing algorithm for RGL that runs in time linear in the size of the input graph .", "entities": []}
{"text": "Definition 3 . A hyperedge replacement grammar G = ( N G , T G , P G , S G ) consists of ranked ( disjoint ) alphabets N G and T G of nonterminal and terminal symbols , respectively , a finite set P G of productions , and a start symbol S G N G . Every production in P G is of the form X G where G is a hypergraph over N G \u222a T G and rank ( G ) = rank ( X ) . For each production p : X G , we use L ( p ) to refer to X ( the left - hand side of p ) and R ( p ) to refer to G ( the right - hand side of p ) . An edge is a terminal edge if its label is terminal and a nonterminal edge if its label is nonterminal . A graph is a terminal graph if all of its edges are terminal . The terminal subgraph of a graph is the subgraph consisting of all terminal edges and their incident nodes . Given a HRG G , we say that graph G immediately derives graph G , denoted G G , iff there is an edge e E G and a nonterminal X N G such that lab G ( e ) = X and G = G [ e / H ] , where X H is in P G . We extend the idea of immediate derivation to its transitive closure G * G , and say here that G derives G . For every X N G we also use X to de - X a b ( 1 ) ( 2 ) G ( 2 ) c ( 1 ) a ( 3 ) d H ( 1 ) c d a ( 2 ) b a G [ e / H ]", "entities": []}
{"text": "( 1 ) 1 go 1 2 I arg0 Y Z s : ( 1 ) ( 2 ) 1 2 1 arg0 arg1 X q : W Y ( 2 ) ( 1 ) 1 2 1 1 2 arg1 arg0 W t : ( 1 ) 1 want Y r : Z X ( 2 ) ( 1 ) 1 2 1 1 2 arg1 arg0 Z u : ( 1 ) 1 need Table 1 : Productions of a HRG . The labels p , q , r , s , t , and u label the productions so that we can refer to them in the text . Note that Y can rewrite in two ways , either via production r or s. note the graph consisting of a single edge e with lab ( e ) = X and nodes ( v 1 , . . . , v rank ( X ) ) such that att G ( e ) = ( v 1 , . . . , v rank ( X ) ) , and we define the language L X ( G ) as { G | X * G G is terminal } . The language of G is L ( G ) = L S G ( G ) . We call the family of languages that can be produced by any HRG the hyperedge replacement languages ( HRL ) . We assume that terminal edges are always of rank 2 , and depict them as directed edges where the direction is determined by the tentacle labels : the tentacle labeled 1 attaches to the source of the edge and the tentacle labeled 2 attaches to the target of the edge . Example 3 . Table 1 shows a HRG deriving AMR graphs for sentences of the form ' I need to want to need to want to ... to want to go ' . Figure 3 is a graph derived by the grammar . The grammar is somewhat unnatural , a point we will return to ( 4 ) . We can use HRGs to generate chain graphs ( strings ) by restricting the form of the productions in the grammars . Figure 4 shows a HRG that produces the context - free string language a n b n . HRGs can simulate the class of mildly context - sensitive languages that is characterized , e.g. , by linear context - free rewriting systems ( LCFRS ; Vijay - Shanker et al 1987 ) , where the fan - out of the LCFRS will influence the maximum rank of nonterminal required in the HRG , see ( Engelfriet and Heyker , 1991 ) .", "entities": []}
{"text": "( 1 ) Y Z We call the family of languages generated by RGGs the regular graph languages ( RGLs ) .", "entities": []}
{"text": "In this section we report details on the database , training parameters and results .", "entities": []}
{"text": "The system was implemented using OpenNMT in PyTorch ( Klein et al , 2017 )", "entities": []}
{"text": "Utilizando la base de datos Epistemonikos , la cual es mantenida mediante bsquedas realizadas en 30 bases de datos , identificamos seis revisiones sistemticas que en conjunto incluyen 36 estudios aleatorizados pertinentes a la pregunta .", "entities": []}
{"text": "Using the Epistemonikos database , which is maintained through searches in 30 databases , we identified six systematic reviews including 36 randomized studies relevant to the question .", "entities": []}
{"text": "Using the Epistemonikos database , which is maintained through searches in 30 databases , we identified six systematic reviews that altogether include 36 randomized studies relevant to the question .", "entities": []}
{"text": "Os resultados dos modelos de regresso mostraram associao entre os fatores de correo estimados e os indicadores de adequao propostos", "entities": []}
{"text": "Regression models showed an association between estimated correction factors and the proposed adequacy indicators .", "entities": []}
{"text": "Authors would like to thank Noe Casas for his valuable comments . This work is supported in", "entities": []}
{"text": "We start with the problem definition . Let G = ( E , R ) be an existing KG where E and R are the sets of entities and relationships ( predicates ) in G , respectively . We consider a sentence S = w 1 , w 2 , ... , w i as the input , where w i is a token at position i in the sentence . We aim to extract a set of triples O = { o 1 , o 2 , ... , o j } from the sentence , where o j = h j , r j , t j , h j , t j E , and r j R. Table 1 illustrates the input and target output of our problem .", "entities": []}
{"text": "In the training phase , in addition to the sentencetriple pairs collected using distant supervision ( see Section 3.2 ) , we also add pairs of Entity - name , Entity - ID of all entities in the KB to the training data , e.g. , New York University , Q49210 . This allows the model to learn the mapping between entity names and entity IDs , especially for the unseen entities .", "entities": []}
{"text": "We further perform manual error analysis . We found that the incorrect output of our model is caused by the same entity name of two different entities ( e.g. , the name of Michael Jordan that refers to the American basketball player or the English footballer ) . The modified beam search can not disambiguate those entities as it only considers the lexical similarity . We consider using context - based similarity as future work .", "entities": []}
{"text": "We begin by surveying existing KGC benchmarks . Table 8 in Appendix A provides an overview of evaluation datasets and tasks on a per - paper basis across the artificial intelligence , machine learning , and natural language processing communities . Note that we focus on data rather than models , so we only overview relevant evaluation benchmarks here . For more on existing KGC models , both neural and symbolic , we refer the reader to ( Meilicke et al , 2018 ) and ( Ji et al , 2020 ) .", "entities": []}
{"text": "These datasets , extracted from the Freebase knowledge graph ( Bollacker et al , 2008 ) , are the most popular for KGC ( see Table 8 in Appendix A ) . Bordes et al ( 2013 ) . It contains 14 , 951 entities , 1 , 345 relations , and 592 , 213 triples covering several domains , with a strong focus on awards , entertainment , and sports .", "entities": []}
{"text": "FB15 K - 237 was introduced by to remedy data leakage in FB15 K , which contains many test triples that invert triples in the training set . FB15 K - 237 contains 14 , 541 entities , 237 relations , and 310 , 116 triples . We compare FB15 K - 237 to CODEX in 6 to assess each dataset 's content and relative difficulty .", "entities": []}
{"text": "In this section we describe the pipeline used to construct CODEX . For reference , we define a knowledge graph G as a multi - relational graph consisting of a set of entities E , relations R , and factual statements in the form of ( head , relation , tail ) triples ( h , r , t ) E \u00d7 R \u00d7 E.", "entities": []}
{"text": "An advantage of Wikidata is that it links entities and relations to various sources of rich auxiliary information . To enable tasks that involve joint learning over knowledge graph structure and such additional information , we collected : Entity types for each entity as given by Wikidata 's instance of and subclass of relations ; Wikidata labels and descriptions for entities , relations , and entity types ; and Wikipedia page extracts ( introduction sections ) for entities and entity types . For the latter two , we collected text where available in Arabic , German , English , Spanish , Russian , and Chinese . We chose these languages because they are all relatively well - represented on Wikidata ( Kaffee et al , 2017 ) . Table 2 provides the coverage by language for each CODEX dataset .", "entities": []}
{"text": "We manually labeled all candidate negative triples generated for CODEX - S and CODEX - M as true or false using the guidelines provided in Appendix C. 3 We randomly selected among the triples labeled as false to create validation and test negatives for CODEX - S and CODEX - M , examples of which are given in Ta - 3 We are currently investigating methods for obtaining highquality crowdsourced annotations of negatives for CODEX - L. ble 3 . To assess the quality of our annotations , we gathered judgments from two independent native English speakers on a random selection of 100 candidate negatives . The annotators were provided the instructions from Appendix C. On average , our labels agreed with those of the annotators 89.5 % of the time . Among the disagreements , 81 % of the time we assigned the label true whereas the annotator assigned the label false , meaning that we were comparatively conservative in labeling negatives .", "entities": []}
{"text": "Symmetric relations are relations r for which ( h , r , t ) G implies ( t , r , h ) G. For each relation , we compute the number of its ( head , tail ) pairs that overlap with its ( tail , head ) pairs , divided by the total number of pairs , and take those with 50 % overlap or higher as symmetric . CODEX datasets have five such relations : diplomatic relation , shares border with , sibling , spouse , and unmarried partner . Table 4 gives the proportion of triples containing symmetric relations per dataset . Symmetric patterns are more prevalent in CODEX - S , whereas the larger datasets are mostly antisymmetric , i.e. , ( h , r , t ) G implies ( t , r , h ) G.", "entities": []}
{"text": "Finally , we conduct a comparative analysis between CODEX - M and FB15 K - 237 ( 2.1 ) to demonstrate the unique value of CODEX . We choose FB15 K - 237 because it is the most popular encyclopedic KGC benchmark after FB15 K , which was already shown to be an easy dataset by . We choose CODEX - M because it is the closest in size to FB15 K - 237 .", "entities": []}
{"text": "Table 9 provides all seed entity and relation types used to collect CODEX . Each type is given first by its natural language label and then by its Wikidata unique ID : Entity IDs begin with Q , whereas relation ( property ) IDs begin with P. For the entity types that apply to people ( e.g. , actor , musician , journalist ) , we retrieved seed entities by querying Wikidata using the occupation relation . For the entity types that apply to things ( e.g. , airline , disease , tourist attraction ) , we retrieved seed entities by querying Wikidata using the instance of and subclass of relations .", "entities": []}
{"text": "We provide additional comparison of the contents in CODEX - M and FB15 K - 237 . Figure 5 , which plots the top - 30 entities by frequency in the two benchmarks , demonstrates that both dataset are biased toward developed Western countries and cultures . However , CODEX - M is more diverse in domain . It covers academia , entertainment , journalism , politics , science , and writing , whereas FB15 K - 237 covers mostly entertaiment and sports . FB15 K - 237 is also much more biased toward the United States in particular , as five of its top - 30 entities are specific to the US : United States of America , United States dollar , New York City , Figure 6 compares the top - 15 entity types in CODEX - M and FB15 K - 237 . Again , CODEX - M is diverse , covering people , places , organizations , movies , and abstract concepts , whereas FB15 K - 237 has many overlapping entity types mostly about entertainment .", "entities": []}
{"text": "The authors thank Micha\u0142 Rybak and Xinyi ( Carol ) Zheng for their contributions . This material is supported by the National Science Foundation under Grant No . IIS 1845491 , Army Young Investigator Award No . W911NF1810397 , and an NSF Graduate Research Fellowship .", "entities": []}
{"text": "Table 10 : Our hyperparameter search space . We follow the naming conventions and ranges given by Ruffinelli et al ( 2020 ) , and explain the meanings of selected hyperparameter settings in Appendix F. As most KGC embedding models have a wide range of configuration options , we encourage future work to follow this tabular scheme for transparent reporting of implementation details . 1.0 - See Tab . 12 - - See Tab . 12 Interval ( Unif ) - See Tab . 12 \u22120.8133 - See Tab . 12 Gain ( XvNorm ) 1.0 See Tab . 12 - - See Tab . 12 Gain ( XvUnif ) - See Tab . 12 - 1.0 See Tab . 12", "entities": []}
{"text": "Scalable Construction and Reasoning of Massive Knowledge Bases - Proposal for a Tutorial at NAACL 2018", "entities": []}
{"text": "In today 's information - based society , there is abundant knowledge out there carried in the form of natural language texts ( e.g. , news articles , social media posts , scientific publications ) , which spans across various domains ( e.g. , corporate documents , advertisements , legal acts , medical reports ) , and grows at an astonishing rate . How to turn such massive and unstructured text data into structured , actionable knowledge for computational machines , and furthermore , how to teach machines learn to reason and complete the extracted knowledge is a grand challenge to the research community .", "entities": []}
{"text": "This tutorial presents a comprehensive overview of techniques for automatic knowledge base construction from text data ( especially from a large , domain - specific text corpora ) , and techniques for reasoning over large - scale knowledge bases . We will discuss the following key issues :", "entities": []}
