{"text": "And finally , the team ranks 5th in this task with a weighted average F1 score of 0.93 on the private leader board .", "entities": [[13, 15, "MetricName", "average F1"]]}
{"text": "We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF .", "entities": [[24, 25, "MetricName", "accuracy"], [30, 31, "MethodName", "LSTM"], [32, 33, "MethodName", "CRF"]]}
{"text": "5.0 English NER , we demonstrate significant speed gains of our ID - CNNs over various recurrent models , while maintaining similar F1 performance .", "entities": [[2, 3, "TaskName", "NER"], [22, 23, "MetricName", "F1"]]}
{"text": "Using the same number of parameters as a simple convolution with the same radius ( i.e. W c has the same dimensionality ) , the \u03b4 > 1 dilated convolution incorporates broader context into the representation of a token than a simple convolution .", "entities": [[3, 6, "HyperparameterName", "number of parameters"], [9, 10, "MethodName", "convolution"], [25, 26, "HyperparameterName", "\u03b4"], [28, 30, "MethodName", "dilated convolution"], [42, 43, "MethodName", "convolution"]]}
{"text": "Let r ( ) denote the ReLU activation function ( Glorot et al , 2011 ) .", "entities": [[6, 7, "MethodName", "ReLU"], [7, 9, "HyperparameterName", "activation function"]]}
{"text": "Sec . 2.1 identifies that the CRF has preferable sample complexity and accuracy since prediction directly reasons in the space of structured outputs .", "entities": [[6, 7, "MethodName", "CRF"], [12, 13, "MetricName", "accuracy"]]}
{"text": "The loss also helps reduce the vanishing gradient problem ( Hochreiter , 1998 ) for deep architectures .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "Huang et al ( 2015 ) achieved state - of - the - art accuracy on partof - speech , chunking and NER using a Bi - LSTM - CRF .", "entities": [[14, 15, "MetricName", "accuracy"], [20, 21, "TaskName", "chunking"], [22, 23, "TaskName", "NER"], [27, 28, "MethodName", "LSTM"], [29, 30, "MethodName", "CRF"]]}
{"text": "We also observe a performance boost in almost all models when broadening the context to incorporate entire documents , achieving an average F1 of 90.65 on CoNLL - 2003 , out - performing the sentence - level model while still decoding at nearly 8 times the speed of the Bi - LSTM - CRF .", "entities": [[21, 23, "MetricName", "average F1"], [51, 52, "MethodName", "LSTM"], [53, 54, "MethodName", "CRF"]]}
{"text": "We also compare our document - level ID - CNNs to a baseline which does not share parameters between blocks ( noshare ) and one that computes loss only at the last block , rather than after every iterated block of dilated convolutions ( 1 - loss ) .", "entities": [[27, 28, "MetricName", "loss"], [46, 47, "MetricName", "loss"]]}
{"text": "Table 1 lists F1 scores of models predicting with sentence - level context on CoNLL - 2003 .", "entities": [[3, 4, "MetricName", "F1"]]}
{"text": "For models that we trained , we report F1 and standard deviation obtained by averaging over 10 random restarts .", "entities": [[8, 9, "MetricName", "F1"]]}
{"text": "The Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN obtain the highest average scores , with the ID - CNN - CRF outperforming the Bi - LSTM - CRF by 0.11 points of F1 on average , and the Bi - LSTM - CRF out - performing the greedy ID - CNN by 0.11 as well .", "entities": [[6, 7, "MethodName", "LSTM"], [8, 9, "MethodName", "CRF"], [14, 15, "MethodName", "CRF"], [32, 33, "MethodName", "CRF"], [37, 38, "MethodName", "LSTM"], [39, 40, "MethodName", "CRF"], [44, 45, "MetricName", "F1"], [52, 53, "MethodName", "LSTM"], [54, 55, "MethodName", "CRF"]]}
{"text": "Our greedy ID - CNN outperforms the Bi - LSTM and the 4 - layer CNN , which uses the same number of parameters as the ID - CNN , and performs similarly to the 5 - layer CNN which uses more parameters but covers the same effective input width .", "entities": [[9, 10, "MethodName", "LSTM"], [21, 24, "HyperparameterName", "number of parameters"]]}
{"text": "All CNN models out - perform the Bi - Model F1 Ratinov and Roth ( 2009 ) 86.82", "entities": [[10, 11, "MetricName", "F1"]]}
{"text": "The most vast speed improvements come when comparing the greedy ID - CNN to the Bi - LSTM - CRF - our ID - CNN is more than 14 times faster than the Bi - LSTM - CRF at test time , with comparable accuracy .", "entities": [[17, 18, "MethodName", "LSTM"], [19, 20, "MethodName", "CRF"], [35, 36, "MethodName", "LSTM"], [37, 38, "MethodName", "CRF"], [44, 45, "MetricName", "accuracy"]]}
{"text": "We emphasize the importance of the dropout regularizer of Ma et al ( 2017 ) in Table 3 , where we observe increased F1 for every model trained with expectation - linear dropout regularization .", "entities": [[23, 24, "MetricName", "F1"]]}
{"text": "Incorporating document - level context further improves our greedy ID - CNN model , attaining 90.65 average F1 .", "entities": [[16, 18, "MetricName", "average F1"]]}
{"text": "In Table 7 , we also list the F1 of our ID - CNN model and the Bi - LSTM - CRF model trained on entire document context .", "entities": [[8, 9, "MetricName", "F1"], [19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"]]}
{"text": "Note that the stateof - the - art results on CoNLL03 have achieved an F1 score of \u223c .93 .", "entities": [[10, 11, "DatasetName", "CoNLL03"], [14, 16, "MetricName", "F1 score"]]}
{"text": "We transform the gold ranking of V ( | V | = n ) into n 2 pairwise comparisons for every candidate pair , and learn to minimize the pairwise ranking violations using hinge loss : L M R = 1 m m k=1 1 n 2 k n k i=1 n k j=1 , i = j max ( 0 ,", "entities": [[34, 35, "MetricName", "loss"], [60, 61, "DatasetName", "0"]]}
{"text": "Following \u0160tajner et al ( 2015 ) , we removed the sentence pairs with high ( > 0.9 ) and low ( < 0.1 ) BLEU ( Papineni et al , 2002 ) scores , which mostly correspond to the near identical and semantically divergent sentence pairs respectively .", "entities": [[25, 26, "MetricName", "BLEU"]]}
{"text": "We report SARI ( Xu et al , 2016 ) , which averages the F1 / precision of n - grams ( n { 1 , 2 , 3 , 4 } ) inserted , deleted and kept when compared to human references .", "entities": [[14, 15, "MetricName", "F1"]]}
{"text": "More specifically , it computes the F1 score for the n - grams that are added ( add ) , 8 which is an important indicator if a model is good at paraphrasing .", "entities": [[6, 8, "MetricName", "F1 score"]]}
{"text": "9 To evaluate a model 's para - 8 We slightly improved the SARI implementation by Xu et al ( 2016 ) to exclude the spurious ngrams while calculating the F1 score for add .", "entities": [[30, 32, "MetricName", "F1 score"]]}
{"text": "deletion as they show high self - BLEU ( > 66.5 ) and FK ( > 8.8 ) scores despite having compression ratios similar to other systems .", "entities": [[7, 8, "MetricName", "BLEU"]]}
{"text": "Our model achieves the lowest self - BLEU ( 48.7 ) , FK ( 7.9 ) , and percentage of sentences identical to the input ( 0.4 ) , and the highest add ( 3.3 ) score and percentage of new words ( 16.2 % ) .", "entities": [[7, 8, "MetricName", "BLEU"]]}
{"text": "Nishihara et al ( 2019 ) proposed a loss which controls word complexity , while Mallinson and Lapata ( 2019 ) concatenated constraints to each word embedding .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "We used Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 0.0001 , linear learning rate warmup of 40k steps , and 100k training steps .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "optimizer"], [13, 15, "HyperparameterName", "learning rate"], [19, 21, "HyperparameterName", "learning rate"]]}
{"text": "We used a batch size of 64 .", "entities": [[3, 5, "HyperparameterName", "batch size"]]}
{"text": "We used Adam optimizer with a learning rate of 0.01 and 10 epochs .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "optimizer"], [6, 8, "HyperparameterName", "learning rate"]]}
{"text": "We just examined few values for learning rate ( 0.001 , 0.01 and 0.1 ) and chose the best based on the SARI score on the dev set .", "entities": [[6, 8, "HyperparameterName", "learning rate"]]}
{"text": "Similar to Newsela , we remove the sentence pairs with high ( > 0.9 ) and low ( < 0.1 ) BLEU ( Papineni et al , 2002 ) scores .", "entities": [[2, 3, "DatasetName", "Newsela"], [21, 22, "MetricName", "BLEU"]]}
{"text": "We use contrastive loss described in ( Chopra et al , 2005 ) as the loss function to the Siamese network .", "entities": [[3, 4, "MetricName", "loss"], [15, 16, "MetricName", "loss"], [19, 21, "MethodName", "Siamese network"]]}
{"text": "The contrastive loss can be given by following equation : L = Y | | G W ( X 1 ) , G W ( X 2 ) | | 2 + ( 1 \u2212 Y ) max ( 0 , m \u2212 | | G W ( X 1 ) , G W ( X 2 ) | | 2 ) where Y is annotated tag , 1 if X 1 and X 2 are similar , 0 otherwise .", "entities": [[2, 3, "MetricName", "loss"], [39, 40, "DatasetName", "0"], [78, 79, "DatasetName", "0"]]}
{"text": "m is margin parameter for hinge loss , which is kept 1 for all our networks .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "We use a bidirectional LSTM to generate a 256 dimensional vector for pair of text and train the model by back propagation using contrastive loss .", "entities": [[3, 5, "MethodName", "bidirectional LSTM"], [24, 25, "MetricName", "loss"]]}
{"text": "The batch size is set to 64 and dropout rate is 0.25 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "We use the same hyper - parameters as original paper that give the best results i.e. \u03b1 = 0.2 , \u03b2 = 0.45 , \u03b7 = 0.4 , \u03c6 = 0.2 , \u03b4 = 0.85 .", "entities": [[16, 17, "HyperparameterName", "\u03b1"], [20, 21, "HyperparameterName", "\u03b2"], [32, 33, "HyperparameterName", "\u03b4"]]}
{"text": "We also compute ROUGE - L ( Lin , 2004 ) , which is recall oriented similarity measure based on longest common subsequence ( LCS ) , as a feature in our system .", "entities": [[3, 6, "MetricName", "ROUGE - L"]]}
{"text": "Table 1 shows the dev and test set accuracy for our system with each feature applied incrementally .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "Low levels of pruning ( 30 - 40 % ) do not affect pre - training loss or transfer to downstream tasks at all .", "entities": [[16, 17, "MetricName", "loss"]]}
{"text": "Model compression ( Bucila et al , 2006 ) , which attempts to shrink a model without losing accuracy , is a viable approach to decreasing GPU usage .", "entities": [[0, 2, "TaskName", "Model compression"], [18, 19, "MetricName", "accuracy"]]}
{"text": "Our findings are as follows : Low levels of pruning ( 30 - 40 % ) do not increase pre - training loss or affect transfer to downstream tasks at all .", "entities": [[22, 23, "MetricName", "loss"]]}
{"text": "4 . Continue training the network to recover any lost accuracy .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "We repeat this for learning rates in [ 2 , 3 , 4 , 5 ] \u00d710 \u22125 and show the results with the best development accuracy in Figure 1 / Table 1 .", "entities": [[26, 27, "MetricName", "accuracy"]]}
{"text": "Figure 1 shows that the first 30 - 40 % of weights pruned by magnitude weight pruning do not impact pre - training loss or inference on any downstream task .", "entities": [[23, 24, "MetricName", "loss"]]}
{"text": "Pre - training loss increases as we prune weights necessary for fitting the pre - training data ( Table 1 ) .", "entities": [[3, 4, "MetricName", "loss"]]}
{"text": "13 Downstream accuracy also begins to degrade at this point .", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "30 - 40 % of weights can be pruned using magnitude weight pruning without decreasing dowsntream accuracy .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "While the results for individual tasks are in Table 1 , each task does not vary much from the average trend , with an exception discussed in Section 4.3 . Figure 2 : ( Left ) Pre - training loss predicts information deletion GLUE accuracy linearly as sparsity increases .", "entities": [[39, 40, "MetricName", "loss"], [43, 44, "DatasetName", "GLUE"], [44, 45, "MetricName", "accuracy"]]}
{"text": "At 70 % sparsity and above , models with information deletion recover some accuracy w.r.t . pruned models , so complexity restriction is a secondary cause of performance degradation .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "Figure 2 shows that the pre - training loss linearly predicts the effects of information deletion on downstream accuracy .", "entities": [[8, 9, "MetricName", "loss"], [18, 19, "MetricName", "accuracy"]]}
{"text": "The average GLUE development accuracy and pruning mask difference for models trained on downstream datasets before pruning 60 % at learning rate 5e - 5 .", "entities": [[2, 3, "DatasetName", "GLUE"], [4, 5, "MetricName", "accuracy"], [20, 22, "HyperparameterName", "learning rate"]]}
{"text": "After pruning , models are trained for an additional 2 epochs to regain accuracy .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "is worth only one accuracy point on QQP but 10 points on CoLA .", "entities": [[4, 5, "MetricName", "accuracy"], [7, 8, "DatasetName", "QQP"], [12, 13, "DatasetName", "CoLA"]]}
{"text": "Indeed , Figure 3 shows that the weights selected for 60 % pruning quickly stabilize and evaluation accuracy does not improve with more training before pruning .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "Michel et al ( 2019 ) showed that after fine - tuning on MNLI , up to 40 % of attention heads can be pruned from BERT without affecting test accuracy .", "entities": [[13, 14, "DatasetName", "MNLI"], [26, 27, "MethodName", "BERT"], [30, 31, "MetricName", "accuracy"]]}
{"text": "We also report standard MT metric BLEU ( 1 - 4 ngrams ) , and perform an additional manual evaluation .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "We fine - tune a T5 model ( t5 - base from Hugging - Face library ) using the Adam optimizer with a batch size of 8 and a learning rate of 1e\u22124 .", "entities": [[5, 6, "MethodName", "T5"], [8, 9, "MethodName", "t5"], [19, 20, "MethodName", "Adam"], [20, 21, "HyperparameterName", "optimizer"], [23, 25, "HyperparameterName", "batch size"], [29, 31, "HyperparameterName", "learning rate"]]}
{"text": "We train all models for a maximum of ten epochs with an early stopping value of 1 ( patience ) based on the validation loss .", "entities": [[12, 14, "MethodName", "early stopping"], [24, 25, "MetricName", "loss"]]}
{"text": "Given its high BLEURT score , it is surprising that T5 - WTA model has low BLEU - 4 .", "entities": [[10, 11, "MethodName", "T5"], [16, 17, "MetricName", "BLEU"]]}
{"text": "We also noted that the CGC - QG model achieves a higher BLEU - 1 than our HTA - WTA model .", "entities": [[12, 13, "MetricName", "BLEU"]]}
{"text": "We argue that this is because the Clue Words Prediction Module learns important cues , increasing the uni - gram overlap with the gold references ( BLEU - 1 ) .", "entities": [[26, 27, "MetricName", "BLEU"]]}
{"text": "We evaluate the results using accuracy ( see Table 4 ) .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "Table 6 in Appendix A.5 presents the F1 scores per skill name .", "entities": [[7, 8, "MetricName", "F1"]]}
{"text": "We also notice that HTA - WTA model performed perfectly on the given sample of Predicting and Figurative Language ( F1 is 1.0 for each skill ) .", "entities": [[20, 21, "MetricName", "F1"]]}
{"text": "In Table 7 , we show the few - shot experiment 's results considering both scoring metrics ( BLEU , and BLUERT ) .", "entities": [[18, 19, "MetricName", "BLEU"]]}
{"text": "( 1 ) To train the model \u03c0 , one typically uses maximum likelihood estimation ( MLE ) , via minimizing the cross - entropy loss , i.e. , J MLE ( \u03c0 )", "entities": [[25, 26, "MetricName", "loss"]]}
{"text": "M = S , A , P , r , \u03b3 , where S is the state space , A is the action space , P is the deterministic environment dynamics , r ( s , y ) is a reward function , and \u03b3 ( 0 , 1 ) is the discrete - time discount factor .", "entities": [[10, 11, "HyperparameterName", "\u03b3"], [44, 45, "HyperparameterName", "\u03b3"], [46, 47, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" 8 T P j 8 p W 5 m g 7 r I V N u D P D 9 n l C 0 6 8 k = \" >", "entities": [[31, 32, "DatasetName", "0"], [34, 36, "HyperparameterName", "k ="]]}
{"text": "O J n M 3 9 1 o r k = \" >", "entities": [[9, 11, "HyperparameterName", "k ="]]}
{"text": "b a s e 6 4 = \" 8 T P j 8 p W 5 m g 7 r I V N u D P D 9 n l C 0 6 8 k = \" >", "entities": [[31, 32, "DatasetName", "0"], [34, 36, "HyperparameterName", "k ="]]}
{"text": "For example , Ranzato et al ( 2016 ) trained a Seq2Seq model by directly optimizing the BLEU / ROUGE scores with the REINFORCE algorithm .", "entities": [[11, 12, "MethodName", "Seq2Seq"], [17, 18, "MetricName", "BLEU"], [23, 24, "MethodName", "REINFORCE"]]}
{"text": "The BLEU scores with different methods are listed in Table 1 .", "entities": [[1, 2, "MetricName", "BLEU"]]}
{"text": "Model Acc ( % ) BLEU BLEU - ref CVAE 73.9 20.7 7.8 Controllable ( Hu et al , 2017 ) 86.7 58.4 - BackTrans ( Prabhumoye et al , 2018 ) 91.2 2.8 2.0 DeleteAndRetrieval ( Li et al , 2018a ) 88.9 36.8 14.7 Guider ( Ours ) 92.7 52.1 25.4", "entities": [[1, 2, "MetricName", "Acc"], [5, 6, "MetricName", "BLEU"], [6, 7, "MetricName", "BLEU"], [9, 10, "MethodName", "CVAE"]]}
{"text": "To measure whether the original sentences ( in the test set ) have been transferred to the desired sentiment , we follow the settings of and employ a pretrained CNN classifier , which achieves an accuracy of 97.4 % on the validation set , to evaluate the transferred sentences .", "entities": [[35, 36, "MetricName", "accuracy"]]}
{"text": "We report BLEU - k ( k from 1 to 4 ) , CIDEr ( Vedantam et al , 2015 ) , and ME - TEOR ( Banerjee and Lavie , 2005 ) scores .", "entities": [[2, 3, "MetricName", "BLEU"], [13, 14, "MetricName", "CIDEr"]]}
{"text": "Adam ( Kingma and Ba , 2014 ) is used for optimization , with learning rate 2 \u00d7 10 \u22124 .", "entities": [[0, 1, "MethodName", "Adam"], [14, 16, "HyperparameterName", "learning rate"]]}
{"text": "We provide an additional comparison with Caccia et al ( 2018 ) and evaluate the diversity and quality with BLEU scores .", "entities": [[19, 20, "MetricName", "BLEU"]]}
{"text": "We also report the F1 - BLEU which considers both diversity and quality in Table 10 .", "entities": [[4, 5, "MetricName", "F1"], [6, 7, "MetricName", "BLEU"]]}
{"text": "For Image COCO , the learning rate of the generator is 0.0002 , the learning rate of the guider 0.0002 , the maximum length of sequence is 25 .", "entities": [[2, 3, "DatasetName", "COCO"], [5, 7, "HyperparameterName", "learning rate"], [14, 16, "HyperparameterName", "learning rate"]]}
{"text": "For WMT , the learning rate of the guider 0.0002 , the learning rate of the guider 0.0002 , the maximum length of sequence is 50 .", "entities": [[4, 6, "HyperparameterName", "learning rate"], [12, 14, "HyperparameterName", "learning rate"]]}
{"text": "We use c = 4 chosen from [ 2 , 3 , 4 , 5 , 8 ] and \u03b3 = 0.25 chosen from [ 0.1 , 0.25 , 0.5 , 0.75 , 0.99 ] .", "entities": [[19, 20, "HyperparameterName", "\u03b3"]]}
{"text": "The learning rate of Discriminator is 0.001 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "For Image Captioning , the learning rate of the guider 0.0002 , the learning rate of the guider 0.0002 , the maximum length of sequence is 25 .", "entities": [[1, 3, "TaskName", "Image Captioning"], [5, 7, "HyperparameterName", "learning rate"], [13, 15, "HyperparameterName", "learning rate"]]}
{"text": "For Style transfer , the learning rate of the guider 0.0001 , the learning rate of the guider 0.0001 , the maximum length of sequence is 15 .", "entities": [[1, 3, "TaskName", "Style transfer"], [5, 7, "HyperparameterName", "learning rate"], [13, 15, "HyperparameterName", "learning rate"]]}
{"text": "Algorithm 2 Guider Matching Generative Adversarial Network ( GMGAN ) Require : generator policy \u03c0 \u03c6 ; discriminator D \u03b8 ; guider network G \u03c8 ; a sequence dataset S = { X 1 ... T } .", "entities": [[4, 7, "MethodName", "Generative Adversarial Network"], [19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "1 : Initialize G \u03c8 , \u03c0 \u03c6 , D \u03b8 with random weights .", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "2 : Pretrain generator \u03c0 \u03c6 , guider G \u03c8 and discriminator D \u03b8 with MLE loss .", "entities": [[13, 14, "HyperparameterName", "\u03b8"], [16, 17, "MetricName", "loss"]]}
{"text": "Test - BLEU - 2 3 4 5 Self - BLEU - 2 3 4 SeqGAN 0.820 0.604 0.361 0.211 0.807 0.577 0.278 RankGAN", "entities": [[2, 3, "MetricName", "BLEU"], [10, 11, "MetricName", "BLEU"]]}
{"text": "Training procedure As with many imitationlearning models ( Bahdanau et al , 2017 ; Rennie et al , 2016 ; Sutskever et al , 2014 ) , we first train the encoder - decoder part based on the off - policy data with an MLE loss .", "entities": [[45, 46, "MetricName", "loss"]]}
{"text": "We adaptively transfer the training from MLE loss to RL loss , similar to ( Paulus et al , 2017 ; Ranzato et al , 2016 ) .", "entities": [[7, 8, "MetricName", "loss"], [10, 11, "MetricName", "loss"]]}
{"text": "( Zhu et al , 2018 ) to evaluate quality of generated samples , where test - BLEU evaluates the reality of generated samples , and self - BLEU measures the diversity .", "entities": [[17, 18, "MetricName", "BLEU"], [28, 29, "MetricName", "BLEU"]]}
{"text": "In practice , we use t = c = 4 and \u03b3 = 0.25 .", "entities": [[11, 12, "HyperparameterName", "\u03b3"]]}
{"text": "Several other works develop unsupervised approach for style transfer by employing Denoising Autoencoding ( DAE ) ( Fu et al , 2017 ) and back - translation ( BT ) ( Lample et al , 2018 ) loss to develop interaction and hence transfer between the source and target domain .", "entities": [[7, 9, "TaskName", "style transfer"], [11, 12, "TaskName", "Denoising"], [37, 38, "MetricName", "loss"]]}
{"text": "Our proposed approach has two key elementsa Transformer - based encoder - decoder model initialized with a pre - trained Transformer Language Model and fine - tuned on DAE loss to achieve style transfer ( Section 3.1 ) and the multiple language models as discriminators stacked together to enable multi - style transfer ( Section 3.2 ) .", "entities": [[7, 8, "MethodName", "Transformer"], [20, 21, "MethodName", "Transformer"], [29, 30, "MetricName", "loss"], [32, 34, "TaskName", "style transfer"], [51, 53, "TaskName", "style transfer"]]}
{"text": "( x | x ) ] , ( 1 ) where \u03b8 G are the trainable parameters of the encoder - decoder model .", "entities": [[11, 12, "HyperparameterName", "\u03b8"]]}
{"text": "This is based on the idea that if the transferred sentence does not fit well in the target style , then the perplexity of language model fine - tuned on that style will be high ( Section 4.1 ) .", "entities": [[22, 23, "MetricName", "perplexity"]]}
{"text": "As discussed in Yang et al ( 2018 ) , we are able to forgo the adversarial training for the discriminator , since the fine - tuned discriminative language model is implicitly capable of assigning high perplexity to negative samples ( out - of - style samples ) , as shown in Section 4.1 .", "entities": [[36, 37, "MetricName", "perplexity"]]}
{"text": "( 3 ) This dictates that transferred sentence x has low perplexity on the language model fine - tuned on style s i , for each target style s i .", "entities": [[11, 12, "MetricName", "perplexity"]]}
{"text": "( 4 ) Using these rewards , the RL objective is to minimize the loss", "entities": [[14, 15, "MetricName", "loss"]]}
{"text": "( 5 ) for style s i , where P \u03b8 G ( x | x ) is as in Equation 1and r ( x ) is the reward in the Equation 4 for the transferred sentence", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "In case of multiple styles , the decoder can be initialized with the language model which is fine - tuned with CLM loss on the mixture of data from target styles , i.e. , CLM loss in Equation 2 with x \u223c T .", "entities": [[22, 23, "MetricName", "loss"], [35, 36, "MetricName", "loss"]]}
{"text": "The style accuracy is evaluated by employing a FastText ( Joulin et al , 2016 ) classifier trained on the corresponding style dimension .", "entities": [[2, 3, "MetricName", "accuracy"], [8, 9, "MethodName", "FastText"]]}
{"text": "Table 1 shows the accuracy of sentences generated by a model fine - tuned on style s i as belonging to the class s i .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "As discussed in discriminative modeling ( Section 3.2 ) , the model fine - tuned with corpus from a certain style is expected to have high perplexity on sentence not from that style and low perplexity otherwise .", "entities": [[26, 27, "MetricName", "perplexity"], [35, 36, "MetricName", "perplexity"]]}
{"text": "As seen in Table 2 , the perplexity for each model is substantially lower on the same corpus as compared to that on the opposite corpus .", "entities": [[7, 8, "MetricName", "perplexity"]]}
{"text": "To measure the accuracy of style transfer , we train two Fasttext 2 classifiers independently for sentiment and formality using the train corpus , as described in Section 4.1 .", "entities": [[3, 4, "MetricName", "accuracy"], [5, 7, "TaskName", "style transfer"], [11, 12, "MethodName", "Fasttext"]]}
{"text": "These classifiers have accuracy of 93.74 % and 88.95 % respectively on test corpus of respective datasets .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "To measure content preservation on transfer , we calculate the BLEU score ( Papineni et al , 2002 ) between the transferred sentence and the input sentence ( self - BLEU ) .", "entities": [[10, 12, "MetricName", "BLEU score"], [30, 31, "MetricName", "BLEU"]]}
{"text": "Score of near 100 on formality lexical scoring imply the transferred text is close in formality to the target corpus . is also apparent from low ref - BLEU scores for our model as well as baselines .", "entities": [[0, 1, "MetricName", "Score"], [28, 29, "MetricName", "BLEU"]]}
{"text": "To measure the fluency of the text , we calculate perplexity assigned to the generated text sequence by a language model trained on the train corpus , as is standard in style transfer literature ( Dai et al , 2019 ; Subramanian et al , 2018 ) .", "entities": [[10, 11, "MetricName", "perplexity"], [31, 33, "TaskName", "style transfer"]]}
{"text": "With \u03b1 = 0.05 , the preferences indicated in human study are significant across all metrics .", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "On the same subset of the DROP dataset for CQA on text , experimental results show that our additions outperform the original NMNs by 3.0 points for the overall F1 score .", "entities": [[6, 7, "DatasetName", "DROP"], [29, 31, "MetricName", "F1 score"]]}
{"text": "For instance , for date - compare questions , MTMSN ( Hu et al , 2019 ) achieves an F1 score of 85.2 1 , whereas NMNs ' performance is 82.6 .", "entities": [[19, 21, "MetricName", "F1 score"]]}
{"text": "Similarly , for count questions , the F1 score is 61.6 for MTMSN and 55.7 for NMNs .", "entities": [[7, 9, "MetricName", "F1 score"]]}
{"text": "Experimental results show that our modifications significantly improve NMNs ' numerical reasoning performance by up to 3.0 absolute F1 points .", "entities": [[18, 19, "MetricName", "F1"]]}
{"text": "In Section 3.3 , we strengthen the auxiliary loss function in NMNs to further concentrate attention in the same sentence .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "Additional computational steps ( Equation 4 to 7 below ) are added after Equation 3 : S n kj = Q k T W n P n j , ( 4 ) A n k = softmax ( S n k ) ,", "entities": [[34, 36, "HyperparameterName", "k ="], [36, 37, "MethodName", "softmax"]]}
{"text": "Gupta et al ( 2020 ) employed an auxiliary loss to constrain the relative positioning of output tokens with respect to input tokens in the \" find - num \" , \" find - date \" and \" relocate \" modules .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "The loss enables the model to concentrate the attention mass of output tokens within a window of size W ( e.g. W = 10 ) .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "Taking the second line in Figure 1 as an example , based on the loss computation procedures , the number \" December 1997 \" will be also \" found \" and connected to the entity \" PUK and KDP \" in NMNs .", "entities": [[14, 15, "MetricName", "loss"]]}
{"text": "We report F1 and Exact Match ( EM ) scores following the literature ( Dua et al , 2019 ; Gupta et al , 2020 ) .", "entities": [[2, 3, "MetricName", "F1"], [4, 6, "MetricName", "Exact Match"], [7, 8, "MetricName", "EM"]]}
{"text": "Row 4 , \" + qai+nepc+aux \" , is our full model , which includes the question - aware interpreter ( + qai ) , the number - entity positional constraint ( + nepc ) , and the improved auxiliary loss ( + aux ) .", "entities": [[40, 41, "MetricName", "loss"]]}
{"text": "It can be observed that compared to \" original \" , our full model achieves significantly higher performance with F1 of 80.4 and EM of 76.6 , representing an increase of 3.0 and 2.6 absolute points respectively .", "entities": [[19, 20, "MetricName", "F1"], [23, 24, "MetricName", "EM"]]}
{"text": "For this variant , the F1 and EM scores improve on the original baseline by 1.6 and 0.9 points respectively .", "entities": [[5, 6, "MetricName", "F1"], [7, 8, "MetricName", "EM"]]}
{"text": "With the addition of the number - entity positional constraint , \" + nepc \" , results show an improvement of 2.5 and 2.0 points for F1 and EM when comparing with \" original \" .", "entities": [[26, 27, "MetricName", "F1"], [28, 29, "MetricName", "EM"]]}
{"text": "Except for the numbercompare type , our model improves on the original NMNs across all other types of questions significantly , by at least 3.2 absolute points for F1 .", "entities": [[28, 29, "MetricName", "F1"]]}
{"text": "Experimental results show that our approach significantly improves NMNs ' numerical reasoning ability , with an increase in F1 of 3.0 absolute points .", "entities": [[18, 19, "MetricName", "F1"]]}
{"text": "After running on the same split of DROP dataset , the F1 and EM scores by NAQANet are 62.1 % and 57.9 % respectively , which are substantially lower than our results in Table 1 , by over 17 % for both scores .", "entities": [[7, 8, "DatasetName", "DROP"], [11, 12, "MetricName", "F1"], [13, 14, "MetricName", "EM"]]}
{"text": "And we did apply these components in Section 3 to other modules , such as the \" extract - argument \" module ( extracts spans or tokens from paragraphs ) , and also obtained better results ( 0.5 % F1 increase ) .", "entities": [[39, 40, "MetricName", "F1"]]}
{"text": "Alternatively , as a more concrete solution , we propose to set d v equal to token embedding dimension while adding head outputs as opposed to the regular approach of concatenation ( Vaswani et al , 2017 ) .", "entities": [[17, 19, "HyperparameterName", "embedding dimension"]]}
{"text": "Without the loss of generality , let a 1 , . . .", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "However , from the first phase of self - attention , the maximum obtainable rank of A l is d k = 64 .", "entities": [[20, 22, "HyperparameterName", "k ="]]}
{"text": "In Vaswani et al ( 2017 ) , d v was bound to be equal to d e /h , where d e is token embedding dimension and h is number of heads .", "entities": [[25, 27, "HyperparameterName", "embedding dimension"]]}
{"text": "We use Adam optimizer , with learning rate = 0.001 , to minimize the cross - entropy loss between the target and predicted label .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "optimizer"], [6, 8, "HyperparameterName", "learning rate"], [17, 18, "MetricName", "loss"]]}
{"text": "For all the experiments , we keep the batch size as 256 and train for 20 epochs .", "entities": [[8, 10, "HyperparameterName", "batch size"]]}
{"text": "For key vector size d k = 1 , 2 , and 4 , We find the identifiable Transformer 's performance is comparable to the regular settings .", "entities": [[5, 7, "HyperparameterName", "k ="], [18, 19, "MethodName", "Transformer"]]}
{"text": "In summary , our contributions are as follows : ( 1 ) We introduce a new data set for non - factoid long QA that to the best of our knowledge is the first data set requiring long answer span detection given non - self - contained and non - factoid questions ; ( 2 ) We show the limitations of the F1 score in evaluating long answers and propose a new evaluation metric ; ( 3 ) To establish baseline results , we experiment with three state - of - the - art models : BERT , RoBERTa , and Longformer , and compare them with human performance .", "entities": [[62, 64, "MetricName", "F1 score"], [96, 97, "MethodName", "BERT"], [98, 99, "MethodName", "RoBERTa"], [101, 102, "MethodName", "Longformer"]]}
{"text": "Exact Match ( EM ) and the macro - averaged F1 score are the two main evaluation metrics in the span detection QA task ( Rajpurkar et al , 2016 ) .", "entities": [[0, 2, "MetricName", "Exact Match"], [3, 4, "MetricName", "EM"], [10, 12, "MetricName", "F1 score"]]}
{"text": "Figure 4 ( left / middle ) compares the F1 and ROUGE - N scores and IoU for the Longformer model on the development set .", "entities": [[9, 10, "MetricName", "F1"], [16, 17, "MetricName", "IoU"], [19, 20, "MethodName", "Longformer"]]}
{"text": "Somewhat surprisingly , the F1 score can be up to 40 % while there is no overlap between the two spans and IoU=0 .", "entities": [[4, 6, "MetricName", "F1 score"]]}
{"text": "ROUGE - 1 similar to F1 can reach 40 % while IoU=0 , but ROUGE - 2 and ROUGE - L are less prone to such over - estimation due to lower chance of overlap of bigrams than unigrams and shorter LCSs in two random non - overlapping sequences .", "entities": [[5, 6, "MetricName", "F1"], [18, 21, "MetricName", "ROUGE - L"]]}
{"text": "Figure 4 ( right ) indicates that the F1 and ROUGE - N scores are higher than IoU for longer answers reiterating the fact that these scores over - estimate more for longer sequences .", "entities": [[8, 9, "MetricName", "F1"], [17, 18, "MetricName", "IoU"]]}
{"text": "Figure 5 shows two spans in a document with high F1 and ROUGE - N percentages , but different meanings .", "entities": [[10, 11, "MetricName", "F1"]]}
{"text": "We have to use a batch size of 12 and 8 , respectively , for the base and large models because of the long input sequence size and memory limitations .", "entities": [[5, 7, "HyperparameterName", "batch size"]]}
{"text": "We use the same batch size of 12 ( batch size of 1 and gradient accumulation over 12 batches ) and learning rate warmup for the first 1 , 000 steps .", "entities": [[4, 6, "HyperparameterName", "batch size"], [9, 11, "HyperparameterName", "batch size"], [21, 23, "HyperparameterName", "learning rate"]]}
{"text": "Due to memory requirements , we limit the experiments to only the Longformer base model ( the large model can not fit on our GPUs even with a batch size of 1 ) .", "entities": [[12, 13, "MethodName", "Longformer"], [28, 30, "HyperparameterName", "batch size"]]}
{"text": "Longformer outperforms the other methods with an IoU of 73.57 % , but the results show that the performance of state - of - the - art question answering systems is far from perfect .", "entities": [[0, 1, "MethodName", "Longformer"], [7, 8, "MetricName", "IoU"], [27, 29, "TaskName", "question answering"]]}
{"text": "That is , as a new heuristic , for each of our differentials dir i , we look out for k = 10 embedded words at the extremes ( having the highest scores in each direction ) and take their average cosine distance within the original embedding D to the differential as a measure .", "entities": [[20, 22, "HyperparameterName", "k ="]]}
{"text": "We select the best suited opposites for a given embedding space by using the Extremal Word Score ( 2.3 ) for d=500 + 44 dimensions ( words + emoji ) .", "entities": [[16, 17, "MetricName", "Score"]]}
{"text": "While the aggregate results are compelling , we use the Krippendorff - alpha metric to measure coder agreement along all six campaigns as shown Tab . 2 ; higher scores depict better agreement .", "entities": [[12, 13, "HyperparameterName", "alpha"]]}
{"text": "The BM25 score between a query sentence Q and a sentence D in the corpus for parent language chv ru BLEU kk 18.47 en 18.61 retrieval C is given by s ( D , Q )", "entities": [[20, 21, "MetricName", "BLEU"]]}
{"text": "Evaluation on test sets is given by SacreBLEU 4 ( Post , 2018 ) , after BPE removal and detokenization .", "entities": [[7, 8, "MetricName", "SacreBLEU"], [16, 17, "MethodName", "BPE"]]}
{"text": "Learning rate warms up for 16 , 000 steps and then follows inverse square root decay .", "entities": [[0, 2, "HyperparameterName", "Learning rate"]]}
{"text": "The peak learning rate is 5 \u00d7 10 \u22124 for parent translation models , and 1 \u00d7 10 \u22124 for child translation models .", "entities": [[2, 4, "HyperparameterName", "learning rate"]]}
{"text": "Early stopping occurs when the validation BLEU does not improve for 10 checkpoints .", "entities": [[0, 2, "MethodName", "Early stopping"], [6, 7, "MetricName", "BLEU"]]}
{"text": "The batch size is 6 , 144 tokens per GPU and 8 NVIDIA V100 GPUs are used .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "For selected finetuning , we use stochastic gradient descent as the optimizer , and the learning rate is 1 \u00d7 10 \u22125 .", "entities": [[6, 9, "MethodName", "stochastic gradient descent"], [11, 12, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"]]}
{"text": "Results in Table 7 demonstrate that ensemble gives BLEU improvements of about 0.8 .", "entities": [[8, 9, "MetricName", "BLEU"]]}
{"text": "For all tasks , we use similar experimental settings for simplicity : we train the model with the batch size of 16 and accumulate gradients every two batches .", "entities": [[18, 20, "HyperparameterName", "batch size"]]}
{"text": "The learning rate is set to be 3e - 4 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "All results are the average F1 scores across 5 runs with different random seeds .", "entities": [[4, 6, "MetricName", "average F1"], [13, 14, "DatasetName", "seeds"]]}
{"text": "Moreover , we can see that our method performs especially well on the ASTE and TASD tasks , the proposed extraction - style method outperforms the previous best models by 7.6 and 3.7 average F1 scores ( across different datasets ) on them respectively .", "entities": [[33, 35, "MetricName", "average F1"]]}
{"text": "However , despite the improvements in state - of - the - art open - domain dialogue modeling , both in terms of distributional accuracy metrics like perplexity , and subjectively in terms of human judgements ( Adiwardana et al , 2020 ; Roller et al , 2021 ) , interactions with those agents reveal that they can not keep their stories straight .", "entities": [[24, 25, "MetricName", "accuracy"], [27, 28, "MetricName", "perplexity"]]}
{"text": "Increasing # of toks or increasing the frequency can lead to improved F1 and RPA , but with significant latency increase for too high values ( e.g. over 11x when applying re - ranking for every partial step using the top 10 tokens each time ) .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "We apply UL loss to the 128 - truncation model in two different ways : ( 1 ) Top - 1 : apply the loss on the token that yields the most incorrect partial sequence RPA classification ; ( 2 ) All : apply the loss to all tokens that yield an incorrect RPA classification on partial sequences .", "entities": [[3, 4, "MetricName", "loss"], [24, 25, "MetricName", "loss"], [45, 46, "MetricName", "loss"]]}
{"text": "We measure generation metrics in terms of RPA ( with PPL and F1 in Table 13 in Appendix E ) , and classification metrics in terms of Hits@1/427 as before .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "Automated Grounding + Multi - Objective Table 5 shows that combining automated grounding with the multi - objective task yields higher hits@1 compared to not using the trainable mask , especially in the first stage of multi - objective training .", "entities": [[21, 22, "MetricName", "hits@1"]]}
{"text": "Results are in Table 4 ; indeed , the combination yields the highest F1 and RPA scores .", "entities": [[13, 14, "MetricName", "F1"]]}
{"text": "When considering performance on automated metrics ( provided in Table 20 in the Appendix ) , we see that generation settings other than beam search , when using a re - ranker , yield lower F1 scores but higher RPA scores , as the RPA re - ranker has more diversity of candidate responses from which to choose ; however , these methods perform worse in human evaluations , with nucleus sampling reranking yielding far more problems and far lower engagingness ratings .", "entities": [[35, 36, "MetricName", "F1"]]}
{"text": "The RPA classifier models are trained with a cross - entropy loss over the correct label , with 99 random negatives chosen from the training set ; we ensured that each character in conversation showed up in the set of candidate labels .", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "The models were trained with a batch size of 16 on 4 32 GB GPUs , with early stopping on the validation set according to valid accuracy .", "entities": [[6, 8, "HyperparameterName", "batch size"], [17, 19, "MethodName", "early stopping"], [26, 27, "MetricName", "accuracy"]]}
{"text": "We used the Adam optimizer ( Kingma and Ba , 2015 ) with weight decay ( Loshchilov and Hutter , 2019 ) , sweeping over learning rates { 1e \u2212 5 , 5e \u2212 6 } .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [13, 15, "MethodName", "weight decay"]]}
{"text": "Generative Models All variants of generative models were trained using 8 32 GB GPUs , with early stopping on perplexity on the validation set .", "entities": [[16, 18, "MethodName", "early stopping"], [19, 20, "MetricName", "perplexity"]]}
{"text": "We used the Adam optimizer , sweeping over learning rates { 1e \u2212 5 , 7e \u2212 6 } , training with a batch size of 128 for the short - truncation models , and 32 for the long - truncation models .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [23, 25, "HyperparameterName", "batch size"]]}
{"text": "We additionally include a third method , Random - 3 , where we apply the loss randomly to 3 tokens that yield incorrect RPA classifications .", "entities": [[15, 16, "MetricName", "loss"]]}
{"text": "Table 13 displays full PPL and F1 scores corresponding to the models in Table 5 .", "entities": [[6, 7, "MetricName", "F1"]]}
{"text": "However , we note that F1 is not a catch - all metric ( Liu et al , 2016 ) .", "entities": [[5, 6, "MetricName", "F1"]]}
{"text": "Thus the task can be considered as a classification problem , that is p ( y t | x , \u03b8 ) , where y t { 0 , 1 } .", "entities": [[20, 21, "HyperparameterName", "\u03b8"], [27, 28, "DatasetName", "0"]]}
{"text": "Each time a new word x t is received , we predict the probability of M + 1 classes as shown in the bottom of Figure 3 , then calculate if the probability of previous M + 1 positions ( x t\u2212M , x t\u2212M +1 , x t ) is larger then a threshold \u03b8 T h .", "entities": [[55, 56, "HyperparameterName", "\u03b8"]]}
{"text": "We evaluate 1 ) the F - score 2 of sentence boundary detection and 2 ) case - sensitive tokenized 4 - gram BLEU ( Papineni et", "entities": [[11, 13, "TaskName", "boundary detection"], [23, 24, "MetricName", "BLEU"]]}
{"text": "We train the machine translation model on WMT 14 with the base version of the Transformer model ( Vaswani et al , 2017 ) , achieving a BLEU score of 27.2 on newstest2014 .", "entities": [[3, 5, "TaskName", "machine translation"], [15, 16, "MethodName", "Transformer"], [27, 29, "MetricName", "BLEU score"]]}
{"text": "In order to keep in line with ( Wang et al , 2016 ) , we add a constraint that sentence should be force segmented if longer than \u03b8 l . N - gram is the method using an N - gram language model to compare the probability of adding vs. not adding a boundary at x t after receiving x t\u2212N +1 , ... ,", "entities": [[28, 29, "HyperparameterName", "\u03b8"]]}
{"text": "The N - gram method without threshold tuning ( with \u03b8 T h = e 0.0 ) divides sentences into small pieces , achieving the lowest average latency of 6.64 .", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "With \u03b8 T h equals to e 2.0 , the F - score of N - gram method increased a little bit ( 0.46 0.48 ) , with a more balanced precision and recall ( precision = 0.51 , recall = 0.48 ) .", "entities": [[1, 2, "HyperparameterName", "\u03b8"]]}
{"text": "The T - LSTM method with the hidden size of 256 performs better than N - gram , but the F - score and BLEU is still limited .", "entities": [[3, 4, "MethodName", "LSTM"], [24, 25, "MetricName", "BLEU"]]}
{"text": "The performance of the four training data organization methods is shown in Figure 4 , all built on IWSLT2014 and conducted under the setup of M = 1 and \u03b8 l = 40 .", "entities": [[29, 30, "HyperparameterName", "\u03b8"]]}
{"text": "The performance of dynamic - force with varying \u03b8 l is shown in Table 4 .", "entities": [[8, 9, "HyperparameterName", "\u03b8"]]}
{"text": "The effect is extremely poor with \u03b8 l = 10 .", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "There are two possible reasons : 1 ) Constraint sentence length less than \u03b8 l is too harsh under small \u03b8 l , 2 ) The discrepancy between the unrestricted training and length - restricted testing causes the poor effect .", "entities": [[13, 14, "HyperparameterName", "\u03b8"], [20, 21, "HyperparameterName", "\u03b8"]]}
{"text": "As elaborated in Figure 5 , For each pair of curves with a same \u03b8 l , dynamic - force and dynamic - base present similar performance .", "entities": [[14, 15, "HyperparameterName", "\u03b8"]]}
{"text": "Moreover , it is interesting to find that the performance of \u03b8 l = 80 is similar with \u03b8 l = 40 at the beginning but falls a little during training .", "entities": [[11, 12, "HyperparameterName", "\u03b8"], [18, 19, "HyperparameterName", "\u03b8"]]}
{"text": "This probably because the setup with \u03b8 l = 40 can filter some inaccurate cases , as the average number of words in IWSLT2014 training set is 20.26 .", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "Increase M from 1 to 2 also promote the performance in both sentence boundary detection f - score and the system BLEU .", "entities": [[13, 15, "TaskName", "boundary detection"], [21, 22, "MetricName", "BLEU"]]}
{"text": "To improve the sentence boundary classification accuracy , some work upgrade the N - gram input to variable - length input by using recurrent neural network ( RNN ) ( Tilk and Alum\u00e4e , 2015 ; Salloum et al , 2017 ) .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "The empirical results on IWSLT2014 demonstrate that our approach achieves significant improvements of 0.19 F - score on sentence segmentation and 1.55 BLEU points compared with the language - model based methods .", "entities": [[18, 20, "TaskName", "sentence segmentation"], [22, 23, "MetricName", "BLEU"]]}
{"text": "Using the best performing models , we build an end - to - end supervised Machine Learning ( ML ) framework for this task that improves the existing F1 score by 16.55 % for the detection and 14.97 % for the resolution subtask .", "entities": [[28, 30, "MetricName", "F1 score"]]}
{"text": "In early stopping the patience is kept to be 10 and the optimizer used is Adam .", "entities": [[1, 3, "MethodName", "early stopping"], [12, 13, "HyperparameterName", "optimizer"], [15, 16, "MethodName", "Adam"]]}
{"text": "We evaluate the performance of our models in terms of F1 - score , computed by taking an average F1 - scores obtained from the 5 - folds results .", "entities": [[10, 13, "MetricName", "F1 - score"], [18, 20, "MetricName", "average F1"]]}
{{"text": "And finally , the team ranks 5th in this task with a weighted average F1 score of 0.93 on the private leader board .", "entities": [[13, 15, "MetricName", "average F1"]]}
{"text": "We describe a distinct combination of network structure , parameter sharing and training procedures that enable dramatic 14 - 20x testtime speedups while retaining accuracy comparable to the Bi - LSTM - CRF .", "entities": [[24, 25, "MetricName", "accuracy"], [30, 31, "MethodName", "LSTM"], [32, 33, "MethodName", "CRF"]]}
{"text": "5.0 English NER , we demonstrate significant speed gains of our ID - CNNs over various recurrent models , while maintaining similar F1 performance .", "entities": [[2, 3, "TaskName", "NER"], [22, 23, "MetricName", "F1"]]}
{"text": "Using the same number of parameters as a simple convolution with the same radius ( i.e. W c has the same dimensionality ) , the \u03b4 > 1 dilated convolution incorporates broader context into the representation of a token than a simple convolution .", "entities": [[3, 6, "HyperparameterName", "number of parameters"], [9, 10, "MethodName", "convolution"], [25, 26, "HyperparameterName", "\u03b4"], [28, 30, "MethodName", "dilated convolution"], [42, 43, "MethodName", "convolution"]]}
{"text": "Let r ( ) denote the ReLU activation function ( Glorot et al , 2011 ) .", "entities": [[6, 7, "MethodName", "ReLU"], [7, 9, "HyperparameterName", "activation function"]]}
{"text": "Sec . 2.1 identifies that the CRF has preferable sample complexity and accuracy since prediction directly reasons in the space of structured outputs .", "entities": [[6, 7, "MethodName", "CRF"], [12, 13, "MetricName", "accuracy"]]}
{"text": "The loss also helps reduce the vanishing gradient problem ( Hochreiter , 1998 ) for deep architectures .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "Huang et al ( 2015 ) achieved state - of - the - art accuracy on partof - speech , chunking and NER using a Bi - LSTM - CRF .", "entities": [[14, 15, "MetricName", "accuracy"], [20, 21, "TaskName", "chunking"], [22, 23, "TaskName", "NER"], [27, 28, "MethodName", "LSTM"], [29, 30, "MethodName", "CRF"]]}
{"text": "We also observe a performance boost in almost all models when broadening the context to incorporate entire documents , achieving an average F1 of 90.65 on CoNLL - 2003 , out - performing the sentence - level model while still decoding at nearly 8 times the speed of the Bi - LSTM - CRF .", "entities": [[21, 23, "MetricName", "average F1"], [51, 52, "MethodName", "LSTM"], [53, 54, "MethodName", "CRF"]]}
{"text": "We also compare our document - level ID - CNNs to a baseline which does not share parameters between blocks ( noshare ) and one that computes loss only at the last block , rather than after every iterated block of dilated convolutions ( 1 - loss ) .", "entities": [[27, 28, "MetricName", "loss"], [46, 47, "MetricName", "loss"]]}
{"text": "Table 1 lists F1 scores of models predicting with sentence - level context on CoNLL - 2003 .", "entities": [[3, 4, "MetricName", "F1"]]}
{"text": "For models that we trained , we report F1 and standard deviation obtained by averaging over 10 random restarts .", "entities": [[8, 9, "MetricName", "F1"]]}
{"text": "The Viterbi - decoding Bi - LSTM - CRF and ID - CNN - CRF and greedy ID - CNN obtain the highest average scores , with the ID - CNN - CRF outperforming the Bi - LSTM - CRF by 0.11 points of F1 on average , and the Bi - LSTM - CRF out - performing the greedy ID - CNN by 0.11 as well .", "entities": [[6, 7, "MethodName", "LSTM"], [8, 9, "MethodName", "CRF"], [14, 15, "MethodName", "CRF"], [32, 33, "MethodName", "CRF"], [37, 38, "MethodName", "LSTM"], [39, 40, "MethodName", "CRF"], [44, 45, "MetricName", "F1"], [52, 53, "MethodName", "LSTM"], [54, 55, "MethodName", "CRF"]]}
{"text": "Our greedy ID - CNN outperforms the Bi - LSTM and the 4 - layer CNN , which uses the same number of parameters as the ID - CNN , and performs similarly to the 5 - layer CNN which uses more parameters but covers the same effective input width .", "entities": [[9, 10, "MethodName", "LSTM"], [21, 24, "HyperparameterName", "number of parameters"]]}
{"text": "All CNN models out - perform the Bi - Model F1 Ratinov and Roth ( 2009 ) 86.82", "entities": [[10, 11, "MetricName", "F1"]]}
{"text": "The most vast speed improvements come when comparing the greedy ID - CNN to the Bi - LSTM - CRF - our ID - CNN is more than 14 times faster than the Bi - LSTM - CRF at test time , with comparable accuracy .", "entities": [[17, 18, "MethodName", "LSTM"], [19, 20, "MethodName", "CRF"], [35, 36, "MethodName", "LSTM"], [37, 38, "MethodName", "CRF"], [44, 45, "MetricName", "accuracy"]]}
{"text": "We emphasize the importance of the dropout regularizer of Ma et al ( 2017 ) in Table 3 , where we observe increased F1 for every model trained with expectation - linear dropout regularization .", "entities": [[23, 24, "MetricName", "F1"]]}
{"text": "Incorporating document - level context further improves our greedy ID - CNN model , attaining 90.65 average F1 .", "entities": [[16, 18, "MetricName", "average F1"]]}
{"text": "In Table 7 , we also list the F1 of our ID - CNN model and the Bi - LSTM - CRF model trained on entire document context .", "entities": [[8, 9, "MetricName", "F1"], [19, 20, "MethodName", "LSTM"], [21, 22, "MethodName", "CRF"]]}
{"text": "Note that the stateof - the - art results on CoNLL03 have achieved an F1 score of \u223c .93 .", "entities": [[10, 11, "DatasetName", "CoNLL03"], [14, 16, "MetricName", "F1 score"]]}
{"text": "We transform the gold ranking of V ( | V | = n ) into n 2 pairwise comparisons for every candidate pair , and learn to minimize the pairwise ranking violations using hinge loss : L M R = 1 m m k=1 1 n 2 k n k i=1 n k j=1 , i = j max ( 0 ,", "entities": [[34, 35, "MetricName", "loss"], [60, 61, "DatasetName", "0"]]}
{"text": "Following \u0160tajner et al ( 2015 ) , we removed the sentence pairs with high ( > 0.9 ) and low ( < 0.1 ) BLEU ( Papineni et al , 2002 ) scores , which mostly correspond to the near identical and semantically divergent sentence pairs respectively .", "entities": [[25, 26, "MetricName", "BLEU"]]}
{"text": "We report SARI ( Xu et al , 2016 ) , which averages the F1 / precision of n - grams ( n { 1 , 2 , 3 , 4 } ) inserted , deleted and kept when compared to human references .", "entities": [[14, 15, "MetricName", "F1"]]}
{"text": "More specifically , it computes the F1 score for the n - grams that are added ( add ) , 8 which is an important indicator if a model is good at paraphrasing .", "entities": [[6, 8, "MetricName", "F1 score"]]}
{"text": "9 To evaluate a model 's para - 8 We slightly improved the SARI implementation by Xu et al ( 2016 ) to exclude the spurious ngrams while calculating the F1 score for add .", "entities": [[30, 32, "MetricName", "F1 score"]]}
{"text": "deletion as they show high self - BLEU ( > 66.5 ) and FK ( > 8.8 ) scores despite having compression ratios similar to other systems .", "entities": [[7, 8, "MetricName", "BLEU"]]}
{"text": "Our model achieves the lowest self - BLEU ( 48.7 ) , FK ( 7.9 ) , and percentage of sentences identical to the input ( 0.4 ) , and the highest add ( 3.3 ) score and percentage of new words ( 16.2 % ) .", "entities": [[7, 8, "MetricName", "BLEU"]]}
{"text": "Nishihara et al ( 2019 ) proposed a loss which controls word complexity , while Mallinson and Lapata ( 2019 ) concatenated constraints to each word embedding .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "We used Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 0.0001 , linear learning rate warmup of 40k steps , and 100k training steps .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "optimizer"], [13, 15, "HyperparameterName", "learning rate"], [19, 21, "HyperparameterName", "learning rate"]]}
{"text": "We used a batch size of 64 .", "entities": [[3, 5, "HyperparameterName", "batch size"]]}
{"text": "We used Adam optimizer with a learning rate of 0.01 and 10 epochs .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "optimizer"], [6, 8, "HyperparameterName", "learning rate"]]}
{"text": "We just examined few values for learning rate ( 0.001 , 0.01 and 0.1 ) and chose the best based on the SARI score on the dev set .", "entities": [[6, 8, "HyperparameterName", "learning rate"]]}
{"text": "Similar to Newsela , we remove the sentence pairs with high ( > 0.9 ) and low ( < 0.1 ) BLEU ( Papineni et al , 2002 ) scores .", "entities": [[2, 3, "DatasetName", "Newsela"], [21, 22, "MetricName", "BLEU"]]}
{"text": "We use contrastive loss described in ( Chopra et al , 2005 ) as the loss function to the Siamese network .", "entities": [[3, 4, "MetricName", "loss"], [15, 16, "MetricName", "loss"], [19, 21, "MethodName", "Siamese network"]]}
{"text": "The contrastive loss can be given by following equation : L = Y | | G W ( X 1 ) , G W ( X 2 ) | | 2 + ( 1 \u2212 Y ) max ( 0 , m \u2212 | | G W ( X 1 ) , G W ( X 2 ) | | 2 ) where Y is annotated tag , 1 if X 1 and X 2 are similar , 0 otherwise .", "entities": [[2, 3, "MetricName", "loss"], [39, 40, "DatasetName", "0"], [78, 79, "DatasetName", "0"]]}
{"text": "m is margin parameter for hinge loss , which is kept 1 for all our networks .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "We use a bidirectional LSTM to generate a 256 dimensional vector for pair of text and train the model by back propagation using contrastive loss .", "entities": [[3, 5, "MethodName", "bidirectional LSTM"], [24, 25, "MetricName", "loss"]]}
{"text": "The batch size is set to 64 and dropout rate is 0.25 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "We use the same hyper - parameters as original paper that give the best results i.e. \u03b1 = 0.2 , \u03b2 = 0.45 , \u03b7 = 0.4 , \u03c6 = 0.2 , \u03b4 = 0.85 .", "entities": [[16, 17, "HyperparameterName", "\u03b1"], [20, 21, "HyperparameterName", "\u03b2"], [32, 33, "HyperparameterName", "\u03b4"]]}
{"text": "We also compute ROUGE - L ( Lin , 2004 ) , which is recall oriented similarity measure based on longest common subsequence ( LCS ) , as a feature in our system .", "entities": [[3, 6, "MetricName", "ROUGE - L"]]}
{"text": "Table 1 shows the dev and test set accuracy for our system with each feature applied incrementally .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "Low levels of pruning ( 30 - 40 % ) do not affect pre - training loss or transfer to downstream tasks at all .", "entities": [[16, 17, "MetricName", "loss"]]}
{"text": "Model compression ( Bucila et al , 2006 ) , which attempts to shrink a model without losing accuracy , is a viable approach to decreasing GPU usage .", "entities": [[0, 2, "TaskName", "Model compression"], [18, 19, "MetricName", "accuracy"]]}
{"text": "Our findings are as follows : Low levels of pruning ( 30 - 40 % ) do not increase pre - training loss or affect transfer to downstream tasks at all .", "entities": [[22, 23, "MetricName", "loss"]]}
{"text": "4 . Continue training the network to recover any lost accuracy .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "We repeat this for learning rates in [ 2 , 3 , 4 , 5 ] \u00d710 \u22125 and show the results with the best development accuracy in Figure 1 / Table 1 .", "entities": [[26, 27, "MetricName", "accuracy"]]}
{"text": "Figure 1 shows that the first 30 - 40 % of weights pruned by magnitude weight pruning do not impact pre - training loss or inference on any downstream task .", "entities": [[23, 24, "MetricName", "loss"]]}
{"text": "Pre - training loss increases as we prune weights necessary for fitting the pre - training data ( Table 1 ) .", "entities": [[3, 4, "MetricName", "loss"]]}
{"text": "13 Downstream accuracy also begins to degrade at this point .", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "30 - 40 % of weights can be pruned using magnitude weight pruning without decreasing dowsntream accuracy .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "While the results for individual tasks are in Table 1 , each task does not vary much from the average trend , with an exception discussed in Section 4.3 . Figure 2 : ( Left ) Pre - training loss predicts information deletion GLUE accuracy linearly as sparsity increases .", "entities": [[39, 40, "MetricName", "loss"], [43, 44, "DatasetName", "GLUE"], [44, 45, "MetricName", "accuracy"]]}
{"text": "At 70 % sparsity and above , models with information deletion recover some accuracy w.r.t . pruned models , so complexity restriction is a secondary cause of performance degradation .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "Figure 2 shows that the pre - training loss linearly predicts the effects of information deletion on downstream accuracy .", "entities": [[8, 9, "MetricName", "loss"], [18, 19, "MetricName", "accuracy"]]}
{"text": "The average GLUE development accuracy and pruning mask difference for models trained on downstream datasets before pruning 60 % at learning rate 5e - 5 .", "entities": [[2, 3, "DatasetName", "GLUE"], [4, 5, "MetricName", "accuracy"], [20, 22, "HyperparameterName", "learning rate"]]}
{"text": "After pruning , models are trained for an additional 2 epochs to regain accuracy .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "is worth only one accuracy point on QQP but 10 points on CoLA .", "entities": [[4, 5, "MetricName", "accuracy"], [7, 8, "DatasetName", "QQP"], [12, 13, "DatasetName", "CoLA"]]}
{"text": "Indeed , Figure 3 shows that the weights selected for 60 % pruning quickly stabilize and evaluation accuracy does not improve with more training before pruning .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "Michel et al ( 2019 ) showed that after fine - tuning on MNLI , up to 40 % of attention heads can be pruned from BERT without affecting test accuracy .", "entities": [[13, 14, "DatasetName", "MNLI"], [26, 27, "MethodName", "BERT"], [30, 31, "MetricName", "accuracy"]]}
{"text": "We also report standard MT metric BLEU ( 1 - 4 ngrams ) , and perform an additional manual evaluation .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "We fine - tune a T5 model ( t5 - base from Hugging - Face library ) using the Adam optimizer with a batch size of 8 and a learning rate of 1e\u22124 .", "entities": [[5, 6, "MethodName", "T5"], [8, 9, "MethodName", "t5"], [19, 20, "MethodName", "Adam"], [20, 21, "HyperparameterName", "optimizer"], [23, 25, "HyperparameterName", "batch size"], [29, 31, "HyperparameterName", "learning rate"]]}
{"text": "We train all models for a maximum of ten epochs with an early stopping value of 1 ( patience ) based on the validation loss .", "entities": [[12, 14, "MethodName", "early stopping"], [24, 25, "MetricName", "loss"]]}
{"text": "Given its high BLEURT score , it is surprising that T5 - WTA model has low BLEU - 4 .", "entities": [[10, 11, "MethodName", "T5"], [16, 17, "MetricName", "BLEU"]]}
{"text": "We also noted that the CGC - QG model achieves a higher BLEU - 1 than our HTA - WTA model .", "entities": [[12, 13, "MetricName", "BLEU"]]}
{"text": "We argue that this is because the Clue Words Prediction Module learns important cues , increasing the uni - gram overlap with the gold references ( BLEU - 1 ) .", "entities": [[26, 27, "MetricName", "BLEU"]]}
{"text": "We evaluate the results using accuracy ( see Table 4 ) .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "Table 6 in Appendix A.5 presents the F1 scores per skill name .", "entities": [[7, 8, "MetricName", "F1"]]}
{"text": "We also notice that HTA - WTA model performed perfectly on the given sample of Predicting and Figurative Language ( F1 is 1.0 for each skill ) .", "entities": [[20, 21, "MetricName", "F1"]]}
{"text": "In Table 7 , we show the few - shot experiment 's results considering both scoring metrics ( BLEU , and BLUERT ) .", "entities": [[18, 19, "MetricName", "BLEU"]]}
{"text": "( 1 ) To train the model \u03c0 , one typically uses maximum likelihood estimation ( MLE ) , via minimizing the cross - entropy loss , i.e. , J MLE ( \u03c0 )", "entities": [[25, 26, "MetricName", "loss"]]}
{"text": "M = S , A , P , r , \u03b3 , where S is the state space , A is the action space , P is the deterministic environment dynamics , r ( s , y ) is a reward function , and \u03b3 ( 0 , 1 ) is the discrete - time discount factor .", "entities": [[10, 11, "HyperparameterName", "\u03b3"], [44, 45, "HyperparameterName", "\u03b3"], [46, 47, "DatasetName", "0"]]}
{"text": "b a s e 6 4 = \" 8 T P j 8 p W 5 m g 7 r I V N u D P D 9 n l C 0 6 8 k = \" >", "entities": [[31, 32, "DatasetName", "0"], [34, 36, "HyperparameterName", "k ="]]}
{"text": "O J n M 3 9 1 o r k = \" >", "entities": [[9, 11, "HyperparameterName", "k ="]]}
{"text": "b a s e 6 4 = \" 8 T P j 8 p W 5 m g 7 r I V N u D P D 9 n l C 0 6 8 k = \" >", "entities": [[31, 32, "DatasetName", "0"], [34, 36, "HyperparameterName", "k ="]]}
{"text": "For example , Ranzato et al ( 2016 ) trained a Seq2Seq model by directly optimizing the BLEU / ROUGE scores with the REINFORCE algorithm .", "entities": [[11, 12, "MethodName", "Seq2Seq"], [17, 18, "MetricName", "BLEU"], [23, 24, "MethodName", "REINFORCE"]]}
{"text": "The BLEU scores with different methods are listed in Table 1 .", "entities": [[1, 2, "MetricName", "BLEU"]]}
{"text": "Model Acc ( % ) BLEU BLEU - ref CVAE 73.9 20.7 7.8 Controllable ( Hu et al , 2017 ) 86.7 58.4 - BackTrans ( Prabhumoye et al , 2018 ) 91.2 2.8 2.0 DeleteAndRetrieval ( Li et al , 2018a ) 88.9 36.8 14.7 Guider ( Ours ) 92.7 52.1 25.4", "entities": [[1, 2, "MetricName", "Acc"], [5, 6, "MetricName", "BLEU"], [6, 7, "MetricName", "BLEU"], [9, 10, "MethodName", "CVAE"]]}
{"text": "To measure whether the original sentences ( in the test set ) have been transferred to the desired sentiment , we follow the settings of and employ a pretrained CNN classifier , which achieves an accuracy of 97.4 % on the validation set , to evaluate the transferred sentences .", "entities": [[35, 36, "MetricName", "accuracy"]]}
{"text": "We report BLEU - k ( k from 1 to 4 ) , CIDEr ( Vedantam et al , 2015 ) , and ME - TEOR ( Banerjee and Lavie , 2005 ) scores .", "entities": [[2, 3, "MetricName", "BLEU"], [13, 14, "MetricName", "CIDEr"]]}
{"text": "Adam ( Kingma and Ba , 2014 ) is used for optimization , with learning rate 2 \u00d7 10 \u22124 .", "entities": [[0, 1, "MethodName", "Adam"], [14, 16, "HyperparameterName", "learning rate"]]}
{"text": "We provide an additional comparison with Caccia et al ( 2018 ) and evaluate the diversity and quality with BLEU scores .", "entities": [[19, 20, "MetricName", "BLEU"]]}
{"text": "We also report the F1 - BLEU which considers both diversity and quality in Table 10 .", "entities": [[4, 5, "MetricName", "F1"], [6, 7, "MetricName", "BLEU"]]}
{"text": "For Image COCO , the learning rate of the generator is 0.0002 , the learning rate of the guider 0.0002 , the maximum length of sequence is 25 .", "entities": [[2, 3, "DatasetName", "COCO"], [5, 7, "HyperparameterName", "learning rate"], [14, 16, "HyperparameterName", "learning rate"]]}
{"text": "For WMT , the learning rate of the guider 0.0002 , the learning rate of the guider 0.0002 , the maximum length of sequence is 50 .", "entities": [[4, 6, "HyperparameterName", "learning rate"], [12, 14, "HyperparameterName", "learning rate"]]}
{"text": "We use c = 4 chosen from [ 2 , 3 , 4 , 5 , 8 ] and \u03b3 = 0.25 chosen from [ 0.1 , 0.25 , 0.5 , 0.75 , 0.99 ] .", "entities": [[19, 20, "HyperparameterName", "\u03b3"]]}
{"text": "The learning rate of Discriminator is 0.001 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "For Image Captioning , the learning rate of the guider 0.0002 , the learning rate of the guider 0.0002 , the maximum length of sequence is 25 .", "entities": [[1, 3, "TaskName", "Image Captioning"], [5, 7, "HyperparameterName", "learning rate"], [13, 15, "HyperparameterName", "learning rate"]]}
{"text": "For Style transfer , the learning rate of the guider 0.0001 , the learning rate of the guider 0.0001 , the maximum length of sequence is 15 .", "entities": [[1, 3, "TaskName", "Style transfer"], [5, 7, "HyperparameterName", "learning rate"], [13, 15, "HyperparameterName", "learning rate"]]}
{"text": "Algorithm 2 Guider Matching Generative Adversarial Network ( GMGAN ) Require : generator policy \u03c0 \u03c6 ; discriminator D \u03b8 ; guider network G \u03c8 ; a sequence dataset S = { X 1 ... T } .", "entities": [[4, 7, "MethodName", "Generative Adversarial Network"], [19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "1 : Initialize G \u03c8 , \u03c0 \u03c6 , D \u03b8 with random weights .", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "2 : Pretrain generator \u03c0 \u03c6 , guider G \u03c8 and discriminator D \u03b8 with MLE loss .", "entities": [[13, 14, "HyperparameterName", "\u03b8"], [16, 17, "MetricName", "loss"]]}
{"text": "Test - BLEU - 2 3 4 5 Self - BLEU - 2 3 4 SeqGAN 0.820 0.604 0.361 0.211 0.807 0.577 0.278 RankGAN", "entities": [[2, 3, "MetricName", "BLEU"], [10, 11, "MetricName", "BLEU"]]}
{"text": "Training procedure As with many imitationlearning models ( Bahdanau et al , 2017 ; Rennie et al , 2016 ; Sutskever et al , 2014 ) , we first train the encoder - decoder part based on the off - policy data with an MLE loss .", "entities": [[45, 46, "MetricName", "loss"]]}
{"text": "We adaptively transfer the training from MLE loss to RL loss , similar to ( Paulus et al , 2017 ; Ranzato et al , 2016 ) .", "entities": [[7, 8, "MetricName", "loss"], [10, 11, "MetricName", "loss"]]}
{"text": "( Zhu et al , 2018 ) to evaluate quality of generated samples , where test - BLEU evaluates the reality of generated samples , and self - BLEU measures the diversity .", "entities": [[17, 18, "MetricName", "BLEU"], [28, 29, "MetricName", "BLEU"]]}
{"text": "In practice , we use t = c = 4 and \u03b3 = 0.25 .", "entities": [[11, 12, "HyperparameterName", "\u03b3"]]}
{"text": "Several other works develop unsupervised approach for style transfer by employing Denoising Autoencoding ( DAE ) ( Fu et al , 2017 ) and back - translation ( BT ) ( Lample et al , 2018 ) loss to develop interaction and hence transfer between the source and target domain .", "entities": [[7, 9, "TaskName", "style transfer"], [11, 12, "TaskName", "Denoising"], [37, 38, "MetricName", "loss"]]}
{"text": "Our proposed approach has two key elementsa Transformer - based encoder - decoder model initialized with a pre - trained Transformer Language Model and fine - tuned on DAE loss to achieve style transfer ( Section 3.1 ) and the multiple language models as discriminators stacked together to enable multi - style transfer ( Section 3.2 ) .", "entities": [[7, 8, "MethodName", "Transformer"], [20, 21, "MethodName", "Transformer"], [29, 30, "MetricName", "loss"], [32, 34, "TaskName", "style transfer"], [51, 53, "TaskName", "style transfer"]]}
{"text": "( x | x ) ] , ( 1 ) where \u03b8 G are the trainable parameters of the encoder - decoder model .", "entities": [[11, 12, "HyperparameterName", "\u03b8"]]}
{"text": "This is based on the idea that if the transferred sentence does not fit well in the target style , then the perplexity of language model fine - tuned on that style will be high ( Section 4.1 ) .", "entities": [[22, 23, "MetricName", "perplexity"]]}
{"text": "As discussed in Yang et al ( 2018 ) , we are able to forgo the adversarial training for the discriminator , since the fine - tuned discriminative language model is implicitly capable of assigning high perplexity to negative samples ( out - of - style samples ) , as shown in Section 4.1 .", "entities": [[36, 37, "MetricName", "perplexity"]]}
{"text": "( 3 ) This dictates that transferred sentence x has low perplexity on the language model fine - tuned on style s i , for each target style s i .", "entities": [[11, 12, "MetricName", "perplexity"]]}
{"text": "( 4 ) Using these rewards , the RL objective is to minimize the loss", "entities": [[14, 15, "MetricName", "loss"]]}
{"text": "( 5 ) for style s i , where P \u03b8 G ( x | x ) is as in Equation 1and r ( x ) is the reward in the Equation 4 for the transferred sentence", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "In case of multiple styles , the decoder can be initialized with the language model which is fine - tuned with CLM loss on the mixture of data from target styles , i.e. , CLM loss in Equation 2 with x \u223c T .", "entities": [[22, 23, "MetricName", "loss"], [35, 36, "MetricName", "loss"]]}
{"text": "The style accuracy is evaluated by employing a FastText ( Joulin et al , 2016 ) classifier trained on the corresponding style dimension .", "entities": [[2, 3, "MetricName", "accuracy"], [8, 9, "MethodName", "FastText"]]}
{"text": "Table 1 shows the accuracy of sentences generated by a model fine - tuned on style s i as belonging to the class s i .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "As discussed in discriminative modeling ( Section 3.2 ) , the model fine - tuned with corpus from a certain style is expected to have high perplexity on sentence not from that style and low perplexity otherwise .", "entities": [[26, 27, "MetricName", "perplexity"], [35, 36, "MetricName", "perplexity"]]}
{"text": "As seen in Table 2 , the perplexity for each model is substantially lower on the same corpus as compared to that on the opposite corpus .", "entities": [[7, 8, "MetricName", "perplexity"]]}
{"text": "To measure the accuracy of style transfer , we train two Fasttext 2 classifiers independently for sentiment and formality using the train corpus , as described in Section 4.1 .", "entities": [[3, 4, "MetricName", "accuracy"], [5, 7, "TaskName", "style transfer"], [11, 12, "MethodName", "Fasttext"]]}
{"text": "These classifiers have accuracy of 93.74 % and 88.95 % respectively on test corpus of respective datasets .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "To measure content preservation on transfer , we calculate the BLEU score ( Papineni et al , 2002 ) between the transferred sentence and the input sentence ( self - BLEU ) .", "entities": [[10, 12, "MetricName", "BLEU score"], [30, 31, "MetricName", "BLEU"]]}
{"text": "Score of near 100 on formality lexical scoring imply the transferred text is close in formality to the target corpus . is also apparent from low ref - BLEU scores for our model as well as baselines .", "entities": [[0, 1, "MetricName", "Score"], [28, 29, "MetricName", "BLEU"]]}
{"text": "To measure the fluency of the text , we calculate perplexity assigned to the generated text sequence by a language model trained on the train corpus , as is standard in style transfer literature ( Dai et al , 2019 ; Subramanian et al , 2018 ) .", "entities": [[10, 11, "MetricName", "perplexity"], [31, 33, "TaskName", "style transfer"]]}
{"text": "With \u03b1 = 0.05 , the preferences indicated in human study are significant across all metrics .", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "On the same subset of the DROP dataset for CQA on text , experimental results show that our additions outperform the original NMNs by 3.0 points for the overall F1 score .", "entities": [[6, 7, "DatasetName", "DROP"], [29, 31, "MetricName", "F1 score"]]}
{"text": "For instance , for date - compare questions , MTMSN ( Hu et al , 2019 ) achieves an F1 score of 85.2 1 , whereas NMNs ' performance is 82.6 .", "entities": [[19, 21, "MetricName", "F1 score"]]}
{"text": "Similarly , for count questions , the F1 score is 61.6 for MTMSN and 55.7 for NMNs .", "entities": [[7, 9, "MetricName", "F1 score"]]}
{"text": "Experimental results show that our modifications significantly improve NMNs ' numerical reasoning performance by up to 3.0 absolute F1 points .", "entities": [[18, 19, "MetricName", "F1"]]}
{"text": "In Section 3.3 , we strengthen the auxiliary loss function in NMNs to further concentrate attention in the same sentence .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "Additional computational steps ( Equation 4 to 7 below ) are added after Equation 3 : S n kj = Q k T W n P n j , ( 4 ) A n k = softmax ( S n k ) ,", "entities": [[34, 36, "HyperparameterName", "k ="], [36, 37, "MethodName", "softmax"]]}
{"text": "Gupta et al ( 2020 ) employed an auxiliary loss to constrain the relative positioning of output tokens with respect to input tokens in the \" find - num \" , \" find - date \" and \" relocate \" modules .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "The loss enables the model to concentrate the attention mass of output tokens within a window of size W ( e.g. W = 10 ) .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "Taking the second line in Figure 1 as an example , based on the loss computation procedures , the number \" December 1997 \" will be also \" found \" and connected to the entity \" PUK and KDP \" in NMNs .", "entities": [[14, 15, "MetricName", "loss"]]}
{"text": "We report F1 and Exact Match ( EM ) scores following the literature ( Dua et al , 2019 ; Gupta et al , 2020 ) .", "entities": [[2, 3, "MetricName", "F1"], [4, 6, "MetricName", "Exact Match"], [7, 8, "MetricName", "EM"]]}
{"text": "Row 4 , \" + qai+nepc+aux \" , is our full model , which includes the question - aware interpreter ( + qai ) , the number - entity positional constraint ( + nepc ) , and the improved auxiliary loss ( + aux ) .", "entities": [[40, 41, "MetricName", "loss"]]}
{"text": "It can be observed that compared to \" original \" , our full model achieves significantly higher performance with F1 of 80.4 and EM of 76.6 , representing an increase of 3.0 and 2.6 absolute points respectively .", "entities": [[19, 20, "MetricName", "F1"], [23, 24, "MetricName", "EM"]]}
{"text": "For this variant , the F1 and EM scores improve on the original baseline by 1.6 and 0.9 points respectively .", "entities": [[5, 6, "MetricName", "F1"], [7, 8, "MetricName", "EM"]]}
{"text": "With the addition of the number - entity positional constraint , \" + nepc \" , results show an improvement of 2.5 and 2.0 points for F1 and EM when comparing with \" original \" .", "entities": [[26, 27, "MetricName", "F1"], [28, 29, "MetricName", "EM"]]}
{"text": "Except for the numbercompare type , our model improves on the original NMNs across all other types of questions significantly , by at least 3.2 absolute points for F1 .", "entities": [[28, 29, "MetricName", "F1"]]}
{"text": "Experimental results show that our approach significantly improves NMNs ' numerical reasoning ability , with an increase in F1 of 3.0 absolute points .", "entities": [[18, 19, "MetricName", "F1"]]}
{"text": "After running on the same split of DROP dataset , the F1 and EM scores by NAQANet are 62.1 % and 57.9 % respectively , which are substantially lower than our results in Table 1 , by over 17 % for both scores .", "entities": [[7, 8, "DatasetName", "DROP"], [11, 12, "MetricName", "F1"], [13, 14, "MetricName", "EM"]]}
{"text": "And we did apply these components in Section 3 to other modules , such as the \" extract - argument \" module ( extracts spans or tokens from paragraphs ) , and also obtained better results ( 0.5 % F1 increase ) .", "entities": [[39, 40, "MetricName", "F1"]]}
{"text": "Alternatively , as a more concrete solution , we propose to set d v equal to token embedding dimension while adding head outputs as opposed to the regular approach of concatenation ( Vaswani et al , 2017 ) .", "entities": [[17, 19, "HyperparameterName", "embedding dimension"]]}
{"text": "Without the loss of generality , let a 1 , . . .", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "However , from the first phase of self - attention , the maximum obtainable rank of A l is d k = 64 .", "entities": [[20, 22, "HyperparameterName", "k ="]]}
{"text": "In Vaswani et al ( 2017 ) , d v was bound to be equal to d e /h , where d e is token embedding dimension and h is number of heads .", "entities": [[25, 27, "HyperparameterName", "embedding dimension"]]}
{"text": "We use Adam optimizer , with learning rate = 0.001 , to minimize the cross - entropy loss between the target and predicted label .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "optimizer"], [6, 8, "HyperparameterName", "learning rate"], [17, 18, "MetricName", "loss"]]}
{"text": "For all the experiments , we keep the batch size as 256 and train for 20 epochs .", "entities": [[8, 10, "HyperparameterName", "batch size"]]}
{"text": "For key vector size d k = 1 , 2 , and 4 , We find the identifiable Transformer 's performance is comparable to the regular settings .", "entities": [[5, 7, "HyperparameterName", "k ="], [18, 19, "MethodName", "Transformer"]]}
{"text": "In summary , our contributions are as follows : ( 1 ) We introduce a new data set for non - factoid long QA that to the best of our knowledge is the first data set requiring long answer span detection given non - self - contained and non - factoid questions ; ( 2 ) We show the limitations of the F1 score in evaluating long answers and propose a new evaluation metric ; ( 3 ) To establish baseline results , we experiment with three state - of - the - art models : BERT , RoBERTa , and Longformer , and compare them with human performance .", "entities": [[62, 64, "MetricName", "F1 score"], [96, 97, "MethodName", "BERT"], [98, 99, "MethodName", "RoBERTa"], [101, 102, "MethodName", "Longformer"]]}
{"text": "Exact Match ( EM ) and the macro - averaged F1 score are the two main evaluation metrics in the span detection QA task ( Rajpurkar et al , 2016 ) .", "entities": [[0, 2, "MetricName", "Exact Match"], [3, 4, "MetricName", "EM"], [10, 12, "MetricName", "F1 score"]]}
{"text": "Figure 4 ( left / middle ) compares the F1 and ROUGE - N scores and IoU for the Longformer model on the development set .", "entities": [[9, 10, "MetricName", "F1"], [16, 17, "MetricName", "IoU"], [19, 20, "MethodName", "Longformer"]]}
{"text": "Somewhat surprisingly , the F1 score can be up to 40 % while there is no overlap between the two spans and IoU=0 .", "entities": [[4, 6, "MetricName", "F1 score"]]}
{"text": "ROUGE - 1 similar to F1 can reach 40 % while IoU=0 , but ROUGE - 2 and ROUGE - L are less prone to such over - estimation due to lower chance of overlap of bigrams than unigrams and shorter LCSs in two random non - overlapping sequences .", "entities": [[5, 6, "MetricName", "F1"], [18, 21, "MetricName", "ROUGE - L"]]}
{"text": "Figure 4 ( right ) indicates that the F1 and ROUGE - N scores are higher than IoU for longer answers reiterating the fact that these scores over - estimate more for longer sequences .", "entities": [[8, 9, "MetricName", "F1"], [17, 18, "MetricName", "IoU"]]}
{"text": "Figure 5 shows two spans in a document with high F1 and ROUGE - N percentages , but different meanings .", "entities": [[10, 11, "MetricName", "F1"]]}
{"text": "We have to use a batch size of 12 and 8 , respectively , for the base and large models because of the long input sequence size and memory limitations .", "entities": [[5, 7, "HyperparameterName", "batch size"]]}
{"text": "We use the same batch size of 12 ( batch size of 1 and gradient accumulation over 12 batches ) and learning rate warmup for the first 1 , 000 steps .", "entities": [[4, 6, "HyperparameterName", "batch size"], [9, 11, "HyperparameterName", "batch size"], [21, 23, "HyperparameterName", "learning rate"]]}
{"text": "Due to memory requirements , we limit the experiments to only the Longformer base model ( the large model can not fit on our GPUs even with a batch size of 1 ) .", "entities": [[12, 13, "MethodName", "Longformer"], [28, 30, "HyperparameterName", "batch size"]]}
{"text": "Longformer outperforms the other methods with an IoU of 73.57 % , but the results show that the performance of state - of - the - art question answering systems is far from perfect .", "entities": [[0, 1, "MethodName", "Longformer"], [7, 8, "MetricName", "IoU"], [27, 29, "TaskName", "question answering"]]}
{"text": "That is , as a new heuristic , for each of our differentials dir i , we look out for k = 10 embedded words at the extremes ( having the highest scores in each direction ) and take their average cosine distance within the original embedding D to the differential as a measure .", "entities": [[20, 22, "HyperparameterName", "k ="]]}
{"text": "We select the best suited opposites for a given embedding space by using the Extremal Word Score ( 2.3 ) for d=500 + 44 dimensions ( words + emoji ) .", "entities": [[16, 17, "MetricName", "Score"]]}
{"text": "While the aggregate results are compelling , we use the Krippendorff - alpha metric to measure coder agreement along all six campaigns as shown Tab . 2 ; higher scores depict better agreement .", "entities": [[12, 13, "HyperparameterName", "alpha"]]}
{"text": "The BM25 score between a query sentence Q and a sentence D in the corpus for parent language chv ru BLEU kk 18.47 en 18.61 retrieval C is given by s ( D , Q )", "entities": [[20, 21, "MetricName", "BLEU"]]}
{"text": "Evaluation on test sets is given by SacreBLEU 4 ( Post , 2018 ) , after BPE removal and detokenization .", "entities": [[7, 8, "MetricName", "SacreBLEU"], [16, 17, "MethodName", "BPE"]]}
{"text": "Learning rate warms up for 16 , 000 steps and then follows inverse square root decay .", "entities": [[0, 2, "HyperparameterName", "Learning rate"]]}
{"text": "The peak learning rate is 5 \u00d7 10 \u22124 for parent translation models , and 1 \u00d7 10 \u22124 for child translation models .", "entities": [[2, 4, "HyperparameterName", "learning rate"]]}
{"text": "Early stopping occurs when the validation BLEU does not improve for 10 checkpoints .", "entities": [[0, 2, "MethodName", "Early stopping"], [6, 7, "MetricName", "BLEU"]]}
{"text": "The batch size is 6 , 144 tokens per GPU and 8 NVIDIA V100 GPUs are used .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "For selected finetuning , we use stochastic gradient descent as the optimizer , and the learning rate is 1 \u00d7 10 \u22125 .", "entities": [[6, 9, "MethodName", "stochastic gradient descent"], [11, 12, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"]]}
{"text": "Results in Table 7 demonstrate that ensemble gives BLEU improvements of about 0.8 .", "entities": [[8, 9, "MetricName", "BLEU"]]}
{"text": "For all tasks , we use similar experimental settings for simplicity : we train the model with the batch size of 16 and accumulate gradients every two batches .", "entities": [[18, 20, "HyperparameterName", "batch size"]]}
{"text": "The learning rate is set to be 3e - 4 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "All results are the average F1 scores across 5 runs with different random seeds .", "entities": [[4, 6, "MetricName", "average F1"], [13, 14, "DatasetName", "seeds"]]}
{"text": "Moreover , we can see that our method performs especially well on the ASTE and TASD tasks , the proposed extraction - style method outperforms the previous best models by 7.6 and 3.7 average F1 scores ( across different datasets ) on them respectively .", "entities": [[33, 35, "MetricName", "average F1"]]}
{"text": "However , despite the improvements in state - of - the - art open - domain dialogue modeling , both in terms of distributional accuracy metrics like perplexity , and subjectively in terms of human judgements ( Adiwardana et al , 2020 ; Roller et al , 2021 ) , interactions with those agents reveal that they can not keep their stories straight .", "entities": [[24, 25, "MetricName", "accuracy"], [27, 28, "MetricName", "perplexity"]]}
{"text": "Increasing # of toks or increasing the frequency can lead to improved F1 and RPA , but with significant latency increase for too high values ( e.g. over 11x when applying re - ranking for every partial step using the top 10 tokens each time ) .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "We apply UL loss to the 128 - truncation model in two different ways : ( 1 ) Top - 1 : apply the loss on the token that yields the most incorrect partial sequence RPA classification ; ( 2 ) All : apply the loss to all tokens that yield an incorrect RPA classification on partial sequences .", "entities": [[3, 4, "MetricName", "loss"], [24, 25, "MetricName", "loss"], [45, 46, "MetricName", "loss"]]}
{"text": "We measure generation metrics in terms of RPA ( with PPL and F1 in Table 13 in Appendix E ) , and classification metrics in terms of Hits@1/427 as before .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "Automated Grounding + Multi - Objective Table 5 shows that combining automated grounding with the multi - objective task yields higher hits@1 compared to not using the trainable mask , especially in the first stage of multi - objective training .", "entities": [[21, 22, "MetricName", "hits@1"]]}
{"text": "Results are in Table 4 ; indeed , the combination yields the highest F1 and RPA scores .", "entities": [[13, 14, "MetricName", "F1"]]}
{"text": "When considering performance on automated metrics ( provided in Table 20 in the Appendix ) , we see that generation settings other than beam search , when using a re - ranker , yield lower F1 scores but higher RPA scores , as the RPA re - ranker has more diversity of candidate responses from which to choose ; however , these methods perform worse in human evaluations , with nucleus sampling reranking yielding far more problems and far lower engagingness ratings .", "entities": [[35, 36, "MetricName", "F1"]]}
{"text": "The RPA classifier models are trained with a cross - entropy loss over the correct label , with 99 random negatives chosen from the training set ; we ensured that each character in conversation showed up in the set of candidate labels .", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "The models were trained with a batch size of 16 on 4 32 GB GPUs , with early stopping on the validation set according to valid accuracy .", "entities": [[6, 8, "HyperparameterName", "batch size"], [17, 19, "MethodName", "early stopping"], [26, 27, "MetricName", "accuracy"]]}
{"text": "We used the Adam optimizer ( Kingma and Ba , 2015 ) with weight decay ( Loshchilov and Hutter , 2019 ) , sweeping over learning rates { 1e \u2212 5 , 5e \u2212 6 } .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [13, 15, "MethodName", "weight decay"]]}
{"text": "Generative Models All variants of generative models were trained using 8 32 GB GPUs , with early stopping on perplexity on the validation set .", "entities": [[16, 18, "MethodName", "early stopping"], [19, 20, "MetricName", "perplexity"]]}
{"text": "We used the Adam optimizer , sweeping over learning rates { 1e \u2212 5 , 7e \u2212 6 } , training with a batch size of 128 for the short - truncation models , and 32 for the long - truncation models .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [23, 25, "HyperparameterName", "batch size"]]}
{"text": "We additionally include a third method , Random - 3 , where we apply the loss randomly to 3 tokens that yield incorrect RPA classifications .", "entities": [[15, 16, "MetricName", "loss"]]}
{"text": "Table 13 displays full PPL and F1 scores corresponding to the models in Table 5 .", "entities": [[6, 7, "MetricName", "F1"]]}
{"text": "However , we note that F1 is not a catch - all metric ( Liu et al , 2016 ) .", "entities": [[5, 6, "MetricName", "F1"]]}
{"text": "Thus the task can be considered as a classification problem , that is p ( y t | x , \u03b8 ) , where y t { 0 , 1 } .", "entities": [[20, 21, "HyperparameterName", "\u03b8"], [27, 28, "DatasetName", "0"]]}
{"text": "Each time a new word x t is received , we predict the probability of M + 1 classes as shown in the bottom of Figure 3 , then calculate if the probability of previous M + 1 positions ( x t\u2212M , x t\u2212M +1 , x t ) is larger then a threshold \u03b8 T h .", "entities": [[55, 56, "HyperparameterName", "\u03b8"]]}
{"text": "We evaluate 1 ) the F - score 2 of sentence boundary detection and 2 ) case - sensitive tokenized 4 - gram BLEU ( Papineni et", "entities": [[11, 13, "TaskName", "boundary detection"], [23, 24, "MetricName", "BLEU"]]}
{"text": "We train the machine translation model on WMT 14 with the base version of the Transformer model ( Vaswani et al , 2017 ) , achieving a BLEU score of 27.2 on newstest2014 .", "entities": [[3, 5, "TaskName", "machine translation"], [15, 16, "MethodName", "Transformer"], [27, 29, "MetricName", "BLEU score"]]}
{"text": "In order to keep in line with ( Wang et al , 2016 ) , we add a constraint that sentence should be force segmented if longer than \u03b8 l . N - gram is the method using an N - gram language model to compare the probability of adding vs. not adding a boundary at x t after receiving x t\u2212N +1 , ... ,", "entities": [[28, 29, "HyperparameterName", "\u03b8"]]}
{"text": "The N - gram method without threshold tuning ( with \u03b8 T h = e 0.0 ) divides sentences into small pieces , achieving the lowest average latency of 6.64 .", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "With \u03b8 T h equals to e 2.0 , the F - score of N - gram method increased a little bit ( 0.46 0.48 ) , with a more balanced precision and recall ( precision = 0.51 , recall = 0.48 ) .", "entities": [[1, 2, "HyperparameterName", "\u03b8"]]}
{"text": "The T - LSTM method with the hidden size of 256 performs better than N - gram , but the F - score and BLEU is still limited .", "entities": [[3, 4, "MethodName", "LSTM"], [24, 25, "MetricName", "BLEU"]]}
{"text": "The performance of the four training data organization methods is shown in Figure 4 , all built on IWSLT2014 and conducted under the setup of M = 1 and \u03b8 l = 40 .", "entities": [[29, 30, "HyperparameterName", "\u03b8"]]}
{"text": "The performance of dynamic - force with varying \u03b8 l is shown in Table 4 .", "entities": [[8, 9, "HyperparameterName", "\u03b8"]]}
{"text": "The effect is extremely poor with \u03b8 l = 10 .", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "There are two possible reasons : 1 ) Constraint sentence length less than \u03b8 l is too harsh under small \u03b8 l , 2 ) The discrepancy between the unrestricted training and length - restricted testing causes the poor effect .", "entities": [[13, 14, "HyperparameterName", "\u03b8"], [20, 21, "HyperparameterName", "\u03b8"]]}
{"text": "As elaborated in Figure 5 , For each pair of curves with a same \u03b8 l , dynamic - force and dynamic - base present similar performance .", "entities": [[14, 15, "HyperparameterName", "\u03b8"]]}
{"text": "Moreover , it is interesting to find that the performance of \u03b8 l = 80 is similar with \u03b8 l = 40 at the beginning but falls a little during training .", "entities": [[11, 12, "HyperparameterName", "\u03b8"], [18, 19, "HyperparameterName", "\u03b8"]]}
{"text": "This probably because the setup with \u03b8 l = 40 can filter some inaccurate cases , as the average number of words in IWSLT2014 training set is 20.26 .", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "Increase M from 1 to 2 also promote the performance in both sentence boundary detection f - score and the system BLEU .", "entities": [[13, 15, "TaskName", "boundary detection"], [21, 22, "MetricName", "BLEU"]]}
{"text": "To improve the sentence boundary classification accuracy , some work upgrade the N - gram input to variable - length input by using recurrent neural network ( RNN ) ( Tilk and Alum\u00e4e , 2015 ; Salloum et al , 2017 ) .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "The empirical results on IWSLT2014 demonstrate that our approach achieves significant improvements of 0.19 F - score on sentence segmentation and 1.55 BLEU points compared with the language - model based methods .", "entities": [[18, 20, "TaskName", "sentence segmentation"], [22, 23, "MetricName", "BLEU"]]}
{"text": "Using the best performing models , we build an end - to - end supervised Machine Learning ( ML ) framework for this task that improves the existing F1 score by 16.55 % for the detection and 14.97 % for the resolution subtask .", "entities": [[28, 30, "MetricName", "F1 score"]]}
{"text": "In early stopping the patience is kept to be 10 and the optimizer used is Adam .", "entities": [[1, 3, "MethodName", "early stopping"], [12, 13, "HyperparameterName", "optimizer"], [15, 16, "MethodName", "Adam"]]}
{"text": "We evaluate the performance of our models in terms of F1 - score , computed by taking an average F1 - scores obtained from the 5 - folds results .", "entities": [[10, 13, "MetricName", "F1 - score"], [18, 20, "MetricName", "average F1"]]}
{"text": "The results on the testset for Precision , Recall and F1 - Score values are presented in Table .2 .", "entities": [[6, 7, "MetricName", "Precision"], [8, 9, "MetricName", "Recall"], [10, 13, "MetricName", "F1 - Score"]]}
{"text": "This obviously results into error propagation into the second model , and lowers the precision value to 82.52 % , recall to 78.66 % and consequently , the F1 - score to 80.55 % of the final system .", "entities": [[28, 31, "MetricName", "F1 - score"]]}
{"text": "As expected , this model improves the F1 - score by 16.55 % for noun ellipsis detection and 14.97 % for noun ellipsis res - olution .", "entities": [[7, 10, "MetricName", "F1 - score"]]}
{"text": "x \" encoderpT , \u03b8 encoder q , ( ) 1 where \u03b8 encoder is the parameters of the encoder .", "entities": [[4, 5, "HyperparameterName", "\u03b8"], [12, 13, "HyperparameterName", "\u03b8"]]}
{"text": "i qq ( 2 ) where \u03b8 attention \" tv a , W a u is the trainable parameters of the attention , which is not updated in this step to learn the word representation x based the good attention learned by the compressor .", "entities": [[6, 7, "HyperparameterName", "\u03b8"]]}
{"text": "\u03b1 \" r\u03b1 1 , \u03b1 2 , ... , \u03b1 | T | s is the attention weights .", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [5, 6, "HyperparameterName", "\u03b1"], [10, 11, "HyperparameterName", "\u03b1"]]}
{"text": "Rq by replacing p \u03b8 pzq with a variational distribution r \u03c8 pzq , 3 We give the main steps as follows and the detailed derivation is provided in supplementary materials .", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "Then , we obtain the lower bound L of IB by substituting Equation 5 and 7 into Equation 3 : L \" E ppr , yq rE p \u03b8 pz | rq rlog q \u03c6 py |", "entities": [[28, 29, "HyperparameterName", "\u03b8"]]}
{"text": "zq\u015b \u03b2\u00a8KLrp \u03b8 pz | rq } r \u03c8 pzqss ( 8 )", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "To compute p \u03b8 pz | rq , we adopt the reparametrization trick for multivariate Gaussians ( Rezende et al , 2014 ) , which obtains the gradient of parameters that derive z from a random noise .", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "We report the accuracy of our VAT and baselines based on LSTM and BERT ( Table 2 ) .", "entities": [[3, 4, "MetricName", "accuracy"], [11, 12, "MethodName", "LSTM"], [13, 14, "MethodName", "BERT"]]}
{"text": "In this section , we evaluate our VAT model using two metrics , AOPC and post - hoc accuracy , which are widely used for explanations ( Chen and Ji , 2020 ) .", "entities": [[18, 19, "MetricName", "accuracy"]]}
{"text": "For the IMDB dataset , when deleting top 20 words ( average length is 268 ) , the accuracy reduces about 19 points for our LSTM - VAT model while it is about 2 points for the random model .", "entities": [[2, 3, "DatasetName", "IMDB"], [18, 19, "MetricName", "accuracy"], [25, 26, "MethodName", "LSTM"]]}
{"text": "We also adopt the post - hoc accuracy ( Chen et al , 2018 ) to evaluate the influence of task - specific essential words on the performance of LSTM - based and BERT - based models .", "entities": [[7, 8, "MetricName", "accuracy"], [29, 30, "MethodName", "LSTM"], [33, 34, "MethodName", "BERT"]]}
{"text": "First , it is interesting to find that the post - hoc accuracy with five most important words on Sbuj dataset ( 89.10 ) is even better than the original sentence ( 89.00 ) .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "Additionally , for the IMDB dataset , the accuracy of LSTM - base with one word selected by our VAT model is even better than the one with 20 words selected randomly .", "entities": [[4, 5, "DatasetName", "IMDB"], [8, 9, "MetricName", "accuracy"], [10, 11, "MethodName", "LSTM"]]}
{"text": "We report word - level precision , recall , and F - measure for evaluating the models ( Table 5 ) , the same as .", "entities": [[10, 13, "MetricName", "F - measure"]]}
{"text": "To have a deep look at how it works , we first provide our VAT model 's accuracy with different iterations ( Table 6 ) .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "( 6 ) The total contribution of the j - th input token is thus given by \u03b1 ij = d B ij E wd H L ijd , ( 7 ) where we dropped the dependence on w for simplicity .", "entities": [[17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "To train our models , we use maximum likelihood estimation ( MLE ) with Adam ( \u03b2 1 = 0.9 , \u03b2 2 = 0.999 , = 1e \u22128 ) starting with a learning rate of 5e \u22124 that we scale by a factor of 0.8 if no improvement ( \u03b4 \u2264 0.01 ) is noticed on the validation loss after three evaluations , we evaluate every 8 K updates .", "entities": [[14, 15, "MethodName", "Adam"], [16, 17, "HyperparameterName", "\u03b2"], [21, 22, "HyperparameterName", "\u03b2"], [33, 35, "HyperparameterName", "learning rate"], [50, 51, "HyperparameterName", "\u03b4"], [59, 60, "MetricName", "loss"]]}
{"text": "We use a beam - search of width 5 without any length or coverage penalty and measure translation quality using the BLEU metric ( Papineni et al , 2002 ) . Baselines .", "entities": [[21, 22, "MetricName", "BLEU"]]}
{"text": "The model is trained with MLE using Nesterov accelerated gradient with a momentum of 0.99 and an initial learning rate of 0.25 decaying by a factor of 0.1 every epoch .", "entities": [[7, 10, "MethodName", "Nesterov accelerated gradient"], [18, 20, "HyperparameterName", "learning rate"]]}
{"text": "For MLE training we use Adam ( \u03b2 1 = 0.9 , \u03b2 2 = 0.98 , = 1e \u22128 ) ( Kingma and Ba , 2015 ) , and a learning rate starting from 1e \u22125 that is increased during 4 , 000 warm - up steps then used a learning rate of 5e \u22124 that follows an inverse - square - root schedule afterwards ( Vaswani et al , 2017 ) .", "entities": [[5, 6, "MethodName", "Adam"], [7, 8, "HyperparameterName", "\u03b2"], [12, 13, "HyperparameterName", "\u03b2"], [31, 33, "HyperparameterName", "learning rate"], [51, 53, "HyperparameterName", "learning rate"]]}
{"text": "We also state the number of parameters of each model and the computational cost of training , estimated in a similar way as Vaswani et al ( 2017 ) , based on the wall clock time of training and the GPU single precision specs .", "entities": [[4, 7, "HyperparameterName", "number of parameters"]]}
{"text": "In Table 1 we see that using max - pooling instead average - pooling across the source dimension increases the performance with around 2 BLEU points .", "entities": [[24, 25, "MetricName", "BLEU"]]}
{"text": "d s on both source and target , the total number of features for token prediction produced by the network is f L = 2d + gL. In Figure 4 we see that for token embedding sizes between 128 to 256 lead to BLEU scores vary between 33.5 and 34 .", "entities": [[43, 44, "MetricName", "BLEU"]]}
{"text": "The growth rate ( g ) has an important impact on performance , increasing it from 8 to 32 increases the BLEU scrore by more than 2.5 point .", "entities": [[21, 22, "MetricName", "BLEU"]]}
{"text": "The depth of the network also has an important impact on performance , increasing the BLEU score by about 2 points when increasing the depth from 8 to 24 layers .", "entities": [[15, 17, "MetricName", "BLEU score"]]}
{"text": "In Table 2 , we note that narrower receptive fields are better than larger ones with less layers at equivalent complextities e.g. comparing ( k = 3 , L = 20 ) to ( k = 5 , L = 12 ) , and ( k = 5 , L = 16 ) with ( k = 7 , L = 12 ) .", "entities": [[24, 26, "HyperparameterName", "k ="], [34, 36, "HyperparameterName", "k ="], [45, 47, "HyperparameterName", "k ="], [55, 57, "HyperparameterName", "k ="]]}
{"text": "Our model has about the same number of parameters as RNNsearch , yet improves performance by almost 3 BLEU points .", "entities": [[6, 9, "HyperparameterName", "number of parameters"], [18, 19, "MetricName", "BLEU"]]}
{"text": "Table 1 shows our results in terms of BLEU scores ( Papineni et al , 2002 ) as calculated on our local machines .", "entities": [[8, 9, "MetricName", "BLEU"]]}
{"text": "In the final evaluation of the shared task , our system achieves a result of 66.53 % in macro - averaged LAS F1 - score .", "entities": [[22, 25, "MetricName", "F1 - score"]]}
{"text": "\u03c3 = [ ROOT ] , \u03b2 = w 1 , ... , w n , A = 1 .", "entities": [[6, 7, "HyperparameterName", "\u03b2"]]}
{"text": "\u03b2 , A \u03c3 | w i , \u03b2 , A 2 . Left - Arc r : \u03c3 |", "entities": [[0, 1, "HyperparameterName", "\u03b2"], [8, 9, "HyperparameterName", "\u03b2"]]}
{"text": "w i | w j , \u03b2 , A \u03c3 | w j , \u03b2 , A \u222a r ( w j , w i ) 3 .", "entities": [[6, 7, "HyperparameterName", "\u03b2"], [14, 15, "HyperparameterName", "\u03b2"]]}
{"text": "We trained and tuned the models on different treebanks , and in the final evaluation , a score of 66.53 % in macro - averaged LAS F1 - score measurement is achieved .", "entities": [[26, 29, "MetricName", "F1 - score"]]}
{"text": "The official evaluation shows that our system achieves 66.53 % in macroaveraged LAS F1 - score measurement on the official blind test .", "entities": [[13, 16, "MetricName", "F1 - score"]]}
{"text": "+ \u03bb \u03b8 2 + \u00b5 m", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "\u00b5 is automatically selected such that it gives the best 5 - fold crossvalidation accuracy on n true labels .", "entities": [[14, 15, "MetricName", "accuracy"]]}
{"text": "IR+Roc : a Rocchio classifier ( \u03b1 = 1 , \u03b2 = 0.5 , \u03b3 = 0 ) ; IR+NB : a multinomial naive Bayes classifier ( Laplace smoothing , \u03b1 = 0.01 ) ; IR+LR a logistic regression classifier ( linear kernel , C = 1 ) .", "entities": [[6, 7, "HyperparameterName", "\u03b1"], [10, 11, "HyperparameterName", "\u03b2"], [14, 15, "HyperparameterName", "\u03b3"], [16, 17, "DatasetName", "0"], [30, 31, "HyperparameterName", "\u03b1"], [37, 39, "MethodName", "logistic regression"]]}
{"text": "ST - 0 : the initial self - training classifier using class labels as \" training documents \" ( multinomial na\u00efve Bayes , Laplace smoothing \u03b1 = 0.01 ) .", "entities": [[2, 3, "DatasetName", "0"], [25, 26, "HyperparameterName", "\u03b1"]]}
{"text": "for the languages tested , with better average intent classification accuracy ( 96.07 % versus 95.50 % ) but worse average slot F1 ( 89.87 % versus 90.81 % ) .", "entities": [[8, 10, "TaskName", "intent classification"], [10, 11, "MetricName", "accuracy"], [22, 23, "MetricName", "F1"]]}
{"text": "When simultaneous translation is performed , average intent classification accuracy degrades by only 1.7 % relative and average slot F1 degrades by only 1.2 % relative .", "entities": [[7, 9, "TaskName", "intent classification"], [9, 10, "MetricName", "accuracy"], [19, 20, "MetricName", "F1"]]}
{"text": "The model consists of 12 encoder layers , 12 decoder layers , a hidden layer size of 1 , 024 , and 16 attention heads , yielding a parameter count of 680M.", "entities": [[13, 16, "HyperparameterName", "hidden layer size"]]}
{"text": "Training was performed on 8 Nvidia V100 GPUs ( 16 GB ) using a batch size of 32 , layer normalization for both the encoder and the decoder ( Xu et", "entities": [[14, 16, "HyperparameterName", "batch size"], [19, 21, "MethodName", "layer normalization"]]}
{"text": "al , 2019 ) ; label smoothed cross entropy with = 0.2 ( Szegedy et al , 2016 ) ; the ADAM optimizer with \u03b2 1 = 0.9 and \u03b2 2 = 0.999 ( Kingma and Ba , 2014 ) ; an initial learning rate of 3 \u00d7 10 \u22125 with polynomial decay over 20 , 000 updates after 1 epoch of warmup ; attention dropout of 0.1 and dropout of 0.2 elsewhere ; and FP16 type for weights .", "entities": [[21, 22, "DatasetName", "ADAM"], [22, 23, "HyperparameterName", "optimizer"], [24, 25, "HyperparameterName", "\u03b2"], [29, 30, "HyperparameterName", "\u03b2"], [43, 45, "HyperparameterName", "learning rate"], [64, 66, "MethodName", "attention dropout"]]}
{"text": "Examining the first training configuration ( 1 , 496 samples for Hindi and 626 for Turkish ) , the nontranslated mBART 's macro - averaged intent classification ( 96.07 % ) outperforms Cross - Lingual BERT by Xu et al ( 2020 ) ( 95.50 % ) , but slot F1 is worse ( 89.87 % for non - translated mBART and 90.81 % for Cross - Lingual BERT ) .", "entities": [[20, 21, "MethodName", "mBART"], [25, 27, "TaskName", "intent classification"], [35, 36, "MethodName", "BERT"], [50, 51, "MetricName", "F1"], [60, 61, "MethodName", "mBART"], [68, 69, "MethodName", "BERT"]]}
{"text": "When translation is performed ( the STIL task ) , intent classification accuracy degrades by 1.7 % relative from 96.07 % to 94.40 % , and slot F1 degrades by 1.2 % relative from 89.87 % to 88.79 % .", "entities": [[10, 12, "TaskName", "intent classification"], [12, 13, "MetricName", "accuracy"], [27, 28, "MetricName", "F1"]]}
{"text": "Macro - averaged intent classification improves from 94.40 % to 95.94 % , and slot F1 improves from 88.79 % to 90.10 % , both of which are statistically significant .", "entities": [[3, 5, "TaskName", "intent classification"], [15, 16, "MetricName", "F1"]]}
{"text": "Language identification F1 is above 99.7 % for all languages , with perfect performance in many cases .", "entities": [[0, 2, "TaskName", "Language identification"], [2, 3, "MetricName", "F1"]]}
{"text": "Applied to images of Shakespeare 's First Folio , our model predicts attributions that agree with the manual judgements of bibliographers with an accuracy of 87 % , even on text that is the output of OCR .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "We compute the one - to - one and many - to - one accuracy , mapping the recovered page groups to the gold compositors to maximize accuracy , as is standard for many unsupervised clustering tasks , e.g. POS induction ( see Christodoulopoulos et al ( 2010 ) ) .", "entities": [[14, 15, "MetricName", "accuracy"], [27, 28, "MetricName", "accuracy"]]}
{"text": "The bestperforming model for both manually transcribed and OCR text uses EDIT features in conjunction with spacing generation and achieves an accuracy of up to 87 % .", "entities": [[21, 22, "MetricName", "accuracy"]]}
{"text": "Note that we set K = 2 ( No . of topics ) , m = 4 ( No . of keywords ) , and n = 2 ( No . of documents ) , just for clear demonstration .", "entities": [[4, 6, "HyperparameterName", "K ="]]}
{"text": "\u03b2 ( t j , D L , D R ) , ( 1 ) where \u03b2 ( t , D L , D R ) represents the polarization score of topic t between D L and D R .", "entities": [[0, 1, "HyperparameterName", "\u03b2"], [16, 17, "HyperparameterName", "\u03b2"]]}
{"text": "As a result , given a document d D C , the model is optimized to classify whether it is from D L or D R by a binary cross - entropy loss , where the [ CLS ] embedding is used to represent the document , as shown in Figure 1 ( b ) .", "entities": [[32, 33, "MetricName", "loss"]]}
{"text": "( 6 ) A higher value of \u03b2 indicates more polarization .", "entities": [[7, 8, "HyperparameterName", "\u03b2"]]}
{"text": "The number of topics K is selected from a grid search in [ 10 , 50 ] and the model with K = 39 produces the best coherence value ( R\u00f6der et al , 2015 ) .", "entities": [[21, 23, "HyperparameterName", "K ="]]}
{"text": "( 8 ) A higher value of \u03b1 signifies more polarization .", "entities": [[7, 8, "HyperparameterName", "\u03b1"]]}
{"text": "We train the model using Adam optimizer , with learning rate 1e - 5 and weight decay 5e - 4 .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "optimizer"], [9, 11, "HyperparameterName", "learning rate"], [15, 17, "MethodName", "weight decay"]]}
{"text": "We use a batch size of 64 and train the model on 4 RTX 2080 GPUs .", "entities": [[3, 5, "HyperparameterName", "batch size"]]}
{"text": "The best validation F1 score on classifying partisanship is 91.3 .", "entities": [[3, 5, "MetricName", "F1 score"]]}
{"text": "We used an Adam optimizer with lr=0.01 , training on a corpus of 16 , 000 clips ( randomly resampled to between 0.6x and 1.0x the original speed ) for 70 epochs with a batch size of 16 ( \u2248 8 hours on an AWS c5.4xlarge EC2 instance ) .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [34, 36, "HyperparameterName", "batch size"]]}
{"text": "Experiments on the Mul - tiATIS++ benchmark show that our method leads to an average improvement of +4.2 % in accuracy for the intent task and +1.8 % in F1 for the slot - filling task over the state - of - the - art across 8 typologically diverse languages .", "entities": [[20, 21, "MetricName", "accuracy"], [29, 30, "MetricName", "F1"]]}
{"text": "sl m = sof tmax ( W sl hm + b sl ) \u2200m ( 1 ) with a multi - class cross - entropy loss 3 for both intent ( L i ) and slots ( L sl ) .", "entities": [[3, 4, "DatasetName", "sof"], [25, 26, "MetricName", "loss"]]}
{"text": "Consistent with the metrics reported for intent prediction and slot filling evaluation in the past , we also accuracy for intent and micro F1 5 to measure slot performance .", "entities": [[9, 11, "TaskName", "slot filling"], [18, 19, "MetricName", "accuracy"], [22, 24, "MetricName", "micro F1"]]}
{"text": "When compared to the state - ofthe - art jointly trained English - only baseline , we see a +4.2 % boost in intent accuracy and +1.8 % boost in slot F1 scores on average by augmenting the dataset via multilingual code - switching without requiring the target language .", "entities": [[24, 25, "MetricName", "accuracy"], [31, 32, "MetricName", "F1"]]}
{"text": "The runtime of the models in Table 5 ( Appendix B ) shows that code - switching is expensive , taking up to five hours for five augmentation rounds ( k = 5 ) .", "entities": [[30, 32, "HyperparameterName", "k ="]]}
{"text": "Code - Switching improved intent accuracy by +12.5 % and slot F1 by +2.3 % , which is quite promising considering the domain mismatch ( tweets vs airline guides ) .", "entities": [[5, 6, "MetricName", "accuracy"], [11, 12, "MetricName", "F1"]]}
{"text": "Joint training added +0.9 % improvement to intent accuracy , however did not seem to help slot F1 .", "entities": [[8, 9, "MetricName", "accuracy"], [17, 18, "MetricName", "F1"]]}
{"text": "k = 0 represents original size with no code - switching , k = 1 represents original size with code - switching , and k = 10 means 10 - times more code - switched data than the original .", "entities": [[0, 2, "HyperparameterName", "k ="], [2, 3, "DatasetName", "0"], [12, 14, "HyperparameterName", "k ="], [24, 26, "HyperparameterName", "k ="]]}
{"text": "The main experiments in Table 3 use k = 5 .", "entities": [[7, 9, "HyperparameterName", "k ="]]}
{"text": "For this analysis , we consider 4 target languages on which code - switching produced significant results in Table 3 on both Intent Accuracy and Slot F1 : Chinese , Japanese , Hindi , and Turkish .", "entities": [[23, 24, "MetricName", "Accuracy"], [26, 27, "MetricName", "F1"]]}
{"text": "For Slot F1 performance in all four cases , unlike Intent , we observe an interesting dip when k = 1 , which represents the augmentation having just one copy of codeswitching ( without the original non - code - switched data ) , as compared to k = 0 .", "entities": [[2, 3, "MetricName", "F1"], [18, 20, "HyperparameterName", "k ="], [47, 49, "HyperparameterName", "k ="], [49, 50, "DatasetName", "0"]]}
{"text": "Adding the original data to one round of code - switched data ( k = 2 ) leads to big improvements .", "entities": [[13, 15, "HyperparameterName", "k ="]]}
{"text": "We observe that , setting k = 5 , XLM - R outperforms mBERT on average ( by 2 % Intent Accuracy and 1.5 % Slot F1 ) .", "entities": [[5, 7, "HyperparameterName", "k ="], [9, 10, "MethodName", "XLM"], [13, 14, "MethodName", "mBERT"], [21, 22, "MetricName", "Accuracy"], [26, 27, "MetricName", "F1"]]}
{"text": "For intent , the ( \u03b1 , \u03b2 ) combination of ( 1.0 , 0.6 ) performed well , while ( 1.0 , 1.0 ) for slots .", "entities": [[5, 6, "HyperparameterName", "\u03b1"], [7, 8, "HyperparameterName", "\u03b2"]]}
{"text": "We use the standard ( exact - match ) F1 score ( Chinchor , 1991 ) to evaluate the output 5 There are potentially more role types depending on the dataset ( e.g. normalized dates , times , locations ) ; we will not consider those here . produced by a template - filling system : F 1 = 2 Precision Recall Precision + Recall", "entities": [[9, 11, "MetricName", "F1 score"], [60, 61, "MetricName", "Precision"], [61, 62, "MetricName", "Recall"], [62, 63, "MetricName", "Precision"], [64, 65, "MetricName", "Recall"]]}
{"text": "Whenever there is a matched mention pair in which the predicted role filler has an exact match to an element of the set of coreferent gold role fillers , this adds 1 to the numerator of both precision and recall .", "entities": [[15, 17, "MetricName", "exact match"]]}
{"text": "Thus , if the predicted role filler is an exact match for the gold role filler , the SCS is 0 .", "entities": [[9, 11, "MetricName", "exact match"], [20, 21, "DatasetName", "0"]]}
{"text": "Table 3 presents the precision , recall , and F1 performance on the MUC - 4 dataset for early models from 1992 alongside those of the more recent DyGIE++ and GTT models .", "entities": [[9, 10, "MetricName", "F1"]]}
{"text": "( 1 ) Span Error .", "entities": [[4, 5, "MetricName", "Error"]]}
{"text": "We learn a topic model M on training corpus T with k topics using LDA ( Blei et al , 2003 ) and quantify topic similarity by comparing the inferred topic distributions \u03b8 D i | M , \u03b8 S", "entities": [[14, 15, "MethodName", "LDA"], [32, 33, "HyperparameterName", "\u03b8"], [38, 39, "HyperparameterName", "\u03b8"]]}
{"text": "= 1 \u2212 JS ( \u03b8 D i | M ,", "entities": [[5, 6, "HyperparameterName", "\u03b8"]]}
{"text": "We set k = 20 and T = D. Abstractivity .", "entities": [[2, 4, "HyperparameterName", "k ="]]}
{"text": "We quantify redundancy as the average ROUGE - L 6 Different names and interpretations have been given for these properties in the literature .", "entities": [[6, 9, "MetricName", "ROUGE - L"]]}
{"text": "We chose to report the results corresponding to k = 20 , T = D. We chose the value for k based on qualitative judgments about topic quality for CNN - DM , PeerRead , and AMI , as we considered these to be a diverse subset of all 10 datasets .", "entities": [[8, 10, "HyperparameterName", "k ="], [29, 32, "DatasetName", "CNN - DM"], [33, 34, "DatasetName", "PeerRead"]]}
{"text": "All hyperparameters are set as default and we discussed the number of topics k and training corpus T in A.2 with the results in the main paper using k = 20 and T = D where T is truncated to be at most 20000 documents .", "entities": [[28, 30, "HyperparameterName", "k ="]]}
{"text": "19 All scores reported in the main paper use ROUGE - L and use the computed F - measure score .", "entities": [[9, 12, "MetricName", "ROUGE - L"], [16, 19, "MetricName", "F - measure"]]}
{"text": "We see a slight ( 0.3 BLEU ) increase in quality for a chunk size of 250 ms , though the initial wait does not improve the BLEU and a considerable increase in the latency .", "entities": [[6, 7, "MetricName", "BLEU"], [27, 28, "MetricName", "BLEU"]]}
{"text": "Recall the average latency for speech input defined by : AL speech = 1 \u03c4 \u2032 ( | X | )", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "We see that an improvement in the offline model ( offline BLEU of 31.36 and 33.14 for Model A and B , respectively ) leads to improvement in the online regime .", "entities": [[11, 12, "MetricName", "BLEU"]]}
{"text": "in medium and high latency regimes using both models ( i.e. , a model trained from scratch and a model based on pretrained wav2vec and mBART ) , and is almost on par in the low latency regime ( Model A is losing 0.35 BLEU and Model B is losing 0.47 BLEU ) .", "entities": [[25, 26, "MethodName", "mBART"], [44, 45, "MetricName", "BLEU"], [51, 52, "MetricName", "BLEU"]]}
{"text": "We see around 70 % relative gain in BLEU point for En \u2194 Ha and around 25 % relative improvements for Bn \u2194 Hi and Xh \u2194 Zu compared to bilingual baselines .", "entities": [[8, 9, "MetricName", "BLEU"]]}
{"text": "y1 < HY P > y2 y which led to around 0.3 BLEU improvement for Ha En , but we have n't tried on other pairs due to time limitation .", "entities": [[12, 13, "MetricName", "BLEU"]]}
{"text": "We report Sacre - BLEU ( Post , 2018 ) on the validation set released in WMT21 , and both SacreBLEU and COMET ( Rei et al , 2020 ) using the available implementation 3 on the official test set released in WMT21 .", "entities": [[4, 5, "MetricName", "BLEU"], [20, 21, "MetricName", "SacreBLEU"]]}
{"text": "We see large gains up to 8 - 10 BLEU points for En \u2194 Ha and nice improvements of up to 2 - 3 BLEU points for Bn \u2194 Hi and Xh \u2194 Zu .", "entities": [[9, 10, "MetricName", "BLEU"], [24, 25, "MetricName", "BLEU"]]}
{"text": "RetNRef \u03b1 is the dialogue retrieval version of RetNRef , which adopts \u03b1 - blending to escape from simply ignoring the retrieved exemplars ( \u03b1 = 0.5 ) .", "entities": [[1, 2, "HyperparameterName", "\u03b1"], [12, 13, "HyperparameterName", "\u03b1"], [24, 25, "HyperparameterName", "\u03b1"]]}
{"text": "When calculating BLEU , we use sentence_bleu function in nltk python package ( Loper and Bird , 2002 ) .", "entities": [[2, 3, "MetricName", "BLEU"]]}
{"text": "We compute the proportion of exact word matches over each blank and the precision of the top k = 5 predictions for both setups .", "entities": [[17, 19, "HyperparameterName", "k ="]]}
{"text": "We see that our transducer model with fixed alignment transition probabilities performs best in terms of predictive accuracy ( exact match and top - 5 precision ) , while the seqseq model with attention is the next best in most comparisons .", "entities": [[17, 18, "MetricName", "accuracy"], [19, 21, "MetricName", "exact match"]]}
{"text": "With fine - tuning , its accuracy is substantially higher , but it still suffers from the same fundamental limitations as our other models ( see Table 5 ) .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "The forward variable \u03b1 i ( j ) representing p ( y 1 : j , a j = i | x 1 : i ) is recursively as \u03b1i ( j ) =", "entities": [[3, 4, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1 k ( j \u2212 1 ) p ( aj = i", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "( 7 ) The marginal likelihood objective is to train the model to optimize \u03b1 m ( n )", "entities": [[14, 15, "HyperparameterName", "\u03b1"]]}
{"text": "Latent variable models can be trained either through directly optimizing the likelihood objective through gradient descent ( as described above ) , or with the Expectation Maximization ( EM ) algorithm ( Dempster et al , 1977 ) , which alternates between calculating expectations over the values of the latent variables given the current parameters , and maximizing the expected complete data log likelihood given those expectations .", "entities": [[28, 29, "MetricName", "EM"]]}
{"text": "We consider training our alignment model with Viterbi EM ( Brown et al , 1993 ) , also known as \" hard \" EM , where at each iteration the most likely assignment of the hidden variables ( alignments ) are found and the parameters are updated to optimize the log likelihood given those alignments .", "entities": [[8, 9, "MetricName", "EM"], [23, 24, "MetricName", "EM"]]}
{"text": "Viterbi EM has been shown to give superior performance to standard EM on unsupervised parsing ( Spitkovsky et al , 2010 ) , due to better convergence properties in practice by making the distribution more peaked .", "entities": [[1, 2, "MetricName", "EM"], [11, 12, "MetricName", "EM"]]}
{"text": "We use the standard Viterbi algorithm , in which we replace the sum in equation ( 7 ) with max , and keep track of the index corresponding to each value of \u03b1 during the forward computation .", "entities": [[32, 33, "HyperparameterName", "\u03b1"]]}
{"text": "Gimpel ( 2018 , 2019 ) found improvements in both speed and accuracy by replacing the use of gradient descent with a method that trains a neural network ( called an \" inference network \" ) to do inference directly .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "A SPEN ( Belanger and McCallum , 2016 ) defines an energy function E \u0398 : X \u00d7 Y R parameterized by \u0398 that computes a scalar energy for an input / output pair .", "entities": [[14, 15, "HyperparameterName", "\u0398"], [22, 23, "HyperparameterName", "\u0398"]]}
{"text": "This becomes intractable when E \u0398 does not decompose into a sum over small \" parts \" of y. Belanger and McCallum ( 2016 ) relaxed this problem by allowing the discrete vector y to be continuous ; Y R denotes the relaxed output space .", "entities": [[5, 6, "HyperparameterName", "\u0398"]]}
{"text": "\u2248 arg min y Y R ( x ) E \u0398 ( x , y ) ( 2 )", "entities": [[10, 11, "HyperparameterName", "\u0398"]]}
{"text": "When training the energy function parameters \u0398 , Tu and Gimpel ( 2018 ) replaced the cost - augmented inference step in the structured hinge loss from Belanger and McCallum ( 2016 ) with a costaugmented inference network F \u03a6 : F \u03a6 ( x )", "entities": [[6, 7, "HyperparameterName", "\u0398"], [25, 26, "MetricName", "loss"]]}
{"text": "Tu and Gimpel ( 2018 ) alternatively optimized \u0398 and \u03a6 , which is similar to training in generative adversarial networks ( Goodfellow et al , 2014 ) .", "entities": [[8, 9, "HyperparameterName", "\u0398"]]}
{"text": "+ perceptron loss ( 5 ) As indicated , this loss can be viewed as the sum of the margin - rescaled hinge and perceptron losses for SPENs .", "entities": [[2, 3, "MetricName", "loss"], [10, 11, "MetricName", "loss"]]}
{"text": "Figure 2 ( b ) shows faster convergence to high accuracy when adding the local CE term .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "To analyze , we compute the cost - augmented loss l 1 = ( F \u03a6 ( x ) , y )", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "\u2212 E \u0398 ( x , F \u03a6 ( x ) ) and the margin - rescaled hinge loss With k = 1 , the setting used by Tu and Gimpel ( 2018 ) , the inference network lags behind the energy , making the energy parameter updates very small , as shown by the small norms in Fig .", "entities": [[2, 3, "HyperparameterName", "\u0398"], [18, 19, "MetricName", "loss"], [20, 22, "HyperparameterName", "k ="]]}
{"text": "However , increasing k too much also harms learning , as evi - denced by the \" plateau \" effect in the l 1 curves for k = 50 ; this indicates that the energy function is lagging behind the inference network .", "entities": [[26, 28, "HyperparameterName", "k ="]]}
{"text": "Using k = 5 leads to more of a balance between l 1 and l 0 and gradient norms that are mostly decreasing during training .", "entities": [[1, 3, "HyperparameterName", "k ="], [15, 16, "DatasetName", "0"]]}
{"text": "In the relaxed output space Y R ( x ) , y t , j can be interpreted as the probability of the tth position being labeled with label j. We then use the following energy for sequence labeling ( Tu and Gimpel , 2018 ) : E \u0398 ( x , y )", "entities": [[48, 49, "HyperparameterName", "\u0398"]]}
{"text": "This energy returns the negative log - likelihood under the TLM of the candidate output y. Tu and Gimpel ( 2018 ) pretrained their h on a large , automatically - tagged corpus and fixed its parameters when optimizing \u0398. Our approach has one critical difference .", "entities": [[5, 8, "MetricName", "log - likelihood"]]}
{"text": "+ log ( y t y t ) + \u03b3 log ( y t y t ) + log ( y t y t ) ( 9 )", "entities": [[9, 10, "HyperparameterName", "\u03b3"]]}
{"text": "GE ( b ) : forward and backward TLMs ( \u03b3 = 0 ) ; GE ( c ) : all four TLMs in Eq .", "entities": [[10, 11, "HyperparameterName", "\u03b3"], [12, 13, "DatasetName", "0"]]}
{"text": "We use a learning rate of 5 10 \u22124 .", "entities": [[3, 5, "HyperparameterName", "learning rate"]]}
{"text": "There are several efforts aimed at stabilizing and improving learning in generative adversarial networks ( GANs ) ( Goodfellow et al , 2014 ; Salimans et al , 2016 ; Zhao et al , 2017 ; from overcoming learning difficulties by modifying loss functions and optimization , and GANs have become more successful and popular as a result .", "entities": [[42, 43, "MetricName", "loss"]]}
{"text": "( 8 ) as the energy function : E \u0398 ( x , y )", "entities": [[9, 10, "HyperparameterName", "\u0398"]]}
{"text": "The seq2seq baseline achieves 82.80 F1 on the development set in our replication of Tran et al ( 2018 ) .", "entities": [[1, 2, "MethodName", "seq2seq"], [5, 6, "MetricName", "F1"]]}
{"text": "Using a SPEN with our stacked parameterization , we obtain 83.22 F1 .", "entities": [[11, 12, "MetricName", "F1"]]}
{"text": "ALBERT ( Lan et al , 2020 ) is a lite BERT for self - supervised learning of language representations , which uses layer - to - layer parameter sharing to reduce the number of parameters of the model , which not only speeds up the model training but also outperforms BERT on certain datasets .", "entities": [[0, 1, "MethodName", "ALBERT"], [11, 12, "MethodName", "BERT"], [13, 17, "TaskName", "self - supervised learning"], [33, 36, "HyperparameterName", "number of parameters"], [51, 52, "MethodName", "BERT"]]}
{"text": "The Adam optimizer ( Ba and Kingma , 2015 ) was used to update all trainable parameters .", "entities": [[1, 2, "MethodName", "Adam"], [2, 3, "HyperparameterName", "optimizer"]]}
{"text": "The loss functions in subtasks 1 and 3 were binary cross - entropy , and subtask 2 was categorical cross - entropy .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "In subtask 2 , the results of our proposed multitask sequence labeling model on the dev set are F 1 - score of 0.215 , Precision of 0.378 , and Recall of 0.151 .", "entities": [[25, 26, "MetricName", "Precision"], [30, 31, "MetricName", "Recall"]]}
{"text": "The results on the test set are F 1 - score of 0.091 , Precision of 0.186 , and Recall of 0.061 .", "entities": [[14, 15, "MetricName", "Precision"], [19, 20, "MetricName", "Recall"]]}
{"text": "For example , our Random Forest Classifier only gave about 54 % accuracy .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "The hidden layer size was 50 nodes , and dense layer size was 200 nodes .", "entities": [[1, 4, "HyperparameterName", "hidden layer size"]]}
{"text": "The optimization method was RMSprop , and the learning rate was 0.001 .", "entities": [[4, 5, "MethodName", "RMSprop"], [8, 10, "HyperparameterName", "learning rate"]]}
{"text": "The Area Under the Curve ( AUC , as noted in Table 1 ) is 0.892 , and as evident from the graph , is much closer to 1 .", "entities": [[6, 7, "MetricName", "AUC"]]}
{"text": "Our system had 81.1 % accuracy with the human analysts in judging tokens as r - less or r - ful , scoring 0.829 for F - measure .", "entities": [[5, 6, "MetricName", "accuracy"], [25, 28, "MetricName", "F - measure"]]}
{"text": "This classification gave an average speaker accuracy of 63.3 % and an average token accuracy of 62.1 % ( Table 2 ) , much lower than our best model 's overall accuracy ( i.e. average across all tokens ) of 81.1 % ( Table 1 ) .", "entities": [[6, 7, "MetricName", "accuracy"], [14, 15, "MetricName", "accuracy"], [30, 32, "MetricName", "overall accuracy"]]}
{"text": "Average Speaker Accuracy 63.3 % Average Token Accuracy 62.1 %", "entities": [[2, 3, "MetricName", "Accuracy"], [7, 8, "MetricName", "Accuracy"]]}
{"text": "When testing the Heselwood et al approach ( Table 2 ) , it only predicted correctly approximately 60 % of the time ; our model performs significantly better , at an accuracy of 81.1 % ( Table 1 ) .", "entities": [[31, 32, "MetricName", "accuracy"]]}
{"text": "We aimed to reach human levels - considering that analyst agreement is 89.9 % for our dataset ( as mentioned above ) , our accuracy of 81.1 % is quite good .", "entities": [[24, 25, "MetricName", "accuracy"]]}
{"text": "Our best performing model , multi - phase fine - tuning of BioBERT with long answer bag - of - word statistics as additional supervision , achieves 68.1 % accuracy , compared to single human performance of 78.0 % accuracy and majority - baseline of 55.2 % accuracy , leaving much room for improvement .", "entities": [[29, 30, "MetricName", "accuracy"], [39, 40, "MetricName", "accuracy"], [47, 48, "MetricName", "accuracy"]]}
{"text": "We denote the original transformer weights of BioBERT as \u03b8 0 .", "entities": [[9, 10, "HyperparameterName", "\u03b8"], [10, 11, "DatasetName", "0"]]}
{"text": "( 4 In reasoning - free setting which we use for bootstrapping , the regularization coefficient \u03b2 is set to 0 because long answers are directly used as input .", "entities": [[16, 17, "HyperparameterName", "\u03b2"], [20, 21, "DatasetName", "0"]]}
{"text": "Thus , in Phase I of multi - phase fine - tuning , we initialize BioBERT with \u03b8 0 , and fine - tune it on PQA - A using question and context as input : \u03b8 I argmin \u03b8 L ( BioBERT \u03b8 ( q A , c A ) , l A ) ( 1 ) Phase II Fine - tuning on Bootstrapped PQA - U : To fully utilize the unlabeled instances in PQA - U , we exploit the easiness of reasoning - free setting to pseudo - label these instances with a bootstrapping strategy : first , we initialize BioBERT with \u03b8 0 , and fine - tune it on PQA - A using question and long answer ( reasoning - free ) , \u03b8 B 1 argmin \u03b8 L ( BioBERT \u03b8 ( q A , a A ) , l A ) ( 2 ) then we further fine - tune BioBERT \u03b8 B 1 on PQA - L , also under the reasoning - free setting : \u03b8 B 2 argmin \u03b8 L ( BioBERT \u03b8 ( q L , a L ) , l L )", "entities": [[17, 18, "HyperparameterName", "\u03b8"], [18, 19, "DatasetName", "0"], [36, 37, "HyperparameterName", "\u03b8"], [39, 40, "HyperparameterName", "\u03b8"], [43, 44, "HyperparameterName", "\u03b8"], [106, 107, "HyperparameterName", "\u03b8"], [107, 108, "DatasetName", "0"], [129, 130, "HyperparameterName", "\u03b8"], [133, 134, "HyperparameterName", "\u03b8"], [137, 138, "HyperparameterName", "\u03b8"], [159, 160, "HyperparameterName", "\u03b8"], [175, 176, "HyperparameterName", "\u03b8"], [179, 180, "HyperparameterName", "\u03b8"], [183, 184, "HyperparameterName", "\u03b8"]]}
{"text": "( 3 ) We pseudo - label PQA - U instances using the most confident predictions of BioBERT \u03b8 B 2 for each class .", "entities": [[18, 19, "HyperparameterName", "\u03b8"]]}
{"text": "Confidence is simply defined by the corresponding softmax probability and then we label a subset which has the same proportions of yes / no / maybe labels as those in the PQA - L : l U pseudo BioBERT \u03b8 B 2 ( q U , a U ) ( 4 )", "entities": [[7, 8, "MethodName", "softmax"], [39, 40, "HyperparameterName", "\u03b8"]]}
{"text": "In phase II , we fine - tune BioBERT \u03b8 I on the bootstrapped PQA - U using question and context ( under reasoning - required setting ) : \u03b8 II argmin \u03b8 L ( BioBERT \u03b8 ( q U , c U ) , l U pseudo ) ( 5 ) Final Phase Fine - tuning on PQA - L : In the final phase , we fine - tune BioBERT \u03b8 II on PQA - L : \u03b8 F argmin \u03b8 L ( BioBERT \u03b8 ( q L , c L ) , l L ) ( 6 ) Final predictions on instances of PQA - L validation and test sets are made using BioBERT \u03b8 F : l pred = BioBERT \u03b8 F ( q L , c L )", "entities": [[9, 10, "HyperparameterName", "\u03b8"], [29, 30, "HyperparameterName", "\u03b8"], [32, 33, "HyperparameterName", "\u03b8"], [36, 37, "HyperparameterName", "\u03b8"], [72, 73, "HyperparameterName", "\u03b8"], [79, 80, "HyperparameterName", "\u03b8"], [82, 83, "HyperparameterName", "\u03b8"], [86, 87, "HyperparameterName", "\u03b8"], [117, 118, "HyperparameterName", "\u03b8"], [124, 125, "HyperparameterName", "\u03b8"]]}
{"text": "Under reasoning - free setting where the annotator can see the conclusions , a single human achieves 90.4 % accuracy and 84.2 % macro - F1 .", "entities": [[19, 20, "MetricName", "accuracy"], [23, 26, "MetricName", "macro - F1"]]}
{"text": "Under reasoning - required setting , the task be - comes much harder , but it 's still possible for humans to solve : a single annotator can get 78.0 % accuracy and 72.2 % macro - F1 .", "entities": [[31, 32, "MetricName", "accuracy"], [35, 38, "MetricName", "macro - F1"]]}
{"text": "Comparison of Training Schedules : Multiphase fine - tuning setting gets 5 out of 9 modelwise best accuracy / macro - F1 .", "entities": [[17, 18, "MetricName", "accuracy"], [19, 22, "MetricName", "macro - F1"]]}
{"text": "Since PQA - A is imbalanced due to its collection process , a trivial majority baseline gets 92.76 % accuracy .", "entities": [[19, 20, "MetricName", "accuracy"]]}
{"text": "We report baseline classification performance on the binary classification task , achieving accuracy of 0.93 and F1 of 0.43 .", "entities": [[12, 13, "MetricName", "accuracy"], [16, 17, "MetricName", "F1"]]}
{"text": "For the level one binary task the Fleiss ' Kappa is 0.484 and the Krippendorf 's alpha is 0.487 .", "entities": [[16, 17, "HyperparameterName", "alpha"]]}
{"text": "Krippendorf 's alpha is similar to the 0.45 reported by Wulczyn et al ( 2017 ) .", "entities": [[2, 3, "HyperparameterName", "alpha"]]}
{"text": "The logistic classifier has the highest precision on misogynistic content ( 0.88 ) but very low recall ( 0.07 ) and a low F1 score ( 0.13 ) .", "entities": [[23, 25, "MetricName", "F1 score"]]}
{"text": "The weighted BERT model has the highest recall ( 0.50 ) and F1 score ( 0.43 ) .", "entities": [[2, 3, "MethodName", "BERT"], [12, 14, "MetricName", "F1 score"]]}
{"text": "Accuracy on all test cases , of which 91.9 % are non - misogynistic , is around 0.90 across models .", "entities": [[0, 1, "MetricName", "Accuracy"]]}
{"text": "This differs from other NLP tasks such as entity recognition , where training on data in the target domain increased the F1 score by over 20 points ( Bamman et al , 2019 ) .", "entities": [[21, 23, "MetricName", "F1 score"]]}
{"text": "Figure 1 shows the accuracy of the Wang et al ( 2017 ) parser on the inter - sentential and intrasentential relations in the RST , respectively .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "\u0398 m ( i ) , ( 1 ) where the \u0398 ( ) denotes the importance evaluation function and M denotes the number of language pairs .", "entities": [[0, 1, "HyperparameterName", "\u0398"], [11, 12, "HyperparameterName", "\u0398"]]}
{"text": "Taylor Expansion We adopt a criterion based on the Taylor Expansion ( Molchanov et al , 2017 ) , where we directly approximate the change in loss when removing a particular neuron .", "entities": [[26, 27, "MetricName", "loss"]]}
{"text": "i = 0 ) is the loss value if the neuron i is pruned and L ( H , h i ) is the loss if it is not pruned .", "entities": [[2, 3, "DatasetName", "0"], [6, 7, "MetricName", "loss"], [24, 25, "MetricName", "loss"]]}
{"text": "i , ( 5 ) where \u03b4 ( 0 , 1 ) .", "entities": [[6, 7, "HyperparameterName", "\u03b4"], [8, 9, "DatasetName", "0"]]}
{"text": "Considering the use of ReLU activation function ( Glorot et al , 2011 ) in the model , the first derivative of loss function tends to be constant , so the second order term tends to be zero in the end of training .", "entities": [[4, 5, "MethodName", "ReLU"], [5, 7, "HyperparameterName", "activation function"], [22, 23, "MetricName", "loss"]]}
{"text": "= k \u00d7 max ( \u0398 m ( i ) ) , m { 1 , . . .", "entities": [[5, 6, "HyperparameterName", "\u0398"]]}
{"text": ", M } , k [ 0 , 1 ] ( 9 ) , where max ( \u0398 m ( i ) ) denotes the maximum importance of this neuron in all language pairs and k is a hyper - parameter .", "entities": [[6, 7, "DatasetName", "0"], [17, 18, "HyperparameterName", "\u0398"]]}
{"text": "All the models were trained on 4 NVIDIA 2080Ti GPUs where each was allocated with a batch size of 4 , 096 tokens for one - to - many scenario and 2 , 048 tokens for the many - to - many scenario .", "entities": [[16, 18, "HyperparameterName", "batch size"]]}
{"text": "We train the baseline model using Adam optimizer ( Kingma and Ba , 2015 ) with \u03b2 1 = 0.9 , \u03b2 2 = 0.98 , and = 10 \u22129 .", "entities": [[6, 7, "MethodName", "Adam"], [7, 8, "HyperparameterName", "optimizer"], [16, 17, "HyperparameterName", "\u03b2"], [21, 22, "HyperparameterName", "\u03b2"]]}
{"text": "\u03b1 = 0.6 .", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "The final translation is detokenized and then the quality is evaluated using the 4 - gram case - sensitive BLEU ( Papineni et", "entities": [[19, 20, "MetricName", "BLEU"]]}
{"text": "al , 2002 ) with the SacreBLEU tool ( Post , 2018 ) .", "entities": [[6, 7, "MetricName", "SacreBLEU"]]}
{"text": "Ro It , and En Ro , whose BLEU scores are 0.67 , 1 , and 1.7 higher than the multilingual baseline , respectively .", "entities": [[8, 9, "MetricName", "BLEU"]]}
{"text": "When the importance of neurons for different languages is determined , the number of language pairs associated with each neuron can be adjusted ac - Figure 5 : \u2206 BLEU over best performance when erasing the general or language - specific neurons randomly on the many - to - many translation task .", "entities": [[29, 30, "MetricName", "BLEU"]]}
{"text": "When k = 1.0 , the threshold is max ( \u0398 m ( i ) ) as computed by Equation 9 , so the neurons will only be allocated to the language pair with the highest importance , and when k = 0 , the threshold is 0", "entities": [[1, 3, "HyperparameterName", "k ="], [10, 11, "HyperparameterName", "\u0398"], [40, 42, "HyperparameterName", "k ="], [42, 43, "DatasetName", "0"], [47, 48, "DatasetName", "0"]]}
{"text": "As shown in Figure 5 , when the general neurons are erased , the BLEU points of all the language pairs drop a lot ( about 15 to 20 BLEU ) , which indicates general neurons do capture the general knowledge across languages .", "entities": [[14, 15, "MetricName", "BLEU"], [29, 30, "MetricName", "BLEU"], [39, 41, "TaskName", "general knowledge"]]}
{"text": "I X X T I \uf8ee \uf8f0 \uf8f9 \uf8fb E\u1e90 Z Dirichlet ( \u03b1 ) GX L rec ( X , X ) MMD ( P Z , Q\u1e90 ) Figure 1 : The framework of GTM .", "entities": [[13, 14, "HyperparameterName", "\u03b1"], [23, 24, "DatasetName", "MMD"]]}
{"text": "Based on the Wasserstein Autoencoder ( Tolstikhin et al , 2017 ) framework , the training objective of GTM is to minimize the document reconstruction loss when the latent topic space is constrained by a prior distribution .", "entities": [[4, 5, "MethodName", "Autoencoder"], [25, 26, "MetricName", "loss"]]}
{"text": "\u2212 2 mn i , j k ( z ( i ) , \u1e91 ( j ) ) , ( 4 ) where m and n are the number of samples from Z and\u1e90 respectively ( m and n are batch sizes and they are equal in our experiments ) , and k : Z\u00d7Z R is the kernel function .", "entities": [[28, 31, "HyperparameterName", "number of samples"]]}
{"text": "We use the RMSProp ( Hinton et al , 2012 ) optimizer with a learning rate of 0.01 to train the model for 100 epochs .", "entities": [[3, 4, "MethodName", "RMSProp"], [11, 12, "HyperparameterName", "optimizer"], [14, 16, "HyperparameterName", "learning rate"]]}
{"text": "RAE ( Zhang et al , 2018 ) further introduces the likelihood that two attribute values co - participate in a common n - ary fact , and adds the corresponding relatedness loss multiplied by a weight factor to the embedding loss of m - TransH.", "entities": [[0, 1, "MethodName", "RAE"], [32, 33, "MetricName", "loss"], [41, 42, "MetricName", "loss"]]}
{"text": "By comparing s F ct with l F ct , we get the binary crossentropy loss : L F ct = \u2212l F ct logs F ct \u2212 ( 1\u2212l F ct ) log ( 1\u2212s F ct ) , ( 8 )", "entities": [[15, 16, "MetricName", "loss"]]}
{"text": "We then optimize NeuInfer via backpropagation , and Adam ( Kingma and Ba , 2015 ) with learning rate \u03bb is used as the optimizer .", "entities": [[8, 9, "MethodName", "Adam"], [17, 19, "HyperparameterName", "learning rate"], [24, 25, "HyperparameterName", "optimizer"]]}
{"text": "Therefore , we adopt MRR and Hits@ { 1 , 3 , 10 } on entity inference , while pouring attention to more finegrained metrics , i.e. , MRR and Hits@1 on relation inference .", "entities": [[4, 5, "MetricName", "MRR"], [28, 29, "MetricName", "MRR"], [30, 31, "MetricName", "Hits@1"]]}
{"text": "The hyper - parameters of NeuInfer are tuned via grid search in the following ranges : The embedding dimension k { 50 , 100 } , the batch size \u03b2 { 128 , 256 } , the learning rate \u03bb { 5e \u22126 , 1e \u22125 , 5e \u22125 , 1e \u22124 , 5e \u22124 , 1e \u22123 } , the numbers n 1 and n 2 of the neural network layers of \" hrt - FCNs \" and \" hrtav - FCNs \" in { 1 , 2 } , the dimension d of the interaction vector", "entities": [[17, 19, "HyperparameterName", "embedding dimension"], [27, 29, "HyperparameterName", "batch size"], [29, 30, "HyperparameterName", "\u03b2"], [37, 39, "HyperparameterName", "learning rate"]]}
{"text": "The adopted optimal settings are : k = 100 ,", "entities": [[6, 8, "HyperparameterName", "k ="]]}
{"text": "\u03b2 = 128 , \u03bb = 5e \u22125 , n 1 = 2 , n 2 = 1 , d = 1200 , and w = 0.1 for JF17 K ; k = 100 ,", "entities": [[0, 1, "HyperparameterName", "\u03b2"], [31, 33, "HyperparameterName", "k ="]]}
{"text": "\u03b2 = 128 , \u03bb", "entities": [[0, 1, "HyperparameterName", "\u03b2"]]}
{"text": "In essence , 0.151 on MRR , 14.6 % on Hits@1 , 16.2 % on Hits@3 , and 15.9 % on Hits@10 .", "entities": [[5, 6, "MetricName", "MRR"], [10, 11, "MetricName", "Hits@1"], [15, 16, "MetricName", "Hits@3"], [21, 22, "MetricName", "Hits@10"]]}
{"text": "NeuInfer improves the performance of Hits@3 even by 16.2 % on JF17K. In this paper , we use only n - ary facts in the datasets to conduct knowledge inference .", "entities": [[5, 6, "MetricName", "Hits@3"]]}
{"text": "For the dependency analysis , we use an extended version of UDPipeFuture ( Straka , 2018 ) which showed its state of the art performance by becoming first in terms of the Morphology - aware Labeled Attachment Score ( MLAS ) 3 metric at the CoNLL Shared Task of dependency parsing in 2018 ( Zeman et al , 2018 ) .", "entities": [[37, 38, "MetricName", "Score"], [49, 51, "TaskName", "dependency parsing"]]}
{"text": "We note that the highest human accuracy score for French of about 85 % was scored with the first architecture coupled with BERT as the generation model ( GM ) and CamemBERT as the language model ( LM ) .", "entities": [[6, 7, "MetricName", "accuracy"], [22, 23, "MethodName", "BERT"]]}
{"text": "According to table 6 which presents the Pearson correlation ( Benesty et al , 2009 ) of the human accuracy with the four metrics and to figure 2 which illustrates the ranking given by each evaluation metric along with the human judgement for each configuration ( i.e. configuration = GM \u00d7 architecture \u00d7 LM ) tested , we can clearly see that the human evaluation results are positively and strongly correlated with the BLEU , the METEOR and the BERT scores .", "entities": [[7, 9, "MetricName", "Pearson correlation"], [19, 20, "MetricName", "accuracy"], [73, 74, "MetricName", "BLEU"], [76, 77, "DatasetName", "METEOR"], [79, 80, "MethodName", "BERT"]]}
{"text": "Table 5 presents the results for the English dataset and shows that the best accuracy scored is about 72 % with A1 , BERT as the generative model and the Generative Pretrained Transformer ( GPT ) as the language model .", "entities": [[14, 15, "MetricName", "accuracy"], [23, 24, "MethodName", "BERT"], [32, 33, "MethodName", "Transformer"], [34, 35, "MethodName", "GPT"]]}
{"text": "The answers we annotated achieved a ROUGE - L of 23.19 , higher than that of human - written answers on the same set of questions ( 21.28 ) .", "entities": [[6, 9, "MetricName", "ROUGE - L"]]}
{"text": "The training batch size is set to 64 , with the initial learning rate as 5e \u2212 5 .", "entities": [[2, 4, "HyperparameterName", "batch size"], [12, 14, "HyperparameterName", "learning rate"]]}
{"text": "We train the model for 10 epochs and report the result of the checkpoint with best validation accuracy , averaged across three random seeds .", "entities": [[17, 18, "MetricName", "accuracy"], [23, 24, "DatasetName", "seeds"]]}
{"text": "We used the batch size of 16 , initial learning rate of 1e\u22124 with AdamW optimizer and a linear learning rate schedule .", "entities": [[3, 5, "HyperparameterName", "batch size"], [9, 11, "HyperparameterName", "learning rate"], [14, 15, "MethodName", "AdamW"], [15, 16, "HyperparameterName", "optimizer"], [19, 21, "HyperparameterName", "learning rate"]]}
{"text": "We train the model for 30 epochs and report the result of the checkpoint with the best validation accuracy , averaged across three random seeds .", "entities": [[18, 19, "MetricName", "accuracy"], [24, 25, "DatasetName", "seeds"]]}
{"text": "We fine - tuned the bert - base - uncased model for 3 epochs , with an initial learning rate of 5e \u2212 5 and batch size of 32 .", "entities": [[18, 20, "HyperparameterName", "learning rate"], [25, 27, "HyperparameterName", "batch size"]]}
{"text": "We use the model with the highest validation F1 as the question classifier , which achieves F1 of 0.97 and 0.94 on validation and test set respectively .", "entities": [[8, 9, "MetricName", "F1"], [16, 17, "MetricName", "F1"]]}
{"text": "+ ( 1 \u2212 \u03b1 ) f doc ( 11 ) where \u03b1 is a tradeoff parameter ( 0 \u2264 \u03b1 \u2264 1 ) .", "entities": [[4, 5, "HyperparameterName", "\u03b1"], [12, 13, "HyperparameterName", "\u03b1"], [18, 19, "DatasetName", "0"], [20, 21, "HyperparameterName", "\u03b1"]]}
{"text": "The learning rate is 0.3 for stochastic gradient descent optimizer .", "entities": [[1, 3, "HyperparameterName", "learning rate"], [6, 9, "MethodName", "stochastic gradient descent"], [9, 10, "HyperparameterName", "optimizer"]]}
{"text": "Unsupervised Sentiment Classification : We then report the unsupervised sentiment classification accuracy of five methods on the Semeval 2013 - 2016 datasets in Table 4 .", "entities": [[2, 3, "TaskName", "Classification"], [11, 12, "MetricName", "accuracy"], [17, 19, "DatasetName", "Semeval 2013"]]}
{"text": "Across four datasets , the average accuracy of HSSWE is 6.6 , 3.1 , 9.6 and 0.94 higher than Sentiment140 , HIT , NN and ET - SL , respectively .", "entities": [[5, 7, "MetricName", "average accuracy"], [19, 20, "DatasetName", "Sentiment140"]]}
{"text": "It equals to HSSWE when \u03b1 = 0 .", "entities": [[5, 6, "HyperparameterName", "\u03b1"], [7, 8, "DatasetName", "0"]]}
{"text": "When \u03b1 is 0 , HSSWE only benefits from the document - level sentiment supervision and when \u03b1 is 1 , HSSWE benefits from only word - level sentiment supervision .", "entities": [[1, 2, "HyperparameterName", "\u03b1"], [3, 4, "DatasetName", "0"], [17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "We observe that HSSWE performs better when \u03b1 is in the range of [ 0.45 , 0.55 ] .", "entities": [[7, 8, "HyperparameterName", "\u03b1"]]}
{"text": "This consistently outperforms all other baseline and competitive methods on all three experimental setups , with differences ranging between +1.95 to +3.11 average F1 across multiple genres when compared to standard approaches .", "entities": [[22, 24, "MetricName", "average F1"]]}
{"text": "( 1 ) where L N ER S and L N ER P stand for the shared layer loss and private layer loss respectively .", "entities": [[18, 19, "MetricName", "loss"], [22, 23, "MetricName", "loss"]]}
{"text": "Our training uses 256 hidden states for BiLSTM with mini - batch size of 32 .", "entities": [[7, 8, "MethodName", "BiLSTM"], [9, 13, "HyperparameterName", "mini - batch size"]]}
{"text": "The model parameters are updated using back - propagation and Adam optimizer ( Kingma and Ba , 2014 ) .", "entities": [[10, 11, "MethodName", "Adam"], [11, 12, "HyperparameterName", "optimizer"]]}
{"text": "Our model - MultDomain - SP - Aux ( S ) - gains the best overall performance in this setup , with 1.95 macro - average F1 increase over the next best method ( InDomain+DomainClassifier ) .", "entities": [[25, 27, "MetricName", "average F1"]]}
{"text": "Results show that our proposed method obtains again the best results , with a consistent margin of 2.24 macro - average F1 improvement over the next method .", "entities": [[20, 22, "MetricName", "average F1"]]}
{"text": "The in - domain models perform 5.21 F1 points lower than our approach , the largest gap in all experimental setups , highlighting the robustness of the multi - domain modeling approach .", "entities": [[7, 8, "MetricName", "F1"]]}
{"text": "The results show that even though our model improves substantially over the in - domain models , an oracle selection method would push performance much higher ( +6.73 F1 on the open data ) .", "entities": [[28, 29, "MetricName", "F1"]]}
{"text": "In the zero - shot genres , the Twitter model performs close to the MultDomain - SP - Aux model ( - 0.56 F1 ) , albeit it is 24 F1 lower on the multi - domain setup .", "entities": [[23, 24, "MetricName", "F1"], [30, 31, "MetricName", "F1"]]}
{"text": "In test phase , we set the batch size as 128 .", "entities": [[7, 9, "HyperparameterName", "batch size"]]}
{"text": "Building on past research , we proposed a new neural architecture that achieves substantial improvements of up to 5 F1 points when compared to standard methods .", "entities": [[19, 20, "MetricName", "F1"]]}
{"text": "Our best model gives the F1 - score of 51 % that is very encouraging , considering the extremely distinct natures of these two languages .", "entities": [[5, 8, "MetricName", "F1 - score"]]}
{"text": "3 . Let E = { e 1 , . . . , e n } and K = { k 1 , . . .", "entities": [[17, 19, "HyperparameterName", "K ="]]}
{"text": "The hyperparameters we experiment on are the combitations of batch size { 16 , 32 } , learning rate { 2e - 5 , 3e - 5 , 5e - 5 } , and number of max epochs { 3 , 4 } .", "entities": [[9, 11, "HyperparameterName", "batch size"], [17, 19, "HyperparameterName", "learning rate"]]}
{"text": "The following 5 hyperparameters are tuned to filter out noisy annotation for training , where \u03c8 , \u03b1 , and \u03b3 are newly introduced by our work : \u03c8 : if True , keep only sentences whose entities are completely matching between the two languages .", "entities": [[17, 18, "HyperparameterName", "\u03b1"], [20, 21, "HyperparameterName", "\u03b3"]]}
{"text": "F1 score improves 2.47 % ( 36.67 % to 39.14 % ) and 2.32 % ( 39.36 % to 41.68 % ) for mBERT and XLM - R , respectively .", "entities": [[0, 2, "MetricName", "F1 score"], [23, 24, "MethodName", "mBERT"], [25, 26, "MethodName", "XLM"]]}
{"text": "Adam with default parameters and a learning rate of 0.0001 are used for optimization .", "entities": [[0, 1, "MethodName", "Adam"], [6, 8, "HyperparameterName", "learning rate"]]}
{"text": "We trained the model for 10 epoch with a batch size of 32 , and evaluate the model per a epoch .", "entities": [[9, 11, "HyperparameterName", "batch size"]]}
{"text": "Much recent work on large - scale AND uses a heuristic that is the initial match of first name and exact match of last name ( Torvik and Smalheiser , 2009 ; Liu et al , 2014 ; Levin et al , 2012 ; Kim et al , 2016 ) .", "entities": [[20, 22, "MetricName", "exact match"]]}
{"text": "* = argmin f ( rx , ry ) R f ( r x , r y ) such that \u2265 ( 1 \u2212 \u03b5 )", "entities": [[24, 25, "HyperparameterName", "\u03b5"]]}
{"text": "Then , as in DNF blocking , the sequential covering algorithm is used to learn the negated DNF formula ( line 26 - 37 ) , which iteratively adds a negated conjunction term until it covers the desired number of samples .", "entities": [[38, 41, "HyperparameterName", "number of samples"]]}
{"text": "Also , note that the termination condition of the loop ( line 26 ) is when \u03b5 of total positive samples are covered with the learned N egDN F .", "entities": [[16, 17, "HyperparameterName", "\u03b5"]]}
{"text": "The first function learns a blocking function with only conjunctions based on our CNF blocking method using k = 1 and a limited set of predicates with nonrelative similarity measures .", "entities": [[17, 19, "HyperparameterName", "k ="]]}
{"text": "We use k = 3 for further experiments .", "entities": [[2, 4, "HyperparameterName", "k ="]]}
{"text": "( 3 ) While the unsupervised objective is trained by maximizing the following variational lower bound U ( x ) on the objective for unlabeled data : log p \u03b8 ( x ) \u2265 E ( y , z )", "entities": [[29, 30, "HyperparameterName", "\u03b8"]]}
{"text": "( 6 ) The weight \u03b1 controls the relative weight between the loss from unlabeled data and labeled data .", "entities": [[5, 6, "HyperparameterName", "\u03b1"], [12, 13, "MetricName", "loss"]]}
{"text": "For character drop - out at the decoder , we empirically set \u03b2 to be 0.4 for all languages .", "entities": [[12, 13, "HyperparameterName", "\u03b2"]]}
{"text": "We set \u03b1 the weight for the unsupervised loss to be 0.8 .", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [8, 9, "MetricName", "loss"]]}
{"text": "We obtain a generation accuracy above 80 % over more than 25 % languages and an average of 87.2 % for both dev and test data .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "The generation accuracy is almost consistent on the dev and test data except that the test data accuracy of Scottish - Gaelic drops by near 21 % .", "entities": [[2, 3, "MetricName", "accuracy"], [17, 18, "MetricName", "accuracy"]]}
{"text": "Here T denotes the source length , \u03b1 ( 0.0 , 1.0 ) is set based on the data statistics and [ ] is the integer rounding operation .", "entities": [[7, 8, "HyperparameterName", "\u03b1"]]}
{"text": "( 10 ) When \u03b1 = 1.0 , ratio - first degenerates to the standard decoding strategy in CRF - based models .", "entities": [[4, 5, "HyperparameterName", "\u03b1"], [18, 19, "MethodName", "CRF"]]}
{"text": "j = y i log ( 1.0 \u2212 p \u03b8 ( y j | h i ; X ) ) , ( 11 ) where h i is the model output state at position i.", "entities": [[9, 10, "HyperparameterName", "\u03b8"]]}
{"text": "= \u2212 log P CRF ( Y | X ) , L = L CRF + \u03bb L CA , ( 12 ) where \u03bb controls the importance of different loss terms and P CRF ( Y | X ) is described in Equation ( 8 ) .", "entities": [[4, 5, "MethodName", "CRF"], [14, 15, "MethodName", "CRF"], [30, 31, "MetricName", "loss"], [34, 35, "MethodName", "CRF"]]}
{"text": "In training , we use Adam optimizer ( Kingma and Ba , 2015 ) .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "optimizer"]]}
{"text": "For evaluation , standard metrics including ROUGE - 1 ( R - 1 ) , ROUGE - 2 ( R - 2 ) and ROUGE - L ( R - L ) ( Lin , 2004 ) are reported .", "entities": [[24, 27, "MetricName", "ROUGE - L"]]}
{"text": "From the results , we observe a moderate performance drop when using ratio - first ( \u03b1 = 0.3 ) .", "entities": [[16, 17, "HyperparameterName", "\u03b1"]]}
{"text": "With \u03b1 = 0.3 , our model achieves the highest inference speedup while still outperforms all compared NAG models .", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "In addition , We also report the results of other standard metrics including ROUGE - 1 , ROUGE - 2 and ROUGE - L. Models F1", "entities": [[25, 26, "MetricName", "F1"]]}
{"text": "By setting \u03b1 as 0.7 , it achieves a 10.00\u00d7 inference speedup while still outperforming other compared NAG baselines .", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "For evaluation , we report results in BLEU scores ( Papineni et al , 2002 ) .", "entities": [[7, 8, "MetricName", "BLEU"]]}
{"text": "By setting \u03b1 as 0.8 , the inference speedup can be further boosted to 13.92\u00d7 while the generation quality is still higher than the best NAG baseline .", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "The experimental results with different \u03b1 are presented in Figure 3 .", "entities": [[5, 6, "HyperparameterName", "\u03b1"]]}
{"text": "It can be observed that , when \u03b1 reaches 0.3 , the model approximately achieves its optimal performance .", "entities": [[7, 8, "HyperparameterName", "\u03b1"]]}
{"text": "Now we illustrate why the near optimal performance can be achieved when \u03b1 reaches 0.3 .", "entities": [[12, 13, "HyperparameterName", "\u03b1"]]}
{"text": "Recall the definition of ratio - first decoding in Equation ( 10 ) , the [ \u03b1 T ] constrains the maximum length of the generated result .", "entities": [[0, 1, "MetricName", "Recall"], [16, 17, "HyperparameterName", "\u03b1"]]}
{"text": "In this case , a proper \u03b1 could be 0.3 which is demonstrated by the results in Figure 3 and 4 .", "entities": [[6, 7, "HyperparameterName", "\u03b1"]]}
{"text": "For both edit intention identification models , we fine - tuned the RoBERTa - large pre - trained checkpoint from Hugging - Face ( Wolf et al , 2020 ) for 2 epochs with a learning rate of 1 \u00d7 10 \u22125 and batch size of 16 .", "entities": [[12, 13, "MethodName", "RoBERTa"], [35, 37, "HyperparameterName", "learning rate"], [43, 45, "HyperparameterName", "batch size"]]}
{"text": "We fine - tuned the model for 5 epochs with a learning rate of 3 \u00d7 10 \u22125 and batch size of 4 .", "entities": [[11, 13, "HyperparameterName", "learning rate"], [19, 21, "HyperparameterName", "batch size"]]}
{"text": "Finally , our text revision generation model achieves 41.78 SARI score ( Xu et al , 2016 ) , 81.11 BLEU score ( Papineni et al , 2002 ) and 89.08 ROUGE - L score ( Lin , 2004 ) on the test set .", "entities": [[20, 22, "MetricName", "BLEU score"], [31, 34, "MetricName", "ROUGE - L"]]}
{"text": "The output layer is a softmax with l2 regularization set at 0.029 .", "entities": [[5, 6, "MethodName", "softmax"], [7, 9, "HyperparameterName", "l2 regularization"]]}
{"text": "Solving an ODE requires one to specify an initial value h ( 0 ) ( input x or its transformation ) and can compute the value at t using an ODE solver ODESolverCompute ( f \u03b8 , h ( 0 ) , 0 , t ) .", "entities": [[12, 13, "DatasetName", "0"], [35, 36, "HyperparameterName", "\u03b8"], [39, 40, "DatasetName", "0"], [42, 43, "DatasetName", "0"]]}
{"text": "For classification problems , cross - entropy loss is used and parameters are learnt through adjoint sensitivity method ( Zhuang et al , 2020 ; Chen et al , 2018 ) which provides efficient back - propagation and gradient computations .", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "Hyperparameters : All the models are trained for 50 epochs with 0.01 learning rate , Adam optimizer , dropout ( 0.2 ) regularizer , batchsize of 50 , hidden representation size of 64 and cross entropy as the loss function .", "entities": [[12, 14, "HyperparameterName", "learning rate"], [15, 16, "MethodName", "Adam"], [16, 17, "HyperparameterName", "optimizer"], [38, 39, "MetricName", "loss"]]}
{"text": "We can observe that AUC for Figures 3 ( a ) and 3 ( e ) corresponding to RNODE and Bi - RNODE respectively are higher than LSTM , GRU , Bi - LSTM , and Bi - GRU .", "entities": [[4, 5, "MetricName", "AUC"], [27, 28, "MethodName", "LSTM"], [29, 30, "MethodName", "GRU"], [33, 34, "MethodName", "LSTM"], [38, 39, "MethodName", "GRU"]]}
{"text": "Unfortunately , the parsing accuracy of existing works including recent state - of - the - arts ( Zhang et al , 2019a , b ) remain unsatisfactory compared to human - level performance , 1 especially in cases where the sentences are rather long and informative , which indicates substantial room for improvement .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "It achieves the best - reported SMATCH scores ( F1 ) : 80.2 % on LDC2017T10 and 75.4 % on LDC2014T12 , surpassing the previous state - of - the - art models by large margins .", "entities": [[9, 10, "MetricName", "F1"], [15, 16, "DatasetName", "LDC2017T10"]]}
{"text": "= ( W V h 1 : n ) \u03b1 t + y t ( 1 ) P ( vocab ) = softmax ( W ( vocab ) MLP ( \u03b1 t )", "entities": [[9, 10, "HyperparameterName", "\u03b1"], [22, 23, "MethodName", "softmax"], [28, 29, "DatasetName", "MLP"], [30, 31, "HyperparameterName", "\u03b1"]]}
{"text": "Second , the attention weights \u03b1 t directly serve as a copy mechanism ( Gu et al , 2016 ; See et al , 2017 ) , i , e. , the probabilities of copying a token lemma from the input text as a node label .", "entities": [[5, 6, "HyperparameterName", "\u03b1"], [37, 38, "DatasetName", "lemma"]]}
{"text": "MLP ( \u03b1 t ) ) , where MLP is the same as in Eq . 1 , and p 0 , p 1 and p 2 are the probabilities of three prediction channels respectively .", "entities": [[0, 1, "DatasetName", "MLP"], [2, 3, "HyperparameterName", "\u03b1"], [8, 9, "DatasetName", "MLP"], [20, 21, "DatasetName", "0"]]}
{"text": "= p 0 P ( vocab ) ( c ) + p 1 ( i L ( c ) \u03b1 t [ i ] )", "entities": [[2, 3, "DatasetName", "0"], [19, 20, "HyperparameterName", "\u03b1"]]}
{"text": "+ p 2 ( i T ( c ) \u03b1 t [ i ] ) , where [ i ] indexes the i - th element and L ( c ) and T ( c ) are index sets of lemmas and tokens respectively that have the surface form as c.", "entities": [[9, 10, "HyperparameterName", "\u03b1"]]}
{"text": "( x t + ( W V h 1 : n ) \u03b1 t ) , where FFN ( y ) is a feed - forward network and W V projects text memories into a value space .", "entities": [[12, 13, "HyperparameterName", "\u03b1"]]}
{"text": "Parameter optimization is performed with the ADAM optimizer ( Kingma and Ba , 2014 ) with \u03b2 1 = 0.9 and \u03b2 2 = 0.999 .", "entities": [[6, 7, "DatasetName", "ADAM"], [7, 8, "HyperparameterName", "optimizer"], [16, 17, "HyperparameterName", "\u03b2"], [21, 22, "HyperparameterName", "\u03b2"]]}
{"text": "Our main findings were that the DNN - ISL learner achieved high accuracy on the Catalan data , with MGL coming in a close second , while ISLFLA and OSTIA performed much worse - either failing to learn any mapping at all or predicting the correct output for less than 5 % of held - out cases , even when lexical exceptions were removed from the data ( see Table 1 ) .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "The model is fully differentiable and was trained with the Adagrad optimizer on small mini - batches for 20 epochs .", "entities": [[10, 11, "MethodName", "Adagrad"], [11, 12, "HyperparameterName", "optimizer"]]}
{"text": "Evaluation Metrics : We convert all three datasets into the CoNLL 2012 format and report the F1 score for MUC , B 3 , and CEAF metrics using the CoNLL - 2012 official scripts .", "entities": [[10, 12, "DatasetName", "CoNLL 2012"], [16, 18, "MetricName", "F1 score"]]}
{"text": "We find that entity - type information gives a boost of 0.96 Avg . F1 ( p < 0.01 ) on LitBank which is the new state - of - the - art score with goldmentions .", "entities": [[14, 15, "MetricName", "F1"], [21, 22, "DatasetName", "LitBank"]]}
{"text": "Similarly , type information also benefits Email - Coref and WikiCoref resulting in an absolute improvement of 1.67 and 2.9 Avg . F1 points respectively ( p < 0.01 ) .", "entities": [[10, 11, "DatasetName", "WikiCoref"], [22, 23, "MetricName", "F1"]]}
{"text": "We also see a 2.4 Avg . F1 improvement ( p < 0.01 ) on OntoNotes , the largest dataset in this study .", "entities": [[7, 8, "MetricName", "F1"], [15, 16, "DatasetName", "OntoNotes"]]}
{"text": "As shown in Table 2 , the models that score lower on the impurity measure get a higher Avg F1 .", "entities": [[18, 20, "MetricName", "Avg F1"]]}
{"text": "The model is trained for 20 epochs , with earlystopping ( patience = 10 ) , and is fine - tuned on the development set for Macro F1 to give more importance to minority type categories .", "entities": [[26, 28, "MetricName", "Macro F1"]]}
{"text": "WikiCoref , however , proves more challenging as the model only manages 38.0 Macro F1 points with original ( orig ) types and 45.0 with common types ( com ) , portraying its lack of ability to learn minority type categories with less data .", "entities": [[0, 1, "DatasetName", "WikiCoref"], [13, 15, "MetricName", "Macro F1"]]}
{"text": "\u03b1 ij h j ( 2 ) \u03b1 ij = exp ( f ( h j , h t i ) )", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [7, 8, "HyperparameterName", "\u03b1"]]}
{"text": "= tanh ( h T j W m h t i + b m ) ( 4 ) where \u03b1 ij indicates the attention weights from the word h t i in the aspect term to the j - th word in the inputs , and tanh is a non - liner activation function .", "entities": [[19, 20, "HyperparameterName", "\u03b1"], [52, 54, "HyperparameterName", "activation function"]]}
{"text": "h i ( 8 ) where \u03b3 i stands for the attention weights from inputs to the words in aspect term , denoting which word in aspect term should be more focused .", "entities": [[6, 7, "HyperparameterName", "\u03b3"]]}
{"text": "+ 1 2 \u03bb \u03b8 2 ( 11 ) where \u03bb is the regularization factor and \u03b8 contains all the parameters .", "entities": [[4, 5, "HyperparameterName", "\u03b8"], [16, 17, "HyperparameterName", "\u03b8"]]}
{"text": "We use Tensorflow ( Abadi et al , 2016 ) to implement our proposed model and employ the Momentum as the training method , whose momentum parameter \u03b3 is set to 0.9 , \u03bb is set to 10 \u22126 , and the initial learning rate is set to 0.01 .", "entities": [[27, 28, "HyperparameterName", "\u03b3"], [43, 45, "HyperparameterName", "learning rate"]]}
{"text": "= \u2212 l log p ( r l | T l ) , ( 13 ) where \u03b8 is all parameters in the framework .", "entities": [[17, 18, "HyperparameterName", "\u03b8"]]}
{"text": "In training , we optimize the following loss functions instead of Eq . 15 , min \u03b8 C E L E adv ( \u03b8 C E )", "entities": [[7, 8, "MetricName", "loss"], [16, 17, "HyperparameterName", "\u03b8"], [23, 24, "HyperparameterName", "\u03b8"]]}
{"text": "j , ( 16 ) where \u03b8 C E and \u03b8 D are all parameters of the consistent sentence encoders and the discriminator .", "entities": [[6, 7, "HyperparameterName", "\u03b8"], [10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "During training process , we combine the extraction and adversarial objective functions as follows , L = Lnre ( \u03b8 ) + \u03bb1L D adv ( \u03b8 D ) + \u03bb2L E adv ( \u03b8 C E ) + \u03bb3L penalty ( \u03b8 E ) , ( 18 ) where \u03bb 1 , \u03bb 2 , and \u03bb 3 are harmonic factors .", "entities": [[19, 20, "HyperparameterName", "\u03b8"], [26, 27, "HyperparameterName", "\u03b8"], [34, 35, "HyperparameterName", "\u03b8"], [42, 43, "HyperparameterName", "\u03b8"]]}
{"text": "In practice , we integrate \u03bb 1 and \u03bb 2 into the alternating ratio among the loss functions , and we calibrate a 1:1:5 ratio among L nre ( \u03b8 )", "entities": [[16, 17, "MetricName", "loss"], [29, 30, "HyperparameterName", "\u03b8"]]}
{"text": "\u03bb 3 L penalty ( \u03b8 E ) , L D adv ( \u03b8 D ) and L E adv ( \u03b8 C E ) .", "entities": [[5, 6, "HyperparameterName", "\u03b8"], [13, 14, "HyperparameterName", "\u03b8"], [21, 22, "HyperparameterName", "\u03b8"]]}
{"text": "The results of precision - recall curves are shown in Figure 2 and the results of AUC are shown in Table 3 .", "entities": [[16, 17, "MetricName", "AUC"]]}
{"text": "AMNRE also outperforms MNRE with 3 percentage points increasing in the AUC results .", "entities": [[11, 12, "MetricName", "AUC"]]}
{"text": "To show the results clearly , we report the precision - recall curves in Figure 3 and the AUC results in Table 4 . From the results , we can observe that : ( 1 ) As compared with the models directly learned with the mono - lingual data , the models exploiting the multi - lingual information perform better in the mono - lingual scenario .", "entities": [[18, 19, "MetricName", "AUC"]]}
{"text": "( 2 ) Our proposed models achieve the best precision over the entire range of recall and also significantly improve the AUC results as compared with both MNRE and mono - lingual RE models .", "entities": [[21, 22, "MetricName", "AUC"]]}
{"text": "The AUC results are shown in Table 5 .", "entities": [[1, 2, "MetricName", "AUC"]]}
{"text": "Out of 211 S i m - H int pairs that are marked as having lexical antonym strategy ( dev set ) , 12 instances are identified by only the dependency parses , 67 instances by the word - alignments , and 100 instances by both ( P / R / F1 scores are 92.1 % , 77.7 % and 84.3 % ) , respectively on dev dataset .", "entities": [[51, 52, "MetricName", "F1"]]}
{"text": "This improves the final recall and on the dev set we achieve 89.0 % precision , 95.7 % recall , and 92.2 % F1 on dev dataset ( Lex ant Strategy ; Table 3 show results both on dev and the test sets ) .", "entities": [[23, 24, "MetricName", "F1"]]}
{"text": "We evaluate the training model on the dev data and the P / R / F1 are 53.2 % , 65.4 % , and 58.6 % , respectively ( in future work we plan to develop more accurate models for RQ detection ) .", "entities": [[15, 16, "MetricName", "F1"]]}
{"text": "The performance of the models is similar on both test and SIGN test sets , showing consistently good performance ( Table 3 ; 90 % F1 for all strategies , except the AntPhrase+PragInf and AN I!D ) .", "entities": [[25, 26, "MetricName", "F1"]]}
{"text": "We aggregate the messages using edge - aware attention following ( Liao et al , 2019 ) : 5 \u03b1 i , j = \u03c3 ( MLP ( e i \u2212 e j ) ) , where \u03c3 is the sigmoid function , and MLP contains two hidden layers with ReLU nonlinearities .", "entities": [[19, 20, "HyperparameterName", "\u03b1"], [26, 27, "DatasetName", "MLP"], [44, 45, "DatasetName", "MLP"], [50, 51, "MethodName", "ReLU"]]}
{"text": "= b \u03b3 b \u03b8 b , i , l , \u03b3 1 , , \u03b3 B = Softmax i , l MLP ( e i \u2212 e l ) , \u03b8 1 , i , l , , \u03b8 B , i , l = \u03c3 ( MLP \u03b8 ( e i \u2212 e l ) ) , where B is the number of mixture components .", "entities": [[2, 3, "HyperparameterName", "\u03b3"], [4, 5, "HyperparameterName", "\u03b8"], [11, 12, "HyperparameterName", "\u03b3"], [15, 16, "HyperparameterName", "\u03b3"], [18, 19, "MethodName", "Softmax"], [22, 23, "DatasetName", "MLP"], [31, 32, "HyperparameterName", "\u03b8"], [39, 40, "HyperparameterName", "\u03b8"], [48, 49, "DatasetName", "MLP"], [49, 50, "HyperparameterName", "\u03b8"]]}
{"text": "We train the model by optimizing the negative loglikelihood loss , L = G G train \u2212 log 2 p ( G ) .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "To evaluate our temporal event graph model , we compute the instance graph perplexity by predicting the instance graphs in the test set , PP = 2 \u2212 1 | G test | G G test log 2 p ( G ) .", "entities": [[13, 14, "MetricName", "perplexity"]]}
{"text": "( 1 ) We calculate the full perplexity for the entire graph using Equation ( 1 ) , and event perplexity using only event nodes , emphasizing the importance of correctly predicting events .", "entities": [[7, 8, "MetricName", "perplexity"], [20, 21, "MetricName", "perplexity"]]}
{"text": "The learning rate is 1e - 4 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "In detail , we first linearize the graph using topological sort , and then train XLNet 9 using the dimension of 128 ( the same as our temporal event graph model ) , and the number of layers is 3 .", "entities": [[15, 16, "MethodName", "XLNet"], [35, 38, "HyperparameterName", "number of layers"]]}
{"text": "The learning rate is 1e - 4 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "Also , the human schema fails to predict INJURE and ATTACK , because it relies on the exact match of event sequences of lengths l \u2265 2 , and can not handle the variants of sequences .", "entities": [[17, 19, "MetricName", "exact match"]]}
{"text": "This is especially apparent from the instance graph perplexity in Table 3 .", "entities": [[8, 9, "MetricName", "perplexity"]]}
{"text": "Unlike metrics for evaluating the quality of generated text , where one can measure correlation between a metric ( such as BLEU ( Papineni et al , 2002 ) ) and human judgement ( Zhang et al , 2019a ; Sagarkar et al , 2018 ) , it is unknown if hu - mans can reliably estimate diversity .", "entities": [[21, 22, "MetricName", "BLEU"]]}
{"text": "This property makes perplexity popular in LM - based NLG models , and often it is the only reported measure for diversity ( Lewis et al , 2017 ;", "entities": [[3, 4, "MetricName", "perplexity"]]}
{"text": "While one can approximate perplexity for such models ( Tevet et al , 2019 ) , ideally , a metric should not be tied to a model .", "entities": [[4, 5, "MetricName", "perplexity"]]}
{"text": "Self - BLEU ( Zhu et al , 2018 ; Shu et al , 2019 ) measures the BLEU score of a generated sentence with respect to another generated sentence ( rather than a gold reference ) .", "entities": [[2, 3, "MetricName", "BLEU"], [18, 20, "MetricName", "BLEU score"]]}
{"text": "3 BERT - Score ( Zhang et al , 2019a ) ; Originally a quality metric , BERT - Score uses BERT 's embeddings to measure similarity between two sen - tences .", "entities": [[1, 2, "MethodName", "BERT"], [3, 4, "MetricName", "Score"], [17, 18, "MethodName", "BERT"], [19, 20, "MetricName", "Score"], [21, 22, "MethodName", "BERT"]]}
{"text": "Standard deviation is up to 0.02 for all automatic metrics for both Spearman 's correlation and accuracy .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "Response set ( k = 3 ) Response set ( k = 32 ) Response set ( k = 318 ) Loud Noise .", "entities": [[3, 5, "HyperparameterName", "k ="], [10, 12, "HyperparameterName", "k ="], [17, 19, "HyperparameterName", "k ="]]}
{"text": "Response set ( k = 3 ) Response set ( k = 32 ) Response set ( k = 318 ) watching curry play in his prime is truly a privilege i know i just do nt want him to play for us he has to be a good center for that he is a great center of football in his prime he s been playing in his prime for a long time he was a great back in the day he s been playing for a while now i do nt know about that he was pretty damn good at that i do nt think he was ever in his prime i do nt think he is a prime minister i do nt know why", "entities": [[3, 5, "HyperparameterName", "k ="], [10, 12, "HyperparameterName", "k ="], [17, 19, "HyperparameterName", "k ="]]}
{"text": "We find that , despite GQA 's compositionality and carefully balanced label distribution , two strong models drop 13 - 17 % in accuracy on our automatically - constructed contrast set compared to the original validation set .", "entities": [[5, 6, "DatasetName", "GQA"], [23, 24, "MetricName", "accuracy"]]}
{"text": "Our results ( Table 4 ) show that sampling more objects leads to similar accuracy levels for the LXMERT model , indicating that quality of our contrast sets does not depend on the specific selection of replacements .", "entities": [[14, 15, "MetricName", "accuracy"], [18, 19, "MethodName", "LXMERT"]]}
{"text": "The original BART only achieves 54.80 % accuracy . model must accurately estimate relative quality of different generated outputs , since effective inference requires comparison among these candidates .", "entities": [[2, 3, "MethodName", "BART"], [7, 8, "MetricName", "accuracy"]]}
{"text": "As Tab . 1 shows , the accuracy is far from ideal .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "We introduce a training paradigm which requires the abstractive model to be able to be accurate with respect to predicting the tokens in the reference summaries and coordinated with respect to Figure 1 : Comparison of MLE loss ( LMLE ) and the contrastive loss ( LCtr ) in our method .", "entities": [[37, 38, "MetricName", "loss"], [44, 45, "MetricName", "loss"]]}
{"text": "The generation model is trained using the standard MLE loss , but to train the evaluation model we introduce a contrastive loss ( Hadsell et al , 2006 ) defined over different candidate summaries generated by pre - trained abstractive models ( Fig . 1 ) , following previous work on ranking - based or contrastive learning ( Hopkins and May , 2011 ; Zhong et al , 2020 ; Liu et al , 2021b ) .", "entities": [[9, 10, "MetricName", "loss"], [21, 22, "MetricName", "loss"], [55, 57, "MethodName", "contrastive learning"]]}
{"text": "D ( i ) ; \u03b8 ) ( 2 ) where \u03b8 denotes the parameters of g and p g \u03b8 denotes the probability distribution entailed by these parameters .", "entities": [[5, 6, "HyperparameterName", "\u03b8"], [11, 12, "HyperparameterName", "\u03b8"], [20, 21, "HyperparameterName", "\u03b8"]]}
{"text": "( i ) } , Eq . 2 is equivalent to minimizing the sum of negative loglikelihoods of the tokens { s * 1 , , s * j , , s * l } in the reference summary S * whose length is l , which is the cross - entropy loss :", "entities": [[52, 53, "MetricName", "loss"]]}
{"text": "* < j ; \u03b8 ) ( 3 ) where S *", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "In practice , label smoothing ( Szegedy et al , 2016 ) is a widely used and effective technique that modifies the target distribution in Eq . 4 to a \" soft \" label by assigning probability mass \u03b2 to other tokens : ptrue ( s | D , S", "entities": [[3, 5, "MethodName", "label smoothing"], [38, 39, "HyperparameterName", "\u03b2"]]}
{"text": "\u03b2 s = s * j \u03b2 N \u22121 s = s * j ( 5 ) where N is the size of the dictionary .", "entities": [[0, 1, "HyperparameterName", "\u03b2"], [6, 7, "HyperparameterName", "\u03b2"]]}
{"text": "One important step in search is estimating the probability of the next word s t given the previous predicted sequence S < t : p g \u03b8 ( s t | D , S < t ; \u03b8 ) ( 6 )", "entities": [[26, 27, "HyperparameterName", "\u03b8"], [37, 38, "HyperparameterName", "\u03b8"]]}
{"text": "In order to achieve this objective , we slightly modify the conditions of Eq . 5 , maintaining the general functional form , but instead specifying the marginal probability of the non - reference candidates S to be \u03b2 , and encouraging coordination of probabilities and qualities among non - reference candidates as follows : \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 p true \u2020", "entities": [[38, 39, "HyperparameterName", "\u03b2"]]}
{"text": "= 1 \u2212 \u03b2 S = S * S S", "entities": [[3, 4, "HyperparameterName", "\u03b2"]]}
{"text": "To coordinate a pre - trained abstractive model , we 1 ) use it to generate different candidate summaries with various levels of quality , 2 then 2 ) encourage the model to assign higher estimated probabilities to better candidates by fine - tuning the model with a contrastive loss , following the previous work ( Hopkins and May , 2011 ;", "entities": [[49, 50, "MetricName", "loss"]]}
{"text": "\u03b1 ( 9 ) where \u03b1 is the length penalty hyperparameter .", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [5, 6, "HyperparameterName", "\u03b1"]]}
{"text": "L mul = L xent + \u03b3L ctr ( 10 ) where \u03b3 is the weight of the contrastive loss .", "entities": [[12, 13, "HyperparameterName", "\u03b3"], [19, 20, "MetricName", "loss"]]}
{"text": "Many margin - based losses used in modern seq2seq models ( Wiseman and Rush , 2016 ; Edunov et al , 2018 ) assume a deterministic ( one - point ) distribution : a model can achieve zero loss if it can assign a much higher probability to the ( pseudo ) - reference , regardless of relative comparisons of other candidate summaries .", "entities": [[8, 9, "MethodName", "seq2seq"], [38, 39, "MetricName", "loss"]]}
{"text": "We label our proposed methods BRIO , with two variants : ( 1 ) BRIO - Ctr is fine - tuned with the contrastive loss ( Eq . 8 ) only ; ( 2 ) BRIO - Mul is fine - tuned with the multi - task loss ( Eq . 10 ) .", "entities": [[24, 25, "MetricName", "loss"], [47, 48, "MetricName", "loss"]]}
{"text": "Table 3 : Model performance with different \u03b3 coefficients weighting the contrastive loss ( Eq . 10 ) on CNNDM .", "entities": [[7, 8, "HyperparameterName", "\u03b3"], [12, 13, "MetricName", "loss"]]}
{"text": "The multitask loss ( Eq . 10 ) used to train our model contains two parts : the cross - entropy loss and the contastive loss .", "entities": [[2, 3, "MetricName", "loss"], [21, 22, "MetricName", "loss"], [25, 26, "MetricName", "loss"]]}
{"text": "As shown in Tab . 3 , as the weight of the contrastive loss ( \u03b3 ) increases , the model 's performance improves .", "entities": [[13, 14, "MetricName", "loss"], [15, 16, "HyperparameterName", "\u03b3"]]}
{"text": "Coefficient ( \u03b3 ) R - 1 R - 2 R - L 0 ( Generation - Finetuning as a Loop Since the fine - tuned model ( BRIO - Mul ) is still able to gen - ( Stahlberg and Byrne , 2019 ) , and the generator may not be able to differentiate them from high - quality candidates .", "entities": [[2, 3, "HyperparameterName", "\u03b3"], [13, 14, "DatasetName", "0"]]}
{"text": "Calibration requires that a model 's confidence on its predictions is equal to the accuracy of these predictions ( Guo et al , 2017 ) .", "entities": [[14, 15, "MetricName", "accuracy"]]}
{"text": "Previous work ( M\u00fcller et al , 2019 ; Kumar and Sarawagi , 2019 ; has found that a more calibrated text generation model tends to have better performance , and techniques like label smoothing can improve both the token - level calibration and sequence - level accuracy ( i.e. the ability of generating better results ) .", "entities": [[9, 10, "DatasetName", "Kumar"], [21, 23, "TaskName", "text generation"], [33, 35, "MethodName", "label smoothing"], [47, 48, "MetricName", "accuracy"]]}
{"text": "We follow previous work by using the Expected Calibration Error ( Naeini et al , 2015 )", "entities": [[9, 10, "MetricName", "Error"]]}
{"text": "n | acc ( B m ) \u2212 conf ( B m ) | ( 12 ) where the samples are grouped into M equal - width buckets by confidence ( conf ) , B m denotes the m - th bucket , and n is the total number of samples .", "entities": [[2, 3, "MetricName", "acc"], [48, 51, "HyperparameterName", "number of samples"]]}
{"text": "We use diverse beam search ( Vijayakumar et al , 2018 ) where warmup denotes the warmup steps , which is set to 10000 , step is the number of updating steps , lr is the learning rate .", "entities": [[36, 38, "HyperparameterName", "learning rate"]]}
{"text": "We set the length penalty factor \u03b1 in the scoring function ( Eq . 9 ) to the same value as used in the original beam search .", "entities": [[6, 7, "HyperparameterName", "\u03b1"]]}
{"text": "We search the value of the margin \u03bb in the contrastive loss ( Eq . 8 ) within the range [ 1 \u00d7 10 \u22125 , 1 ] , and decide the value based on the model performance on the validation set .", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "\u03b1 ( Eq . 9 ) \u03b3 ( Eq . 10 ) CNNDM 0.001 2.0 100 XSum 0.1 0.6 100 NYT 0.001 2.0 100", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [6, 7, "HyperparameterName", "\u03b3"], [16, 17, "DatasetName", "XSum"]]}
{"text": "We choose to keep the two embedding matrices separate despite the high token overlap : this is partly to keep our approach robust to lower vocabulary overlap settings , and partly due to empirical considerations described in Section 6 . Let \u03b8 s /eb s and \u03b8 t /eb t denote the transformer layer and embedding weights for the student and teacher models respectively .", "entities": [[41, 42, "HyperparameterName", "\u03b8"], [46, 47, "HyperparameterName", "\u03b8"]]}
{"text": "The loss defined in Equation 1 is the MLM cross entropy summed over masked positions M t in the teacher input .", "entities": [[1, 2, "MetricName", "loss"], [8, 9, "DatasetName", "MLM"]]}
{"text": "Equation 2 shows the student MLM loss where M s is the set of positions masked in the student input .", "entities": [[5, 6, "DatasetName", "MLM"], [6, 7, "MetricName", "loss"]]}
{"text": "We optimize the loss using LAMB ( You et al , 2019 ) with a max learning rate of .00125 , linear warmup for the first 10 % of steps , batch size of 2048 and sequence length of 128 .", "entities": [[3, 4, "MetricName", "loss"], [5, 6, "MethodName", "LAMB"], [16, 18, "HyperparameterName", "learning rate"], [21, 23, "MethodName", "linear warmup"], [31, 33, "HyperparameterName", "batch size"], [34, 35, "DatasetName", "2048"]]}
{"text": "For all downstream task evaluations on GLUE , we finetune for 10 epochs using LAMB with a learning rate of 0.0001 and batch size of 64 .", "entities": [[6, 7, "DatasetName", "GLUE"], [14, 15, "MethodName", "LAMB"], [17, 19, "HyperparameterName", "learning rate"], [22, 24, "HyperparameterName", "batch size"]]}
{"text": "For all experiments on SNIPS , we use ADAM with a learning rate of 0.0001 and a batch size of 64 .", "entities": [[4, 5, "DatasetName", "SNIPS"], [8, 9, "DatasetName", "ADAM"], [11, 13, "HyperparameterName", "learning rate"], [17, 19, "HyperparameterName", "batch size"]]}
{"text": "Even so , our 12 - layer model performs credibly compared with the two , presenting a competitive size - accuracy tradeoff , particularly when compared to the 6x larger BERT - of - Theseus .", "entities": [[20, 21, "MetricName", "accuracy"], [30, 31, "MethodName", "BERT"]]}
{"text": "Our smallest 6 - layer model retains over 95 % of the BERT BASE model 's slot filling F1 score ( Sang and Buchholz , 2000 ) while being 30x smaller ( < 10 MB w/o quantization ) and 57x faster on a mobile device , yet task - agnostic .", "entities": [[12, 13, "MethodName", "BERT"], [13, 14, "MethodName", "BASE"], [16, 18, "TaskName", "slot filling"], [18, 20, "MetricName", "F1 score"], [36, 37, "TaskName", "quantization"]]}
{"text": "Our other larger distilled models also demonstrate strong performance ( 0.2 - 0.5 % slot F1 higher than the respective NoKD baselines ) with small model sizes and latencies low enough for real - time inference .", "entities": [[15, 16, "MetricName", "F1"]]}
{"text": "This indicates that small multi - task BERT models ( Tsai et al , 2019 ) present better trade - offs for on - device usage for size , accuracy and latency versus recurrent encoder - based models such as StackProp .", "entities": [[7, 8, "MethodName", "BERT"], [29, 30, "MetricName", "accuracy"]]}
{"text": "On the SST - 2 and MNLI - m dev sets , this model obtained 90.9 % and 83.7 % accuracy respectively - only 1.8 % and 0.7 % lower respectively compared to BERT BASE .", "entities": [[2, 3, "DatasetName", "SST"], [6, 9, "DatasetName", "MNLI - m"], [20, 21, "MetricName", "accuracy"], [33, 34, "MethodName", "BERT"], [34, 35, "MethodName", "BASE"]]}
{"text": "This model showed small gains ( 0.1 % / 0.5 % accuracy on SST - 2 / MNLI - m dev ) over our analogous distilled model , but with 30 % more parameters solely due to the larger vocabulary .", "entities": [[11, 12, "MetricName", "accuracy"], [13, 14, "DatasetName", "SST"], [17, 20, "DatasetName", "MNLI - m"]]}
{"text": "Our proposed model achieves an MRR of about 84.8 % for the task of next utterance selection on a newly introduced DailyDialog dataset , and outperform the baseline models .", "entities": [[5, 6, "MetricName", "MRR"], [21, 22, "DatasetName", "DailyDialog"]]}
{"text": "In a conversation , all words in first K utterances can be stringed together to form a single long chain and passed to an RNN encoder as following : e k = f 1 embed ( w k ) \u2200k 1 , 2 , . . .", "entities": [[30, 32, "HyperparameterName", "k ="]]}
{"text": "h e k = f 1 rnn ( h e k\u22121 , e k ) \u2200k 1 , 2 , . . .", "entities": [[2, 4, "HyperparameterName", "k ="]]}
{"text": "d k = f 2 rnn ( h d k\u22121 , f 2 embed ( w k ) ) \u2200k", "entities": [[1, 3, "HyperparameterName", "k ="]]}
{"text": "n \u2212 1 P k = Logistic ( h d k ) .", "entities": [[4, 6, "HyperparameterName", "k ="]]}
{"text": "The sequence of operations for the DA - encoder are as follows : e da k = f 3 embed ( da k ) \u2200k 1 , 2 , . . .", "entities": [[15, 17, "HyperparameterName", "k ="]]}
{"text": "K h da k = f 4 rnn ( h da k\u22121 , e da k ) \u2200k 1 , 2 , . . .", "entities": [[3, 5, "HyperparameterName", "k ="]]}
{"text": "K = h da K ( 4 )", "entities": [[0, 2, "HyperparameterName", "K ="]]}
{"text": "g K + ( 1 \u2212 \u03b1 )", "entities": [[6, 7, "HyperparameterName", "\u03b1"]]}
{"text": "The maximum batch size allowed was 32 .", "entities": [[2, 4, "HyperparameterName", "batch size"]]}
{"text": "Models were trained to minimize cross entropy using Adam optimizer with learning rate of 0.0003 ( optimized over 0.0001 , 0.0003 , 0.0005 , 0.0007 , 0.001 ) .", "entities": [[8, 9, "MethodName", "Adam"], [9, 10, "HyperparameterName", "optimizer"], [11, 13, "HyperparameterName", "learning rate"]]}
{"text": "We found that a higher learning rate up - to 0.0005 helps the model to learn quickly , whereas learning rate greater than 0.0005 leads to oscillations .", "entities": [[5, 7, "HyperparameterName", "learning rate"], [19, 21, "HyperparameterName", "learning rate"]]}
{"text": "In the generative case , the plain ED has an MRR of 0.474 , whereas the same model , when conditioned with DA - Encoder , has an MRR of 0.54 , an improvement of 13.9 % .", "entities": [[10, 11, "MetricName", "MRR"], [28, 29, "MetricName", "MRR"]]}
{"text": "The hierarchical encoder - decoder HRED and HRED - DA has an MRR of 0.523 and 0.583 , respectively , an improvement of 11.4 % .", "entities": [[12, 13, "MetricName", "MRR"]]}
{"text": "Generative models are sequence - to - sequence models and rather complex in nature , so it is interesting to note that even a much simpler discriminative model , i.e. plain Siamese model , without any dialogue act information , has an MRR of 0.8 compared to 0.58 of the best performing generative model , i.e. HRED - DA .", "entities": [[42, 43, "MetricName", "MRR"]]}
{"text": "The proposed model improves these baseline numbers by incorporating hierarchy and dialogue act information , and pushes the MRR to 0.848 .", "entities": [[18, 19, "MetricName", "MRR"]]}
{"text": "In Table 4 ( b ) , first row in recall@1 column is 0.65 , which indicates that out of the total number of test conversations where dialogue act of the last utterance of context was I , 65 % of true candidate responses were ranked 1 by the HSiamese model .", "entities": [[10, 11, "MetricName", "recall@1"]]}
{"text": "We also propose a novel discriminative model that leverages the hierarchical structure in a conversation and dialogue act information to produce much improved results , an MRR of 0.848 .", "entities": [[26, 27, "MetricName", "MRR"]]}
{"text": "They report 61 % accuracy on 5 - second segments , and 84 % accuracy on 120 second segments .", "entities": [[4, 5, "MetricName", "accuracy"], [14, 15, "MetricName", "accuracy"]]}
{"text": "They achieve 80 % - 86 % accuracy for all of their attempts .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "Hanani et al ( 2015 ) perform recognition of several Palestinian regional accents , evaluating four different acoustic models , achieving 81.5 % accuracy for their best system , an I - vector framework with 64 Gaussian components .", "entities": [[23, 24, "MetricName", "accuracy"]]}
{"text": "We train our model with Adam ( Kingma and Ba , 2014 ) optimizer with learning rate 0.01 and without any dropout .", "entities": [[5, 6, "MethodName", "Adam"], [13, 14, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"]]}
{"text": "The number of epochs is 800 and we do not use mini - batches or dropout regularization technique .", "entities": [[1, 4, "HyperparameterName", "number of epochs"]]}
{"text": "We tried a different number of character n - grams and we achieve the best result ( 0.555 macro F 1score ) on the development data using three inputs - character unigrams , bigrams and trigrams , with learning rate 0.005 , mini - batches of size 256 for 11 epochs and with the Adam optimizer .", "entities": [[38, 40, "HyperparameterName", "learning rate"], [54, 55, "MethodName", "Adam"], [55, 56, "HyperparameterName", "optimizer"]]}
{"text": "This improved tweet classification accuracy by about 5 % , but actually decreased user classification accuracy on the development set .", "entities": [[4, 5, "MetricName", "accuracy"], [15, 16, "MetricName", "accuracy"]]}
{"text": "Indeed , compared to fullprecision , our 8 - bit models score greater or equal BLEU on most tasks .", "entities": [[15, 16, "MetricName", "BLEU"]]}
{"text": "successfully quantize a stacked sequence - tosequence LSTM to 8 - bit without any loss in translation quality .", "entities": [[7, 8, "MethodName", "LSTM"], [14, 15, "MetricName", "loss"]]}
{"text": "For example , in the context of 8 - bit quantization , k = 8 .", "entities": [[10, 11, "TaskName", "quantization"], [12, 14, "HyperparameterName", "k ="]]}
{"text": "Reported perplexity is per token and BLEU was measured with multi - bleu.pl 1 on the newstest2014 2 test set .", "entities": [[1, 2, "MetricName", "perplexity"], [6, 7, "MetricName", "BLEU"]]}
{"text": "Our BLEU score was computed on the test set using the checkpoint with the highest validation accuracy over In Table 2 , we show performance of our method on the WMT14 EN - DE and WMT14 EN - FR for a fixed amount of training steps .", "entities": [[1, 3, "MetricName", "BLEU score"], [16, 17, "MetricName", "accuracy"], [30, 31, "DatasetName", "WMT14"], [35, 36, "DatasetName", "WMT14"]]}
{"text": "Standard deviation of the BLEU scores did not seem higher for any method and ranged between 0.09 and 0.51 .", "entities": [[4, 5, "MetricName", "BLEU"]]}
{"text": "As for post - training quantization , the BLEU score was computed on the test set using the best validation performance out of 20 trials .", "entities": [[5, 6, "TaskName", "quantization"], [8, 10, "MetricName", "BLEU score"]]}
{"text": "For example in the case of WMT14 EN - DE , the maximum BLEU FullyQT base 8 - bit obtained was 26.98 , while the baseline 's highest was 26.64 .", "entities": [[6, 7, "DatasetName", "WMT14"], [13, 14, "MetricName", "BLEU"]]}
{"text": "In 24 out of 27 experiments , performance was better than our full - precision baseline of 38.34 BLEU .", "entities": [[18, 19, "MetricName", "BLEU"]]}
{"text": "We found post - quantization BLEU scores to vary by about 0.2 BLEU .", "entities": [[4, 5, "TaskName", "quantization"], [5, 6, "MetricName", "BLEU"], [12, 13, "MetricName", "BLEU"]]}
{"text": "Models were trained for 10 epochs with a batch size of 20 and sequence length of 35 .", "entities": [[8, 10, "HyperparameterName", "batch size"]]}
{"text": "Learning rate is set to 5 , dropout to 0.2 and gradient clipping to 0.25 .", "entities": [[0, 2, "HyperparameterName", "Learning rate"], [11, 13, "MethodName", "gradient clipping"]]}
{"text": "Loss and perplexity are computed on the test set and averaged over 10 trials for WikiText - 2 and 3 trials for WikiText - 3 .", "entities": [[2, 3, "MetricName", "perplexity"]]}
{"text": "We empirically found z = 0.025 to work well , with higher thresholds causing BLEU to quickly decay .", "entities": [[14, 15, "MetricName", "BLEU"]]}
{"text": "e t \u03a0I , 1 Note crucially that here the underspecification for the pronoun term @ 1 e is resolved before the meaning contribution of the RNR'ed S which contains it as a subterm is copies into each conjunct via \u03b2 - reduction .", "entities": [[40, 41, "HyperparameterName", "\u03b2"]]}
{"text": "After @ - elimination and \u03b2 - reduction , we obtain the final translation in ( 25 ) , which corresponds to the parallel bound reading for the sentence .", "entities": [[5, 6, "HyperparameterName", "\u03b2"]]}
{"text": "By \u03b2 - reducing the term after anaphora resolution , we obtain ( 26 ) , where the pronoun refers to Bobby Fisher in each conjunct .", "entities": [[1, 2, "HyperparameterName", "\u03b2"]]}
{"text": "If we resolve underspecification before \u03b2 - reducing the term , we obtain the wide scope reading for the indefinite as in ( 28 ) : ( 28 ) \u03bbx", "entities": [[5, 6, "HyperparameterName", "\u03b2"]]}
{"text": "If , on the other hand , we first \u03b2 - reduce the term and then resolve underspecification , the \u03a3 - type that has the existential force associated with the indefinite is introduced in the smallest local context in each conjunct , via ( 18 ) .", "entities": [[9, 10, "HyperparameterName", "\u03b2"]]}
{"text": "One translation that our analysis can assign to ( 31 ) is the following : Here , \u03b2 - conversion for the \u03bb - bound variables y ,", "entities": [[17, 18, "HyperparameterName", "\u03b2"]]}
{"text": "( 2 ) Increasing batch size to a certain amount can obtain better performance on downstream tasks , especially for the response selection .", "entities": [[4, 6, "HyperparameterName", "batch size"]]}
{"text": "We train TOD - BERT with AdamW ( Loshchilov and Hutter , 2017 ) optimizer with a dropout ratio of 0.1 on all layers and attention weights .", "entities": [[4, 5, "MethodName", "BERT"], [6, 7, "MethodName", "AdamW"], [14, 15, "HyperparameterName", "optimizer"]]}
{"text": "Models are early - stopped using perplexity scores of a held - out development set , with mini - batches containing 32 sequences of maximum length 512 tokens .", "entities": [[6, 7, "MetricName", "perplexity"]]}
{"text": "The model is trained with binary cross - entropy loss and the i - th dialogue act is considered as a triggered dialogue act if A i > 0.5 .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "TOD - BERT outperforms BERT and other strong baselines in one of the largest intent recognition Model Acc ( all ) Acc ( in ) Acc ( out ) Recall ( out ) 1 - Shot BERT 29.3 % \u00b1 3.4 % 35.7 % \u00b1 4.1 % 81.3 % \u00b1 0.4 % 0.4 % \u00b1 0.3 % TOD - BERT - mlm 38.9 % \u00b1 6.3 % 47.4 % \u00b1 7.6 % 81.6 % \u00b1 0.2 % 0.5 % \u00b1 0.2 % TOD - BERT - jnt 42.5 % \u00b1 0.1 % 52.0 % \u00b1 0.1 % 81.7 % \u00b1 0.1 % 0.1 % \u00b1 0.1 % 10 - Shot BERT 75.5 % \u00b1 1.1 % 88.6 % \u00b1 1.1 % 84.7 % \u00b1 0.3 % 16.5 % \u00b1 1.7 % TOD - BERT - mlm 76.6 % \u00b1 0.8 % 90.5 % \u00b1 1.2 % 84.3 % \u00b1 0.2 % 14.0 % \u00b1 1.3 % TOD - BERT - jnt 77.3 % \u00b1 0.5 % 91.0 % \u00b1 0.5 % 84.5 % \u00b1 0.4 % 15.3 % \u00b1 2.1 % datasets , as shown in Table 2 .", "entities": [[2, 3, "MethodName", "BERT"], [4, 5, "MethodName", "BERT"], [14, 16, "TaskName", "intent recognition"], [17, 18, "MetricName", "Acc"], [21, 22, "MetricName", "Acc"], [25, 26, "MetricName", "Acc"], [29, 30, "MetricName", "Recall"], [36, 37, "MethodName", "BERT"], [59, 60, "MethodName", "BERT"], [61, 62, "DatasetName", "mlm"], [84, 85, "MethodName", "BERT"], [110, 111, "MethodName", "BERT"], [133, 134, "MethodName", "BERT"], [135, 136, "DatasetName", "mlm"], [158, 159, "MethodName", "BERT"]]}
{"text": "TOD - BERT - jnt has 13.2 % all - intent accuracy improvement and 16.3 % in - domain accuracy improvement compared to BERT in the 1 - shot setting .", "entities": [[2, 3, "MethodName", "BERT"], [11, 12, "MetricName", "accuracy"], [19, 20, "MetricName", "accuracy"], [23, 24, "MethodName", "BERT"]]}
{"text": "In Table 5 , we compare BERT to TOD - BERTjnt on the MWOZ 2.1 dataset and find the latter has 2.4 % joint goal accuracy improvement .", "entities": [[6, 7, "MethodName", "BERT"], [25, 26, "MetricName", "accuracy"]]}
{"text": "We run two other baselines , MLP and RNN , to further show the strengths of BERT - based MWOZ ( 13 ) DSTC2 ( 9 ) GSIM ( 6 ) micro - F1 macro - F1 micro - F1 macro - F1 micro - F1 macro - F1 1 % Data BERT 84.0 % \u00b1 0.6 % 66.7 % \u00b1 1.7 % 77.1 % \u00b1 2.1 % 25.8 % \u00b1 0.8 % 67.3 % \u00b1 1.4 % 26.9 % \u00b1 1.0 % TOD - BERT - mlm 87.5 % \u00b1 0.6 % 73.3 % \u00b1 1.5 % 79.6 % \u00b1 1.0 % 26.4 % \u00b1 0.5 % 82.7 % \u00b1 0.7 % 35.7 % \u00b1 0.3 % TOD - BERT - jnt 86.9 % \u00b1 0.2 % 72.4 % \u00b1 0.8 % 82.9 % \u00b1 0.4 % 28.0 % \u00b1 0.1 % 78.4 % \u00b1 3.2 % 32.9 % \u00b1 2.1 % 10 % Data BERT 89.7 % \u00b1 0.2 % 78.4 % \u00b1 0.3 % 88.2 % \u00b1 0.7 % 34.8 % \u00b1 1.3 % 98.4 % \u00b1 0.3 % 45.1 % \u00b1 0.2 % TOD - BERT - mlm 90.1 % \u00b1 0.2 % 78.9 % \u00b1 0.1 % 91.8 % \u00b1 1.7 % 39.4 % \u00b1 1.7 % 99.2 % \u00b1 0.1 % 45.6 % \u00b1 0.1 % TOD - BERT - jnt 90.2 % \u00b1 0.2 % 79.6 % \u00b1 0.7 % 90.6 % \u00b1 3.2 % 38.8 % \u00b1 2.2 % 99.3 % \u00b1 0.1 % 45.7 % \u00b1 0.0 %", "entities": [[6, 7, "DatasetName", "MLP"], [16, 17, "MethodName", "BERT"], [31, 34, "MetricName", "micro - F1"], [34, 37, "MetricName", "macro - F1"], [37, 40, "MetricName", "micro - F1"], [40, 43, "MetricName", "macro - F1"], [43, 46, "MetricName", "micro - F1"], [46, 49, "MetricName", "macro - F1"], [52, 53, "MethodName", "BERT"], [85, 86, "MethodName", "BERT"], [87, 88, "DatasetName", "mlm"], [120, 121, "MethodName", "BERT"], [156, 157, "MethodName", "BERT"], [189, 190, "MethodName", "BERT"], [191, 192, "DatasetName", "mlm"], [224, 225, "MethodName", "BERT"]]}
{"text": "Slot Acc 1 % Data BERT 6.4 % \u00b1 1.4 % 84.4 % \u00b1 1.0 % TOD - BERT - mlm 9.9 % \u00b1 0.6 % 86.6 % \u00b1 0.5 % TOD - BERT - jnt 8.0 % \u00b1 1.0 % 85.3 % \u00b1 0.4 % 5 % Data BERT 19.6 % \u00b1 0.1 % 92.0 % \u00b1 0.5 % TOD - BERT - mlm 28.1 % \u00b1 1.6 % 93.9 % \u00b1 0.1 % TOD - BERT - jnt 28.6 % \u00b1 1.4 % 93.8 % \u00b1 0.3 % 10 % Data BERT 32.9 % \u00b1 0.6 % 94.7 % \u00b1 0.1 % TOD - BERT - mlm 39.5 % \u00b1 0.7 % 95.6 % \u00b1 0.1 % TOD - BERT - jnt 37.0 % \u00b1 0.1 % 95.2 % \u00b1 0.1 % 25 % Data BERT 40.8 % \u00b1 1.0 % 95.8 % \u00b1 0.1 % TOD - BERT - mlm 44.0 % \u00b1 0.4 % 96.4 % \u00b1 0.1 % TOD - BERT - jnt 44.3 % \u00b1 0.3 % 96.3 % \u00b1 0.2 % models .", "entities": [[1, 2, "MetricName", "Acc"], [5, 6, "MethodName", "BERT"], [18, 19, "MethodName", "BERT"], [20, 21, "DatasetName", "mlm"], [33, 34, "MethodName", "BERT"], [49, 50, "MethodName", "BERT"], [62, 63, "MethodName", "BERT"], [64, 65, "DatasetName", "mlm"], [77, 78, "MethodName", "BERT"], [93, 94, "MethodName", "BERT"], [106, 107, "MethodName", "BERT"], [108, 109, "DatasetName", "mlm"], [121, 122, "MethodName", "BERT"], [137, 138, "MethodName", "BERT"], [150, 151, "MethodName", "BERT"], [152, 153, "DatasetName", "mlm"], [165, 166, "MethodName", "BERT"]]}
{"text": "In the fewshot experiments , TOD - BERT - mlm outperforms BERT by 3.5 % micro - F1 and 6.6 % macro - F1 on MWOZ corpus in the 1 % data scenario .", "entities": [[7, 8, "MethodName", "BERT"], [9, 10, "DatasetName", "mlm"], [11, 12, "MethodName", "BERT"], [15, 18, "MetricName", "micro - F1"], [21, 24, "MetricName", "macro - F1"]]}
{"text": "To evaluate response selection in task - oriented dialogues , we follow the k - to - 100 accuracy , which is becoming a research community standard ( Yang et al , 2018 ; Henderson et al , 2019a ) .", "entities": [[18, 19, "MetricName", "accuracy"]]}
{"text": "We report 1 - to - 100 and 3 - to - 100 accuracy , which is similar to recall1 and recall@3 given 100 candidates .", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "TOD - BERT - jnt achieves 65.8 % 1 - to - 100 accuracy and 87.0 % 3 - to - 100 accuracy on MWOZ , which surpasses BERT by 18.3 % and 11.5 % , respectively .", "entities": [[2, 3, "MethodName", "BERT"], [13, 14, "MetricName", "accuracy"], [22, 23, "MetricName", "accuracy"], [28, 29, "MethodName", "BERT"]]}
{"text": "In our experiments , we set batch size equals to 25 for all the models .", "entities": [[6, 8, "HyperparameterName", "batch size"]]}
{"text": "For K=10 , TOD - BERT has a 0.143 NMI score , and BERT only has 0.094 .", "entities": [[5, 6, "MethodName", "BERT"], [9, 10, "MetricName", "NMI"], [13, 14, "MethodName", "BERT"]]}
{"text": "For K=20 , TOD - BERT achieves a 0.213 NMI score , while BERT has 0.109 .", "entities": [[5, 6, "MethodName", "BERT"], [9, 10, "MetricName", "NMI"], [13, 14, "MethodName", "BERT"]]}
{"text": "For the feedforward networks in the transformer layers we use the activation as in Siegelmann and Sontag ( 1992 ) , namely the saturated linear activation function \u03c3 ( x ) which takes value 0 for x < 0 , value x for 0", "entities": [[25, 27, "HyperparameterName", "activation function"], [34, 35, "DatasetName", "0"], [38, 39, "DatasetName", "0"], [43, 44, "DatasetName", "0"]]}
{"text": "K = ( k 1 , . .", "entities": [[0, 2, "HyperparameterName", "K ="]]}
{"text": "a = \u03b1 1 v 1 + \u03b1 2 v 2 + + \u03b1 n v n , where ( i ) ( \u03b1 1 , . .", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [7, 8, "HyperparameterName", "\u03b1"], [13, 14, "HyperparameterName", "\u03b1"], [23, 24, "HyperparameterName", "\u03b1"]]}
{"text": "A single - layer encoder is a function Enc ( X ; \u03b8 ) , with input X = ( x 1 , . . .", "entities": [[12, 13, "HyperparameterName", "\u03b8"]]}
{"text": "Formally , for 0 \u2264 \u2264 L\u22121 and Y 0 : = Y we have Y +1 = Dec ( ( K e , V e ) , Y ; \u03b8 ) ,", "entities": [[3, 4, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [30, 31, "HyperparameterName", "\u03b8"]]}
{"text": "[ h t\u22121 , 0 d h , 0 d b , \u03b4 t , 1 2 t+1 , 0 m , 0 m , \u03c9 t ] , where essentially \u03b4 t = \u03c3 ( \u03c9 t \u2212 \u03c9 t\u22121 ) , except with a change for the last coordinate due to special status of the last symbol $ in the processing of RNN .", "entities": [[4, 5, "DatasetName", "0"], [8, 9, "DatasetName", "0"], [12, 13, "HyperparameterName", "\u03b4"], [19, 20, "DatasetName", "0"], [22, 23, "DatasetName", "0"], [31, 32, "HyperparameterName", "\u03b4"]]}
{"text": "Now p ( 2 ) t , k e j = \u03b4 t , s j = \u03b4 t , j , which is positive if and only if s j = s t , as mentioned earlier .", "entities": [[11, 12, "HyperparameterName", "\u03b4"], [17, 18, "HyperparameterName", "\u03b4"]]}
{"text": "This is because by the definition of hard attention the vector ( \u03b1 1 , . . .", "entities": [[12, 13, "HyperparameterName", "\u03b1"]]}
{"text": ", \u03b1 n ) is characterized by the set of zero coordinates and there are at most 2 n \u2212 1 such sets ( all coordinates can not be zero ) .", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "While tuning the models , we vary the number of layers from 1 to 4 , the learning rate , warmup steps and the number of heads .", "entities": [[8, 11, "HyperparameterName", "number of layers"], [17, 19, "HyperparameterName", "learning rate"]]}
{"text": "We will work with an alphabet \u03a3 = { \u03b2 1 , . . .", "entities": [[9, 10, "HyperparameterName", "\u03b2"]]}
{"text": ", \u03b2 m } , with \u03b2 1 = # and \u03b2 m = $ .", "entities": [[1, 2, "HyperparameterName", "\u03b2"], [6, 7, "HyperparameterName", "\u03b2"], [11, 12, "HyperparameterName", "\u03b2"]]}
{"text": "For the feed - forward networks in the transformer layers we use the activation as in ( Siegelmann and Sontag , 1992 ) , namely the saturated linear activation function : \u03c3 ( x ) = \uf8f4 \uf8f2 \uf8f4 \uf8f3 0", "entities": [[28, 30, "HyperparameterName", "activation function"], [40, 41, "DatasetName", "0"]]}
{"text": "K = ( k 1 , . .", "entities": [[0, 2, "HyperparameterName", "K ="]]}
{"text": "The q - attention over keys K and values V , denoted by Att ( q , K , V ) , is a vector a given by ( \u03b1 1 , . . .", "entities": [[29, 30, "HyperparameterName", "\u03b1"]]}
{"text": ", f att ( q , k n ) ) , a = \u03b1 1 v 1 + \u03b1 2 v 2 + + \u03b1 n v n .", "entities": [[13, 14, "HyperparameterName", "\u03b1"], [18, 19, "HyperparameterName", "\u03b1"], [24, 25, "HyperparameterName", "\u03b1"]]}
{"text": "A single - layer encoder is a function Enc ( X ; \u03b8 ) , where \u03b8 is the parameter vector and the input X = ( x 1 , . . .", "entities": [[12, 13, "HyperparameterName", "\u03b8"], [16, 17, "HyperparameterName", "\u03b8"]]}
{"text": "The complete L - layer transformer encoder TEnc ( L ) ( X ; \u03b8 ) has the same input X = ( x 1 , . . .", "entities": [[14, 15, "HyperparameterName", "\u03b8"]]}
{"text": "Formally , for 1 \u2264 \u2264 L \u2212 1 and X 1 : = X , we have X +1 = Enc ( X ; \u03b8 ) ,", "entities": [[25, 26, "HyperparameterName", "\u03b8"]]}
{"text": "Formally , for 1 \u2264 \u2264 L \u2212 1 and Y 1 = Y we have Y +1 = Dec ( ( K e , V e ) , Y ; \u03b8 ) ,", "entities": [[31, 32, "HyperparameterName", "\u03b8"]]}
{"text": "While tuning the network , we vary the number of layer from 1 to 4 , the learning rate , the number of heads , the warmup steps , embedding size and feedforward embedding size .", "entities": [[17, 19, "HyperparameterName", "learning rate"]]}
{"text": "While tuning the models , we vary the layers from 1 to 4 , the learning rate , warmup steps and the number of heads .", "entities": [[15, 17, "HyperparameterName", "learning rate"]]}
{"text": "From the defintion of \u03c9 t , it follows that at any step 1 \u2264 k \u2264 | \u03a3 | we have where \u03c6 t , k denotes the number of times the k - th symbol \u03b2", "entities": [[37, 38, "HyperparameterName", "\u03b2"]]}
{"text": "Note that 0 \u2264 \u2206 t , k \u2264 1 and 0 \u2264 \u03b4 t , k \u2264 1 for 0", "entities": [[2, 3, "DatasetName", "0"], [11, 12, "DatasetName", "0"], [13, 14, "HyperparameterName", "\u03b4"], [20, 21, "DatasetName", "0"]]}
{"text": "At the t - th step in the decoder - encoder attention block of layer 1 we have where ( \u03b1 t , k \u0113 t = hardmax ( 0 , . . .", "entities": [[20, 21, "HyperparameterName", "\u03b1"], [29, 30, "DatasetName", "0"]]}
{"text": "Denote the L.H.S. in ( 16 ) by \u03b3 t .", "entities": [[8, 9, "HyperparameterName", "\u03b3"]]}
{"text": "On the other hand , if s t = \u03b2 k , then \u2264 0 .", "entities": [[9, 10, "HyperparameterName", "\u03b2"], [14, 15, "DatasetName", "0"]]}
{"text": "For 0 \u2264 t \u2264 n and 1 \u2264 k \u2264 | \u03a3 | , this leads to Recall that p ( 2 ) t", "entities": [[1, 2, "DatasetName", "0"], [18, 19, "MetricName", "Recall"]]}
{"text": "t < n , if 0 \u2264 j \u2264 t is such that s j = s t , then t , k e j = \u03b4 t , s j = \u03b4 t , i = 0 .", "entities": [[5, 6, "DatasetName", "0"], [26, 27, "HyperparameterName", "\u03b4"], [32, 33, "HyperparameterName", "\u03b4"], [37, 38, "DatasetName", "0"]]}
{"text": "And for 0 < t < n , if 0 \u2264 j \u2264 t is such that s j = s t = \u03b2", "entities": [[2, 3, "DatasetName", "0"], [9, 10, "DatasetName", "0"], [23, 24, "HyperparameterName", "\u03b2"]]}
{"text": "i , then t , k e j = \u03b4 t , s j = \u03b4 t , i = t \u2212 \u03c6 t\u22121 , j t ( t + 1 ) \u2265 1 t ( t + 1 ) .", "entities": [[9, 10, "HyperparameterName", "\u03b4"], [15, 16, "HyperparameterName", "\u03b4"]]}
{"text": "We evaluate the current state - of - the - art model , Self - MM ( Yu et al , 2021 ) , and report the mean absolute error ( MAE ) on the multimodal sentiment analysis task .", "entities": [[31, 32, "MetricName", "MAE"], [35, 38, "TaskName", "multimodal sentiment analysis"]]}
{"text": "We use Adam as the optimizer and the learning rate is 5e - 5 .", "entities": [[2, 3, "MethodName", "Adam"], [5, 6, "HyperparameterName", "optimizer"], [8, 10, "HyperparameterName", "learning rate"]]}
{"text": "The batch size is 64 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "For the MOSI - SpeechBrain , MOSI - IBM , and MOSI - iFlytek datasets , following previous work ( Yu et al , 2021 ) , we take 2 - class accuracy ( Acc - 2 ) , F1 score ( F1 ) , mean absolute error ( MAE ) , and correlation ( Corr ) as our evaluation metrics .", "entities": [[2, 3, "DatasetName", "MOSI"], [6, 7, "DatasetName", "MOSI"], [11, 12, "DatasetName", "MOSI"], [13, 14, "DatasetName", "iFlytek"], [32, 33, "MetricName", "accuracy"], [34, 35, "MetricName", "Acc"], [39, 41, "MetricName", "F1 score"], [42, 43, "MetricName", "F1"], [49, 50, "MetricName", "MAE"]]}
{"text": "And for Acc - 2 and F1 - Score , we calculate them in two ways , negative / non - negative ( Non0 - Acc , Non0 - F1 ) and negative / positive ( Has0 - Acc , Has0 - F1 ) .", "entities": [[2, 3, "MetricName", "Acc"], [6, 9, "MetricName", "F1 - Score"], [25, 26, "MetricName", "Acc"], [29, 30, "MetricName", "F1"], [38, 39, "MetricName", "Acc"], [42, 43, "MetricName", "F1"]]}
{"text": "The performance of slots 1 and 2 , for both subtasks , are measured by means of the F - score , while slot 3 is evaluated by means of the accuracy .", "entities": [[31, 32, "MetricName", "accuracy"]]}
{"text": "The monotonic dimensions of a vector a is given by : M a = { j | w j \u2264 a j \u2264 w j , j { 1 , ... , D } } \u222a { j | w j \u2265 a j \u2265 w j , j { 1 , ... , D } } , where D is the word embedding dimension .", "entities": [[62, 65, "HyperparameterName", "word embedding dimension"]]}
{"text": "We use k = 20 % in our experiments .", "entities": [[2, 4, "HyperparameterName", "k ="]]}
{"text": "Moreover , we find that the average Pearson correlation between log - odds and WAE is 0.32 and the correlation is 0.45 if we consider the eight settings where we outperform IG .", "entities": [[7, 9, "MetricName", "Pearson correlation"]]}
{"text": "( 5 ) Here , F is the neural network , x is a baseline embedding , and \u03b1 is the step size .", "entities": [[18, 19, "HyperparameterName", "\u03b1"], [21, 23, "HyperparameterName", "step size"]]}
{"text": "Summary measures such as precision , recall , and F1 are limited in their capacity to inform about the quality of the predictions of an automated system ( Derczynski , 2016 ; Yacouby and Axman , 2020 ) .", "entities": [[9, 10, "MetricName", "F1"], [28, 29, "DatasetName", "Derczynski"]]}
{"text": "Table 1 shows the Pearson r , Spearman correlation coefficient \u03c1 , and Root Mean Squared Error ( RMSE ) for the total daily protest cell counts of the Baseline and participant systems , over the 35 days target time range .", "entities": [[7, 9, "MetricName", "Spearman correlation"], [16, 17, "MetricName", "Error"], [18, 19, "MetricName", "RMSE"]]}
{"text": "Table 2 reports Pearson r , Spearman correlation coefficient \u03c1 , and Root Mean Squared Error ( RMSE ) over cell - day event counts of the Baseline and participant systems with respect to Gold Standard , for the 35 days time range .", "entities": [[6, 8, "MetricName", "Spearman correlation"], [15, 16, "MetricName", "Error"], [17, 18, "MetricName", "RMSE"]]}
{"text": "To this end , we propose a surrogate learning criteria L SG ( \u03b8 ) to optimize the causal relation based on representation learning : 4 The proof is shown in Appendix LSG ( \u03b8 )", "entities": [[13, 14, "HyperparameterName", "\u03b8"], [22, 24, "TaskName", "representation learning"], [34, 35, "HyperparameterName", "\u03b8"]]}
{"text": "2 ) FS - ClusterLoss ( Lai et al , 2020 ) , which add two auxiliary loss functions when training .", "entities": [[17, 18, "MetricName", "loss"]]}
{"text": "Compared with the original metric - based models , our method achieves 8.7 % and 1.6 % micro - F1 ( average ) improvement in prototypical network and relation network respectively .", "entities": [[17, 20, "MetricName", "micro - F1"]]}
{"text": "Compared with best score in baselines , our method gains 7.5 % , 1.0 % , and 2.0 % micro - F1 improvements on ACE05 , MAVEN and KBP17 datasets respectively .", "entities": [[19, 22, "MetricName", "micro - F1"], [26, 27, "DatasetName", "MAVEN"]]}
{"text": "q i j ) , k = 0 , 1 ( 6 )", "entities": [[5, 7, "HyperparameterName", "k ="], [7, 8, "DatasetName", "0"]]}
{"text": "Then , pair - Algorithm 1 Inter - modal Interactive Module for Multi - modal Sentiment and Emotion Recognition ( IIM - MMSE ) procedure IIM - MMSE ( t , v , a ) for i 1 , ... , K do K = # modalities for j 1 , ... , K do \u2200x , y [ T , V , A ] ,", "entities": [[17, 19, "TaskName", "Emotion Recognition"], [43, 45, "HyperparameterName", "K ="]]}
{"text": "Y IIM Decoder ( C XY ) loss cross entropy ( Y , Y ) Backpropagation to update the weights return C XY Algorithm 3 Context - aware Attention Module ( CAM ) procedure CAM ( M , BI ) P M.BI T Cross product for i , j 1 , ... , u do u = # utterances", "entities": [[7, 8, "MetricName", "loss"], [31, 32, "MethodName", "CAM"], [34, 35, "MethodName", "CAM"]]}
{"text": "( Tong et al , 2017 ) ) and F1 - score for the classification problems , while for the intensity prediction task , we compute Pearson correlation scores and mean - absolute - error ( MAE ) .", "entities": [[9, 12, "MetricName", "F1 - score"], [26, 28, "MetricName", "Pearson correlation"], [36, 37, "MetricName", "MAE"]]}
{"text": "For MOSEI dataset , with tri - modal inputs , our proposed system reports 79.02 % F1 - score and 62.97 % weighted - accuracy for emotion classification .", "entities": [[16, 19, "MetricName", "F1 - score"], [24, 25, "MetricName", "accuracy"], [26, 28, "TaskName", "emotion classification"]]}
{"text": "For sentiment classification , we obtain 78.23 % , 80.37 % , 49.15 % and 50.14 % as F1 - score for two - class , five - class and seven - class , respectively .", "entities": [[18, 21, "MetricName", "F1 - score"]]}
{"text": "For sentiment intensity prediction task , our proposed system yields MAE and Pearson score of 0.683 and 0.594 , respectively .", "entities": [[10, 11, "MetricName", "MAE"]]}
{"text": "For emotion classification , our proposed approach achieves approximately 3 and 0.6 percentage higher F1 - ( Zadeh et al , 2018c ) .", "entities": [[1, 3, "TaskName", "emotion classification"], [14, 15, "MetricName", "F1"]]}
{"text": "On average , we observe 1 to 5 % improvement in accuracy values in comparison to the next best systems .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "If both w i , i and w i , i are within a prefixed interval [ \u03b4 min , \u03b4 max ] , we consider ( v i , v i ) as a sentence node pair with uncertain pairwise ordering and add it into VP ( 0 ) .", "entities": [[17, 18, "HyperparameterName", "\u03b4"], [20, 21, "HyperparameterName", "\u03b4"], [48, 49, "DatasetName", "0"]]}
{"text": "We empirically set thresholds \u03b4 min and \u03b4 max as 0.2 and 0.8 , and set \u03b7 as 20 % , 15 % , 25 % , 15 % , 15 % according to accuracies of initial classifier on validation sets .", "entities": [[4, 5, "HyperparameterName", "\u03b4"], [7, 8, "HyperparameterName", "\u03b4"]]}
{"text": "We adopt Adadelta ( Zeiler , 2012 ) with = 10 \u22126 , \u03c1 = 0.95 and initial learning rate 1.0 as the optimizer .", "entities": [[2, 3, "MethodName", "Adadelta"], [18, 20, "HyperparameterName", "learning rate"], [23, 24, "HyperparameterName", "optimizer"]]}
{"text": "We employ L2 weight decay with coefficient 10 \u22125 , batch size of 16 and dropout rate of 0.5 .", "entities": [[3, 5, "MethodName", "weight decay"], [10, 12, "HyperparameterName", "batch size"]]}
{"text": "Concretely , we set sizes of hidden states and node states to 768 , the learning rate of BERT as 3e - 3 , the batch size as 16 , 32 , 128 , 128 , 64 for the five datasets . Baselines .", "entities": [[15, 17, "HyperparameterName", "learning rate"], [18, 19, "MethodName", "BERT"], [25, 27, "HyperparameterName", "batch size"]]}
{"text": "This metric calculates the ratio of samples where the entire sequence is correctly predicted ; 3 ) Accuracy ( Acc ) :", "entities": [[17, 18, "MetricName", "Accuracy"], [19, 20, "MetricName", "Acc"]]}
{"text": "We evaluate this approach on product data from the e - commerce website Rakuten France , and find that the top proposal of the system is the normalized author name with 72 % accuracy .", "entities": [[33, 34, "MetricName", "accuracy"]]}
{"text": "Thus , we want to use the dataset of name entities to learn a specialized notion of similarity - this is known as distance metric learning ( Kulis et al , 2013 ) .", "entities": [[23, 25, "HyperparameterName", "distance metric"]]}
{"text": "The Siamese network is trained with contrastive loss ( Hadsell et al , 2006 ) in order to push the similarity towards 1 for similar pairs , and below a certain margin ( that we set to 0 ) for dissimilar pairs .", "entities": [[1, 3, "MethodName", "Siamese network"], [7, 8, "MetricName", "loss"], [37, 38, "DatasetName", "0"]]}
{"text": "The optimization is done using Adam ( Kingma and Ba , 2014 ) , with a learning rate of 10 \u22123 and a gradient clipping value of 5 .", "entities": [[5, 6, "MethodName", "Adam"], [16, 18, "HyperparameterName", "learning rate"], [23, 25, "MethodName", "gradient clipping"]]}
{"text": "The training is performed by minimizing the categorical cross - entropy loss , using teacher forcing ( Williams and Zipser , 1989 ) .", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "We find an accuracy of 79.8 % for the Siamese network , against 71.1 % for the n - gram baseline with n =", "entities": [[3, 4, "MetricName", "accuracy"], [9, 11, "MethodName", "Siamese network"]]}
{"text": "This approach alone reaches a top - 10 accuracy of 42 % on the entire test set , 26 % on a test set containing only names with initials , and 53 % on a test set containing only minor spelling mistakes .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "Ranking of the proposals With a decision threshold of p = 0.5 , the trained classifier has an accuracy of 93 % for both positive and negative candidates in the test set .", "entities": [[18, 19, "MetricName", "accuracy"]]}
{"text": "The metric used is the top - k accuracy on the ranked list of proposals for each book ; results are summarized in Table 2 .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "Excluding from the evaluation books where the ground truth for the author 's name equals the catalog value , this accuracy drops to 49 % .", "entities": [[20, 21, "MetricName", "accuracy"]]}
{"text": "Finally , for the combination of the above two restrictions , we find a top - 1 accuracy of 35 % .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "Distance metric learning with neural networks has been used for merging datasets on names ( Srinivas et al , 2018 ) , for normalizing job titles ( Neculoiu et al , 2016 ) , and for the disambiguation of researchers ( Zhang et al , 2018 ) .", "entities": [[0, 2, "HyperparameterName", "Distance metric"]]}
{"text": "This in - cludes constraints like Levenshtein distance as well as n - gram based measures such as BLEU , ME - TEOR and chRF ( Papineni et al , 2002 ; Denkowski and Lavie , 2014 ; Popovi\u0107 , 2015 ) .", "entities": [[18, 19, "MetricName", "BLEU"]]}
{"text": "TEXTFOOLER uses the Universal Sentence Encoder ( USE ) , which achieved a Pearson correlation score of 0.782 on the STS benchmark ( Cer et al , 2018 ) .", "entities": [[7, 8, "MethodName", "USE"], [13, 15, "MetricName", "Pearson correlation"], [20, 22, "DatasetName", "STS benchmark"]]}
{"text": "Recently , Zellers et al ( 2019 ) demonstrated that GROVER , a transformer - based text generation model , could classify its own generated news articles as human or machine - written with high accuracy .", "entities": [[16, 18, "TaskName", "text generation"], [35, 36, "MetricName", "accuracy"]]}
{"text": "If all generated examples were non - suspicious , judges would average 50 % accuracy , as they would not be able to distinguish between real and perturbed examples .", "entities": [[14, 15, "MetricName", "accuracy"]]}
{"text": "In this case , judges achieved 69.2 % accuracy .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "Participants were able to guess with 58.8 % accuracy whether inputs were computer - altered .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "The accuracy is over 10 % lower than the accuracy on the examples generated by TEXTFOOLER .", "entities": [[1, 2, "MetricName", "accuracy"], [9, 10, "MetricName", "accuracy"]]}
{"text": "Figure 2 plots the test accuracy over 10 training epochs , averaged over 5 random seeds per dataset .", "entities": [[5, 6, "MetricName", "accuracy"], [15, 16, "DatasetName", "seeds"]]}
{"text": "For example , the performance ( MRR ) of ROTATE model ( Sun et al , 2019 ) drops by 11 points ( absolute ) on WN18RR in this setting ( 3.4 ) .", "entities": [[6, 7, "MetricName", "MRR"], [26, 27, "DatasetName", "WN18RR"]]}
{"text": "For example , on the test - II evaluation subset of FB122 where all triples can be inferred by logical rules , Das et al ( 2020 ) scores quite low ( 63 MRR ) because of learning incorrect rules .", "entities": [[11, 12, "DatasetName", "FB122"], [33, 34, "MetricName", "MRR"]]}
{"text": "On the other hand , we score significantly higher ( 94.83 MRR ) demonstrating that we can learn more effective rules .", "entities": [[11, 12, "MetricName", "MRR"]]}
{"text": "The main summary of the results are ( i ) RotatE model converges to a much lower performance in the online setting losing at least 8 MRR points in FB122 and at least 11 points in WN18RR .", "entities": [[10, 11, "MethodName", "RotatE"], [26, 27, "MetricName", "MRR"], [29, 30, "DatasetName", "FB122"], [36, 37, "DatasetName", "WN18RR"]]}
{"text": "For forex trade data y , we apply a 3 - layer perceptron to access the trade data representation R t , and each layer is a non - linear transform with Relu activation function .", "entities": [[32, 33, "MethodName", "Relu"], [33, 35, "HyperparameterName", "activation function"]]}
{"text": "The loss function of the proposed model includes two parts : the negative log - likelihood training loss and the L 2 regularization item : Loss = \u2212f * log p ( f", "entities": [[1, 2, "MetricName", "loss"], [13, 16, "MetricName", "log - likelihood"], [17, 18, "MetricName", "loss"]]}
{"text": "+ \u03bb 2 \u03b8 2 2 ( 10 )", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "Then the data samples will be twice as large as no overlap condition ( In the USD - EUR ( 20 - 10 ) dataset , the number of samples will increase from 31k to 62k ) .", "entities": [[27, 30, "HyperparameterName", "number of samples"]]}
{"text": "We adopt the Adam ( Kingma and Ba , 2014 ) optimizer with the initial learning rate of 0.001 .", "entities": [[3, 4, "MethodName", "Adam"], [11, 12, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"]]}
{"text": "The batch size is 32 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "The learning rate begins to decay after 10 epoch .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "The Figure 4 shows BHAM - Category model 's performances ( macro - F1 % ) on USD - JPY pair under different combinations of input time and prediction delay .", "entities": [[11, 14, "MetricName", "macro - F1"]]}
{"text": "Experiments about time effect prove that the proper input time is about 40 minutes and the prediction accuracy declines with the increase of prediction delay .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "In our first model , we use the lexical translation model and probability function p t in Equation 1 as the weighting function , which can be learned efficiently off - line using the EM algorithm .", "entities": [[34, 35, "MetricName", "EM"]]}
{"text": "( 3 ) p \u0398 ( z i |", "entities": [[4, 5, "HyperparameterName", "\u0398"]]}
{"text": "bias i is then used to modify Equation 5 in the following way : f bias ( \u0398 ,", "entities": [[17, 18, "HyperparameterName", "\u0398"]]}
{"text": "We follow exactly the experimental setup and data splits from Richardson and Kuhn ( 2017b ) , and measure the accuracy at 1 ( Acc@1 ) , accuracy in top 10 ( Acc@10 ) , and MRR .", "entities": [[20, 21, "MetricName", "accuracy"], [27, 28, "MetricName", "accuracy"], [36, 37, "MetricName", "MRR"]]}
{"text": "As summarized in Figure 3 , our experiments show that training polyglot models on multiple datasets can lead to large improvements over training individual models , especially on the Py27 datasets where using a polyglot model resulted in a nearly 9 % average increase in accuracy @1 .", "entities": [[45, 46, "MetricName", "accuracy"]]}
{"text": "Beyond increases in accuracy , our polyglot models support zero - shot translation as shown in Figure 4 , which can be used for translating between unobserved language pairs ( e.g. , ( es , Clojure ) , ( ru , Haskell ) as shown in 1 - 2 ) , or for finding related functionality across different software projects ( as shown in 3 ) .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "A source sentence s and target sentence t form an edit pair s \u2212 t if f ( s , t ) > , where f is sentencelevel BLEU 4 without smoothing and = 0.1 in our case .", "entities": [[28, 29, "MetricName", "BLEU"]]}
{"text": "We use a training / validation / test split of 1 , 000K/10K/10 K edits , and train for 3 epochs with a fixed learning rate of 0.0001 , and a batch size of 128 .", "entities": [[24, 26, "HyperparameterName", "learning rate"], [31, 33, "HyperparameterName", "batch size"]]}
{"text": "We validate every 200 steps and select the model with the lowest validation loss .", "entities": [[13, 14, "MetricName", "loss"]]}
{"text": "One natural metric to consider is BLEU ( ( Papineni et al , 2002 ) ) .", "entities": [[6, 7, "MetricName", "BLEU"]]}
{"text": "BLEU shows high correlation with human judgement on machine translation ( Papineni et al , 2002 ; Doddington , 2002 ) .", "entities": [[0, 1, "MetricName", "BLEU"], [8, 10, "TaskName", "machine translation"]]}
{"text": "sentence - level BLEU with epsilon smoothing and equally weighted n - grams , with n up to 4 .", "entities": [[3, 4, "MetricName", "BLEU"], [5, 6, "HyperparameterName", "epsilon"]]}
{"text": "| WE ( s , s ) | , and F1 score , F 1 , WE ( s , h , s ) = 2 P WE R WE P WE", "entities": [[10, 12, "MetricName", "F1 score"]]}
{"text": "Notice that the parrot baseline is able to achieve a considerably high BLEU score , as expected , while the GPT - 2 baseline surprisingly achieves a high word edit recall score .", "entities": [[3, 4, "MethodName", "parrot"], [12, 14, "MetricName", "BLEU score"], [20, 21, "MethodName", "GPT"]]}
{"text": "We use a Bonferroni corrected \u03b1 = 0.0125 to determine significance .", "entities": [[5, 6, "HyperparameterName", "\u03b1"]]}
{"text": "Similarly to ( Faruqui et al , 2018 ) , for each sentence s i in the source document , we only look at the target sentences { t i\u2212k , ... , t i , ... , t i+k } , with k = 20 .", "entities": [[43, 45, "HyperparameterName", "k ="]]}
{"text": "If there are both an unmatched source sentence s and target sentence t , we consider them to form an edit s \u2212 t if f ( s , t ) > , where f is the BLEU score and = 0.1 .", "entities": [[37, 39, "MetricName", "BLEU score"]]}
{"text": "We use an embedding size of both the source and target words of 512 dimension , and use a batch size of 128 sentences .", "entities": [[19, 21, "HyperparameterName", "batch size"]]}
{"text": "Moreover , we use the Adam optimizer ( Kingma and Ba , 2015 ) with an initial learning rate of 0.0002 , and save the checkpoint every 1500 updates .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "optimizer"], [17, 19, "HyperparameterName", "learning rate"]]}
{"text": "Model training process stops after 8 checkpoints without improvements on the validation perplexity .", "entities": [[12, 13, "MetricName", "perplexity"]]}
{"text": "Following Niu et al ( 2018a ) , we select the 4 best checkpoint based on the validation perplexity values and combine them in a linear ensemble for decoding .", "entities": [[18, 19, "MetricName", "perplexity"]]}
{"text": "We evaluate the machine translation performance by using the case - sensitive BLEU score ( Papineni et al , 2002 ) with standard tokenization .", "entities": [[3, 5, "TaskName", "machine translation"], [12, 14, "MetricName", "BLEU score"]]}
{"text": "Table 4 shows the BLEU scores of the general NMT model and baseline NMT model on machine translation task .", "entities": [[4, 5, "MetricName", "BLEU"], [16, 18, "TaskName", "machine translation"]]}
{"text": "Table 5 shows the BLEU scores of the baseline NMT model , bi - directional NMT model , and multi - task neural model on the machine translation task between Turkish and English .", "entities": [[4, 5, "MetricName", "BLEU"], [26, 28, "TaskName", "machine translation"]]}
{"text": "The function of epochs and perplexity values on the validation dataset in different neural translation models are shown in Figure 3 .", "entities": [[5, 6, "MetricName", "perplexity"], [9, 11, "DatasetName", "validation dataset"]]}
{"text": "More details about the data are shown below : Table 7 shows the BLEU scores of the proposed multi - task neural model on using different external monolingual data .", "entities": [[13, 14, "MetricName", "BLEU"]]}
{"text": "Moreover , the proposed approach with external monolingual data is more useful for translating into the agglutinative language , which achieves an improvement of +1.42 BLEU points for translation from English into Turkish and +1.45 BLEU points from Chinese into Uyghur .", "entities": [[25, 26, "MetricName", "BLEU"], [35, 36, "MetricName", "BLEU"]]}
{"text": "Formally , for a model f ( ; \u03b8 ) parameterized by \u03b8 = { \u03b8 i } n i=1 where each \u03b8 i represents an individual parameter or a group of parameters , the method introduces a set of binary masks z , drawn from some distribution q ( z | \u03c0 ) parametrized by \u03c0 , and learns a sparse model f ( ; \u03b8 z ) by optimizing : min \u03c0 E q ( z | \u03c0 ) 1", "entities": [[8, 9, "HyperparameterName", "\u03b8"], [12, 13, "HyperparameterName", "\u03b8"], [15, 16, "HyperparameterName", "\u03b8"], [22, 23, "HyperparameterName", "\u03b8"], [66, 67, "HyperparameterName", "\u03b8"]]}
{"text": "+ \u03bb \u03b8 0 s.t.\u03b8 = \u03b8 z , ( 1 ) where is the Hadamard ( elementwise ) product , L ( ) is some task loss and \u03bb is a hyper - parameter .", "entities": [[2, 3, "HyperparameterName", "\u03b8"], [3, 4, "DatasetName", "0"], [6, 7, "HyperparameterName", "\u03b8"], [27, 28, "MetricName", "loss"]]}
{"text": "We use the Adam optimizer ( Kingma and Ba , 2014 ) and exploit the same learning rate schedule as Lample and Conneau ( 2019 ) .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [16, 18, "HyperparameterName", "learning rate"]]}
{"text": "Using mixed precision , we fit a batch of 128 for each GPU and the total batch size is 512 .", "entities": [[16, 18, "HyperparameterName", "batch size"]]}
{"text": "Training Corpus Size By comparing the validation perplexity on Swahili and Telugu in Figure 2 , we find that while both monolingual models outperform bilingual models in the first few epochs , the Swahili model 's perplexity starts to increase and is eventually surpassed by the bilingual model in later epochs .", "entities": [[7, 8, "MetricName", "perplexity"], [36, 37, "MetricName", "perplexity"]]}
{"text": "Particularly , we consider adding language - specific feedforward layers , attention layers , and residual adapter layers ( Rebuffi et al , 2017 ; Houlsby et al , 2019 ) , denoted as ffn , attn and adpt respectively .", "entities": [[11, 13, "HyperparameterName", "attention layers"]]}
{"text": "\u03b8 * = arg min \u03b8 1 L L i=1", "entities": [[0, 1, "HyperparameterName", "\u03b8"], [5, 6, "HyperparameterName", "\u03b8"]]}
{"text": "L i train ( \u03b8 , \u03c6 i ) , ( 2 ) where L i val and L i train denote the training and the validation MLM loss for the i - th language .", "entities": [[4, 5, "HyperparameterName", "\u03b8"], [27, 28, "DatasetName", "MLM"], [28, 29, "MetricName", "loss"]]}
{"text": "Since directly solving this problem can be prohibitive due to the expensive inner optimization , we approximate \u03b8 * by adapting the current \u03b8 ( t ) using a single gradient step , similar to techniques used in prior meta - learning methods ( Finn et al , 2017 ) .", "entities": [[17, 18, "HyperparameterName", "\u03b8"], [23, 24, "HyperparameterName", "\u03b8"], [39, 42, "TaskName", "meta - learning"]]}
{"text": "i , \u2207 \u03c6 ( t ) i 1 L L j=1 L j val ( \u03b8 ( t )", "entities": [[16, 17, "HyperparameterName", "\u03b8"]]}
{"text": "8 : end while based on the \u03b8 's validation MLM loss : \u03b8 ( t+1 ) GradientUpdate ( \u03b8 ( t ) , \u2207 \u03b8 ( t )", "entities": [[7, 8, "HyperparameterName", "\u03b8"], [10, 11, "DatasetName", "MLM"], [11, 12, "MetricName", "loss"], [13, 14, "HyperparameterName", "\u03b8"], [19, 20, "HyperparameterName", "\u03b8"], [25, 26, "HyperparameterName", "\u03b8"]]}
{"text": "i 1 L L j=1 L j val ( \u03b8 , \u03c6 ( t ) j ) \u03b8 = \u03b8 ( t )", "entities": [[9, 10, "HyperparameterName", "\u03b8"], [17, 18, "HyperparameterName", "\u03b8"], [19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "L i train ( \u03b8 ( t ) , \u03c6 ( t ) i ) , ( 3 ) where \u03b1 and \u03b2 are learning rates .", "entities": [[4, 5, "HyperparameterName", "\u03b8"], [20, 21, "HyperparameterName", "\u03b1"], [22, 23, "HyperparameterName", "\u03b2"]]}
{"text": "Particularly , by applying chain rule to the gradient of \u03c6 ( t ) i , we can observe that it contains a higher - order term : \u2207 2 \u03c6 ( t ) i , \u03b8 ( t )", "entities": [[36, 37, "HyperparameterName", "\u03b8"]]}
{"text": "L i train ( \u03b8 ( t ) , \u03c6 ( t ) i ) \u2207 \u03b8 1 L L j=1 L j val ( \u03b8 , \u03c6 ( t ) j ) ( 4 ) This is important , since it shows that \u03c6 i can obtain information from other languages through higherorder gradients .", "entities": [[4, 5, "HyperparameterName", "\u03b8"], [16, 17, "HyperparameterName", "\u03b8"], [25, 26, "HyperparameterName", "\u03b8"]]}
{"text": "This is because , in Eq . 3 , while \u2207 \u03b8 ( t ) is based on the i - th language only , the validation loss is computed for all languages .", "entities": [[11, 12, "HyperparameterName", "\u03b8"], [27, 28, "MetricName", "loss"]]}
{"text": "For NER , POS and QA , we search the following hyperparameters : batch size { 16 , 32 } ; learning rate { 2e - 5 , 3e - 5 , 5e - 5 } .", "entities": [[1, 2, "TaskName", "NER"], [13, 15, "HyperparameterName", "batch size"], [21, 23, "HyperparameterName", "learning rate"]]}
{"text": "For XNLI , we search for : batch size { 4 , 8 } ; encoder learning rate { 1e - 6 , 5e - 6 , 2e - 5 } ; classifier learning rate { 5e - 6 , 2e - 5 , 5e - 5 } .", "entities": [[1, 2, "DatasetName", "XNLI"], [7, 9, "HyperparameterName", "batch size"], [16, 18, "HyperparameterName", "learning rate"], [33, 35, "HyperparameterName", "learning rate"]]}
{"text": "2 ( 8 ) where \u03b8 \u00b1 = \u03b8 ( t ) \u00b1 \u2207 \u03b8 1 L L j=1 L j val ( \u03b8 , \u03c6 ( t ) j ) and is a small scalar .", "entities": [[5, 6, "HyperparameterName", "\u03b8"], [8, 9, "HyperparameterName", "\u03b8"], [14, 15, "HyperparameterName", "\u03b8"], [23, 24, "HyperparameterName", "\u03b8"]]}
{"text": "We use the same value for learning rates \u03b1 and \u03b2 in Eq 3 , to be consistent with standard learning rate schedule used in XLM ( Lample and Conneau , 2019 ) .", "entities": [[8, 9, "HyperparameterName", "\u03b1"], [10, 11, "HyperparameterName", "\u03b2"], [20, 22, "HyperparameterName", "learning rate"], [25, 26, "MethodName", "XLM"]]}
{"text": "We choose a subset of these measures using methods detailed in Section 6.1 , combine them with a limited set of features and use Support Vector Regression and Kernel Ridge Regression to generate a Similarity Score ( Section 6.2 ) .", "entities": [[35, 36, "MetricName", "Score"]]}
{"text": "This is captured in Equation 2 where score t represents the Similarity Score assigned to Type t by either of the measures detailed in Section 3 , count t represents the number of words of Type t in both sentences , w t the weight of Type t in the current iteration , and T is the total number of Types .", "entities": [[12, 13, "MetricName", "Score"]]}
{"text": "The system produced a Pearson correlation of 0.69 on the SemEval 2014 definitions data set .", "entities": [[4, 6, "MetricName", "Pearson correlation"]]}
{"text": "In our experiments , we employ an equivalent SVR formulation known as \u03bd - SVR ( Chang and Lin , 2002 ) , where \u03bd is the configurable proportion of support vectors to keep with respect to the number of samples in the data set .", "entities": [[38, 41, "HyperparameterName", "number of samples"]]}
{"text": "Each conv layer is equipped with Rectified Linear Units ( ReLU ) ( Nair and Hinton , 2010 ) as the activation function .", "entities": [[6, 9, "MethodName", "Rectified Linear Units"], [10, 11, "MethodName", "ReLU"], [21, 23, "HyperparameterName", "activation function"]]}
{"text": "The best performance in terms of both mean absolute error ( MAE ) and mean squared error ( MSE ) was attained by a string kernel based on the blended spectrum of 3 to 5 character n - grams .", "entities": [[11, 12, "MetricName", "MAE"], [18, 19, "MetricName", "MSE"]]}
{"text": "We optimize the hybrid CNN using the Adam optimization algorithm ( Kingma and Ba , 2015 ) with an initial learning rate of 10 \u22123 , chosen from the range [ 10 \u22125 , 10 \u22122 ] , a weight decay of 10 \u22127 , selected in the initial range [ 10 \u22129 , 10 \u22126 ] , and the learning rate decay of 0.999 .", "entities": [[7, 8, "MethodName", "Adam"], [20, 22, "HyperparameterName", "learning rate"], [39, 41, "MethodName", "weight decay"], [60, 62, "HyperparameterName", "learning rate"]]}
{"text": "The batch size , unanimously used , is 32 , with a training that goes on for a maximum of 50 epochs .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "We optimize the models using Adam , with an initial learning rate \u03b1 = 5 10 \u22125 .", "entities": [[5, 6, "MethodName", "Adam"], [10, 12, "HyperparameterName", "learning rate"], [12, 13, "HyperparameterName", "\u03b1"]]}
{"text": "Moving to the particularities of each of the three fine - tuned BERT models , we note a difference in the choice of the loss function , such that the first model employs the L 2 loss ( MSE ) , while the other two models use the L 1 loss ( MAE ) .", "entities": [[12, 13, "MethodName", "BERT"], [24, 25, "MetricName", "loss"], [36, 37, "MetricName", "loss"], [38, 39, "MetricName", "MSE"], [50, 51, "MetricName", "loss"], [52, 53, "MetricName", "MAE"]]}
{"text": "The XGBoost regressor , deemed optimal by the hyperparameter tuning , has default values for most parameters 2 , except for the number of estimators and the maximum tree depth .", "entities": [[22, 25, "HyperparameterName", "number of estimators"]]}
{"text": "We set the number of estimators to 100 for the latitude regressor and to 1000 for the longitude regressor .", "entities": [[3, 6, "HyperparameterName", "number of estimators"]]}
{"text": "Similarly , the maximum depth of the trees is 10 for the latitude model and 20 for the longitude one .", "entities": [[3, 5, "HyperparameterName", "maximum depth"]]}
{"text": "As evaluation metrics we use the Strict F1 score , which is commonly adopted for this task ( Segura - Bedmar et al , 2013 ) .", "entities": [[7, 9, "MetricName", "F1 score"]]}
{"text": "The SBO loss measures if it is possible to reconstruct each w i S m : n using the embeddings of the boundary words w m\u22121 and w n+1 . BioBERT ( Lee et al , 2020 ) , pretrained from a BERT checkpoint , on PubMed abstracts .", "entities": [[2, 3, "MetricName", "loss"], [42, 43, "MethodName", "BERT"]]}
{"text": "As a strong baseline , we used the TMRLeiden architecture ( Dirkson and Verberne , 2019 ) , which achieved the 2nd best Strict F1 - Score in the latest SMM4H shared task ( Weissenbacher et al , 2019 ) and is composed of a BiLSTM taking as input a concatenation of BERT and Flair embeddings ( Akbik et al , 2019 ) .", "entities": [[24, 27, "MetricName", "F1 - Score"], [30, 31, "DatasetName", "SMM4H"], [45, 46, "MethodName", "BiLSTM"], [52, 53, "MethodName", "BERT"]]}
{"text": "The results for the two datasets are shown in Table 1 ( we focus on the F1 - score , but Precision and Recall are reported in Appendix D ) .", "entities": [[16, 19, "MetricName", "F1 - score"], [21, 22, "MetricName", "Precision"], [23, 24, "MetricName", "Recall"]]}
{"text": "3 this new baccy is just making my cough so much worse but ahh well need my nicotine 6 what a great store @walmart is : i loss iq points , gained weight & got addicted to nicotine - all in under 15 min from going in ! !", "entities": [[27, 28, "MetricName", "loss"]]}
{"text": "The optimal parameters differ for each target : 0.015 - 0.3 for the slack variable ; standard hinge or squared hinge for the loss function ; and L2 norm for the penalty term .", "entities": [[23, 24, "MetricName", "loss"]]}
{"text": "The parameters for random forest are : 50 , 70 , or 90 for the number of trees ; 500 or All for the number of features to consider when looking for the best split ; 200 , 500 , or unlimited for the maximum depth of trees .", "entities": [[44, 46, "HyperparameterName", "maximum depth"]]}
{"text": "The optimal parameter range for different targets are : 80 - 100 for number of estimators ; 0.05 - 0.3 for learning rate ; false for warm start ; and 0.5 - 1.0 for subsample ratio .", "entities": [[13, 16, "HyperparameterName", "number of estimators"], [21, 23, "HyperparameterName", "learning rate"]]}
{"text": "The best parameters are different in each target : 7 - 29 for the number of neighbors ; default minority voting for class voting in most cases ; Modified Value Distance , Jeffrey divergence , and cosine distance for distance metric ; and gain ratio for feature weight in most cases .", "entities": [[39, 41, "HyperparameterName", "distance metric"]]}
{"text": "Our proposed system achieved a rank of 41 with an F1 score of 66.16 % .", "entities": [[10, 12, "MetricName", "F1 score"]]}
{"text": "The pre - training was done for 0.2 million steps with a batch size of 32 and a learning rate of 2e - 5 .", "entities": [[12, 14, "HyperparameterName", "batch size"], [18, 20, "HyperparameterName", "learning rate"]]}
{"text": "The model trained on the combined gold and silver dataset gave better performance ( F1 score : 66.13 % ) than the one trained only on the gold dataset ( F1 score : 65.66 % ) .", "entities": [[14, 16, "MetricName", "F1 score"], [30, 32, "MetricName", "F1 score"]]}
{"text": "We repeated this process for one more iteration with another random set of 10 , 000 samples ( F1 score : 66.34 % ) .", "entities": [[18, 20, "MetricName", "F1 score"]]}
{"text": "The evaluation measure for a sample is the F1 score between the predicted spans and the ground truth spans as defined in the SemEval - 2021 Task 5 paper ( Pavlopoulos et al , 2021 ) .", "entities": [[8, 10, "MetricName", "F1 score"]]}
{"text": "The RoBERTa model was followed by two dense layers with 512 and 128 units with relu ( Agarap , 2018 ) as the activation function and a dropout rate of 0.1 .", "entities": [[1, 2, "MethodName", "RoBERTa"], [15, 16, "MethodName", "relu"], [23, 25, "HyperparameterName", "activation function"]]}
{"text": "Finally , using two rounds of self - training helped us achieve our best F1 score , 66.34 % 3 .", "entities": [[14, 16, "MetricName", "F1 score"]]}
{"text": "One interesting observation that can be drawn from Table 1 is that for almost all the models , the recall remains constant and improvement in F1 is due to improvement in precision .", "entities": [[25, 26, "MetricName", "F1"]]}
{"text": "Figure 3 shows the variation of the F1 score across different toxic span lengths on the test dataset .", "entities": [[7, 9, "MetricName", "F1 score"]]}
{"text": "Our model achieved a very high F1 score when one ( Span Length 1 - 9 , Mean F1 Score : 83.17 % ) or two ( Span Length 10 - 17 , Mean F1 Score : 74.44 % ) words are marked as toxic in a text sample .", "entities": [[6, 8, "MetricName", "F1 score"], [18, 20, "MetricName", "F1 Score"], [34, 36, "MetricName", "F1 Score"]]}
{"text": "As the number of characters marked as toxic increases , the F1 score falls drastically , reaching as low as 24.82 % when more than 58 characters are marked as toxic .", "entities": [[11, 13, "MetricName", "F1 score"]]}
{"text": "This is also reflected in Figure 3 , as the mean F1 score of all the samples with zero span length is 11.42 % .", "entities": [[11, 13, "MetricName", "F1 score"]]}
{"text": "Experiments consider classification and generation tasks , yielding among other results a pruned model that is a 2.4x faster , 74 % smaller BERT on SQuAD v1 , with a 1 % drop on F1 , competitive both with distilled models in speed and pruned models in size .", "entities": [[23, 24, "MethodName", "BERT"], [25, 26, "DatasetName", "SQuAD"], [34, 35, "MetricName", "F1"]]}
{"text": "Movement pruning produces 77 % savings in parameter storage for a 1 % drop in accuracy on SQuAD v1.1 .", "entities": [[0, 2, "MethodName", "Movement pruning"], [15, 16, "MetricName", "accuracy"], [17, 18, "DatasetName", "SQuAD"]]}
{"text": "Results show a 2.4x speedup on SQuAD v1.1 with a 1 % drop of F1 , and a 2.3x speedup on QQP with a 1 % loss of F1 .", "entities": [[6, 7, "DatasetName", "SQuAD"], [14, 15, "MetricName", "F1"], [21, 22, "DatasetName", "QQP"], [26, 27, "MetricName", "loss"], [28, 29, "MetricName", "F1"]]}
{"text": "In implementations , this projection is made with the matrices in their folded tensor form In standard fine - tuning , starting from \u03b8 , we optimize the loss L ( for instance , cross - entropy for classification ) : arg min \u03b8 L ( \u03b8 ) Score - based pruning methods ( Ramanujan et al , 2019 ) modify the model by introducing score parameters S for each parameter i and replace the original parameter matrices with a masked version W = W M ( S ) .", "entities": [[23, 24, "HyperparameterName", "\u03b8"], [28, 29, "MetricName", "loss"], [43, 44, "HyperparameterName", "\u03b8"], [46, 47, "HyperparameterName", "\u03b8"], [48, 49, "MetricName", "Score"]]}
{"text": "= 1 ( S > \u03c4 ) for a threshold parameter \u03c4 , and optimizes a regularized objective , arg min \u03b8 , S L ( \u03b8 )", "entities": [[21, 22, "HyperparameterName", "\u03b8"], [26, 27, "HyperparameterName", "\u03b8"]]}
{"text": "Movement pruning , combined with distillation , has shown to be a very effective method to reduce the number of parameters in an existing model yielding 94 % pruning in our tests for a F1 of 87.5 on SQuAD v1.1 ( BERT - base is 88.5 ) .", "entities": [[0, 2, "MethodName", "Movement pruning"], [18, 21, "HyperparameterName", "number of parameters"], [34, 35, "MetricName", "F1"], [38, 39, "DatasetName", "SQuAD"], [41, 42, "MethodName", "BERT"]]}
{"text": "We report the performance on the development set as measured by the accuracy for MNLI and SST - 2 , F1 for QQP , the exact match ( EM ) and F1 for SQuAD and ROUGE for CNN / DailyMail .", "entities": [[12, 13, "MetricName", "accuracy"], [14, 15, "DatasetName", "MNLI"], [16, 17, "DatasetName", "SST"], [20, 21, "MetricName", "F1"], [22, 23, "DatasetName", "QQP"], [25, 27, "MetricName", "exact match"], [28, 29, "MetricName", "EM"], [31, 32, "MetricName", "F1"], [33, 34, "DatasetName", "SQuAD"]]}
{"text": "To measure inference speed on GPU , we use a 24 GB 3090 RTX and an Intel i7 CPU , using a large batch size ( 128 ) for evaluation and using PyTorch CUDA timing primitives .", "entities": [[23, 25, "HyperparameterName", "batch size"]]}
{"text": "We , therefore , use square block pruning on the attention layer , with a block size of ( 32 , 32 ) which showed the best tradeoff between performance and accuracy .", "entities": [[31, 32, "MetricName", "accuracy"]]}
{"text": "For instance , for the same F1 performance of 87.5 , Hybrid Filled models display a 2.5x speedup against 1.88 for TinyBERT .", "entities": [[6, 7, "MetricName", "F1"]]}
{"text": "TinyBERT and Distil - BERT have 50 % of BERT 's encoder parameters , whereas Hybrid Filled models have 25 % BERT parameters for the same level of accuracy .", "entities": [[4, 5, "MethodName", "BERT"], [9, 10, "MethodName", "BERT"], [21, 22, "MethodName", "BERT"], [28, 29, "MetricName", "accuracy"]]}
{"text": "For these experiments , we compare with MobileBERT , which uses a BERT - large teacher and reaches an F1 of 90.0 on SQuAD v1.1 on its fastest version .", "entities": [[7, 8, "MethodName", "MobileBERT"], [12, 13, "MethodName", "BERT"], [19, 20, "MetricName", "F1"], [23, 24, "DatasetName", "SQuAD"]]}
{"text": "Figure 2 shows that we have comparable results on SQuAD v1.1 , with a simpler optimization approach : we get a slightly better model ( F1=90.3 ) for the same speedup of 1.6x , and we get a speedup of 2.2x at BERT - base accuracy ( F1=88.5 ) .", "entities": [[9, 10, "DatasetName", "SQuAD"], [42, 43, "MethodName", "BERT"], [45, 46, "MetricName", "accuracy"]]}
{"text": "We observe that using a large teacher is beneficial even at high levels of pruning : up to 80 % of sparsity , the resulting student model has better accuracy for the same number of parameters when using a BERTlarge teacher instead of a base one .", "entities": [[29, 30, "MetricName", "accuracy"], [33, 36, "HyperparameterName", "number of parameters"]]}
{"text": "We observe similar results : a 18 % dense BERT - large has a F1 of 90.2 , with a speedup of 3.2x compared to BERT - large with a F1 of 93.2 .", "entities": [[9, 10, "MethodName", "BERT"], [14, 15, "MetricName", "F1"], [25, 26, "MethodName", "BERT"], [30, 31, "MetricName", "F1"]]}
{"text": "For a 17 % dense model , we obtain a F1 of 82.6 , whereas structured pruning gets a 25 % dense model with a F1 of 81.5 .", "entities": [[10, 11, "MetricName", "F1"], [25, 26, "MetricName", "F1"]]}
{"text": "Block Size Influence Figure 3 shows the impact of different block sizes on Block pruning : pruning is done on attention layers and FFNs with the same square block size , from ( 4 , 4 ) to ( 32 , 32 ) , with a BERT - base teacher .", "entities": [[20, 22, "HyperparameterName", "attention layers"], [46, 47, "MethodName", "BERT"]]}
{"text": "We can see that we reach the BERT - base original F1 for all block sizes from 4 to 32 , but with a speedup that increases with the block size .", "entities": [[7, 8, "MethodName", "BERT"], [11, 12, "MetricName", "F1"]]}
{"text": "The maximum reachable speedup without F1 drop is 1.3 for a block size of 32 .", "entities": [[5, 6, "MetricName", "F1"]]}
{"text": "But when some drop of F1 is allowed , the speedup increases quickly with the block size and plateau when reaching 16 .", "entities": [[5, 6, "MetricName", "F1"]]}
{"text": "We then reach a speedup of 1.75 for an F1 drop of 2 % and a block size of 32 .", "entities": [[9, 10, "MetricName", "F1"]]}
{"text": "Compress EM F1 As shown in Table 7 , combining hybrid pruning with distillation always performs better than pruning alone , but that it is not critical for the approach to work .", "entities": [[1, 2, "MetricName", "EM"], [2, 3, "MetricName", "F1"]]}
{"text": "We show here the effect of the pattern on the head number reduction : using block instead of row / column pruning leads to a much larger number of pruned heads while improving accuracy , here on the SST - 2 task .", "entities": [[33, 34, "MetricName", "accuracy"], [38, 39, "DatasetName", "SST"]]}
{"text": "APE for MT has been using automatic metrics , such as BLEU , to benchmark progress ( Libovick\u1ef3 et al , 2016 ) .", "entities": [[0, 1, "DatasetName", "APE"], [11, 12, "MetricName", "BLEU"]]}
{"text": "It relies on a convolutional neural network ( CNN ) classifier over words and on a deep neural network ( DNN ) trained by using a triplet ranking loss ( Bengio and Heigold , 2014 ; Wang et al , 2014 ; Weston et al , 2011 ) .", "entities": [[28, 29, "MetricName", "loss"]]}
{"text": "The Edition distance is computed as follows : SER = # In + # Sub + # Del # symbols in the ref erence word \u00d7 100 ( 1 ) where SER stands for Symbol Error rate , symbols correspond to the letters for orthographic representations , and to the phonemes for phonetic ones , and In , Sub and Del correspond respectively to insertion , substitution and deletion .", "entities": [[35, 36, "MetricName", "Error"]]}
{"text": "( 14 ) And K is a Epanechnikov kernel function ( Wand and Jones , 1994 ) with K =", "entities": [[18, 20, "HyperparameterName", "K ="]]}
{"text": "\u03b3 \u2212 x , x [ 0 , \u03b3 ) 0 , x \u2265 \u03b3 ( 15 ) where \u03b3 is the maximum Poincar\u00e9 distance between two points in the Poincar\u00e9 ball , which is d B ( p , 0 ) with p = 1 \u2212 ( = 10 \u22125 ) to avoid numerical errors .", "entities": [[0, 1, "HyperparameterName", "\u03b3"], [6, 7, "DatasetName", "0"], [8, 9, "HyperparameterName", "\u03b3"], [10, 11, "DatasetName", "0"], [14, 15, "HyperparameterName", "\u03b3"], [19, 20, "HyperparameterName", "\u03b3"], [40, 41, "DatasetName", "0"]]}
{"text": "W c and the bias b c are learnable parameters updated by minimizing the binary cross - entropy loss ( Liu et al , 2017 )", "entities": [[18, 19, "MetricName", "loss"]]}
{"text": "The proposed HYPERCAPS is evaluated on the four benchmark datasets by comparing with the six baselines in terms of P@k and nDCG@k with k = 1 , 3 , 5 .", "entities": [[23, 25, "HyperparameterName", "k ="]]}
{"text": "The performance on tail labels of the four benchmark datasets is evaluated in terms of nDCG@k with k = 1 , 3 , 5 .", "entities": [[17, 19, "HyperparameterName", "k ="]]}
{"text": "Figure 5 shows the results on EUR - LEX57 K in terms of P@k with k = 1 , 3 , 5 .", "entities": [[15, 17, "HyperparameterName", "k ="]]}
{"text": "al , 2019 ) applies a regularized loss specified for label co - occurrence .", "entities": [[7, 8, "MetricName", "loss"]]}
{"text": "( Hinton et al , 2018 ) proposes the EM algorithm based routing procedure between capsule layers .", "entities": [[9, 10, "MetricName", "EM"]]}
{"text": "k r t+k ( 1 ) where T is the maximal step , and \u03b3 is the discount factor .", "entities": [[14, 15, "HyperparameterName", "\u03b3"]]}
{"text": "Usually the parameters \u03b8 can be iteratively updated by policy gradient ( Williams , 1992 ) approach .", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "hi , t ; \u03b8 ) ( Gi , t \u2212b ) ( 2 ) where N is the number of sampled episodes in a batch , G i , t = T \u2212t k=0", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "| \u03c0 ( a | ht ; \u03b8 ) ) ( 3 ) where \u03c0 ( a | h t ; \u03b8 ) and \u03c0 ( a | h t ; \u03b8 ) are the policy distributions over A s and A s given the same dialog history h t .", "entities": [[7, 8, "HyperparameterName", "\u03b8"], [21, 22, "HyperparameterName", "\u03b8"], [31, 32, "HyperparameterName", "\u03b8"]]}
{"text": "As a result , the KL term in equation ( 3 ) can be defined as follows : KL ( \u03c0 ( a | ht ; \u03b8 )", "entities": [[26, 27, "HyperparameterName", "\u03b8"]]}
{"text": "As | k=1 \u03c0 ( a k | ht ; \u03b8 ) log\u03c0 ( a k | ht ; \u03b8 ) \u2212 log\u03c0 ( a k | ht ; \u03b8 ) ( 4 ) As the original policy parameters \u03b8 are fixed , the loss function in equation ( 3 ) can be rewritten as : L ( \u03b8 ; D , \u03b8 )", "entities": [[10, 11, "HyperparameterName", "\u03b8"], [19, 20, "HyperparameterName", "\u03b8"], [29, 30, "HyperparameterName", "\u03b8"], [39, 40, "HyperparameterName", "\u03b8"], [44, 45, "MetricName", "loss"], [58, 59, "HyperparameterName", "\u03b8"], [62, 63, "HyperparameterName", "\u03b8"]]}
{"text": "As | k=1 \u03c0 ( a k | ht ; \u03b8 ) log\u03c0 ( a k | ht ; \u03b8 ) ( 5 )", "entities": [[10, 11, "HyperparameterName", "\u03b8"], [19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "A s | k=1 1 { a k = a l } log \u03c0 ( a k | ht ; \u03b8 ) ( 6 ) Where 1 { } is the indicate function .", "entities": [[7, 9, "HyperparameterName", "k ="], [20, 21, "HyperparameterName", "\u03b8"]]}
{"text": "Equation ( 6 ) suggests the new dialog manager \u03c0 ( \u03b8 ) will be penalized if it violates the instructions defined by the dialog rules .", "entities": [[11, 12, "HyperparameterName", "\u03b8"]]}
{"text": "= L ( \u03b8 ; D , R ) if ht HR ; L ( \u03b8 ; D , \u03b8 ) else ( 7 ) When the dialog context h t in the t - th turn satisfies a condition defined in H R , we distill knowledge of rules into the new system .", "entities": [[3, 4, "HyperparameterName", "\u03b8"], [15, 16, "HyperparameterName", "\u03b8"], [19, 20, "HyperparameterName", "\u03b8"]]}
{"text": "Discounted factor \u03b3 = 0.9 .", "entities": [[2, 3, "HyperparameterName", "\u03b3"]]}
{"text": "The batch size N is set to be 32 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "With appropriate training techniques , our model is able to achieve 0.9218 F1score on public validation set and the ensemble version settles at top 9 F1 - score ( 0.9005 ) and top 2 Recall ( 0.9301 ) on private test set .", "entities": [[25, 28, "MetricName", "F1 - score"], [34, 35, "MetricName", "Recall"]]}
{"text": "Our best model results in a high F1 - score of 0.9005 on the task 's private test dataset and that of 0.9218 on the public validation set with Multilayer Perceptron Head .", "entities": [[7, 10, "MetricName", "F1 - score"]]}
{"text": "Hence , we employ warm - up learning rate scheduler ( Howard and Ruder , 2018 ) to help the model converge faster while preserving its good initialization .", "entities": [[7, 9, "HyperparameterName", "learning rate"]]}
{"text": "y k = y k ( 1 \u2212 \u03b1 ) + \u03b1 / K , where y k is output probabilities of K classes .", "entities": [[1, 3, "HyperparameterName", "k ="], [8, 9, "HyperparameterName", "\u03b1"], [11, 12, "HyperparameterName", "\u03b1"]]}
{"text": "In that case , the loss will become 1 log 0", "entities": [[5, 6, "MetricName", "loss"], [10, 11, "DatasetName", "0"]]}
{"text": "By setting \u03b1 = 0 , this problem can be solved .", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [4, 5, "DatasetName", "0"]]}
{"text": "We trained our networks with PyTorch framework on GPU GeForce GTX 2080Ti with batch size 32 for 20 epochs .", "entities": [[13, 15, "HyperparameterName", "batch size"]]}
{"text": "We used AdamW ( Loshchilov and Hutter , 2017 ) for the optimization and a learning rate of 3e \u2212 5 , decayed 0.01 except for LayerNorm layers .", "entities": [[2, 3, "MethodName", "AdamW"], [15, 17, "HyperparameterName", "learning rate"]]}
{"text": "Label smoothing hyperparameter \u03b1 was empirically experimented with multiple values of 0 , 0.1 , 0.15 , 0.2 and the last value possessed promising results .", "entities": [[0, 2, "MethodName", "Label smoothing"], [3, 4, "HyperparameterName", "\u03b1"], [11, 12, "DatasetName", "0"]]}
{"text": "We ended up at top 9 on the leaderboard with 0.9005 F1 - score and", "entities": [[11, 14, "MetricName", "F1 - score"]]}
{"text": "White et al , 1994 ; Koehn and Monz , 2006 ; Callison - Burch et al , 2007 ; Roturier and Bensadoun , 2011 ; Graham et al , 2013 ; Barrault et al , 2019 ) , and all of them rely on at least one of the three translation quality criteria : comprehensibility ( comprehension , intelligibility ) , adequacy ( fidelity , semantic accuracy ) , and fluency ( grammaticality ) .", "entities": [[67, 68, "MetricName", "accuracy"]]}
{"text": "In the 3rd Workshop on Computational Approaches to Linguistic Code - Switching Shared Task , we achieved second place with 62.76 % harmonic mean F1 - score for English - Spanish language pair without using any gazetteer and knowledge - based information .", "entities": [[24, 27, "MetricName", "F1 - score"]]}
{"text": "We used batch size equals to 64 .", "entities": [[2, 4, "HyperparameterName", "batch size"]]}
{"text": "Adam Optimizer was chosen with an initial learning rate of 0.01 .", "entities": [[0, 1, "MethodName", "Adam"], [1, 2, "HyperparameterName", "Optimizer"], [7, 9, "HyperparameterName", "learning rate"]]}
{"text": "We applied time - based decay of \u221a 2 decay rate and stop after two consecutive epochs without improvement .", "entities": [[9, 11, "HyperparameterName", "decay rate"]]}
{"text": "We tuned our model with the development set and evaluated our best model with the test set using harmonic mean F1 - score metric with the script provided by Aguilar et al ( 2018 ) .", "entities": [[20, 23, "MetricName", "F1 - score"]]}
{"text": "High perplexity ( PPL ) and a low BLEU ( Papineni et al , 2002 ) score may suggest novelty , but they are not sufficient for testing for originality .", "entities": [[1, 2, "MetricName", "perplexity"], [8, 9, "MetricName", "BLEU"]]}
{"text": "The binary search in F ( line 10 ) has a runtime complexity of \u03b8 ( log n ) .", "entities": [[14, 15, "HyperparameterName", "\u03b8"]]}
{"text": "Depending on the result of the binary search of F ( line 10 ) there may be an insertion to F ( line 14 ) which has a runtime complexity of \u03b8 ( log n ) .", "entities": [[31, 32, "HyperparameterName", "\u03b8"]]}
{"text": "The code segment of lines 21 - 26 takes \u03b8 ( n ) time because the number of wl - token fragments in the ground truth dataset ( of n sentences where each sentence consists of c tokens on average ) is at most cn .", "entities": [[9, 10, "HyperparameterName", "\u03b8"]]}
{"text": "Overall , Transformer - based models are capable of reducing the RMSE by up tp 6.5 % , with respect to previous approaches .", "entities": [[2, 3, "MethodName", "Transformer"], [11, 12, "MetricName", "RMSE"]]}
{"text": "With this approach , DistilBERT is able to retain 95 % of BERT 's performance on several language understanding tasks using about half the number of parameters of BERT .", "entities": [[4, 5, "MethodName", "DistilBERT"], [12, 13, "MethodName", "BERT"], [24, 27, "HyperparameterName", "number of parameters"], [28, 29, "MethodName", "BERT"]]}
{"text": "Table 1 displays the Mean Absolute Error ( MAE ) and the Root Mean Squared Error ( RMSE ) of the different configurations ; the error is the difference between the IRT difficulty and the estimation of the Transformer model .", "entities": [[6, 7, "MetricName", "Error"], [8, 9, "MetricName", "MAE"], [15, 16, "MetricName", "Error"], [17, 18, "MetricName", "RMSE"], [38, 39, "MethodName", "Transformer"]]}
{"text": "Table 2 and Table 3 show the results of the experiments on QDE for CloudAcademy and ASSISTments , by displaying the MAE and the RMSE obtained on Q TEST by the Transformer models and the chosen baselines ( all the possible input configurations are considered for the baselines ) .", "entities": [[21, 22, "MetricName", "MAE"], [24, 25, "MetricName", "RMSE"], [31, 32, "MethodName", "Transformer"]]}
{"text": "In order to understand how well the models estimate the difficulty of very easy and very challenging questions , we also show the MAE and RMSE measured on the questions whose difficulty b is such that | b | > 2 ( also referred to as \" extreme \" questions ) .", "entities": [[23, 24, "MetricName", "MAE"], [25, 26, "MetricName", "RMSE"]]}
{"text": "Table 2 shows that also R2DE and ELMo are able to leverage the text of the possible choices to improve the accuracy of the estimation ; indeed , the best performing input configuration is Q+all for R2DE and Q+correct for ELMo .", "entities": [[7, 8, "MethodName", "ELMo"], [21, 22, "MetricName", "accuracy"], [40, 41, "MethodName", "ELMo"]]}
{"text": "All models have larger errors on \" extreme \" questions than on general ones , but the increase is different for each of them : the best performing model has an increase in the MAE of 1.19 , which is lower than the other Transformers ( from 1.28 to 1.41 ) , ELMo ( 1.37 ) and R2DE ( 1.52 ) .", "entities": [[34, 35, "MetricName", "MAE"], [52, 53, "MethodName", "ELMo"]]}
{"text": "Results are similar for the RMSE : the increase is 1.19 for the best model , between 1.19 and 1.30 for the other Transformers , 1.23 for ELMo ( Q+correct ) and 1.37 for R2DE ( Q+all ) .", "entities": [[5, 6, "MetricName", "RMSE"], [27, 28, "MethodName", "ELMo"]]}
{"text": "From the average errors for the different types of questions , we observed that both BERT and R2DE perform slightly worse on cloze questions : BERT 's MAE is 0.804 on cloze questions and 0.756 on the other questions ; similarly , R2DE 's MAE is 0.893 on cloze questions and 0.794 on the other questions .", "entities": [[15, 16, "MethodName", "BERT"], [25, 26, "MethodName", "BERT"], [27, 28, "MetricName", "MAE"], [44, 45, "MetricName", "MAE"]]}
{"text": "For BERT , the overall MAE is 0.774 and on the questions with multiple correct choices it is 0.764 ; in the case of R2DE , the MAE is 0.813 overall and 0.750 for questions with multiple correct choices .", "entities": [[1, 2, "MethodName", "BERT"], [5, 6, "MetricName", "MAE"], [27, 28, "MetricName", "MAE"]]}
{"text": "For fine - tuning on QDE from text we select the hyperparameters from the following pool of candi - dates : batch size = 16 , 32 , 64 ; learning rate = 1e - 5 , 2e - 5 , 3 - 5 ; patience early stopping = 10 epochs ; dropout additional layer = 0.1 , 0.2 , 0.3 , 0.4 , 0.5 ; internal dropout = 0.1 , 0.2 , 0.3 , 0.4 , 0.5 .", "entities": [[21, 23, "HyperparameterName", "batch size"], [30, 32, "HyperparameterName", "learning rate"], [46, 48, "MethodName", "early stopping"]]}
{"text": "For the additional pre - training on MLM , the hyperparameters are selected between the following candidates : batch size = 64 ; learning rate = 1e - 5 ; number of epochs = 4 , 12 , 24 , 36 ; dropout = 0.1 .", "entities": [[7, 8, "DatasetName", "MLM"], [18, 20, "HyperparameterName", "batch size"], [23, 25, "HyperparameterName", "learning rate"], [30, 33, "HyperparameterName", "number of epochs"]]}
{"text": "In both cases we use sequence length = 256 and the Adam optimizer .", "entities": [[11, 12, "MethodName", "Adam"], [12, 13, "HyperparameterName", "optimizer"]]}
{"text": "We model question difficulty as defined in the oneparameter IRT model ( also named Rasch model ( Rasch , 1960 ) ) , which associates a skill level \u03b8 to each student and a difficulty level b to each question .", "entities": [[28, 29, "HyperparameterName", "\u03b8"]]}
{"text": "Also , if a question is too difficult or too easy ( i.e. b j or b j \u2212 ) , all the students will answer in the same way ( i.e. P C 0 or P C 1 ) regardless of \u03b8 i .", "entities": [[34, 35, "DatasetName", "0"], [42, 43, "HyperparameterName", "\u03b8"]]}
{"text": "Once Q is constructed , topic recovery requires O ( KV 2 + K 2 V I ) , where K is the number of topics , V is the vocabulary size , and I is the average number of iterations ( typically 100 - 1000 ) .", "entities": [[38, 41, "HyperparameterName", "number of iterations"]]}
{"text": "Thus , we do not aim for state - of - the - art accuracy , 2 but the experiment shows title - based tandem anchors yield topics closer to the underlying classes than Gram - Schmidt anchors .", "entities": [[14, 15, "MetricName", "accuracy"]]}
{"text": "We use our own implementation of Gibbs sampling with fixed topics and a symmetric documenttopic Dirichlet prior with concentration \u03b1 = .01 .", "entities": [[19, 20, "HyperparameterName", "\u03b1"]]}
{"text": "We then train a hinge - loss linear classifier on the newsgroup labels using Vowpal Wabbit 4 with topic - word pairs as features .", "entities": [[6, 7, "MetricName", "loss"]]}
{"text": "5 Since 20NEWS has twenty classes , accuracy alone does not capture confusion between closely related newsgroups .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "The first clustering metric is the adjusted Rand index ( Yeung and Ruzzo , 2001 ) , which is akin to accuracy for clustering , as it gives the percentage of correct pairing decisions from a reference clustering .", "entities": [[21, 22, "MetricName", "accuracy"]]}
{"text": "Topics from user generated multiword anchors yield higher classification accuracy ( Figure 3 ) .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "Not only is our approach more scalable than the Interactive Topic Model , but we also achieve Figure 3 : Classification accuracy and coherence using topic features gleaned from user provided multiword and single word anchors .", "entities": [[20, 21, "TaskName", "Classification"], [21, 22, "MetricName", "accuracy"]]}
{"text": "higher classification accuracy than Hu et al ( 2014 ) .", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "10 Topic coherence is based solely on the top n words of a topic , while both accuracy and topic significance depend on the entire topicword distributions .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "We denote a relation as an Exact Match if the same ( head , predicate , tail ) triple appears in the train set ; as a Partial 1 More implementation details in Appendix A Match if one of its arguments appears in the same position in a training relation of same type ; and as New otherwise .", "entities": [[6, 8, "MetricName", "Exact Match"]]}
{"text": "We report micro - averaged Precision , Recall and F1 scores for both NER and RE in Table 1 .", "entities": [[5, 6, "MetricName", "Precision"], [7, 8, "MetricName", "Recall"], [9, 10, "MetricName", "F1"], [13, 14, "TaskName", "NER"]]}
{"text": "For ACE05 , we use the standard bert - base - uncased LM but use the bert - base - cased version on CoNLL04 which results in a significant +2.4 absolute improvement in RE Strict micro F1 score .", "entities": [[35, 37, "MetricName", "micro F1"]]}
{"text": "We present a more complete report of overall Precision , Recall and F1 scores that can be interpreted in light of these statistics in Table 6 .", "entities": [[8, 9, "MetricName", "Precision"], [10, 11, "MetricName", "Recall"], [12, 13, "MetricName", "F1"]]}
{"text": "Notably , HacRED is one of the largest Chinese document - level RE datasets and achieves a high 96 % F1 score on data quality .", "entities": [[20, 22, "MetricName", "F1 score"]]}
{"text": "Recently , various models ( Zeng et al , 2018 ; Takanobu et al , 2019 ; Fu et al , 2019 ; have been proposed to identify the relational facts and achieved state - of - theart ( SOTA ) performance , among which the latest method CasRel achieves notable 91.8 % F1 score on WebNLG ( Gardent et al , 2017 ) and 89.6 % on NYT ( Riedel et al , 2010 ) .", "entities": [[53, 55, "MetricName", "F1 score"], [56, 57, "DatasetName", "WebNLG"]]}
{"text": "The F1 scores under these scenarios drop significantly from 89.3 % to 62.8 % .", "entities": [[1, 2, "MetricName", "F1"]]}
{"text": "It achieves 89.3 F1 scores and has won the champion in the Competition of DuIE held by Baidu Inc. ( 3 ) Document - level Relation Classification Models : LSR ( Nan et al , 2020 ) is a model that empowers the relational reasoning across sentences by automatically inducing the latent document - level graph .", "entities": [[3, 4, "MetricName", "F1"], [25, 27, "TaskName", "Relation Classification"], [43, 45, "TaskName", "relational reasoning"]]}
{"text": "Meanwhile , we randomly sample 400 instances from HacRED and compute the precision , recall , and F1 score with annotations based on the revision of humans .", "entities": [[17, 19, "MetricName", "F1 score"]]}
{"text": "Figure 3 shows the F1 curve of the performances w.r . in performance .", "entities": [[4, 5, "MetricName", "F1"]]}
{"text": "The SOTA model CasRel still outperforms other joint models and achieves great F1 on 100 % D ec .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "The precision , recall , and F1 score of the three major categories of models are shown in Table 8 .", "entities": [[6, 8, "MetricName", "F1 score"]]}
{"text": "For the NER task , PURE has a separate entity model but results in a 30.61 % F1 when all entities in a document are considered , including entities with no positive relation labels .", "entities": [[2, 3, "TaskName", "NER"], [17, 18, "MetricName", "F1"]]}
{"text": "Table 11 shows the F1 score of existing models when extracting from texts with different number of triples .", "entities": [[4, 6, "MetricName", "F1 score"]]}
{"text": "However , all models get F1 score below average when text mentions have more than 16 triples .", "entities": [[5, 7, "MetricName", "F1 score"]]}
{"text": "Specifically , the F1 scores of overlapping head and tail mentions are 66.38 % and 47.44 % respectively .", "entities": [[3, 4, "MetricName", "F1"]]}
{"text": "As illustrated in Table 10 , we observe a drop of the F1 .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "[ l ( \u2212g ( x ) ) ] } ( 1 ) where \u03c0 p = p ( y = 1 ) , g is decision function , l is surrogate loss function .", "entities": [[32, 33, "MetricName", "loss"]]}
{"text": "| | 2 + \u03b1Jg ( 9 ) where \u03b1 is the balancing factor and J g is defined in ( 6 ) .", "entities": [[9, 10, "HyperparameterName", "\u03b1"]]}
{"text": "\u2212 r f ( \u00fb f t ) | | 2 + \u03b1Jg ( 10 ) where \u03b1 is the balancing factor .", "entities": [[17, 18, "HyperparameterName", "\u03b1"]]}
{"text": "w h\u0175 ( 11 ) where h\u0175 = 2 k=\u22122 : k = 0 w i+k represents the context vector and N denotes the utterance length .", "entities": [[11, 13, "HyperparameterName", "k ="], [13, 14, "DatasetName", "0"]]}
{"text": "We demonstrate the results for our proposed algorithms and other competing algorithms in Table 2 , from which we can easily conclude that that ( i ) our Decoupled NBT does not affect the performance , and ( ii ) our cross - lingual NBT framework is able to achieve significantly better accuracy for both languages in both parallel - resource scenarios .", "entities": [[52, 53, "MetricName", "accuracy"]]}
{"text": "Error Analysis Here we showcase the most frequent error types in subsection 6.1 .", "entities": [[0, 1, "MetricName", "Error"]]}
{"text": "From the table experimental results are not very sensitive to \u03b1 , a dramatic change of \u03b1 will not harm the final results too much , we simply choose \u03b1 = 1 as the hyper - parameter .", "entities": [[10, 11, "HyperparameterName", "\u03b1"], [16, 17, "HyperparameterName", "\u03b1"], [29, 30, "HyperparameterName", "\u03b1"]]}
{"text": "Experiments show that our model achieves 79.9 F1 on the DROP hidden test set , creating new state - of - the - art results .", "entities": [[7, 8, "MetricName", "F1"], [10, 11, "DatasetName", "DROP"]]}
{"text": "Our MTMSN model outperforms all existing approaches on the DROP hidden test set by achieving 79.9 F1 score , a 32.9 % absolute gain over prior best work at the time of submission .", "entities": [[9, 10, "DatasetName", "DROP"], [16, 18, "MetricName", "F1 score"]]}
{"text": "Again , MTMSN surpasses it by obtaining a 13.2 F1 increase on the development set .", "entities": [[9, 10, "MetricName", "F1"]]}
{"text": "We use Adam optimizer with a learning rate of 3e - 5 and warmup over the first 5 % steps to train .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "optimizer"], [6, 8, "HyperparameterName", "learning rate"]]}
{"text": "The maximum number of epochs is set to 10 for base models and 5 for large models , while the batch size is 12 or 24 respectively .", "entities": [[2, 5, "HyperparameterName", "number of epochs"], [20, 22, "HyperparameterName", "batch size"]]}
{"text": "As illustrated in Table 2 , the use of addition and subtraction is extremely crucial : the EM / F1 performance of both the base and large models drop drastically by more than 20 points if it is removed .", "entities": [[17, 18, "MetricName", "EM"], [19, 20, "MetricName", "F1"]]}
{"text": "Moreover , enhancing the model with the negation type significantly increases the F1 by roughly 9 percent on both models .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "Specifically , we find that removing the question and passage vectors from all involved computation leads to 1.3 % drop on F1 .", "entities": [[21, 22, "MetricName", "F1"]]}
{"text": "Finally , we share parameters between the arithmetic expression component and the negation component , and find the performance drops by 1.1 % on F1 .", "entities": [[24, 25, "MetricName", "F1"]]}
{"text": "Moreover , significant improvements are also obtained in the multi - span category , where the F1 score increases by more than 40 points .", "entities": [[16, 18, "MetricName", "F1 score"]]}
{"text": "Error analysis Finally , to better understand the remaining challenges , we randomly sample 100 incorrectly predicted examples based on EM and categorize them into 7 classes .", "entities": [[0, 1, "MetricName", "Error"], [20, 21, "MetricName", "EM"]]}
{"text": "Our model achieves 79.9 F1 on the DROP hidden test set , creating new state - of - the - art results .", "entities": [[4, 5, "MetricName", "F1"], [7, 8, "DatasetName", "DROP"]]}
{"text": "We achieve an absolute improvement of +1.5 and +16.0 in ROUGE1 F1 ( R1 F1 ) on the development and test sets , respectively , compared to the system which does not rely on data augmentation .", "entities": [[11, 12, "MetricName", "F1"], [14, 15, "MetricName", "F1"], [34, 36, "TaskName", "data augmentation"]]}
{"text": "To fit the model to the GPU cluster , a batch size equal to 4 , 096 is selected for training .", "entities": [[10, 12, "HyperparameterName", "batch size"]]}
{"text": "The validation batch size is set to 8 .", "entities": [[2, 4, "HyperparameterName", "batch size"]]}
{"text": "We use an initial learning rate of 2 , drop out of 0.2 and 8 , 000 warm - up steps .", "entities": [[4, 6, "HyperparameterName", "learning rate"]]}
{"text": "To evaluate the proposed algorithms , we use ROUGE ( Recall - Oriented Understudy for Gisting Evaluation ) score , which is a popular metric for text summarization task , and has several variants like ROUGE - N , and ROUGE - L , which measure the overlap of n - grams between the system and reference summary ( LIN , 2004 ) .", "entities": [[10, 11, "MetricName", "Recall"], [26, 28, "TaskName", "text summarization"], [40, 43, "MetricName", "ROUGE - L"]]}
{"text": "We use ROUGE 1 F1 ( R1 F1 ) , ROUGE 2 F1 ( R2 F1 ) , and ROUGE L F1 ( RL F1 ) for scoring the generated summary .", "entities": [[4, 5, "MetricName", "F1"], [7, 8, "MetricName", "F1"], [12, 13, "MetricName", "F1"], [15, 16, "MetricName", "F1"], [21, 22, "MetricName", "F1"], [24, 25, "MetricName", "F1"]]}
{"text": "In addition , we also use the SacreBLEU 4 evaluation metric ( Post , 2018 ) .", "entities": [[7, 8, "MetricName", "SacreBLEU"]]}
{"text": "These approaches rely on additional annotations / cues such as human - based attention maps ( Das et al , 2017 ) , textual explanations ( Huk Park et al , 2018 ) and object label predictions ( Ren et al , 2015 ) to identify relevant regions , and train the model to base its predictions on those regions , showing large improvements ( 8 - 10 % accuracy ) on the VQA - CPv2 dataset .", "entities": [[69, 70, "MetricName", "accuracy"], [73, 74, "TaskName", "VQA"]]}
{"text": "The question - only model is either used to perform adversarial regularization ( Grand and Belinkov , 2019 ; Ramakrishnan et al , 2018 ) or to re - scale the loss based on the difficulty of the question ( Cadene et al , 2019 ) .", "entities": [[31, 32, "MetricName", "loss"]]}
{"text": "However , when these ideas are applied to the UpDn model ( Anderson et al , 2018 ) , which attempts to learn correct visual grounding , these approaches achieve 4 - 7 % lower accuracy compared to the state - of - the - art methods .", "entities": [[24, 26, "TaskName", "visual grounding"], [35, 36, "MetricName", "accuracy"]]}
{"text": "HINT proposes a ranking loss between humanbased importance scores ( Das et al , 2016 ) and the gradient - based sensitivities .", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "( 2 ) Existing methods propose the following training objectives to improve grounding using S : HINT uses a ranking loss , which penalizes the model if the pair - wise rankings of the sensitivities of visual regions towards ground truth answers a gt are different from the ranks computed from the human - based attention maps .", "entities": [[20, 21, "MetricName", "loss"]]}
{"text": "Both methods improve baseline accuracy by 8 - 10 % .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "We then average the accuracy of each subset across 5 runs , obtaining 5000 values .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "Using a confidence level of 95 % ( \u03b1 = 0.05 ) , we fail to reject the null hypothesis that the mean difference between the paired values is 0 , showing that the variants are not statistically significantly different from each other .", "entities": [[8, 9, "HyperparameterName", "\u03b1"], [29, 30, "DatasetName", "0"]]}
{"text": "As shown in Table 1 , the baseline method has the highest training results , while the other methods cause 6.0 \u2212 14.0 % and 3.3\u221210.5 % drops in the training accuracy on VQA - CPv2 and VQAv2 , respectively .", "entities": [[31, 32, "MetricName", "accuracy"], [33, 34, "TaskName", "VQA"]]}
{"text": "To test this hypothesis , we devise a simple loss function which continuously degrades the training accuracy by training the network to always predict a score of zero for all possible answers i.e. produce a zero vector ( 0 ) .", "entities": [[9, 10, "MetricName", "loss"], [16, 17, "MetricName", "accuracy"], [38, 39, "DatasetName", "0"]]}
{"text": "The overall loss function can be written as : L : = BCE ( P ( A ) , A gt ) + \u03bbBCE ( P ( A ) , 0 ) , where , BCE refers to the binary cross entropy loss and P ( A ) is a vector consisting of predicted scores for all possible answers .", "entities": [[2, 3, "MetricName", "loss"], [30, 31, "DatasetName", "0"], [42, 43, "MetricName", "loss"]]}
{"text": "The first term is the binary cross entropy loss between model predictions and ground truth answer vector ( A gt ) , and the second term is our regularizer with a coefficient of \u03bb = 1 .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "As shown in Table 1 , we present results when this loss is used on : a ) Fixed subset covering 1 % of the dataset , b ) Varying subset covering 1 % of the dataset , where a new random subset is sampled every epoch and c ) 100 % of the dataset .", "entities": [[11, 12, "MetricName", "loss"]]}
{"text": "For all variants , we fine - tune a pretrained UpDn , which was trained on either VQA - CPv2 or VQAv2 for 40 epochs with a learning rate of 10 \u22123 .", "entities": [[17, 18, "TaskName", "VQA"], [27, 29, "HyperparameterName", "learning rate"]]}
{"text": "When fine - tuning with HINT , SCR or our method , we also use the main binary cross entropy VQA loss , whose weight is set to 1 .", "entities": [[20, 21, "TaskName", "VQA"], [21, 22, "MetricName", "loss"]]}
{"text": "The batch size is set to 384 for all of the experiments .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "The learning rate is set to 2 \u00d7 10 \u22125 and the weight for the HINT loss is set to 2 .", "entities": [[1, 3, "HyperparameterName", "learning rate"], [16, 17, "MetricName", "loss"]]}
{"text": "For the first phase , which strengthens the influential objects , we use a learning rate of 5 \u00d7 10 \u22125 , loss weight of 3 and train the model to a maximum of 12 epochs .", "entities": [[14, 16, "HyperparameterName", "learning rate"], [22, 23, "MetricName", "loss"]]}
{"text": "For the second phase , we use a learning rate of 10 \u22124 and weight of 1000 , which is applied alongside the loss term used in the first phase .", "entities": [[8, 10, "HyperparameterName", "learning rate"], [23, 24, "MetricName", "loss"]]}
{"text": "The weight for the loss is set to 2 .", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "A6 shows the results when we apply our loss to 1 \u2212 100 % of the training instances .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "Clearly , the ability to regularize the model does not vary much with respect to the size of the train subset , with the best performance occurring when our loss is applied to 1 % of the training instances .", "entities": [[29, 30, "MetricName", "loss"]]}
{"text": "So eliminating the morphological feature inf from the subrelation acl : inf does not lead to any information loss .", "entities": [[18, 19, "MetricName", "loss"]]}
{"text": "4 We then fine - tune the resulting ELECTRA model on the GLUE tasks using the multimodal vectors following standard fine - tuning protocols ; that is , we add a linear layer with a softmax to the pre - trained model and use the ADAM solver with a learning rate of 2e - 5 for 3 epochs .", "entities": [[8, 9, "MethodName", "ELECTRA"], [12, 13, "DatasetName", "GLUE"], [31, 33, "MethodName", "linear layer"], [35, 36, "MethodName", "softmax"], [45, 46, "DatasetName", "ADAM"], [49, 51, "HyperparameterName", "learning rate"]]}
{"text": "As explained in Lu et al ( 2019 ) , the interaction between the two transformers is mediated by two co - attention layers where attention in one modality is conditioned on inputs from the other modality .", "entities": [[22, 24, "HyperparameterName", "attention layers"]]}
{"text": "2 - freeze The embedding layer was frozen for two epochs , then the weights were unfrozen for the rest of training ; prior work has shown that freezing layers after a certain number of epochs can improve results ( Liu et al , 2021 ) ; we opt for two because it still allows the finetuning to overpower the exiting embeddings if needed and preliminary results showed that freezing the weights for all epochs resulted in poor model performance .", "entities": [[33, 36, "HyperparameterName", "number of epochs"]]}
{"text": "We used the settings that were used to train the baseline model ( i.e. , learning rate of 2e - 5 ) .", "entities": [[15, 17, "HyperparameterName", "learning rate"]]}
{"text": "This is partially due to the fact that our training regime was altered due to hardware limitations ( i.e. , we could only use a batch size of 8 on a single 12 GB GPU ) .", "entities": [[25, 27, "HyperparameterName", "batch size"]]}
{"text": "As noted in Murahari et al ( 2019 ) , the NDCG metric is actually counter to MRR , but is important because it takes multiple dialogue response annotations into account .", "entities": [[17, 18, "MetricName", "MRR"]]}
{"text": "Recall that we have at most 3 NPs as the preverbal context , and therefore , we use a 4 - gram model so that the model has access to the complete context in a given condition to make a verbal prediction .", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "Let \u03b1 ( t ) be 1 if x ( t ) is ( or [ , it is copied to c ( t ) Name Formula Saliency R ( c ) t , i ( X )", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "At each time step , g ( t ) k = tanh ( m u ( t ) ) , where m \u226b 0 and u ( t ) = 2 k \u03b1 ( t ) + k\u22121 \u2211 j=1 2 j\u22121 h ( t\u22121 ) j .", "entities": [[9, 11, "HyperparameterName", "k ="], [23, 24, "DatasetName", "0"], [32, 33, "HyperparameterName", "\u03b1"]]}
{"text": "( 1 ) Observe that m u ( t ) \u226b 0 when \u03b1 ( t ) = 1 , and m u ( t ) \u226a 0 when \u03b1 ( t ) = \u22121 .", "entities": [[11, 12, "DatasetName", "0"], [13, 14, "HyperparameterName", "\u03b1"], [27, 28, "DatasetName", "0"], [29, 30, "HyperparameterName", "\u03b1"]]}
{"text": "j + sign ( z ( l \u2032 ) j ) \u03b5 , where l \u2032 ranges over all layers to which l has a forward connection via W ( l \u2032 l ) and \u03b5 > 0 is a stabilizing constant .", "entities": [[11, 12, "HyperparameterName", "\u03b5"], [35, 36, "HyperparameterName", "\u03b5"], [37, 38, "DatasetName", "0"]]}
{"text": "Recall from Section 4 that counter values for this network are expressed in multiples of the scaling factor v. We control the saturation of the network via the parameter u = tanh \u22121 ( v ) .", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "Recall from heatmaps # 3 and # 4 of Table 3 that for the counting task network , LRP assigns scores of 0 to prefixes with equal numbers of as and bs .", "entities": [[0, 1, "MetricName", "Recall"], [22, 23, "DatasetName", "0"]]}
{"text": "Recall that the white - box LSTM gates approximate 1", "entities": [[0, 1, "MetricName", "Recall"], [6, 7, "MethodName", "LSTM"]]}
{"text": "In Table 5 we report the percentage of examples for which such prefixes receive LRP scores of 0 , along with the network 's accuracy on this testing set and the average value of c ( t ) when the counter reaches 0 .", "entities": [[17, 18, "DatasetName", "0"], [24, 25, "MetricName", "accuracy"], [42, 43, "DatasetName", "0"]]}
{"text": "i ( t ) \u27e8q , x\u27e9 = { 1 , \u03b4 ( q \u2032 , x )", "entities": [[11, 12, "HyperparameterName", "\u03b4"]]}
{"text": "b ( i ) \u27e8q , x\u27e9 = { m , \u03b4 ( q 0 , x )", "entities": [[11, 12, "HyperparameterName", "\u03b4"], [14, 15, "DatasetName", "0"]]}
{"text": "Recall from Subsection 4.2 that we implement a bounded stack of size k using 2k + 1 hidden units , with the following interpretation : c 2k+1 is a bit , which is set to be positive if the stack is empty and nonpositive otherwise .", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "For human performance , we asked 5 other researchers majoring in human language analysis to manually annotate the test set and took the averaged F1 scores as human performance .", "entities": [[24, 25, "MetricName", "F1"]]}
{"text": "We manually set the dropout rate , learning rate , L2 regularization value by 0.2 , 1e - 5 , and 1e - 5 , respectively , according to their contributions to F1 - score , and the number of hyper - parameter search trials was around 15 .", "entities": [[7, 9, "HyperparameterName", "learning rate"], [10, 12, "HyperparameterName", "L2 regularization"], [32, 35, "MetricName", "F1 - score"]]}
{"text": "We trained the models iteratively on the training corpus for 20 rounds with the batch size set to 1 ( document ) , and we got the best model around the 18 - th round .", "entities": [[14, 16, "HyperparameterName", "batch size"]]}
{"text": "i=1 \u03b1 t , i x i ( 6 )", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "( W a h t + U a x i ) ( 8 ) \u03b1 t , i = exp ( e t , i )", "entities": [[14, 15, "HyperparameterName", "\u03b1"]]}
{"text": "( \u03b2 1 = 0.9 , \u03b2 2 = 0.999", "entities": [[1, 2, "HyperparameterName", "\u03b2"], [6, 7, "HyperparameterName", "\u03b2"]]}
{"text": "and = 1\u00d7 10 \u22128 ) as the optimizer .", "entities": [[8, 9, "HyperparameterName", "optimizer"]]}
{"text": "The learning rate is set to 5 \u00d7 10 \u22124 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "The batch size is 128 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "Test ( BLEU ) Baseline 25.7", "entities": [[2, 3, "MetricName", "BLEU"]]}
{"text": "We obtained +3.9 BLEU score when tuning the single best model on a mixture of 2.5 M synthetic bilingual pairs and 2.5 M bilingual pairs selected from CWMT parallel data randomly .", "entities": [[3, 5, "MetricName", "BLEU score"]]}
{"text": "We further gain +1.5 BLEU score when ensembling 4 models .", "entities": [[4, 6, "MetricName", "BLEU score"]]}
{"text": "Thus , we claim that it is a 100 % format accuracy result .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "Recall that C can be arbitrary and flexible , thus we can rebuild a new format C based on the generated result Y by masking partial content , say C = { c 0 c 1 c 2 love , c 0 c 1 c 2 c 3 c 4 remove . }", "entities": [[0, 1, "MetricName", "Recall"], [33, 34, "DatasetName", "0"], [41, 42, "DatasetName", "0"]]}
{"text": "Finally , the training objective is to minimize the negative log - likelihood over the whole sequence : L nll = \u2212 n t=1 log P ( y t | y < t ) ( 10 )", "entities": [[10, 13, "MetricName", "log - likelihood"], [19, 20, "MetricName", "nll"]]}
{"text": "Recall that in the task definition in Section 2 , we claim that our model owns the ability of refining and polishing .", "entities": [[0, 1, "MetricName", "Recall"]]}
{"text": "The number of layers L = 12 , and hidden size is 768 .", "entities": [[1, 4, "HyperparameterName", "number of layers"]]}
{"text": "Without loss of generality , we align C and Y from the beginning , and calculate the format quality according to the following rules : ( 1 ) the length difference | |", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "| \u2264 \u03b4 ; ( 2 ) the punctuation characters must be same .", "entities": [[2, 3, "HyperparameterName", "\u03b4"]]}
{"text": "For SongCi , we let \u03b4 = 0 and rule ( 2 ) must be conforming .", "entities": [[5, 6, "HyperparameterName", "\u03b4"], [7, 8, "DatasetName", "0"]]}
{"text": "For Sonnet , we relax the condition where we let \u03b4 = 1 and ignore rule ( 2 ) .", "entities": [[10, 11, "HyperparameterName", "\u03b4"]]}
{"text": "Radford et al , 2019 ) to conduct the generation , and we let k = 32 here .", "entities": [[14, 16, "HyperparameterName", "k ="]]}
{"text": "Therefore , in the experiments we let k = 32 to obtain a trade - off between the diversity and the general quality .", "entities": [[7, 9, "HyperparameterName", "k ="]]}
{"text": "The usefulness of BERT for metaphor detection has been shown by Mao et al ( 2019 ) , where a BERT - based system posted F1 = 0.717 on VUA AllPOS , hence our use of a BERT - based system as Baseline 3 .", "entities": [[3, 4, "MethodName", "BERT"], [20, 21, "MethodName", "BERT"], [25, 26, "MetricName", "F1"], [37, 38, "MethodName", "BERT"]]}
{"text": "The best system in 2018 performed at F1 = 0.651 ; the best performance in 2020 is more than 10 points better - F1 = 0.769 .", "entities": [[7, 8, "MetricName", "F1"], [23, 24, "MetricName", "F1"]]}
{"text": "The gap between the best and worst performance across genres for the same system remains wide - between 11.4 F1 points and 24.3 F1 points .", "entities": [[19, 20, "MetricName", "F1"], [23, 24, "MetricName", "F1"]]}
{"text": "Overall , the relative performance rankings are consistent - F1 scores are correlated at r = .92 and team ranks are correlated at r = 0.95 across the two datasets .", "entities": [[9, 10, "MetricName", "F1"]]}
{"text": "All teams posted better performance on the VUA data than on the TOEFL data ; the difference ( see column 4 in Table 6 : VUA Dataset : Performance ( F1 - score ) of the best systems submitted to All - POS track by genre subsets of the test data .", "entities": [[30, 33, "MetricName", "F1 - score"]]}
{"text": "F1 point ( zhengchang ) to 5 F1 points ( DeepMet , umd bilstm , Zenith ) .", "entities": [[0, 1, "MetricName", "F1"], [7, 8, "MetricName", "F1"], [13, 14, "MethodName", "bilstm"]]}
{"text": "The BERT baseline posted a relatively large difference of 9 F1 points ; this could be because BNC data is more similar to the data on which BERT has been pre - trained than TOEFL data .", "entities": [[1, 2, "MethodName", "BERT"], [10, 11, "MetricName", "F1"], [27, 28, "MethodName", "BERT"]]}
{"text": "However , even though the medium proficiency essays might have deficiencies in grammar , spelling , coherence and other properties of the essay that could interfere with metaphor detection , we generally observe relatively small differences in performance by proficiency - up to 3.5 F1 points , with a few ex - ceptions ( zhengchang , Go Figure ! ) .", "entities": [[44, 45, "MetricName", "F1"]]}
{"text": "The average gap between best and worst POS performance has also stayed similar - 11 F1 points ( it was 9 % in 2018 ) .", "entities": [[15, 16, "MetricName", "F1"]]}
{"text": "Furthermore , the gap between best and worst POS performance is large - 17 F1 points on average , ranging between 11 and 22 points .", "entities": [[14, 15, "MetricName", "F1"]]}
{"text": "The best performance on Nouns is only F1 = 0.641 ; it would have ranked 10th out of 12 on Adjectives .", "entities": [[7, 8, "MetricName", "F1"]]}
{"text": "9 : VUA and TOEFL Datasets by POS : Performance ( F1 - score ) of the best systems submitted to All - POS track by POS subsets of the test data .", "entities": [[11, 14, "MetricName", "F1 - score"]]}
{"text": "\u2212 ( 1 \u2212 \u03bb ) max e Et SIM ( s j , e ) ( 1 ) BISIM ( s j , D , q t ) = \u03b2 SIM ( s j , D )", "entities": [[30, 31, "HyperparameterName", "\u03b2"]]}
{"text": "+ ( 1 \u2212 \u03b2 ) SIM ( s j , q t ) ( 2 ) where \u03bb [ 0 , 1 ] balances salience and redundancy and \u03b2 [ 0 , 1 ] balances a sentence 's salience within its document set and its resemblance to the current query .", "entities": [[4, 5, "HyperparameterName", "\u03b2"], [20, 21, "DatasetName", "0"], [29, 30, "HyperparameterName", "\u03b2"], [31, 32, "DatasetName", "0"]]}
{"text": "= 0.6 ( following Lebanoff et al , 2018 ) and \u03b2 = 0.5 ( see Appendix B.3 ) .", "entities": [[11, 12, "HyperparameterName", "\u03b2"]]}
{"text": "e. Then , a state representation g t considers z t and all sentence representations with the glimpse operation ( Vinyals et al , 2016 ) : a t j = v 1 tanh ( W 1\u0109 t j + W 2 z t ) ( 5 ) \u03b1 t = softmax ( a t ) ( 6 ) g t = j \u03b1 t j W 1\u0109 t j ( 7 ) where v 1 , W 1 and W 2 are model parameters , and a t represents the vector composed of a t j .", "entities": [[48, 49, "HyperparameterName", "\u03b1"], [51, 52, "MethodName", "softmax"], [63, 64, "HyperparameterName", "\u03b1"]]}
{"text": "Formally , at time step t , the reward r t of selected phrase e out t is : r t = PF ( e out t , R ) \u2212 \u03b3 1 PFMAX ( e out t , E in , R )", "entities": [[31, 32, "HyperparameterName", "\u03b3"]]}
{"text": "\u2212 \u03b3 2 PFMAX ( e out t , E t \\ E in , R )", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "Different weights are given to the PFMAX against the input history ( \u03b3 1 ) and that of the phrases output so far ( \u03b3 2 ) .", "entities": [[12, 13, "HyperparameterName", "\u03b3"], [24, 25, "HyperparameterName", "\u03b3"]]}
{"text": "In both cases , the M Sugg model used was set with \u03b3 1 = 0.5 and \u03b3 2 = 0.9 after some hyperparameter tuning ( Appendix B.4 ) .", "entities": [[12, 13, "HyperparameterName", "\u03b3"], [17, 18, "HyperparameterName", "\u03b3"]]}
{"text": "Training with a batch size of 8 used about 3 GB GPU memory for M Summ , and about 9 GB memory for M Sugg ( since there are many more input units per document set , i.e. , all noun phrases versus sentences ) .", "entities": [[3, 5, "HyperparameterName", "batch size"]]}
{"text": "Further - more , there is a single reward function for learning the policy , computed per selected sentence e out t as ROUGE - L F 1 w.r.t .", "entities": [[23, 26, "MetricName", "ROUGE - L"]]}
{"text": "Both M Summ and M Sugg are further trained on our adjusted DUC 2007 data using an Adam optimizer with a learning rate of 5e - 4 and no weight decay .", "entities": [[12, 14, "DatasetName", "DUC 2007"], [17, 18, "MethodName", "Adam"], [18, 19, "HyperparameterName", "optimizer"], [21, 23, "HyperparameterName", "learning rate"], [29, 31, "MethodName", "weight decay"]]}
{"text": "The batch size was 8 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "( 1 ) The \u03b2 value in the query - focused MMR function in Equation 2 , that impacts the weight of the query on a sentence versus the document set on the sentence .", "entities": [[4, 5, "HyperparameterName", "\u03b2"]]}
{"text": "We tried out a few \u03b2 values and mainly noticed that a value of 0.5 kept validation results more stable across configurations , or kept training time shorter .", "entities": [[5, 6, "HyperparameterName", "\u03b2"]]}
{"text": "( 1 ) In the reward function in Equation 13 , we set \u03b3 1 = 0.5 and \u03b3 2 = 0.9 , i.e. , the preceding output phrases are more strongly accounted for than the phrases in the session history .", "entities": [[13, 14, "HyperparameterName", "\u03b3"], [18, 19, "HyperparameterName", "\u03b3"]]}
{"text": "We also kept the same AUC length limits ( 106 to 250 tokens ) for easy comparability to Table 2 .", "entities": [[5, 6, "MetricName", "AUC"]]}
{"text": "The area under the curve ( AUC ) is computed for each of the curves , and reported in the first row of Tables 2 and 3 .", "entities": [[6, 7, "MetricName", "AUC"]]}
{"text": "The higher AUC scores obtained from the recall curves of our models , compared to those of the S 2 baseline , highlight the ability to expose more salient information earlier in the session .", "entities": [[2, 3, "MetricName", "AUC"]]}
{"text": "The normalized AUC score for the validation metric ( explained in 3.3 ) is computed over the recall curve produced from the accumulating summary expansions .", "entities": [[2, 3, "MetricName", "AUC"]]}
{"text": "( x ; \u03b8 ) \u21a6 { 0 , 1 } with parameters \u03b8 on the training data .", "entities": [[3, 4, "HyperparameterName", "\u03b8"], [7, 8, "DatasetName", "0"], [13, 14, "HyperparameterName", "\u03b8"]]}
{"text": "We denote the test accuracy of a model f ( \u22c5 ) on examples perturbed by g ( \u22c5 ) in Experiment 1 as A 1 ( f , g , D * test ) .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "Similarly , the test accuracy in Experiment 0 is A 0 ( f , D test ) .", "entities": [[4, 5, "MetricName", "accuracy"], [7, 8, "DatasetName", "0"], [10, 11, "DatasetName", "0"]]}
{"text": "To improve robust accuracy ( Tu et al , 2020 ) ( i.e. , accuracy on the perturbed test set ) , it is a common practice to leverage data augmentation ( Li and Specia , 2019 ; Min et al , 2020 ; Tan and Joty , 2021 ) .", "entities": [[3, 4, "MetricName", "accuracy"], [14, 15, "MetricName", "accuracy"], [29, 31, "TaskName", "data augmentation"]]}
{"text": "( ) 2 where A 2 ( f , g , D * test ) denotes the test accuracy of Experiment 2 .", "entities": [[18, 19, "MetricName", "accuracy"]]}
{"text": "In this work , we focus on the robust accuracy ( Section 2.1 ) , which is accuracy on the perturbed test set .", "entities": [[9, 10, "MetricName", "accuracy"], [17, 18, "MetricName", "accuracy"]]}
{"text": "( x ; \u03b8 ) \u21a6 { 0 , 1 } , perturbation g \u2236", "entities": [[3, 4, "HyperparameterName", "\u03b8"], [7, 8, "DatasetName", "0"]]}
{"text": "( \u22c5 ) on D \u2032 * train 21 : A ( f , g , p , D \u2032 * test ) f ( \u22c5 ) accuracy on D \u2032 * test 22 : A ( f , g , p , D \u2032 test )", "entities": [[27, 28, "MetricName", "accuracy"]]}
{"text": "( \u22c5 ) accuracy on D \u2032 test 23 : return A ( f , g , p , D \u2032 * test )", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "In Section 3.2 , we propose an accuracy - based identification of ATE .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "Notice that the accuracy - based definition of outcome Y ( Equation 6 ) can also be written in a similar form to the probability - based one ( Equation 11 ) :", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "The major difference is that , accuracy - based IT E is a discrete variable falling in { \u22121 , 0 , 1 } , while probability - based IT E is a continuous one ranging from - 1 to 1 .", "entities": [[6, 7, "MetricName", "accuracy"], [20, 21, "DatasetName", "0"]]}
{"text": "For example , if a model learns to identify a perturbation and thus changes its prediction from wrong ( before perturbation ) to correct ( after perturbation ) , accuracy - based IT E will be 1 \u2212 0 = 1 while probability - based IT E will be less than 1 .", "entities": [[29, 30, "MetricName", "accuracy"], [38, 39, "DatasetName", "0"]]}
{"text": "Empirically , we find that accuracy - based average learnability varies greatly ( \u03c3 = 0.375 , Table 4 ) and thus can better distinguish between different model - perturbation pairs than probabilitybased one ( \u03c3 = 0.288 , Table 4 ) .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "Next , we propose two improved modelsone that uses the negative graphs in a max - margin formulation and another that uses both positive and negative graphs with a InfoNCE ( van den Oord et al , 2018 ) contrastive loss .", "entities": [[29, 30, "MethodName", "InfoNCE"], [40, 41, "MetricName", "loss"]]}
{"text": "Saha et al ( 2021b ) evaluate explanation graphs by defining two accuracy metrics - ( 1 ) Structural Correctness Accuracy ( StCA ) :", "entities": [[12, 13, "MetricName", "accuracy"], [20, 21, "MetricName", "Accuracy"]]}
{"text": "Fraction of graphs that satisfy all structural constraints , and ( 2 ) Semantic Correctness Accuracy ( SeCA ) : Fraction of graphs that are both structurally and semantically correct .", "entities": [[15, 16, "MetricName", "Accuracy"]]}
{"text": "2 shows an example of a positive graph perturbation where the node \" loss of jobs \" is replaced with \" going of business \" .", "entities": [[13, 14, "MetricName", "loss"]]}
{"text": "= i max ( 0 , logP \u03b8 ( y ( g )", "entities": [[4, 5, "DatasetName", "0"], [7, 8, "HyperparameterName", "\u03b8"]]}
{"text": "h ( p ) } { h ( n ) i } M i=1 , our overall loss combines the cross - entropy loss L CE and the InfoNCE contrastive loss ( van den Oord et al , 2018 )", "entities": [[17, 18, "MetricName", "loss"], [23, 24, "MetricName", "loss"], [28, 29, "MethodName", "InfoNCE"], [30, 31, "MetricName", "loss"]]}
{"text": "We set \u03b4 = 0.4 and \u03b3 = 0.5 ( tuned on the dev set ) and train the Max - margin model using these additionally generated human - like negative graphs .", "entities": [[2, 3, "HyperparameterName", "\u03b4"], [6, 7, "HyperparameterName", "\u03b3"]]}
{"text": "6 All models for the ExplaGraphs dataset 7 ( Saha et al , 2021b ) are trained with a batch size of 8 and an initial learning rate of 3 * 10 \u22125 for a maximum of 15 epochs .", "entities": [[19, 21, "HyperparameterName", "batch size"], [26, 28, "HyperparameterName", "learning rate"]]}
{"text": "For the max - margin graph generation model , we set both the hyperparameters \u03b1 ( mixing ratio ) and \u03b2 ( margin ) to 1.0 while for the contrastive graph generation model , we set \u03b1 to 0.1 .", "entities": [[5, 7, "TaskName", "graph generation"], [14, 15, "HyperparameterName", "\u03b1"], [20, 21, "HyperparameterName", "\u03b2"], [30, 32, "TaskName", "graph generation"], [36, 37, "HyperparameterName", "\u03b1"]]}
{"text": "For the temporal graph generation task 8 ( Madaan and Yang , 2021 ) , we train all models with a batch size of 4 and an initial learning rate of 3 * 10 \u22125 for a maximum of 10 epochs .", "entities": [[3, 5, "TaskName", "graph generation"], [21, 23, "HyperparameterName", "batch size"], [28, 30, "HyperparameterName", "learning rate"]]}
{"text": "On this task , the hyperparameters \u03b1 and \u03b2 for the max - margin model are again set to 1.0 while for the contrastive graph generation model , we set \u03b1 to 0.2 .", "entities": [[6, 7, "HyperparameterName", "\u03b1"], [8, 9, "HyperparameterName", "\u03b2"], [24, 26, "TaskName", "graph generation"], [30, 31, "HyperparameterName", "\u03b1"]]}
{"text": "The batch size and learning rate are manually tuned in the range { 4 , 8 , 16 } and { 10 \u22125 , 2 * 10 \u22125 , 3 * 10 \u22125 } respectively and the best models are chosen based on the respective validation set performance .", "entities": [[1, 3, "HyperparameterName", "batch size"], [4, 6, "HyperparameterName", "learning rate"]]}
{"text": "For instance , \" Escherichia coli \" has an exact match that can be successfully normalized to the referent concept with an ID \" 562 \" in the NCBI taxonomy .", "entities": [[9, 11, "MetricName", "exact match"]]}
{"text": "The performances of the submitted systems are evaluated with their Precision values , which are calculated as : Precision = S p / N ( 2 ) where S p indicates the total Wang similarity W for all predictions ( Deleger et al , 2016 ) , and N is the number of predicted entities .", "entities": [[10, 11, "MetricName", "Precision"], [18, 19, "MetricName", "Precision"]]}
{"text": "Our system ( BOUN - ISIK - 2 ) achieved the best performance with 67.9 % Precision in the BB - norm sub - task ( Entity Normalization ) .", "entities": [[16, 17, "MetricName", "Precision"]]}
{"text": "We use area under the ROC curve ( AUC ) as the primary evaluation metric for SLA modeling ( Fawcett , 2006 ) .", "entities": [[8, 9, "MetricName", "AUC"]]}
{"text": "We also report F1 score - the harmonic mean of precision and recall - as a secondary metric , since it is more common in similar skewed - class labeling tasks ( e.g. , Ng et al , 2013 ) .", "entities": [[3, 5, "MetricName", "F1 score"]]}
{"text": "Note , however , that F1 can be significantly improved simply by tuning the classification threshold ( fixed at 0.5 for our evaluations ) without affecting AUC .", "entities": [[5, 6, "MetricName", "F1"], [14, 16, "HyperparameterName", "classification threshold"], [26, 27, "MetricName", "AUC"]]}
{"text": "System ranks are determined by sorting teams according to AUC , and using DeLong 's test ( DeLong et al , 1988 ) to identify statistical ties .", "entities": [[9, 10, "MetricName", "AUC"]]}
{"text": "The intercept can be interpreted as the \" average \" AUC of .786 .", "entities": [[10, 11, "MetricName", "AUC"]]}
{"text": "Controlling for the random effects of user ( which exhibits a wide standard deviation of \u00b1.086 AUC ) , team ( \u00b1.013 ) , and track ( \u00b1.011 ) , three of the algorithmic choices are at least marginally significant ( p < .1 ) .", "entities": [[16, 17, "MetricName", "AUC"]]}
{"text": "For example , we might expect a system that uses RNNs to model learner mastery over time would add + .028 to learner - specific AUC ( all else being equal ) .", "entities": [[25, 26, "MetricName", "AUC"]]}
{"text": "Table 5 reports AUC for various ensemble methods as well as some of the top performing team systems for all three tracks .", "entities": [[3, 4, "MetricName", "AUC"]]}
{"text": "Interestingly , the oracle is exceptionally accurate ( > .993 AUC and > .884 F1 , not shown ) .", "entities": [[10, 11, "MetricName", "AUC"], [14, 15, "MetricName", "F1"]]}
{"text": "Note that RSA is meaningfully different from , and complementary to , methods that employ saturating functions of representation distances ( e.g. decoding accuracy , mutual information ) , which suffer from ( a ) a ceiling effect : being able to distinguish experimental phenomenon A from B with with an accuracy of 100 % and experimental phenomenon C from D with an accuracy of 100 % does not mean that the distance between A and B is the same as that between C and D ; and ( b ) discretization ( Nili et al , 2014 ) .", "entities": [[23, 24, "MetricName", "accuracy"], [51, 52, "MetricName", "accuracy"], [63, 64, "MetricName", "accuracy"]]}
{"text": "We evaluate the model with ROUGE metric and report the F1 scores for ROUGE - 1 ( R1 ) and ROUGE - L ( RL ) , which measure the word overlap and the longest common sequence between the reference answer and the generated answer , respectively .", "entities": [[10, 11, "MetricName", "F1"], [20, 23, "MetricName", "ROUGE - L"]]}
{"text": "Total Retained A comparison with the organizer - provided parallel training data used in our WMT18 system ( which is largely the same as the provided parallel data for WMT19 in the Russian - English language pair ) on baseline Marian transformer systems with identical training conditions show that aggressive language ID based filtering yields an approximate +1 BLEU point improvement as measured by SacreBLEU ( Post , 2018 ) .", "entities": [[58, 59, "MetricName", "BLEU"], [64, 65, "MetricName", "SacreBLEU"]]}
{"text": "Utilizing the best - performing BPE parameters from Section 2.2 , we first trained a baseline system in each of the two network architectures , noting the Transformer system 's better performance of +0.82 BLEU on average across decoded test sets .", "entities": [[5, 6, "MethodName", "BPE"], [27, 28, "MethodName", "Transformer"], [34, 35, "MetricName", "BLEU"]]}
{"text": "Borrowing from our Moses training approach , we utilize a multi - iteration decode and optimize feature weights using the \" Expected Corpus BLEU \" ( ECB ) metric with the Drem optimizer ( Erdmann and Gwinnup , 2015 ) .", "entities": [[23, 24, "MetricName", "BLEU"], [32, 33, "HyperparameterName", "optimizer"]]}
{"text": "We experimented using newstest2014 and newstest2017 as tuning sets - 2017 did not help performance , but using 2014 did improve performance by up to +0.9 BLEU 4 over the non - tuned ensemble .", "entities": [[26, 27, "MetricName", "BLEU"]]}
{"text": "System weights were tuned with the Drem ( Erdmann and Gwinnup , 2015 ) optimizer using the \" Expected Corpus BLEU \"", "entities": [[14, 15, "HyperparameterName", "optimizer"], [20, 21, "MetricName", "BLEU"]]}
{"text": "Individual component system and final combination scores are shown in Table 6 for cased , detokenized BLEU and BEER 2.0 ( Stanojevi\u0107 and Sima'an , 2014 ) .", "entities": [[16, 17, "MetricName", "BLEU"]]}
{"text": "This highlights the difficulties of using traditional reliability metrics like Krippendorff 's \u03b1 for crowdsourced annotations on subjective tasks ( D'Arcey et al , 2019 ) .", "entities": [[12, 13, "HyperparameterName", "\u03b1"]]}
{"text": "Krippendorff 's \u03b1 is a number between 0 and 1 intended to indicate the extent to which annotators agree compared with what would have happened if they guessed randomly .", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [7, 8, "DatasetName", "0"]]}
{"text": "Firstly , to allow for comparison with other literature in the field , we report the K - \u03b1 for judgements on each attribute in Table 2 .", "entities": [[18, 19, "HyperparameterName", "\u03b1"]]}
{"text": "The one exception is the set of judgements on whether a comment has a place in a healthy conversation , with a lower K - \u03b1 of 0.26 .", "entities": [[25, 26, "HyperparameterName", "\u03b1"]]}
{"text": "Specifically , as shown in Figure 5 , we see that as we increase the trustworthiness threshold for annotators whose judgements are included , the resulting K - \u03b1 steadily increase .", "entities": [[28, 29, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b3 is used to distinguish when it is more important to focus on the precision or F 0.5 of a correction .", "entities": [[0, 1, "HyperparameterName", "\u03b3"]]}
{"text": "When we move to the final ensemble with confidence tables of existing systems , if the confidence is larger than \u03b3 , we select the correction proposed by the system that has the best F 0.5 on the type of this correction .", "entities": [[20, 21, "HyperparameterName", "\u03b3"]]}
{"text": "4 GPU is obviously better than 2 GPU sets , which is probably because of the larger batch size accumulation for gradient calculation .", "entities": [[17, 19, "HyperparameterName", "batch size"]]}
{"text": "To get higher F 0.5 , in the case where the precision is greater than a predefined threshold ( controlled by \u03b3 ) , we will choose the model with the highest F 0.5 for the corresponding error type .", "entities": [[21, 22, "HyperparameterName", "\u03b3"]]}
{"text": "In an attempt to study the overlap between graph - based approaches and approaches targeting extractive summarization with argument mining , evaluation results suggest a positive effect on the sub - task , achieving an accuracy of 50 % on the corpus compiled by Hasan and Ng ( 2014 ) from online debate forums and on a corpus of persuasive essays ( Stab and Gurevych , 2014a ) .", "entities": [[15, 17, "TaskName", "extractive summarization"], [18, 20, "TaskName", "argument mining"], [35, 36, "MetricName", "accuracy"]]}
{"text": ", t k } representing the latent variables that are most representative for the sentences S. Second , topic modeling returns a distribution of sentences over topics \u03b8 = { \u03b8 s 1 , . . .", "entities": [[27, 28, "HyperparameterName", "\u03b8"], [30, 31, "HyperparameterName", "\u03b8"]]}
{"text": "To this end , the distribution of sentences over topics \u03b8 is exploited with the aim at determining the best topic assignment for each sentence of S. The result is an attraction set A = { s 1 , a 1 , . . .", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "Interannotation agreement has been measured to unitized alpha ( Krippendorff , 2004 )", "entities": [[7, 8, "HyperparameterName", "alpha"]]}
{"text": "\u03b1 U = 0.724 .", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "The corpus contains documents of various sizes , with a mean size of 11.44 \u00b1 11.70 sentences per document , while the inter - annotator agreement was measured as \u03b1 U = 0.48 .", "entities": [[29, 30, "HyperparameterName", "\u03b1"]]}
{"text": "In Figure 3 , we report the precision , recall , and F1 - measure for A2 T and for the baseline .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "A characteristic of this corpus is that argumentative units are frequently located in the introduction or the conclusion of an essay , which is also reflected by the baseline that achieved an F1 - measure of 0.35 for a threshold of \u03c4 = 0.05 ( with the baseline being particularly precise , suggesting that argumentative units are very frequently at the beginning and end of essays ) .", "entities": [[32, 33, "MetricName", "F1"]]}
{"text": "Both A2 T and the baseline achieve low results , but the accuracy of A2 T is 0.3 against the 0.1 of the baseline .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "More recently , multilingual NMT models have focused on maximizing transfer accuracy for low - resource language pairs , while preserving high - resource language accuracy ( Platanios et al , 2018 ; Neubig and Hu , 2018 ; Aharoni et al , 2019 ; Arivazhagan et al , 2019 ) , known as the ( positive ) transfer - ( negative ) interference trade - off .", "entities": [[11, 12, "MetricName", "accuracy"], [25, 26, "MetricName", "accuracy"]]}
{"text": "f i ( 4 ) r i = SA ( w i ; \u0398 ( ad ) )", "entities": [[13, 14, "HyperparameterName", "\u0398"]]}
{"text": "( 5 ) where \u0398 ( ad ) denotes the adapter modules .", "entities": [[4, 5, "HyperparameterName", "\u0398"]]}
{"text": "l e ( 6 ) 3 Due to CPG , the number of adapter parameters is multiplied by language embedding size , resulting in a larger model compared to the baseline ( more details in Appendix A.1 ) . where l e R M , W ( ad ) R P ( ad ) \u00d7M , W ( bf ) R P ( bf ) \u00d7M , M is the language embedding size , P ( ad ) and P ( bf ) are the number of parameters for adapters and biaffine attention respectively .", "entities": [[85, 88, "HyperparameterName", "number of parameters"]]}
{"text": "In our approach , pre - trained BERT weights are frozen , and only adapters and biaffine attention are trained , thus we use the same learning rate for the whole network by applying an inverse square root learning rate decay with linear warmup ( Howard and Ruder , 2018 ) .", "entities": [[7, 8, "MethodName", "BERT"], [26, 28, "HyperparameterName", "learning rate"], [38, 40, "HyperparameterName", "learning rate"], [42, 44, "MethodName", "linear warmup"]]}
{"text": "and , therefore , no principled way to represent additional languages . Taken together with the parsing results of 4.1 , these plots suggest that UDapter embeddings strike a good balance between a linguistically motivated representation space and one solely optimized for in - training language accuracy .", "entities": [[46, 47, "MetricName", "accuracy"]]}
{"text": "models are trained separately so the total number of parameters for 13 languages is 2.5B ( 13x191 M ) .", "entities": [[7, 10, "HyperparameterName", "number of parameters"]]}
{"text": "We train the model to minimize the negative log - likelihood loss using back - propagation with stochastic gradient descent and a mini - batch size of 16 .", "entities": [[8, 11, "MetricName", "log - likelihood"], [11, 12, "MetricName", "loss"], [17, 20, "MethodName", "stochastic gradient descent"], [22, 26, "HyperparameterName", "mini - batch size"]]}
{"text": "For optimization , we use the AdamW optimizer ( Loshchilov and Hutter , 2019 ) with gradient clipping ( Pascanu et al , 2013 ) and a linear scheduler with no warm - up .", "entities": [[6, 7, "MethodName", "AdamW"], [7, 8, "HyperparameterName", "optimizer"], [16, 18, "MethodName", "gradient clipping"]]}
{"text": "We use FP - 16 mixed precision ( Micikevicius et al , 2018 ) training ( and inference ) in order to afford a larger batch size and increased training speed .", "entities": [[25, 27, "HyperparameterName", "batch size"]]}
{"text": "More specifically , we use \u03b2 1 = 0.9 , \u03b2 2 = 0.999 and \u03f5", "entities": [[5, 6, "HyperparameterName", "\u03b2"], [10, 11, "HyperparameterName", "\u03b2"]]}
{"text": "= 10 \u22128 for the AdamW optimizer parameter values and a learning rate of 0.00005 .", "entities": [[5, 6, "MethodName", "AdamW"], [6, 7, "HyperparameterName", "optimizer"], [11, 13, "HyperparameterName", "learning rate"]]}
{"text": "Our system performed considerably better than the average in sub - task 1a and surpassed the existing state - of - the - art F1 - score of 0.63 reported in ( Magge et al , 2021 ) .", "entities": [[24, 27, "MetricName", "F1 - score"]]}
{"text": "Metric is F1 - score for class 1 . Section [ 1 ] , our main efforts were dedicated to subtask 1a and the system developed did not transfer well to the remaining sub - tasks .", "entities": [[2, 5, "MetricName", "F1 - score"]]}
{"text": "Using only a standard finetuning approach , our model obtained competitive results , outperforming the average of all submissions for sub - task 1 by a 9 % absolute difference in F1 - score .", "entities": [[31, 34, "MetricName", "F1 - score"]]}
{"text": "( Hui and Belkin , 2021 ) , in an extensive series of experiments , show that the established practice of using a cross - entropy loss for classification is not well - founded and show through a variety of diverse experiments that a square loss can , in many cases , significantly improve performance .", "entities": [[26, 27, "MetricName", "loss"], [45, 46, "MetricName", "loss"]]}
{"text": "When evaluated on the Spider dataset , our approach achieves 4.6 % and 9.8 % accuracy gain in the test and dev sets , respectively .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "In the experiment on the Spider dataset , we achieve 24.3 % and 28.8 % exact SQL matching accuracy on the test and dev set respectively , which outperforms the previous state - of - the - art approach ( Yu et al , 2018b ) by 4.6 % and 9.8 % .", "entities": [[18, 19, "MetricName", "accuracy"]]}
{"text": "o ( i ) col R d\u00d7 | L | as follows : \u03b1 = softmax ( w T tanh ( o ( i ) col ) ) ( 1 ) h ( i ) col", "entities": [[13, 14, "HyperparameterName", "\u03b1"], [15, 16, "MethodName", "softmax"]]}
{"text": "= o ( i ) col \u03b1 T ( 2 ) where | L | is the number of tokens in the column and w R d is a trainable parameter .", "entities": [[6, 7, "HyperparameterName", "\u03b1"]]}
{"text": "Then we apply the softmax classifier to choose the sketch as follows : \u03b1 s = softmax ( w T s tanh ( H Q ) ) ( 3 ) r s = H Q", "entities": [[4, 5, "MethodName", "softmax"], [13, 14, "HyperparameterName", "\u03b1"], [16, 17, "MethodName", "softmax"]]}
{"text": "\u03b1 T s ( 4 ) P sketch = softmax ( W s r s + b s ) ( 5 ) where w s R d , W s R ns\u00d7d , b s R ns are trainable parameters and n s is the number of possible sketches .", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [9, 10, "MethodName", "softmax"]]}
{"text": "\u03b1 ( t ) = softmax ( d ( t ) col T H Q ) ( 6 ) r ( t )", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [5, 6, "MethodName", "softmax"]]}
{"text": "H Q \u03b1 ( t ) T ( 7 )", "entities": [[2, 3, "HyperparameterName", "\u03b1"]]}
{"text": "For the evaluation metrics , we use 1 ) accuracy of exact SQL matching and 2 ) F1 score of SQL component matching , proposed by ( Yu et al , 2018c ) .", "entities": [[9, 10, "MetricName", "accuracy"], [17, 19, "MetricName", "F1 score"]]}
{"text": "For the training , we use Adam optimizer ( Kingma and Ba , 2014 ) with a learning rate of 1e - 4 and use early stopping with 50 epochs .", "entities": [[6, 7, "MethodName", "Adam"], [7, 8, "HyperparameterName", "optimizer"], [17, 19, "HyperparameterName", "learning rate"], [25, 27, "MethodName", "early stopping"]]}
{"text": "Table 1 shows the exact SQL matching accuracy of our model and previous models .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "Without recursive sub - query generation , the accuracy drops by 5.7 % and 3.6 % for hard and extra hard queries , respectively .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "When using the final LSTM hidden state as in Yu et al ( 2018b ) instead of using self - attention for schema encoding , the accuracy drops by 4.0 % on all queries .", "entities": [[4, 5, "MethodName", "LSTM"], [26, 27, "MetricName", "accuracy"]]}
{"text": "Finally , when using only an encoder - decoder architecture without sketch generation for columns prediction , the accuracy drops by 4.7 % .", "entities": [[18, 19, "MetricName", "accuracy"]]}
{"text": "Using bag - of - words features , unigram and bigram part - of - speech features , type - token ratio , the proportion of words appearing on a list of easier words , and parse features similar to Petersen 's , their binary classifier achieved an accuracy of 80.8 % on this task .", "entities": [[11, 14, "DatasetName", "part - of"], [48, 49, "MetricName", "accuracy"]]}
{"text": "Using a hard cut - off ( i.e. rank ( sent english ) > rank ( sent simple ) ) , their model achieved about 59 % accuracy , although this improved to 70 % by relaxing the inequality constraint to include equality .", "entities": [[27, 28, "MetricName", "accuracy"]]}
{"text": "Ambati et al ( 2016 ) provide accuracy information for their own features as well as Vajjala & Meurers ' ( 2014 ) features on the English and Simple English Wikipedia corpus ( ESEW ) which we use , but used a 60 - 20 - 20 training - dev - test split where we used 10 - fold cross - validation , making the results not directly comparable .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "Moving to Figure 2 , we see that our BASE - LINE features achieved an accuracy of 71.24 % , despite using only average word length and sentence length .", "entities": [[9, 10, "MethodName", "BASE"], [11, 12, "MethodName", "LINE"], [15, 16, "MetricName", "accuracy"]]}
{"text": "This is 1.48 percentage points higher than the 69.76 % accuracy of the PSYCHOLINGUIS - TIC model , which includes surprisal , embedding depth , integration cost , and idea density .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "al found that their features based on incremental CCG derivations achieved an accuracy of 72.12 % , while the offline psycholinguistic features of Vajjala & Meurers came in at 74.58 % , 1.36 percentage points better than our 73.22 % .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "where k = { k 1 , , k K } is a list of different ker - nel sizes .", "entities": [[1, 3, "HyperparameterName", "k ="]]}
{"text": "The loss function consists of four parts : ( 1 ) L s : a binary cross - entropy loss of each generated positive and negative procedure ; ( 2 ) L r : the regression loss with a smooth l1 - loss ( Ren et al , 2015 ) of a time span between the extracted and the ground - truth procedure .", "entities": [[1, 2, "MetricName", "loss"], [19, 20, "MetricName", "loss"], [36, 37, "MetricName", "loss"], [42, 43, "MetricName", "loss"]]}
{"text": "( 3 ) L p : the cross - entropy loss of each proposed procedure in the predicted sequence of proposals .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "( 4 ) L c : the cross - entropy loss of each token in the generated procedure captions .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "\u03b1 s L s + \u03b1 r L r + \u03b1 p L p + \u03b1 c L c ( 5 )", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [5, 6, "HyperparameterName", "\u03b1"], [10, 11, "HyperparameterName", "\u03b1"], [15, 16, "HyperparameterName", "\u03b1"]]}
{"text": "Here we regard a sample as positive if its IoU ( Intersection of Union ) with any ground - truth procedure is more than 0.8 .", "entities": [[9, 10, "MetricName", "IoU"]]}
{"text": "If the IoU is less than 0.2 , we treat it as negative .", "entities": [[2, 3, "MetricName", "IoU"]]}
{"text": "The p l is the classification result of the procedure extraction module and the value of 1 will be 1 if the predicted class of extracted procedure proposal is identical to the class of the groundtruth proposal with the maximal IoU and 0 otherwise .", "entities": [[40, 41, "MetricName", "IoU"], [42, 43, "DatasetName", "0"]]}
{"text": "For procedure extraction , we adopt the widely used mJacc ( mean of Jaccard ) ( Bojanowski et al , 2014 ) and mIoU ( mean of IoU ) metrics for evaluating the procedure proposition .", "entities": [[23, 24, "MetricName", "mIoU"], [27, 28, "MetricName", "IoU"]]}
{"text": "For procedure captioning , we adopt BLEU - 4 ( Papineni et al , 2002 ) and METEOR ( Banerjee and Lavie , 2005 ) as the metrics to evaluate the performance on the result of captioning based on both extracted and ground - truth procedures .", "entities": [[6, 7, "MetricName", "BLEU"], [17, 18, "DatasetName", "METEOR"]]}
{"text": "We adopt an Adam optimizer ( Kingma and Ba , 2015 ) with a starting learning rate of 0.000025 and \u03b1 = 0.8 and \u03b2 = 0.999 to train the model .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [15, 17, "HyperparameterName", "learning rate"], [20, 21, "HyperparameterName", "\u03b1"], [24, 25, "HyperparameterName", "\u03b2"]]}
{"text": "The batch size of training is 4 for each GPU and we use 4 GPUs to train our model so the overall batch size is 16 .", "entities": [[1, 3, "HyperparameterName", "batch size"], [22, 24, "HyperparameterName", "batch size"]]}
{"text": "Additionally , our model achieves the SOTA result on BLEU - 4 and METEOR metrics when using the ground - truth procedures as well as the extracted procedures .", "entities": [[9, 10, "MetricName", "BLEU"], [13, 14, "DatasetName", "METEOR"]]}
{"text": "Motivated by this , we conduct another experiment to use cherry picked sentence like add the chicken ( or beef , carrot , onion , etc . ) to the pan and stir or add pepper and salt to the bowl as the captions for all procedures and can still achieve a good result on BLEU ( 4.0 + ) and METEOR ( 16.0 + ) .", "entities": [[55, 56, "MetricName", "BLEU"], [61, 62, "DatasetName", "METEOR"]]}
{"text": "Our system achieves a macro - average of 68.31 % LAS F1 score , with an improvement of 2.51 % compared with the UDPipe .", "entities": [[11, 13, "MetricName", "F1 score"]]}
{"text": "In terms of all the above model improvement , compared to the UDPipe baseline , our system achieves a macro - average of 68.31 % LAS F1 score , with an improvement of 2.51 % in this task .", "entities": [[26, 28, "MetricName", "F1 score"]]}
{"text": "We optimize with Adam ( Kingma and Ba , 2015 ) , setting the learning rate to 1e", "entities": [[3, 4, "MethodName", "Adam"], [14, 16, "HyperparameterName", "learning rate"]]}
{"text": "\u22123 and \u03b2 1 =", "entities": [[2, 3, "HyperparameterName", "\u03b2"]]}
{"text": "\u03b2 2 = 0.9 .", "entities": [[0, 1, "HyperparameterName", "\u03b2"]]}
{"text": "Moreover , we train models for up to 100 epochs with batch size 32 on 3 NVIDIA GeForce GTX 1080Ti GPUs with 200 to 500 sentences per second and occupying 2 to 3 GB graphic memory each model .", "entities": [[11, 13, "HyperparameterName", "batch size"]]}
{"text": "In addition , we compare our model with the best and the average results of top ten models on each treebank , using LAS F1 for the evaluation metric , as shown in Figure 2 .", "entities": [[24, 25, "MetricName", "F1"]]}
{"text": "Our hybrid pointer - generator network facilitates copying words from the source text via pointing ( Vinyals et al , 2015 ) , which improves accuracy and handling of OOV words , while retaining the ability to generate new words .", "entities": [[25, 26, "MetricName", "accuracy"]]}
{"text": "loss t = \u2212 log P ( w * t ) ( 6 ) and the overall loss for the whole sequence is : loss = 1 T \u2211 T t=0 loss t ( 7 )", "entities": [[0, 1, "MetricName", "loss"], [17, 18, "MetricName", "loss"], [24, 25, "MetricName", "loss"], [31, 32, "MetricName", "loss"]]}
{"text": "The loss function is as described in equations ( 6 ) and ( 7 ) , but with respect to our modified probability distribution P ( w ) given in equation ( 9 ) .", "entities": [[1, 2, "MetricName", "loss"]]}
{"text": "We find it necessary ( see section 5 ) to additionally define a coverage loss to penalize repeatedly attending to the same locations : covloss t = \u2211 i min ( a t i , c t i ) ( 12 ) Note that the coverage loss is bounded ; in particular covloss t \u2264 \u2211 i a t i = 1 .", "entities": [[14, 15, "MetricName", "loss"], [46, 47, "MetricName", "loss"]]}
{"text": "Equation ( 12 ) differs from the coverage loss used in Machine Translation .", "entities": [[8, 9, "MetricName", "loss"], [11, 13, "TaskName", "Machine Translation"]]}
{"text": "Finally , the coverage loss , reweighted by some hyperparameter \u03bb , is added to the primary loss function to yield a new composite loss function : loss t = \u2212 log P ( w * t ) + \u03bb \u2211 i min ( a t i , c t i ) ( 13 ) 3 Related Work Neural abstractive summarization .", "entities": [[4, 5, "MetricName", "loss"], [17, 18, "MetricName", "loss"], [24, 25, "MetricName", "loss"], [27, 28, "MetricName", "loss"], [60, 61, "TaskName", "summarization"]]}
{"text": "We train using Adagrad ( Duchi et al , 2011 ) with learning rate 0.15 and an initial accumulator value of 0.1 .", "entities": [[3, 4, "MethodName", "Adagrad"], [12, 14, "HyperparameterName", "learning rate"]]}
{"text": "We train on a single Tesla K40 m GPU with a batch size of 16 .", "entities": [[11, 13, "HyperparameterName", "batch size"]]}
{"text": "To obtain our final coverage model , we added the coverage mechanism with coverage loss weighted to \u03bb = 1 ( as described in equation 13 ) , and trained for a further 3000 iterations ( about 2 hours ) .", "entities": [[14, 15, "MetricName", "loss"]]}
{"text": "In this time the coverage loss converged to about 0.2 , down from an initial value of about 0.5 .", "entities": [[5, 6, "MetricName", "loss"]]}
{"text": "We also tried a more aggressive value of \u03bb = 2 ; this reduced coverage loss but increased the primary loss function , thus we did not use it .", "entities": [[15, 16, "MetricName", "loss"], [20, 21, "MetricName", "loss"]]}
{"text": "We evaluate our models with the standard ROUGE metric ( Lin , 2004b ) , reporting the F 1 scores for ROUGE - 1 , ROUGE - 2 and ROUGE - L ( which respectively measure the word - overlap , bigram - overlap , and longest common sequence between the reference summary and the summary to be evaluated ) .", "entities": [[29, 32, "MetricName", "ROUGE - L"]]}
{"text": "4 We also evaluate with the METEOR metric ( Denkowski and Lavie , 2014 ) , both in exact match mode ( rewarding only exact matches between words ) and full mode ( which additionally rewards matching stems , synonyms and paraphrases ) .", "entities": [[6, 7, "DatasetName", "METEOR"], [18, 20, "MetricName", "exact match"]]}
{"text": "Nevertheless , given that the disparity in the lead - 3 scores is ( +1.1 ROUGE - 1 , +2.0 ROUGE - 2 , +1.1 ROUGE - L ) points respectively , and our best model scores exceed by ( +4.07 ROUGE - 1 , +3.98 ROUGE - 2 , +3.73 ROUGE - L ) points , we may estimate that we outperform the only previous abstractive system by at least 2 ROUGE points allround .", "entities": [[25, 28, "MetricName", "ROUGE - L"], [51, 54, "MetricName", "ROUGE - L"]]}
{"text": "However , due to time required and relatively high cost of annotation , the great majority of research papers on summarization use exclusively automatic evaluation metrics , such as ROUGE ( Lin , 2004 ) , JS - 2 ( Louis and Nenkova , 2013 ) , S3 ( Peyrard et al , 2017 ) , BERTScore ( Zhang et al , 2020 ) , Mover - Score ( Zhao et al , 2019 ) etc .", "entities": [[20, 21, "TaskName", "summarization"], [67, 68, "MetricName", "Score"]]}
{"text": "MoverScore and JS - 2 performs worse both in extractive ( only achieved nearly 0.1 Pearson correlation ) and abstractive summaries .", "entities": [[15, 17, "MetricName", "Pearson correlation"]]}
{"text": "ROUGE - L measures overlap of the longest common subsequence between two texts ( Lin , 2004 ) .", "entities": [[0, 3, "MetricName", "ROUGE - L"]]}
{"text": "Pearson Correlation is a measure of linear correlation between two variables and is popular in metaevaluating metrics at the system level ( Lee Rodgers , 1988 ) .", "entities": [[0, 2, "MetricName", "Pearson Correlation"]]}
{"text": "After this filtering , we obtain an average inter - annotator agreement ( Krippendorff 's alpha ( Krippendorff , 2011 ) ) of 0.66 .", "entities": [[15, 16, "HyperparameterName", "alpha"]]}
{"text": "A dark green value in cell ( i , j ) denotes metric m i has a significantly higher Pearson correlation with human scores compared to metric m j ( p - value < 0.05 ) .", "entities": [[19, 21, "MetricName", "Pearson correlation"]]}
{"text": "13 ' - ' in cell ( i , j ) refers to the case when Pearson correlation of m i with human scores is less that of m j ( Sec . 4.1 ) .", "entities": [[16, 18, "MetricName", "Pearson correlation"]]}
{"text": "If m is a good proxy for human judgments , the F1 score ( Goutte and Gaussier , 2005 ) between y m pred and y true should be high .", "entities": [[11, 13, "MetricName", "F1 score"]]}
{"text": "While on the TAC 2009 dataset , JS - 2 achieves the highest F1 score , its performance is low on CNNDM Ext .", "entities": [[13, 15, "MetricName", "F1 score"]]}
{"text": "We use Eq . 1 to calculate Pearson correlation between different metrics and human judgments for different datasets and collected system outputs .", "entities": [[7, 9, "MetricName", "Pearson correlation"]]}
{"text": "The effectiveness of different automatic metrics - ROUGE - 2 ( Lin , 2004 ) , ROUGE - L ( Lin , 2004 ) , ROUGE - WE ( Ng and Abrecht , 2015 ) , JS - 2 ( Louis and Nenkova , 2013 ) and S3 ( Peyrard et al , 2017 ) is commonly evaluated based on their correlation with human judgments ( e.g. , on the TAC - 2008 ( Dang and Owczarzak , 2008 ) and TAC - 2009 ( Dang andOwczarzak , 2009 ) datasets ) .", "entities": [[16, 19, "MetricName", "ROUGE - L"]]}
{"text": "De - identification tasks based on the CRF machine learning algorithm has been carried out on this data set previously with precision scores ranging between 85 % and 95 % , recalls ranging between 71 % and 87 % and F1 - scores between 0.76 and 0.91 ( Dalianis and Velupillai , 2010 ; Berg and Dalianis , 2019 ) .", "entities": [[0, 3, "TaskName", "De - identification"], [7, 8, "MethodName", "CRF"], [40, 41, "MetricName", "F1"]]}
{"text": "Fairly high precision of 89.1 % was obtained , but with a recall of 54.3 % and F1 - score of 64.8 .", "entities": [[17, 20, "MetricName", "F1 - score"]]}
{"text": "Not presented in the table is the combination of training on real data and evaluation of pseudo data ( Real - Pseudo ) , but the results of this combination gave a precision of 86.37 and recall of 77.80 % and an F1 - score of 81.86 .", "entities": [[42, 45, "MetricName", "F1 - score"]]}
{"text": "The result of this combination is a precision of 65.83 % and recall of 74.79 % and F1 - score of 70.03 .", "entities": [[17, 20, "MetricName", "F1 - score"]]}
{"text": "Furthermore , we performed soft voting over the models , achieving the best results among the models , accomplishing a macro F1 - Score of 0.8291 , 0.7578 , and 0.7951 in English , Spanish , and Portuguese , respectively .", "entities": [[21, 24, "MetricName", "F1 - Score"]]}
{"text": "The soft voting approach has achieved macro F1 - Scores of 0.8291 , 0.7578 , and 0.7951 for English , Spanish , and Portuguese , respectively .", "entities": [[6, 8, "MetricName", "macro F1"]]}
{"text": "We achieve a new state - of - the - art on 24 fine - grained types of emotions ( with an average accuracy of 87.58 % ) .", "entities": [[22, 24, "MetricName", "average accuracy"]]}
{"text": "We also extend the task beyond emotion types to model Robert Plutchik 's 8 primary emotion dimensions , acquiring a superior accuracy of 95.68 % .", "entities": [[6, 7, "DatasetName", "emotion"], [15, 16, "DatasetName", "emotion"], [21, 22, "MetricName", "accuracy"]]}
{"text": "The authors cite an agreement of 0.50 Krippendorff 's alpha ( \u03b1 ) between the lab / expert annotators , and an ( \u03b1 ) of 0.28 between experts and AMT workers .", "entities": [[9, 10, "HyperparameterName", "alpha"], [11, 12, "HyperparameterName", "\u03b1"], [23, 24, "HyperparameterName", "\u03b1"]]}
{"text": "We use a much larger dataset and report an accuracy of the hashtag approach at 90 % based on human judgement as reported in Section 4 .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "Overall , we agree with ( Mohammad , 2012 ; Wang et al , 2012 ) that the accuracy acquired by enforcing a strict pipeline and limiting to emotion hashtags to final position is a reasonable measure for warranting good - quality data for training supervised systems , an assumption we have also validated with our empirical findings here .", "entities": [[18, 19, "MetricName", "accuracy"], [28, 29, "DatasetName", "emotion"]]}
{"text": "To train the GRNNs , we optimize the hyper - parameters of the network on a development set as we describe below , choosing a vocabulary size of 80 K words ( a vocabulary size we also use for the out - of - core classifiers ) , a word embedding vector of size 300 dimensions learnt directly from the training data , an input maximum length of 30 words , 7 epochs , and the Adam ( Kingma and Ba , 2014 ) optimizer with a learning rate of 0.001 .", "entities": [[76, 77, "MethodName", "Adam"], [84, 85, "HyperparameterName", "optimizer"], [87, 89, "HyperparameterName", "learning rate"]]}
{"text": "As Table 5 shows , we are able to model the 8 dimensions with an overall superior accuracy of 95.68 % .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "As Table 6 shows , on this subset of emotions , our system is 4.53 % ( acc ) higher than the best published results ( Volkova and Bachrach , 2016 ) , facilitated by the fact that we have an order of magnitude more training data .", "entities": [[17, 18, "MetricName", "acc"]]}
{"text": "As shown in Table 7 , we also apply ( Volkova and Bachrach , 2016 ) 's pre - trained model on our test set of the 6 emotions they predict ( which belong to plutchik - 2 ) , and acquire an overall accuracy of 26.95 % , which is significantly lower than our accuracy .", "entities": [[43, 45, "MetricName", "overall accuracy"], [55, 56, "MetricName", "accuracy"]]}
{"text": "On average over all languages and variables , the MTL models achieve 6.7 % - points higher Pearson correlation .", "entities": [[17, 19, "MetricName", "Pearson correlation"]]}
{"text": "For each language , the model was trained for roughly 15k iterations ( exactly 168 epochs ) with a batch size of 128 using the Adam optimizer ( Kingma and Ba , 2015 ) with learning rate 10 \u22123 , and .5 dropout on the hidden layers and .2 on the input layer .", "entities": [[19, 21, "HyperparameterName", "batch size"], [25, 26, "MethodName", "Adam"], [26, 27, "HyperparameterName", "optimizer"], [35, 37, "HyperparameterName", "learning rate"]]}
{"text": "As nonlinear activation function we used leaky ReLU with \" leakage \" of 0.01 .", "entities": [[2, 4, "HyperparameterName", "activation function"], [6, 8, "MethodName", "leaky ReLU"]]}
{"text": "The widely - used metrics include ( 1 ) BLEU ( Papineni et", "entities": [[9, 10, "MetricName", "BLEU"]]}
{"text": "al , 2002 ) , which was originally developed to evaluate machine translation systems ; ( 2 ) METEOR ( Denkowski and Lavie , 2014 ) , which aims to address BLEU 's weakness of being unable to measure semantic equivalents when applied to low - resource languages and has a better correlation with human judgment at the sentence / segment level than BLEU ; ( 3 ) ROUGE ( Lin , 2004 ) , a recall - based evaluation metric originally developed for text summarization , has also been used to evaluate paraphrase generation .", "entities": [[11, 13, "TaskName", "machine translation"], [18, 19, "DatasetName", "METEOR"], [31, 32, "MetricName", "BLEU"], [63, 64, "MetricName", "BLEU"], [84, 86, "TaskName", "text summarization"], [93, 95, "TaskName", "paraphrase generation"]]}
{"text": "Other reward functions have been explored by researchers , including ROUGE score , perplexity score and language fluency ( Siddique et al , 2020 ; .", "entities": [[13, 14, "MetricName", "perplexity"]]}
{"text": "Furthermore , ( Cao and Wan , 2020 ) also incorporated their model with a diversity loss to control diversity .", "entities": [[16, 17, "MetricName", "loss"]]}
{"text": "6 State - of - the - Art Performance Table 3 shows the ROUGE and BLEU scores of state - of - the - art performance on some most frequently used evaluation corpus in recent years : MSCOCO and Quora .", "entities": [[15, 16, "MetricName", "BLEU"], [37, 38, "DatasetName", "MSCOCO"]]}
{"text": "As shown in Table 5 , examples with low BLEU scores might include both relatively good and bad paraphrasing because BLEU scores only measure the overlap between outputs and references .", "entities": [[9, 10, "MetricName", "BLEU"], [20, 21, "MetricName", "BLEU"]]}
{"text": "Evaluation metrics As stated in Section 6 , BLEU scores and other automatic evaluation metrics based on similar principle are not good enough to evaluate paraphrase generation .", "entities": [[8, 9, "MetricName", "BLEU"], [25, 27, "TaskName", "paraphrase generation"]]}
{"text": "7 This portion contains roughly 1.81 M Hebrew tokens , most of which are dotted , with a varying level of accuracy , varying dotting styles , and varying degree of similarity to Modern Hebrew .", "entities": [[21, 22, "MetricName", "accuracy"]]}
{"text": "We provide document - level macro - averaged accuracy percentage results for a single run over our test set in Table 3 .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "Kontorovich ( 2001 ) trains an HMM on a vocalized and morphologically - tagged portion of the Hebrew Bible containing 30 , 743 words , and evaluates the result on a test set containing 2 , 852 words , achieving 81 % WOR accuracy .", "entities": [[43, 44, "MetricName", "accuracy"]]}
{"text": "Similarly , varying the number of LSTM layers between 2 and 5 ( keeping the total number of parameters roughly constant , close to the 5 , 313 , 223 parameters of our final model ) had little to no impact on the accuracy on the validation set .", "entities": [[6, 7, "MethodName", "LSTM"], [16, 19, "HyperparameterName", "number of parameters"], [43, 44, "MetricName", "accuracy"]]}
{"text": "We report case - sensitive de - tokenized BLEU using sacreBLEU ( Post , 2018 ) .", "entities": [[8, 9, "MetricName", "BLEU"], [10, 11, "MetricName", "sacreBLEU"]]}
{"text": "The training objective is to minimize the Connectionist Temporal Classification ( CTC ) loss ( Graves et al , 2006 ) .", "entities": [[9, 10, "TaskName", "Classification"], [11, 12, "DatasetName", "CTC"], [13, 14, "MetricName", "loss"]]}
{"text": "For each task , 2 additional downstream architectures are created by modifying the number of layers and the hidden dimensions compared to our default setting .", "entities": [[13, 16, "HyperparameterName", "number of layers"]]}
{"text": "We train all networks with Adam optimizer ( Kingma and Ba , 2015 ) and a decaying learning rate schedule .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "optimizer"], [17, 19, "HyperparameterName", "learning rate"]]}
{"text": "All models are trained up to 30 epochs and the best models are selected based on validation loss .", "entities": [[17, 18, "MetricName", "loss"]]}
{"text": "To evaluate the models , we use the following metrics : Joint Accuracy and Slot Accuracy ( Henderson et al , 2014b ) , Inform and Success ( Wen et al , 2017 ) , and BLEU score ( Papineni et al , 2002 ) .", "entities": [[12, 13, "MetricName", "Accuracy"], [15, 16, "MetricName", "Accuracy"], [36, 38, "MetricName", "BLEU score"]]}
{"text": "In Figure 2 and 3 , we plotted the Joint Goal Accuracy and BLEU metrics of our model by dialogue turn .", "entities": [[11, 12, "MetricName", "Accuracy"], [13, 14, "MetricName", "BLEU"]]}
{"text": "The dialogue agent achieves the highest accuracy in state tracking at the 1 st turn and gradually reduces to zero accuracy at later dialogue steps , i.e. 15 th to 18 th turns .", "entities": [[2, 3, "DatasetName", "agent"], [6, 7, "MetricName", "accuracy"], [20, 21, "MetricName", "accuracy"]]}
{"text": "The dialogue agent obtains the highest BLEU scores at the 3 rd turn and fluctuates between the 2 nd and 13 th turn .", "entities": [[2, 3, "DatasetName", "agent"], [6, 7, "MetricName", "BLEU"]]}
{"text": "For decoders , the previous word in training is the word in real court 's view , and the loss for timestep t is the negative log - likelihood of the target word w * t : L t = \u2212logP ( w * t ) ( 13 ) and the overall generation loss is : L gen = 1 T T t=0 L t ( 14 ) where T is the length of real court 's view .", "entities": [[19, 20, "MetricName", "loss"], [26, 29, "MetricName", "log - likelihood"], [53, 54, "MetricName", "loss"]]}
{"text": "The exact loss for the support decoder is : L sup = L gen\u0135 = 1 0\u0135 = 0 ( 15 ) the loss for the non - support decoder L nsup is obtained by the opposite way .", "entities": [[2, 3, "MetricName", "loss"], [18, 19, "DatasetName", "0"], [23, 24, "MetricName", "loss"]]}
{"text": "Thus , the total loss is : L total = L sup + L nsup + \u03bbL pred ( 16 ) where we set \u03bb to 0.1 in our model .", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "We use random sampling test to ensure that the annotation accuracy is over 95 % .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "BLEU 5", "entities": [[0, 1, "MetricName", "BLEU"]]}
{"text": "We use BLEU - 1 , BLEU - 2 to evaluate from the perspectives of unigram , bigram .", "entities": [[2, 3, "MetricName", "BLEU"], [6, 7, "MetricName", "BLEU"]]}
{"text": "BLEU - N is an average of BLEU - 1 , BLEU2 , BLEU - 3 ,", "entities": [[0, 1, "MetricName", "BLEU"], [7, 8, "MetricName", "BLEU"], [13, 14, "MetricName", "BLEU"]]}
{"text": "BLEU - 4 .", "entities": [[0, 1, "MetricName", "BLEU"]]}
{"text": "Accuracy of judgment prediction To evaluate the performance of the predictor , we calculate the precision ( p ) , recall ( r ) and , f 1 - score of supported and non - supported cases , respectively .", "entities": [[0, 1, "MetricName", "Accuracy"]]}
{"text": "Tab . 2 demonstrates the results of court 's view generation with ROUGE , BLEU , and BERT SCORE .", "entities": [[14, 15, "MetricName", "BLEU"], [17, 18, "MethodName", "BERT"]]}
{"text": "4 . Results of court 's view generation : From Tab . 2 , we can conclude that : ( 1 ) S2S tends to repeat words , which makes it get high BLEU but low BERT SCORE .", "entities": [[33, 34, "MetricName", "BLEU"], [36, 37, "MethodName", "BERT"]]}
{"text": "Based on the AC - NLG method , in the future , we can explore the following directions : ( 1 ) Improve the accuracy of judgment on a claim - level .", "entities": [[24, 25, "MetricName", "accuracy"]]}
{"text": "The defendants B and C jointly repaid the loan principal of $ 20 , 000 and the interest loss ( calculated from the bank 's loan interest rate at the same period from the date of prosecution to the date when the judgment is confirmed ) .", "entities": [[18, 19, "MetricName", "loss"]]}
{"text": "Therefore , this court supports the claim of the plaintiff that the defendant should return the loan of $ 20 , 000 and the corresponding loss of interest Acceptance .", "entities": [[25, 26, "MetricName", "loss"]]}
{"text": "( 13 ) Given M sentences S = { s 1 , s 2 , ... , s M } with i d = { i d 1 , ... , i d M } , we can minimize the loss for training : J ( \u03b8 )", "entities": [[40, 41, "MetricName", "loss"], [46, 47, "HyperparameterName", "\u03b8"]]}
{"text": "In addition , the dropout rate , learning rate , and maximal sequence length are set to 0.5 , 0.001 , and 100 , respectively .", "entities": [[7, 9, "HyperparameterName", "learning rate"]]}
{"text": "Adam optimizer ( Kingma and Ba , 2015 ) is adopted to optimize our model .", "entities": [[0, 1, "MethodName", "Adam"], [1, 2, "HyperparameterName", "optimizer"]]}
{"text": "To be consistent with various baselines ( Fan et al , 2019 ; Chen et al , 2020 ; , the term - level F1 score is used as the evaluation metric for both TOWE and AOPE tasks .", "entities": [[24, 26, "MetricName", "F1 score"]]}
{"text": "The F1 score of TSMSA ( BERT ) is in average 8 % higher than TSMSA ( Base ) and IOG .", "entities": [[1, 3, "MetricName", "F1 score"], [6, 7, "MethodName", "BERT"]]}
{"text": "Figure 2 ( a ) reveals that our model gradually converges as the number of epochs increases .", "entities": [[13, 16, "HyperparameterName", "number of epochs"]]}
{"text": "Figure 2 ( c ) shows that the best performance is achieved when the number of multi - head self - attention layers is 6 , and as the number increased , the model might be confronted with overfitting .", "entities": [[21, 23, "HyperparameterName", "attention layers"]]}
{"text": "Indeed , we find that issuegeneric and issue - specific classifiers achieve higher F1 scores on tweets written by conservative authors compared to liberal authors ( Figure 1 ) , even though there are fewer conservative tweets in the training data ( 334 conservative vs 385 liberal tweets ) .", "entities": [[13, 14, "MetricName", "F1"]]}
{"text": "Error Analysis We identify classification errors by qualitatively analyzing a random sample of 200 tweets that misclassified at least one frame .", "entities": [[0, 1, "MetricName", "Error"]]}
{"text": "We are interested in the successful classification of aggressive posts only and therefore , rather than reporting precision , recall and F - measures , we re - port accuracy as in equation ( 1 ) : Accuracy = true positives true positives + false negatives ( 1 )", "entities": [[29, 30, "MetricName", "accuracy"], [37, 38, "MetricName", "Accuracy"]]}
{"text": "To avoid over - fitting we set parameters fairly conservatively , with a maximum tree depth of 6 , the number of rounds at 10 and early stopping set to 5 , gamma at 1 , and the learning rate at 0.3 .", "entities": [[26, 28, "MethodName", "early stopping"], [32, 33, "HyperparameterName", "gamma"], [38, 40, "HyperparameterName", "learning rate"]]}
{"text": "We report classifier accuracy according to equation ( 1 ) on gold aggression : true labels in our CrimeBB test corpus .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "To ensure any change in accuracy is not due to the decrease in aggression : true training instances , we run a second experiment in which for all values of t both label subsets ( aggression : true and aggression : false ) are randomly reduced to 3223 instances - the size of the smallest attack score subcorpus ( per the cumulative n.posts column in Table 1 ) .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "For this latter experiment we report accuracies averaged over one hundred runs to smooth variation in the random sampling process ( identified as ' Acc . Control ' in Table 3 ) .", "entities": [[24, 25, "MetricName", "Acc"]]}
{"text": "In the case of the controlled training data setting there is at first a small increase in accuracy as t rises from 1 to 3 .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "This result suggests that the levels in the Wiki - Comments Corpus most closely matching the aggressive posts on HackForums are those in the attack score range 1 to 5 , and that the optimal value of t is between 2 and 3 . To illustrate the rise and fall in classification accuracy as t increases , we plot accuracies as boxplots for the 100 runs in the controlled training data setting ( Figure 4.3 ) .", "entities": [[52, 53, "MetricName", "accuracy"]]}
{"text": "Finally , we are also interested in applications of our research , including the questions of desired accuracy of any deployed system , the appropriate actions to take , and the ethics of data collection , analysis and intervention ( Kennedy et al , 2017 ; .", "entities": [[17, 18, "MetricName", "accuracy"]]}
{"text": "Pruning accuracy : the performance on a 3 - way classification task to automatically detect the level of granularity of a concept as a proxy to quantify the overall quality of the output taxonomies .", "entities": [[1, 2, "MetricName", "accuracy"]]}
{"text": "Pruning accuracy is estimated using gold - standard annotations that are created from a random sample of 1 , 000 nodes for each NKG .", "entities": [[1, 2, "MetricName", "accuracy"]]}
{"text": "Thanks to our method , in fact , we are able to achieve , even despite the baseline already reaching very high performance levels ( well above 90 % accuracy ) , improvements of up to 6 points , with an overall error reduction between around 40 % and 60 % .", "entities": [[29, 30, "MetricName", "accuracy"]]}
{"text": "4 In Table 3 , we report the mean perplexity of the models for both strategies .", "entities": [[9, 10, "MetricName", "perplexity"]]}
{"text": "As shown , the 2 - gram perplexity is lower than the 3 - gram perplexity in both cases .", "entities": [[7, 8, "MetricName", "perplexity"], [15, 16, "MetricName", "perplexity"]]}
{"text": "Table 4 lists the mean perplexity values over the 10 folds .", "entities": [[5, 6, "MetricName", "perplexity"]]}
{"text": "Here , the perplexity is lower for 3 - grams than for 2 - grams , which can be expected to yield better performance .", "entities": [[3, 4, "MetricName", "perplexity"]]}
{"text": "In case of the logos strategy , the model has a mean squared error ( MSE ) of 0.05 .", "entities": [[15, 16, "MetricName", "MSE"]]}
{"text": "In case of pathos the MSE is 0.03 .", "entities": [[5, 6, "MetricName", "MSE"]]}
{"text": "Table 5 : Accuracy of n - gram overlaps between the human - generated arguments for each strategy and the arguments computationally synthesized by our model and the baseline .", "entities": [[3, 4, "MetricName", "Accuracy"]]}
{"text": "We show that while entailment pairs are relatively easy ( accuracy scores ranging from .86 to .89 ) , the non - entailment pairs are exceedingly challenging , with the roberta - large model achieving accuracy scores ranging from .311 to .539 .", "entities": [[10, 11, "MetricName", "accuracy"], [35, 36, "MetricName", "accuracy"]]}
{"text": "We observe that idiomatic entailments are relatively easy to classify , with accuracy scores over .84 .", "entities": [[12, 13, "MetricName", "accuracy"]]}
{"text": "We use a fixed set of hyperparameters for all NLI fine - tuning experiments : learning rate of 1e \u22125 , batch size 32 , and maximum input length of 128 tokens .", "entities": [[15, 17, "HyperparameterName", "learning rate"], [21, 23, "HyperparameterName", "batch size"]]}
{"text": "Our best systems achieved 1st rank and scored 0.86 mAP and 0.58 macro average accuracy in Task - 1 and Task - 2 respectively .", "entities": [[9, 10, "MetricName", "mAP"], [13, 15, "MetricName", "average accuracy"]]}
{"text": "Comparing with other participating systems in the shared task , our submission is ranked 1 st with a mAP score of 0.86 .", "entities": [[18, 19, "MetricName", "mAP"]]}
{"text": "Comparing with other participating systems in the shared task , our submission is ranked 1 st with a macro average accuracy of 0.58 .", "entities": [[19, 21, "MetricName", "average accuracy"]]}
{"text": "\u03b1 D \u03b1 2 \u03b1 1 V i = p ( V", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [2, 3, "HyperparameterName", "\u03b1"], [4, 5, "HyperparameterName", "\u03b1"]]}
{"text": "( 5 ) Here , \u03b1 i is an attention score of each word i in the document v , learned via supervised training .", "entities": [[5, 6, "HyperparameterName", "\u03b1"]]}
{"text": "BM25 - Extra : The relevance score of BM - 25 is combined with four extra features : ( 1 ) percentage of query words with exact match in the document , ( 2 ) percentage of query words bigrams matched in the document , ( 3 ) IDF weighted document vector for feature # 1 , and ( 4 ) IDF weighted document vector for feature # 2 .", "entities": [[26, 28, "MetricName", "exact match"]]}
{"text": "Figure 1 ( right ) shows that we perform document ranking using the probability scores ( col - 2 ) of the RDoC construct ( e.g. loss ) within the cluster C1 .", "entities": [[9, 11, "TaskName", "document ranking"], [26, 27, "MetricName", "loss"]]}
{"text": "| | 2 where \u03b2 [ 0 , 1 ] controls the relevance of title , determined by cross - validation .", "entities": [[4, 5, "HyperparameterName", "\u03b2"], [6, 7, "DatasetName", "0"]]}
{"text": "We use \u03b2 { 0 , 1 } .", "entities": [[2, 3, "HyperparameterName", "\u03b2"], [4, 5, "DatasetName", "0"]]}
{"text": "SVM achieves a classification accuracy of 0.947 and mean average precision .", "entities": [[0, 1, "MethodName", "SVM"], [4, 5, "MetricName", "accuracy"], [9, 11, "MetricName", "average precision"]]}
{"text": "( mAP ) of 0.992 by ranking the abstracts in their respective clusters using the supervised prediction probabilities ( p ( q | v ) ) .", "entities": [[1, 2, "MetricName", "mAP"]]}
{"text": "By using the pretrained embeddings , the classification accuracy increases from 0.912 to 0.965 , this shows that distributional pretrained embeddings carry significant semantic knowledge .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "Furthermore , re - ranking using reRank ( BM25 - Extra ) and reRank ( QAR ) further results in the improvement of mAP score ( 0.994 vs 0.983 ) by shifting the intruder documents at the bottom of each impure cluster .", "entities": [[23, 24, "MetricName", "mAP"]]}
{"text": "For unsupervised model , using reRank ( BM25 - Extra ) relevance score between a query ( q ) , label ( RDoC construct ) of an abstract , and all the sentences ( s j ) in an abstract , we get an macroaverage accuracy ( MAA ) of 0.631 .", "entities": [[45, 46, "MetricName", "accuracy"]]}
{"text": "For supervised model , we get an MAA score of 0.772 and 0.737 by setting \u03b2", "entities": [[15, 16, "HyperparameterName", "\u03b2"]]}
{"text": "We achieve the highest MAA score of 0.789 by combining the predictions of ( 1 ) reRank ( BM25 - Extra ) , ( 2 ) version1 , and ( 3 ) r sup f with \u03b2 = 0 .", "entities": [[36, 37, "HyperparameterName", "\u03b2"], [38, 39, "DatasetName", "0"]]}
{"text": "Observe that our submission ( MIC - CIS ) scored a mAP score of 0.86 and MAA of 0.58 in Task - 1 and Task - 2 , respectively .", "entities": [[11, 12, "MetricName", "mAP"]]}
{"text": "We selected an LSTM for the recurrent layer ( Hochreiter and Schmidhuber , 1997 ) , with the AdaDelta optimizer ( Zeiler , 2012 ) .", "entities": [[3, 4, "MethodName", "LSTM"], [18, 19, "MethodName", "AdaDelta"], [19, 20, "HyperparameterName", "optimizer"]]}
{"text": "We used the development set to tune the number of hid - den units ( 128 ) , and the number of epochs ( 12 ) .", "entities": [[20, 23, "HyperparameterName", "number of epochs"]]}
{"text": "Since Task5 is synthetic data generated using rules , it is possible to obtain perfect accuracy using rules ( line 1 ) .", "entities": [[15, 16, "MetricName", "accuracy"]]}
{"text": "Figure 2 : Training dialog count vs. turn accuracy for bAbI dialog Task5 - OOV and Task6 .", "entities": [[8, 9, "MetricName", "accuracy"]]}
{"text": "The architecture and training of the RNN was the same as in Section 4 , except that here we did not have enough data for a validation set , so we instead trained until we either achieved 100 % accuracy on the training set or reached 200 epochs .", "entities": [[39, 40, "MetricName", "accuracy"]]}
{"text": "( G\u2212b ) ( 1 ) where \u03b1 is a learning rate ; a t is the action taken at timestep t ; h t is the dialog history at time t ; G is the return of the dialog ; x F denotes the Jacobian of F with respect to x ; b is a baseline described below ; and \u03c0 ( a | h ; w ) is the LSTM - i.e. , a stochastic policy which outputs a distribution over a given a dialog history h , parameterized by weights", "entities": [[7, 8, "HyperparameterName", "\u03b1"], [10, 12, "HyperparameterName", "learning rate"], [71, 72, "MethodName", "LSTM"]]}
{"text": "The baseline b is an estimate of the average return of the current policy , estimated on the last 100 dialogs using weighted importance sampling .", "entities": [[8, 10, "MetricName", "average return"]]}
{"text": "i ) , ( 5 ) where { \u03b2 t } are parameters to be learnt in training .", "entities": [[8, 9, "HyperparameterName", "\u03b2"]]}
{"text": "These representations are concatenated so that the dimension of the pooled attention vectors \u03b1 c , \u03b1 e is 1 + 2 + 4 + 8=15 .", "entities": [[13, 14, "HyperparameterName", "\u03b1"], [16, 17, "HyperparameterName", "\u03b1"]]}
{"text": "Finally , the prediction is made according to Y = softmax linear ( [ \u03b1 e ; \u03b1 c ; E ] ) , ( 8 ) where \u03b1 c , \u03b1 e and E are concatenated .", "entities": [[10, 11, "MethodName", "softmax"], [14, 15, "HyperparameterName", "\u03b1"], [17, 18, "HyperparameterName", "\u03b1"], [28, 29, "HyperparameterName", "\u03b1"], [31, 32, "HyperparameterName", "\u03b1"]]}
{"text": "We use one annotator 's annotation as the gold answer , and the other 's annotation as the prediction , and compute the F1 scores to measure the agreement , which is shown in Figure 3 . Table 1 shows the basic statistics of the dataset .", "entities": [[23, 24, "MetricName", "F1"]]}
{"text": "The optimizer is stochastic gradient descent ( SGD ) with a learning rate 0.1 .", "entities": [[1, 2, "HyperparameterName", "optimizer"], [3, 6, "MethodName", "stochastic gradient descent"], [7, 8, "MethodName", "SGD"], [11, 13, "HyperparameterName", "learning rate"]]}
{"text": "Compared with the feature - based method , DiSA has comparable performance on identifying thesis but has superior results on identifying main idea ( 9 % higher in F1 score ) and evidence ( 21 % higher in F1 score ) .", "entities": [[28, 30, "MetricName", "F1 score"], [38, 40, "MetricName", "F1 score"]]}
{"text": "RelativeSPE performs best with improvements of 2 - 3 % macro - F1 score compared with Sinusoidal and PosEmbedding .", "entities": [[10, 13, "MetricName", "macro - F1"]]}
{"text": "Removing ISA , the accuracy and the macro - F1 score decreases 1.8 % and 2.2 % .", "entities": [[4, 5, "MetricName", "accuracy"], [7, 10, "MetricName", "macro - F1"]]}
{"text": "Table 9 shows the macro - F1 scores of DiSA on identifying specific argument components .", "entities": [[4, 7, "MetricName", "macro - F1"]]}
{"text": "Without the ISA module , the identification of major claims and claims would decline by 3 % and 1.4 % absolute F1 score , respectively .", "entities": [[21, 23, "MetricName", "F1 score"]]}
{"text": "y = Softmax ( W h + b ) , \u03b8 = { W , b } ( 1 )", "entities": [[2, 3, "MethodName", "Softmax"], [10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "Finally , the parameters of the text classification network with attention supervision are trained to minimize both loss terms together as follows : L = L task ( \u0177 , y ) + \u00b5 L att ( \u03b1 , \u03b1 ) ( 2 ) where \u00b5 is a preference weight .", "entities": [[6, 8, "TaskName", "text classification"], [17, 18, "MetricName", "loss"], [37, 38, "HyperparameterName", "\u03b1"], [39, 40, "HyperparameterName", "\u03b1"]]}
{"text": "Requiring humans to explicitly annotate soft labels \u03b1 has been considered unrealistic ( Barrett et al , 2018 ) , and often delegated to implicit signals such as eye gaze .", "entities": [[7, 8, "HyperparameterName", "\u03b1"]]}
{"text": "T t = 1 exp ( \u03bb A ( w t ) ) ( 3 ) where \u03bb is a positive hyper - parameter that controls the variance of scores : when \u03bb increases , the distribution of \u03b1 becomes more skewed , guiding to attend a few of more important words .", "entities": [[38, 39, "HyperparameterName", "\u03b1"]]}
{"text": "Our key idea is to leverage causal signals ( Johansson et al , 2016 ) from human annotation A ( or attention labels \u03b1 ) of an input sample x to its corresponding model prediction\u0177 .", "entities": [[23, 24, "HyperparameterName", "\u03b1"]]}
{"text": "Then , knowing the quantity | \u0177 \u2212\u0233 t | , measured as the individualized treatment effect ( ITE ) , enables measuring how Algorithm 1 SANA Input : Training dataset D , Task - level annotation A Output : Model parameters { \u03c6 , \u03b8 } Initialize attention labels \u03b1 from A Using Eq ( 3 ) { \u03c6 , \u03b8 } argmin \u03c6 , \u03b8 L ( D , \u03b1 ; \u03c6 , \u03b8 )", "entities": [[45, 46, "HyperparameterName", "\u03b8"], [50, 51, "HyperparameterName", "\u03b1"], [61, 62, "HyperparameterName", "\u03b8"], [66, 67, "HyperparameterName", "\u03b8"], [71, 72, "HyperparameterName", "\u03b1"], [75, 76, "HyperparameterName", "\u03b8"]]}
{"text": "Using Eq ( 2 ) for z = 1 to z max do for each ( x , y ) D do h , \u03b1 f \u03c6", "entities": [[24, 25, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b8 ( h , \u03b1 ) for each w t x do if A ( w t ) > 0", "entities": [[0, 1, "HyperparameterName", "\u03b8"], [4, 5, "HyperparameterName", "\u03b1"], [19, 20, "DatasetName", "0"]]}
{"text": "\u03bb In Eq ( 3 ) Update attention labels \u03b1 from A Using Eq ( 3 ) { \u03c6 , \u03b8 } argmin \u03c6 , \u03b8 L ( D , \u03b1 ; \u03c6 , \u03b8 )", "entities": [[9, 10, "HyperparameterName", "\u03b1"], [20, 21, "HyperparameterName", "\u03b8"], [25, 26, "HyperparameterName", "\u03b8"], [30, 31, "HyperparameterName", "\u03b1"], [34, 35, "HyperparameterName", "\u03b8"]]}
{"text": "Using Eq ( 2 ) end return { \u03c6 , \u03b8 } much the word w t contributes to the original prediction via attention mechanism .", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "If T V D value is too low , we can give a penalty by decaying the human annotation A ( w t ) with a factor of \u03b3 , which we empirically set as 0.5 , to update the attention labels .", "entities": [[28, 29, "HyperparameterName", "\u03b3"]]}
{"text": "As described in Alg . 1 , SANA starts with the classification model trained with the initial attention labels \u03b1 .", "entities": [[19, 20, "HyperparameterName", "\u03b1"]]}
{"text": "Finally , based on\u0177 and\u0233 t , as defined in Eq ( 4 ) , we compute T V D and update the human annotation A by threshold and decay ratio \u03b3 .", "entities": [[31, 32, "HyperparameterName", "\u03b3"]]}
{"text": "For the stable update , we observe that increasing the coefficient \u03bb in Eq ( 3 ) is crucial , as T V D is not an optimal metric , preventing \u03b1 from being flattened .", "entities": [[31, 32, "HyperparameterName", "\u03b1"]]}
{"text": "We use 1layered GRU for each direction with hidden size of 150 for both SST2 and IMDB , and 300 for 20NG dataset , with g \u03b8 of 300 dimension with 0.5 dropout rate .", "entities": [[3, 4, "MethodName", "GRU"], [14, 15, "DatasetName", "SST2"], [16, 17, "DatasetName", "IMDB"], [26, 27, "HyperparameterName", "\u03b8"]]}
{"text": "In Alg . 1 , we use decay ratio \u03b3", "entities": [[9, 10, "HyperparameterName", "\u03b3"]]}
{"text": "Setting \u03b3 = 2.0 leads to the reported performance within z max = 5 .", "entities": [[1, 2, "HyperparameterName", "\u03b3"]]}
{"text": "For BERT , we train BERT - base architecture with a batch size of 4 over 3 epochs .", "entities": [[1, 2, "MethodName", "BERT"], [5, 6, "MethodName", "BERT"], [11, 13, "HyperparameterName", "batch size"]]}
{"text": "We used Adam with a learning rate of 6.25e - 5 and PiecewiseLinear scheduler .", "entities": [[2, 3, "MethodName", "Adam"], [5, 7, "HyperparameterName", "learning rate"]]}
{"text": "All parameters are optimized until convergence , using Adam optimizer of learning rate 0.001 .", "entities": [[8, 9, "MethodName", "Adam"], [9, 10, "HyperparameterName", "optimizer"], [11, 13, "HyperparameterName", "learning rate"]]}
{"text": "Tab . 2 shows the classification accuracy for three classification datasets .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "As we expected , the attention supervision using the sample - specific annotations gets a higher accuracy than that using the task - level annotations , but can not be scaled - up above 500 training samples , which is represented by the red reference line .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "In contrast , SANA improves accuracy with \u2265 1000 samples and its scalability .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "For better alignment , ( Tutek and\u0160najder , 2020 ) utilizes masked language model ( MLM ) loss and ( Mohankumar et al , 2020 ) invents orthogonal LSTM representations .", "entities": [[15, 16, "DatasetName", "MLM"], [17, 18, "MetricName", "loss"], [28, 29, "MethodName", "LSTM"]]}
{"text": "Target F - measure is calculated using the subset metric ( similar to metrics used by Yang and Cardie ( 2013 ) , Irsoy and Cardie ( 2014 ) ) ; if either the predicted or gold target tokens are a subset of the other , the match is counted when computing F - measure .", "entities": [[1, 4, "MetricName", "F - measure"], [52, 55, "MetricName", "F - measure"]]}
{"text": "F - pos , F - neg , and Acc - sent show the performance of the sentiment model on only the correctly predicted targets 5 .", "entities": [[9, 10, "MetricName", "Acc"]]}
{"text": "Significance thresholds are calculated for the best performing systems ( Tables 4 - 5 ) using the approximate randomization test ( Yeh , 2000 ) for target recall , precision , F - measure , Acc - sent and F - all .", "entities": [[31, 34, "MetricName", "F - measure"], [35, 36, "MetricName", "Acc"]]}
{"text": "A confidence interval of almost four F - measure points is required to obtain p < 0.05 .", "entities": [[6, 9, "MetricName", "F - measure"]]}
{"text": "The improvements in F - measure and F - all observed by using the best value of k is statistically significant for all schemes ( k=10 for lemma , k=250 for lemma+ATB , k=500 for lemma+D3 , with F - all values of 40.7 , 41.5 , and 39.1 respectively ) .", "entities": [[3, 6, "MetricName", "F - measure"], [27, 28, "DatasetName", "lemma"]]}
{"text": "This combined scheme results in the best results overall : F - score of 61.4 for targets , accuracy of 75.4 for sentiment and overall F - measure of 43.1 .", "entities": [[18, 19, "MetricName", "accuracy"], [25, 28, "MetricName", "F - measure"]]}
{"text": "Adding the richer linguistic resources results in both improved target precision , recall , and sentiment scores , with F - measure for positive targets reaching 67.7 for positive targets and 80 for negative targets .", "entities": [[19, 22, "MetricName", "F - measure"]]}
{"text": "However , the following three features have significant influence on the final results : modified lexical semantic vectors ( +3 % of the mean of T+S F1 scores ) , shared words ( +2 % ) , POS tags difference ( +2 % ) .", "entities": [[26, 27, "MetricName", "F1"]]}
{"text": "By optimizing the feature set , we were able to increase the mean score to 0.6484 of T+S F1 measure .", "entities": [[18, 19, "MetricName", "F1"]]}
{"text": "Specifically , CSANMT sets the new state of the art among existing augmentation techniques on the WMT14 English - German task with 30.94 BLEU score .", "entities": [[16, 17, "DatasetName", "WMT14"], [23, 25, "MetricName", "BLEU score"]]}
{"text": "( 1 ) can be improved as J mle ( \u0398 )", "entities": [[10, 11, "HyperparameterName", "\u0398"]]}
{"text": "Output : A set of augmented samples R = { r ( 1 ) , r ( 2 ) , ... , r ( K ) } 1 : Normalizing the importance of each element inr = ry \u2212 rx : Wr = | r | \u2212min ( | r | ) max ( | r | ) \u2212min ( | r | ) 2 : Set k = 1 , \u03c9 ( 1 ) \u223c N ( 0 , diag ( W 2 r ) ) , r ( 1 ) = r + \u03c9 ( 1 ) ( ry \u2212 rx ) 3 : Initialize the set of samples as R = { r ( 1 ) } .", "entities": [[67, 69, "HyperparameterName", "k ="], [78, 79, "DatasetName", "0"]]}
{"text": "( 6 ) can become a stationary distribution with the increase of the number of samples , which describes the fact that the diversity of each training instance is not infinite .", "entities": [[13, 16, "HyperparameterName", "number of samples"]]}
{"text": "( 3 ) and J ctl ( \u0398 \u2032 ) in Eq .", "entities": [[7, 8, "HyperparameterName", "\u0398"]]}
{"text": "The learning rate for finetuning the semantic encoder at the second training stage is set as 1e \u2212 5 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "We use the default setup of K = 40 for all three tasks , \u03b7 = 0.6 for both Zh En and En De while \u03b7 = 0.45 for En Fr .", "entities": [[6, 8, "HyperparameterName", "K ="]]}
{"text": "From the results , we can conclude that our approach outperforms existing augmentation strategies such as back - translation ( Sennrich et al , 2016a ; Wei et al , 2020a ) and switchout ( Wang et al , 2018 ) by a large margin ( up to 3.63 BLEU ) , which verifies that augmentation in continuous space is more effective than methods with discrete manipulations .", "entities": [[49, 50, "MetricName", "BLEU"]]}
{"text": "From Table 2 , our approach consistently performs better than existing methods ( Sennrich et al , 2016a ; Wang et al , 2018 ; Wei et al , 2020a ; Cheng et al , 2020 ) , yielding significant gains ( 0.65\u223c1.76 BLEU ) on the En De and En Fr tasks .", "entities": [[43, 44, "MetricName", "BLEU"]]}
{"text": "Moreover , we observe that CSANMT gives 30.16 BLEU on the En De task with the base setting , significantly outperforming the vanilla Transformer by 2.49 BLEU points .", "entities": [[8, 9, "MetricName", "BLEU"], [23, 24, "MethodName", "Transformer"], [26, 27, "MetricName", "BLEU"]]}
{"text": "Our approach yields a further improvement of 0.68 BLEU by equipped with the wider architecture , demonstrating superiority over the standard Transformer by 2.15 BLEU .", "entities": [[8, 9, "MetricName", "BLEU"], [21, 22, "MethodName", "Transformer"], [24, 25, "MetricName", "BLEU"]]}
{"text": "From Figures 4 ( a ) - 4 ( c ) , we can observe that gradually increasing the number of samples significantly improves BLEU scores , which demonstrates large gaps between K = 10 and K = 40 .", "entities": [[19, 22, "HyperparameterName", "number of samples"], [24, 25, "MetricName", "BLEU"], [32, 34, "HyperparameterName", "K ="], [36, 38, "HyperparameterName", "K ="]]}
{"text": "In our experiments , we default set K = 40 to achieve a balance between the training efficiency and translation quality .", "entities": [[7, 9, "HyperparameterName", "K ="]]}
{"text": "For the vanilla Transformer , the BLEU score reaches its peak at about 52 K iterations .", "entities": [[3, 4, "MethodName", "Transformer"], [6, 8, "MetricName", "BLEU score"]]}
{"text": "Figure 7 illustrates the prediction accuracy of both frequent and rare words .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "As expected , CSANMT generalizes to rare words better than the vanilla Transformer , and the gap of word prediction accuracy is as large as 16 % .", "entities": [[12, 13, "MethodName", "Transformer"], [20, 21, "MetricName", "accuracy"]]}
{"text": "From the results on WMT14 testsets in Table 6 , we can observe that CSANMT still outperforms the vanilla Transformer ( by more than 1.2 BLEU ) under the same amount of parameters , which shows that the additional parameters are not the key to the improvement .", "entities": [[4, 5, "DatasetName", "WMT14"], [19, 20, "MethodName", "Transformer"], [25, 26, "MetricName", "BLEU"]]}
{"text": "Moreover , CSANMT yields at least 0.9 BLEU gains equipped with much stronger baselines .", "entities": [[7, 8, "MetricName", "BLEU"]]}
{"text": "For example , the scale Transformer , which originally gives 29.3 BLEU in the En De task , now gives 31.37 BLEU with our continuous semantic augmentation strategy .", "entities": [[5, 6, "MethodName", "Transformer"], [11, 12, "MetricName", "BLEU"], [21, 22, "MetricName", "BLEU"]]}
{"text": "Compared to the vanilla Transformer , the proposed CSANMT improve the BLEU scores of the two tasks by 2.7 and 2.9 points , respectively .", "entities": [[4, 5, "MethodName", "Transformer"], [11, 12, "MetricName", "BLEU"]]}
{"text": "We measure the performance with the 4gram BLEU score ( Papineni et al , 2002 ) .", "entities": [[7, 9, "MetricName", "BLEU score"]]}
{"text": "Both the case - sensitive tokenized BLEU ( compued by multi - bleu.pl ) and the detokenized sacrebleu 3 ( Post , 2018 ) are reported on the En De and En Fr tasks .", "entities": [[6, 7, "MetricName", "BLEU"], [17, 18, "MetricName", "sacrebleu"]]}
{"text": "We train all models using the Adam optimizer with adaptive learning rate schedule ( warm - up step with 4 K ) as in ( Vaswani et al , 2017 ) .", "entities": [[6, 7, "MethodName", "Adam"], [7, 8, "HyperparameterName", "optimizer"], [10, 12, "HyperparameterName", "learning rate"]]}
{"text": "Each input sequence is tokenized by the tokenizer , and GPT - 2 optimizes the model weights by minimizing the negative log - likelihood for the next - token prediction .", "entities": [[10, 11, "MethodName", "GPT"], [21, 24, "MetricName", "log - likelihood"]]}
{"text": "Therefore , this model generated an interviewer 's response text rather than were excluded from the dataset . Table 3 : Average BLEU - 4 scores .", "entities": [[22, 23, "MetricName", "BLEU"]]}
{"text": "BLEU - 4 score ( standard deviation )", "entities": [[0, 1, "MetricName", "BLEU"]]}
{"text": "As an evaluation of semantic content consisting of a combination of the verb and object - features , we show the average of BLEU scores using 4grams in the test set in Table 3 .", "entities": [[23, 24, "MetricName", "BLEU"]]}
{"text": "We changed the dialogue context length from 1 to 5 and found that a model with a dialogue context length of three achieved the best performance in the validation dataset .", "entities": [[4, 6, "HyperparameterName", "context length"], [18, 20, "HyperparameterName", "context length"], [28, 30, "DatasetName", "validation dataset"]]}
{"text": "The results showed that the model performance for the seven - classes classification was 0.39 in accuracy and 0.30 in weighted average of the F1 score .", "entities": [[16, 17, "MetricName", "accuracy"], [24, 26, "MetricName", "F1 score"]]}
{"text": "In the model evaluation for generating semantic content , the proposed model outperformed the two baseline models , retrieval and generative , in the automatic evaluation using BLEU - 4 .", "entities": [[27, 28, "MetricName", "BLEU"]]}
{"text": "Further , a user study should be conducted , as it is known that automatic evaluation using BLEU does not always correlate with human evaluation ( Liu et al , 2016 ) .", "entities": [[17, 18, "MetricName", "BLEU"]]}
{"text": "Model Optimization To account for the imbalance present among the labels , we use classbalanced focal loss as the optimization loss function ( Cui et al , 2019 ) , as formulated in Equation 5 .", "entities": [[15, 17, "MethodName", "focal loss"], [20, 21, "MetricName", "loss"]]}
{"text": "[ 0 , 1 ) , where n y is the number of samples in the ground truth class y.", "entities": [[1, 2, "DatasetName", "0"], [11, 14, "HyperparameterName", "number of samples"]]}
{"text": "= 1 \u2212 \u03b2 1 \u2212 \u03b2 ny L ( p , y ) ( 5 )", "entities": [[3, 4, "HyperparameterName", "\u03b2"], [6, 7, "HyperparameterName", "\u03b2"]]}
{"text": "The two objectives are weighted by a parameter \u03b3 , which controls the importance placed on the auxiliary task ( 1 \u2212 \u03b3 for the primary task ) .", "entities": [[8, 9, "HyperparameterName", "\u03b3"], [22, 23, "HyperparameterName", "\u03b3"]]}
{"text": "+ \u03b1 ( s ) = 1 ) , as formulated in Equation 7to regulate the information resulting from the two encoders ( Figure 2 ) .", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1 ( p ) h ( p ) + \u03b1 ( s ) h ( s ) ( 7 )", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [9, 10, "HyperparameterName", "\u03b1"]]}
{"text": "Grid search was performed to find the optimal value of hyperparameters and their range is summarized as : size of BiLSTM and dense layers { 128 , 256 , 512 } , embedding size d { 100 , 200 , 300 } , dropout \u03b4 { 0.1 , 0.2 , 0.3 , 0.4 , 0.5.0.6 } , learning rate \u03bb { 10 \u22125 , 10 \u22124 , 10 \u22123 , 10 \u22122 , 10 \u22121 } , weight decay \u03c9 { 10 \u22126 , 10 \u22125 , 10 \u22124 , 10 \u22123 } , optimizer { Adam , Adadelta } , batch size b { 32 , 64 , 128 } and epochs ( < 100 ) .", "entities": [[20, 21, "MethodName", "BiLSTM"], [44, 45, "HyperparameterName", "\u03b4"], [57, 59, "HyperparameterName", "learning rate"], [77, 79, "MethodName", "weight decay"], [94, 95, "HyperparameterName", "optimizer"], [96, 97, "MethodName", "Adam"], [98, 99, "MethodName", "Adadelta"], [101, 103, "HyperparameterName", "batch size"]]}
{"text": "For the MTL experiments , we tune the weightage of the auxiliary task ( \u03b3 [ 0.1 , 0.9 ] with intervals of 0.1 ) for each task pair .", "entities": [[14, 15, "HyperparameterName", "\u03b3"]]}
{"text": "For instance , we find the optimal value of \u03b3 for hate speech ( as the auxiliary task ) to be 0.4 in all Homogeneous task cases and of emotion detection to be 0.2 for the Heterogeneous tasks .", "entities": [[9, 10, "HyperparameterName", "\u03b3"], [11, 13, "DatasetName", "hate speech"], [29, 30, "DatasetName", "emotion"]]}
{"text": "BiLSTM classifier has hidden size = 256 , and the number of units in the penultimate dense layer is 128 .", "entities": [[0, 1, "MethodName", "BiLSTM"], [10, 13, "HyperparameterName", "number of units"]]}
{"text": "For all our experiments , we use Adam optimizer ( Kingma and Ba , 2014 ) and initialize model weights using Xavier initialization ( Glorot and Bengio , 2010 ) .", "entities": [[7, 8, "MethodName", "Adam"], [8, 9, "HyperparameterName", "optimizer"], [21, 23, "MethodName", "Xavier initialization"]]}
{"text": "We set the batch size to 128 and the learning rate to 1e \u2212 3 .", "entities": [[3, 5, "HyperparameterName", "batch size"], [9, 11, "HyperparameterName", "learning rate"]]}
{"text": "We report the average macro F1 scores across the 5 folds to account for imbalance , as previously used in multi - label settings ( Zhang and Zhou , 2013 6 Results and Discussion", "entities": [[4, 6, "MetricName", "macro F1"]]}
{"text": "We measure and report denotation accuracy by evaluating all predicted queries using the SEMPRE toolkit ( Berant et al , 2013 ) .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "On Overnight , prompt tuned denotation accuracy exceeds fine - tuned counterparts by up to 5 points with T5 - large and T5 - xl .", "entities": [[6, 7, "MetricName", "accuracy"], [18, 19, "MethodName", "T5"], [22, 23, "MethodName", "T5"]]}
{"text": "For T5 - small and T5 - base , prompt tuning remains competitive ( within 1 % average accuracy ) with fine - tuning when predicting canonical forms .", "entities": [[1, 2, "MethodName", "T5"], [5, 6, "MethodName", "T5"], [17, 19, "MetricName", "average accuracy"]]}
{"text": "On TOPv2 , prompt tuning achieves an absolute improvement of 15 % mean accuracy over fine - tuning on the lowest SPIS split .", "entities": [[1, 2, "DatasetName", "TOPv2"], [13, 14, "MetricName", "accuracy"]]}
{"text": "On Overnight , our best model - T5 - xl PT with canonical representations and constrained decodingoutperforms the BART FT model of Shin et al ( 2021 ) by 5 accuracy points , and GPT - 3 by more than 2 points .", "entities": [[7, 8, "MethodName", "T5"], [18, 19, "MethodName", "BART"], [30, 31, "MetricName", "accuracy"], [34, 35, "MethodName", "GPT"]]}
{"text": "On TOPv2 , we use a learning rate of 10 \u22124 and batch size of 128 .", "entities": [[1, 2, "DatasetName", "TOPv2"], [6, 8, "HyperparameterName", "learning rate"], [12, 14, "HyperparameterName", "batch size"]]}
{"text": "On Overnight , we use a learning rate of 10 \u22123 and a batch size of 64 across all sizes of T5 .", "entities": [[6, 8, "HyperparameterName", "learning rate"], [13, 15, "HyperparameterName", "batch size"], [21, 22, "MethodName", "T5"]]}
{"text": "We use 150 prompt tokens for all model sizes with a learning rate of 0.3 optimized with AdaFactor .", "entities": [[11, 13, "HyperparameterName", "learning rate"], [17, 18, "MethodName", "AdaFactor"]]}
{"text": "x ( i ) ) , ( 1 ) where N is the number of examples in the training set , y seg and y typ are the decomposed segmentation and type tags corresponding to the two sub - task modules , and \u03b1 and \u03b2 are the hyperparameters controlling the importance of the two modules contributions respectively .", "entities": [[43, 44, "HyperparameterName", "\u03b1"], [45, 46, "HyperparameterName", "\u03b2"]]}
{"text": "We used the original 10 - fold cross validation splits to calculate averaged F1 score , using 10 % of the training set for development .", "entities": [[13, 15, "MetricName", "F1 score"]]}
{"text": "The values of \u03b1 and \u03b2 in the objective function were always set to 1.0 .", "entities": [[3, 4, "HyperparameterName", "\u03b1"], [5, 6, "HyperparameterName", "\u03b2"]]}
{"text": "We used the Stochastic Gradient Descent ( SGD ) optimization of batch size 10 , with a momentum 0.9 to update the model parameters , with the learning rate 0.01 , the decay rate 0.05 ; The learning rate decays over epochs by \u03b7/ ( 1 + e * \u03c1 ) , where \u03b7 is the learning rate , e is the epoch number , and \u03c1 is the decay rate .", "entities": [[3, 6, "MethodName", "Stochastic Gradient Descent"], [7, 8, "MethodName", "SGD"], [11, 13, "HyperparameterName", "batch size"], [27, 29, "HyperparameterName", "learning rate"], [32, 34, "HyperparameterName", "decay rate"], [37, 39, "HyperparameterName", "learning rate"], [56, 58, "HyperparameterName", "learning rate"], [62, 64, "HyperparameterName", "epoch number"], [69, 71, "HyperparameterName", "decay rate"]]}
{"text": "In this study , \u03b1 and \u03b2 in Eq . 1 are both set to 1.0 , and we leave other tuning choices for future investigation .", "entities": [[4, 5, "HyperparameterName", "\u03b1"], [6, 7, "HyperparameterName", "\u03b2"]]}
{"text": "We achieve an overall ROC AUC of 98.46 on Kaggletoxic comment dataset and show that it beats other architectures by a good margin .", "entities": [[4, 6, "MetricName", "ROC AUC"]]}
{"text": "There are several ways to handle this problem , however , we choose a more recent technique which modifies the standard cross entropy loss function known as Focal Loss ( Lin et al , 2017 ) .", "entities": [[23, 24, "MetricName", "loss"], [27, 29, "MethodName", "Focal Loss"]]}
{"text": "Proposed Model : The model proposed in ( Zhao et al , 2018 ) has been used for the experimentation with an inclusion of Focal Loss ( Lin et al , 2017 ) as a loss function to address the class imbalance problem .", "entities": [[24, 26, "MethodName", "Focal Loss"], [35, 36, "MetricName", "loss"]]}
{"text": "= \u2212\u03b1 t ( 1 \u2212 p t ) \u03b3 log ( p t ) , where p t = { p if y = 1 1 \u2212 p else \u03b3 is", "entities": [[9, 10, "HyperparameterName", "\u03b3"], [30, 31, "HyperparameterName", "\u03b3"]]}
{"text": "In our experiments we observed that RMSProp ( Tieleman and Hinton , 2012 ) and Adam ( Kingma and Ba , 2014 ) as an optimizer works well for training RNNs and CNNs respectively and used this throughout .", "entities": [[6, 7, "MethodName", "RMSProp"], [15, 16, "MethodName", "Adam"], [25, 26, "HyperparameterName", "optimizer"]]}
{"text": "The learning rate was kept between [ .1 and .001 ] .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "The \u03b1 and \u03b3 values in focal loss were experimented for [ 1.5 , 2 , 2.5 , 3 , 3.5 ] and [ .2 , .25 , .3 ] and finally \u03b1 = 2 and \u03b3=0.25 were taken .", "entities": [[1, 2, "HyperparameterName", "\u03b1"], [3, 4, "HyperparameterName", "\u03b3"], [6, 8, "MethodName", "focal loss"], [32, 33, "HyperparameterName", "\u03b1"]]}
{"text": "We demonstrated that using focal loss along with CapsNet gave us .25 raise in the ROC - AUC for Kaggle 's toxic comment classification and 1.39 and .80 gain in F1 scores on TRAC shared task dataset in English , from Facebook and Twitter comments respectively .", "entities": [[4, 6, "MethodName", "focal loss"], [8, 9, "MethodName", "CapsNet"], [15, 18, "MetricName", "ROC - AUC"], [21, 24, "TaskName", "toxic comment classification"], [30, 31, "MetricName", "F1"]]}
{"text": "BERT has two parameter intensive settings : BERT BASE : 12 layers , 768 hidden dimensions and 12 attention heads ( in transformer ) with the total number of parameters , 110 M ; BERT LARGE : 24 layers , 1024 hidden dimensions and 16 attention heads ( in transformer ) with the total number of parameters , 340M. We only extend BERT with one extra taskspecific layer and fine - tune BERT on each end task .", "entities": [[0, 1, "MethodName", "BERT"], [7, 8, "MethodName", "BERT"], [8, 9, "MethodName", "BASE"], [27, 30, "HyperparameterName", "number of parameters"], [34, 35, "MethodName", "BERT"], [54, 57, "HyperparameterName", "number of parameters"], [62, 63, "MethodName", "BERT"], [72, 73, "MethodName", "BERT"]]}
{"text": "Then the hidden representation is passed to two separate dense layers followed by softmax functions : Training the RRC model involves minimizing the loss that is designed as the averaged cross entropy on the two pointers : l 1 = softmax ( W 1 h + b 1 ) and l 2 = softmax ( W 2 h + b 2 ) , where W 1 , W 2 R r h and b 1 , b 2", "entities": [[13, 14, "MethodName", "softmax"], [23, 24, "MetricName", "loss"], [40, 41, "MethodName", "softmax"], [53, 54, "MethodName", "softmax"]]}
{"text": "The labels are predicted as taking argmax function at each position of l 3 and the loss function is the averaged cross entropy across all positions of a sequence .", "entities": [[16, 17, "MetricName", "loss"]]}
{"text": "1 \u2207 \u0398 L 0 2 { D DK , 1 , . . .", "entities": [[2, 3, "HyperparameterName", "\u0398"], [4, 5, "DatasetName", "0"]]}
{"text": "+ L MRC ( D MRC , i ) u 6 \u2207 \u0398 L \u2207 \u0398 L + BackProp ( L partial ) 7 end 8 \u0398 ParameterUpdates ( \u2207 \u0398 L ) to be able to fit into GPU memory .", "entities": [[12, 13, "HyperparameterName", "\u0398"], [15, 16, "HyperparameterName", "\u0398"], [26, 27, "HyperparameterName", "\u0398"], [30, 31, "HyperparameterName", "\u0398"]]}
{"text": "In line 5 , it computes the partial joint loss L partial of two subbatches D DK , i and D MRC , i from the i - th iteration through forward pass .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "Line 6 accumulates the gradients produced by backpropagation from the partial joint loss .", "entities": [[12, 13, "MetricName", "loss"]]}
{"text": "Only the gradients \u2207 \u0398 are kept throughout all iterations and used to update parameters ( based on the chosen optimizer ) in line 8 .", "entities": [[4, 5, "HyperparameterName", "\u0398"], [20, 21, "HyperparameterName", "optimizer"]]}
{"text": "We set a static loss scale of 2 in FP16 , which can avoid any over / under - flow of floating point computation .", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "The maximum length of post - training is set to 320 with a batch size of 16 for each type of knowledge .", "entities": [[13, 15, "HyperparameterName", "batch size"]]}
{"text": "The number of subbatch u is set to 2 , which is good enough to store each sub - batch iteration into a GPU memory of 11G. We use Adam optimizer and set the learning rate to be 3e - 5 .", "entities": [[29, 30, "MethodName", "Adam"], [30, 31, "HyperparameterName", "optimizer"], [34, 36, "HyperparameterName", "learning rate"]]}
{"text": "We use all default hyper - parameter settings for this baseline except the number of epochs , which is set as 60 for better convergence .", "entities": [[13, 16, "HyperparameterName", "number of epochs"]]}
{"text": "To be consistent with existing research on MRC , we use the same evaluation script from SQuAD 1.1 ( Rajpurkar et al , 2016 ) for RRC , which reports Exact Match ( EM ) and F1 scores .", "entities": [[16, 17, "DatasetName", "SQuAD"], [30, 32, "MetricName", "Exact Match"], [33, 34, "MetricName", "EM"], [36, 37, "MetricName", "F1"]]}
{"text": "For ASC , we compute both accuracy and Macro - F1 over 3 classes of polarities , where Macro - F1 is the major metric as the imbalanced classes introduce biases on accuracy .", "entities": [[6, 7, "MetricName", "accuracy"], [8, 11, "MetricName", "Macro - F1"], [18, 21, "MetricName", "Macro - F1"], [32, 33, "MetricName", "accuracy"]]}
{"text": "We set the maximum number of epochs to 4 for BERT variants , though most runs converge just within 2 epochs .", "entities": [[4, 7, "HyperparameterName", "number of epochs"], [10, 11, "MethodName", "BERT"]]}
{"text": "During testing , for a given input of length N , the self attention vector \u03b1 = \u03b1 1 , \u03b1 2 , ... , \u03b1 N is first extracted ( details skipped for brevity , refer Xu et", "entities": [[15, 16, "HyperparameterName", "\u03b1"], [17, 18, "HyperparameterName", "\u03b1"], [20, 21, "HyperparameterName", "\u03b1"], [25, 26, "HyperparameterName", "\u03b1"]]}
{"text": "\u03b1 i = 0 ,", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [3, 4, "DatasetName", "0"]]}
{"text": "if \u03b1 i > \u00b5 + 2\u03c3 1 , otherwise ( 1 )", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "( 3 ) where L is the loss function and M \u03b8 is the translation system with parameter", "entities": [[7, 8, "MetricName", "loss"], [11, 12, "HyperparameterName", "\u03b8"]]}
{"text": "We speculate that minimizing the token - level cross entropy loss in Eq . 3 may help produce an output that is grammatically correct but not sarcastic enough .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "For the neutralization module , the embedding dimension size is set to 128 , two layers of LSTMs of hidden dimension of 200 are used .", "entities": [[6, 8, "HyperparameterName", "embedding dimension"]]}
{"text": "The classifier trains for 10 epochs with a batch size of 32 , and achieves a validation accuracy of 96 % and training accuracy of 98 % .", "entities": [[8, 10, "HyperparameterName", "batch size"], [17, 18, "MetricName", "accuracy"], [23, 24, "MetricName", "accuracy"]]}
{"text": "Training happens in 100 , 000 iterations and the batch size is set to 64 .", "entities": [[9, 11, "HyperparameterName", "batch size"]]}
{"text": "At the end of the training , the bigram BLEU score on the validation set turns out to be 59.3 % .", "entities": [[9, 11, "MetricName", "BLEU score"]]}
{"text": "It trains with a batch size of 64 and produces a validation accuracy of 78.3 % .", "entities": [[4, 6, "HyperparameterName", "batch size"], [12, 13, "MetricName", "accuracy"]]}
{"text": "The model was trained with 10 epochs and batch size of 128 .", "entities": [[8, 10, "HyperparameterName", "batch size"]]}
{"text": "The second architecture was identical to the first , except the first layer was a word embedding using GloVe 2 pre - trained on Twitter data with embedding dimension of 100 .", "entities": [[18, 19, "MethodName", "GloVe"], [27, 29, "HyperparameterName", "embedding dimension"]]}
{"text": "Among all architectures , the best results came from ELMo embedding ( F1 = 0.64 ) .", "entities": [[9, 10, "MethodName", "ELMo"], [12, 13, "MetricName", "F1"]]}
{"text": "The model performed less well for the validation set ( F1 = 0.41 ) , below the average F1 score of 0.50 among all teams , which might result from overfitting .", "entities": [[10, 11, "MetricName", "F1"], [17, 19, "MetricName", "average F1"]]}
{"text": "k = { t 1 , . . .", "entities": [[0, 2, "HyperparameterName", "k ="]]}
{"text": "The cross - entropy between the decoder 's output and the original template is used as the loss function : L = \u2212 m c=1 log P ( tc | t1 , c\u22121 , X ) ( 5 ) 4 Experiments We choose the SemEval - 2014 restaurant review ( Rest14 ) ( Pontiki et al , 2014a ) , a variant of Rest14 ( Rest14 - hard ) ( Xue and Li , 2018 ) and the multiaspect multi - sentiment ( MAMS ) ( Jiang et al , 2019 ) datasets for sentence - level sentiment , the Tri - pAdvisor ( Wang et al , 2010 ) and BeerAdvocate ( McAuley et al , 2012 ; Lei et al , 2016 ) datasets for document - level sentiment .", "entities": [[17, 18, "MetricName", "loss"], [83, 84, "DatasetName", "MAMS"], [111, 112, "DatasetName", "BeerAdvocate"]]}
{"text": "We select the fine - tuning learning rate from { 4e - 5 , 2e - 5 , and 1e - 5 } and batch size from { 8 , 16 , 24 } for different models .", "entities": [[6, 8, "HyperparameterName", "learning rate"], [24, 26, "HyperparameterName", "batch size"]]}
{"text": "For instance , \" The given_category category has a polarity_type label \" and \" The sentiment polarity of given_category is polarity_type \" give 82.31 % and 83.78 % accuracy , respectively , indicating that the template has influence on the final performance .", "entities": [[28, 29, "MetricName", "accuracy"]]}
{"text": "Third , BART generation performs significantly better than BART MLM , giving absolutely 3.89 % stronger accuracy on MAMS , demonstrating the effectiveness of the generation method .", "entities": [[2, 3, "MethodName", "BART"], [8, 9, "MethodName", "BART"], [9, 10, "DatasetName", "MLM"], [16, 17, "MetricName", "accuracy"], [18, 19, "DatasetName", "MAMS"]]}
{"text": "For example , when there are only 10 training instances , our model gives accuracy scores of 82.01 % on Rest14 , as compared to 38.57 % by BERT classification and 50.16 % by BART classification .", "entities": [[14, 15, "MetricName", "accuracy"], [28, 29, "MethodName", "BERT"], [34, 35, "MethodName", "BART"]]}
{"text": "Figure 5 shows the accuracy of BART classification and our model against the frequency .", "entities": [[4, 5, "MetricName", "accuracy"], [6, 7, "MethodName", "BART"]]}
{"text": "In the zero frequency , our method gives absolutely 8.03 % stronger accuracy than BART classification .", "entities": [[12, 13, "MetricName", "accuracy"], [14, 15, "MethodName", "BART"]]}
{"text": "Current text attackers , like 1 Our code and data are available at : https://github.com/JonRusert/SampleShielding TextFooler and Bert - Attack ( Li et al , 2020 ) , are able to reduce near perfect classification accuracy down to 5 % .", "entities": [[35, 36, "MetricName", "accuracy"]]}
{"text": "When the attacker does not have knowledge of Sample Shielding , our defense reduces attack success rate from near total decimation 90 - 100 % down to 13 - 36 % , while still maintaining accuracy on original texts .", "entities": [[35, 36, "MetricName", "accuracy"]]}
{"text": "5 We calibrated classifier accuracies against previous research ( Li et al , 2020 ; 6 huggingface.co/textattack 7 textattack.readthedocs.io/en/latest/index.html we set k = 100 and p = 0.3 .", "entities": [[21, 23, "HyperparameterName", "k ="]]}
{"text": "Additionally , shifting sampling ( Section 2.2.1 ) shielding typically achieved 10 - 20 points lower accuracy compared to random , thus we do not include it in the results .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "We examine accuracy and Attack Success Rate : accuracy = # examples_classif ied_correctly # total_examples ( 1 ) ASR = Original Acc .", "entities": [[2, 3, "MetricName", "accuracy"], [8, 9, "MetricName", "accuracy"], [21, 22, "MetricName", "Acc"]]}
{"text": "BERT is the strongest classifier achieving 91 - 100 % accuracy on the original datasets .", "entities": [[0, 1, "MethodName", "BERT"], [10, 11, "MetricName", "accuracy"]]}
{"text": "For AG News , attacks are less successful against BERT ; accuracy drops to 19 % in the strongest attack ( TextFooler ) , and only to 49 % in the weakest ( TextBugger", "entities": [[1, 3, "DatasetName", "AG News"], [9, 10, "MethodName", "BERT"], [11, 12, "MetricName", "accuracy"]]}
{"text": "While our settings of p = 0.3 and k = 100 for our main results are reasonable values ( Table 1 , Table 2 ) they are not necessarily optimal .", "entities": [[8, 10, "HyperparameterName", "k ="]]}
{"text": "Sample Shielding , an intuitively designed defense which is attacker and classifier agnostic , protects effectively ; reducing ASR from 90 - 100 % down to 14 - 34 % with minimal accuracy loss ( 3 % ) in original texts .", "entities": [[32, 33, "MetricName", "accuracy"], [33, 34, "MetricName", "loss"]]}
{"text": "Our German French system achieved 35.0 BLEU and ranked the second among all anonymous submissions , and our French German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions .", "entities": [[6, 7, "MetricName", "BLEU"], [23, 24, "MetricName", "BLEU"]]}
{"text": "Formally , for the n - th block of the encoder : B n e = BLOCK e ( B n\u22121 e ) , ( 1 ) where BLOCK e ( ) is the block function , in which the layer function F ( ) is iterated M n times , i.e. where l { 1 , 2 , ... , M n } , H n , l e and \u0398 n , l e are the representation and parameters of the l - th layer in the n - th block , respectively .", "entities": [[71, 72, "HyperparameterName", "\u0398"]]}
{"text": "We set the batch size as 4096 and the parameter - - update - freq as 16 .", "entities": [[3, 5, "HyperparameterName", "batch size"]]}
{"text": "We report case - sensitive SacreBLEU scores using Sacre - BLEU ( Post , 2018 ) 3 , using international tokenization for German\u2194French .", "entities": [[5, 6, "MetricName", "SacreBLEU"], [10, 11, "MetricName", "BLEU"]]}
{"text": "German French For De Fr , iterative BT improves our baseline performance on newstest 2019 by about 2.5 BLEU .", "entities": [[18, 19, "MetricName", "BLEU"]]}
{"text": "The addition of KD and model ensemble improves single model performance by 0.8 BLEU , but combining this with fine - tuning and reranking gives us a total of 2 BLEU .", "entities": [[13, 14, "MetricName", "BLEU"], [30, 31, "MetricName", "BLEU"]]}
{"text": "Our final submission for WMT20 achieves 35.0 BLEU points for German French translation ( ranked in the second place ) .", "entities": [[7, 8, "MetricName", "BLEU"]]}
{"text": "French German For Fr De , we see similar improvements with iterative BT by about 2.3 BLEU .", "entities": [[16, 17, "MetricName", "BLEU"]]}
{"text": "KD , ensembling , and fine - tuning add an additional 1.4 BLEU , with reranking contributing 0.9 BLEU .", "entities": [[12, 13, "MetricName", "BLEU"], [18, 19, "MetricName", "BLEU"]]}
{"text": "Our final submission for WMT20 achieves 36.6 BLEU points for French German translation ( ranked in the fourth among anonymous submissions ) .", "entities": [[7, 8, "MetricName", "BLEU"]]}
{"text": "Finally , our German French system achieved 35.0 BLEU and ranked the second among all anonymous submissions , and our French German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions .", "entities": [[8, 9, "MetricName", "BLEU"], [25, 26, "MetricName", "BLEU"]]}
{"text": "They further describe a tiered variant ( TVA ) , in which a larger learning rate is used for the embeddings of these 99 new wordpieces .", "entities": [[14, 16, "HyperparameterName", "learning rate"]]}
{"text": "To pretrain LAPT and VA models , we use the code of Chau et al ( 2020 ) , who modify the pretraining code of Devlin et al ( 2019 ) to only use the masked language modeling ( MLM ) loss .", "entities": [[35, 38, "TaskName", "masked language modeling"], [39, 40, "DatasetName", "MLM"], [41, 42, "MetricName", "loss"]]}
{"text": "For LAPT , VA , and BERT , we train for up to 20 epochs total , selecting the highest - performing epoch based on validation masked language modeling loss .", "entities": [[6, 7, "MethodName", "BERT"], [26, 29, "TaskName", "masked language modeling"], [29, 30, "MetricName", "loss"]]}
{"text": "Training of downstream parsers and taggers follows Chau et al ( 2020 ) and Kondratyuk and Straka ( 2019 ) , with an inverse square - root learning rate decay and linear warmup , and layer - wise gradual unfreezing and discriminative finetuning .", "entities": [[27, 29, "HyperparameterName", "learning rate"], [31, 33, "MethodName", "linear warmup"]]}
{"text": "To quantify the strength of this association , we also compute the language - level Spearman correlation between the change in UNK token percentage on the unlabeled dataset 7 from the MBERT to VA vocabulary and the task - specific performance improvements from LAPT to VA .", "entities": [[15, 17, "MetricName", "Spearman correlation"], [31, 32, "MethodName", "MBERT"]]}
{"text": "Both vectors are used to compute the predicted conditional probability and calculate the loss . can not express P ( X ) = 0 exactly , but can get arbitrarily close in order to represent the probability of a phrase that is extremely unlikely .", "entities": [[13, 14, "MetricName", "loss"], [23, 24, "DatasetName", "0"]]}
{"text": "We use the Adam optimizer with a learning rate of 0.001 , and a dropout rate of 0.5 .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [7, 9, "HyperparameterName", "learning rate"]]}
{"text": "To compute log P ( x ) , we sum the elements of x and clip the sum to the range ( log ( 10 \u221210 ) , \u22120.0001 ) in order to avoid errors caused by passing log ( 0 ) values to the loss function .", "entities": [[40, 41, "DatasetName", "0"], [45, 46, "MetricName", "loss"]]}
{"text": "We train our model on the training data ( 42 million phrase pairs ) with batch size 512 for 10 epochs , and use the mean KL divergence on the conditional probabilities in the development data to select the best model .", "entities": [[15, 17, "HyperparameterName", "batch size"]]}
{"text": "We evaluate our model using 1 ) the KL divergences D KL ( P | | Q ) of the gold individual and conditional probabilities P ( x ) and P ( x | y ) against the corresponding predicted probabilities Q , and 2 ) the Pearson correlation r , which expresses the correlation between two variables ( the per - item gold and predicted probabilities ) as a value between \u22121 ( total negative correlation ) and 1 ( total positive correlation ) .", "entities": [[47, 49, "MetricName", "Pearson correlation"]]}
{"text": "On the subset of 3 , 100 test phrase pairs where at We also analyze our model 's accuracy on phrase pairs where the gold P ( x | y ) is either 0 or 1 .", "entities": [[18, 19, "MetricName", "accuracy"], [33, 34, "DatasetName", "0"]]}
{"text": "Model We first train an LSTM similar to the 100d LSTM that achieved the best accuracy of the neural models in Bowman et al ( 2015 ) .", "entities": [[5, 6, "MethodName", "LSTM"], [10, 11, "MethodName", "LSTM"], [15, 16, "MetricName", "accuracy"]]}
{"text": "We train the LSTM on the SNLI training data with batch size 512 for 10 epochs .", "entities": [[3, 4, "MethodName", "LSTM"], [6, 7, "DatasetName", "SNLI"], [10, 12, "HyperparameterName", "batch size"]]}
{"text": "We use the Adam optimizer with a learning rate of 0.001 and a dropout rate of 0.85 , and use the development data to select the best model .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [7, 9, "HyperparameterName", "learning rate"]]}
{"text": "Holding the parameters of the LSTM fixed , we train this model for 10 epochs on the SNLI training data with batch size 512 .", "entities": [[5, 6, "MethodName", "LSTM"], [17, 18, "DatasetName", "SNLI"], [21, 23, "HyperparameterName", "batch size"]]}
{"text": "Our baseline LSTM achieves the same 77.2 % accuracy reported by Bowman et al ( 2015 ) , whereas a classifier that combines the output of this LSTM with only a single feature from the output of our probability model improves to 78.2 % accuracy .", "entities": [[2, 3, "MethodName", "LSTM"], [8, 9, "MetricName", "accuracy"], [27, 28, "MethodName", "LSTM"], [44, 45, "MetricName", "accuracy"]]}
{"text": "When we add the predicted conditional probability as a single feature for each SICK sentence pair , performance increases from 81.5 % to 82.7 % accuracy .", "entities": [[13, 14, "DatasetName", "SICK"], [25, 26, "MetricName", "accuracy"]]}
{"text": "For both PPG and DRG , perplexity ( PPL ) is reported to measure the intrinsic performance with the ground truth output ( Roller et al , 2021 ) .", "entities": [[6, 7, "MetricName", "perplexity"]]}
{"text": "We adopt well - known sequence evaluation metrics weighted BLEU ( Papineni et al , 2002 ) and Fmeasure for ROUGE - L ( Lin , 2004 ) as the extrinsic evaluations .", "entities": [[9, 10, "MetricName", "BLEU"], [20, 23, "MetricName", "ROUGE - L"]]}
{"text": "This is a multi - task learning ( MTL ) comparison model ( Lee et al , 2021 ) trained to maximise the objective : \u03b1L PPG + ( 1 \u2212 \u03b1 ) L DRG , where L PPG represents the auxiliary PPG likelihood , and L DRG represents the DRG likelihood .", "entities": [[3, 7, "TaskName", "multi - task learning"], [31, 32, "HyperparameterName", "\u03b1"]]}
{"text": "For supervised phase , we set Adam ( Kingma and Ba , 2015 ) as our optimizer , with hyperparameters \u03b7 = 5e\u22124 , \u03b2 1 = 0.9 , \u03b2 2 = 0.999 , \u03f5 = 1e\u22128 .", "entities": [[6, 7, "MethodName", "Adam"], [16, 17, "HyperparameterName", "optimizer"], [24, 25, "HyperparameterName", "\u03b2"], [29, 30, "HyperparameterName", "\u03b2"]]}
{"text": "For RL phase , we set Adam as our optimizer , with \u03b7 = 5e\u22126 , \u03b2 1 = 0.9 , \u03b2 2 = 0.999 , \u03f5 = 1e\u22128 .", "entities": [[6, 7, "MethodName", "Adam"], [9, 10, "HyperparameterName", "optimizer"], [16, 17, "HyperparameterName", "\u03b2"], [21, 22, "HyperparameterName", "\u03b2"]]}
{"text": "= 5e\u22126 , \u03b2 1 = 0.9 , \u03b2 2 = 0.999 , \u03f5 = 1e\u22128 .", "entities": [[3, 4, "HyperparameterName", "\u03b2"], [8, 9, "HyperparameterName", "\u03b2"]]}
{"text": "We present the progressive change of the testing perplexity for DRG and PPG on PERSONACHAT - ORI in Figure 4 . 4", "entities": [[8, 9, "MetricName", "perplexity"]]}
{"text": "Our RL phase consumes about 15 hours to achieve the best validation loss before being early stopped .", "entities": [[12, 13, "MetricName", "loss"]]}
{"text": "In order to approximate \u03b8 via gradient descent the reparametrization trick ( Kingma and Welling , 2013 ) was introduced .", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "To prevent the model from pushing \u03c3 close to 0 and thus falling back to a deterministic autoencoder , the objective is extended by the Kullback - Leibler ( KL ) diver - gence between prior p ( z ) and q ( z | x ) : L ( \u03b8 ; x )", "entities": [[9, 10, "DatasetName", "0"], [17, 18, "MethodName", "autoencoder"], [50, 51, "HyperparameterName", "\u03b8"]]}
{"text": "The model is trained using the Adam optimizer ( Kingma and Ba , 2014 ) with an initial learning rate of 0.005 .", "entities": [[6, 7, "MethodName", "Adam"], [7, 8, "HyperparameterName", "optimizer"], [18, 20, "HyperparameterName", "learning rate"]]}
{"text": "The training stops after 20 epochs and the latent variable z has k = 50 dimensions .", "entities": [[12, 14, "HyperparameterName", "k ="]]}
{"text": "The trained VAE achieves a reconstruction loss of 45.3 and KL divergence of 13.2 on the SST2 training set .", "entities": [[2, 3, "MethodName", "VAE"], [6, 7, "MetricName", "loss"], [16, 17, "DatasetName", "SST2"]]}
{"text": "Gen uniform has a strong start but , after around 200 instances , is outperformed by the nearest neighbor approaches which yield the highest F1 - scores .", "entities": [[24, 25, "MetricName", "F1"]]}
{"text": "The module then feeds the token representations { v 1 , , v n } of the input sentence into a linear - chain CRF layer to obtain the conditional probability p \u03b8 ( y | x ) : \u03c8 ( y \u2032 , y , v i )", "entities": [[24, 25, "MethodName", "CRF"], [32, 33, "HyperparameterName", "\u03b8"]]}
{"text": "( 1 ) p \u03b8 ( y | x ) =", "entities": [[4, 5, "HyperparameterName", "\u03b8"]]}
{"text": "Given predictions { \u0177 \u03b8 1 , , \u0177 \u03b8m } from m models with different random seeds , we use majority voting to generate the final prediction\u0177 .", "entities": [[4, 5, "HyperparameterName", "\u03b8"], [17, 18, "DatasetName", "seeds"]]}
{"text": "In comparison , most of the publicly available NER datasets ( e.g. , CoNLL 2002CoNLL , 2003 are evaluated with the entity - level micro F1 scores , which emphasize common labels .", "entities": [[8, 9, "TaskName", "NER"], [24, 26, "MetricName", "micro F1"]]}
{"text": "Moreover , our system outperforms our baseline by 14.39 F1 on average , which shows the knowledge retrieval module is extremely helpful for disambiguating complex entities leading to significant improvement on model performance .", "entities": [[9, 10, "MetricName", "F1"]]}
{"text": "This observation shows that the knowledge from Wikipedia can not only improve the performance of the LOWNER domain which is the same domain as the KB , but also has very strong cross - domain Table 2 : Per - domain macro F1 score on the test set of our system and our baseline system for each language .", "entities": [[41, 43, "MetricName", "macro F1"]]}
{"text": "Assuming that the character sets 11 of query and retrieval result are A and B respectively , then the character - level IoU is A\u2229B A\u222aB .", "entities": [[22, 23, "MetricName", "IoU"]]}
{"text": "We calculate the character - level IoU of the sentence and its top - 1 retrieval result on all tracks , and plot its distribution on the training , development and test set in Figure 3 .", "entities": [[6, 7, "MetricName", "IoU"]]}
{"text": "We have the following observations : 1 ) the IoU values are concentrated around 1.0 on the training and development sets of EN , ES , NL , RU , TR , KO , FA , which indicates that most of the samples were derived from Wikipedia .", "entities": [[9, 10, "MetricName", "IoU"], [34, 35, "MethodName", "FA"]]}
{"text": "On TR , the character - level IoU values of the samples and query results cluster at around 0.5 .", "entities": [[7, 8, "MetricName", "IoU"]]}
{"text": "In Table 4 , we also list the performance of ITER G for reference , which can be seen as using the predicted mentions with 100 % accuracy .", "entities": [[27, 28, "MetricName", "accuracy"]]}
{"text": "To further analyze the observation in Table 4 , we evaluate the mention F1 score of the NER models with sentence retrieval .", "entities": [[13, 15, "MetricName", "F1 score"], [17, 18, "TaskName", "NER"]]}
{"text": "From the results in Table 5 , we suspect the low mention F1 introduces noises in the knowledge retrieval module for BN and HI , which lead to the decline of performance as shown in Table 4 .", "entities": [[12, 13, "MetricName", "F1"]]}
{"text": "Moreover , the mention F1 of mention detection models ( second row of Table 5 ) only outperform that of the NER models ( first row of Table 5 ) in a moderate scale .", "entities": [[4, 5, "MetricName", "F1"], [21, 22, "TaskName", "NER"]]}
{"text": "In masked language model pretraining , we use a learning rate of 5 \u00d7 10 \u22125 .", "entities": [[9, 11, "HyperparameterName", "learning rate"]]}
{"text": "For the NER module , we use a learning rate of 5 \u00d7 10 \u22126 for fine - tuning the XLM - R embeddings and use a learning rate of 0.05 to update the parameters in the CRF layer following Wang et al ( 2021b ) .", "entities": [[2, 3, "TaskName", "NER"], [8, 10, "HyperparameterName", "learning rate"], [20, 21, "MethodName", "XLM"], [27, 29, "HyperparameterName", "learning rate"], [37, 38, "MethodName", "CRF"]]}
{"text": "Following most of the previous efforts , we use SGD optimizer with a learning rate of 0.01 .", "entities": [[9, 10, "MethodName", "SGD"], [10, 11, "HyperparameterName", "optimizer"], [13, 15, "HyperparameterName", "learning rate"]]}
{"text": "The translations were extracted automatically from several corpora , including Europarl ( Koehn and Monz , 2005 ) , JRC - Acquis ( Steinberger et al , 2006 ) , OPUS EMEA ( Tiedemann , 2009 ) and others , using a combination of complementary alignment and extraction methods : GIZA ( Och and Ney , 2003 ) , Anymalign ( Lardilleux and Lepage , 2009 ) , spelling similarity measure SpSim ( Gomes and Lopes , 2011 ) combined with co - occurrence Dice measure , and others .", "entities": [[31, 32, "MethodName", "EMEA"], [84, 85, "MetricName", "Dice"]]}
{"text": "As such , several documents were removed from the original training data , composed by the medlinepubmed , biological and health sets , applying the training methods on the remaining documents and using the selected ones to translate and compare the translations against their originals by determining their BLEU ( Papineni et al , 2002 ) scores .", "entities": [[48, 49, "MetricName", "BLEU"]]}
{"text": "Table 4 shows the average results on both translation directions of those preliminary tests , consisting of the average BLEU scores for the conservative ( cons . )", "entities": [[19, 20, "MetricName", "BLEU"]]}
{"text": "The Europarl corpus 4 , which is significantly larger ( 54 , 543 , 044 words in English and 60 , 375 , 477 words in Portuguese ) , was tested as a source of additional term coverage , which allowed a translation quality improvement lower than 1 BLEU point .", "entities": [[48, 49, "MetricName", "BLEU"]]}
{"text": "Had we included europarl , judging by Table 4 , we would have taken nearly 200 hours , which is more than a week , expecting to simply gain 0.75 BLEU points , on average , so we had no other option than leaving it out .", "entities": [[30, 31, "MetricName", "BLEU"]]}
{"text": "i , j = exp ( \u03b1 j ) k exp ( \u03b1 k ) r c i = j \u03b2 i , j h i , j ( 3 )", "entities": [[6, 7, "HyperparameterName", "\u03b1"], [12, 13, "HyperparameterName", "\u03b1"], [20, 21, "HyperparameterName", "\u03b2"]]}
{"text": "i otherwise 0 , where B is the batch size and K is the number of types .", "entities": [[2, 3, "DatasetName", "0"], [8, 10, "HyperparameterName", "batch size"]]}
{"text": "k , m k = B b=1 \u03a6 bk ( 7 ) m k is a normalization term for class k. Transition matrix H derived from z ( ( m i , c i ) ; \u03b8 z ) should be in keeping with T .", "entities": [[3, 5, "HyperparameterName", "k ="], [36, 37, "HyperparameterName", "\u03b8"]]}
{"text": "For instance , if T ij is close to 1 , H ij needs to be bigger , which results in the growth of A ij and finally optimize \u03b8 z ( Eq.4 ) .", "entities": [[29, 30, "HyperparameterName", "\u03b8"]]}
{"text": "The extended loss function models paths of different length s between samples on the graph : L clsc = \u2212 1 S m 1 B 2 Sm s=1 B i=1 B j=1 T", "entities": [[2, 3, "MetricName", "loss"]]}
{"text": "L sup is supervision loss defined by KL divergence : L sup = \u2212 1 B c Bc i=1", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "Hence , the overall loss function is : L f inal = L sup + \u03bb clsc \u00d7 L clsc ( 14 )", "entities": [[4, 5, "MetricName", "loss"]]}
{"text": "We compare the proposed method with several state - of - the - art FET systems 3 : Attentive ( Shimaoka et al , 2016 ) uses an attention based feature extractor and does n't distinguish clean from noisy data ; AFET ( Ren et al , 2016a ) trains label embedding with partial label loss ; AAA ( Abhishek et al , 2017 ) learns joint representation of mentions and type labels ; PLE+HYENA / FIGER ( Ren et al , 2016b ) proposes heterogeneous partial - label embedding for label noise reduction to boost typing systems .", "entities": [[55, 56, "MetricName", "loss"], [76, 77, "DatasetName", "FIGER"]]}
{"text": "We compare two PLE models with HYENA ( Yogatama et al , 2015 ) and FIGER ( Ling and Weld , 2012 ) as the base typing system respectively ; NFETC ( Xu and Barbosa , 2018 ) trains neural fine - grained typing system with hierarchy - aware loss .", "entities": [[15, 16, "DatasetName", "FIGER"], [49, 50, "MetricName", "loss"]]}
{"text": "We use the NFETC model as our base model , based on which we apply Compact Latent Space Clustering Regularization as described in Section 3.2 ; Similarly , we report results produced by using both KLdivergense - based loss ( NFETC - CLSC ) and KL+hierarchical loss ( NFETC - CLSC hier ) .", "entities": [[38, 39, "MetricName", "loss"], [46, 47, "MetricName", "loss"]]}
{"text": "For evaluation metrics , we adopt strict accuracy , loose macro , and loose micro F - scores widely used in the FET task ( Ling and Weld , 2012 ) .", "entities": [[7, 8, "MetricName", "accuracy"]]}
{"text": "With the fine - tuned hyperparameter as mentioned in 4.4 , we run the model five times and report the average strict accuracy , macro F1 and micro F1 on the test set .", "entities": [[22, 23, "MetricName", "accuracy"], [24, 26, "MetricName", "macro F1"], [27, 29, "MetricName", "micro F1"]]}
{"text": "i and output dropout keep probability p o for LSTM layers ( in context encoder and LSTM mention encoder ) , the L2 regularization parameter \u03bb , the factor of hierarchical loss normalization \u03b1 ( \u03b1 > 0 means use the normalization ) ,", "entities": [[9, 10, "MethodName", "LSTM"], [16, 17, "MethodName", "LSTM"], [22, 24, "HyperparameterName", "L2 regularization"], [31, 32, "MetricName", "loss"], [33, 34, "HyperparameterName", "\u03b1"], [35, 36, "HyperparameterName", "\u03b1"], [37, 38, "DatasetName", "0"]]}
{"text": "( Xu and Barbosa , 2018 ) proposed hierarchical loss to handle over - specific noise .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "i = 1 max ( 0 , cos ( q , i pi ) \u2212 \u03b3 ) ,", "entities": [[5, 6, "DatasetName", "0"], [15, 16, "HyperparameterName", "\u03b3"]]}
{"text": "i = 0 ( 7 ) where \u03b1 and \u03b2 are the hyperparameter weights and 1 ( , ) denotes L1 loss between two input vectors .", "entities": [[2, 3, "DatasetName", "0"], [7, 8, "HyperparameterName", "\u03b1"], [9, 10, "HyperparameterName", "\u03b2"], [21, 22, "MetricName", "loss"]]}
{"text": "Eq 5 is the cross - entropy loss corresponding to relevance condition ; Eq 6 regularizes the diversity condition ; Eq 7 is the cosine - embedding loss 2 for the compactness condition and \u03b3 > 0 is the margin to encourage data samples with better question coverage .", "entities": [[7, 8, "MetricName", "loss"], [27, 28, "MetricName", "loss"], [34, 35, "HyperparameterName", "\u03b3"], [36, 37, "DatasetName", "0"]]}
{"text": "p i , p j , i = j 1 ( pi , pj ) ( 8 ) where \u03b1 and \u03b2 are hyperparameters similar to Eq 4 .", "entities": [[19, 20, "HyperparameterName", "\u03b1"], [21, 22, "HyperparameterName", "\u03b2"]]}
{"text": "The baseline uses the same BERT architecture as our approach described in Section 2.2 , but is trained with only the relevancy loss ( Eq 5 ) and therefore only consider the relevancy when selecting evidence .", "entities": [[5, 6, "MethodName", "BERT"], [22, 23, "MetricName", "loss"]]}
{"text": "On HotpotQA - 50 , all systems have low EM scores , because of the relatively low recall of the BM25 retrieval .", "entities": [[1, 2, "DatasetName", "HotpotQA"], [9, 10, "MetricName", "EM"]]}
{"text": "On MNLI - 12 , the EM score is around 50 % .", "entities": [[1, 2, "DatasetName", "MNLI"], [6, 7, "MetricName", "EM"]]}
{"text": "The latest dense retrieval methods ( Lee et al , 2019 ; Karpukhin et al , 2020 ; Guu et al , 2020 ) show promising results on efficient inference on the full set of Wikipedia articles , which allows to skip the initial standard BM25 retrieval and avoid the significant loss during the pre - processing step .", "entities": [[51, 52, "MetricName", "loss"]]}
{"text": "The mention level accuracy for our internal PH data is unusable at an accuracy of 0.506 with RoBERTa+LM .", "entities": [[3, 4, "MetricName", "accuracy"], [13, 14, "MetricName", "accuracy"]]}
{"text": "BLEU - 4 score is used as the metric .", "entities": [[0, 1, "MetricName", "BLEU"]]}
{"text": "BLEU - 4 score is used as the metric .", "entities": [[0, 1, "MetricName", "BLEU"]]}
{"text": "\u03b1 l i , where p l i is the percentage of the language l i in the entire corpus , the smoothing factor \u03b1 is set to 0.3 .", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [24, 25, "HyperparameterName", "\u03b1"]]}
{"text": "We follow MLM to sample language pairs in each batch with \u03b1 = 0.3 .", "entities": [[2, 3, "DatasetName", "MLM"], [11, 12, "HyperparameterName", "\u03b1"]]}
{"text": "( 1 ) Shuffle the input text X by adding a noise \u03b1 \u223c U ( 0 , 3 ) to the input indices and then re - ordering X based on the rank of the noised indices .", "entities": [[12, 13, "HyperparameterName", "\u03b1"], [16, 17, "DatasetName", "0"]]}
{"text": "l i L X l i { \u03b1 0", "entities": [[7, 8, "HyperparameterName", "\u03b1"], [8, 9, "DatasetName", "0"]]}
{"text": "+ \u03b1 1 | Y | \u22121 t=1 log p ( y", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "Following Yan et al ( 2020 ) , we set \u03b1 0", "entities": [[1, 4, "DatasetName", "Yan et al"], [10, 11, "HyperparameterName", "\u03b1"], [11, 12, "DatasetName", "0"]]}
{"text": "= \u03b1 1 = 1 .", "entities": [[1, 2, "HyperparameterName", "\u03b1"]]}
{"text": "In the pre - training stage , we first initialize Unicoder LC with XLM - R base , and then run continue pre - training with the accumulated 8 , 192 batch size with gradients accumulation .", "entities": [[13, 14, "MethodName", "XLM"], [31, 33, "HyperparameterName", "batch size"]]}
{"text": "We use Adam Optimizer with a linear warm - up and set the learning rate to 3e - 5 .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "Optimizer"], [13, 15, "HyperparameterName", "learning rate"]]}
{"text": "In the fine - tuning stage , the batch size is set to 32 .", "entities": [[8, 10, "HyperparameterName", "batch size"]]}
{"text": "We use Adam Optimizer ( Kingma and Ba , 2014 ) with warm - up and set the learning rate to 5e - 6 .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "Optimizer"], [18, 20, "HyperparameterName", "learning rate"]]}
{"text": "And for POS Tagging , we set the learning rate to 2e - 5 .", "entities": [[8, 10, "HyperparameterName", "learning rate"]]}
{"text": "For MLQA , we set the learning rate to 3e - 5 , batch size to 12 and train 2 epochs following BERT for SQuAD .", "entities": [[1, 2, "DatasetName", "MLQA"], [6, 8, "HyperparameterName", "learning rate"], [13, 15, "HyperparameterName", "batch size"], [22, 23, "MethodName", "BERT"], [24, 25, "DatasetName", "SQuAD"]]}
{"text": "In the pre - training stage , we first initialize encoder and decoder with XLM - R , and then run continue pre - training with 1 , 024 batch size .", "entities": [[14, 15, "MethodName", "XLM"], [29, 31, "HyperparameterName", "batch size"]]}
{"text": "We use Adam optimizer with warm - up and set the learning rate to 2e - 4 .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "optimizer"], [11, 13, "HyperparameterName", "learning rate"]]}
{"text": "In the fine - tuning stage , the batch size is 1024 .", "entities": [[8, 10, "HyperparameterName", "batch size"]]}
{"text": "We use Adam Optimizer with learning rate 1e - 5 and warm - up steps 2000 .", "entities": [[2, 3, "MethodName", "Adam"], [3, 4, "HyperparameterName", "Optimizer"], [5, 7, "HyperparameterName", "learning rate"]]}
{"text": "We set the batch size to 1 , 024 , training steps to 350 , 000 .", "entities": [[3, 5, "HyperparameterName", "batch size"]]}
{"text": "The learning rate is set to 1e - 4 .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "In the fine - tuning stage , we use Adam Optimizer and set the learning rate to 1e - 4 .", "entities": [[9, 10, "MethodName", "Adam"], [10, 11, "HyperparameterName", "Optimizer"], [14, 16, "HyperparameterName", "learning rate"]]}
{"text": "We set the batch size to 64 and the warm - up steps to 1 , 000 .", "entities": [[3, 5, "HyperparameterName", "batch size"]]}
{"text": "On the full Wikitext - 103 dataset , our implementation has a test perplexity of 24.2 ( published result for this base model was 24.0 ) .", "entities": [[13, 14, "MetricName", "perplexity"]]}
{"text": "We train our models using the standard likelihood objective for language models with a total batch size of 64 on 8 V100 GPUs .", "entities": [[15, 17, "HyperparameterName", "batch size"]]}
{"text": "Adam optimizer is used with an initial learning rate of 2.5 \u00d7 10 \u22124 , which decays up to 200k steps following a cosine curve .", "entities": [[0, 1, "MethodName", "Adam"], [1, 2, "HyperparameterName", "optimizer"], [7, 9, "HyperparameterName", "learning rate"]]}
{"text": "We evaluate model performance by ( 1 ) computing model perplexity on held - out text and ( 2 ) drawing samples from the model and comparing that to the ground truth text article .", "entities": [[10, 11, "MetricName", "perplexity"]]}
{"text": "We use BLEU score ( Papineni et al , 2002 ) to measure the similarity of our generated samples to the ground truth .", "entities": [[2, 4, "MetricName", "BLEU score"]]}
{"text": "Table 3 : The perplexity and the generated text reverse - BLEU score of different types of graph - conditioned models .", "entities": [[4, 5, "MetricName", "perplexity"], [11, 13, "MetricName", "BLEU score"]]}
{"text": "In Table 3 , we show the perplexity and the rBLEU score of the unconditional , BoW , nodes - only , and GNN conditioned models .", "entities": [[7, 8, "MetricName", "perplexity"]]}
{"text": "As a reference , a standard Transformer - XL model trained on the full Wikitext - 103 training set reaches 25.08 perplexity on our test set , which contains 71.7 % of the original test articles .", "entities": [[6, 9, "MethodName", "Transformer - XL"], [21, 22, "MetricName", "perplexity"]]}
{"text": "Embedding Dimension Size : 300 .", "entities": [[0, 2, "HyperparameterName", "Embedding Dimension"]]}
{"text": "Output candidate hypernym lists are evaluated against gold hypernym lists using the following evaluation criteria : Mean Reciprocal Rank ( MRR ) , Mean Average Precision ( MAP ) and Precision At k ( P@k ) , where k is 1 , 3 , 5 and 15 .", "entities": [[20, 21, "MetricName", "MRR"], [24, 26, "MetricName", "Average Precision"], [27, 28, "DatasetName", "MAP"], [30, 31, "MetricName", "Precision"]]}
{"text": "\u2212 k u k v 2 m \u03b4 ( c u , c v ) m is the sum of all edge weights in the graph , k u = v W ( u , v ) is the sum of the weights of the edges attached to node u , c u is the community to which u is assigned , and \u03b4 is Kronecker delta function .", "entities": [[7, 8, "HyperparameterName", "\u03b4"], [63, 64, "HyperparameterName", "\u03b4"]]}
{"text": "In section 9 , we additionally extrinsically evaluate the accuracy of static embeddings derived from a sense - induced Wikipedia dataset .", "entities": [[9, 10, "MetricName", "accuracy"]]}
{"text": "We follow previous work Komninos and Manandhar , 2016 ; Amrami and Goldberg , 2019 ) and evaluate SemEval 2010 using F - Score and V - Measure and SemEval 2013 using Fuzzy Normalized Mutual Information ( FNMI ) and Fuzzy B - Cubed ( FBC ) as well as their geometric mean ( AVG ) .", "entities": [[23, 24, "MetricName", "Score"], [29, 31, "DatasetName", "SemEval 2013"]]}
{"text": "We report F1 averaged over words in Table 3 .", "entities": [[2, 3, "MetricName", "F1"]]}
{"text": "The results in Table 4 show accuracy on this task .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "Metrics Camacho - Collados and Navigli ( 2016 ) proposed evaluating outlier detection using the accuracy ( The fraction of correctly classified outliers among the total cases ) and Outlier Position Percentage ( OPP ) metric .", "entities": [[11, 13, "TaskName", "outlier detection"], [15, 16, "MetricName", "accuracy"]]}
{"text": "While such annotation was uncommon ( 3 out of 117 senses for annotator 1 and 5 out of 149 senses for annotator 2 ) , it does affect our system 's micro F1 score for the better .", "entities": [[31, 33, "MetricName", "micro F1"]]}
{"text": "= P q \u03b8 ; teacher ( p , D ) , ( 2 ) where \u03c6\u03b8 denotes the relevance score estimated by a pretrained model parameterized by\u03b8 and \u03c4 , the temperature hyperparameter used in the KD framework .", "entities": [[3, 4, "HyperparameterName", "\u03b8"]]}
{"text": "3 Our Approach arg min \u03b8 q Q B p D B L \u03c6 \u03b8 , \u03c6\u03b8 , ( 3 ) where L \u03c6 \u03b8 , \u03c6\u03b8 is : P q \u03b8 ;", "entities": [[5, 6, "HyperparameterName", "\u03b8"], [14, 15, "HyperparameterName", "\u03b8"], [24, 25, "HyperparameterName", "\u03b8"], [31, 32, "HyperparameterName", "\u03b8"]]}
{"text": "( NDCG@10 ) for MARCO Dev ( TREC - DL ' 19 ) and Recall@1 K , denoted as R@1K. To compare with current state - of - the - art models , we evaluate our design , TCT - ColBERT , under two approaches for negative sampling : ( 1 ) BM25 and ( 2 ) hard negatives retrieved by the bi - encoder itself .", "entities": [[1, 2, "MetricName", "NDCG@10"], [7, 8, "DatasetName", "TREC"], [14, 15, "MetricName", "Recall@1"]]}
{"text": "For a fair comparison with our models based on KL - divergence KD , we also implement our KD - T2 using the precomputed pairwise softmax probabilities provided by Hofst\u00e4tter et al ( 2020 ) ( who use MSE margin loss for KD ) .", "entities": [[25, 26, "MethodName", "softmax"], [38, 39, "MetricName", "MSE"], [40, 41, "MetricName", "loss"]]}
{"text": "All our models are fine - tuned with batch size 96 and learning rate 7 \u00d7 10 \u22126 for 500 K steps on a single TPU - V2 .", "entities": [[8, 10, "HyperparameterName", "batch size"], [12, 14, "HyperparameterName", "learning rate"]]}
{"text": "For TCT - ColBERT , there are two steps in our training procedure : ( 1 ) finetune \u03c6\u03b8 ; MaxSim as our teacher model , ( 2 ) freeze \u03c6\u03b8 ; MaxSim and distill knowledge into our student model \u03c6 \u03b8 .", "entities": [[41, 42, "HyperparameterName", "\u03b8"]]}
{"text": "It further demonstrates that training bi - encoders with many in - batch negatives ( batch size up to 4096 ) significantly improves ranking effectiveness ; however , this approach is computationally expensive ( the authors report using 8\u00d7V100 GPUs for training ) .", "entities": [[15, 17, "HyperparameterName", "batch size"]]}
{"text": "For a more fair comparison , we also report the ranking effectiveness of their model trained with a smaller batch size of 128 .", "entities": [[19, 21, "HyperparameterName", "batch size"]]}
{"text": "It is also worth noting that our TCT - ColBERT ( w/ TCT HN+ ) with batch size 96 yields competitive ranking effectiveness compared to RocketQA ( the current state of the art ) , which uses batch size 4096 .", "entities": [[16, 18, "HyperparameterName", "batch size"], [37, 39, "HyperparameterName", "batch size"]]}
{"text": "Similar to the passage condition , we evaluate model effectiveness on two test sets of queries : Per official guidelines , we report different metrics for the two query sets : MRR@100 for MARCO Dev and NDCG@10 for TREC - DL ' 19 .", "entities": [[36, 37, "MetricName", "NDCG@10"], [38, 39, "DatasetName", "TREC"]]}
{"text": "MARCO Dev TREC - DL ' 19 MRR@100 NDCG@10 ANCE .368 .614 LTRe ( Zhan et al , 2020 ) - .634 SEED - Encoder", "entities": [[2, 3, "DatasetName", "TREC"], [8, 9, "MetricName", "NDCG@10"], [22, 23, "DatasetName", "SEED"]]}
{"text": "For the sparse and dense retrieval combinations , we tune the hyperparameter \u03b1 on 6000 randomly sampled queries from the MS MARCO training set .", "entities": [[12, 13, "HyperparameterName", "\u03b1"], [20, 22, "DatasetName", "MS MARCO"]]}
{"text": "However , large swaths of morphology still are , with a couple of examples from English given below : ( 3 ) a. una - do xx b. break xxxxx - able - b ( 4 ) a. punch xxxxx - ed - c b. put xxx - \u03b5 - c Any kind of affix that only consists of one part and whose distribution is determined within a locally bounded domain is part of strictly local morphotactics .", "entities": [[48, 49, "HyperparameterName", "\u03b5"]]}
{"text": "It obtains new state - of - the - art results on eleven natural language processing tasks , including pushing the GLUE score to 80.5 % ( 7.7 % point absolute improvement ) , MultiNLI accuracy to 86.7 % ( 4.6 % absolute improvement ) , SQuAD v1.1 question answering Test F1 to 93.2 ( 1.5 point absolute improvement ) and SQuAD v2.0 Test F1 to 83.1 ( 5.1 point absolute improvement ) .", "entities": [[21, 22, "DatasetName", "GLUE"], [34, 35, "DatasetName", "MultiNLI"], [35, 36, "MetricName", "accuracy"], [46, 47, "DatasetName", "SQuAD"], [48, 50, "TaskName", "question answering"], [51, 52, "MetricName", "F1"], [61, 62, "DatasetName", "SQuAD"], [64, 65, "MetricName", "F1"]]}
{"text": "For example , in OpenAI GPT , the authors use a left - toright architecture , where every token can only attend to previous tokens in the self - attention layers of the Transformer ( Vaswani et al , 2017 ) .", "entities": [[5, 6, "MethodName", "GPT"], [29, 31, "HyperparameterName", "attention layers"], [33, 34, "MethodName", "Transformer"]]}
{"text": "2 In this work , we denote the number of layers ( i.e. , Transformer blocks ) as L , the hidden size as H , and the number of self - attention heads as A. 3 We primarily report results on two model sizes : BERT BASE ( L=12 , H=768 , A=12 , Total Param - eters=110 M ) and BERT LARGE ( L=24 , H=1024 , A=16 , Total Parameters=340 M ) .", "entities": [[8, 11, "HyperparameterName", "number of layers"], [14, 15, "MethodName", "Transformer"], [46, 47, "MethodName", "BERT"], [47, 48, "MethodName", "BASE"], [62, 63, "MethodName", "BERT"]]}
{"text": "We use a batch size of 32 and fine - tune for 3 epochs over the data for all GLUE tasks .", "entities": [[3, 5, "HyperparameterName", "batch size"], [19, 20, "DatasetName", "GLUE"]]}
{"text": "For each task , we selected the best fine - tuning learning rate ( among 5e - 5 , 4e - 5 , 3e - 5 , and 2e - 5 ) on the Dev set .", "entities": [[11, 13, "HyperparameterName", "learning rate"]]}
{"text": "Both BERT BASE and BERT LARGE outperform all systems on all tasks by a substantial margin , obtaining 4.5 % and 7.0 % respective average accuracy improvement over the prior state of the art .", "entities": [[1, 2, "MethodName", "BERT"], [2, 3, "MethodName", "BASE"], [4, 5, "MethodName", "BERT"], [24, 26, "MetricName", "average accuracy"]]}
{"text": "For the largest and most widely reported GLUE task , MNLI , BERT obtains a 4.6 % absolute accuracy improvement .", "entities": [[7, 8, "DatasetName", "GLUE"], [10, 11, "DatasetName", "MNLI"], [12, 13, "MethodName", "BERT"], [18, 19, "MetricName", "accuracy"]]}
{"text": "We fine - tune for 3 epochs with a learning rate of 5e - 5 and a batch size of 32 .", "entities": [[9, 11, "HyperparameterName", "learning rate"], [17, 19, "HyperparameterName", "batch size"]]}
{"text": "Our best performing system outperforms the top leaderboard system by +1.5 F1 in ensembling and +1.3 F1 as a single system .", "entities": [[11, 12, "MetricName", "F1"], [16, 17, "MetricName", "F1"]]}
{"text": "Without TriviaQA fine - tuning data , we only lose 0.1 - 0.4 F1 , still outperforming all existing systems by a wide margin .", "entities": [[1, 2, "DatasetName", "TriviaQA"], [13, 14, "MetricName", "F1"]]}
{"text": "We fine - tuned for 2 epochs with a learning rate of 5e - 5 and a batch size of 48 .", "entities": [[9, 11, "HyperparameterName", "learning rate"], [17, 19, "HyperparameterName", "batch size"]]}
{"text": "We observe a +5.1 F1 improvement over the previous best system .", "entities": [[4, 5, "MetricName", "F1"]]}
{"text": "We fine - tune the model for 3 epochs with a learning rate of 2e - 5 and a batch size of 16 .", "entities": [[11, 13, "HyperparameterName", "learning rate"], [19, 21, "HyperparameterName", "batch size"]]}
{"text": "In this table , we report the average Dev Set accuracy from 5 random restarts of fine - tuning .", "entities": [[10, 11, "MetricName", "accuracy"]]}
{"text": "We can see that larger models lead to a strict accuracy improvement across all four datasets , even for MRPC which only has 3 , 600 labeled training examples , and is substantially different from the pre - training tasks .", "entities": [[10, 11, "MetricName", "accuracy"], [19, 20, "DatasetName", "MRPC"]]}
{"text": "It has long been known that increasing the model size will lead to continual improvements on large - scale tasks such as machine translation and language modeling , which is demonstrated by the LM perplexity of held - out training data shown in Table 6 .", "entities": [[22, 24, "TaskName", "machine translation"], [34, 35, "MetricName", "perplexity"]]}
{"text": "al ( 2016 ) mentioned in passing that increasing hidden dimension size from 200 to 600 helped , but increasing further to 1 , 000 did not bring further improvements .", "entities": [[9, 12, "HyperparameterName", "hidden dimension size"]]}
{"text": "The best performing method concatenates the token representations from the top four hidden layers of the pre - trained Transformer , which is only 0.3 F1 behind fine - tuning the entire model .", "entities": [[19, 20, "MethodName", "Transformer"], [25, 26, "MetricName", "F1"]]}
{"text": "We train with batch size of 256 sequences ( 256 sequences * 512 tokens = 128 , 000 tokens / batch ) for 1 , 000 , 000 steps , which is approximately 40 epochs over the 3.3 billion word corpus .", "entities": [[3, 5, "HyperparameterName", "batch size"]]}
{"text": "We use Adam with learning rate of 1e - 4 , \u03b2 1 = 0.9 , \u03b2 2 = 0.999 , L2 weight decay of 0.01 , learning rate warmup over the first 10 , 000 steps , and linear decay of the learning rate .", "entities": [[2, 3, "MethodName", "Adam"], [4, 6, "HyperparameterName", "learning rate"], [11, 12, "HyperparameterName", "\u03b2"], [16, 17, "HyperparameterName", "\u03b2"], [22, 24, "MethodName", "weight decay"], [27, 29, "HyperparameterName", "learning rate"], [43, 45, "HyperparameterName", "learning rate"]]}
{"text": "The optimal hyperparameter values are task - specific , but we found the following range of possible values to work well across all tasks : Batch size : 16 , 32 We also observed that large data sets ( e.g. , 100k+ labeled training examples ) were far less sensitive to hyperparameter choice than small data sets .", "entities": [[25, 27, "HyperparameterName", "Batch size"]]}
{"text": "GPT uses a sentence separator ( GPT was trained for 1 M steps with a batch size of 32 , 000 words ; BERT was trained for 1 M steps with a batch size of 128 , 000 words .", "entities": [[0, 1, "MethodName", "GPT"], [6, 7, "MethodName", "GPT"], [15, 17, "HyperparameterName", "batch size"], [23, 24, "MethodName", "BERT"], [32, 34, "HyperparameterName", "batch size"]]}
{"text": "GPT used the same learning rate of 5e - 5 for all fine - tuning experiments ; BERT chooses a task - specific fine - tuning learning rate which performs the best on the development set .", "entities": [[0, 1, "MethodName", "GPT"], [4, 6, "HyperparameterName", "learning rate"], [17, 18, "MethodName", "BERT"], [26, 28, "HyperparameterName", "learning rate"]]}
{"text": "The GLUE webpage notes that there are issues with the construction of this dataset , 15 and every trained system that 's been submitted to GLUE has performed worse than the 65.1 baseline accuracy of predicting the majority class .", "entities": [[1, 2, "DatasetName", "GLUE"], [25, 26, "DatasetName", "GLUE"], [33, 34, "MetricName", "accuracy"]]}
{"text": "Table 3 shows the training set accuracy of each machine learning model .", "entities": [[6, 7, "MetricName", "accuracy"]]}
{"text": "Subsequently , we evaluated the ranked list with regard the metrics of precision , recall and f - score at rank 10 , i.e. P@10 , R@10 and F@10 .", "entities": [[26, 27, "MetricName", "R@10"]]}
{"text": "The R@10 corresponds to the rate of positives candidate documents in the top 10 over the total of all positive instances , i.e. R@10 = T P @10 N um . P ositive .", "entities": [[1, 2, "MetricName", "R@10"], [23, 24, "MetricName", "R@10"]]}
{"text": "Finally , the F@10 is the harmonic average of the P@10 and R@10 above , i.e. F @10 = 2 * P @10 * R@10 P @10+R@10 .", "entities": [[12, 13, "MetricName", "R@10"], [24, 25, "MetricName", "R@10"]]}
{"text": "For datasets which contain more than 10 positive examples , we considered the number of positive instances to be equal to 10 in the equation of R@10 .", "entities": [[26, 27, "MetricName", "R@10"]]}
{"text": "MC dropout has been shown to improve predictive accuracy and perform on par or even better compared to deep ensembles for MT evaluation tasks ( Glushkova et al , 2021 ) .", "entities": [[8, 9, "MetricName", "accuracy"], [18, 20, "MethodName", "deep ensembles"]]}
{"text": "Indeed , when fine - tuning on the Task2 data , the model using the M1 M encoder ( M1 M - ADAPT in the table 3 ) provides a performance boost for the Pearson correlation in most language pairs , and competitive performance for the rest .", "entities": [[34, 36, "MetricName", "Pearson correlation"]]}
{"text": "We first pre - train a discourse parsing model ( Shi and Huang , 2019 ) on a humanannotated multiparty dialogue corpus ( Asher et al , 2016 ) , with 0.775 F1 score on link predictions and 0.557 F1 score on relation classifications , which are comparable to the state - of - the - art results ( Shi and Huang , 2019 ) .", "entities": [[6, 8, "TaskName", "discourse parsing"], [32, 34, "MetricName", "F1 score"], [39, 41, "MetricName", "F1 score"]]}
{"text": "U + \u03b1x S ( 6 ) where \u03b1 is one trainable parameter instead of a fixed value 1 , which modulates updates from cross attentions over graphs .", "entities": [[8, 9, "HyperparameterName", "\u03b1"]]}
{"text": "For parameters in the original BART encoder / decoder , we followed the default settings and set the learning rate 3e - 5 with 120 warm - up steps .", "entities": [[5, 6, "MethodName", "BART"], [18, 20, "HyperparameterName", "learning rate"]]}
{"text": "For graph encoders , we set the number of hidden dimensions as 768 , the number of attention heads as 2 , the number of layers as 2 , and the dropout rate as 0.2 .", "entities": [[23, 26, "HyperparameterName", "number of layers"]]}
{"text": "The weights \u03b1 in ReZero residual connections were initialized with 1 .", "entities": [[2, 3, "HyperparameterName", "\u03b1"], [4, 5, "MethodName", "ReZero"]]}
{"text": "The learning rate for parameters in newly added modules was 3e - 4 with 60 warm - up steps .", "entities": [[1, 3, "HyperparameterName", "learning rate"]]}
{"text": "ROUGE scores ( Lin and Och , 2004 ) 3 , and reported ROUGE - 1 , ROUGE - 2 , and ROUGE - L in Table 2 .", "entities": [[22, 25, "MetricName", "ROUGE - L"]]}
{"text": "Combining two different structured graphs produced better ROUGE scores compared to previous state - of - the - art methods and our base models , with an increase of 2.0 % on ROUGE - 1 , 4.3 % on ROUGE - 2 , and 1.2 % on ROUGE - L compared to our base model , BART .", "entities": [[47, 50, "MetricName", "ROUGE - L"], [56, 57, "MethodName", "BART"]]}
{"text": "Consistent with in - domain scenarios , our S - BART w. Discourse&Action achieved better results , with an increase of 66.2 % on ROUGE - 1 , 373.4 % on ROUGE - 2 , and 82.2 % on ROUGE - L over BART .", "entities": [[10, 11, "MethodName", "BART"], [39, 42, "MetricName", "ROUGE - L"], [43, 44, "MethodName", "BART"]]}
{"text": "Visualizing ReZero Weights We further tested our structure - aware BART with two ReZero settings : ( i ) initializing \u03b1 from 0 , ( ii ) initializing \u03b1 from 1 , and found initializing \u03b1 from 1 would bring in more performance gains ( see Appendix ) .", "entities": [[1, 2, "MethodName", "ReZero"], [10, 11, "MethodName", "BART"], [13, 14, "MethodName", "ReZero"], [20, 21, "HyperparameterName", "\u03b1"], [22, 23, "DatasetName", "0"], [28, 29, "HyperparameterName", "\u03b1"], [35, 36, "HyperparameterName", "\u03b1"]]}
{"text": "We then visualized the average \u03b1 over different decoder layers after training in Figure 3 , and observed that ( i ) when \u03b1 was initialized with 1 , the final \u03b1 was much larger than the setting where \u03b1 was initialized with 0 , which might because randomly initialized modules barely received supervisions at early stages and therefore contributes less to BART .", "entities": [[5, 6, "HyperparameterName", "\u03b1"], [23, 24, "HyperparameterName", "\u03b1"], [31, 32, "HyperparameterName", "\u03b1"], [39, 40, "HyperparameterName", "\u03b1"], [43, 44, "DatasetName", "0"], [62, 63, "MethodName", "BART"]]}
{"text": "2 < 10.0 , ROUGE - L < 10.0 ) .", "entities": [[4, 7, "MetricName", "ROUGE - L"]]}
{"text": "We tested our structure - aware BART ( S - BART w. Discourse / Action ) within two ReZero settings : ( i ) initializing \u03b1 from 0 , ( ii ) initializing \u03b1 from 1 .", "entities": [[6, 7, "MethodName", "BART"], [10, 11, "MethodName", "BART"], [18, 19, "MethodName", "ReZero"], [25, 26, "HyperparameterName", "\u03b1"], [27, 28, "DatasetName", "0"], [33, 34, "HyperparameterName", "\u03b1"]]}
{"text": "( U , K , \u03b3 ( ) ) to obtain K most informative unlabeled samples X = { x 1 , . . .", "entities": [[5, 6, "HyperparameterName", "\u03b3"]]}
{"text": "Y = { y 1 , , y K } , where \u03b3 ( ) is the query policy .", "entities": [[12, 13, "HyperparameterName", "\u03b3"]]}
{"text": "For a CRF model ( Lafferty et al , 2001 ) , we calculate \u03b3 with the predicted sequential label y * as \u03b3 LC ( x )", "entities": [[2, 3, "MethodName", "CRF"], [14, 15, "HyperparameterName", "\u03b3"], [23, 24, "HyperparameterName", "\u03b3"]]}
{"text": "x ; \u03b8 ) , ( 1 ) where y * is the Viterbi parse .", "entities": [[2, 3, "HyperparameterName", "\u03b8"]]}
{"text": "For BERT ( Devlin et al , 2019 ) with a token classification head , we adopt a variant of the least confidence measure : \u03b3 LC ' ( x ) = T t=1 ( 1 \u2212 max yt P ( y t | x ; \u03b8 ) ) , ( 2 ) where P ( y t | x ; \u03b8 )", "entities": [[1, 2, "MethodName", "BERT"], [11, 13, "TaskName", "token classification"], [25, 26, "HyperparameterName", "\u03b3"], [46, 47, "HyperparameterName", "\u03b8"], [61, 62, "HyperparameterName", "\u03b8"]]}
{"text": "m=1 P m ( y t | x , \u03b8 ) log P m ( y t | x , \u03b8 ) , ( 3 )", "entities": [[9, 10, "HyperparameterName", "\u03b8"], [20, 21, "HyperparameterName", "\u03b8"]]}
{"text": "In each iteration , we actively select instances from U with a query policy \u03b3 ( ) ( Section 2.2 ) to obtain the top K samples X = \u03c8", "entities": [[14, 15, "HyperparameterName", "\u03b3"]]}
{"text": "y j , ( 6 ) where \u03bb \u223c Beta ( \u03b1 , \u03b1 ) is the mixing coefficient .", "entities": [[11, 12, "HyperparameterName", "\u03b1"], [13, 14, "HyperparameterName", "\u03b1"]]}
{"text": "end x sub = { w 1 , , w T } y sub = { y 1 , , y T } // replace the original sequences for k in { i , j } d\u00f5 x k = x k \u2212 x ksub", "entities": [[38, 40, "HyperparameterName", "k ="]]}
{"text": "For the screening , we utilize a language model GPT - 2 ( Radford et al , 2019 ) to score sequence x by computing its perplexity : Perplexity ( x )", "entities": [[9, 10, "MethodName", "GPT"], [26, 27, "MetricName", "perplexity"], [28, 29, "MetricName", "Perplexity"]]}
{"text": "Based on the perplexity and a score range [ s 1 , s 2 ] , the discriminator can give judgment for sequence x : d ( x )", "entities": [[3, 4, "MetricName", "perplexity"]]}
{"text": "= 1 { s 1 \u2264 Perplexity ( x ) \u2264 s 2 } .", "entities": [[6, 7, "MetricName", "Perplexity"]]}
{"text": "( 10 ) The lower the perplexity score , the more natural the sequence .", "entities": [[6, 7, "MetricName", "perplexity"]]}
{"text": "The learning rate of the underlying model is 5e - 5 , and the batch size is 32 .", "entities": [[1, 3, "HyperparameterName", "learning rate"], [14, 16, "HyperparameterName", "batch size"]]}
{"text": "For the parameters of SeqMix , we set \u03b1 = 8 to sample \u03bb from Beta ( \u03b1 , \u03b1 ) .", "entities": [[8, 9, "HyperparameterName", "\u03b1"], [17, 18, "HyperparameterName", "\u03b1"], [19, 20, "HyperparameterName", "\u03b1"]]}
{"text": "The results show that sub - sequence mixup and label - constrained sub - sequence mixup can provide a statistical significance ( the confidence level \u03b1 = 0.05 and the number of data points N = 6 ) for all the comparisons with active learning baselines on used datasets .", "entities": [[7, 8, "MethodName", "mixup"], [15, 16, "MethodName", "mixup"], [25, 26, "HyperparameterName", "\u03b1"], [43, 45, "TaskName", "active learning"]]}
{"text": "The whole - sequence mixup passes the statistical significance test with \u03b1 = 0.1 and N = 6 on CoNLL - 03 and WebPage , but fails on ACE05 .", "entities": [[4, 5, "MethodName", "mixup"], [11, 12, "HyperparameterName", "\u03b1"]]}
{"text": "The comparison between 3 different score thresholds demonstrates the lower the perplexity , the better the generation quality .", "entities": [[11, 12, "MetricName", "perplexity"]]}
{"text": "Actually , even with a moderate augment rate \u03b1 = 0.2 , the combination ( s = 6 , n = 5 ) has been unable to provide enough generation .", "entities": [[8, 9, "HyperparameterName", "\u03b1"]]}
{"text": "We show the performance with different \u03b1 in Figure 4 ( b ) .", "entities": [[6, 7, "HyperparameterName", "\u03b1"]]}
{"text": "Among the values { 0.5 , 1 , 2 , 4 , 8 , 16 } , we observed \u03b1 = 8 presents the best performance .", "entities": [[19, 20, "HyperparameterName", "\u03b1"]]}
{"text": "From the perspective of Beta distribution , larger \u03b1 will make the sampled \u03bb more concentrated around 0.5 , which assigns more balance weights to the parent samples to be mixed .", "entities": [[8, 9, "HyperparameterName", "\u03b1"]]}
{"text": "Among the generated tokens , \" Ohio \" inherits the label B - ORG from \" COLORADO \" and the label B - LOC from \" Slovenia \" , and the distribution Beta ( \u03b1 , \u03b1 ) assigns the two labels with weights \u03bb = 0.39 and ( 1 \u2212 \u03bb )", "entities": [[34, 35, "HyperparameterName", "\u03b1"], [36, 37, "HyperparameterName", "\u03b1"]]}
{"text": "The generated sequence i is not reasonable enough intuitively , and its perplexity score 877 exceeds the threshold , so it is not added into the training set .", "entities": [[12, 13, "MetricName", "perplexity"]]}
{"text": "This generation behaves closely to a normal sequence and earns 332 perplexity score , which permits its incorporation into the training set .", "entities": [[11, 12, "MetricName", "perplexity"]]}
{"text": "The key parameters setting in our framework are stated here : ( 1 ) The number of active learning round is 5 for all the three datasets , but the size of seed set and the number of samples in each round differs from the dataset .", "entities": [[17, 19, "TaskName", "active learning"], [36, 39, "HyperparameterName", "number of samples"]]}
{"text": "( 3 ) We set \u03b1 = 8 for the Beta distribution .", "entities": [[5, 6, "HyperparameterName", "\u03b1"]]}
{"text": "( 5 ) For BERT configuration , we choose 5e - 5 for learning rate , 128 for padding length , 32 for batch size , 0.1 for dropout rate , 1e - 8 for in Adam .", "entities": [[4, 5, "MethodName", "BERT"], [13, 15, "HyperparameterName", "learning rate"], [23, 25, "HyperparameterName", "batch size"], [36, 37, "MethodName", "Adam"]]}
{"text": "For the mixing coefficient \u03bb , we follow ( Zhang et al , 2018 ) to sample it from Beta ( \u03b1 , \u03b1 ) and explore \u03b1 ranging from [ 0.5 , 16 ] .", "entities": [[21, 22, "HyperparameterName", "\u03b1"], [23, 24, "HyperparameterName", "\u03b1"], [27, 28, "HyperparameterName", "\u03b1"]]}
{"text": "The cosine similarity at a value of 0.028 was statistically significant with a Pearson correlation coefficient value 0.012 ( p - value 0.0034 )", "entities": [[13, 15, "MetricName", "Pearson correlation"]]}
{"text": "Table 3 summarizes the results in terms of accuracy and macro averaged F1 - score .", "entities": [[8, 9, "MetricName", "accuracy"], [12, 15, "MetricName", "F1 - score"]]}
{"text": "The best performing model is based on unigrams , with an accuracy of 75.3 % .", "entities": [[11, 12, "MetricName", "accuracy"]]}
{"text": "Our model achieved a macro F1 - score of 79.40 % on the SemEval dataset .", "entities": [[5, 8, "MetricName", "F1 - score"]]}
{"text": "We use cross entropy as error function for our model and the optimizer ' adam ' to update our network weights ( Kingma and Ba , 2014 ) .", "entities": [[12, 13, "HyperparameterName", "optimizer"]]}
{"text": "The batch size for the gradient update is set to 32 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "For the comparison model , the SVM performs best on the OLID dataset with an F1 - score of 70.22 % averaged over a 10 - fold cross - validation .", "entities": [[6, 7, "MethodName", "SVM"], [11, 12, "DatasetName", "OLID"], [15, 18, "MetricName", "F1 - score"]]}
{"text": "The SVM also shows the best results on the GermEval dataset with an F1 - score of 66.61 % .", "entities": [[1, 2, "MethodName", "SVM"], [13, 16, "MetricName", "F1 - score"]]}
{"text": "The evaluation on the test set results in 66.78 % F1 - score for the GermEval gold test set .", "entities": [[10, 13, "MetricName", "F1 - score"]]}
{"text": "The C - BiGRU achieved a 76.28 % F1 - score on the OLID and a 71.13 % F1 - score on the GermEval dataset on average over a 10 - fold cross - validation .", "entities": [[3, 4, "MethodName", "BiGRU"], [8, 11, "MetricName", "F1 - score"], [13, 14, "DatasetName", "OLID"], [18, 21, "MetricName", "F1 - score"]]}
{"text": "On the OLID gold test set , our model achieved an F1 - score of 79.40 % .", "entities": [[2, 3, "DatasetName", "OLID"], [11, 14, "MetricName", "F1 - score"]]}
{"text": "The difference between the F1 - scores for the English and German dataset might be attributed to the smaller size of the German training set , which contains only about 5 , 000 tweets .", "entities": [[4, 5, "MetricName", "F1"]]}
{"text": "For the OLID dataset which contains English tweets , a macro F1 - score of 79.40 % is reached , while our network resulted in an F1 - score of 72.41 % on the GermEval dataset , which consists of German tweets .", "entities": [[2, 3, "DatasetName", "OLID"], [11, 14, "MetricName", "F1 - score"], [26, 29, "MetricName", "F1 - score"]]}
{"text": "We use the implementation of LDA topic modeling provided in the MALLET toolkit ( McCallum , 2002 ) and train models with k = 5 , 10 , .. , 40 topics .", "entities": [[5, 6, "MethodName", "LDA"], [22, 24, "HyperparameterName", "k ="]]}
{"text": "As coherence scores tend to increase with increasing k , we select k as the first local maxima of coherence scores , which we found to be k = 25 .", "entities": [[27, 29, "HyperparameterName", "k ="]]}
{"text": "Topics marked with * have a statistically significant percentage of outliers ( with Bonferroni correction ; \u03b1 = 0.05 before correction ) .", "entities": [[16, 17, "HyperparameterName", "\u03b1"]]}
{"text": "This material is based in part on work supported by the Precision Health initiative at the University of Michigan , the NSF ( grant # 1815291 ) , and the John Templeton Foundation ( grant # 61156 ) .", "entities": [[11, 12, "MetricName", "Precision"]]}
{"text": "We measure inter - annotator agreement with Krippendorff 's alpha ( Krippendorff , 2004 ) and find that , over all labels , there are substantial levels of agreement within groups of annotators :", "entities": [[9, 10, "HyperparameterName", "alpha"]]}
{"text": "\u03b1 = 0.79 for trained annotators and \u03b1 = 0.71 and 72 for untrained annotators on the Yahoo and IAC threads , respectively .", "entities": [[0, 1, "HyperparameterName", "\u03b1"], [7, 8, "HyperparameterName", "\u03b1"]]}
{"text": "While some of the labels have only moderate agreement ( 0.5 < \u03b1 < 0.6 ) , we find these results satisfactory as the agreement levels are higher than those reported for similarly subjective discourse annotation tasks ( e.g. ,", "entities": [[12, 13, "HyperparameterName", "\u03b1"], [33, 35, "DatasetName", "subjective discourse"]]}
{"text": "Lexical features alone improved classification accuracy by 6 to 15 % over a balanced baseline .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "We first test the separability of the positive and negative sentences with an SVM classifier from Weka 3.8 , using as baselines only unigrams and LIWC ( Pennebaker et al , 2001 ) illustrating that the positive and negative classes can be separated with F1 above .70 , and that both unigrams and LIWC perform worse on the negative class .", "entities": [[13, 14, "MethodName", "SVM"], [44, 45, "MetricName", "F1"]]}
{"text": "The .67 F1 of FRAME is slightly lower than LIWC in Table 3 , but in our view , more interpretable .", "entities": [[2, 3, "MetricName", "F1"]]}
{"text": "Here we discuss all patterns with a \u03b8 p > .7", "entities": [[7, 8, "HyperparameterName", "\u03b8"]]}
{"text": "Top 10 Word for Each Topic NPMI dollar rate rates exchange currency market dealers central interest point 0.369 year growth rise government economic economy expected domestic inflation report 0.355 gold reserves year tons company production exploration ounces feet mine 0.290 billion year rose dlrs fell marks earlier figures surplus rise - 0.005 year tonnes crop production week grain sugar estimated expected area 0.239 dlrs company sale agreement unit acquisition assets agreed subsidiary sell - 0.043 bank billion banks money interest market funds credit debt loans 0.239 tonnes wheat export sugar tonne exports sources shipment sales week 0.218 plan bill industry farm proposed government administration told proposal change 0.212 prices production price crude output barrels barrel increase demand industry 0.339 group company investment stake firm told companies capital chairman president 0.191 trade countries foreign officials told official world government imports agreement 0.298 offer company shares share dlrs merger board stock tender shareholders 0.074 shares stock share common dividend company split shareholders record outstanding 0.277 dlrs year quarter earnings company share sales reported expects results - 0.037 market analysts time added long analyst term noted high back 0.316 coffee meeting stock producers prices export buffer quotas market price 0.170 loss dlrs profit shrs includes year gain share mths excludes - 0.427 spokesman today government strike union state yesterday workers officials told 0.201 program corn dlrs prior futures price loan contract contracts cents - 0.287 Top 10 Word for Each Topic NPMI rise increase growth fall change decline drop gains cuts rising 0.238 president chairman minister house baker administration secretary executive chief washington 0.111 make continue result include reduce open support work raise remain 0.101 january march february april december june september october july friday 0.043 year quarter week month earlier months years time period term 0.146 rose fell compared reported increased estimated revised adjusted unchanged raised 0.196 today major made announced recent full previously strong final additional 0.125 share stock shares dividend common cash stake shareholders outstanding preferred 0.281 dlrs billion tonnes marks francs barrels cents tonne barrel tons - 0.364 sales earnings business operations companies products markets assets industries operating 0.115 sale acquisition merger sell split sold owned purchase acquire held 0.003 board meeting report general commission annual bill committee association council 0.106 loss profit revs record note oper prior shrs gain includes 0.221 company corp group unit firm management subsidiary trust pacific holdings 0.058 prices price current total lower higher surplus system high average 0.198 offer agreement agreed talks tender plan terms program proposed issue 0.138 bank trade market rate exchange dollar foreign interest rates banks 0.327 told official added department analysts officials spokesman sources statement reuters 0.181 production export exports industry wheat sugar imports output crude domestic 0.262 japan government international world countries american japanese national states united 0.251", "entities": [[197, 198, "MetricName", "loss"], [205, 206, "DatasetName", "mths"], [372, 373, "MetricName", "loss"]]}
{"text": "When compared with other domain - specific pretrained language models ( e.g. BIOBERT and SCIBERT ) , SAPBERT also brings substantial improvement by up to 20 % on accuracy across all tasks .", "entities": [[7, 10, "TaskName", "pretrained language models"], [28, 29, "MetricName", "accuracy"]]}
{"text": "We adapted the Multi - Similarity loss ( MS loss , Wang et al 2019 ) , a SOTA metric learning objective on visual recognition , for learning from the positive and negative pairs : L = 1 | X b | | X b | i=1 1", "entities": [[6, 7, "MetricName", "loss"], [9, 10, "MetricName", "loss"], [19, 21, "TaskName", "metric learning"]]}
{"text": "\u03b1 log 1 + n N i", "entities": [[0, 1, "HyperparameterName", "\u03b1"]]}
{"text": "+ 1 \u03b2 log 1 + p P", "entities": [[2, 3, "HyperparameterName", "\u03b2"]]}
{"text": "i e \u2212\u03b2 ( S ip \u2212 ) , ( 2 ) where \u03b1 , \u03b2 are temperature scales ; is an offset applied on the similarity matrix ;", "entities": [[13, 14, "HyperparameterName", "\u03b1"], [15, 16, "HyperparameterName", "\u03b2"]]}
{"text": "During training , we use AdamW ( Loshchilov and Hutter , 2018 ) with a learning rate of 2e - 5 and weight decay rate of 1e - 2 .", "entities": [[5, 6, "MethodName", "AdamW"], [15, 17, "HyperparameterName", "learning rate"], [22, 25, "HyperparameterName", "weight decay rate"]]}
{"text": "Models are trained on the prepared pairwise UMLS data for 1 epoch ( approximately 50k iterations ) with a batch size of 512 ( i.e. , 256 pairs per mini - batch ) .", "entities": [[7, 8, "DatasetName", "UMLS"], [19, 21, "HyperparameterName", "batch size"]]}
{"text": "We train with Automatic Mixed Precision ( AMP ) 10 provided in PyTorch 1.7.0 .", "entities": [[5, 6, "MetricName", "Precision"], [7, 8, "MethodName", "AMP"]]}
{"text": "We use the same optimiser and learning rates but train with a batch size of 256 ( to accommodate the memory of 1 GPU ) .", "entities": [[12, 14, "HyperparameterName", "batch size"]]}
{"text": "SAPBERT obtains consistent improvement over all * BERT models across all datasets , with larger gains ( by up to 31.0 % absolute Acc @1 increase ) observed in the social media domain .", "entities": [[7, 8, "MethodName", "BERT"], [23, 24, "MetricName", "Acc"]]}
{"text": "Measured by Acc @1 , SAPBERT achieves new SOTA with statistical significance on 5 of the 6 datasets and for the dataset ( BC5CDR - c ) where SAPBERT is not significantly better , it performs on par with SOTA ( 96.5 vs. 96.6 ) .", "entities": [[2, 3, "MetricName", "Acc"], [23, 24, "DatasetName", "BC5CDR"]]}
{"text": "( Basaldella et al , 2020 ) 64.6 74.6 NCA loss ( Goldberger et al , 2005 ) 65.2 77.0 Lifted - Structure loss ( Oh Song et", "entities": [[10, 11, "MetricName", "loss"], [23, 24, "MetricName", "loss"]]}
{"text": "al , 2016 ) 62.0 72.1 InfoNCE ( Oord et al , 2018 ; He et al , 2020 ) 63.3 74.2 Circle loss ( Sun et al , 2020 ) 66.7 78.7 Multi - Similarity loss ( Wang et al , 2019 ) 67.2 80.3 Schumacher et al ( 2020 ) for clinical concept linking .", "entities": [[6, 7, "MethodName", "InfoNCE"], [23, 24, "MetricName", "loss"], [36, 37, "MetricName", "loss"]]}
{"text": "Lifted - Structure loss ( Oh Song et al , 2016 ) and NCA loss ( Goldberger et al , 2005 ) are two very classic metric learning objectives .", "entities": [[3, 4, "MetricName", "loss"], [14, 15, "MetricName", "loss"], [26, 28, "TaskName", "metric learning"]]}
{"text": "Multi - Similarity loss ( Wang et al , 2019 ) and Circle loss ( Sun et al , 2020 ) are two recently proposed metric learning objectives and have been considered as SOTA on large - scale visual recognition benchmarks .", "entities": [[3, 4, "MetricName", "loss"], [13, 14, "MetricName", "loss"], [25, 27, "TaskName", "metric learning"]]}
{"text": "In Tab . 7 we list number of parameters trained in the three ADAPTER variants along with full - modeltuning for easy comparison . BIOBERT ( Lee et al , 2020 ) https://huggingface.co/dmis - lab / biobert - v1.1 BLUEBERT ( Peng et al , 2019 ) https://huggingface.co/bionlp/bluebert_pubmed_mimic_uncased_L - 12_H - 768_A - 12 CLINICALBERT ( Alsentzer et al , 2019 )", "entities": [[6, 9, "HyperparameterName", "number of parameters"]]}
{"text": "In 2001 , the Spearman correlation between the available human rankings of systems at the 50word and 400 - word lengths is 0.61 .", "entities": [[4, 6, "MetricName", "Spearman correlation"]]}
{"text": "As a baseline , we consider the ROUGE Recall scores obtained by the standard reference summary configuration ( Standard , first row in Table 2 ) , that is , when system summaries of each length ( table columns ) are evaluated against reference summaries of the same length .", "entities": [[8, 9, "MetricName", "Recall"]]}
{"text": "For each reference summary configuration , we compute ROUGE Recall system scores 1 for the three common ROUGE variants R - 1 , R - 2 and R - L , which compare unigrams , bigrams and the longest common subsequence , respectively .", "entities": [[9, 10, "MetricName", "Recall"]]}
{"text": "We then calculate their Pearson correlation 2 with the available human mean content coverage scores for the systems .", "entities": [[4, 6, "MetricName", "Pearson correlation"]]}
{"text": "The classifiers are trained with online learning ( Sahoo et al , 2018 ) to keep improving the accuracy and lower down the frequency of post - correction in consequence .", "entities": [[5, 7, "TaskName", "online learning"], [18, 19, "MetricName", "accuracy"]]}
{"text": "Automatic Evaluation In Table 5 , we report the perplexity , BLEU scores and distinct uni / bigrams for three model sizes .", "entities": [[9, 10, "MetricName", "perplexity"], [11, 12, "MetricName", "BLEU"]]}
{"text": "Table 6 measures the accuracy of predicting dialogue act ( DA ) , aspect and movie tracker of our model .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "In Table 2 , wEns appears to provide the best F1 , F2 , and TPR scores over the test set of subtask 1 , while our LR outperforms the AUC of the baseline method .", "entities": [[10, 11, "MetricName", "F1"], [30, 31, "MetricName", "AUC"]]}
{"text": "Recently , adapters have been proposed as an alternative approach to decrease the substantial number of parameters of PLMs in the fine - tuning stage ( Houlsby et al , 2019 ) .", "entities": [[14, 17, "HyperparameterName", "number of parameters"]]}
{"text": "Appendix A. Note that for the pretraining step , we use a batch size of 8 and accumulate the gradient for every 32 steps to be consistent with the hyperparameter setting in Gururangan et al , 2020 .", "entities": [[12, 14, "HyperparameterName", "batch size"]]}
{"text": "We sweep the learning rates in { 2e - 5 , 1e - 4 , 3e - 4 , 6e - 4 } and the number of epochs in { 10 , 20 } on the validation set and report the test score that performs the best on the validation set .", "entities": [[25, 28, "HyperparameterName", "number of epochs"]]}
{"text": "Furthermore , directly fine - tuned adapter model only trains 1.42 % of the entire parameters which leads to the 30 % faster - training step than the full model and skips the pretraining stage that typically expensive to train than the fine - tuning , substantially reducing Figure 2 : F 1 score as a function of learning rate on test set with log scale on x - axis .", "entities": [[58, 60, "HyperparameterName", "learning rate"]]}
{"text": "For RoBERTa and TAPT , we follow the hyper - parameter settings in Gururangan et al , 2020 except for the learning rate .", "entities": [[1, 2, "MethodName", "RoBERTa"], [21, 23, "HyperparameterName", "learning rate"]]}
{"text": "Since we sweep the learning rates and the number of epochs in the range that includes larger figures compared to those in the full model when fine - tuning adapters and kept the other hyper - parameters the same as in Gururangan et al , 2020 , we the larger learning rate zeroes out the benefits of pretraining .", "entities": [[8, 11, "HyperparameterName", "number of epochs"], [50, 52, "HyperparameterName", "learning rate"]]}
{"text": "Figure 2 . shows the average F 1 score across all tasks as a function of learning rate .", "entities": [[16, 18, "HyperparameterName", "learning rate"]]}
{"text": "We sweep the learning rates in { 1e - 5 , 2e - 5 , 3e - 5 } and the number of epochs in { 10 , 20 } on the validation set and report the test score that performs the best on the validation set .", "entities": [[21, 24, "HyperparameterName", "number of epochs"]]}
{"text": "Figure 3 : F 1 score as a function of learning rate on development setwith log scale on x - axis .", "entities": [[10, 12, "HyperparameterName", "learning rate"]]}
{"text": "Here we sweep the learning rates in { 1e - 4 , 3e - 4 , 6e - 4 } , the number of epochs in { 10 , 20 } , and the patience factor in { 3 , 5 } .", "entities": [[22, 25, "HyperparameterName", "number of epochs"]]}
{"text": "The system was implemented in CRFSuite ( Okazaki , 2007 ) , using the passive aggressive optimizer and 10 epochs , a setting which proved to yield best results in previous experiments ( Ljube\u0161i\u0107 and Erjavec , 2016 ) .", "entities": [[16, 17, "HyperparameterName", "optimizer"]]}
{"text": "3 setup token accuracy with stdev word2vec cbow 0.8407 \u00b1 0.0025 word2vec", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "Adding the character - level representation has shown the biggest impact among all the experiments , with \u223c 2 accuracy points increase , and a minor difference between encoding the character sequence with a single - direction or a bi - directional LSTM .", "entities": [[19, 20, "MetricName", "accuracy"], [42, 43, "MethodName", "LSTM"]]}
{"text": "The largest difference that can be observed between the four systems are 2 accuracy points on Slovene between the basic CRF implementation ( JANES ) and the simplified BiLSTM implementation ( JSIsimpler ) .", "entities": [[13, 14, "MetricName", "accuracy"], [20, 21, "MethodName", "CRF"], [28, 29, "MethodName", "BiLSTM"]]}
{"text": "Following Garg et al ( 2019 ) , given an alignment matrix AM M , N and an attention matrix computed by a cross attention head AH M , N , for each target word i , we use the following cross - entropy loss L a to minimize the Kullback - Leibler divergence between AH and AM : L a ( AH , AM )", "entities": [[44, 45, "MetricName", "loss"]]}
{"text": "The overall loss L is : L = L t + \u03b3L a ( AH , AM ) ( 3 ) where L t is the standard NLL translation loss , and \u03b3 is a hyperparameter .", "entities": [[2, 3, "MetricName", "loss"], [27, 28, "MetricName", "NLL"], [29, 30, "MetricName", "loss"], [32, 33, "HyperparameterName", "\u03b3"]]}
{"text": "We use \u03b3 = 0.05 , supervising only one cross attention head at the third last layer .", "entities": [[2, 3, "HyperparameterName", "\u03b3"]]}
{"text": "2 Given the sparse nature of the alignments , we replace the softmax operator in the cross attention head with the \u03b1 - entmax function ( Peters et al , 2019 ; Correia et al , 2019 ) .", "entities": [[12, 13, "MethodName", "softmax"], [21, 22, "HyperparameterName", "\u03b1"]]}
{"text": "Entmax allows sparse attention weights for any \u03b1 > 1 .", "entities": [[7, 8, "HyperparameterName", "\u03b1"]]}
{"text": "\" EN - > X ( 16 ) \" and \" X - > EN ( 16 ) \" : average BLEU scores for English to Non - English languages and for Non - English languages to English on 16 language pairs respectively .", "entities": [[21, 22, "MetricName", "BLEU"]]}
{"text": "\" BLEU zero ( 4 ) \" and \" ACC zero ( 4 ) \" : average BLEU scores and target language identification accuracy over 4 zero - shot language directions .", "entities": [[1, 2, "MetricName", "BLEU"], [9, 10, "MetricName", "ACC"], [17, 18, "MetricName", "BLEU"], [21, 23, "TaskName", "language identification"], [23, 24, "MetricName", "accuracy"]]}
{"text": "We report average BLEU and accuracy scores , plus the standard deviation over 3 training runs with different random seeds .", "entities": [[3, 4, "MetricName", "BLEU"], [5, 6, "MetricName", "accuracy"], [19, 20, "DatasetName", "seeds"]]}
{"text": "( Papineni et al , 2002 ) to be comparable with Aharoni et al ( 2019 ) for the TED Talks benchmark , and SACREBLEU 5 ( Post , 2018 ) for WMT - 2018 and OPUS - 100 .", "entities": [[24, 25, "MetricName", "SACREBLEU"]]}
{"text": "6 As an additional evaluation , we report the target language identification accuracy score for the zeroshot cases , called ACC zero .", "entities": [[10, 12, "TaskName", "language identification"], [12, 13, "MetricName", "accuracy"], [20, 21, "MetricName", "ACC"]]}
{"text": "5 Signature : BLEU+case.mixed+numrefs.1+smooth.exp+ tok . { 13a , ja - mecab - 0.996 - IPA , zh } + version.1.5.0 6 We report average BLEU over all test sets .", "entities": [[25, 26, "MetricName", "BLEU"]]}
{"text": "In contrast , zero - shot results vary across the board , with 5 attaining the best performance , with almost 2 BLEU points better than its baseline 2 .", "entities": [[22, 23, "MetricName", "BLEU"]]}
{"text": "Moreover , 5 considerably improves target language identification accuracy ( ACC zero ) , with more stable results , i.e. lower standard deviation , than counterparts .", "entities": [[6, 8, "TaskName", "language identification"], [8, 9, "MetricName", "accuracy"], [10, 11, "MetricName", "ACC"]]}
{"text": "Enriching the model with alignment supervision ( c ) results in the best system overall , with an improvement of more than 3 BLEU points in the zero - shot ( 2020 ) .", "entities": [[23, 24, "MetricName", "BLEU"]]}
{"text": "Average BLEU , target language identification accuracy and standard deviation of 3 training runs . testbed compared to baseline 2 , and with stable results across three training runs ( standard deviations of 0.12 and 0.82 ) .", "entities": [[1, 2, "MetricName", "BLEU"], [4, 6, "TaskName", "language identification"], [6, 7, "MetricName", "accuracy"]]}
{"text": "The average score over 30 zeroshot language pairs is low but the individual results range from 0.3 to 17.5 BLEU showing the potentials of multilingual models in this challenging data set as well .", "entities": [[19, 20, "MetricName", "BLEU"]]}
{"text": "Zero - shot improvements , i.e. BLEU zero and ACC zero , are large in two benchmarks out of three , i.e. Ted Talks and WMT - 2018 , and with a similar trend in OPUS - 100 .", "entities": [[6, 7, "MetricName", "BLEU"], [9, 10, "MetricName", "ACC"]]}
{"text": "We applied 0.1 as dropout for both residual layers and attention weights , using the Adam optimizer ( Kingma and Ba , 2015 ) with \u03b21 = 0.9 , and \u03b22 = 0.998 , with learning rate set at 3 and 40 K warmup steps as in Aharoni et al ( 2019 ) .", "entities": [[15, 16, "MethodName", "Adam"], [16, 17, "HyperparameterName", "optimizer"], [35, 37, "HyperparameterName", "learning rate"]]}
{"text": "Our final model , which is an ensemble of AlBERT - XL pretrained on CoQA and QuAC independently , with the chosen answer having the highest average probability score , achieves an F1 - Score of 70.9 % on the official test - set .", "entities": [[14, 15, "DatasetName", "CoQA"], [16, 17, "DatasetName", "QuAC"], [32, 35, "MetricName", "F1 - Score"]]}
{"text": "2 For both datasets , we filter out samples which do not adhere to SQuADlike extractive QA setup ( e.g. yes / no questions ) or have a context length of more than 5000 characters .", "entities": [[28, 30, "HyperparameterName", "context length"]]}
{"text": "The shared - task relies on Exact Match ( EM ) and F1 metrics to evaluate the systems on subtask 1 .", "entities": [[6, 8, "MetricName", "Exact Match"], [9, 10, "MetricName", "EM"], [12, 13, "MetricName", "F1"]]}
{"text": "+ QuAC is the best - performing single model according to the EM metric ( EM = 52.60 ) , whereas albert - xl + CoQA performs the best on F1 metric ( F 1 = 69.48 ) on the test set .", "entities": [[1, 2, "DatasetName", "QuAC"], [12, 13, "MetricName", "EM"], [15, 16, "MetricName", "EM"], [25, 26, "DatasetName", "CoQA"], [30, 31, "MetricName", "F1"]]}
{"text": "However , the best performance on the test set was achieved by ensembling over the albert - xl models pre - trained independently on CoQA and QuAC ( EM = 53.5 , F 1 = 70.9 ) .", "entities": [[24, 25, "DatasetName", "CoQA"], [26, 27, "DatasetName", "QuAC"], [28, 29, "MetricName", "EM"]]}
{"text": "Specifically , we explored factors such as answer length , relative position of the answer in the context , question length , and context length in Table 4 .", "entities": [[23, 25, "HyperparameterName", "context length"]]}
{"text": "We also use the dataset of Li and Roth ( 2002 ) to train a BERT classifier to predict answer type of a question with 97 % accuracy .", "entities": [[15, 16, "MethodName", "BERT"], [27, 28, "MetricName", "accuracy"]]}
{"text": "Overall , the size of the selected sample is only 20 % of the original dataset , yet achieves a higher EM score than the combined dataset as seen in Table 2 .", "entities": [[21, 22, "MetricName", "EM"]]}
{"text": "Our final submission ensembles two AlBERT - XL models independently pretrained on CoQA and QuAC and achieves an F1 - Score of 70.9 % and EM - Score of 53.5 % on the competition test - set .", "entities": [[12, 13, "DatasetName", "CoQA"], [14, 15, "DatasetName", "QuAC"], [18, 21, "MetricName", "F1 - Score"], [25, 26, "MetricName", "EM"], [27, 28, "MetricName", "Score"]]}
{"text": "Our best model achieves a macro - average F1 - score of 80.53 % , which we improve further by using semi - supervised learning .", "entities": [[8, 11, "MetricName", "F1 - score"]]}
{"text": "BERTweet achieves a macro - average F1 - score of 80.53 % , which we improve further by using semisupervised learning ; 3 ) The union of P - STANCE and previous benchmark datasets provides more opportunities for studying other stance detection tasks , e.g. , cross - target stance detection and crosstopic stance detection .", "entities": [[6, 9, "MetricName", "F1 - score"], [40, 42, "TaskName", "stance detection"], [49, 51, "TaskName", "stance detection"], [53, 55, "TaskName", "stance detection"]]}
{"text": "After obtaining the annotation results , we computed Krippendorff 's alpha ( Krippendorff , 2011 ) as the measure of inter - annotator agreement , as shown in Table 5 .", "entities": [[10, 11, "HyperparameterName", "alpha"]]}
{"text": "We observed that annotators had difficulties in reaching an agreement on tweets with label \" None \" and the average of Krippendorff 's alpha values increases from 0.60 to 0.81 when we consider two classes : \" Favor \" and \" Against \" .", "entities": [[23, 24, "HyperparameterName", "alpha"]]}
{"text": "Similar to Mohammad et al ( 2017 ) and , F avg and macro - average of F1 - score ( F macro ) are adopted to evaluate the performance of our baseline models .", "entities": [[17, 20, "MetricName", "F1 - score"]]}
{"text": "The maximum sequence length is set to 128 and the batch size is 32 .", "entities": [[10, 12, "HyperparameterName", "batch size"]]}
{"text": "We use AdamW optimizer ( Loshchilov and Hutter , 2019 ) and the learning rate is 2e - 5 .", "entities": [[2, 3, "MethodName", "AdamW"], [3, 4, "HyperparameterName", "optimizer"], [13, 15, "HyperparameterName", "learning rate"]]}
{"text": "2 ) We incorporate the teacher confidence in the student loss by penalizing the student 's misclassified examples in which the teacher has high confidence .", "entities": [[10, 11, "MetricName", "loss"]]}
{"text": "= \u2212 1 | D | ( X , Y ) D log ( p ( Y | X , \u0398 ) ) , ( 1 ) where p ( Y | X , \u0398 ) denotes the conditional probability of Y given X.", "entities": [[20, 21, "HyperparameterName", "\u0398"], [34, 35, "HyperparameterName", "\u0398"]]}
{"text": "Here , noise r is sampled uniformly from the interval [ 0 , 1 ] , and \u03b2 random R \u22650 is a hyper - parameter that controls the noise scale .", "entities": [[11, 12, "DatasetName", "0"], [17, 18, "HyperparameterName", "\u03b2"]]}
{"text": "If we set \u03b2 random = 0 , then BACK - TRANS ( NOISY ) is identical to standard backtranslation .", "entities": [[3, 4, "HyperparameterName", "\u03b2"], [6, 7, "DatasetName", "0"]]}
{"text": "We use \u03b2 random = 6 for BACKTRANS ( NOISY ) 12 .", "entities": [[2, 3, "HyperparameterName", "\u03b2"]]}
{"text": "In one such experiment , we observe jumps of up to 2.6 BLEU points over the primary system by pretraining on a synthetic , backtranslated corpus followed by fine - tuning on the original parallel training data .", "entities": [[12, 13, "MetricName", "BLEU"]]}
{"text": "While still a sequence - to - sequence ( Sutskever et al , 2014 ) model composed of an encoder and a decoder , Transformer models are highly parallelizable thanks to being composed purely of feedforward and self - attention layers rather than recurrent layers ( Hochreiter and Schmidhuber , 1997 ; Cho et al , 2014b ) .", "entities": [[24, 25, "MethodName", "Transformer"], [39, 41, "HyperparameterName", "attention layers"]]}
{"text": "More precisely , we chose Adam ( Kingma and Ba , 2015 ) as the optimizer and Adam betas were set to 0.9 and 0.98 , respectively .", "entities": [[5, 6, "MethodName", "Adam"], [15, 16, "HyperparameterName", "optimizer"], [17, 18, "MethodName", "Adam"]]}
{"text": "Learning rate was set to 0.0005 , with an inverse squared root decay schedule and 4000 steps of warmup updates .", "entities": [[0, 2, "HyperparameterName", "Learning rate"]]}
{"text": "We trained all our models for a fixed number of epochs , determined separately for each system , and chose the last checkpoint for reporting BLEU ( Papineni et al , 2002 ) scores on the test sets .", "entities": [[8, 11, "HyperparameterName", "number of epochs"], [25, 26, "MetricName", "BLEU"]]}
{"text": "Table 2 shows cased BLEU scores for various systems .", "entities": [[4, 5, "MetricName", "BLEU"]]}
{"text": "Our primary systems achieved a BLEU score of 47.6 for Upper Sorbian German and 45.2 for German Upper Sorbian translation .", "entities": [[5, 7, "MetricName", "BLEU score"]]}
{"text": "We achieved an improvement of 0.3 and 0.4 BLEU points , respectively , by training further till 80 epochs in each direction .", "entities": [[8, 9, "MetricName", "BLEU"]]}
{"text": "We also evaluated a third system , synth - auth - finetune , as described in Section 4 , which provided a jump of 2.6 points in BLEU score over the primary system for Upper Sorbian German and 2.5 for German Upper Sorbian .", "entities": [[27, 29, "MetricName", "BLEU score"]]}
{"text": "The second result is notable since the regime of pretraining followed by fine - tuning improves the BLEU scores by up to 4 points on this test set when compared to training only on the original bitext .", "entities": [[17, 18, "MetricName", "BLEU"]]}
{"text": "In the SemEval 2017 , our method ranks 1st in Accuracy , 5th in AverageR.", "entities": [[10, 11, "MetricName", "Accuracy"]]}
{"text": "All parameters are learned by Adam optimizer ( Kingma and Ba , 2014 ) with the learning rate 0.001 .", "entities": [[5, 6, "MethodName", "Adam"], [6, 7, "HyperparameterName", "optimizer"], [16, 18, "HyperparameterName", "learning rate"]]}
{"text": "Compared with the median system , our method has improvements of about 5 % in both accuracy and R Average .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "Compared with the embedding baselines , our ensemble method obtains improvements of 2.7 % and 1.5 % in accuracy and R Average respectively , which demonstrates the effectiveness of the proposed method .", "entities": [[18, 19, "MetricName", "accuracy"]]}
{"text": "For English , the accuracy of SRL has reached approximately 80 % - 90 % ( Ouchi et al , 2018 ; Strubell et al , 2018 ; Tan et al , 2018 ) .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "However , there are many omissions of arguments in Japanese , and the accuracy of Japanese PAS analysis on omitted arguments is still around 50 % - 60 % ( Shibata et al , 2016 ; Shibata and Kurohashi , 2018 ;", "entities": [[13, 14, "MetricName", "accuracy"]]}
{"text": "A reason for such low accuracy is the shortage of gold datasets and knowledge about PAS analysis , which require a prohibitive cost of creation ( Iida et al , 2007 ; Kawahara et al , 2002 ) .", "entities": [[5, 6, "MetricName", "accuracy"]]}
{"text": "MC has made remarkable progress in the last couple of years , and MC models have even exceeded human accuracy in some datasets ( Devlin et al , 2019 ) .", "entities": [[19, 20, "MetricName", "accuracy"]]}
{"text": "However , MC accuracy is not necessarily high for documents that contain anaphoric phenomena and those that need external knowledge or inference ( Mihaylov et al , 2018 ; .", "entities": [[3, 4, "MetricName", "accuracy"]]}
{"text": "For example , Devlin et al ( 2019 ) proposed an MC model using a language representation model , BERT , which achieved a high - ranked accuracy on the SQuAD 1.1 leaderboard as of September 30 , 2019 .", "entities": [[19, 20, "MethodName", "BERT"], [27, 28, "MetricName", "accuracy"], [30, 31, "DatasetName", "SQuAD"]]}
{"text": "We also compare our methods with the neural network - based PAS analysis model ( Shibata and Kurohashi , 2018 ) ( hereafter , NN - PAS ) , which achieved the state - of - theart accuracy on Japanese PAS analysis .", "entities": [[37, 38, "MetricName", "accuracy"]]}
{"text": "The number of epochs for the pre - training was 30 .", "entities": [[1, 4, "HyperparameterName", "number of epochs"]]}
{"text": "Note that the proposed approach is orthogonal to model compression techniques that reduce the number of layers / parameters ( Sun et al , 2019 ; Jiao et al , 2020 ) , since we do not reduce the number of parameters from the UNITER baseline .", "entities": [[8, 10, "TaskName", "model compression"], [14, 17, "HyperparameterName", "number of layers"], [39, 42, "HyperparameterName", "number of parameters"], [44, 45, "MethodName", "UNITER"]]}
{"text": "We denote the Transformer - based ( Vaswani et al , 2017 ) image encoder and language encoder by f \u03b8 V and f \u03b8 L , respectively ( \u03b8 V , \u03b8 L are learnable parameters ) .", "entities": [[3, 4, "MethodName", "Transformer"], [20, 21, "HyperparameterName", "\u03b8"], [24, 25, "HyperparameterName", "\u03b8"], [29, 30, "HyperparameterName", "\u03b8"], [32, 33, "HyperparameterName", "\u03b8"]]}
{"text": "The image encoder f \u03b8 V encodes this sequence of image regions into a d - dimensional space f \u03b8 V ( v ) = h = { h 0 , . . . , h N } ( h j R d ) .", "entities": [[4, 5, "HyperparameterName", "\u03b8"], [19, 20, "HyperparameterName", "\u03b8"], [29, 30, "DatasetName", "0"]]}
{"text": "= \u2212 1 M M k=1 log P \u03b8 mlm ( w m k |", "entities": [[8, 9, "HyperparameterName", "\u03b8"], [9, 10, "DatasetName", "mlm"]]}
{"text": "z m k ) , ( 1 ) where \u03b8 mlm is the additional parameters introduced to map hidden states z to word probabilities .", "entities": [[9, 10, "HyperparameterName", "\u03b8"], [10, 11, "DatasetName", "mlm"]]}
{"text": "= \u2212 1 M M k=1 log P \u03b8 mlm ( w m k |", "entities": [[8, 9, "HyperparameterName", "\u03b8"], [9, 10, "DatasetName", "mlm"]]}
{"text": "k + h 0 ) , ( 2 ) where \u03b8 = { \u03b8 V , \u03b8 L } and the word probabilities P \u03b8 are conditioned on the corresponding image i via the global image representation h 0 .", "entities": [[3, 4, "DatasetName", "0"], [10, 11, "HyperparameterName", "\u03b8"], [13, 14, "HyperparameterName", "\u03b8"], [16, 17, "HyperparameterName", "\u03b8"], [24, 25, "HyperparameterName", "\u03b8"], [38, 39, "DatasetName", "0"]]}
{"text": "Although VMLM takes a similar mathematical form to the MLM task proposed in UNITER , they differ in two main aspects : 1 ) LightningDOT uses two separate encoders ( h 0 is computed by f \u03b8 V ) ; and 2 ) visual dependency is explicitly injected to text representations ( z m k + h 0 ) , instead of implicitly learned through cross - modal attention .", "entities": [[9, 10, "DatasetName", "MLM"], [13, 14, "MethodName", "UNITER"], [31, 32, "DatasetName", "0"], [36, 37, "HyperparameterName", "\u03b8"], [57, 58, "DatasetName", "0"]]}
{"text": "6 In MRFR , the L 2 distance between two feature vectors x and y is defined as : D \u03b8 fr ( x , y )", "entities": [[20, 21, "HyperparameterName", "\u03b8"]]}
{"text": "= k x k \u2212 g \u03b8 fr ( y k ) 2 2 , where 2 denotes L 2 - norm , and g \u03b8 fr ( ) is a learnable Multi - layer Perceptron ( MLP ) with parameters \u03b8 fr .", "entities": [[6, 7, "HyperparameterName", "\u03b8"], [25, 26, "HyperparameterName", "\u03b8"], [37, 38, "DatasetName", "MLP"], [41, 42, "HyperparameterName", "\u03b8"]]}
{"text": "z 0 , h 0 , ( 5 ) where , denotes the inner product between two vectors , and h 0 and z 0 are the output [ CLS ] embeddings from image encoder f \u03b8 V and language encoder f \u03b8 L , respectively .", "entities": [[1, 2, "DatasetName", "0"], [4, 5, "DatasetName", "0"], [21, 22, "DatasetName", "0"], [24, 25, "DatasetName", "0"], [36, 37, "HyperparameterName", "\u03b8"], [42, 43, "HyperparameterName", "\u03b8"]]}
{"text": "The final CMR loss for batch B is : L CMR ( B ) = 1 2n n k=1 L", "entities": [[3, 4, "MetricName", "loss"]]}
{"text": "In LightningDOT , we first apply the image encoder f \u03b8 V to all images in I , and cache the resulting global image representations { h ( Johnson et al , 2019 ) in memory for later use .", "entities": [[10, 11, "HyperparameterName", "\u03b8"]]}
{"text": "During inference , given a text query t , we encode it with the language encoder \u03b8 L , and then compute its similarity score to the embedding of every image in I ( stored in memory index ) via Eqn ( 5 ) .", "entities": [[16, 17, "HyperparameterName", "\u03b8"]]}
{"text": "All methods are tested on a single TITAN RTX GPU , with batch size of 400 .", "entities": [[7, 8, "DatasetName", "TITAN"], [12, 14, "HyperparameterName", "batch size"]]}
{"text": "While LightningDOT only takes minutes to evaluate , UNITER - base is estimated to take about 28 days 9 to evaluate under the full setting for both Text Retrieval Image Retrieval Method R@1", "entities": [[8, 9, "MethodName", "UNITER"], [29, 31, "TaskName", "Image Retrieval"], [32, 33, "MetricName", "R@1"]]}
{"text": "For both pre - training and finetuning , AdamW ( Loshchilov and Hutter , 2019 ) is used to optimize the model training , with \u03b2 1 = 0.9 , \u03b2 2 = 0.98 .", "entities": [[8, 9, "MethodName", "AdamW"], [25, 26, "HyperparameterName", "\u03b2"], [30, 31, "HyperparameterName", "\u03b2"]]}
{"text": "We adopt a learning rate warmup strategy , where the learning rate is linearly increased during the first 10 % of training steps , followed by a linear decay to 0 .", "entities": [[3, 5, "HyperparameterName", "learning rate"], [10, 12, "HyperparameterName", "learning rate"], [30, 31, "DatasetName", "0"]]}
{"text": "We set the batch size to 10240 per GPU ( batch size is specified by # tokens + # regions , as in UNITER ) .", "entities": [[3, 5, "HyperparameterName", "batch size"], [10, 12, "HyperparameterName", "batch size"], [23, 24, "MethodName", "UNITER"]]}
{"text": "Pre - training experiments are conducted on 8\u00d7 V100 GPUs with 6 - step gradient accumulation , and the learning rate is set to be 5e - 5 .", "entities": [[19, 21, "HyperparameterName", "learning rate"]]}
{"text": "For ablation studies presented in Table 5 , the ablated instances of our model are pre - trained for 30k steps on COCO dataset ( Lin et al , 2014 ) only , and the same choice of learning rate and batch size are applied as in the best pre - training setting .", "entities": [[22, 23, "DatasetName", "COCO"], [38, 40, "HyperparameterName", "learning rate"], [41, 43, "HyperparameterName", "batch size"]]}
{"text": "For finetuning , we set batch size n to 96 ( n is in examples , instead of the sequence length of tokens and regions ) , and search learning rate from { 1e - 5 , 2e - 5 , 5e - 5 } .", "entities": [[5, 7, "HyperparameterName", "batch size"], [29, 31, "HyperparameterName", "learning rate"]]}
{"text": "The best learning rate is 5e - 5 for COCO and 1e - 5 for Flickr30K.", "entities": [[2, 4, "HyperparameterName", "learning rate"], [9, 10, "DatasetName", "COCO"]]}
{"text": "This idea also lies at the basis of the approach from Soares et al ( 2019 ) , who train a relation encoder by fine - tuning BERT ( Devlin et al , 2019 ) with a link prediction loss .", "entities": [[27, 28, "MethodName", "BERT"], [37, 39, "TaskName", "link prediction"], [39, 40, "MetricName", "loss"]]}
{"text": "We will use the same training data and loss function that we use for fine - tuning the LM ; see Section 3.2 .", "entities": [[8, 9, "MetricName", "loss"]]}
{"text": "+ \u03b3 ) ( 1 ) where \u03c0 , \u03c4 , \u03b3 are hyper - parameters which determine the length of the template .", "entities": [[1, 2, "HyperparameterName", "\u03b3"], [11, 12, "HyperparameterName", "\u03b3"]]}
{"text": "3 P - tuning employs the same template initialization as AutoPrompt but its trigger tokens are newly introduced special tokens with trainable embeddings\u00ea 1 : \u03c0+\u03c4 + \u03b3 , which are learned using a taskspecific loss function while the LM 's weights are frozen .", "entities": [[27, 28, "HyperparameterName", "\u03b3"], [35, 36, "MetricName", "loss"]]}
{"text": "In particular , we use the triplet loss from Schroff et al ( 2015 ) and the classification loss from Reimers and Gurevych ( 2019 ) , both of which are based on this intuition .", "entities": [[6, 8, "MethodName", "triplet loss"], [18, 19, "MetricName", "loss"]]}
{"text": "Formally , this is accomplished using the following triplet loss function : L t = max 0 , x a \u2212 x p \u2212 x a \u2212 x n + \u03b5 3 We note that in most implementations of AutoPrompt the vocabulary to sample trigger tokens is restricted to that of the training data .", "entities": [[8, 10, "MethodName", "triplet loss"], [16, 17, "DatasetName", "0"], [30, 31, "HyperparameterName", "\u03b5"]]}
{"text": "However , given the nature of our training data ( i.e. , pairs of words and not sentences ) , we consider the full pre - trained LM 's vocabulary . where \u03b5 > 0 is the margin and is the l 2 norm .", "entities": [[32, 33, "HyperparameterName", "\u03b5"], [34, 35, "DatasetName", "0"]]}
{"text": "For AutoPrompt and Ptuning , we consider all combinations of \u03c0 { 8 , 9 } , \u03c4 { 1 , 2 } , \u03b3 { 1 , 2 } .", "entities": [[24, 25, "HyperparameterName", "\u03b3"]]}
{"text": "We use the Adam optimizer ( Kingma and Ba , 2014 ) with learn - ing rate 0.00002 , batch size 64 and we fine - tune the model for 1 epoch .", "entities": [[3, 4, "MethodName", "Adam"], [4, 5, "HyperparameterName", "optimizer"], [19, 21, "HyperparameterName", "batch size"]]}
{"text": "For AutoPrompt , the top - 50 tokens are considered and the number of iterations is set to 50 .", "entities": [[12, 15, "HyperparameterName", "number of iterations"]]}
{"text": "Concretely , we tune the learning rate from [ 0.001 , 0.0001 , 0.00001 ] and the hidden layer size from [ 100 , 150 , 200 ] .", "entities": [[5, 7, "HyperparameterName", "learning rate"], [17, 20, "HyperparameterName", "hidden layer size"]]}
{"text": "CogALex - V only has testing fragments so for this dataset we employ the default configuration of Scikit - Learn ( Pedregosa et al , 2011 ) , which uses a 100 - dimensional hidden layer and is optimized using Adam with a learning rate of 0.001 .", "entities": [[40, 41, "MethodName", "Adam"], [43, 45, "HyperparameterName", "learning rate"]]}
{"text": "Table 2 shows the accuracy on the analogy benchmarks .", "entities": [[4, 5, "MetricName", "accuracy"]]}
{"text": "Table 3 summarizes the results of the lexical relation classification experiments , in terms of macro and micro averaged F1 score .", "entities": [[8, 10, "TaskName", "relation classification"], [19, 21, "MetricName", "F1 score"]]}
{"text": "Figure 4 shows the absolute accuracy drop from RelBERT ( i.e. the model with fine - tuning ) to the vanilla RoBERTa model ( i.e. without fine - tuning ) with the same prompt .", "entities": [[5, 6, "MetricName", "accuracy"], [21, 22, "MethodName", "RoBERTa"]]}
{"text": "Table 7 shows the accuracy on the analogy questions , while Table 8 shows the accuracy on the relation classification task .", "entities": [[4, 5, "MetricName", "accuracy"], [15, 16, "MetricName", "accuracy"], [18, 20, "TaskName", "relation classification"]]}
{"text": "Table 10 shows the best prompt configuration based on the validation loss for the SemEval 2012 Task 2 dataset in our main experiments using RoBERTa .", "entities": [[11, 12, "MetricName", "loss"], [24, 25, "MethodName", "RoBERTa"]]}
{"text": "Our experiments on 5 standard corpora show that the proposed method increases F1 - score over relying solely on human expertise and can also be on par with simple supervised approaches .", "entities": [[12, 15, "MetricName", "F1 - score"]]}
{"text": "For instance , Nigam et al ( 2000 ) propose to follow the Expectation - Maximization ( EM ) algorithm by iteratively using the set of labeled data to obtain probabilistically - weighted class labels for each unlabeled document and then training a classifier on the complete corpus based on those annotations .", "entities": [[17, 18, "MetricName", "EM"]]}
{"text": "Overall , the various configurations of our method , all leveraging embeddings for semantic expansion , outperform the simple unsupervised baselines , leading to a doubling of the F1 - score for all corpora , the least affected being the 5Abstracts - Group where F1 goes from 38.1 to 68.3 percent , comparing with the all keywords variant of our method .", "entities": [[28, 31, "MetricName", "F1 - score"], [44, 45, "MetricName", "F1"]]}
{"text": "The difference in performance however is not very large , with the exception of 20NewsGroup where F1 - score increases from 52.6 with generic embeddings to 61 with domain specific ones .", "entities": [[16, 19, "MetricName", "F1 - score"]]}
{"text": "Comparing now our best unsupervised performance with the supervised baseline , we observe that the ratio of the best F1 - score performance over the supervised baseline performance varies from 0.71 to 1.11 with two datasets yielding ratios above 1 .", "entities": [[19, 22, "MetricName", "F1 - score"]]}
{"text": "In creating this dataset , even with the assistance of dictionaries , human performance varied from an F1 - score of about 39 % to 70 % for the collocation task .", "entities": [[17, 20, "MetricName", "F1 - score"]]}
{"text": "Their best method achieved an F1 - score of about 95 % on the VNC tokens dataset .", "entities": [[5, 8, "MetricName", "F1 - score"]]}
{"text": "The accuracy of our tagger is 92.11 % , which is Collocation / Idiom Extractor .", "entities": [[1, 2, "MetricName", "accuracy"]]}
{"text": "On the gold - standard dataset , ICE 's F1 - score was 40.40 % , MWE - Toolkit 's F1 - score was 18.31 % , and Text - NSP had 18 % .", "entities": [[9, 12, "MetricName", "F1 - score"], [20, 23, "MetricName", "F1 - score"]]}
{"text": "We also compared our idiom extraction with AMALGr method ( Schneider et al , 2014 ) on their dataset and the highest F1 - score achieved by ICE was 95 % compared to 67.42 % for AMALGr .", "entities": [[22, 25, "MetricName", "F1 - score"]]}
{"text": "The dropout rate was set to 0.15 ; the best learning rate for IEMOCAP was 0.02 , while for CMU - MOSI and CMU - MOSEI it was 0.01 , with batch sizes of 32 , 128 , and 40 , respectively .", "entities": [[10, 12, "HyperparameterName", "learning rate"], [13, 14, "DatasetName", "IEMOCAP"], [21, 22, "DatasetName", "MOSI"], [23, 26, "DatasetName", "CMU - MOSEI"]]}
{"text": "This dataset is intended for multilabel emotion classification ; we evaluate on the four labeled emotions ( Happy , Sad , Angry , and Neutral ) used in previous work [ Wang et al , 2019 ] ; also following previous work , we report binary accuracy and F1 score as the evaluation metrics on this dataset .", "entities": [[6, 8, "TaskName", "emotion classification"], [46, 47, "MetricName", "accuracy"], [48, 50, "MetricName", "F1 score"]]}
{"text": "Using the hyperparameter settings provided 2 , we were nevertheless unable to match those systems ' reported performance , possibly due to differences 2 Batch size for FMT * is not given ; we use 20 , the default .", "entities": [[24, 26, "HyperparameterName", "Batch size"]]}
{"text": "In training MuLT * and FMT * , we observe that the models are overfitting , with a mean difference of 15 - 20 % between the train and test accuracy ; in contrast , the largest train - test accuracy difference among our three models is only about 10 % .", "entities": [[30, 31, "MetricName", "accuracy"], [40, 41, "MetricName", "accuracy"]]}
{"text": "We compare the training time and memory footprint of our models with MuLT * and FMT * in Table 4 on CMU - MOSEI ( the number of epochs needed for MuLT to converge , as reported by Tsai et al [ 2019 ] ) .", "entities": [[21, 24, "DatasetName", "CMU - MOSEI"], [26, 29, "HyperparameterName", "number of epochs"]]}
{"text": "( 6 ) For S - LSTM , sentence - level cross entropy error are employed to calculate the corresponding loss : 3 Experiment Loss ( S )", "entities": [[6, 7, "MethodName", "LSTM"], [20, 21, "MetricName", "loss"]]}
{"text": "The batch size is set to 32 .", "entities": [[1, 3, "HyperparameterName", "batch size"]]}
{"text": "We employ ADAM ( Kingma and Ba , 2014 ) as optimizer and the default settings of ADAM are used .", "entities": [[2, 3, "DatasetName", "ADAM"], [11, 12, "HyperparameterName", "optimizer"], [17, 18, "DatasetName", "ADAM"]]}
{"text": "Evaluation was performed with a 5 - cross fold validation and using Spearman correlation score ( \u03c1 ) between predicted and gold labels as evaluation metric .", "entities": [[12, 14, "MetricName", "Spearman correlation"]]}
{"text": "Moreover , it can be noticed that , unlike Maxand Min - pooling , the representations computed with Mean and Sum methods tend to lose their average precision in encoding our set of linguistic properties across the 12 layers .", "entities": [[26, 28, "MetricName", "average precision"]]}
{"text": "My best run in Subtask - A has achieved Macro - F1 score of 0.656 .", "entities": [[9, 12, "MetricName", "Macro - F1"]]}
{"text": "For Subtask - A , we have submitted 4 runs based on four different algorithms , namely - Logistic Regression ( Sammut and Webb , 2010 ) , SVM ( Noble , 2006 ) , LSTM ( Hochreiter and Schmidhuber , 1997 ) , Bert ( Devlin et al , 2018 ) with different parameters like batch size , epochs , number of perceptron etc .", "entities": [[18, 20, "MethodName", "Logistic Regression"], [28, 29, "MethodName", "SVM"], [35, 36, "MethodName", "LSTM"], [56, 58, "HyperparameterName", "batch size"]]}
{"text": "We scored maximum F1 score 0.656 using BERT .", "entities": [[3, 5, "MetricName", "F1 score"], [7, 8, "MethodName", "BERT"]]}
{"text": "2 . For LSTM and BERT , we have used batch size = 2 , epochs = 3 and number of layers = 2 .", "entities": [[3, 4, "MethodName", "LSTM"], [5, 6, "MethodName", "BERT"], [10, 12, "HyperparameterName", "batch size"], [19, 22, "HyperparameterName", "number of layers"]]}
{"text": "The results of Subtask - A are represented in terms of Macro - F1 ( shown in Table 2 ) .", "entities": [[11, 14, "MetricName", "Macro - F1"]]}
{"text": "The best score as Macro - F1 for Subtask - A we get is 0.656 .", "entities": [[4, 7, "MetricName", "Macro - F1"]]}
{"text": "For Subtask - A BERT performs better than all other models with the parameters batch size = 2 , epochs = 3 , number of hidden layers = 2 and number of perceptron 's is 128 in first layer and 64 in second layer .", "entities": [[4, 5, "MethodName", "BERT"], [14, 16, "HyperparameterName", "batch size"]]}
{"text": "BInc is the geometric mean of two subscores : a directional score , Weeds Precision ( Weeds and Weir , 2003 ) , measuring how much one vector 's features \" cover \" the other 's ; and a symmetrical score , Lin Similarity ( Lin , 1998 ) , which downweights infrequent predicates that cause spurious false positives .", "entities": [[14, 15, "MetricName", "Precision"]]}
{"text": "Models do n't answer all the questions , so following Lewis and Steedman ( 2013 ) who design a similar QA task , we evaluate models on the accuracy of their K most confident predictions .", "entities": [[28, 29, "MetricName", "accuracy"]]}
{"text": "For the reader , all machine reading comprehension models are trained with 12 epochs , an initial learning rate of 2e - 6 , a maximum sequence length of 512 , a batch size of 5 .", "entities": [[5, 8, "TaskName", "machine reading comprehension"], [17, 19, "HyperparameterName", "learning rate"], [32, 34, "HyperparameterName", "batch size"]]}
{"text": "We use accuracy as the metric to evaluate different methods , and provide baseline results , as well as human pass mark ( 60 % ) instead of human performance due to the wide variations exist in human performance , from almost full marks to can not even pass the exam .", "entities": [[2, 3, "MetricName", "accuracy"]]}
{"text": "To evaluate whether retrieved documents can cover enough evidence to answer questions , we sampled 5 % ( 681 ) questions from the development sets of five categories using stratified random sampling , and manually annotate each question by five medical experts with 3 labels : ( 1 ) Exactly Match ( EM ) : the retrieved documents exactly match the question .", "entities": [[52, 53, "MetricName", "EM"]]}
{"text": "First , most retrieved documents indicate PM with the questions , while the matching rates of EM and MM achieve maximums of 20.83 % ( CWM ) and 50 % ( PH ) , respectively .", "entities": [[16, 17, "MetricName", "EM"]]}
{"text": "From the results we can see that even with 100 % covered materials , the best model can only achieve 16.88 % higher accuracy on the test set than ours , which indicates that using Wikipedia as information sources is not that terrible compared with medical books , and the main reason for baseline performance may come from machine reading comprehension models that lack sophisticated reasoning ability .", "entities": [[23, 24, "MetricName", "accuracy"], [58, 61, "TaskName", "machine reading comprehension"]]}
{"text": "Then s \u0393 * denotes that s is a sequence of arbitrary length , each element of which is in \u0393. We denote by | s | the length of s. A ranked alphabet is an alphabet \u0393 paired with an arity mapping ( i.e. , a total function ) rank : \u0393 N. Definition 1 .", "entities": [[2, 3, "HyperparameterName", "\u0393"], [37, 38, "HyperparameterName", "\u0393"], [52, 53, "HyperparameterName", "\u0393"]]}
{"text": "When the identity of the sequence is immaterial we abbreviate it as \u03b1 , for example writing X \u03b1 . We present our recognizer as a deductive proof system ( Shieber et al , 1995 ) .", "entities": [[12, 13, "HyperparameterName", "\u03b1"], [18, 19, "HyperparameterName", "\u03b1"]]}
{"text": "[ \u03c6p ( \u0113i ) , q : Y \u03b1 , \u03c6 0 q [ ext R ( q ) = \u03c6p ( \u0113i ) ]", "entities": [[9, 10, "HyperparameterName", "\u03b1"], [12, 13, "DatasetName", "0"]]}
{"text": "Using multi - source languages from the same family allows improvements of over 6 BLEU points .", "entities": [[14, 15, "MetricName", "BLEU"]]}
{"text": "The fact of using similar languages in a multi - source system may be a factor towards improving the final system which ends up with over 6 BLEU points of improvement over the single source system .", "entities": [[27, 28, "MetricName", "BLEU"]]}
{"text": "Table 1 shows BLEU results for the baseline systems , the single - language and multi - source approaches .", "entities": [[3, 4, "MetricName", "BLEU"]]}
{"text": "Best improvements achieve an increase of 6 BLEU points in translation quality .", "entities": [[7, 8, "MetricName", "BLEU"]]}
{"text": "Our model outperforms state - of - theart baselines by 15.51 % and 8.38 % in terms of F1 score on two real - world datasets .", "entities": [[18, 20, "MetricName", "F1 score"]]}
{"text": "Various followup studies ( Fader et al , 2011 ; Mausam et al , 2012 ; Angeli et al , 2015 ; Mausam , 2016 ) improved the accuracy of Open IE , by adding handcrafted patterns or by using distant supervision .", "entities": [[1, 2, "DatasetName", "followup"], [28, 29, "MetricName", "accuracy"]]}
{"text": "Our model learns the entity embeddings by minimizing a margin - based objective function J E : JE = tr Tr t r T r max 0 , \u03b3", "entities": [[4, 6, "TaskName", "entity embeddings"], [26, 27, "DatasetName", "0"], [28, 29, "HyperparameterName", "\u03b3"]]}
{"text": "n i \uf8f6 \uf8f8 \uf8f9 \uf8fb ( 8 ) \u03b1", "entities": [[9, 10, "HyperparameterName", "\u03b1"]]}
{"text": "n x n j ) ( 9 ) Here , c d t is the context vector of the decoder at timestep t , h e is the last hidden state of the encoder , the superscript n indicates the n - gram combination , x is the word embeddings of input sentence , | X n | is the total number of n - gram token combination , N indicates the maximum value of n used in the n - gram combinations ( N = 3 in our experiments ) , W and V are learned parameter matrices , and \u03b1 is the attention weight .", "entities": [[48, 50, "TaskName", "word embeddings"], [101, 102, "HyperparameterName", "\u03b1"]]}
{"text": "k ( k = 10 in our experiments ) entity IDs that are predicted by the decoder by computing the edit distance between the entity names ( obtained from the KB ) and every n - gram token of the input sentence .", "entities": [[2, 4, "HyperparameterName", "k ="]]}
{"text": "We use Adam ( Kingma and Ba , 2015 ) with a learning rate of 0.0002 .", "entities": [[2, 3, "MethodName", "Adam"], [12, 14, "HyperparameterName", "learning rate"]]}
{"text": "Our proposed model outperforms the best existing model ( MinIE ) by 33.39 % and 34.78 % in terms of F1 score on the WIKI and GEO test dataset respectively .", "entities": [[20, 22, "MetricName", "F1 score"]]}
{"text": "Our proposed n - gram attention model outperforms the end - to - end models by 15.51 % and 8.38 % in terms of F1 score on the WIKI and GEO test datasets , respectively .", "entities": [[24, 26, "MetricName", "F1 score"]]}
{"text": "Experimental results show that our proposed model outperforms the existing models by 33.39 % and 34.78 % in terms of F1 score on the WIKI and GEO test dataset respectively .", "entities": [[20, 22, "MetricName", "F1 score"]]}
{"text": "Our proposed n - gram attention model outperforms the other encoder - decoder models by 15.51 % and 8.38 % in terms of F1 score on the two real - world datasets .", "entities": [[23, 25, "MetricName", "F1 score"]]}
{"text": "While NELL - 995 is general and covers many domains , its mean average precision was less than 50 % around its 1000th iteration ( Mitchell et al , 2018 ) .", "entities": [[1, 2, "DatasetName", "NELL"], [13, 15, "MetricName", "average precision"]]}
{"text": "CODEX - S ( k = 15 ) , which has 36k triples .", "entities": [[4, 6, "HyperparameterName", "k ="]]}
{"text": "CODEX - M ( k = 10 ) , which has 206k triples .", "entities": [[4, 6, "HyperparameterName", "k ="]]}
{"text": "CODEX - L ( k = 5 ) , which has 612k triples .", "entities": [[4, 6, "HyperparameterName", "k ="]]}
{"text": "5 At a high level , for each dataset and model , we generate both quasirandom and BO trials per negative sampling and loss function combination , ensuring that we search over a wide range of hyperparameters for different types of training strategy .", "entities": [[23, 24, "MetricName", "loss"]]}
{"text": "For example , on the most frequent symmetric relation ( diplomatic relation ) , ComplEx achieves 0.859 MRR , compared to 0.793 for ConvE , 0.490 for RESCAL , and 0.281 for TransE.", "entities": [[17, 18, "MetricName", "MRR"], [27, 28, "MethodName", "RESCAL"]]}
{"text": "For example , on the most frequent compositional relation in CODEX - L ( languages spoken , written , or signed ) , TuckER achieves 0.465 MRR , compared to 0.464 for RESCAL , 0.463 for ConvE , 0.456 for ComplEx , and 0.385 for TransE.", "entities": [[23, 24, "MethodName", "TuckER"], [26, 27, "MetricName", "MRR"], [32, 33, "MethodName", "RESCAL"]]}
{"text": "Effect of hyperparameters As shown by Figure 1 , hyperparameters have a strong impact on link prediction performance : Validation MRR for all models varies by over 30 percentage points depending on the training strategy and input configuration .", "entities": [[15, 17, "TaskName", "link prediction"], [20, 21, "MetricName", "MRR"]]}
{"text": "Each model consistently achieved its respective peak performance with cross - entropy ( CE ) loss , a finding which is corroborated by several other KGC comparison papers ( Kadlec et al , 2017 ; Ruffinelli et al , 2020 ; Jain et al , 2020 ) .", "entities": [[15, 16, "MetricName", "loss"]]}
{"text": "On negatives generated uniformly at random , performance scores are nearly identical at almost 100 % accuracy .", "entities": [[16, 17, "MetricName", "accuracy"]]}
{"text": "Early works reported high triple classification accuracy on sampled negatives ( Socher et al , 2013 ; Wang et al , 2014 ) , perhaps leading the community to believe that the task was nearly solved .", "entities": [[4, 6, "TaskName", "triple classification"], [6, 7, "MetricName", "accuracy"]]}
{"text": "Setup We compare our baseline to the best pretrained embedding model per dataset : RESCAL for FB15 K - 237 , which was released by Ruffinelli et al ( 2020 ) , and ComplEx for CODEX - M. We evaluate performance with MRR and Hits@10 .", "entities": [[14, 15, "MethodName", "RESCAL"], [42, 43, "MetricName", "MRR"], [44, 45, "MetricName", "Hits@10"]]}
{"text": "Surprisingly , the improvement for both MRR and Hits@10 is less than five percentage points for nearly 40 % of FB15 K - 237 's test set , and is zero or negative 15 % of the time .", "entities": [[6, 7, "MetricName", "MRR"], [8, 9, "MetricName", "Hits@10"]]}
{"text": "The search strategy for each CODEX dataset is as follows : CODEX - S : Per negative sampling type / loss combination , we generate 30 quasi - random trials followed by 10 BO trials .", "entities": [[20, 21, "MetricName", "loss"]]}
{"text": "We also terminate a trial after 50 epochs if the model does not reach \u2265 0.05 MRR .", "entities": [[16, 17, "MetricName", "MRR"]]}
{"text": "CODEX - M : Per negative sampling type / loss combination , we generate 20 quasi - random trials .", "entities": [[9, 10, "MetricName", "loss"]]}
{"text": "The maximum number of epochs and early stopping criteria are the same as for CODEX - S. CODEX - L : Per negative sampling type / loss combination , we generate 10 quasi - random trials of 20 training epochs instead of 400 .", "entities": [[2, 5, "HyperparameterName", "number of epochs"], [6, 8, "MethodName", "early stopping"], [26, 27, "MetricName", "loss"]]}
{"text": "In most cases , MRR plateaus after 20 - 30 epochs , an observation which is consistent with ( Ruffinelli et al , 2020 ) .", "entities": [[4, 5, "MetricName", "MRR"]]}
{"text": "Then , we take the best - performing model by validation MRR over all such combinations , and retrain that model for a maximum of 400 epochs .", "entities": [[11, 12, "MetricName", "MRR"]]}
