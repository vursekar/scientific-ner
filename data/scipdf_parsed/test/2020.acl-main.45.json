{"authors": "Xiaoya Li; \u2663 Xiaofei; Yuxian Meng; Junjun Liang; Fei Wu; Jiwei Li; Shannon Ai", "pub_date": "", "title": "Dice Loss for Data-imbalanced NLP Tasks", "abstract": "Many NLP tasks such as tagging and machine reading comprehension (MRC) are faced with the severe data imbalance issue: negative examples significantly outnumber positive ones, and the huge number of easy-negative examples overwhelms training. The most commonly used cross entropy criteria is actually accuracy-oriented, which creates a discrepancy between training and test. At training time, each training instance contributes equally to the objective function, while at test time F1 score concerns more about positive examples.", "sections": [{"heading": "", "text": "In this paper, we propose to use dice loss in replacement of the standard cross-entropy objective for data-imbalanced NLP tasks. Dice loss is based on the S\u00f8rensen-Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977), which attaches similar importance to false positives and false negatives, and is more immune to the data-imbalance issue. To further alleviate the dominating influence from easy-negative examples in training, we propose to associate training examples with dynamically adjusted weights to deemphasize easy-negative examples. Experimental results show that this strategy narrows down the gap between the F1 score in evaluation and the dice loss in training.\nWith the proposed training objective, we observe significant performance boosts over a wide range of data imbalanced NLP tasks. Notably, we are able to achieve SOTA results on CTB5, CTB6 and UD1.4 for the part of speech tagging task, and competitive or even better results on CoNLL03, OntoNotes5.0, MSRA and OntoNotes4.0 for the named entity recognition task along with the machine reading comprehension and paraphrase identification tasks. The code can be found at https://github.com/ShannonAI/ dice_loss_for_NLP.  (Rajpurkar et al., 2016) 10.3M 175K 55.9 SQuAD 2.0 (Rajpurkar et al., 2018) 15.4M 188K 82.0 QUOREF (Dasigi et al., 2019) 6.52M 38.6K 169 ", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Introduction", "text": "Data imbalance is a common issue in a variety of NLP tasks such as tagging and machine reading comprehension. Table 1 gives concrete examples: for the Named Entity Recognition (NER) task Nadeau and Sekine, 2007), most tokens are backgrounds with tagging class O. Specifically, the number of tokens with tagging class O is 5 times as many as those with entity labels for the CoNLL03 dataset and 8 times for the OntoNotes5.0 dataset; Dataimbalanced issue is more severe for MRC tasks (Rajpurkar et al., 2016;Nguyen et al., 2016;Rajpurkar et al., 2018;Ko\u010disk\u1ef3 et al., 2018;Dasigi et al., 2019) with the value of negative-positive ratio being 50-200, which is due to the reason that the task of MRC is usually formalized as predicting the starting and ending indexes conditioned on the query and the context, and given a chunk of text of an arbitrary length, only two tokens are positive (or of interest) with all the rest being background.\nData imbalance results in the following two issues:\n(1) the training-test discrepancy: Without balancing the labels, the learning process tends to converge to a point that strongly biases towards class with the majority label.  (Lample et al., 2016;Devlin et al., 2018;Yu et al., 2018a;McCann et al., 2018;Ma and Hovy, 2016;, handles neither of the issues.\nTo handle the first issue, we propose to replace CE or MLE with losses based on the S\u00f8rensen-Dice coefficient (Sorensen, 1948) or Tversky index (Tversky, 1977). The S\u00f8rensen-Dice coefficient, dice loss for short, is the harmonic mean of precision and recall. It attaches equal importance to false positives (FPs) and false negatives (FNs) and is thus more immune to data-imbalanced datasets. Tversky index extends dice loss by using a weight that trades precision and recall, which can be thought as the approximation of the F \u03b2 score, and thus comes with more flexibility. Therefore, we use dice loss or Tversky index to replace CE loss to address the first issue.\nOnly using dice loss or Tversky index is not enough since they are unable to address the dominating influence of easy-negative examples. This is intrinsically because dice loss is actually a soft version of the F1 score. Taking the binary classification task as an example, at test time, an example will be classified as negative as long as its probability is smaller than 0.5, but training will push the value to 0 as much as possible. The rest of this paper is organized as follows: related work is presented in Section 2. We describe different proposed losses in Section 3. Experimental results are presented in Section 4. We perform ablation studies in Section 5, followed by a brief conclusion in Section 6.\n2 Related Work", "n_publication_ref": 13, "n_figure_ref": 0}, {"heading": "Data Resampling", "text": "The idea of weighting training examples has a long history. Importance sampling (Kahn and Marshall, 1953) assigns weights to different samples and changes the data distribution. Boosting algorithms such as AdaBoost (Kanduri et al., 2018) select harder examples to train subsequent classifiers. Similarly, hard example mining (Malisiewicz et al., 2011) downsamples the majority class and exploits the most difficult examples. Oversampling (Chen et al., 2010;Chawla et al., 2002) (Jiang et al., 2017;Fan et al., 2018) proposed to learn a separate network to predict sample weights.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Data Imbalance Issue in Computer Vision", "text": "The background-object label imbalance issue is severe and thus well studied in the field of object detection (Li et al., 2015;Girshick, 2015;Girshick et al., 2013;. The idea of hard negative mining (HNM) (Girshick et al., 2013) has gained much attention recently. Pang et al. (2019) proposed a novel method called IoU-balanced sampling and  designed a ranking model to replace the conventional classification task with an average-precision loss to alleviate the class imbalance issue. The efforts made on object detection have greatly inspired us to solve the data imbalance issue in NLP. Sudre et al. (2017) addressed the severe class imbalance issue for the image segmentation task. They proposed to use the class re-balancing property of the Generalized Dice Loss as the training objective for unbalanced tasks. Shen et al. (2018) investigated the influence of Dice-based loss for multi-class organ segmentation using a dataset of abdominal CT volumes. Kodym et al. (2018)  3 Losses", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Notation", "text": "For illustration purposes, we use the binary classification task to demonstrate how different losses work. The mechanism can be easily extended to multi-class classification. Let X denote a set of training instances and each instance x i \u2208 X is associated with a golden binary label y i = [y i0 , y i1 ] denoting the ground-truth class x i belongs to, and p i = [p i0 , p i1 ] is the predicted probabilities of the two classes respectively, where y i0 , y i1 \u2208 {0, 1}, p i0 , p i1 \u2208 [0, 1] and p i1 + p i0 = 1.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Cross Entropy Loss", "text": "The vanilla cross entropy (CE) loss is given by:\nCE = \u2212 1 N i j\u2208{0,1} y ij log p ij (1)\nAs can be seen from Eq.1, each x i contributes equally to the final objective. Two strategies are normally used to address the the case where we wish that not all x i are treated equally: associating different classes with different weighting factor \u03b1 or resampling the datasets. For the former, Eq.1 is adjusted as follows:\nWeighted CE = \u2212 1 N i \u03b1 i j\u2208{0,1} y ij log p ij (2)\nwhere \u03b1 i \u2208 [0, 1] may be set by the inverse class frequency or treated as a hyperparameter to set by cross validation. In this work, we use lg( n\u2212nt nt +K) to calculate the coefficient \u03b1, where n t is the number of samples with class t and n is the total number of samples in the training set. K is a hyperparameter to tune. Intuitively, this equation assigns less weight to the majority class and more weight to the minority class. The data resampling strategy constructs a new dataset by sampling training examples from the original dataset based on human-designed criteria, e.g. extracting equal training samples from each class. Both strategies are equivalent to changing the data distribution during training and thus are of the same nature. Empirically, these two methods are not widely used due to the trickiness of selecting \u03b1 especially for multi-class classification tasks and that inappropriate selection can easily bias towards rare classes (Valverde et al., 2017).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Dice Coefficient and Tversky Index", "text": "S\u00f8rensen-Dice coefficient (Sorensen, 1948;Dice, 1945), dice coefficient (DSC) for short, is an F1oriented statistic used to gauge the similarity of two sets. Given two sets A and B, the vanilla dice coefficient between them is given as follows:\nDSC(A, B) = 2|A \u2229 B| |A| + |B| (3)\nIn our case, A is the set that contains all positive examples predicted by a specific model, and B is the set of all golden positive examples in the dataset. When applied to boolean data with the definition of true positive (TP), false positive (FP), and false negative (FN), it can be then written as follows:\nDSC = 2TP 2TP + FN + FP = 2 TP TP+FN TP TP+FP TP TP+FN + TP TP+FP = 2Pre \u00d7 Rec Pre+Rec = F 1\n(4) For an individual example x i , its corresponding dice coefficient is given as follows:\nDSC(x i ) = 2p i1 y i1 p i1 + y i1(5)\nAs can be seen, a negative example (y i1 = 0) does not contribute to the objective. For smoothing purposes, it is common to add a \u03b3 factor to both the nominator and the denominator, making the form to be as follows (we simply set \u03b3 = 1 in the rest of Loss Formula (one sample x i ) \nCE \u2212 j\u2208{0,1} y ij log p ij WCE \u2212\u03b1 i j\u2208{0,1} y ij log p ij DL 1 \u2212 2p i1 y i1 +\u03b3 p 2 i1 +y 2 i1 +\u03b3 TL 1 \u2212 p i1 y i1 +\u03b3 p i1 y i1 +\u03b1 p i1 y i0 +\u03b2 p i0 y i1 +\u03b3 DSC 1 \u2212 2(1\u2212p i1 )p i1 \u2022y i1 +\u03b3 (1\u2212p i1 )p i1 +y i1 +\u03b3 FL \u2212\u03b1 i j\u2208{0,1} (1 \u2212 p ij ) \u03b3 log p ij\nDSC(x i ) = 2p i1 y i1 + \u03b3 p i1 + y i1 + \u03b3 (6)\nAs can be seen, negative examples whose DSC is \u03b3 p i1 +\u03b3 , also contribute to the training. Additionally, Milletari et al. (2016) proposed to change the denominator to the square form for faster convergence, which leads to the following dice loss (DL):\nDL = 1 N i 1 \u2212 2p i1 y i1 + \u03b3 p 2 i1 + y 2 i1 + \u03b3 (7)\nAnother version of DL is to directly compute setlevel dice coefficient instead of the sum of individual dice coefficient, which is easier for optimization:\nDL = 1 \u2212 2 i p i1 y i1 + \u03b3 i p 2 i1 + i y 2 i1 + \u03b3 (8)\nTversky index (TI), which can be thought as the approximation of the F \u03b2 score, extends dice coefficient to a more general case. Given two sets A and B, tversky index is computed as follows:\nTI = |A \u2229 B| |A \u2229 B| + \u03b1|A\\B| + \u03b2|B\\A| (9)\nTversky index offers the flexibility in controlling the tradeoff between false-negatives and falsepositives. It degenerates to DSC if \u03b1 = \u03b2 = 0.5. The Tversky loss (TL) is thus given as follows:\nTL = 1 N i 1 \u2212 pi1yi1 + \u03b3 pi1yi1 + \u03b1 pi1yi0 + \u03b2 pi0yi1 + \u03b3 (10)", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Self-adjusting Dice Loss", "text": "Consider a simple case where the dataset consists of only one example x i , which is classified as positive as long as p i1 is larger than 0.5. The computation of F 1 score is actually as follows: The derivative of DSC approaches zero right after p exceeds 0.5, and for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push p to 1 as much as possible.\nF1(x i ) = 2 I(p i1 > 0.5)y i1 I(p i1 > 0.5) + y i1(11)\nComparing Eq.5 with Eq.11, we can see that Eq.5 is actually a soft form of F 1, using a continuous p rather than the binary I(p i1 > 0. To address this issue, we propose to multiply the soft probability p with a decaying factor (1 \u2212 p), changing Eq.11 to the following adaptive variant of DSC:\nDSC(x i ) = 2(1 \u2212 p i1 )p i1 \u2022 y i1 + \u03b3 (1 \u2212 p i1 )p i1 + y i1 + \u03b3 (12\n)\nOne can think (1 \u2212 p i1 ) as a weight associated with each example, which changes as training proceeds.\nThe intuition of changing p i1 to (1 \u2212 p i1 )p i1 is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, (1 \u2212 p i1 )p i1 makes the model attach significantly less focus to them.\nA close look at Eq.12 reveals that it actually mimics the idea of focal loss (FL for short) (Lin et al., 2017)    In Table 2, we summarize all the aforementioned losses. Figure 1 gives an explanation from the perspective in derivative: The derivative of DSC approaches zero right after p exceeds 0.5, which suggests the model attends less to examples once they are correctly classified. But for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push p to 1 as much as possible.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Experiments", "text": "We evaluated the proposed method on four NLP tasks, part-of-speech tagging, named entity recognition, machine reading comprehension and paraphrase identification. Hyperparameters are tuned on the corresponding development set of each dataset. More experiment details including datasets and hyperparameters are shown in supplementary material.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Part-of-Speech Tagging", "text": "Settings Part-of-speech tagging (POS) is the task of assigning a part-of-speech label (e.g., noun, verb, adjective) to each word in a given text. In this paper, we choose BERT (Devlin et al., 2018) as the backbone and conduct experiments on three widely used Chinese POS datasets including Chinese Treebank (Xue et al., 2005) 5.0/6.0 and UD1.4 and English datasets including Wall Street Journal (WSJ) and the dataset proposed by Ritter et al. (2011). We report the span-level micro-averaged precision, recall and F1 for evaluation.\nBaselines We used the following baselines: Results   ", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Named Entity Recognition", "text": "Settings Named entity recognition (NER) is the task of detecting the span and semantic category of entities within a chunk of text. Our implementation uses the current state-of-the-art model proposed by  as the backbone, and changes the MLE loss to DSC loss. Datasets that we use include OntoNotes4.0 (Pradhan et al., 2011), MSRA (Levow, 2006), CoNLL2003 (Sang and Meulder, 2003 and OntoNotes5.0 (Pradhan et al., 2013). We report span-level micro-averaged precision, recall and F1.\nBaselines We use the following baselines:\n\u2022 ELMo: a tagging model with pretraining from Peters et al. (2018). \u2022 Lattice-LSTM: Zhang and Yang (2018)  Results ", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Machine Reading Comprehension", "text": "Settings The task of machine reading comprehension (MRC) (Seo et al., 2016;Wang and Jiang, 2016;Shen et al., 2017; predicts the answer span in the passage given a question and the passage. We followed the standard protocols in Seo et al. (2016), in which the start and end indexes of answer are predicted. We report Extract Match (EM) as well as F1 score on validation set. We use three datasets on this task: SQuAD v1.1, SQuAD v2.0 (Rajpurkar et al., 2016(Rajpurkar et al., , 2018 and Quoref (Dasigi et al., 2019).\nBaselines We used the following baselines:    enables learning bidirectional contexts.\nResults Table 6 shows the experimental results for MRC task. With either BERT or XLNet, our proposed DSC loss obtains significant performance boost on both EM and F1. For SQuADv1.1, our proposed method outperforms XLNet by +1.25 in terms of F1 score and +0.84 in terms of EM. For SQuAD v2.0, the proposed method achieves 87.65 on EM and 89.51 on F1. On QuoRef, the proposed method surpasses XLNet by +1.46 on EM and +1.41 on F1.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Paraphrase Identification", "text": "Settings Paraphrase identification (PI) is the task of identifying whether two sentences have the same meaning or not. We conduct experiments on the two widely-used datasets: MRPC (Dolan and Brockett, 2005) and QQP. F1 score is reported for comparison. We use BERT (Devlin et al., 2018) and XLNet (Yang et al., 2019) as baselines.\nResults Table 7 shows the results. We find that replacing the training objective with DSC introduces performance boost for both settings, +0.58 for MRPC and +0.73 for QQP.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Ablation Studies", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets imbalanced to different extents", "text": "It is interesting to see how differently the proposed objectives affect datasets imbalanced to different extents. We use the paraphrase identification dataset QQP (37% positive and 63% negative) for studies. To construct datasets with different imbalance degrees, we used the original QQP dataset to construct synthetic training sets with different positive-negative ratios. Models are trained on these different synthetic sets and then test on the same original test set.  Results are shown in Table 8. We first look at the first line, with all results obtained using the MLE objective. We can see that + positive outperforms original, and +negative underperforms original. This is in line with our expectation since + positive creates a balanced dataset while +negative creates a more imbalanced dataset. Despite the fact that -negative creates a balanced dataset, the number of training data decreases, resulting in inferior performances.\nDSC achieves the highest F1 score across all datasets. Specially, for +positive, DSC achieves minor improvements (+0.05 F1) over DL. In contrast, it significantly outperforms DL for +negative dataset. This is in line with our expectation since DSC helps more on more imbalanced datasets. The performance of FL and DL are not consistent across different datasets, while DSC consistently performs the best on all datasets.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dice loss for accuracy-oriented tasks?", "text": "We argue that the cross-entropy objective is actually accuracy-oriented, whereas the proposed losses perform as a soft version of F1 score. To  These results verify that the proposed dice loss is not accuracy-oriented, and should not be used for accuracy-oriented tasks.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Hyper-parameters in Tversky Index", "text": "As mentioned in Section 3.3, Tversky index (TI) offers the flexibility in controlling the tradeoff between false-negatives and false-positives. In this subsection, we explore the effect of hyperparameters (i.e., \u03b1 and \u03b2) in TI to test how they manipulate the tradeoff. We conduct experiments on the Chinese OntoNotes4.0 NER dataset and English QuoRef MRC dataset. Experimental results are shown in Table 10. The highest F1 on Chinese OntoNotes4.0 is 84.67 when \u03b1 is set to 0.6 while for QuoRef, the highest F1 is 68.44 when \u03b1 is set to 0.4. In addition, we can observe that the performance varies a lot as \u03b1 changes in distinct datasets, which shows that the hyperparameters \u03b1, \u03b2 acturally play an important role in TI.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In Table 10: The effect of hyperparameters in Tversky Index. We set \u03b2 = 1 \u2212 \u03b1 and thus we only list \u03b1 here.\nto achieve significant performance boost without changing model architectures.\nannotation of grammar (parts of speech, morphological features, and syntactic dependencies) across different human languages. In this work, we use UD1.4 for Chinese POS tagging.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A.2 Named Entity Recognition", "text": "Datasets For the NER task, we consider both Chinese datasets, i.e., OntoNotes4.0 5 and MSRA 6 , and English datasets, i.e., CoNLL2003 7 and OntoNotes5.0 8 .\n\u2022 CoNLL2003 is an English dataset with 4 entity types: Location, Organization, Person and Miscellaneous. We followed data processing protocols in (Ma and Hovy, 2016).\n\u2022 English OntoNotes5.0 consists of texts from a wide variety of sources and contains 18 entity types. We use the standard train/dev/test split of CoNLL2012 shared task.\n\u2022 Chinese MSRA performs as a Chinese benchmark dataset containing 3 entity types. Data in MSRA is collected from news domain. Since the development set is not provided in the original MSRA dataset, we randomly split the training set into training and development splits by 9:1. We use the official test set for evaluation.\n\u2022 Chinese OntoNotes4.0 is a Chinese dataset and consists of texts from news domain, which has 18 entity types. In this paper, we take the same data split as  did.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A.3 Machine Reading Comprephension", "text": "Datasets For MRC task, we use three datasets: SQuADv1.1/v2.0 9 and Queref 10 datasets.\n\u2022 SQuAD v1.1 and SQuAD v2.0 are the most widely used QA benchmarks. SQuAD1.1 is a collection of 100K crowdsourced question-answer pairs, and SQuAD2.0 extends SQuAD1.1 allowing no short answer exists in the provided passage. \u2022 MRPC is a corpus of sentence pairs automatically extracted from online news sources, with human annotations of whether the sentence pairs are semantically equivalent. The MRPC dataset has imbalanced classes (6800 pairs in total, and 68% for positive, 32% for negative).\n\u2022 QQP is a collection of question pairs from the community question-answering website Quora. The class distribution in QQP is also unbalanced (over 400,000 question pairs in total, and 37% for positive, 63% for negative).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgement", "text": "We thank all anonymous reviewers, as well as Qinghong Han, Wei Wu and Jiawei Wu for their comments and suggestions. The work is supported by the National Natural Science Foundation of China (NSFC No. 61625107 and 61751209).", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Morphosyntactic tagging with a meta-bilstm model over context sensitive token encodings", "journal": "Long Papers", "year": "2018-07-15", "authors": "Bernd Bohnet; Ryan T Mcdonald; Gon\u00e7alo Sim\u00f5es; Daniel Andor; Emily Pitler; Joshua Maynez"}, {"title": "Active bias: Training more accurate neural networks by emphasizing high variance samples", "journal": "", "year": "2017", "authors": " Haw-Shiuan; Erik G Chang; Andrew Learned-Miller;  Mccallum"}, {"title": "Smote: Synthetic minority over-sampling technique", "journal": "J. Artif. Intell. Res", "year": "2002", "authors": "N V Chawla; K W Bowyer; Lawrence O Hall; W P Kegelmeyer"}, {"title": "Reading wikipedia to answer opendomain questions", "journal": "", "year": "2017", "authors": "Danqi Chen; Adam Fisch; Jason Weston; Antoine Bordes"}, {"title": "Towards accurate one-stage object detection with ap-loss", "journal": "", "year": "2019-06-16", "authors": "Kean Chen; Jianguo Li; Weiyao Lin; John See; Ji Wang; Lingyu Duan; Zhibo Chen; Changwei He; Junni Zou"}, {"title": "Ramoboost: Ranked minority oversampling in boosting", "journal": "IEEE Transactions on Neural Networks", "year": "2010", "authors": "Shijuan Chen; Haibo He; Edwardo A Garcia"}, {"title": "Semi-supervised sequence modeling with cross-view training", "journal": "", "year": "2018-10-31", "authors": "Kevin Clark; Minh-Thang Luong; Christopher D Manning; Quoc V Le"}, {"title": "Quoref: A reading comprehension dataset with questions requiring coreferential reasoning", "journal": "", "year": "2019", "authors": "Pradeep Dasigi; F Nelson; Ana Liu;  Marasovic; A Noah; Matt Smith;  Gardner"}, {"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Measures of the amount of ecologic association between species", "journal": "Ecology", "year": "1945", "authors": " Lee R Dice"}, {"title": "Automatically constructing a corpus of sentential paraphrases", "journal": "", "year": "2005", "authors": "B William; Chris Dolan;  Brockett"}, {"title": "Learning to teach", "journal": "ArXiv", "year": "2018", "authors": "Yang Fan; Fei Tian; Tao Qin; Xiuping Li; Tie-Yan Liu"}, {"title": "Fast r-cnn", "journal": "", "year": "2015", "authors": "Ross B Girshick"}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "journal": "", "year": "2013", "authors": "Ross B Girshick; Jeff Donahue; Trevor Darrell; Jitendra Malik"}, {"title": "Improving and Interpreting Neural Networks for Word-Level Prediction Tasks in Natural Language Processing", "journal": "", "year": "2019", "authors": "Fr\u00e9deric Godin"}, {"title": "Deep residual learning for image recognition", "journal": "", "year": "2015", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"title": "Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels", "journal": "", "year": "2017", "authors": "Lu Jiang; Zhengyuan Zhou; Thomas Leung; Li-Jia Li; Li Fei-Fei"}, {"title": "Methods of reducing sample size in monte carlo computations", "journal": "Operations Research", "year": "1953", "authors": "H Kahn; A W Marshall"}, {"title": "adboost: Thermal aware performance boosting through dark silicon patterning", "journal": "IEEE Trans. Computers", "year": "2018", "authors": "Anil Kanduri; Mohammad Hashem Haghbayan; M Amir; Muhammad Rahmani; Axel Shafique; Pasi Jantsch;  Liljeberg"}, {"title": "Not all samples are created equal: Deep learning with importance sampling", "journal": "", "year": "2018", "authors": "Angelos Katharopoulos; Fran\u00e7ois Fleuret"}, {"title": "The narrativeqa reading comprehension challenge", "journal": "Transactions of the Association of Computational Linguistics", "year": "2018", "authors": "Tom\u00e1\u0161 Ko\u010disk\u1ef3; Jonathan Schwarz; Phil Blunsom; Chris Dyer; Karl Moritz Hermann; G\u00e1abor Melis; Edward Grefenstette"}, {"title": "", "journal": "", "year": "", "authors": "Oldrich Kodym; Michal Spanel; Adam Herout"}, {"title": "Segmentation of head and neck organs at risk using CNN with batch dice loss", "journal": "", "year": "2018-10-09", "authors": ""}, {"title": "Self-paced learning for latent variable models", "journal": "", "year": "2010-12", "authors": "M Pawan Kumar; Benjamin Packer; Daphne Koller"}, {"title": "Neural architectures for named entity recognition", "journal": "", "year": "2016", "authors": "Guillaume Lample; Miguel Ballesteros; Sandeep Subramanian; Kazuya Kawakami; Chris Dyer"}, {"title": "The third international Chinese language processing bakeoff: Word segmentation and named entity recognition", "journal": "", "year": "2006", "authors": "Gina-Anne Levow"}, {"title": "A convolutional neural network cascade for face detection", "journal": "", "year": "2015", "authors": "H Li; Z Lin; X Shen; J Brandt; G Hua"}, {"title": "A unified MRC framework for named entity recognition", "journal": "", "year": "1910", "authors": "Xiaoya Li; Jingrong Feng; Yuxian Meng; Qinghong Han; Fei Wu; Jiwei Li"}, {"title": "Focal loss for dense object detection", "journal": "", "year": "2017", "authors": "Tsung-Yi Lin; Priya Goyal; Ross Girshick"}, {"title": "End-to-end sequence labeling via bi-directional lstm-cnns-crf", "journal": "", "year": "2016", "authors": "Xuezhe Ma; Eduard Hovy"}, {"title": "Ensemble of exemplar-svms for object detection and beyond", "journal": "", "year": "2011-11-06", "authors": "Tomasz Malisiewicz; Abhinav Gupta; Alexei A Efros"}, {"title": "The natural language decathlon: Multitask learning as question answering", "journal": "", "year": "2018", "authors": "Bryan Mccann; Nitish Shirish Keskar; Caiming Xiong; Richard Socher"}, {"title": "Dsreg: Using distant supervision as a regularizer", "journal": "", "year": "2019", "authors": "Yuxian Meng; Muyu Li; Wei Wu; Jiwei Li"}, {"title": "V-net: Fully convolutional neural networks for volumetric medical image segmentation", "journal": "IEEE", "year": "2016", "authors": ""}, {"title": "A survey of named entity recognition and classification. Lingvisticae Investigationes", "journal": "", "year": "2007", "authors": "David Nadeau; Satoshi Sekine"}, {"title": "Ms marco: A human generated machine reading comprehension dataset", "journal": "", "year": "2016", "authors": "Tri Nguyen; Mir Rosenberg; Xia Song; Jianfeng Gao; Saurabh Tiwary; Rangan Majumder; Li Deng"}, {"title": "Libra R-CNN: towards balanced learning for object detection", "journal": "", "year": "2019-06-16", "authors": "Jiangmiao Pang; Kai Chen; Jianping Shi; Huajun Feng; Wanli Ouyang; Dahua Lin"}, {"title": "Deep contextualized word representations", "journal": "", "year": "2018", "authors": "E Matthew; Mark Peters; Mohit Neumann; Matt Iyyer; Christopher Gardner; Kenton Clark; Luke Lee;  Zettlemoyer"}, {"title": "Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task. ACL", "journal": "", "year": "2011", "authors": "Mitchell P Sameer Pradhan; Martha Marcus; Lance A Palmer; Ralph M Ramshaw; Nianwen Weischedel;  Xue"}, {"title": "Towards robust linguistic analysis using OntoNotes", "journal": "Association for Computational Linguistics", "year": "2013", "authors": "Alessandro Sameer Pradhan; Nianwen Moschitti; Hwee Tou Xue; Anders Ng; Olga Bj\u00f6rkelund; Yuchen Uryupina; Zhi Zhang;  Zhong"}, {"title": "Know what you don't know: Unanswerable questions for squad", "journal": "", "year": "2018", "authors": "Pranav Rajpurkar; Robin Jia; Percy Liang"}, {"title": "Squad: 100,000+ questions for machine comprehension of text", "journal": "", "year": "2016", "authors": "Pranav Rajpurkar; Jian Zhang; Konstantin Lopyrev; Percy Liang"}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": "2015", "authors": "Kaiming Shaoqing Ren; Ross B He; Jian Girshick;  Sun"}, {"title": "Named entity recognition in tweets: An experimental study", "journal": "", "year": "2011", "authors": "Alan Ritter; Sam Clark; Mausam ; Oren Etzioni"}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "journal": "", "year": "2003", "authors": "F Erik; Fien Sang;  De Meulder"}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "journal": "", "year": "2003-05-31", "authors": "Erik F Tjong; Kim Sang; Fien De Meulder"}, {"title": "Bidirectional attention flow for machine comprehension", "journal": "", "year": "2016", "authors": "Minjoon Seo; Aniruddha Kembhavi; Ali Farhadi; Hannaneh Hajishirzi"}, {"title": "Continuous dice coefficient: a method for evaluating probabilistic segmentations", "journal": "", "year": "1906", "authors": "Reuben R Shamir; Yuval Duchin; Jinyoung Kim; Guillermo Sapiro; Noam Harel"}, {"title": "Character-based joint segmentation and pos tagging for chinese using bidirectional rnncrf", "journal": "", "year": "2017", "authors": "Yan Shao; Christian Hardmeier; J\u00f6rg Tiedemann; Joakim Nivre"}, {"title": "On the influence of dice loss function in multi-class organ segmentation of abdominal CT using 3d fully convolutional networks", "journal": "CoRR", "year": "2018", "authors": "Chen Shen; R Holger; Hirohisa Roth; Masahiro Oda; Yuichiro Oda; Kazunari Hayashi; Kensaku Misawa;  Mori"}, {"title": "Reasonet: Learning to stop reading in machine comprehension", "journal": "ACM", "year": "2017", "authors": "Yelong Shen; Po-Sen Huang; Jianfeng Gao; Weizhu Chen"}, {"title": "A method of establishing groups of equal amplitude in plant sociology based on similarity of species content and its application to analyses of the vegetation on danish commons", "journal": "Biol. Skar", "year": "1948", "authors": "A Th;  Sorensen"}, {"title": "Generalised dice overlap as a deep learning loss function for highly unbalanced segmentations", "journal": "", "year": "2017-09-14", "authors": "Carole H Sudre; Wenqi Li; Tom Vercauteren; S\u00e9bastien Ourselin; M. Jorge Cardoso"}, {"title": "Features of similarity. Psychological review", "journal": "", "year": "1977", "authors": "Amos Tversky"}, {"title": "Improving automated multiple sclerosis lesion segmentation with a cascaded 3d convolutional neural network approach", "journal": "NeuroImage", "year": "2017", "authors": "Mariano Sergi Valverde; Eloy Cabezas; Sandra Roura; Deborah Gonz\u00e1lez-Vill\u00e0; Joan C Pareto; Llu\u00eds Vilanova; \u00c0lex Rami\u00f3-Torrent\u00e0; Arnau Rovira; Xavier Oliver;  Llad\u00f3"}, {"title": "Machine comprehension using match-lstm and answer pointer", "journal": "", "year": "2016", "authors": "Shuohang Wang; Jing Jiang"}, {"title": "Multi-perspective context matching for machine comprehension", "journal": "", "year": "2016", "authors": "Zhiguo Wang; Haitao Mi; Wael Hamza; Radu Florian"}, {"title": "The penn chinese treebank: Phrase structure annotation of a large corpus", "journal": "", "year": "2005", "authors": "Wei Wu; Yuxian Meng; Qinghong Han; Muyu Li; Xiaoya Li; Jie Mei; Ping Nie; Xiaofei Sun; Jiwei Li"}, {"title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "", "year": "1906", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime G Carbonell; Ruslan Salakhutdinov; V Quoc;  Le"}, {"title": "Qanet: Combining local convolution with global self-attention for reading comprehension", "journal": "", "year": "2018", "authors": "Adams Wei Yu; David Dohan; Minh-Thang Luong; Rui Zhao; Kai Chen; Mohammad Norouzi; Quoc V Le"}, {"title": "", "journal": "", "year": "", "authors": "Adams Wei Yu; David Dohan; Minh-Thang Luong; Rui Zhao; Kai Chen; Mohammad Norouzi; V Quoc"}, {"title": "Qanet: Combining local convolution with global self-attention for reading comprehension", "journal": "", "year": "2018-04-30", "authors": " Le"}, {"title": "Chinese ner using lattice lstm", "journal": "", "year": "2018", "authors": "Yue Zhang; Jie Yang"}, {"title": "A Dataset Details A.1 Part-of-Speech Tagging Datasets We conduct experiments on three widely used benchmark", "journal": "", "year": "", "authors": ""}, {"title": "\u2022 CTB5 is a Chinese dataset for tagging and parsing, which contains 507,222 words, 824,983 characters and 18,782 sentences extracted from newswire sources, including 698 articles from Xinhua", "journal": "", "year": "1994", "authors": ""}, {"title": "\u2022 CTB6 is an extension of CTB5, containing 781,351 words, 1,285,149 characters and 28", "journal": "", "year": "", "authors": ""}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "proposed to use the batch soft Dice loss function to train the CNN network for the task of segmentation of organs at risk (OAR) of medical images. Shamir et al. (2019) extended the definition of the classical Dice coefficient to facilitate the direct comparison of a ground truth binary image with a probabilistic map. In this paper, we introduce dice loss into NLP tasks as the training objective and propose a dynamic weight adjusting strategy to address the dominating influence of easy-negative examples.", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure1: An illustration of derivatives of the four losses. The derivative of DSC approaches zero right after p exceeds 0.5, and for the other losses, the derivatives reach 0 only if the probability is exactly 1, which means they will push p to 1 as much as possible.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "\u2022Joint-POS: Shao et al. (2017) jointly learns Chinese word segmentation and POS. \u2022 Lattice-LSTM: Zhang and Yang (2018) constructs a word-character lattice network. \u2022 Bert-Tagger: Devlin et al. (2018) treats partof-speech as a tagging task.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_3", "figure_caption": "constructs a word-character lattice, only used in Chinese datasets. \u2022 CVT: Clark et al. (2018) uses Cross-View Training(CVT) to improve the representations of a Bi-LSTM encoder. \u2022 Bert-Tagger: Devlin et al. (2018) treats NER as a tagging task. \u2022 Glyce-BERT: Wu et al. (2019) combines Chinese glyph information with BERT pretraining. \u2022 BERT-MRC: Li et al. (2019) formulatesNER as a machine reading comprehension task and achieves SOTA results on Chinese and English NER benchmarks.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_4", "figure_caption": "\u2022QANet: Yu et al. (2018b) builds a model based on convolutions and self-attentions. Convolutions are used to model local interactions and self-attention are used to model global interactions. \u2022 BERT: Devlin et al. (2018) scores each candidate span and the maximum scoring span is used as a prediction. \u2022 XLNet: Yang et al. (2019) proposes a generalized autoregressive pretraining method that", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Number of positive and negative examples and their ratios for different data-imbalanced NLP tasks.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "number of easy-negative example is large. The huge number of easy examples tends to overwhelm the training, making the model not sufficiently learn to distinguish between positive examples and hard-negative examples. The crossentropy objective (CE for short) or maximum likelihood (MLE) objective, which is widely adopted as the training objective for data-imbalanced NLP tasks", "figure_data": "This actually creates a dis-crepancy between training and test: at training time,each training instance contributes equally to the"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Different losses and their formulas. We add +1 to DL, TL and DSC so that they are positive.", "figure_data": "this paper):"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Sig)(Shao et al., 2017) 93.68 94.47 94.07 --90.81 89.28 89.54 89.41 Joint-POS(Ens)(Shao et al., 2017) 93.95 94.81 94.38 ---89.67 89.86 89.75 Lattice-LSTM(Zhang and Yang, 2018) 94.77 95.51 95.14 92.00 90.86 91.43 90.47 89.70 90.09 BERT-Tagger(Devlin et al., 2018) 95.86 96.26 96.06 94.91 94.63 94.77 95.42 94.17 94.79 BERT+FL 96.11 97.42 96.76 95.80 95.08 95.44 96.33 95.85 96.81", "figure_data": "CTB5CTB6UD1.4ModelPrec. Rec.F1Prec. Rec.F1Prec. Rec.F1Joint-POS((+0.70)(+0.67)(+2.02)BERT+DL96.77 98.87 97.81 94.08 96.12 95.09 96.10 97.79 96.94(+1.75)(+0.32)(+2.15)BERT+DSC97.10 98.75 97.92 96.29 96.85 96.57 96.24 97.73 96.98(+1.86)(+1.80)(+2.19)"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Experimental results for Chinese POS datasets including CTB5, CTB6 and UD1.4.", "figure_data": "English WSJModelPrec. Rec.F1Meta BiLSTM(Bohnet et al., 2018)--98.23BERT-Tagger (Devlin et al., 2018) 99.21 98.36 98.86BERT-Tagger+FL98.36 98.97 98.88(+0.02)BERT-Tagger+DL99.34 98.22 98.91(+0.05)BERT-Tagger+DSC99.41 98.93 99.38(+0.52)English TweetsModelPrec. Rec.F1FastText+CNN+CRF(Godin, 2019)--91.78BERT-Tagger (Devlin et al., 2018) 92.33 91.98 92.34BERT-Tagger+FL91.24 93.22 92.47(+0.13)BERT-Tagger+DL91.44 92.88 92.52(+0.18)BERT-Tagger+DSC92.87 93.54 92.58(+0.24)"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Experimental results for English POS datasets.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": "presents the experimental resultson Chinese datasets. As can be seen, the proposedDSC loss outperforms the best baseline results bya large margin, i.e., outperforming BERT-taggerby +1.86 in terms of F1 score on CTB5, +1.80 onCTB6 and +2.19 on UD1.4. As far as we know,we are achieving SOTA performances on the threedatasets. Focal loss only obtains a little perfor-mance improvement on CTB5 and CTB6, and thedice loss obtains huge gain on CTB5 but not onCTB6, which indicates the three losses are not con-sistently robust in solving the data imbalance issue."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "", "figure_data": "English CoNLL 2003ModelPrec. Rec.F1ELMo(Peters et al., 2018)--92.22CVT(Clark et al., 2018)--92.6BERT-Tagger(Devlin et al., 2018)--92.8BERT-MRC(Li et al., 2019)92.33 94.61 93.04BERT-MRC+FL93.13 93.09 93.11(+0.06)BERT-MRC+DL93.22 93.12 93.17(+0.12)BERT-MRC+DSC93.41 93.25 93.33(+0.29)English OntoNotes 5.0ModelPrec. Rec.F1CVT (Clark et al., 2018)--88.8BERT-Tagger (Devlin et al., 2018)90.01 88.35 89.16BERT-MRC(Li et al., 2019)92.98 89.95 91.11BERT-MRC+FL90.13 92.34 91.22(+0.11)BERT-MRC+DL91.70 92.06 91.88(+0.77)BERT-MRC+DSC91.59 92.56 92.07(+0.96)Chinese MSRAModelPrec. Rec.F1Lattice-LSTM (Zhang and Yang, 2018) 93.57 92.79 93.18BERT-Tagger (Devlin et al., 2018)94.97 94.62 94.80Glyce-BERT (Wu et al., 2019)95.57 95.51 95.54BERT-MRC(Li et al., 2019)96.18 95.12 95.75BERT-MRC+FL95.45 95.89 95.67(-0.08)BERT-MRC+DL96.20 96.68 96.44(+0.69)BERT-MRC+DSC96.67 96.77 96.72(+0.97)Chinese OntoNotes 4.0ModelPrec. Rec.F1Lattice-LSTM (Zhang and Yang, 2018) 76.35 71.56 73.88BERT-Tagger (Devlin et al., 2018)78.01 80.35 79.16Glyce-BERT (Wu et al., 2019)81.87 81.40 80.62BERT-MRC(Li et al., 2019)82.98 81.25 82.11BERT-MRC+FL83.63 82.97 83.30(+1.19)BERT-MRC+DL83.97 84.05 84.01(+1.90)BERT-MRC+DSC84.22 84.72 84.47(+2.36)"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Experimental results for NER task.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "", "figure_data": "shows experimental results onNER datasets. DSC outperforms BERT-MRC(Liet al., 2019) by +0.29, +0.96, +0.97 and +2.36 re-spectively on CoNLL2003, OntoNotes5.0, MSRAand OntoNotes4.0. As far as we are concerned, weare setting new SOTA performances on all of thefour NER datasets."}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Experimental results for MRC task.", "figure_data": "MRPCQQPModelF1F1BERT (Devlin et al., 2018)88.091.3BERT+FL88.4391.86(+0.43) (+0.56)BERT+DL88.7191.92(+0.71) (+0.62)BERT+DSC88.9292.11(+0.92) (+0.81)XLNet (Yang et al., 2019)89.291.8XLNet+FL89.2592.31(+0.05) (+0.51)XLNet+DL89.3392.39(+0.13) (+0.59)XLNet+DSC89.7892.60(+0.58) (+0.79)"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_16", "figure_caption": "Experimental results for PI task.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "+0.56) 92.64(+0.37) 90.61(+0.53) 90.79(+1.06) 93.45(+0.31) BERT+DL 91.92(+0.62) 92.87(+0.60) 90.22(+0.14) 90.49(+0.76) 93.52(+0.38) BERT+DSC 92.11(+0.81) 92.92(+0.65) 90.78(+0.70) 90.80(+1.07) 93.63(+0.49)", "figure_data": "original+ positive+ negative-negative+ positive & negativeBERT91.392.2790.0889.7393.14BERT+FL91.86(\u2022 Original training set (original) The originaldataset with 363,871 examples, with 37% be-ing positive and 63% being negative\u2022 Positive augmentation (+ positive)We created a balanced dataset by adding posi-tive examples. We first randomly chose posi-tive training examples in the original trainingset as templates. Then we used Spacy 1 to re-trieve entity mentions and replace them withnew ones by linking mentions to their corre-sponding entities in DBpedia. The augmentedset contains 458,477 examples, with 50% be-ing positive and 50% being negative.\u2022 Negative augmentation (+ negative)We created a more imbalanced dataset. Thesize of the newly constructed training set and"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "The effect of different data augmentation ways for QQP in terms of F1-score.", "figure_data": "the data augmented technique are exactly thesame as +negative, except that we chose neg-ative training examples as templates. The aug-mented training set contains 458,477 exam-ples, with 21% being positive and 79% beingnegative.\u2022 Negative downsampling (-negative)We down-sampled negative examples in theoriginal training set to get a balanced trainingset. The down-sampled set contains 269,165examples, with 50% being positive and 50%being negative.\u2022 Positive and negative augmentation (+ pos-itive & +negative)We augmented the original training data withadditional positive and negative exampleswith the data distribution staying the same.The augmented dataset contains 458,477 ex-amples, with 50% being positive and 50% be-ing negative."}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "The effect of DL and DSC on sentiment classification tasks. BERT+CE refers to fine-tuning BERT and setting cross-entropy as the training objective.", "figure_data": "explore the effect of the dice loss on accuracy-oriented tasks such as text classification, we con-duct experiments on the Stanford Sentiment Tree-bank (SST) datasets including SST-2 and SST-5.We fine-tuned BERT Large with different training ob-jectives. Experimental results for SST are shownin Table 9. For SST-5, BERT with CE achieves55.57 in terms of accuracy, while DL and DSCperform slightly worse (54.63 and 55.19, respec-tively). Similar phenomenon is observed for SST-2."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_22", "figure_caption": "https://catalog.ldc.upenn.edu/ LDC2011T03 6 http://sighan.cs.uchicago.edu/ bakeoff2006/ 7 https://www.clips.uantwerpen.be/ conll2003/ner/ 8 https://catalog.ldc.upenn.edu/ LDC2013T19 9 https://rajpurkar.github.io/ SQuAD-explorer/ 10 https://allennlp.org/quoref \u2022 Quoref is a QA dataset which tests the coreferential reasoning capability of reading comprehension systems, containing 24K questions over 4.7K paragraphs from Wikipedia. A.4 Paraphrase Identification Datasets Experiments are conducted on two PI datasets: MRPC 11 and QQP 12 .", "figure_data": ""}], "doi": "10.1109/CVPR.2015.7299170"}