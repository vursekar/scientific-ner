{"authors": "Wenhao Yu; Chenguang Zhu; Lianhui Qin; Zhihan Zhang; Tong Zhao; Meng Jiang", "pub_date": "", "title": "Diversifying Content Generation for Commonsense Reasoning with Mixture of Knowledge Graph Experts", "abstract": "Generative commonsense reasoning (GCR) in natural language is to reason about the commonsense while generating coherent text. Recent years have seen a surge of interest in improving the generation quality of commonsense reasoning tasks. Nevertheless, these approaches have seldom investigated diversity in the GCR tasks, which aims to generate alternative explanations for a real-world situation or predict all possible outcomes. Diversifying GCR is challenging as it expects to generate multiple outputs that are not only semantically different but also grounded in commonsense knowledge. In this paper, we propose MoKGE, a novel method that diversifies the generative reasoning by a mixture of expert (MoE) strategy on commonsense knowledge graphs (KG). A set of knowledge experts seek diverse reasoning on KG to encourage various generation outputs. Empirical experiments demonstrated that MoKGE can significantly improve the diversity while achieving on par performance on accuracy on two GCR benchmarks, based on both automatic and human evaluations.", "sections": [{"heading": "Introduction", "text": "An important desideratum of natural language generation (NLG) is to produce outputs that are not only correct but also diverse (Tevet and Berant, 2021). The term \"diversity\" in NLG is defined as the ability of a generative model to create a set of possible outputs that are each valid given the input and vary as widely as possible in terms of content, language style, and word variability (Gupta et al., 2018). This research problem is also referred as one-to-many generation (Shen et al., 2019;Cho et al., 2019;Shen et al., 2022).\nDiversity in NLG has been extensively studied for various tasks in the past few years, such as machine translation (Shen et al., 2019) and paraphrase \u00a7 Codes of our model and baselines are available at https://github.com/DM2-ND/MoKGE. [1]\n[4] [3] [1 ] [3] [4] [4] [1] [3] [4] [2]\n[4] [1] (1) You can produce music when pressing keys on the piano, so it is an instrument .  generation (Gupta et al., 2018). In these tasks, output spaces are constrained by input context, i.e., the contents of multiple outputs should be similar, and globally, under the same topic. However, many NLG tasks, e.g., generative commonsense reasoning, pose unique challenges for generating multiple reasonable outputs that are semantically different. Figure 1 shows an example in the commonsense explanation generation (ComVE) task. The dataset has collected explanations to counterfactual statements for sense-making from three annotators (Wang et al., 2020). From the annotations, we observed that different annotators gave explanations to the unreasonable statement from different perspectives to make them diverse in terms of content, e.g., wrong effect and inappropriate usage.\nIn order to create diversity, existing methods attempted to produce uncertainty by introducing random noise into a latent variable (Gupta et al., 2018) or sampling next token widely from the vo- cabulary . However, these methods were not able to explicitly control varying semantics units and produce outputs of diverse content. Meanwhile, the input text alone contains too limited knowledge to support diverse reasoning and produce multiple reasonable outputs (Yu et al., 2022c). As an example, Table 1 shows the human evaluation results on two GCR tasks. While human annotators were able to produce 2.60 different yet reasonable explanations on the ComVE dataset, one SoTA diversity-promoting method (i.e., nucleus sampling ) could produce only 2.15 reasonable explanations.\nTo improve the diversity in outputs for GCR tasks, we investigated the ComVE task and found that 75% of the concepts (nouns and verbs) in human annotations were among 2-hop neighbors of the concepts contained in the input sequence on the commonsense KG ConceptNet 1 . Therefore, to produce diverse GCR, our idea is enabling NLG models to reason from different perspectives of knowledge on commonsense KG and use them to generate diverse outputs like the human annotators.\nThus, we present a novel Mixture of Knowledge Graph Expert (MoKGE) method for diverse generative commonsense reasoning on KG. MoKGE contains two major components: (i) a knowledge graph (KG) enhanced generative reasoning module to reasonably associate relevant concepts into the generation process, and (ii) a mixture of expert (MoE) module to produce diverse reasonable outputs. Specifically, the generative reasoning module performs compositional operations on KG to obtain structure-aware representations of concepts and relations. Then, each expert uses these representations to seek different yet relevant sets of concepts and sends them into a standard Transformer model to generate the corresponding output. To encourage different experts to specialize in different reasoning abilities, we employ the stochastic hard-EM algorithm by assigning full responsibility of the largest joint probability to each expert.\nWe conducted experiments on two GCR benchmarks, i.e., commonsense explanation generation and abductive commonsense reasoning. Empirical experiments demonstrated that our proposed MoKGE can outperform existing diversitypromoting generation methods in diversity, while achieving on par performance in quality.\nTo the best of our knowledge, this is the first work to boost diversity in NLG by diversifying knowledge reasoning on commonsense KG.", "n_publication_ref": 14, "n_figure_ref": 1}, {"heading": "Related Work", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Diversity Promoting Text Generation", "text": "Generating multiple valid outputs given a source sequence has a wide range of applications, such as machine translation (Shen et al., 2019), paraphrase generation (Gupta et al., 2018), question generation (Cho et al., 2019), dialogue system (Dou et al., 2021), and story generation . For example, in machine translation, there are often many plausible and semantically equivalent translations due to information asymmetry between different languages (Lachaux et al., 2020).\nMethods of improving diversity in NLG have been explored from various perspectives. Sampling-based decoding is one of the most effective solutions to improve diversity. For example, nucleus sampling  samples next tokens from the dynamic nucleus of tokens containing the vast majority of the probability mass, instead of decoding text by maximizing the likelihood. Another line of work focused on introducing random noise (Gupta et al., 2018) or changing latent variables (Lachaux et al., 2020) to produce uncertainty. In addition, Shen et al. (2019) adopted a mixture of experts to diversify machine translation, where a minimum-loss predictor is assigned to each source input. Shi et al. (2018) employed an inverse reinforcement learning approach for unconditional diverse text generation.\nHowever, no existing work considered performing diverse knowledge reasoning to generate multiple reasonable outputs of different contents.", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Knowledge Graph for Text Generation", "text": "Incorporating external knowledge is essential for many NLG tasks to augment the limited textual  information (Yu et al., 2022c;Dong et al., 2021;Yu et al., 2022b). Some recent work explored using graph neural networks (GNN) to reason over multihop relational knowledge graph (KG) paths (Zhou et al., 2018;Jiang et al., 2019;Zhang et al., 2020a;Wu et al., 2020;Yu et al., 2022a;Zeng et al., 2021). For example, Zhou et al. (2018) enriched the context representations of the input sequence with neighbouring concepts on ConceptNet using graph attention. Ji et al. (2020) performed dynamic multi-hop reasoning on multi-relational paths extracted from the external commonsense KG. Recently, some work attempted to integrate external commonsense knowledge into generative pretrained language models (Guan et al., 2020;Bhagavatula et al., 2020;Liu et al., 2021). For example, Guan et al. (2020) conducted post-training on sythetic data constructed from commonsense KG by translating triplets into natural language texts using templates. Yu et al. (2022c) wrote a comprehensive survey for more detailed comparisons of different knowledge graph enhanced NLG methods.", "n_publication_ref": 16, "n_figure_ref": 0}, {"heading": "Proposed Method", "text": "Problem formulation. In this paper, we focus on diversifying the outputs of generative commonsense reasoning (GCR) tasks, e.g. commonsense explanation generation and abductive commonsense reasoning. These tasks require one-to-many generation, i.e., creating a set of reasonable outputs that vary as widely as possible in terms of con-tents, language style and word variability. Formally, given a source input x, our goal is to model a conditional distribution for the target outputs p(y|x) that assigns high values to {p(y\n1 |x), \u2022 \u2022 \u2022 , p(y K |x)} for K mappings, i.e., {x \u2192 y 1 , \u2022 \u2022 \u2022 , x \u2192 y K }.\nMeanwhile, the outputs {y 1 , \u2022 \u2022 \u2022 , y K } are expected to be diverse with each other in terms of contents.\nExisting diversity-promoting methods only varied the language styles and failed to perform different knowledge reasoning to generate diverse contents (Cho et al., 2019;Shen et al., 2019;. Here, incorporating commonsense KG is essential for the generative reasoning (GR) tasks because the KG cannot only augment the limited information in the input text, but also provide a rich searching space for knowledge reasoning. Therefore, we propose to employ commonsense KG to play the central role of performing diverse knowledge reasoning, then use different sets of selected concepts to produce diverse outputs.\nModel Outline. Our model has two major components: (i) a knowledge graph (KG) enhanced generative reasoning module to reasonably associate relevant concepts and background into the generation process, and (ii) a mixture of expert (MoE) module to diversify the generation process and produce multiple reasonable outputs.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "KG-enhanced Generative Reasoning", "text": "The KG-enhanced generative reasoning module is illustrated in Figure 2. It consists of four steps.\nFirst, a sequence-associated subgraph is retrieved from the KG given the input sequence ( \u00a73.1.1). Then, a multi-relational graph encoder iteratively updates the representation of each node by aggregating information from its neighboring nodes and edges ( \u00a73. 1.2). Next, the model selects salient concepts that should be considered during generation ( \u00a73. 1.3). Finally, the model generates outputs by integrating the token embeddings of both the input sequence and the top-ranked concepts ( \u00a73.1.4).", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Sequence-aware subgraph construction", "text": "To facilitate the reasoning process, we resort to an external commonsense knowledge graph G = {V, E}, where V denotes the concept set and E denotes the edges with relations. Since direct reasoning on the entire graph is intractable, we extract a sequence-associated subgraph G x = {V x , E x }, where V x consists of the concepts extracted from the input sequence (denoted as C x ) and their inter-connected concepts within two hops, i.e.,\nV x = {C x \u222a N (C x ) \u222a N (N (C x ))}. For exam- ple, in Figure 2, C x = {piano, sport, kind} and V x = {piano, sport, kind, art, music, press, ...}.\nNext, the generation task is to maximize the conditional probability p(y|x, G x ).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Multi-relational graph encoding", "text": "To model the relational information in the commonsen KG, we employ the relational graph convolutional network (R-GCN) (Schlichtkrull et al., 2018) which generalizes GCN with relation specific weight matrices. We follow Vashishth et al. (2020) and Ji et al. (2020) to use a non-parametric compositional operation \u03d5(\u2022) to combine the concept node embedding and the relation embedding. Specifically, given the input subgraph G x = {V x , E x } and an R-GCN with L layers, we update the embedding of each node v \u2208 V x at the (l+1)-th layer by aggregating information from the embeddings of its neighbours in N (v) at the l-th layer:\no l v = 1 |N (v)| (u,v,r)\u2208E W l N \u03d5(h l u , h l r ), (1) h l+1 v = ReLU(o l v + W l S h l v ),(2)\nwhere h v and h r are node embedding and relation embedding. We define the compositional operation as \u03d5(h u , h r ) = h u \u2212h r inspired by the TransE (Bordes et al., 2013). The relation embedding is also updated via another linear transformation:\nh l+1 r = W l R h l r .(3)\nFinally, we obtain concept embedding h L v that encodes the sequence-associated subgraph context.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Concept selection on knowledge graph", "text": "Not all concepts in G appear in the outputs. Thus, we design a concept selection module to choose salient concepts that should be considered during generation. For each concept v \u2208 V x , we calculate its probability of being selected by taking a multilayer perception (MLP) on the top of graph encoder:\np v = P r[v is selected|x] = MLP(h L v\n). To supervise the concept selection process, we use the overlapping concepts between concepts appearing in the output sequence C y and concepts in input sequence associated subgraph G x , i.e., V x \u2229 C y , as a simple proxy for the ground-truth supervision. So, the concept selection loss (here only for one expert, see MoE loss in Eq.( 8)) is:\nL concept = \u2212 v\u2208Vx\u2229Cy v log p v (4) + v\u2208Vx\u2212Cy (1 \u2212 v) log(1 \u2212 p v ) .\nFinally, the top-N ranked concepts on the subgraph G x (denoted as v 1 , ..., v N ) are selected as the additional input to the generation process.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Concept-aware sequence generation", "text": "We utilize a standard Transformer (Vaswani et al., 2017) as our generation model. It takes the concatenation of the sequence x and all the selected concepts v 1 , ..., v N as input and auto-regressively generates the outputs y. We adopt the cross-entropy loss, which can be written as:\nL generation = \u2212 log p(y|x, v 1 , \u2022 \u2022 \u2022 , v N ) (5) = \u2212 |y| t=1 log p(y t |x, v 1 , \u2022 \u2022 \u2022 , v N , y <t ).\nNote that since the selected concepts do not have a rigorous order, we only apply positional encodings (used in Transformer) to the input sequence x.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Overall objective", "text": "We jointly optimizes the following loss:\nL = L generation + \u03bb \u2022 L concept . (6\n)\nwhere \u03bb is a hyperparameter to control the importance of different tasks 2 .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "MoE-Promoted Diverse Generation", "text": "To empower the generation model to produce multiple reasonable outputs, we employ a mixture of expert (MoE) module to model uncertainty and generate diverse outputs. While the MoE models have primarily been explored as a means of increasing model capacity, they are also being used to boost diverse generation process (Shen et al., 2019;Cho et al., 2019). Formally, the MoE module introduces a multinomial latent variable z \u2208 {1, \u2022 \u2022 \u2022 , K}, and decomposes the marginal likelihood as follows:\np(y|x, G x ) = K z=1 p(z|x, G x )p(y|z, x, G x ). (7)\nTraining. We minimize the loss function (in Eq.( 6)) using the MoE decomposition,\n\u2207 log p(y|x, G x ) (8) = K z=1 p(z|x, y, G x ) \u2022 \u2207 log p(y, z|x, G x ),\nand train the model with the EM algorithm (Dempster et al., 1977). Ideally, we would like different experts to specialize in different reasoning abilities so that they can generate diverse outputs. The specialization of experts means that given the input, only one element in {p(y, z|x, G x )} K z=1 should dominate in value (Shen et al., 2019). To encourage this, we employ a hard mixture model to maximize max z p(y, z|x, G x ) by assigning full responsibility to the expert with the largest joint probability. Training proceeds via hard-EM can be written as:\n\u2022 E-step: estimate the responsibilities of each\nexpert r z \u2190 1[z = arg max z p(y, z|x, G x )]\nusing the current parameters \u03b8; \u2022 M-step: update the parameters with gradients of the chosen expert (r z = 1) from E-step.\nExpert parameterization. Independently parameterizing each expert may exacerbate overfitting since the number of parameters increases linearly with the number of experts (Shen et al., 2019). We follow the parameter sharing schema in Cho et al. (2019); Shen et al. (2019) to avoid this issue. This only requires a negligible increase in parameters over the baseline model that does not uses MoE. In our experiments, we compared adding a unique expert embedding to each input token with adding an expert prefix token before the input text sequence, where they achieved very similar performance.\nProducing K outputs during inference. In order to generate K different outputs on test set, we follow Shen et al. (2019) to enumerate all latent variables z and then greedily decoding each token by\u0177 t = arg max p(y|\u0177 1:t\u22121 , z, x). In other words, we ask each expert to seek different sets of concepts on the knowledge graph, and use the selected concepts to generate K different outputs. Notably, this decoding procedure is efficient and easily parallelizable. Furthermore, to make fair comparisons with sampling-based methods, we use greedy decoding without any sampling strategy.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Experiments 4.1 Tasks and Datasets", "text": "Commonsense explanation generation. It aims to generate an explanation given a counterfactual statement for sense-making (Wang et al., 2019). We use the benchmark dataset ComVE from SemEval-2020 Task 4 (Wang et al., 2020). The dataset contains 10,000 / 997 / 1,000 examples for training / development / test sets, respectively. The average input/output length is 7. ", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Baseline Methods", "text": "We note that as we targeted at the one-to-many generation problem, we excluded those baseline methods mentioned in the related work that cannot produce multiple outputs, e.g., Zhang et al. (2020a); Ji et al. (2020); Liu et al. (2021). Different from aforementioned methods, our MoKGE can seek diverse reasoning on KG to encourage various generation outputs without any additional conditions.\nTo the best of our knowledge, we are the first work to explore diverse knowledge reasoning on commonsense KG to generate multiple diverse output sequences. Therefore, we only compared our MoKGE with existing diversity-promoting baselines without using knowledge graph. VAE-based method. The variational auto-encoder (VAE) (Kingma and Welling, 2014) is a deep generative latent variable model. VAE-based methods produce diverse outputs by sampling different latent variables from an approximate posterior distribution. CVAE-SVG (SVG is short for sentence variant generation) (Gupta et al., 2018) is a conditional VAE model that can produce multiple outputs based an original sentence as input. MoE-based method. Mixture models provide an alternative approach to generate diverse outputs by sampling different mixture components. We compare against two mixture of experts (MoE) implementations by Shen et al. (2019) and Cho et al. (2019). We refer them as MoE-prompt (Shen et al., 2019) and MoE-embed (Cho et al., 2019). Sampling-based method. Sampling methods create diverse outputs by sampling next token widely from the vocabulary. We compare against two sampling algorithms for decoding, including truncated sampling (Fan et al., 2018) and nucleus sampling . Truncated sampling (Fan et al., 2018) randomly samples words from top-k probability candidates of the predicted distribution at each decoding step. Nucleus sampling  avoids text degeneration by truncating the unreliable tails and sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Implementation Details", "text": "All baseline methods were built on the Transformer architecture with 6-layer encoder and decoder, and initialized with pre-trained parameters from BARTbase (Lewis et al., 2020), which is one of the stateof-the-art pre-trained Transformer models for natural language generation (Gehrmann et al., 2021). In our MoKGE, the Transformer parameters were also initialized by BART-base, in order to make fair comparison with all baseline methods. The R-GCN parameters were random initialized.\nFor model training, we used Adam with batch size of 60, learning rate of 3e-5, L2 weight decay of 0.01, learning rate warm up over the first 10,000 steps, and linear decay of learning rate. Our models were trained by one Tesla V100 GPU card with 32GB memory, and implemented on PyTorch with the Huggingface's Transformer (Wolf et al., 2020). All Transformer-based methods were trained with 30 epochs, taken about 4-5 hours on the ComVE dataset and 7-9 hours on the \u03b1-NLG dataset.\nIn addition to our MoKGE implementation, we also provide the baseline implementation code on GitHub https://github.com/DM2-ND/MoKGE.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Automatic Evaluation", "text": "We evaluated the performance of different generation models from two aspects: quality (or say accuracy) and diversity. Quality tests the appropriateness of the generated response with respect to the context, and diversity tests the lexical and semantic diversity of the appropriate sequences generated by the model. These evaluation metrics have been widely used in existing work (Ott et al., 2018;Vijayakumar et al., 2018;Cho et al., 2019;.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Quality metrics (\u21d1).", "text": "The quality is measured by standard N-gram based metrics, including the BLEU score (Papineni et al., 2002) and the ROUGE score (Lin, 2004). This measures the highest accuracy comparing the best hypothesis among the top-K with the target (Vijayakumar et al., 2018). Concretely, we generate hypotheses {\u0176 (1) , \u2022 \u2022 \u2022\u0176 (K) } from each source X and keep the hypothesis\u0176 best that achieves the best sentencelevel metric with the target Y . Then we calculate a corpus-level metric with the greedily-selected hypotheses {Y (i),best } N i=1 and references {Y (i) } N i=1 . The diversity of evaluated by three aspects: concept, pairwise and corpus diversity.\nConcept diversity. The number of unique concepts (short as Uni.C) measures how many unique concepts on the commonsense KG are covered in the generated outputs. A higher value indicates the higher concept diversity. Besides, we also measure the pairwise concept diversity by using Jaccard similarity. It is defined as the size of the intersection divided by the size of the union of two sets. Lower value indicates the higher concept diversity.\nPairwise diversity (\u21d3). Referred as \"self-\" (e.g., self-BLEU) , it measures the within-distribution similarity. This metric computes the average of sentence-level metrics between all pairwise combinations of hypotheses {Y (1) , \u2022 \u2022 \u2022 , Y (K) } generated from each source sequence X. Lower pairwise metric indicates high diversity between generated hypotheses.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Corpus diversity (\u21d1)", "text": ". Distinct-k (Li et al., 2016) measures the total number of unique k-grams normalized by the total number of generated k-gram tokens to avoid favoring long sentences. Entropyk  reflects how evenly the empirical k-gram distribution is for a given sentence when word frequency is considered.    ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experimental results", "text": "Comparison with baseline methods. We evaluated our proposed MoKGE and baseline methods based on both quality and diversity. As shown in Table 2, MoE-based methods achieved the best performance among all baseline methods. MoKGE can further boost diversity by at least 1.57% and 1.83% on Self-BLEU-3 and Self-BLEU-4, compared with the vanilla MoE methods. At the same time, MoKGE achieved on par performance with other baseline methods based on the quality evaluation. Specifically, on the ComVE dataset, MoKGE achieved the best performance on BLEU-4 and ROUGE-L, and on the \u03b1-NLG dataset, the perfor-mance gap between MoKGE and the best baseline method was always less than 0.5% on BLEU-4.\nAblation study. We conducted an ablation study to analyze the two major components in the MoKGE.\nThe experimental results are shown in Table 3. First, we note that when not using MoE (line -w/o MoE), we used the most basic decoding strategy -beam search -to generate multiple outputs. We observed that the outputs generated by beam search differed only on punctuation and minor morphological variations, and typically only the last few words were different from others. Besides, integrating commonsense knowledge graph into the MoEbased generation model brought both quality and   diversity improvement on the ComVE, but might sacrifice a little quality (less than 0.5% on BLEU-4) on the \u03b1-NLG dataset. Overall, our MoKGE benefited from KG and MoE modules, and achieved great performance on both diversity and quality.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Human Evaluation", "text": "Automatic diversity evaluation (e.g., Self-BLEU, Distinct-k) cannot reflect the content-level diversity. Therefore, we conducted extensive human evaluations to assess both the quality and diversity of outputs generated from different models. The human evaluation was divided into two parts: independent scoring and pairwise comparisons. All evaluations were conducted on Amazon Mechanical Turk (AMT), and each evaluation form was answered by at least three AMT workers. Independent scoring. In this part, human annotators were asked to evaluate the generated outputs from a single model. We first presented top-3 generated outputs from a certain model to human annotators. The annotators would first evaluate the diversity by answering \"How many different meanings do three outputs express?\" Then we presented human-written outputs to the annotators. The annotator would evaluate the quality by comparing machine generated outputs and human-written outputs, and answering \"How many machine generated out-puts are correct?\" The diversity and quality scores are normalized to the range from 0 to 3. Besides, the annotators need to give a fluency and grammar score from 1 to 4 for each generated output.\nPairwise comparisons. In this part, the annotators were given two sets of top-3 generated explanations from two different methods each time and instructed to pick the more diverse set. The choices are \"win,\" \"lose,\" or \"tie.\"\nAs shown in Table 4-5, our MoKGE can significantly outperform the state-of-the-art samplingbased methods in diversity evaluation (p-value < 0.05 under paired t-test), even slightly better than human performance on the ComVE task. At the same time, we can observe MoKGE is able to obtain on par performance with other methods based on quality evaluation. The p-value is not smaller than 0.05 (i.e., not significant difference) under paired t-test between MoKGE and baseline methods based on the quality evaluation. (1) Billy's parents took him to the zoo as a reward.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Case Study", "text": "(2) Billy wanted to go to the zoo. He saw elephants.\n(3) Billy went to the store and bought an elephant.\n(1) Billy's parents sent him on an African safari for a reward.\n(2) He went to the zoo later in the day and saw elephants. (1) Billy wanted to go to the zoo and see elephants.\n(2) Billy was excited to go on his trip to the zoo.\n(3) Billy went to the zoo to see the animals.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Nucleus sampling", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Human references big", "text": "[4] [4] [1] [1] [4] [3] [4] [4] [1] [1] [1] [2] [1]\nComVE --Input: Cars are made of fuel. Goal (explanation for sense-making): [ ].\n(1) Cars are not made of fuel.\n(2) Cars burn fuel to produce energy and work.\n(3) Fuel is a liquid which cannot make cars. meanings, e.g., \"go to the zoo and see elephants\" and \"took him to the zoo and see elephants\" in the \u03b1-NLG case. On the contrary, MoKGE can generate semantically richer and more diverse contents than the other two methods by incorporating more commonsense concepts on the knowledge graph.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "MoKGE (ours)", "text": "Nucleus", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Future Directions", "text": "Improving content diversity in NLG. Most of the existing diversity-promoting work has focused on improving syntactic and lexical diversity, such as different language style in machine translation (Shen et al., 2019) and word variability in paraphrase generation (Gupta et al., 2018). Nevertheless, methods for improving content diversity in NLG systems have been rarely studied in the existing literature. We believe that generating diverse content is one of the most promising aspects of machine intelligence, which can be applied to a wide range of real-world applications, not only limited to commonsense reasoning.\nBesides, leveraging knowledge graph is not the only way to promote content diversity as it is a highly knowledge-intensive task. Many existing knowledge-enhanced methods (Yu et al., 2022c) can be used to acquire different external knowledge for producing diverse outputs, e.g., taking different retrieved documents as conditions for generator.\nDesigning neural diversity metrics. In spite of growing interest in NLG models that produce diverse outputs, there is currently no principled neu-ral method for evaluating the diversity of an NLG system. As described in Tevet and Berant (2021), existing automatic diversity metrics (e.g. Self-BLEU) perform worse than humans on the task of estimating content diversity, indicating a low correlation between metrics and human judgments.\nTherefore, neural-based diversity metrics are highly demanded. Intuitively, the metrics should include computational comparisons of multiple references and hypotheses by projecting them into the same semantic space, unlike metrics for evaluating the generation quality, e.g., BERTScore (Zhang et al., 2020b) and BLEURT (Sellam et al., 2020), which only measures the correlation between a pair of reference and hypothesis.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Conclusions", "text": "In this paper, we proposed a novel method that diversified the generative reasoning by a mixture of expert strategy on commonsense knowledge graph. To the best of our knowledge, this is the first work to boost diversity in NLG by diversifying knowledge reasoning on commonsense knowledge graph. Experiments on two generative commonsense reasoning benchmarks demonstrated that MoKGE outperformed state-of-the-art methods on diversity, while achieving on par performance on quality.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "The work is supported by NSF IIS-1849816, CCF-1901059, IIS-2119531 and IIS-2142827.   ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Fuel is not a vehicle material. (2) Fuel is not used to make cars. They use gasoline. (3) Cars are not made of fuel", "journal": "", "year": "", "authors": ""}, {"title": "Cars are made of metal. but not fuel. (2) Cars are made of aluminum, not made by fuel. (3) Fuel is used to make cars more efficient", "journal": "", "year": "", "authors": ""}, {"title": "Cars are made of rubber. Fuel is not used to make cars", "journal": "", "year": "", "authors": ""}, {"title": "Cars are made of aluminum, which is not fuel", "journal": "", "year": "", "authors": ""}, {"title": "Cars are powered by electric motors and not by fuel. (1) Billy went to the zoo to see the animals", "journal": "", "year": "", "authors": ""}, {"title": "Billy was excited to go to the zoo with his friends", "journal": "", "year": "", "authors": ""}, {"title": "Billy's parents took him to the zoo to see elephants", "journal": "", "year": "", "authors": ""}, {"title": "", "journal": "", "year": "", "authors": "( Moe;  Shen"}, {"title": "Abductive commonsense reasoning", "journal": "", "year": "2020", "authors": "Chandra References; Ronan Bhagavatula; Chaitanya Le Bras; Keisuke Malaviya; Ari Sakaguchi; Hannah Holtzman; Doug Rashkin; Scott Downey; Yih Wen-Tau; Yejin Choi"}, {"title": "Translating embeddings for modeling multirelational data", "journal": "", "year": "2013", "authors": "Antoine Bordes; Nicolas Usunier; Alberto Garcia-Duran; Jason Weston; Oksana Yakhnenko"}, {"title": "Mixture content selection for diverse sequence generation", "journal": "", "year": "2019", "authors": "Jaemin Cho; Minjoon Seo; Hannaneh Hajishirzi"}, {"title": "Maximum likelihood from incomplete data via the em algorithm", "journal": "In Journal of the Royal Statistical Society (Methodological", "year": "1977", "authors": "P Arthur; Nan M Dempster; Donald B Laird;  Rubin"}, {"title": "Injecting entity types into entity-guided text generation", "journal": "", "year": "2021", "authors": "Xiangyu Dong; Wenhao Yu; Chenguang Zhu; Meng Jiang"}, {"title": "Multitalk: A highly-branching dialog testbed for diverse conversations", "journal": "", "year": "2021", "authors": "Yao Dou; Maxwell Forbes; Ari Holtzman; Yejin Choi"}, {"title": "Hierarchical neural story generation", "journal": "", "year": "2018", "authors": "Angela Fan; Mike Lewis; Yann Dauphin"}, {"title": "The gem benchmark: Natural language generation, its evaluation and metrics", "journal": "", "year": "2021", "authors": "Sebastian Gehrmann; Tosin Adewumi; Karmanya Aggarwal; Pawan Sasanka Ammanamanchi; Anuoluwapo Aremu; Antoine Bosselut; Miruna-Adriana Khyathi Raghavi Chandu; Dipanjan Clinciu; Kaustubh Das;  Dhole"}, {"title": "A knowledge-enhanced pretraining model for commonsense story generation", "journal": "", "year": "2020", "authors": "Jian Guan; Fei Huang; Zhihao Zhao; Xiaoyan Zhu; Minlie Huang"}, {"title": "A deep generative framework for paraphrase generation", "journal": "", "year": "2018", "authors": "Ankush Gupta; Arvind Agarwal; Prawaan Singh; Piyush Rai"}, {"title": "The curious case of neural text degeneration", "journal": "", "year": "2020", "authors": "Ari Holtzman; Jan Buys; Li Du; Maxwell Forbes; Yejin Choi"}, {"title": "Language generation with multi-hop reasoning on commonsense knowledge graph", "journal": "EMNLP", "year": "2020", "authors": "Haozhe Ji; Pei Ke; Shaohan Huang; Furu Wei; Xiaoyan Zhu; Minlie Huang"}, {"title": "The role of\" condition\" a novel scientific knowledge graph representation and construction model", "journal": "", "year": "2019", "authors": "Tianwen Jiang; Tong Zhao; Bing Qin; Ting Liu; V Nitesh; Meng Chawla;  Jiang"}, {"title": "Autoencoding variational bayes", "journal": "", "year": "2014", "authors": "P Diederik; Max Kingma;  Welling"}, {"title": "Target conditioning for one-to-many generation", "journal": "", "year": "2020", "authors": "Marie-Anne Lachaux; Armand Joulin; Guillaume Lample"}, {"title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2020", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Veselin Stoyanov; Luke Zettlemoyer"}, {"title": "A diversity-promoting objective function for neural conversation models", "journal": "", "year": "2016", "authors": "Jiwei Li; Michel Galley; Chris Brockett; Jianfeng Gao; Bill Dolan"}, {"title": "Rouge: A package for automatic evaluation of summaries", "journal": "", "year": "2004", "authors": "Chin-Yew Lin"}, {"title": "Kg-bart: Knowledge graph-augmented bart for generative commonsense reasoning", "journal": "", "year": "2021", "authors": "Ye Liu; Yao Wan; Lifang He; Hao Peng; Philip S Yu"}, {"title": "Analyzing uncertainty in neural machine translation", "journal": "", "year": "2018", "authors": "Myle Ott; Michael Auli; David Grangier; Marc'aurelio Ranzato"}, {"title": "Bleu: a method for automatic evaluation of machine translation", "journal": "", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"title": "Modeling relational data with graph convolutional networks", "journal": "", "year": "2018", "authors": "Michael Schlichtkrull; N Thomas; Peter Kipf; Rianne Bloem;  Van Den; Ivan Berg; Max Titov;  Welling"}, {"title": "Bleurt: Learning robust metrics for text generation", "journal": "", "year": "2020", "authors": "Thibault Sellam; Dipanjan Das; Ankur Parikh"}, {"title": "Mixture models for diverse machine translation: Tricks of the trade", "journal": "", "year": "2019", "authors": "Tianxiao Shen; Myle Ott; Michael Auli; Marc'aurelio Ranzato"}, {"title": "Diversified query generation guided by knowledge graph", "journal": "", "year": "2022", "authors": "Xinyao Shen; Jiangjie Chen; Jiaze Chen; Chun Zeng; Yanghua Xiao"}, {"title": "Toward diverse text generation with inverse reinforcement learning", "journal": "", "year": "2018", "authors": "Zhan Shi; Xinchi Chen; Xipeng Qiu; Xuanjing Huang"}, {"title": "Evaluating the evaluation of diversity in natural language generation", "journal": "", "year": "2021", "authors": "Guy Tevet; Jonathan Berant"}, {"title": "Composition-based multirelational graph convolutional networks", "journal": "", "year": "2020", "authors": "Shikhar Vashishth; Soumya Sanyal; Vikram Nitin; Partha Talukdar"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Diverse beam search for improved description of complex scenes", "journal": "", "year": "2018", "authors": "K Ashwin; Michael Vijayakumar;  Cogswell; R Ramprasaath; Qing Selvaraju; Stefan Sun; David Lee; Dhruv Crandall;  Batra"}, {"title": "Semeval-2020 task 4: Commonsense validation and explanation", "journal": "", "year": "2020", "authors": "Cunxiang Wang; Shuailong Liang; Yili Jin; Yilong Wang; Xiaodan Zhu; Yue Zhang"}, {"title": "Does it make sense? and why? a pilot study for sense making and explanation", "journal": "", "year": "2019", "authors": "Cunxiang Wang; Shuailong Liang; Yue Zhang; Xiaonan Li; Tian Gao"}, {"title": "Transformers: State-of-theart natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf"}, {"title": "Diverse and informative dialogue generation with context-specific commonsense knowledge awareness", "journal": "", "year": "2020", "authors": "Sixing Wu; Ying Li; Dawei Zhang; Yang Zhou; Zhonghai Wu"}, {"title": "Kg-fid: Infusing knowledge graph in fusion-in-decoder for opendomain question answering", "journal": "", "year": "2022", "authors": "Donghan Yu; Chenguang Zhu; Yuwei Fang; Wenhao Yu; Shuohang Wang; Yichong Xu; Xiang Ren; Yiming Yang; Michael Zeng"}, {"title": "Dict-bert: Enhancing language model pre-training with dictionary", "journal": "", "year": "2022", "authors": "Wenhao Yu; Chenguang Zhu; Yuwei Fang; Donghan Yu; Shuohang Wang; Yichong Xu; Michael Zeng; Meng Jiang"}, {"title": "A survey of knowledge-enhanced text generation", "journal": "", "year": "2022", "authors": "Wenhao Yu; Chenguang Zhu; Zaitang Li; Zhiting Hu; Qingyun Wang; Ji Heng; Meng Jiang"}, {"title": "Sentence-permuted paragraph generation", "journal": "", "year": "2021", "authors": "Wenhao Yu; Chenguang Zhu; Tong Zhao; Zhichun Guo; Meng Jiang"}, {"title": "Enhancing taxonomy completion with concept generation via fusing relational representations", "journal": "", "year": "2021", "authors": "Qingkai Zeng; Jinfeng Lin; Wenhao Yu; Jane Cleland-Huang; Meng Jiang"}, {"title": "Grounded conversation generation as guided traverses in commonsense knowledge graphs", "journal": "", "year": "2020", "authors": "Houyu Zhang; Zhenghao Liu; Chenyan Xiong; Zhiyuan Liu"}, {"title": "Bertscore: Evaluating text generation with bert", "journal": "", "year": "2020", "authors": "Tianyi Zhang; Varsha Kishore; Felix Wu; Q Kilian; Yoav Weinberger;  Artzi"}, {"title": "Generating informative and diverse conversational responses via adversarial information maximization", "journal": "", "year": "2018", "authors": "Yizhe Zhang; Michel Galley; Jianfeng Gao; Zhe Gan; Xiujun Li; Chris Brockett; Bill Dolan"}, {"title": "Commonsense knowledge aware conversation generation with graph attention", "journal": "", "year": "2018", "authors": "Hao Zhou; Tom Young; Minlie Huang; Haizhou Zhao; Jingfang Xu; Xiaoyan Zhu"}, {"title": "Texygen: A benchmarking platform for text generation models", "journal": "", "year": "2018", "authors": "Yaoming Zhu; Sidi Lu; Lei Zheng; Jiaxian Guo; Weinan Zhang; Jun Wang; Yong Yu"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": ": UsedFor [2]: PartOf [3]: IsA [4]: RelatedTo [1]", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "( 2 )2Piano is a musical instrument used in songs to produce different musical tones .(3) Piano is a kind of art form .", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 1 :1Figure 1: An example of diverse commonsense explanation generation. It aims at generating multiple reasonable explanations given a counterfactual statement. Relevant concepts on the commonsense KG (in shade) can help to perform diverse knowledge reasoning.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 2 :2Figure 2: The overall architecture of MoKGE. The MoKGE consists of four steps: (S1) the model constructs a sequence-associated subgraph from the commonsense KG; (S2) a relational-GCN iteratively updates the representation of a concept node by aggregating information from its neighboring nodes and edges; (S3) each knowledge expert selects different salient concepts that should be considered during generation; (S4) the model generates the outputs by integrating the token embeddings of the input sequence and the top-ranked entities.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_4", "figure_caption": "7 / 9.0 words. All examples in the dataset have 3 references. Abductive commonsense reasoning. It is also referred as \u03b1-NLG. It is the task of generating a valid hypothesis about the likely explanations to partially observable past and future. We use the ART benchmark dataset (Bhagavatula et al., 2020) that consists of 50,481 / 1,779 / 3,560 examples for training / development / test sets. The average input/output length is 17.4 / 10.8 words. Each example in the ART dataset has 1 to 5 references.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_6", "figure_caption": "SB-3/4: Self-BLEU-3/4 (\u21d3), D-2: Distinct-2 (\u21d1), E-4: Entropy-4 (\u21d1), B-4: BLEU-4 (\u21d1), R-L: ROUGE-L (\u21d1)", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_7", "figure_caption": "Figure 33Figure3demonstrates human-written explanations and generated explanations from different diversitypromoting methods, including nucleus sampling, mixture of experts (MoE) and our MoKGE. Overall, we observed that the nucleus sampling and MoE methods typically expressed very similar", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_8", "figure_caption": "( 3 )3His mother stopped by the store and bought him a stuffed elephant.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_9", "figure_caption": "Figure 3 :3Figure 3: Case studies. MoKGE can produce diverse knowledge reasoning on commonsense KG, select different relevant concepts (in shades of different colors), then generate diverse outputs. The outputs diversity of MoKGE is significantly better than that of beam search and nucleus sampling, and close to human performance.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Under human evaluation, the performance of existing diversity promoting methods is still far from that of humans. Our method MoKGE can exceed the human performance on the ComVE task.", "figure_data": "ComVE \u03b1-NLGAvg. # human references3.004.20Avg. # meanings (\u21d1)Human references2.603.79Nucleus sampling2.153.35MoKGE (our method)2.633.72"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "Concept Selection (S3)Top-ranked conceptsExpert 1entertainmentsoccersource conceptsinstrumentexercisePiano is \u2026 sport music press \u2026KGlocate subKG(S1)piano playYou can produce music when pressing \u2026 Transformer (S4)artentertainmentsoccermusicsportTop-ranked conceptsExpert 2musicpianoplaypianistoccupationactionactionsportPiano is \u2026 sport art form \u2026songGNN Encoder (S2) press instrumentkindartformkindpressPiano is a kind of art form . Transformer (S4)"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Diversity and quality evaluation on the ComVE (upper part) and \u03b1-NLG (lower part) datasets. Each model is required to generate three outputs. All experiments are run three times with different random seeds, and the average results on the test set is calculated as the final performance, with standard deviations as subscripts.", "figure_data": "MethodsModel VariantConcept diversity #Uni.C(\u21d1) Jaccard (\u21d3) SB-3 (\u21d3) SB-4 (\u21d3) D-2(\u21d1) E-4(\u21d1) B-4 (\u21d1) R-L (\u21d1) Pairwise diversity Corpus diversity Qualityz = 164.56 0.164.74 0.366.66 0.462.83 0.533.75 0.5 9.13 0.1 16.67 0.3 41.52 0.3CVAEz = 325.03 0.347.27 0.859.20 1.354.30 1.532.86 1.1 9.07 0.5 17.04 0.2 42.17 0.5z = 644.67 0.054.69 0.855.02 0.849.58 1.032.55 0.5 9.07 0.2 15.54 0.4 41.03 0.3Truncated samplingk = 5 k = 20 k = 504.37 0.0 4.60 0.0 4.68 0.171.38 0.7 63.42 1.2 60.98 1.874.20 0.2 64.47 2.1 61.39 2.471.38 0.2 60.33 2.4 56.93 2.831.32 0.4 9.18 0.1 16.44 0.2 40.99 0.2 33.69 0.6 9.26 0.1 17.70 0.2 42.58 0.5 34.80 0.3 9.29 0.1 17.48 0.4 42.44 0.5Nucleus samplingp = .5 p = .75 p = .954.19 0.1 4.41 0.1 4.70 0.172.78 1.0 67.01 1.7 61.92 2.677.66 0.8 71.41 2.5 63.43 3.475.14 0.9 68.22 2.9 59.23 3.828.36 0.6 9.05 0.3 16.09 0.6 40.95 0.5 31.21 0.3 9.16 0.1 17.07 0.5 41.88 0.7 34.17 0.3 9.27 0.2 17.68 0.4 42.60 0.8MoEembed prompt5.41 0.0 5.45 0.247.55 0.5 47.54 0.433.64 0.2 33.42 0.328.21 0.1 28.40 0.346.57 0.2 9.61 0.1 18.66 0.5 43.72 0.2 46.93 0.2 9.60 0.2 18.91 0.4 43.71 0.5MoKGEembed5.35 0.248.18 0.535.36 1.129.71 1.247.51 0.4 9.63 0.1 19.13 0.1 43.70 0.1(ours)prompt5.48 0.244.37 0.430.93 0.925.30 1.148.44 0.2 9.67 0.2 19.01 0.1 43.83 0.3"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Ablation studies. When not suing MoE (line -w/o MoE), we set beam as three to generate three outputs. MoKGE 25.30 1.1 48.44 0.2 9.67 0.2 19.01 0.1 43.83 0.3 22.43 2.4 38.01 0.6 10.88 0.2 14.17 0.2 38.82 0.7 \u22a2 w/o KG 28.40 0.3 46.93 0.2 9.60 0.2 18.91 0.4 43.71 0.5 23.18 1.9 36.71 0.1 10.85 0.0 14.26 0.3 38.78 0.4", "figure_data": "ComVE (left part: diversity; right part: quality)\u03b1-NLG (left part: diversity; right part: quality)MethodsSB-4 (\u21d3) D-2 (\u21d1) E-4 (\u21d1) B-4 (\u21d1) R-L (\u21d1) SB-4 (\u21d3) D-2 (\u21d1) E-4 (\u21d1) B-4 (\u21d1) R-L (\u21d1)\u22a2 w/o MoE74.15 0.2 31.92 0.1 9.14 0.0 15.87 0.1 40.24 0.2 77.34 0.2 19.19 0.1 10.10 0.0 12.84 0.1 37.52 0.2"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Human evaluations by independent scoring based on diveristy, quality, flency and grammar. In addition, * indicates p-value < 0.05 under paired t-test between MoKGE and baseline methods.", "figure_data": "MethodsComVE\u03b1-NLGDiversityQualityFlu. & Gra.DiversityQualityFlu. & Gra.Truncated samp.2.15\u00b10.762.22\u00b11.013.47\u00b10.752.31\u00b10.762.63\u00b10.773.89\u00b10.36Nucleus samp.2.03\u00b10.732.29\u00b11.033.52\u00b10.702.39\u00b10.732.67\u00b10.723.91\u00b10.28MoKGE (ours)2.63\u00b10.51*2.10\u00b10.993.46\u00b10.812.66\u00b10.51*2.57\u00b10.713.87\u00b10.34Human Ref.2.60\u00b10.593.004.002.71\u00b10.573.004.00"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Human evaluations by pairwise comparison: MoKGE v.s. two baseline methods based on diversity. Truncated samp. 47.85\u00b15.94 37.09\u00b14.56 15.06\u00b13.31 45.35\u00b15.06 43.19\u00b12.78 11.46\u00b12.31 v.s. Nucleus samp. 54.30\u00b14.62 36.02\u00b12.74 9.68\u00b13.48 41.53\u00b11.55 46.99\u00b12.04 11.48\u00b12.36", "figure_data": "Against methodsComVE\u03b1-NLGWin (%)Tie (%)Lose (%)Win (%)Tie (%)Lose (%)v.s."}], "doi": ""}