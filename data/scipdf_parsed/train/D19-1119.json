{"authors": "Shun Kiyono; Jun Suzuki; Masato Mita; Tomoya Mizumoto; Kentaro Inui", "pub_date": "", "title": "An Empirical Study of Incorporating Pseudo Data into Grammatical Error Correction", "abstract": "The incorporation of pseudo data in the training of grammatical error correction models has been one of the main factors in improving the performance of such models. However, consensus is lacking on experimental configurations, namely, choosing how the pseudo data should be generated or used. In this study, these choices are investigated through extensive experiments, and state-of-the-art performance is achieved on the CoNLL-2014 test set (F 0.5 = 65.0) and the official test set of the BEA-2019 shared task (F 0.5 = 70.2) without making any modifications to the model architecture.", "sections": [{"heading": "Introduction", "text": "To date, many studies have tackled grammatical error correction (GEC) as a machine translation (MT) task, in which ungrammatical sentences are regarded as the source language and grammatical sentences are regarded as the target language. This approach allows cutting-edge neural MT models to be adopted. For example, the encoder-decoder (EncDec) model (Sutskever et al., 2014;Bahdanau et al., 2015), which was originally proposed for MT, has been applied widely to GEC and has achieved remarkable results in the GEC research field (Ji et al., 2017;Chollampatt and Ng, 2018;.\nHowever, a challenge in applying EncDec to GEC is that EncDec requires a large amount of training data (Koehn and Knowles, 2017), but the largest set of publicly available parallel data  in GEC has only two million sentence pairs (Mizumoto et al., 2011). Consequently, the method of augmenting the data by incorporating pseudo training data has been studied intensively (Xie et al., 2018;Ge et al., 2018;Lichtarge et al., 2019;Zhao et al., 2019). * Current affiliation: Future Corporation When incorporating pseudo data, several decisions must be made about the experimental configurations, namely, (i) the method of generating the pseudo data, (ii) the seed corpus for the pseudo data, and (iii) the optimization setting (Section 2). However, consensus on these decisions in the GEC research field is yet to be formulated. For example, Xie et al. (2018) found that a variant of the backtranslation (Sennrich et al., 2016b) method (BACKTRANS (NOISY)) outperforms the generation of pseudo data from raw grammatical sentences (DIRECTNOISE). By contrast, the current state of the art model (Zhao et al., 2019) uses the DIRECTNOISE method.\nIn this study, we investigate these decisions regarding pseudo data, our goal being to provide the research community with an improved understanding of the incorporation of pseudo data. Through extensive experiments, we determine suitable settings for GEC. We justify the reliability of the proposed settings by demonstrating their strong performance on benchmark datasets. Specifically, without any task-specific techniques or architecture, our model outperforms not only all previous single-model results but also all ensemble results except for the ensemble result by Grundkiewicz et al. (2019) 1 . By applying task-specific techniques, we further improve the performance and achieve state-of-the-art performance on the CoNLL-2014 test set and the official test set of the BEA-2019 shared task.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Problem Formulation and Notation", "text": "In this section, we formally define the GEC task discussed in this paper. Let D be the GEC training data that comprise pairs of an ungrammatical source sentence X and grammatical target sentence\nY , i.e., D = {(X n , Y n )} n .\nHere, |D| denotes the number of sentence pairs in the dataset D.\nLet \u0398 represent all trainable parameters of the model. Our objective is to find the optimal parameter set \u0398 that minimizes the following objective function L(D, \u0398) for the given training data D:\nL(D, \u0398) = \u2212 1 |D| (X,Y )\u2208D log(p(Y |X, \u0398)), (1\n)\nwhere p(Y |X, \u0398) denotes the conditional probability of Y given X.\nIn the standard supervised learning setting, the parallel data D comprise only \"genuine\" parallel data D g (i.e., D = D g ). However, in GEC, incorporating pseudo data D p that are generated from grammatical sentences Y \u2208 T , where T represents seed corpus (i.e., a set of grammatical sentences), is common (Xie et al., 2018;Zhao et al., 2019;Grundkiewicz et al., 2019).\nOur interest lies in the following three nontrivial aspects of Equation 1. Aspect (i): multiple methods for generating pseudo data D p are available (Section 3). Aspect (ii): options for the seed corpus T are numerous. To the best of our knowledge, how the seed corpus domain affects the model performance is yet to be shown. We compare three corpora, namely, Wikipedia, Simple Wikipedia (SimpleWiki) and English Gigaword, as a first trial. Wikipedia and SimpleWiki have similar domains, but different grammatical complexities. Therefore, we can investigate how grammatical complexity affects model performance by comparing these two corpora. We assume that Gigaword contains the smallest amount of noise among the three corpora. We can therefore use Gigaword to investigate whether clean text improves model performance. Aspect (iii): at least two major settings for incorporating D p into the optimization of Equation 1 are available. One is to use the two datasets jointly by concatenating them as D = D g \u222a D p , which hereinafter we refer to as JOINT. The other is to use D p for pretraining, namely, minimizing L(D p , \u0398) to acquire \u0398 , and then fine-tuning the model by minimizing L(D g , \u0398 ); hereinafter, we refer to this setting as PRETRAIN. We investigate these aspects through our extensive experiments (Section 4).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Methods for Generating Pseudo Data", "text": "In this section, we describe three methods for generating pseudo data. In Section 4, we experimentally compare these methods.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "BACKTRANS (NOISY) and BACKTRANS (SAM-PLE)", "text": "Backtranslation for the EncDec model was proposed originally by Sennrich et al. (2016b). In backtranslation, a reverse model, which generates an ungrammatical sentence from a given grammatical sentence, is trained. The output of the reverse model is paired with the input and then used as pseudo data.\nBACKTRANS (NOISY) is a variant of backtranslation that was proposed by Xie et al. (2018) 2 . This method adds r\u03b2 random to the score of each hypothesis in the beam for every time step. Here, noise r is sampled uniformly from the interval [0, 1], and \u03b2 random \u2208 R \u22650 is a hyper-parameter that controls the noise scale. If we set \u03b2 random = 0, then BACK-TRANS (NOISY) is identical to standard backtranslation.\nBACKTRANS (SAMPLE) is another variant of backtranslation, which was proposed by Edunov et al. (2018) for MT. In BACKTRANS (SAMPLE), sentences are decoded by sampling from the distribution of the reverse model. DIRECTNOISE Whereas BACKTRANS (NOISY) and BACKTRANS (SAMPLE) generate ungrammatical sentences with a reverse model, DIRECT-NOISE injects noise \"directly\" into grammatical sentences (Edunov et al., 2018;Zhao et al., 2019). Specifically, for each token in the given sentence, this method probabilistically chooses one of the following operations: (i) masking with a placeholder token mask , (ii) deletion, (iii) insertion of a random token, and (iv) keeping the original 3 . For each token, the choice is made based on the categorical distribution (\u00b5 mask , \u00b5 deletion , \u00b5 insertion , \u00b5 keep ).", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Experiments", "text": "The goal of our experiments is to investigate aspect (i)-(iii) introduced in Section 2. To ensure that the experimental findings are applicable to GEC in general, we design our experiments by using the following two strategies: (i) we use an off-the-shelf EncDec model without any task-specific architecture or techniques; (ii) we conduct hyper-parameter tuning, evaluation and comparison of each method or setting on the validation set. At the end of experiments (Section 4.5), we summarize our findings and propose suitable settings. We then perform a single-shot evaluation of their performance on the test set.  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Configurations Dataset", "text": "The BEA-2019 workshop official dataset 4 is the origin of the training and validation data of our experiments. Hereinafter, we refer to the training data as BEA-train. We create validation data (BEA-valid) by randomly sampling sentence pairs from the official validation split 5 .\nAs a seed corpus T , we use SimpleWiki 6 , Wikipedia 7 or Gigaword 8 . We apply the noizing methods described in Section 3 to each corpus and generate pseudo data D p . The characteristics of each dataset are summarized in Table 1. Evaluation We report results on BEA-valid, the official test set of the BEA-2019 shared task (BEA-test), the CoNLL-2014 test set (CoNLL-2014) (Ng et al., 2014), and the JFLEG test set (JFLEG) (Napoles et al., 2017). All reported results (except ensemble) are the average of five distinct trials using five different random seeds. We report the scores measured by ERRANT (Bryant et al., 2017;Felice et al., 2016) for BEA-valid, BEA-test, and CoNLL-2014. As the reference sentences of BEAtest are publicly unavailable, we evaluate the model outputs on CodaLab 9 for BEA-test. We also report results measured by the M 2 scorer (Dahlmeier and Ng, 2012) on CoNLL-2014 to compare them with those of previous studies. We use the GLEU metric (Napoles et al., 2015(Napoles et al., , 2016 for JFLEG. Model We adopt the Transformer EncDec model (Vaswani et al., 2017) using the fairseq toolkit (Ott et al., 2019) and use the \"Transformer (big)\" settings of Vaswani et al. (2017). Optimization For the JOINT setting, we opti-  mize the model with Adam (Kingma and Ba, 2015).\nFor the PRETRAIN setting, we pretrain the model with Adam and then fine-tune it on BEA-train using Adafactor (Shazeer and Stern, 2018) 10 .", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Aspect (i): Pseudo Data Generation", "text": "We compare the effectiveness of the BACK-TRANS (NOISY), BACKTRANS (SAMPLE), and DIRECTNOISE methods for generating pseudo data.\nIn DIRECTNOISE, we set (\u00b5 mask , \u00b5 deletion , \u00b5 insertion , \u00b5 keep ) = (0.5, 0.15, 0.15, 0.2) 11 . We use \u03b2 random = 6 for BACKTRANS (NOISY) 12 . In addition, we use (i) the JOINT setting and (ii) all of SimpleWiki as the seed corpus T throughout this section.\nThe results are summarized in Table 2. BACK-TRANS (NOISY) and BACKTRANS (SAMPLE) show competitive values of F 0.5 . Given this result, we exclusively use BACKTRANS (NOISY) and discard BACKTRANS (SAMPLE) for the rest of the experiments. The advantage of BACKTRANS (NOISY) is that its effectiveness in GEC has already been demonstrated by Xie et al. (2018). In addition, in our preliminary experiment, BACKTRANS (NOISY) decoded ungrammatical sentence 1.2 times faster than BACKTRANS (SAMPLE) did. We also use DI-RECTNOISE because it achieved the best value of F 0.5 among all the methods.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Aspect (ii): Seed Corpus T", "text": "We investigate the effectiveness of the seed corpus T for generating pseudo data D p . The three corpora (Wikipedia, SimpleWiki and Gigaword) are compared in Table 3. We set |D p | = 1.4M. The difference in F 0.5 is small, which implies that the seed corpus T has only a minor effect on the model performance. Nevertheless, Gigaword consistently outperforms the other two corpora. In particular,  DIRECTNOISE with Gigaword achieves the best value of F 0.5 among all the configurations.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Aspect (iii): Optimization Setting", "text": "We compare the JOINT and PRETRAIN optimization settings. We are interested in how each setting performs when the scale of the pseudo data D p compared with that of the genuine parallel data D g is (i) approximately the same (|D p | = 1.4M) and (ii) substantially bigger (|D p | = 14M). Here, we use Wikipedia as the seed corpus T instead of SimpleWiki or Gigaword for two reasons. First, SimpleWiki is too small for the experiment (b) (see Table 1). Second, the fact that Gigaword is not freely available makes it difficult for other researchers to replicate our results.\n(a) Joint Training or Pretraining Table 4 presents the results. The most notable result here is that PRETRAIN demonstrates the properties of more pseudo data and better performance, whereas JOINT does not. For example, in BACKTRANS (NOISY), increasing |D p | (1.4M \u2192 14M) improves F 0.5 on PRETRAIN (41.1 \u2192 44.5). By contrast, F 0.5 does not improve on JOINT (40.4 \u2192 40.3). An intuitive explanation for this case is that when pseudo data D p are substantially more than genuine data D g , the teaching signal from D p becomes dominant in JOINT. PRETRAIN alleviates this problem because the model is trained with only D g during fine-tuning. We therefore suppose that PRETRAIN is crucial for utilizing extensive pseudo data.   F 0.5 = 45.9.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Comparison with Current Top Models", "text": "The present experimental results show that the following configurations are effective for improving the model performance: (i) the combination of JOINT and Gigaword (Section 4.3), (ii) the amount of pseudo data D p not being too large in JOINT (Section 4.4(a)), and (iii) PRETRAIN with BACK-TRANS (NOISY) using large pseudo data D p (Section 4.4(b)). We summarize these findings and attempt to combine PRETRAIN and JOINT. Specifically, we pretrain the model using 70M pseudo data of BACKTRANS (NOISY). We then fine-tune the model by combining BEA-train and relatively small DIRECTNOISE pseudo data generated from Gigaword (we set |D p | = 250K). However, the performance does not improve on BEA-valid. Therefore, the best approach available is simply to pretrain the model with large (70M) BACKTRANS (NOISY) pseudo data and then fine-tune using BEAtrain, which hereinafter we refer to as PRETLARGE. We use Gigaword for the seed corpus T because it has the best performance in Table 3. We evaluate the performance of PRETLARGE on test sets and compare the scores with the current top models.  our PRETLARGE achieves F 0.5 = 61.3 on CoNLL-2014. This result outperforms not only all previous single-model results but also all ensemble results except for that by Grundkiewicz et al. (2019).\nTo further improve the performance, we incorporate the following techniques that are widely used in shared tasks such as BEA-2019 and WMT 13 : Synthetic Spelling Error (SSE) Lichtarge et al. (2019) proposed the method of probabilistically injecting character-level noise into the source sentence of pseudo data D p . Specifically, one of the following operations is applied randomly at a rate of 0.003 per character: deletion, insertion, replacement, or transposition of adjacent characters. Right-to-left Re-ranking (R2L) Following Sennrich et al. (2016aSennrich et al. ( , 2017; Grundkiewicz et al. (2019), we train four right-to-left models. The ensemble of four left-to-right models generate n-best candidates and their corresponding scores (i.e., conditional probabilities). We then pass each candidate to the ensemble of the four right-to-left models and compute the score. Finally, we re-rank the n-best candidates based on the sum of the two scores. Sentence-level Error Detection (SED) SED classifies whether a given sentence contains a grammatical error. Asano et al. (2019) proposed incorporating SED into the evaluation pipeline and reported improved precision. Here, the GEC model is applied only if SED detects a grammatical error in the given source sentence. The motivation is that SED could potentially reduce the number of false-positive errors of the GEC model. We use the re-implementation of the BERT-based SED model (Asano et al., 2019).\nTable 5 presents the results of applying SSE, 13 http://www.statmt.org/wmt19/ R2L, and SED. It is noteworthy that PRET-LARGE+SSE+R2L achieves state-of-the-art performance on both CoNLL-2014 (F 0.5 = 65.0) and BEA-test (F 0.5 = 69.8), which are better than those of the best system of the BEA-2019 shared task (Grundkiewicz et al., 2019). In addition, PRET-LARGE+SSE+R2L+SED can further improve the performance on BEA-test (F 0.5 = 70.2). However, unfortunately, incorporating SED decreased the performance on CoNLL-2014 and JFLEG. This fact implies that SED is sensitive to the domain of the test set since the SED model is fine-tuned with the official validation split of BEA dataset. We leave this sensitivity issue as our future work.", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this study, we investigated several aspects of incorporating pseudo data for GEC. Through extensive experiments, we found the following to be effective: (i) utilizing Gigaword as the seed corpus, and (ii) pretraining the model with BACKTRANS (NOISY) data. Based on these findings, we proposed suitable settings for GEC. We demonstrated the effectiveness of our proposal by achieving stateof-the-art performance on the CoNLL-2014 test set and the BEA-2019 test set.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "We thank the three anonymous reviewers for their insightful comments. We are deeply grateful to Takumi Ito and Tatsuki Kuribayashi for kindly sharing the re-implementation of BACKTRANS (NOISY). The work of Jun Suzuki was supported in part by JSPS KAKENHI Grant Number  JP19104418 and AIRPF Grant Number 30AI036-8.   ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "The AIP-Tohoku System at the BEA-2019 Shared Task", "journal": "", "year": "2019", "authors": "Hiroki Asano; Masato Mita; Tomoya Mizumoto"}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "journal": "", "year": "2015", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction", "journal": "", "year": "2017", "authors": "Christopher Bryant; Mariano Felice; Ted Briscoe"}, {"title": "A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction", "journal": "", "year": "2018", "authors": "Shamil Chollampatt; Hwee Tou Ng"}, {"title": "Better Evaluation for Grammatical Error Correction", "journal": "", "year": "2012", "authors": "Daniel Dahlmeier; Hwee Tou Ng"}, {"title": "Building a Large Annotated Corpus of Learner English: The NUS Corpus of Learner English", "journal": "", "year": "2013", "authors": "Daniel Dahlmeier; Siew Mei Hwee Tou Ng;  Wu"}, {"title": "Understanding Back-Translation at Scale", "journal": "", "year": "2018", "authors": "Sergey Edunov; Myle Ott; Michael Auli; David Grangier"}, {"title": "Automatic Extraction of Learner Errors in ESL Sentences Using Linguistically Enhanced Alignments", "journal": "", "year": "2016", "authors": "Mariano Felice; Christopher Bryant; Ted Briscoe"}, {"title": "Fluency Boost Learning and Inference for Neural Grammatical Error Correction", "journal": "", "year": "2018", "authors": "Tao Ge; Furu Wei; Ming Zhou"}, {"title": "The computer learner corpus: A versatile new source of data for SLA research", "journal": "Learner English on Computer", "year": "1998", "authors": "Sylviane Granger"}, {"title": "Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation", "journal": "", "year": "2018", "authors": "Roman Grundkiewicz; Marcin Junczys-Dowmunt"}, {"title": "Neural grammatical error correction systems with unsupervised pre-training on synthetic data", "journal": "", "year": "2019", "authors": "Roman Grundkiewicz; Marcin Junczys-Dowmunt; Kenneth Heafield"}, {"title": "A Nested Attention Neural Hybrid Model for Grammatical Error Correction", "journal": "", "year": "2017", "authors": "Jianshu Ji; Qinlong Wang; Kristina Toutanova; Yongen Gong; Steven Truong; Jianfeng Gao"}, {"title": "Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task", "journal": "", "year": "2018", "authors": "Marcin Junczys-Dowmunt; Roman Grundkiewicz; Shubha Guha; Kenneth Heafield"}, {"title": "Adam: A Method for Stochastic Optimization", "journal": "", "year": "2015", "authors": "Diederik Kingma; Jimmy Ba"}, {"title": "Six Challenges for Neural Machine Translation", "journal": "", "year": "2017", "authors": "Philipp Koehn; Rebecca Knowles"}, {"title": "Corpora Generation for Grammatical Error Correction", "journal": "", "year": "2019", "authors": "Jared Lichtarge; Chris Alberti; Shankar Kumar; Noam Shazeer; Niki Parmar; Simon Tong"}, {"title": "Mining Revision Log of Language Learning SNS for Automated Japanese Error Correction of Second Language Learners", "journal": "", "year": "2011", "authors": "Tomoya Mizumoto; Mamoru Komachi; Masaaki Nagata; Yuji Matsumoto"}, {"title": "Ground Truth for Grammatical Error Correction Metrics", "journal": "", "year": "2015", "authors": "Courtney Napoles; Keisuke Sakaguchi; Matt Post; Joel Tetreault"}, {"title": "", "journal": "", "year": "2016", "authors": "Courtney Napoles; Keisuke Sakaguchi; Matt Post; Joel Tetreault"}, {"title": "JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction", "journal": "", "year": "2017", "authors": "Courtney Napoles; Keisuke Sakaguchi; Joel Tetreault"}, {"title": "The CoNLL-2014 Shared Task on Grammatical Error Correction", "journal": "", "year": "2014", "authors": " Hwee Tou Ng; Mei Siew; Ted Wu; Christian Briscoe; Raymond Hendy Hadiwinoto; Christopher Susanto;  Bryant"}, {"title": "fairseq: A Fast, Extensible Toolkit for Sequence Modeling", "journal": "", "year": "2019", "authors": "Myle Ott; Sergey Edunov; Alexei Baevski; Angela Fan; Sam Gross; Nathan Ng; David Grangier; Michael Auli"}, {"title": "The university of Edinburgh's neural MT systems for WMT17", "journal": "", "year": "2017", "authors": "Rico Sennrich; Alexandra Birch; Anna Currey; Ulrich Germann; Barry Haddow; Kenneth Heafield; Antonio Valerio Miceli; Philip Barone;  Williams"}, {"title": "Edinburgh neural machine translation systems for WMT 16", "journal": "", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Improving Neural Machine Translation Models with Monolingual Data", "journal": "", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Neural Machine Translation of Rare Words with Subword Units", "journal": "", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost", "journal": "", "year": "2018", "authors": "Noam Shazeer; Mitchell Stern"}, {"title": "Sequence to Sequence Learning with Neural Networks", "journal": "", "year": "2014", "authors": "Ilya Sutskever; Oriol Vinyals; V Quoc;  Le"}, {"title": "Rethinking the inception architecture for computer vision", "journal": "", "year": "2016", "authors": "Christian Szegedy; Vincent Vanhoucke; Sergey Ioffe; Jon Shlens; Zbigniew Wojna"}, {"title": "Tense and Aspect Error Correction for ESL Learners Using Global Context", "journal": "", "year": "2012", "authors": "Toshikazu Tajiri; Mamoru Komachi; Yuji Matsumoto"}, {"title": "Attention Is All You Need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction", "journal": "", "year": "2018", "authors": "Ziang Xie; Guillaume Genthial; Stanley Xie; Andrew Ng; Dan Jurafsky"}, {"title": "Developing an Automated Writing Placement system for ESL Learners", "journal": "Applied Measurement in Education", "year": "2018", "authors": "Helen Yannakoudakis; \u00d8istein E Andersen; Ardeshir Geranpayeh; Ted Briscoe; Diane Nicholls"}, {"title": "A New Dataset and Method for Automatically Grading ESOL Texts", "journal": "", "year": "2011", "authors": "Helen Yannakoudakis; Ted Briscoe; Ben Medlock"}, {"title": "Improving Grammatical Error Correction via Pre-Training a Copy-Augmented Architecture with Unlabeled Data", "journal": "", "year": "2019", "authors": "Wei Zhao; Liang Wang; Kewei Shen; Ruoyu Jia; Jingming Liu"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "(b) Amount of Pseudo Data We investigate how increasing the amount of pseudo data affects the PRETRAIN setting. We pretrain the model with different amounts of pseudo data {1.4M, 7M, 14M, 30M, 70M}. The results in Figure 1 show that BACKTRANS (NOISY) has superior sample efficiency to DIRECTNOISE. The best model (pretrained with 70M BACKTRANS (NOISY)) achieves Optimization Method |Dp| Prec. Rec.", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 1 :1Figure 1: Performance on BEA-valid for different amounts of pseudo data (|D p |). The seed corpus T is Wikipedia.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Summary of datasets used in our experiments. Dataset marked with \"*\" is a seed corpus T .", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Performance of models on BEA-valid: a value in bold indicates the best result within the column. The seed corpus T is SimpleWiki.", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Performance on BEA-valid when changing the seed corpus T used for generating pseudo data (|D p | = 1.4M).", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Performance of the model with different optimization settings on BEA-valid. The seed corpus T is Wikipedia.", "figure_data": "46F 0.5 score42 44Backtrans (noisy)40DirectNoiseBaseline010203040506070Amount of Pseudo Data |D p | (M)"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": "shows a remarkable result, that is,"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Comparison of our best model and current top models: a bold value indicates the best result within the column.", "figure_data": ""}], "doi": ""}