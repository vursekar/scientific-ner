{"authors": "John X Morris; Eli Lifland; Jack Lanchantin; Yangfeng Ji; Yanjun Qi", "pub_date": "", "title": "Reevaluating Adversarial Examples in Natural Language", "abstract": "State-of-the-art attacks on NLP models lack a shared definition of what constitutes a successful attack. These differences make the attacks difficult to compare and hindered the use of adversarial examples to understand and improve NLP models. We distill ideas from past work into a unified framework: a successful natural language adversarial example is a perturbation that fools the model and follows four proposed linguistic constraints. We categorize previous attacks based on these constraints. For each constraint, we suggest options for human and automatic evaluation methods. We use these methods to evaluate two state-of-the-art synonym substitution attacks. We find that perturbations often do not preserve semantics, and 38% introduce grammatical errors. Next, we conduct human studies to find a threshold for each evaluation method that aligns with human judgment. Human surveys reveal that to successfully preserve semantics, we need to significantly increase the minimum cosine similarities between the embeddings of swapped words and between the sentence encodings of original and perturbed sentences. With constraints adjusted to better preserve semantics and grammaticality, the attack success rate drops by over 70 percentage points. 1   ", "sections": [{"heading": "Introduction", "text": "One way to evaluate the robustness of a machine learning model is to search for inputs that produce incorrect outputs. Inputs intentionally designed to fool deep learning models are referred to as adversarial examples (Goodfellow et al., 2017). Adversarial examples have successfully tricked deep neural networks for image classification: two images that look exactly the same to a human receive \u21e4 * Equal contribution 1 Our code and datasets are available here.\nFigure 1: An adversarial example generated by TFAD-JUSTED for BERT fine-tuned on the Rotten Tomatoes sentiment analysis dataset. Swapping a single word causes the prediction to change from positive to negative.\ncompletely different predictions from the classifier (Goodfellow et al., 2014).\nWhile applicable in the image case, the idea of an indistinguishable change lacks a clear analog in text. Unlike images, two different sequences of text are never entirely indistinguishable. This raises the question: if indistinguishable perturbations are not possible, what are adversarial examples in text?\nThe literature contains many potential answers to this question, proposing varying definitions for successful adversarial examples (Zhang et al., 2019). Even attacks with similar definitions of success often measure it in different ways. The lack of a consistent definition and standardized evaluation has hindered the use of adversarial examples to understand and improve NLP models. 2 Therefore, we propose a unified definition for successful adversarial examples in natural language: perturbations that both fool the model and fulfill a set of linguistic constraints. In Section 2, we present four categories of constraints NLP adversarial examples may follow, depending on the context: semantics, grammaticality, overlap, and non-suspicion to human readers.\nBy explicitly laying out categories of constraints adversarial examples may follow, we introduce a shared vocabulary for discussing constraints on adversarial attacks. In Section 4, we suggest options for human and automatic evaluation methods for each category. We use these methods to evaluate two SOTA synonym substitution attacks: GENETICATTACK by Alzantot et al. (2018) and TEXTFOOLER by Jin et al. (2019). Human surveys show that the perturbed examples often fail to fulfill semantics and non-suspicion constraints. Additionally, a grammar checker detects 39% more errors in the perturbed examples than in the original inputs, including many types of errors humans almost never make.\nIn Section 5, we produce TFADJUSTED, an attack with the same search process as TEXTFOOLER, but with constraint enforcement tuned to generate higher quality adversarial examples. To enforce semantic preservation, we tighten the thresholds on the cosine similarity between embeddings of swapped words and between the sentence encodings of original and perturbed sentences. To enforce grammaticality, we validate perturbations with a grammar checker. As in TEXTFOOLER, these constraints are applied at each step of the search. Human evaluation shows that TFADJUSTED generates perturbations that better preserve semantics and are less noticeable to human judges. However, with stricter constraints, the attack success rate decreases from over 80% to under 20%. When used for adversarial training, TEXTFOOLER's examples decreased model accuracy, but TFADJUSTED's examples did not. Without a shared vocabulary for discussing constraints, past work has compared the success rate of search methods with differing constraint application techniques. Jin et al. (2019) reported a higher attack success rate for TEXTFOOLER than Alzantot et al. (2018) did for GENETICATTACK, but it was not clear whether the improvement was due to a better search method 3 or more lenient constraint application 4 . In Section 6 we compare the search methods with constraint application held constant. We find that GENETICATTACK's search method is more successful than TEXTFOOLER's, contrary to the implications of Jin et al. (2019).\nThe five main contributions of this paper are:\n\u2022 A definition for constraints on adversarial perturbations in natural language and suggest evaluation methods for each constraint. \u2022 Constraint evaluations of two SOTA synonymsubstitution attacks, revealing that their perturbations often do not preserve semantics, grammaticality, or non-suspicion. \u2022 Evidence that by aligning automatic constraint application with human judgment, it is possible for attacks to produce successful, valid adversarial examples. \u2022 Demonstration that reported differences in attack success between TEXTFOOLER and GENET-ICATTACK are the result of more lenient constraint enforcement. \u2022 Our framework enables fair comparison between attacks, by separating effects of search methods from effects of loosened constraints.", "n_publication_ref": 11, "n_figure_ref": 1}, {"heading": "Constraints on Adversarial Examples in Natural Language", "text": "We define F : X ! Y as a predictive model, for example, a deep neural network classifier. X is the input space and Y is the output space. We focus on adversarial perturbations which perturb a correctly predicted input, x 2 X , into an input x adv . The boolean goal function G(F, x adv ) represents whether the goal of the attack has been met. We define C 1 ...C n as a set of boolean functions indicating whether the perturbation satisfies a certain constraint.\nAdversarial attacks search for a perturbation from x to x adv which fools F by both achieving some goal, as represented by G(F, x adv ), and fulfilling each constraint C i (x, x adv ).\nThe definition of the goal function G depends on the purpose of the attack. Attacks on classification frequently aim to either induce any incorrect classification (untargeted) or induce a particular classification (targeted). Attacks on other types of models may have more sophisticated goals. For example, attacks on translation may attempt to change every word of a translation, or introduce targeted keywords into the translation (Cheng et al., 2018).\nIn addition to defining the goal of the attack, the attacker must decide the constraints perturbations must meet. Different use cases require different Input, x: \"Shall I compare thee to a summer's day?\" -William Shakespeare, Sonnet XVIII", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Constraint", "text": "Perturbation, x adv Explanation Semantics Shall I compare thee to a winter's day?\nx adv has a different meaning than x. Grammaticality Shall I compares thee to a summer's day?\nx adv is less grammatically correct than x. Edit Distance Sha1l i conpp$haaare thee to a 5umm3r's day? x and x adv have a large edit distance. Non-suspicion Am I gonna compare thee to a summer's day? A human reader may suspect this sentence to have been modified. 1\n1 Shakespeare never used the word \"gonna\". Its first recorded usage wasn't until 1806, and it didn't become popular until the 20th century. constraints. We build on the categorization of attack spaces introduced by Gilmer et al. (2018) to introduce a categorization of constraints for adversarial examples in natural language.\nIn the following, we define four categories of constraints on adversarial perturbations in natural language: semantics, grammatically, overlap, and non-suspicion. Table 1 provides examples of adversarial perturbations that violate each constraint.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Semantics", "text": "Semantics constraints require the semantics of the input to be preserved between x and x adv . Many attacks include constraints on semantics as a way to ensure the correct output is preserved (Zhang et al., 2019). As long as the semantics of an input do not change, the correct output will stay the same. There are exceptions: one could imagine tasks for which preserving semantics does not necessarily preserve the correct output. For example, consider the task of classifying passages as written in either Modern or Early Modern English. Perturbing \"why\" to \"wherefore\" may retain the semantics of the passage, but change the correct label from Modern to Early Modern English 5", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Grammaticality", "text": "Grammaticality constraints place restrictions on the grammaticality of x adv . For example, an adversary attempting to generate a plagiarised paper which fools a plagiarism checker would need to ensure that the paper remains grammatically correct. Grammatical errors don't necessarily change semantics, as illustrated in Table 1.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Overlap", "text": "Overlap constraints restrict the similarity between x and x adv at the character level. This in-cludes constraints like Levenshtein distance as well as n-gram based measures such as BLEU, ME-TEOR and chRF (Papineni et al., 2002;Denkowski and Lavie, 2014;Popovi\u0107, 2015).\nSetting a maximum edit distance is useful when the attacker is willing to introduce misspellings. Additionally, the edit distance constraint is sometimes used when improving the robustness of models. For example, Huang et al. (2019) uses Interval Bound Propagation to ensure model robustness to perturbations within some edit distance of the input.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Non-suspicion", "text": "Non-suspicion constraints specify that x adv must appear to be unmodified. Consider the example in Table 1. While the perturbation preserves semantics and grammar, it switches between Modern and Early Modern English and thus may seem suspicious to readers.\nNote that the definition of the non-suspicious constraint is context-dependent. A sentence that is non-suspicious in the context of a kindergartner's homework assignment might be suspicious in the context of an academic paper. An attack scenario where non-suspicion constraints do not apply is illegal PDF distribution, similar to a case discussed by Gilmer et al. (2018). Consumers of an illegal PDF may tacitly collude with the person uploading it. They know the document has been altered, but do not care as long as semantics are preserved. Attacks by Synonym Substitution: Some works focus on an easier way to generate a subset of paraphrases: replacing words from the input with synonyms (Alzantot et al., 2018;Jin et al., 2019;Kuleshov et al., 2018;Papernot et al., 2016;Ren et al., 2019). Each attack applies a search algorithm to determine which words to replace with which synonyms. Like the general paraphrase case, they aim to create examples that preserve semantics, grammaticality, and non-suspicion. While not all have an explicit edit distance constraint, some limit the number of words perturbed.\nAttacks by Character Substitution: Some studies have proposed to attack natural language classification models by deliberately misspelling words (Ebrahimi et al., 2017;Gao et al., 2018;Li et al., 2018). These attacks use character replacements to change a word into one that the model doesn't recognize. The replacements are designed to create character sequences that a human reader would easily correct into the original words. If there aren't many misspellings, non-suspicion may be preserved. Semantics are preserved as long as human readers can correct the misspellings.\nAttacks by Word Insertion or Removal: Liang et al. (2017) and Samanta and Mehta (2017) devised a way to determine the most important words in the input and then used heuristics to generate perturbed inputs by adding or removing important words. In some cases, these strategies are combined with synonym substitution. These attacks aim to follow all constraints.\nUsing constraints defined in Section 2 we categorize a sample of current attacks in Table 2.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Constraint Evaluation Methods and Case Study", "text": "For each category of constraints introduced in Section 2, we discuss best practices for both human and automatic evaluation. We leave out overlap due to ease of automatic evaluation.\nAdditionally, we perform a case study, evaluating how well black-box synonym substitution attacks GENETICATTACK and TEXTFOOLER fulfill constraints. Both attacks find adversarial examples by swapping out words for their synonyms until the classifier is fooled. GENETICATTACK uses a genetic algorithm to attack an LSTM trained on the IMDB 6 document-level sentiment classification dataset. TEXTFOOLER uses a greedy approach to attack an LSTM, CNN, and BERT trained on five classification datasets. We chose these attacks because:\n\u2022 They claim to create perturbations that preserve semantics, maintain grammaticality, and are not suspicious to readers. However, our inspection of the perturbations revealed that many violated these constraints. \u2022 They report high attack success rates. 7 \u2022 They successfully attack two of the most effective models for text classification: LSTM and BERT.\nTo generate examples for evaluation, we attacked BERT using TEXTFOOLER and attacked an LSTM using GENETICATTACK. We evaluate both methods on the IMDB dataset. In addition, we evaluate TEXTFOOLER on the Yelp polarity document-level sentiment classification dataset and the Movie Review (MR) sentence-level sentiment classification dataset (Pang and Lee, 2005;Zhang et al., 2015). We use 1, 000 examples from each dataset. Table 3 shows example violations of each constraint.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Evaluation of Semantics", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Human Evaluation", "text": "A few past studies of attacks have included human evaluation of semantic preservation (Ribeiro et al., 2018;Iyyer et al., 2018;Alzantot et al., 2018;Jin et al., 2019). However, studies often simply ask users to simply rate the \"similarity\" of x and x adv . We believe this phrasing does not generate an accurate measure of semantic preservation, as users may consider two sentences with different semantics \"similar\" if they only differ by a few words. Instead, users should be explicitly asked whether changes between x and x adv preserve the meaning of the original passage.\nWe propose to ask human judges to rate if meaning is preserved on a Likert scale of 1-5, where 1 is \"Strongly Disagree\" and 5 is \"Strongly Agree\" (Likert, 1932). A perturbation is semantics-preserving if the average score is at least \u270f sem . We propose", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Selected Attacks Generating Adversarial Examples in Natural Language", "text": "Semantics Grammaticality Edit Distance Non-Suspicion Synonym Substitution. (Alzantot et al., 2018;Kuleshov et al., 2018;Jin et al., 2019;Ren et al., 2019) 3 3 3 3\nCharacter Substitution. (Ebrahimi et al., 2017;Gao et al., 2018;Li et al., 2018) 3 5 3 3\nWord Insertion or Removal. (Liang et al., 2017;Samanta and Mehta, 2017) 3 3 3 3 General Paraphrase. (Zhao et al., 2017;Ribeiro et al., 2018;Iyyer et al., 2018) 3 3 5 3 A \"3\" indicates that the respective attack is supposed to meet the constraint, and a \"5\" means the attack is not supposed to meet the constraint.", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "Constraint Violated", "text": "Input, x Perturbation, x adv Semantics Jagger, Stoppard and director Michael Apted deliver a riveting and surprisingly romantic ride.\nJagger, Stoppard and director Michael Apted deliver a baffling and surprisingly sappy motorbike. Grammaticality A grating, emaciated flick.\nA grates, lanky flick. Non-suspicion Great character interaction.\nGargantuan character interaction. \u270f sem = 4 as a general rule: on average, humans should at least \"Agree\" that x and x adv have the same meaning.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Automatic Evaluation", "text": "Automatic evaluation of semantic similarity is a well-studied NLP task. The STS Benchmark is used as a common measurement (Cer et al., 2017). Michel et al. (2019) explored the use of common evaluation metrics for machine translation as a proxy for semantic similarity in the attack setting. While n-gram overlap based approaches are computationally cheap and work well in the machine translation setting, they do not correlate with human judgment as well as sentence encoders . Some attacks have used sentence encoders to encode two sentences into a pair of fixed-length vectors, then used the cosine distance between the vectors as a proxy for semantic similarity. TEXTFOOLER uses the Universal Sentence Encoder (USE), which achieved a Pearson correlation score of 0.782 on the STS benchmark (Cer et al., 2018). Another option is BERT fine-tuned for semantic similarity, which achieved a score of 0.865 (Devlin et al., 2018).\nAdditionally, synonym substitution methods, including TEXTFOOLER and GENETICATTACK, often require that words be substituted only with neighbors in the counter-fitted embedding space, which is designed to push synonyms together and antonyms apart (Mrksic et al., 2016). These automatic metrics of similarity produce a score that represents the similarity between x and x adv . Attacks depend on a minimum threshold value for each metric to determine whether the changes between x and x adv preserve semantics. Human evaluation is needed to find threshold values such that people generally \"agree\" that semantics is preserved.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Case Study", "text": "To quantify semantic similarity of x and x adv , we asked users whether they agreed that the changes between the two passages preserved meaning on a scale of 1 (Strongly Disagree) to 5 (Strongly Agree). We averaged scores for each attack method to determine if the method generally preserves semantics.\nPerturbations generated by TEXTFOOLER were rated an average of 3.28, while perturbations generated by GENETICATTACK were rated on average 2.70. 8 The average rating given for both methods was significantly less than our proposed \u270f sem of 4. Using a clear survey question illustrates that humans, on average, don't assess these perturbations as semantics-preserving.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Evaluation of Grammaticality", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Human Evaluation", "text": "Both Jin et al. (2019) and Iyyer et al. (2018) reported a human evaluation of grammaticality, but neither study clearly asked if any errors were introduced by a perturbation. For human evaluation of the grammaticality constraint, we propose presenting x and x adv together and asking judges if grammatical errors were introduced by the changes made. However, due to the rule-based nature of grammar, automatic evaluation is preferred.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Automatic Evaluation", "text": "The simplest way to automatically evaluate grammatical correctness is with a rule-based grammar checker. Free grammar checkers are available online in many languages. One popular checker is LanguageTool, an open-source proofreading tool (Naber, 2003). LanguageTool ships with thousands of human-curated rules for the English language and provides an interface for identifying grammatical errors in sentences. LanguageTool uses rules to detect grammatical errors, statistics to detect uncommon sequences of words, and language model perplexity to detect commonly confused words.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Case Study", "text": "We ran each of the generated (x, x adv ) pairs through LanguageTool to count grammatical errors. LanguageTool detected more grammatical errors in x adv than x for 50% of perturbations generated by TEXTFOOLER, and 32% of perturbations generated by GENETICATTACK.\nAdditionally, perturbations often contain errors that humans rarely make. LanguageTool detected 6 categories for which errors in the perturbed samples appear at least 10 times more frequently than in the original content. Details regarding these error categories and examples of violations are shown in Table 4.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Evaluation of Non-suspicion", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Human Evaluation", "text": "We propose evaluation of non-suspicion by having judges view a shuffled mix of real and adversarial inputs and guess whether each is real or computer-altered. This is similar to the human evaluation done by Ren et al. ( 2019), but we formulate it as a binary classification task rather than on a 1-5 scale. A perturbed example x adv is not suspicious if the percentage of judges who identify x adv as computer-altered is at most \u270f ns , where 0 \uf8ff \u270f ns \uf8ff 1.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Automatic Evaluation", "text": "Automatic evaluation may be used to guess whether or not an adversarial example is suspicious. Models can be trained to classify passages as real or perturbed, just as human judges do. For example, Warstadt et al. (2018) trained sentence encoders on a real/fake task as a proxy for evaluation of linguistic acceptability. Recently, Zellers et al. (2019) demonstrated that GROVER, a transformer-based text generation model, could classify its own generated news articles as human or machine-written with high accuracy.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Case Study", "text": "We presented a shuffled mix of real and perturbed examples to human judges and asked if they were real or computer-altered. As this is a time-consuming task for long documents, we only evaluated adversarial examples generated by TEXTFOOLER on the sentence-level MR dataset.\nIf all generated examples were non-suspicious, judges would average 50% accuracy, as they would not be able to distinguish between real and perturbed examples. In this case, judges achieved 69.2% accuracy.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Producing Higher Quality Adversarial Examples", "text": "In Section 4, we evaluated how well generated examples met constraints. We found that although attacks in NLP aspire to meet linguistic constraints, in practice, they frequently violate them. Now, we adjust automatic constraints applied during the course of the attack to produce better quality adversarial examples.\nWe set out to find if a set of constraint application methods with appropriate thresholds could produce adversarial examples that are semanticspreserving, grammatical and non-suspicious. We modified TEXTFOOLER to produce TFADJUSTED, a new attack with stricter constraint application. To enforce grammaticality, we added Language-Tool. To enforce semantic preservation, we tuned two thresholds which filter out invalid word substitutions: (a) minimum cosine similarity between counter-fitted word embeddings and (b) minimum  cosine similarity between sentence embeddings.\nThrough human studies, we found threshold values of 0.9 for (a) and 0.98 for (b) 9 . We implemented TFADJUSTED using TextAttack, a Python framework for implementing adversarial attacks in NLP (Morris et al., 2020).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "With Adjusted Constraint Application", "text": "We tested TFADJUSTED to determine the effect of tightening constraint application. We used the IMDB, Yelp, and MR datasets for classifcation as in Section 4. We added the SNLI and MNLI entailment datasets (Bowman et al., 2015;Williams et al., 2018) for the portions not requring human evaluation. Table 5 shows the results.\nSemantics. TEXTFOOLER generates perturbations for which human judges are on average \"Not sure\" if semantics are preserved. With perturbations generated by TFADJUSTED, human judges on average \"Agree\" that semantics are preserved.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Grammaticality.", "text": "Since all examples produced by TFADJUSTED are checked with LanguageTool, no perturbation can introduce grammatical errors. 10 Non-suspicion. We repeated the non-suspicion study from Section 4.3 with the examples generated by TFADJUSTED. Participants were able to guess with 58.8% accuracy whether inputs were computer-altered. The accuracy is over 10% lower than the accuracy on the examples generated by TEXTFOOLER.\nAttack success. For each of the three datasets, the attack success rate decreased by at least 71 percentage points (see last row of Table 5).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Adversarial Training With Higher", "text": "Quality Examples\nUsing the 9, 595 samples in the MR training set as seed inputs, TEXTFOOLER generated 7,382 adversarial examples, while TFADJUSTED generated just 825. We append each set of adversarial examples to a copy of the original MR training set and fine-tuned a pre-trained BERT model for 10 epochs. Figure 2 plots the test accuracy over 10 training epochs, averaged over 5 random seeds per dataset. While neither training method strongly impacts accuracy, the augmentation using TFADJUSTED has a better impact than that of TEXTFOOLER.\nWe then re-ran the two attacks using 1000 examples from the MR test set as seeds. Again averaging over 5 random seeds, we found no significant change in robustness. That is, models trained on the original MR dataset were approximately as robust as those trained on the datasets augmented with TEXTFOOLER and TFADJUSTED examples. This corroborates the findings of Alzantot et al. (2018) and contradicts those of Jin et al. (2019). We include further analysis along with some hypotheses for the discrepancies in adversarial training results in A.4.   ", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Ablation of TFADJUSTED Constraints", "text": "TFADJUSTED generated better quality adversarial examples by constraining its search to exclude examples that fail to meet three constraints: word embedding distance, sentence encoder similarity, and grammaticality. We performed an ablation study to understand the relative impact of each on attack success rate.\nWe reran three TFADJUSTED attacks (one for each constraint removed) on each dataset. Table 6 shows attack success rate after individually removing each constraint. The word embedding distance constraint was the greatest inhibitor of attack success rate, followed by the sentence encoder.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Comparing Search Methods", "text": "When an attack's success rate improves, it may be the result of either (a) improvement of the search method for finding adversarial perturbations or (b) more lenient constraint definitions or constraint application. TEXTFOOLER achieves a higher success rate than GENETICATTACK, but Jin et al. (2019) did not identify whether the improvement was due to (a) or (b). Since TEXTFOOLER uses both a different search method and different constraint application methods than GENETICATTACK, the source of the difference in attack success rates is unclear.\nTo determine which search method is more effective, we used TextAttack to compose attacks from the search method of GENETICATTACK and the constraint application methods of each of TEXTFOOLER and TFADJUSTED (Morris et al., 2020). With the constraint application held constant, we can identify the source of the difference in attack success rate. Table 7 reveals that the genetic algorithm of GENETICATTACK is more successful than the greedy search of TEXTFOOLER at both constraint application levels. This reveals the source of improvement in attack success rate between GENETICATTACK and TEXTFOOLER to be more lenient constraint application. However, GE-NETICATTACK's genetic algorithm is far more computationally expensive, requiring over 40x more model queries. Table 7: Comparison of the search methods from GENETICATTACK and TEXTFOOLER with two sets of constraints (TEXTFOOLER and TFADJUSTED). Attacks were run on 1000 samples against BERT fine-tuned on the MR dataset. GENETICATTACK's genetic algorithm is more successful than TEXTFOOLER's greedy strategy, albeit much less efficient.\nthat preserve semantics and grammaticality, NLP models are relatively robust to current synonym substitution attacks. Note that our set of constraints isn't necessarily optimal for every attack scenario. Some contexts may require fewer constraints or less strict constraint application.\nDecoupling search methods and constraints. It is critical that researchers decouple new search methods from new constraint evaluation and constraint application methods. Demonstrating the performance of a new attack that simultaneously introduces a new search method and new constraints makes it unclear whether empirical gains indicate a more effective attack or a more relaxed set of constraints. This mirrors a broader trend in machine learning where researchers report differences that come from changing multiple independent variables, making the sources of empirical gains unclear (Lipton and Steinhardt, 2018). This is especially relevant in adversarial NLP, where each experiment depends on many parameters.\nTowards improved methods for generating textual adversarial examples. As models improve at paraphrasing inputs, we will be able to explore the space of adversarial examples beyond synonym substitutions. As models improve at measuring semantic similarity, we will be able to more rigorously ensure that adversarial perturbations preserve semantics. It remains to be seen how robust BERT is when subject to paraphrase attacks that rigorously preserve semantics and grammaticality.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Related Work", "text": "The goal of creating adversarial examples that preserve semantics and grammaticality is common in the NLP attack literature (Zhang et al., 2019). However, previous works use different definitions of adversarial examples, making it difficult to compare methods. We provide a unified definition of an adversarial example based on a goal function and a set of linguistic constraints. Gilmer et al. (2018) laid out a set of potential constraints for the attack space when generating adversarial examples, which are each useful in different real-world scenarios. However, they did not discuss NLP attacks in particular. Michel et al. (2019) defined a framework for evaluating attacks on machine translation models, focusing on meaning preservation constraints, but restricted their definitions to sequence-to-sequence models. Other research on NLP attacks has suggested various constraints but has not introduced a shared vocabulary and categorization that allows for effective comparisons between attacks.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "We showed that two state-of-the-art synonym substitution attacks, TEXTFOOLER and GENETI-CATTACK, frequently violate the constraints they claim to follow. We created TFADJUSTED, which applies constraints that produce adversarial examples judged to preserve semantics and grammaticality.\nDue to the lack of a shared vocabulary for discussing NLP attacks, the source of improvement in attack success rate between TEXTFOOLER and GENETICATTACK was unclear. Holding constraint application constant revealed that the source of TEXTFOOLER's improvement was lenient constraint application (rather than a better search method). With a shared framework for defining and applying constraints, future research can focus on developing better search methods and better constraint application techniques for preserving semantics and grammaticality.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Generating natural language adversarial examples", "journal": "", "year": "2018", "authors": "Moustafa Alzantot; Yash Sharma; Ahmed Elgohary; Bo-Jhang Ho; Mani Srivastava; Kai-Wei Chang"}, {"title": "A large annotated corpus for learning natural language inference", "journal": "CoRR", "year": "2015", "authors": "R Samuel; Gabor Bowman; Christopher Angeli; Christopher D Potts;  Manning"}, {"title": "SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Daniel Cer; Mona Diab; Eneko Agirre; I\u00f1igo Lopez-Gazpio; Lucia Specia"}, {"title": "Chris Tar, Yun-Hsuan Sung, Brian Strope, and Ray Kurzweil", "journal": "", "year": "2018", "authors": "Daniel Cer; Yinfei Yang; Nan Sheng Yi Kong; Nicole Hua; Rhomni Limtiaco;  St; Noah John; Mario Constant; Steve Guajardo-Cespedes;  Yuan"}, {"title": "Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial examples", "journal": "", "year": "2018", "authors": "Minhao Cheng; Jinfeng Yi; Huan Zhang; Pin-Yu Chen; Cho-Jui Hsieh"}, {"title": "Meteor universal: Language specific translation evaluation for any target language", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Michael Denkowski; Alon Lavie"}, {"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Hotflip: White-box adversarial examples for text classification", "journal": "", "year": "2017", "authors": "Javid Ebrahimi; Anyi Rao; Daniel Lowd; Dejing Dou"}, {"title": "Black-box generation of adversarial text sequences to evade deep learning classifiers", "journal": "SPW", "year": "2018", "authors": "Ji Gao; Jack Lanchantin; Mary Lou Soffa; Yanjun Qi"}, {"title": "Motivating the rules of the game for adversarial example research", "journal": "CoRR", "year": "2018", "authors": "Justin Gilmer; Ryan P Adams; Ian J Goodfellow; David Andersen; George E Dahl"}, {"title": "Attacking machine learning with adversarial examples", "journal": "", "year": "2017", "authors": "Ian Goodfellow; Nicolas Papernot; Sandy Huang; Rocky Duan; Pieter Abbeel; Jack Clark"}, {"title": "Explaining and harnessing adversarial examples", "journal": "", "year": "2014", "authors": "J Ian; Jonathon Goodfellow; Christian Shlens;  Szegedy"}, {"title": "Achieving verified robustness to symbol substitutions via interval bound propagation", "journal": "ArXiv", "year": "2019", "authors": "Po-Sen Huang; Robert Stanforth; Johannes Welbl; Chris Dyer; Dani Yogatama; Sven Gowal; Krishnamurthy Dvijotham; Pushmeet Kohli"}, {"title": "Adversarial example generation with syntactically controlled paraphrase networks", "journal": "CoRR", "year": "2018", "authors": "Mohit Iyyer; John Wieting; Kevin Gimpel; Luke Zettlemoyer"}, {"title": "Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment. arXiv e-prints", "journal": "", "year": "2019", "authors": "Di Jin; Zhijing Jin; Joey Tianyi Zhou; Peter Szolovits"}, {"title": "Adversarial examples for natural language classification problems", "journal": "", "year": "2018", "authors": "Volodymyr Kuleshov; Shantanu Thakoor; Tingfung Lau; Stefano Ermon"}, {"title": "Textbugger: Generating adversarial text against real-world applications", "journal": "", "year": "2018", "authors": "Jinfeng Li; Shouling Ji; Tianyu Du; Bo Li; Ting Wang"}, {"title": "Deep text classification can be fooled", "journal": "", "year": "2017", "authors": "Bin Liang; Hongcheng Li; Miaoqiang Su; Pan Bian; Xirong Li; Wenchang Shi"}, {"title": "A Technique for the Measurement of Attitudes. Number nos. 136-165 in A Technique for the Measurement of Attitudes", "journal": "", "year": "1932", "authors": "R Likert"}, {"title": "Troubling trends in machine learning scholarship", "journal": "ArXiv", "year": "2018", "authors": "Zachary Chase Lipton; Jacob Steinhardt"}, {"title": "On evaluation of adversarial perturbations for sequence-to-sequence models", "journal": "CoRR", "year": "2019", "authors": "Paul Michel; Xian Li; Graham Neubig; Juan Miguel Pino"}, {"title": "Textattack: A framework for adversarial attacks in natural language processing", "journal": "", "year": "2020", "authors": "John X Morris; Eli Lifland; Jin Yong Yoo; Yanjun Qi"}, {"title": "Counter-fitting word vectors to linguistic constraints", "journal": "", "year": "2016", "authors": "Nikola Mrksic; Diarmuid\u00f3 S\u00e9aghdha; Blaise Thomson; Milica Gasic; Lina Maria Rojas-Barahona; Pei Hao Su; David Vandyke; Tsung-Hsien Wen; Steve J Young"}, {"title": "A rule-based style and grammar checker", "journal": "", "year": "2003", "authors": "Daniel Naber"}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "journal": "", "year": "2005", "authors": "Bo Pang; Lillian Lee"}, {"title": "Crafting adversarial input sequences for recurrent neural networks", "journal": "IEEE", "year": "2016", "authors": "Nicolas Papernot; Patrick Mcdaniel; Ananthram Swami; Richard Harang"}, {"title": "Bleu: a method for automatic evaluation of machine translation", "journal": "Association for Computational Linguistics", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"title": "chrF: character n-gram f-score for automatic MT evaluation", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Maja Popovi\u0107"}, {"title": "Sentencebert: Sentence embeddings using siamese bertnetworks", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Nils Reimers; Iryna Gurevych"}, {"title": "Generating natural language adversarial examples through probability weighted word saliency", "journal": "", "year": "2019", "authors": "Yihe Shuhuai Ren; Kun Deng; Wanxiang He;  Che"}, {"title": "Semantically equivalent adversarial rules for debugging nlp models", "journal": "Long Papers", "year": "2018", "authors": "Sameer Marco Tulio Ribeiro; Carlos Singh;  Guestrin"}, {"title": "Towards crafting text adversarial samples", "journal": "", "year": "2017", "authors": "Suranjana Samanta; Sameep Mehta"}, {"title": "Bowman", "journal": "", "year": "2018", "authors": "Alex Warstadt; Amanpreet Singh; Samuel R "}, {"title": "Paranmt-50m: Pushing the limits of paraphrastic sentence embeddings with millions of machine translations", "journal": "Long Papers", "year": "2018", "authors": "John Wieting; Kevin Gimpel"}, {"title": "A broad-coverage challenge corpus for sentence understanding through inference", "journal": "Long Papers", "year": "2018", "authors": "Adina Williams; Nikita Nangia; Samuel Bowman"}, {"title": "Defending against neural fake news. CoRR, abs", "journal": "", "year": "1905", "authors": "Rowan Zellers; Ari Holtzman; Hannah Rashkin; Yonatan Bisk; Ali Farhadi; Franziska Roesner; Yejin Choi"}, {"title": "Generating textual adversarial examples for deep learning models: A survey", "journal": "", "year": "1901", "authors": "Wei Emma Zhang; Z Quan; Ahoud Sheng; F Abdulrahmn;  Alhazmi"}, {"title": "Character-level convolutional networks for text classification", "journal": "", "year": "2015", "authors": "Xiang Zhang; Junbo Zhao; Yann Lecun"}, {"title": "Generating natural adversarial examples", "journal": "", "year": "2017", "authors": "Zhengli Zhao; Dheeru Dua; Sameer Singh"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "3Review and Categorization of SOTA: Attacks by Paraphrase: Some studies have generated adversarial examples through paraphrase. Iyyer et al. (2018) used neural machine translation systems to generate paraphrases. Ribeiro et al. (2018) proposed semantically-equivalent adversarial rules. By definition, paraphrases preserve semantics. Since the systems aim to generate perfect paraphrases, they implicitly follow constraints of grammaticality and non-suspicion.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Adversarial Constraints and Violations. For each of the four proposed constraints, we show an example for which violates the specified constraint.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Summary of Constraints and Attacks. This table shows a selection of prior work (rows) categorized by constraints (columns).", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Replace with one of[know]  ...ees at person they don't really want to knew PRP VBG 3 112 Did you mean \"we're wanting\", \"we are wanting\", or \"we were wanting\"? --Replace with one of [we're wanting,we are wanting,we were wanting] while we wanting macdowell's character to retrieve her h...", "figure_data": "Grammarxx adv ExplanationContextRule IDTO NON BASE 123 Did you mean \"know\"? --A PLURAL 2 20 294 Don't use indefinite articles with plural words. Did youa grates, lanky flickmean \"a grate\", \"a gratis\" or simply \"grates\"? --Replace with one of [a grate,a gratis,grates]DID BASEFORM 25328 The verb 'can't' requires base form of this verb: \"compare\"...first two cinema in the series, i can't compares--Replace with one of [compare]friday after next to them, but nothing ...PRP VB673Do not use a noun immediately after the pronoun 'it'. Use...ble of being gravest, so thick with wry it game likea verb or an adverb, or possibly some other part of speech.a readings from bartlett's familia...--Replace game with one of []PRP MD NN446It seems that a verb or adverb has been misspelled or is...y bit as awful as borchardt's coven, we canmissing here. --Replace with one of [can beappreciative it anywayappreciative,can have appreciative]NON3PRS VERB 778The pronoun 'they' must be used with a non-third-personthey does a ok operating of painting this family ...form of a verb: \"do\" --Replace with one of [do]"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Adversarial Examples Contain Uncommon Grammatical Errors. This table shows grammatical errors detected by LanguageTool that appeared far more often in the perturbed samples. x and x adv denote the numbers of errors detected in x and x adv across 3,115 examples generated by TEXTFOOLER and GENETICATTACK.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Results from running TEXTFOOLER (before) and TFADJUSTED (after). Attacks are on BERT classification models fine-tuned for five respective NLP datasets.", "figure_data": "Figure 2: Accuracy of adversarially trained models onthe MR test set. Augmentation with adversarial ex-amples generated by TEXTFOOLER (blue), althoughhigher in quantity, decreases the overall test accuracywhile examples generated by TFADJUSTED (orange)have a small positive effect.Constraint Removed Yelp IMDB MR MNLI SNLI(Original -all used)5.313.910.614.37.2Sentence Encoding22.945.028.744.431.2Word Embedding74.687.152.982.769.8Grammar Checking5.815.011.615.49.0"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": ": Ablation study: effect of removal of a singleconstraint on TFADJUSTED attack success rate. At-tacks against BERT fine-tuned on each dataset."}], "doi": "10.18653/v1/S17-2001"}