{"authors": "Yaobo Liang; Nan Duan; Yeyun Gong; Ning Wu; Fenfei Guo; Weizhen Qi; Ming Gong; Linjun Shou; Daxin Jiang; Guihong Cao; Xiaodong Fan; Ruofei Zhang; Rahul Agrawal; Edward Cui; Sining Wei; Taroon Bharti; Ying Qiao; Jiun-Hung Chen; Winnie Wu; Shuguang Liu; Fan Yang; Daniel Campos; Rangan Majumder; Ming Zhou", "pub_date": "", "title": "XGLUE: A New Benchmark Dataset for Cross-lingual Pre-training, Understanding and Generation", "abstract": "In this paper, we introduce XGLUE, a new benchmark dataset that can be used to train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora and evaluate their performance across a diverse set of cross-lingual tasks. Comparing to GLUE  (Wang et al., 2019), which is labeled in English for natural language understanding tasks only, XGLUE has two main advantages: (1) it provides 11 diversified tasks that cover both natural language understanding and generation scenarios; (2) for each task, it provides labeled data in multiple languages. We extend a recent cross-lingual pre-trained model Unicoder  to cover both understanding and generation tasks, which is evaluated on XGLUE as a strong baseline. We also evaluate the base versions (12-layer) of Multilingual BERT, XLM and XLM-R for comparison. 1   ", "sections": [{"heading": "Introduction", "text": "Pre-training + Fine-tuning has become a new NLP paradigm, where the general knowledge are firstly learnt from large-scale corpus by self-supervised learning and then transferred to downstream tasks by task-specific fine-tuning. Three different types of pre-trained models are explored recently, including monolingual pre-trained models (Radford et al., 2018;Devlin et al., 2019;Yang et al., 2019b;Lewis et al., 2019a), multilingual and cross-lingual pre-trained models (Devlin et al., 2019;Conneau and Lample, 2019; and multimodal pre-trained models (Lu et al., 2019;Li et al., 2020;. In this paper, we focus on the cross-lingual pretrained models, due to their importance to alleviating the low-resource issue among languages, where an NLP task often has rich training data in one language (such as English) but has few or no training data in other languages (such as French and German). In order to further advance the development of cross-lingual pre-trained models for various downstream tasks in different languages, this paper introduces XGLUE, a new benchmark dataset that can be used to: (i) train large-scale cross-lingual pre-trained models using multilingual and bilingual corpora, (ii) evaluate generalization capabilities of the cross-lingual pre-trained models across a diverse set of cross-lingual tasks.\nThe contribution of XGLUE is two-fold. First, it provides 11 diversified cross-lingual tasks covering both understanding and generation scenarios. XTREME ) is a concurrent work of XGLUE. But it includes cross-lingual understanding tasks only. Besides, XGLUE introduces 6 new tasks selected from Search, Ads and News scenarios,which makes XGLUE have more practical values. Second, an extended version of Unicoder  is described and evaluated as a strong cross-lingual pre-trained model baseline on XGLUE for both understanding and generation tasks. We also evaluate the base versions (12-layer) of Multilingual BERT (Devlin et al., 2019), XLM (Conneau and Lample, 2019) and XLM-R  for comparison.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "XGLUE Benchmark", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Pre-training Corpus", "text": "We collect two corpora, Small Corpus and Large Corpus, with different sizes for cross-lingual pretraining. Table 1 lists the data statistics.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Small Corpus (SC)", "text": "Multilingual Corpus We extract raw sentences from Wikipedia using WikiExtractor. It leads to a 101G multilingual corpus covering 100 languages.  Bilingual Corpus We use an in-house pipeline to extract bilingual sentence pairs from the Web, which leads to a 146G bilingual corpus covering 27 languages, including Arabic, Bulgarian, Danish, German, Greek, English, Spanish, Finnish, French, Hebrew, Hindi, Hungarian, Indonesian, Italian, Japanese, Korean, Dutch, Polish, Portuguese, Russian, Swedish, Swahili, Thai, Turkish, Urdu, Vietnamese and Chinese. All the bilingual pairs are English to another language.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Large Corpus (LC)", "text": "Multilingual Corpus Following , we construct a clean version of Common Crawl (CC) 2 as the multilingual corpus. First, we use a language identification model trained based on Wikipedia to classify the language of each page in CC. Then, we train a language model for each language using the corresponding part of the Wikipedia corpus, and use it to filter documents as  did. We use one CC dump for English and twelve CC dumps for other languages. It leads to a 2,500G multilingual corpus covering 89 languages. We also include the 101G multilingual corpus described in Section 2.1.1.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Bilingual Corpus", "text": "We reuse the bilingual corpus described in Section 2.1.1. We will add CCMatrix (Schwenk et al., 2019) in the future.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Downstream Tasks", "text": "We select 11 cross-lingual tasks in XGLUE, which are categorized into 3 groups: single-input understanding tasks, pair-input understanding tasks, and generation tasks. For each task, training set is only available in English. In order to obtain a good performance on XGLUE, a model should be able to learn how to do a task well using its English training set, and then transfer this ability to test sets in other languages. Table 2 gives the dataset statistics and Table 3 lists languages covered by all tasks.\n2 https://commoncrawl.org/.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Single-input Understanding Tasks", "text": "NER We select a subset of the following two NER tasks, CoNLL-2002NER (Sang, 2002 and CoNLL-2003 NER (Sang andDe Meulder, 2003), to form this cross-lingual NER dataset. It covers 4 languages, including English, German, Spanish and Dutch, and 4 types of named entities, including Person, Location, Organization and Miscellaneous entities that do not belong to the previous three types. F1 score is used as the metric.\nPOS Tagging (POS) Following (Kim et al., 2017), we select a subset of Universal Dependencies (UD) Treebanks (v2.5) (Zeman et al., 2019), which covers 18 languages. Accuracy (ACC) of the predicted POS tags is used as the metric.\nNews Classification (NC) This task aims to predict the category given a news article. It covers 5 languages, including English, Spanish, French, German and Russian. Each labeled instance is a 3-tuple: <news title, news body, category>. The category number is 10. We crawl this dataset from Microsoft News (MSN). Accuracy (ACC) of the multi-class classification is used as the metric.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Pair-input Understanding Tasks", "text": "MLQA The MLQA (Lewis et al., 2019b) is a multilingual machine reading comprehension task, which contains QA annotations labeled in 7 languages, including English, Arabic, German, Spanish, Hindi, Vietnamese and Chinese. F1 score of the predicted answers is used as the metric.\nXNLI We reuse the original XNLI dataset (Conneau et al., 2018) in XGLUE.\nPAWS-X The PAWS-X (Yang et al., 2019a) is a paraphrase identification dataset, which extends the Wikipedia portion of the PAWS (Zhang et al., 2019) evaluation to more languages. We select 4 languages, including English, Spanish, French and German, from the original dataset and use them in XGLUE. Accuracy (ACC) of the binary classification is used as the metric.\nQuery-Ad Matching (QADSM) This task aims to predict whether an advertisement (ad) is relevant to an input query. It covers 3 languages, including English, French and German. Each labeled instance is a 4-tuple: <query, ad title, ad description, label>. The label indicates whether the ad is relevant to the query (Good), or not (Bad). We con-  struct this dataset based on Bing. Accuracy (ACC) of the binary classification is used as the metric.\nWeb Page Ranking (WPR) This task aims to predict whether a web page is relevant to an input query. It covers 7 languages, including English, German, French, Spanish, Italian, Portuguese and Chinese. Each labeled instance is a 4-tuple: <query, web page title, web page snippet, label>. The relevance label contains 5 ratings: Perfect (4), Excellent (3), Good (2), Fair (1) and Bad (0). We construct this dataset based on Bing. Normalize Discounted Cumulative Gain (nDCG) is used as the metric.\nQA Matching (QAM) This task aims to predict whether a <question, passage> pair is a QA pair. It covers 3 languages, including English, French and German. Each labeled instance is a 3-tuple: <question, passage, label>. The label indicates whether the passage is the answer of the question\n(1), or not (0). We construct this dataset based on Bing. Accuracy (ACC) of the binary classification is used as the metric.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Generation Tasks", "text": "Question Generation (QG) This task aims to generate a question for a given passage. We collect <passage, question> pairs from Bing. It covers 6 languages, including English, French, German, Spanish, Italian and Portuguese. BLEU-4 score is used as the metric.\nNews Title Generation (NTG) This task aims to generate a proper title for a given news body.\nWe collect <news body, news title> pairs from Microsoft News (MSN). It covers 5 languages, including German, English, French, Spanish and Russian. BLEU-4 score is used as the metric.\n3 Pre-train Unicoder for Cross-lingual Understanding Tasks\nWe select Unicoder  as the backbone model. Section 3 introduces a simplified version of Unicoder using two pre-training tasks (MLN and TLM) for cross-lingual understanding tasks. Section 4 describes how to extend Unicoder to cover cross-lingual generation tasks. The original Unicoder  includes more pre-training tasks besides MLM and TLM. But to keep the baseline pre-trained model simple and to reduce the experimental cost, we just use MLM and TLM in this paper. It means for understanding tasks, Unicoder is almost equal to XLM, except some hyper-parameter differences.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Masked Language Model (MLM)", "text": "Following Devlin et al. (2019), this task extends the masked language model task to multiple languages. At each iteration, a batch is composed of sentences sampled from different languages. The sampling probability of a language l i is defined as\n\u03bb l i = p \u03b1 l i / l i p \u03b1 l i ,\nwhere p l i is the percentage of the language l i in the entire corpus, the smoothing factor \u03b1 is set to 0.3. For each batch, we randomly sample 15% of the words and replace them with (i) a special symbol [MASK], (ii) a random token or (iii) keep them unchanged with probability 80%, 10% and 10%, respectively. For each token, we only use its token embedding and position embedding, and discard segment embedding and language embedding.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Translation Language Model (TLM)", "text": "Following Conneau and Lample (2019), this task extends the MLM task to bilingual corpus. Given a bilingual sentence pair, TLM first concatenates them into a single sentence, and then masks words using the same strategy of MLM. The pre-trained model learns to recover each masked word based on the bilingual context. We follow MLM to sample language pairs in each batch with \u03b1 = 0.3. ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Multilingual Denoising Auto-Encoding (xDAE)", "text": "Motivated by BART (Lewis et al., 2019a), xDAE aims to predict the original text X = (x 1 , x 2 , ..., x |X| ) \u2208 l i from a language l i based on its corrupted form c(X), where c(X) is a noising function that corrupts an input text X as its output.\nFour different text noising strategies for c(\u2022) are explored in this paper. (1) Shuffle the input text X by adding a noise \u03b1 \u223c U(0, 3) to the input indices and then re-ordering X based on the rank of the noised indices. (2) Drop words with a probability of 0.1. (3) Replace 10% of the input words in X with the [MASK] symbol. (4) Sample a number of token spans from X with span lengths drawn from a Poisson distribution (\u03bb = 3), and then replace each token span with a single [MASK] token. Here, 0-length spans correspond to the insertion of [MASK] tokens. Based on the performance of different noising strategies (Table 10), we select (4) and use it in pre-training. We leave finding better text noising strategies for future work.\nWe train Unicoder using this task by maximizing the following loss function L xDAE :\nL xDAE = l i \u2208L X\u2208l i |X| t=1 log p(x t |x <t , c(X))\nwhere L = l 1 , ..., l N denotes N languages, X is an instance in the i th language l i , p(x t |x <t , c(X)) denotes the probability of generating a single token x t at time step t given c(X) and x <t .", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Multilingual Future N-gram Prediction (xFNP)", "text": "Motivated by ProphetNet (Yan et al., 2020), xFNP introduces a future n-gram prediction mechanism to natural language generation. It encourages the model to plan for the future tokens explicitly and prevents over-fitting on strong local correlations. Given an input text X = (x 1 , x 2 , ..., x |X| ) \u2208 l i from a language l i , we randomly mask k token spans of X to generate the masked text X as the input, and concatenate all masked token spans into Y as the output. Details of this mask strategy are described in Section 6.1. After this, xFNP first encodes X to H enc with the encoder:\nH enc = Encoder(X )\nThen, instead of predicting the next token only at each time step, xFNP generates n future tokens simultaneously at time step t with the decoder: p(y t |y <t , X ), ..., p(y t+n\u22121 |y <t , X ) = Decoder(y <t , H enc )\nFollowing Yan et al. (2020), we set n = 2.\nWe train Unicoder using this task by maximizing the following loss function L xF N P :\nL xF N P = l i \u2208L X\u2208l i {\u03b1 0 \u2022 |Y | t=1 log p(y t |y <t , X ) +\u03b1 1 \u2022 |Y |\u22121 t=1 log p(y t+1 |y <t , X )}\nwhere X and Y are generated from X based on the method mentioned above. Following Yan et al. (2020), we set \u03b1 0 = \u03b1 1 = 1.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Experiments", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Data Labeling", "text": "For tasks QADSM, WPR, QAM and QG, we label the data on an Microsoft internal crowdsourcing platform. Each labeler must learn the guideline and pass the labeling test. Each sample is labeled by three labeler. We only keep the samples with two or three labeler have same label.\nFor tasks NC and NTG, we directly use the category label on MSN website. All the category label on MSN is review by human.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Settings", "text": "Understanding Tasks The hyper-parameters are set as follows: 768 hidden units, 12 heads, GELU activation, a dropout rate of 0.1, 512 max input length, 12 layers in encoder.\nIn the pre-training stage, we first initialize Unicoder LC with XLM-R base , and then run continue pre-training with the accumulated 8,192 batch size with gradients accumulation. We use Adam Optimizer with a linear warm-up and set the learning rate to 3e-5. We select different understanding tasks randomly in different batches. This costed 12 days on 16 V100.\nIn the fine-tuning stage, the batch size is set to 32. We use Adam Optimizer (Kingma and Ba, 2014) with warm-up and set the learning rate to 5e-6. For all sentence classification tasks, we finetune 10 epochs. For POS Tagging and NER, we fine-tune 20 epochs. And for POS Tagging, we set the learning rate to 2e-5. For MLQA, we set the learning rate to 3e-5, batch size to 12 and train 2 epochs following BERT for SQuAD. After each epoch, we test the fine-tuned model on the dev sets of all languages. We select the model with the best average result on the dev sets of all languages. , the hyper-parameters are set as follows: 768 hidden units, 12 heads, GELU activation, a dropout rate of 0.1, 512 max input length, 12 layers in encoder, 12 layers in decoder.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Generation Tasks", "text": "In the pre-training stage, we first initialize encoder and decoder with XLM-R, and then run continue pre-training with 1,024 batch size. We use Adam optimizer with warm-up and set the learning rate to 2e-4. This costed 10 days on 16 V100.\nIn the fine-tuning stage, the batch size is 1024. We use Adam Optimizer with learning rate 1e-5 and warm-up steps 2000.\nFor Unicoder xF N P SC , the hyper-parameters are set as follows: 1,024 hidden size, 12 layers in encoder, 12 layers in decoder, 512 max input length.\nIn the pre-training stage, we pre-train the model from scratch and follow ProphetNet (Yan et al., 2020) to randomly mask a continuous span (with a fixed length 9) in every 64 tokens. About 15% of the tokens in original sequence are masked in this step. We use a special symbol [MASK] to replace 80% of the masked tokens, keep 10% unchanged, and random replace 10% of the masked tokens. We set the batch size to 1,024, training steps to 350,000. The learning rate is set to 1e-4. We set the number of future tokens n to 2.\nIn the fine-tuning stage, we use Adam Optimizer and set the learning rate to 1e-4. We set the batch size to 64 and the warm-up steps to 1,000.     (Devlin et al., 2019), XLM (Conneau and Lample, 2019) and XLM-R base   All models are (12-layer) based ones. Given a task, each pre-trained model is fine-tuned using its English training set only, and then applied to all test sets in different languages. AVG 2 U and AVG 2 G denote the average score of the average scores on 9 understanding tasks and 2 generation tasks, respectively.   12-layer Unicoder xF N P SC trained on Wikipedia corpus for 100 languages. Given a downstream task, each pre-trained model is fine-tuned using its English training set and then applied to all test sets in different languages. Note that, all results are reproduced by this paper, except the XLM \u2020 results on XNLI are from Conneau and Lample (2019).", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Main Result", "text": "QG M-BERT - - 0.1 - 7.8 0.1 0.1 - 0.2 - - 0.1 - - - - - - - 1.4 XLM-Rbase - - 0.1 - 6.0 0.0 0.0 - 0.1 - - 0.0 - - - - - - - 1.\nWe find (1) Unicoder LC performs slightly better than M-BERT and XLM-R base on the 9 understanding tasks, as it is pre-trained based on multilingual and bilingual corpora at the same time and uses TLM; . But it is not a fair comparison, because they use different text denoising tasks (sentence prediction vs. span prediction) and different generation mechanisms (single-token prediction vs. multi-token prediction). We leave combining these two tasks for future work.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ablation Study", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Pivot-language Fine-tuning", "text": "We define pivot-language (pl) fine-tuning as finetune a pre-trained model for a downstream task using its labeled data in a pivot language (e.g. English) and then apply the fine-tuned model to all languages. Table 4 chooses English as the pivot language, as all tasks in XGLUE have labeled data in English. But is English always the optimal choice? Will the results become better, if we do fine-tuning using other pivot languages?\nTo answer these questions, we evaluate Unicoder on XNLI and NTG using different pivot languages in fine-tuning and list comparison results in Table 5 and Table 6, respectively. (1) For each test set in language l i in Table 5 and Table 6, its best result is often achieved when the model is fine-tuned using l i as the pivot language; (2) For XNLI in Table 5, the best pivot languages are Spanish (es), Greek (el) and Turkish (tr), rather than English (en). For NTG in Table 6, the best pivot language is French (fr) for both Unicoder xDAE SC and Unicoder xF N P SC . It means the average quality of a cross-lingual pre-trained model could be further improved on a downstream task, by selecting a specific pivot language in finetuning.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Multi-language Fine-tuning", "text": "We define multi-language (ml) fine-tuning as finetune a pre-trained model for a downstream task using all its available labeled data in different languages. We evaluate Unicoder on XNLI and NTG using this fine-tuning method and list evaluation results in Table 7 and Table 8, respectively.\nWe find multi-language fine-tuning can achieve better results than pivot-language fine-tuning on both XNLI and NTG. It means the average quality of a cross-lingual pre-trained model could be significantly improved on a downstream task, by using combined labeled data in multiple languages.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Multi-task Fine-tuning", "text": "We define multi-task (mt) fine-tuning as fine-tune a pre-trained model for multiple downstream tasks using their combined labeled data. To reduce the experimental cost, we evaluate Unicoder on 5 understanding tasks: XNLI, PAWS-X, NC, QAM and QADSM, using their merged English labeled data in fine-tuning. Results are listed in Table 9.\nWe find PAWS-X and QADSM can benefit from the joint fine-tuning strategy, but XNLI, NC and QAM cannot. We leave discovering relationships between different tasks for better downstream task fine-tuning for future work.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Impacts of Text Noising Strategies", "text": "We investigate the impacts of different text noising strategies (Section 4.1) in Unicoder xDAE SC , and list comparison results in Table 10, where (1)+(2)+(3) denotes the result of using the first three strategies in pre-training, (4) denotes the result of using the last strategy in pre-training, (1)+(2)+(3)+(4) denotes the result of using all strategies in pretraining. To reduce experiment cost, we set max sequence length to 256 and only train 60K steps. We find that (4) can achieve the best average result on NTG. So all results of Unicoder xDAE SC reported in this paper is pre-trained using (4) only.    ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Dataset GLUE  includes 9 natural language understanding tasks that are labeled in English only. Comparing to GLUE, XGLUE not only expands task annotations to multiple languages, but also includes natural language generation tasks. XNLI (Conneau et al., 2018), NER (Sang, 2002;Sang and De Meulder, 2003), POS Tagging (Kim et al., 2017), MLQA (Lewis et al., 2019b) and PAWS-X (Yang et al., 2019a) are 5 multilingual datasets built for specific tasks.   XGLUE not only includes these 5 existing tasks, but also introduces 6 new tasks selected from real-world scenarios (i.e., Search, Ads and News). This makes XGLUE have more practical values. XTREME ) is a concurrent work of XGLUE. Comparing to it, XGLUE includes both understanding and generation tasks, which, to the best of our knowledge, is the first attempt in the cross-lingual dataset construction efforts.  ) is a RoBERTa -version XLM without using translation language model in pre-training. It is trained based on a much larger multilingual corpus (i.e. Com-mon Crawl) and become the new state-of-the-art on XNLI. In this paper, we use both the Common Crawl corpus and the bilingual corpus, aiming to build a stronger baseline model on XGLUE. BART (Lewis et al., 2019a) and ProphetNet (Yan et al., 2020) are two latest generative pre-trained models. We borrow ideas from these two works and extend Unicoder to cross-lingual generation tasks, which goes a step further to verify and explore different text generation approaches in the cross-lingual scenario.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Cross-lingual", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "We present XGLUE as a new cross-lingual benchmark and conduct comprehensive evaluations with interesting findings observed. We thank STC-A NLP, Bing Answers, Bing Ads, Bing Relevance and Microsoft News for providing the datasets.\nA The fine-tune parameters of Unicoder on XGLUE.   ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": " son, Bamba Dione, Peter Dirix, Kaja Dobrovoljc,  Timothy Dozat, Kira Droganova, Puneet Dwivedi,  Hanne Eckhoff, Marhaba Eli, Ali Elkahky, Binyam  Ephrem, Olga Erina, Toma\u017e Erjavec, Aline Etienne, Wograine Evelyn, Rich\u00e1rd Farkas, Hector  Fernandez Alcalde, Jennifer Foster, Cl\u00e1udia Freitas, Kazunori Fujita, Katar\u00edna Gajdo\u0161ov\u00e1, Daniel  Galbraith, Marcos Garcia, Moa G\u00e4rdenfors, Sebastian Garza, Kim Gerdes, Filip Ginter, Iakes  Goenaga, Koldo Gojenola, Memduh G\u00f6k\u0131rmak,  Yoav Goldberg, Xavier G\u00f3mez Guinovart, Berta  Gonz\u00e1lez Saavedra, Bernadeta Grici\u016bt\u0117, Matias Grioni, Normunds Gr\u016bz\u012btis, Bruno  ", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Uniter: Learning universal image-text representations. arXiv", "journal": "", "year": "2019", "authors": "Yen-Chun Chen; Linjie Li; Licheng Yu; Ahmed El Kholy; Faisal Ahmed; Zhe Gan; Yu Cheng; Jingjing Liu"}, {"title": "Cross-lingual natural language generation via pre-training", "journal": "", "year": "2019", "authors": "Zewen Chi; Li Dong; Furu Wei; Wenhui Wang; Xian-Ling Mao; Heyan Huang"}, {"title": "", "journal": "", "year": "2019", "authors": "Alexis Conneau; Kartikay Khandelwal; Naman Goyal; Vishrav Chaudhary; Guillaume Wenzek; Francisco Guzm\u00e1n; Edouard Grave; Myle Ott; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Crosslingual language model pretraining", "journal": "", "year": "2019", "authors": "Alexis Conneau; Guillaume Lample"}, {"title": "Xnli: Evaluating crosslingual sentence representations", "journal": "", "year": "2018", "authors": "Alexis Conneau; Guillaume Lample; Ruty Rinott; Adina Williams; R Samuel; Holger Bowman; Veselin Schwenk;  Stoyanov"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Unified language model pre-training for natural language understanding and generation", "journal": "NeurIPS", "year": "2019", "authors": "Li Dong; Nan Yang; Wenhui Wang; Furu Wei; Xiaodong Liu; Yu Wang; Jianfeng Gao; Ming Zhou; Hsiao-Wuen Hon"}, {"title": "Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization", "journal": "", "year": "2020", "authors": "Junjie Hu; Sebastian Ruder; Aditya Siddhant; Graham Neubig; Orhan Firat; Melvin Johnson"}, {"title": "Unicoder: A universal language encoder by pre-training with multiple cross-lingual tasks", "journal": "", "year": "2019", "authors": "Haoyang Huang; Yaobo Liang; Nan Duan; Ming Gong; Linjun Shou; Daxin Jiang; Ming Zhou"}, {"title": "Cross-lingual transfer learning for POS tagging without cross-lingual resources", "journal": "", "year": "2017", "authors": "Joo-Kyung Kim; Young-Bum Kim; Ruhi Sarikaya; Eric Fosler-Lussier"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2019", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal ; Abdelrahman Mohamed; Omer Levy; Ves Stoyanov; Luke Zettlemoyer"}, {"title": "Mlqa: Evaluating cross-lingual extractive question answering", "journal": "", "year": "2019", "authors": "Patrick Lewis; Barlas Oguz; Ruty Rinott; Sebastian Riedel; Holger Schwenk"}, {"title": "Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training", "journal": "AAAI", "year": "2020", "authors": "Gen Li; Nan Duan; Yuejian Fang; Ming Gong; Daxin Jiang; Zhou Zhou"}, {"title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks", "journal": "", "year": "2019", "authors": "Jiasen Lu; Dhruv Batra; Devi Parikh; Stefan Lee"}, {"title": "Improving language understanding by generative pre-training", "journal": "", "year": "2018", "authors": "Alec Radford; Karthik Narasimhan"}, {"title": "Introduction to the conll-2003 shared task: Languageindependent named entity recognition", "journal": "", "year": "2003", "authors": "Erik Tjong ; Kim Sang; Fien De Meulder"}, {"title": "Ef: Introduction to the conll-2002 shared task", "journal": "", "year": "2002", "authors": "Tjong Kim; Sang "}, {"title": "Ccmatrix: Mining billions of high-quality parallel sentences on the web. arXiv", "journal": "", "year": "2019", "authors": "Holger Schwenk; Guillaume Wenzek; Sergey Edunov; Edouard Grave; Armand Joulin"}, {"title": "Glue: A multi-task benchmark and analysis platform for natural language understanding", "journal": "", "year": "2019", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel Bowman"}, {"title": "Ccnet: Extracting high quality monolingual datasets from web crawl data", "journal": "", "year": "2019", "authors": "Guillaume Wenzek; Marie-Anne Lachaux; Alexis Conneau; Vishrav Chaudhary; Francisco Guzman; Armand Joulin; Edouard Grave"}, {"title": "Prophetnet: Predicting future ngram for sequence-to-sequence pre-training", "journal": "", "year": "2020", "authors": "Yu Yan; Weizhen Qi; Yeyun Gong; Dayiheng Liu; Nan Duan; Jiusheng Chen; Ruofei Zhang; Ming Zhou"}, {"title": "Paws-x: A cross-lingual adversarial dataset for paraphrase identification", "journal": "", "year": "2019", "authors": "Yinfei Yang; Yuan Zhang; Chris Tar; Jason Baldridge"}, {"title": "Xlnet: Generalized autoregressive pretraining for language understanding", "journal": "NeurIPS", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime Carbonell; Ruslan Salakhutdinov; V Quoc;  Le"}, {"title": "Aur\u00e9lie Collomb, \u00c7 agr\u0131 \u00c7\u00f6ltekin, Miriam Connor", "journal": "Maximilan Wendt", "year": "", "authors": "Daniel Zeman; Joakim Nivre; Mitchell Abrams; No\u00ebmi Aepli; \u017deljko Agi\u0107; Lars Ahrenberg; Gabriel\u0117 Aleksandravi\u010di\u016bt\u0117; Lene Antonsen; Katya Aplonova; Maria Jesus Aranzabe; Gashaw Arutie; Masayuki Asahara; Luma Ateyah; Mohammed Attia; Aitziber Atutxa; Liesbeth Augustinus; Elena Badmaeva; Miguel Ballesteros; Esha Banerjee; Sebastian Bank; Victoria Verginica Barbu Mititelu; Colin Basmov; John Batchelor; Sandra Bauer; Kepa Bellato; Yevgeni Bengoetxea;  Berzak; Ahmad Irshad; Riyaz Ahmad Bhat; Erica Bhat; Eckhard Biagetti; Agn\u0117 Bick; Rogier Bielinskien\u0117; Victoria Blokland; Lo\u00efc Bobicev; Emanuel Borges Boizou; Carl V\u00f6lker; Cristina B\u00f6rstell; Gosse Bosco; Sam Bouma; Adriane Bowman; Kristina Boyd; Aljoscha Brokait\u0117; Marie Burchardt; Bernard Candito; Gauthier Caron; Tatiana Caron; G\u00fcl\u015fen Cavalcanti; Flavio Cebiroglu Eryigit; Giuseppe G A Massimiliano Cecchini;  Celano; Savas Slavom\u00edr\u010d\u00e9pl\u00f6; Fabricio Cetin; Jinho Chalub; Yongseok Choi; Jayeol Cho; Alessandra T Chun; Silvie Cignarella; Jana Cinkov\u00e1 ; Milan Straka; Alane Strnadov\u00e1; Umut Suhr; Shingo Sulubacak; Zsolt Suzuki; Dima Sz\u00e1nt\u00f3; Yuta Taji; Fabio Takahashi; Takaaki Tamburini; Isabelle Tanaka; Guillaume Tellier; Liisi Thomas; Trond Torga; Mary Trosterud ; Alina Wr\u00f3blewska; Naoki Yako; Chunxiao Yamazaki; Koichi Yan;  Yasuoka; M Marat; Zhuoran Yavrumyan;  Yu; Amir Zden\u011bk\u017eabokrtsk\u00fd; Manying Zeldes;  Zhang"}, {"title": "Paws: Paraphrase adversaries from word scrambling", "journal": "", "year": "2019", "authors": "Yuan Zhang; Jason Baldridge; Luheng He"}, {"title": "Unified vision-language pre-training for image captioning and vqa", "journal": "AAAI", "year": "2020", "authors": "Luowei Zhou; Hamid Palangi; Lei Zhang; Houdong Hu; Jason Corso; Jianfeng Gao"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "7cross-lingual pre-trained models are evaluated on XGLUE and compared in", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "as baselines. Unicoder SC and Unicoder LC are pre-trained using small corpus and large corpus, respectively. Unicoder xDAE SC and Unicoder xF N P SC are pre-trained by xDAE and xFNP for 100 languages, respectively. For the results of M-BERT/XLM-R on generation tasks, we initialize the encoder-decoder model with M-BERT/XLM-R and fine-tune it on each downstream task without pre-training.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_2", "figure_caption": "(2) Unicoder LC performs better than Unicoder SC , as it is pre-trained based on the larger corpus; (3) Unicoder xDAE SC and Unicoder xF N P SC show good cross-lingual transfer capabilities and perform significantly better than M-BERT and XLM-R base on the 2 generation tasks. It proves the importance of introducing generation tasks into pre-training for cross-lingual text generation; (4) Unicoder xF N P SC performs slightly better than Unicoder xDAE SC", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Pre-trained Model Multilingual BERT (M-BERT) (Devlin et al., 2019) performs pre-training based on the multilingual corpus with the masked language model task. By sharing the model parameters and the vocabulary for all languages, M-BERT can obtain the cross-lingual capability over 102 languages. XLM (Conneau and Lample, 2019) performs cross-lingual pre-training based on multilingual corpus and bilingual corpus, by introducing the translation language model task into pre-training. Based on XLM, Unicoder (Huang et al., 2019) uses more cross-lingual pretraining tasks and achieves better results on XNLI. XLM-R", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "The statistics of two pre-training corpora.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Task# of Languages |Train| en |Dev| avg |Test| avg For each task, training set is only available in English. |Train| en denotes the number of labeled instances in the training set. |Dev| avg and |Test| avg denote the average numbers of labeled instances in the dev sets and test sets, respectively. * denotes the corresponding dataset is constructed by this paper.", "figure_data": "MetricData SourceNER415.0K2.8K3.4KF1ECI Multilingual Text CorpusPOS1825.4K1.0K0.9KACCUD Tree-banks (v2.5)NC  *5100K10K10KACCMSNMLQA787.6K0.6K5.7KF1WikipediaXNLI15433K2.5K5KACCMultiNLI CorpusPAWS-X449.4K2K2KACCWikipediaQADSM  *3100K10K10KACCBingWPR  *7100K10K10KnDCGBingQAM  *3100K10K10KACCBingQG  *6100K10K10KBLEU-4BingNTG  *5300K10K10KBLEU-4MSNTable 2: 11 downstream tasks in XGLUE. Task ar bg de el en es fr hi it nl pl pt ru sw th tr ur vi zhNERPOSNC  *MLQAXNLIPAWS-XQADSM"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": The 19 languages covered by the 11 downstream tasks: Arabic (ar), Bulgarian (bg), German (de), Greek(el), English (en), Spanish (es), French (fr), Hindi (hi), Italian (it), Dutch (nl), Polish (pl), Portuguese (pt), Russian(ru), Swahili (sw), Thai (th), Turkish (tr), Urdu (ur), Vietnamese (vi), and Chinese (zh). All these 6 new tasks with  *are labeled by human, except es, it and pt datasets in QG (80+% accuracy) are obtained by an in-house QA ranker."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "We evaluate Unicoder xDAE SC", "figure_data": "and Unicoder xF N P SC For Unicoder xDAE as two separate models. SC"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ": 12-layer M-"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "The overall evaluation results on XGLUE. We use M-BERT", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Impacts of different pivot languages on XNLI. Given each pivot language, the corresponding fine-tuned XNLI results on all languages are listed in the same row. Each bolded number is the best result in that column.", "figure_data": "PivotenesfrderuAVGen15.6/15.89.0/11.98.7/9.96.8/7.57.7/8.49.6/10.7es7.8/8.817.1/17.1 10.6/10.97.6/8.08.0/8.610.2/10.7fr8.2/8.711.4/12.5 19.4/20.98.3/8.27.6/7.811.0/11.6de8.2/8.69.9/11.29.5/10.214.1/13.78.4/8.010.0/10.3ru6.9/7.49.3/10.88.8/9.96.9/7.016.6/16.79.7/10.4"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Impacts of different pivot languages on NTG.", "figure_data": "Unicoder xDAE SC/Unicoder xF N P SCevaluated by BLEU-4."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Rbase (pl) 84.6 78.2 79.2 77.0 75.9 77.5 75.5 72.9 72.1 74.8 71.6 73.7 69.8 64.7 65.1 74.2 XLM-Rbase (ml) 85.7 81.5 82.5 81.2 79.7 81.7 80.0 79.0 77.1 80.1 77.9 79.2 76.5 73.0 71.3 79.1 UnicoderLC (pl) 85.4 79.2 79.8 78.2 77.3 78.5 76.7 73.8 73.9 75.9 71.8 74.7 70.1 67.4 66.3 75.3 UnicoderLC (ml) 85.8 81.9 82.3 81.5 80.8 82.0 79.9 78.7 78.1 80.2 78.4 79.3 76.2 73.2 72.4 79.4", "figure_data": "enfresdeelbgrutrarvithzhhiswurAVGXLM-"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "Impact of multi-language fine-tuning on XNLI. pl and ml denote pivot-language fine-tuning (English as pivot) and multi-language fine-tuning, respectively.", "figure_data": "ModelenesfrderuAVGUnicoder xDAE SC(pl)15.69.08.76.87.79.6Unicoder xDAE SC(ml) 18.5 18.3 28.2 15.5 33.4 22.8Unicoder xF N P SC(pl)15.8 11.99.97.58.410.7Unicoder xF N P SC(ml) 15.6 17.1 19.1 13.9 15.8 16.3"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "Impact of multi-language fine-tuning on NTG. pl and ml denote pivot-language fine-tuning (English as pivot) and multi-language fine-tuning, respectively. BLUE-4 is the metric.", "figure_data": "ModelXNLI PAWS-XNCQAM QADSM AVGUnicoderLC (pl)75.390.183.568.968.477.2UnicoderLC (mt)74.490.283.468.769.077.1"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "Impacts of multi-task fine-tuning on XNLI, PAWS-X, NC, QAM and QADSM. pl and mt denote pivot-language fine-tuning (English as pivot) on each task and multi-task fine-tuning, respectively.", "figure_data": "We also compare Unicoder xDAE SCwith XNLG(Chi et al., 2019) on the Abstractive Summariza-tion task. For fairly comparison, we implementxDAE in same code base and use same pre-traininglanguages as XNLG. The zero-shot comparison re-sults are listed in Table 11. We can see that by usingxDAE only in pre-training, Unicoder xDAE SCcan out-perform XNLG significantly, which is pre-trainedusing 4 tasks including MLM, DAE, XMLM andXAE. It verifies the effectiveness of the fourth textnoising strategy described in Section 4.1 for gener-ation tasks."}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_17", "figure_caption": "", "figure_data": ": Impact of different text noising strategieson NTG using pivot-language fine-tuning (English aspivot). BLUE-4 is the metric.ModelfrzhAVGXNLG (Chi et al., 2019) 36.3 38.9 37.6Unicoder xDAE SC37.9 42.2 40.1"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_18", "figure_caption": "The zero-shot results on Abstractive Summarization. Unicoder xDAE SC and XNLG are fine-tuned using English labeled data. ROUGE-L is the metric.", "figure_data": ""}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_20", "figure_caption": "The fine-tune parameters of understanding tasks.", "figure_data": "TaskModelbatch size learning rate warm up stepsQGUnicoder xDAE SC641e-41000NTG Unicoder xDAE SC641e-41000QGUnicoder xF N P SC10241e-52000NTG Unicoder xF N P SC10241e-52000"}, {"figure_label": "13", "figure_type": "table", "figure_id": "tab_21", "figure_caption": "The fine-tune parameters of generation tasks.", "figure_data": ""}], "doi": ""}