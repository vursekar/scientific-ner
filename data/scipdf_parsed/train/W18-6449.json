{"authors": "Brian Tubay; Marta R Costa-Juss\u00e0", "pub_date": "", "title": "Neural Machine Translation with the Transformer and Multi-Source Romance Languages for the Biomedical WMT 2018 task", "abstract": "The Transformer architecture has become the state-of-the-art in Machine Translation. This model, which relies on attention-based mechanisms, has outperformed previous neural machine translation architectures in several tasks. In this system description paper, we report details of training neural machine translation with multi-source Romance languages with the Transformer model and in the evaluation frame of the biomedical WMT 2018 task. Using multi-source languages from the same family allows improvements of over 6 BLEU points.", "sections": [{"heading": "Introduction", "text": "Neural Machine Translation (NMT) (Bahdanau et al., 2015) proved to be competitive with the encoder-decoder architecture based on recurrent neural networks and attention. After this architecture, new proposals based on convolutional neural networks (Gehring et al., 2017) or only attention-based mechanisms  appeared. The latter architecture has achieved great success in Machine Translation (MT) and it has already been extended to other tasks such as Parsing , Speech Recognition 1 , Speech Translation (Cros et al., 2018), Chatbots  among others.\nHowever, training with low resources is still a big drawback for neural architectures and NMT is not an exception (Koehn and Knowles, 2017). To face low resource scenarios, several techniques have been proposed, like using multi-source (Zoph and Knight, 2016), multiple languages (Johnson et al., 2017) or unsupervised techniques (Lample et al., 2018;Artetxe et al., 2018), among many others.\nIn this paper, we use the Transformer enhanced with the multi-source technique to participate in the Biomedical WMT 2018 task, which can be somehow considered a low-resourced task, given the large quantity of data that it is required for NMT. Our multi-source enhancement is done only with Romance languages. The fact of using similar languages in a multi-source system may be a factor towards improving the final system which ends up with over 6 BLEU points of improvement over the single source system.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "The Transformer architecture", "text": "The Transformer model is the first NMT model relying entirely on self-attention to compute representations of its input and output without using recurrent neural networks (RNN) or convolutional neural networks (CNN).\nRNNs read one word at a time, having to perform multiple steps before generating an output that depends on words that are far away. But it has been demonstrated that the more steps required, the harder it is to the network to learn how to make these decisions (Bahdanau et al., 2015). In addition, given the sequential nature of the RNNs, it is difficult to fully take advantage of modern computing devices such as Tensor Processing Units (TPUs) or Graphics Processing Units (GPUs) which rely on parallel processing. The Transformer is an encoder-decoder model that was conceived to solve these problems.\nThe encoder is composed of three stages. In the first stage input words are projected into an embedded vector space. In order to capture the notion of token position within the sequence, a positional encoding is added to the embedded input vectors. Without positional encodings, the output of the multi-head attention network would be the same for the sentences \"I love you more than her\" and \"I love her more than you\". The second stage is a multi-head self-attention. Instead of computing a single attention, this stage computes multiple attention blocks over the source, concatenates them and projects them linearly back onto a space with the initial dimensionality. The individual attention blocks compute the scaled dot-product attention with different linear projections. Finally a position-wise fully connected feed-forward network is used, which consists of two linear transformations with a ReLU activation (Vinod Nair, 2010)   The decoder operates similarly, but generates one word at a time, from left to right. It is composed of five stages. The first two are similar to the encoder: embedding and positional encoding and a masked multi-head self-attention, which unlike in the encoder, forces to attend only to past words. The third stage is a multi-head attention that not only attends to these past words, but also to the final representations generated by the encoder. The fourth stage is another position-wise feed-forward network. Finally, a softmax layer allows to map target word scores into target word probabilities. For more specific details about the architecture, refer to the original paper .", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Multi-Source translation", "text": "Multi-source translation consists in exploiting multiple text inputs to improve NMT (Zoph and Knight, 2016). In our case, we are using this approach in the Transformer architecture described above and using only inputs from the same language family.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experiments", "text": "In this section we report details on the database, training parameters and results.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Databases and Preprocessing", "text": "The experimental framework is the Biomedical Translation Task (WMT18) 2 . The corpus used to train the model are the one provided for the task for the selected languages pairs: Spanishto-English (es2en), French-to-English (fr2en) and Portuguese-to-English (pt2en).\nSources are mainly from Scielo and Medline and detailed in Table 3.  Validation sets were taken from Khresmoi development data 3 , as recommended in the task description. Each validation dataset contains 500 sentence pairs. Test sets were the ones provides by the task for the previous year competition (WMT17 4 ).\nPreprocessing relied on three basic steps: tokenization, truecasing and limiting sentence length to 80 words. Words were segmented by means of Byte-Pair Encoding (BPE) (Sennrich et al., 2015).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Parameters", "text": "The system was implemented using OpenNMT in PyTorch (Klein et al., 2017) ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Spanish", "text": "Utilizando la base de datos Epistemonikos, la cual es mantenida mediante bsquedas realizadas en 30 bases de datos, identificamos seis revisiones sistemticas que en conjunto incluyen 36 estudios aleatorizados pertinentes a la pregunta.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Single-Language", "text": "Using the Epistemonikos database, which is maintained through searches in 30 databases, we identified six systematic reviews including 36 randomized studies relevant to the question.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Multi-Language", "text": "Using the Epistemonikos database, which is maintained through searches in 30 databases, we identified six systematic reviews that altogether include 36 randomized studies relevant to the question.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Portuguese", "text": "Os resultados dos modelos de regresso mostraram associao entre os fatores de correo estimados e os indicadores de adequao propostos", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Single-Language", "text": "Regression models showed an association between estimated correction factors and the proposed adequacy indicators.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Multi-Language", "text": "The results of the regression models showed an association between the estimated correction factors and the proposed adequacy indicators.   We trained three single-language systems, one for each language pair. We required 14 epochs for the Spanish-to-English system (7 hours of training), 16 epochs for the French-to-English system (9 hours of training), and 17 epochs for the Portuguese-to-English system (7 hours of training). For the multi-source system, which concatenated the three parallel corpus together, we required 11 epochs (23 hours of training). We stopped training when the validation accuracy did not increase in two consecutive epochs.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results", "text": "Best ranking systems from WMT17 and WMT18 are shown in Table 1, except for French-to-English WMT17 since the references for this set are not available. For this pair, we used 1000 sentences from the Khresmoi development data. Table 1 shows BLEU results for the baseline systems, the single-language and multi-source approaches.\nThe Transformer architecture outperforms WMT17 best system. Results become even better with the system is trained with the common corpus of Romance languages, what we call the multi-source approach. The latter is consistent with the universal truth that more data equals better results, even if the source language is not the same.\nFinally, Table 2 shows some examples of the output translations.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusions", "text": "The main conclusions of our experiments are that the multi-source inputs of the same family applied to the Transformer architecture can improve the single input. Best improvements achieve an increase of 6 BLEU points in translation quality.\npart by the Spanish Ministerio de Econom\u00eda y Competitividad, the European Regional Development Fund and the Agencia Estatal de Investigaci\u00f3n, through the postdoctoral senior grant Ram\u00f3n y Cajal, the contract TEC2015-69266-P (MINECO/FEDER,EU) and the contract PCIN-2017-079 (AEI/MINECO).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "Authors would like to thank Noe Casas for his valuable comments. This work is supported in", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Unsupervised neural machine translation", "journal": "", "year": "2018", "authors": "Mikel Artetxe; Gorka Labaka; Eneko Agirre; Kyunghyun Cho"}, {"title": "Neural machine translation by jointly learning to align and translate", "journal": "CoRR", "year": "2015", "authors": "Dzmitry Bahdanau; Kyunghyun Cho; Yoshua Bengio"}, {"title": "Experimental research on encoder-decoder architectures with attention for chatbots", "journal": "", "year": "2018", "authors": "Marta R Costa-Juss\u00e0; \u00c1lvaro Nuez; Carlos Segura"}, {"title": "End-to-end speech translation with the transformer", "journal": "", "year": "2018", "authors": "Laura Cros; Carlos Escolano; A R Jos\u00e9; Marta R Fonollosa;  Costa-Juss\u00e0"}, {"title": "Convolutional sequence to sequence learning", "journal": "CoRR", "year": "2017", "authors": "Jonas Gehring; Michael Auli; David Grangier; Denis Yarats; Yann N Dauphin"}, {"title": "Google's multilingual neural machine translation system: Enabling zero-shot translation", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Melvin Johnson; Mike Schuster; Quoc V Le; Maxim Krikun; Yonghui Wu; Zhifeng Chen; Nikhil Thorat; Fernanda Vi\u00e9gas; Martin Wattenberg; Greg Corrado; Macduff Hughes; Jeffrey Dean"}, {"title": "One model to learn them all", "journal": "", "year": "2017", "authors": "Lukasz Kaiser; Aidan N Gomez; Noam Shazeer; Ashish Vaswani; Niki Parmar; Llion Jones; Jakob Uszkoreit"}, {"title": "OpenNMT: Open-source toolkit for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Guillaume Klein; Yoon Kim; Yuntian Deng; Jean Senellart; Alexander Rush"}, {"title": "Six challenges for neural machine translation", "journal": "", "year": "2017", "authors": "Philipp Koehn; Rebecca Knowles"}, {"title": "", "journal": "", "year": "", "authors": "Guillaume Lample; Ludovic Denoyer"}, {"title": "Unsupervised machine translation using monolingual corpora only", "journal": "", "year": "2018", "authors": "Aurelio Marc;  Ranzato"}, {"title": "Neural machine translation of rare words with subword units", "journal": "", "year": "2015", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Attention is all you need", "journal": "Curran Associates, Inc", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Illia Kaiser;  Polosukhin"}, {"title": "Rectified linear units improve restricted boltzmann machines", "journal": "", "year": "2010", "authors": "Geoffrey E Hinton Vinod;  Nair"}, {"title": "Multi-source neural translation", "journal": "", "year": "2016", "authors": "Barret Zoph; Kevin Knight"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Simplified diagram of the Transformer model", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "French(Traduit par Docteur Serge Messier). Single-Language [Doctor Serge Messier]. Multi-Language [(Translated by Doctor Serge Messier)].", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "in between.", "figure_data": "OutputProbabilitiesSoftmaxFeed ForwardFeed ForwardMulti-Head AttentionMulti-HeadMasked Multi-HeadSelf-AttentionSelf-AttentionEmbeddingEmbedding&&Positional EncodingPositional EncodingEncoderDecoderInputsTargets"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Trained systems results for WMT17 and WMT18 official test sets.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "", "figure_data": ": Spanish/Portuguese/French to English examples for WMT18were trained with same architecture and parame-ters.HparamText-to-TextEncoder layers6Decoder layers6Batch size4096Adam optimizer \u03b2 1 = 0.9 \u03b2 2 = 0.998Attention heads 8"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Training parameters.", "figure_data": ""}], "doi": "10.18653/v1/W18-64076"}