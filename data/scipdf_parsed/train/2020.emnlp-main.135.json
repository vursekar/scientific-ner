{"authors": "Suzanna Sia; Ayush Dalmia; Sabrina J Mielke", "pub_date": "", "title": "Tired of Topic Models? Clusters of Pretrained Word Embeddings Make for Fast and Good Topics too!", "abstract": "Topic models are a useful analysis tool to uncover the underlying themes within document collections. The dominant approach is to use probabilistic topic models that posit a generative story, but in this paper we propose an alternative way to obtain topics: clustering pretrained word embeddings while incorporating document information for weighted clustering and reranking top words. We provide benchmarks for the combination of different word embeddings and clustering algorithms, and analyse their performance under dimensionality reduction with PCA. The best performing combination for our approach performs as well as classical topic models, but with lower runtime and computational complexity.", "sections": [{"heading": "Introduction", "text": "Topic models are the standard approach for exploratory document analysis (Boyd-Graber et al., 2017), which aims to uncover main themes and underlying narratives within a corpus. But in times of distributed and even contextualized embeddings, are they the only option?\nThis work explores an alternative to topic modeling by casting 'key themes' or 'topics' as clusters of word types under the modern distributed representation learning paradigm: unsupervised pre-trained word embeddings provide a representation for each word type as a vector, allowing us to cluster them based on their distance in high-dimensional space. The goal of this work is not to strictly outperform, but rather to benchmark standard clustering of modern embedding methods against the classical approach of Latent Dirichlet Allocation (LDA;Blei et al., 2003).\nWe restrict our study to influential embedding methods and focus on centroid-based clustering algorithms as they provide a natural way to obtain the top words in each cluster based on distance from the cluster center. 1 Aside from reporting the best performing combination of word embeddings and clustering algorithm, we are also interested in whether there are consistent patterns: embeddings which perform consistently well across clustering algorithms might be good representations for unsupervised document analysis, clustering algorithms that perform consistently well are more likely to generalize to future word embedding methods.\nTo make our approach reliably work as well as LDA, we incorporate corpus frequency statistics directly into the clustering algorithm, and quantify the effects of two key methods, 1) weighting terms during clustering and 2) reranking terms for obtaining the top J representative words. Our contributions are as follows:\n\u2022 We systematically apply centroid-based clustering algorithms on top of a variety of pretrained word embeddings and embedding methods for document analysis.\n\u2022 Through weighted clustering and reranking of top words we obtain sensible topics; the best performing combination is comparable with LDA, but with smaller time complexity and empirical runtime.\n\u2022 We show that further speedups are possible by reducing the embedding dimensions by up to 80% using PCA.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Related Work and Background", "text": "Analyzing documents by clustering word embeddings is a natural idea-clustering has been used for readability assessment (Cha et al., 2017), argument mining (Reimers et al., 2019), document classification and document clustering (Sano et al., 2017), inter alia. So far, however, clustering word embeddings has not seen much success for the purposes of topic modeling. While many modern efforts have attempted to incorporate word embeddings into the probabilistic LDA framework (Liu et al., 2015;Nguyen et al., 2015;Das et al., 2015;Batmanghelich et al., 2016;Xun et al., 2017;Dieng et al., 2019), relatively little work has examined the feasibility of clustering embeddings directly. Xie and Xing (2013) and Viegas et al. (2019) first cluster documents and subsequently find words within each cluster for document analysis. Sridhar (2015) targets short texts where LDA performs poorly in particular, fitting GMMs to learned word2vec representations. De Miranda et al. ( 2019) cluster using self-organising maps, but provide only qualitative results.\nIn contrast, our proposed approach is straightforward to implement, feasible for regular length documents, requires no retraining of embeddings, and yields qualitatively and quantitatively convincing results. We focus on centroid based k-means (KM), Spherical k-means (SK), and k-medoids (KD) for hard clustering, and von Mises-Fisher Models (VMFM) and Gaussian Mixture Models (GMM) for soft clustering; as pre-trained embeddings we consider word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017, Spherical (Meng et al., 2019), ELMo (Peters et al., 2018), andBERT (Devlin et al., 2018).", "n_publication_ref": 17, "n_figure_ref": 0}, {"heading": "Methodology", "text": "After preprocessing and extracting the vocabulary from our training documents, each word type is converted to its embedding representation (averaging all of its tokens for contextualized embeddings; details in \u00a75.3). Following this we apply the various clustering algorithms on the entire training corpus vocabulary to obtain k clusters, using weighted ( \u00a73.2) or unweighted word types. After the clustering algorithm has converged, we obtain the top J words ( \u00a73.1) from each cluster for evaluation. Note that one potential shortcoming of our approach is the possibility of outliers forming their own cluster, which we leave to future work.\nFigure 1: The figure on the left shows the cluster center ( ) without weighting, while the figure on the right shows that after weighting (larger points have higher weight) a hopefully more representative cluster center is found. Note that top words based on distance from the cluster center could still very well be low frequency word types, motivating reranking ( \u00a73.3).", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Obtaining top-J words", "text": "In traditional topic modeling (LDA), the top J words are those with highest probability under each topic-word distribution. For centroid based clustering algorithms, the top words of some cluster i are naturally those closest to the cluster center c (i) , or with highest probability under the cluster parameters. Formally, this means choosing the set of types J as\nargmin J : |J|=10 j\u2208J \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 c (i) \u2212 x j 2 2 for KM/KD, cos(c (i) , x j ) for SK, f (x j | c (i) , \u03a3 i ) for GMM/VMFM.\nOur results in \u00a76 focus on KM and GMM, as we observe that k-medoids, spherical KM and von Mises-Fisher tend to perform worse than KM and GMM (see App. A, App. B).\nNote that it is possible to extend this approach to obtain the top topics given a document: compute similarity scores between learned topic cluster centers and all word embeddings from that particular document, and normalize them using softmax to obtain a (non-calibrated) probability distribution.\nCrucial to our method is the incorporation of corpus statistics on top of vanilla clustering algorithms, which we will describe in the remainder of this section.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Weighting while clustering", "text": "The intuition of weighted clustering is based on the formulation of classical LDA which models the probability of the word type t belonging to a topic i as\nN t,i +\u03b2t t N t i +\u03b2 t\n, where N t,i refers to the number of times word type t has been assigned to topic i, and \u03b2 is a parameter of the Dirichlet prior on the pertopic word distribution. In our case, illustrated by the schematic in Fig. 1, weighting is a natural way to account for the frequency effects of vocabulary terms during clustering.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Reranking when obtaining topics", "text": "When obtaining the top-J words that make up a cluster's topic, we also consider reranking terms, as there is no guarantee that words closest to cluster centers are important word types. We will show in Table 2 that without reranking, clustering yields \"sensible\" topics but low NPMI scores.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Which corpus statistics?", "text": "To incorporate corpus statistics into the clustering algorithm, we examine three different schemes 2 to assign weights to word types, where n t is the count of word type t in corpus D, and d is a document:\ntf = n t t n t (1) tf-df = tf \u2022 |{d \u2208 D | t \u2208 d}| |D| (2) tf-idf = tf \u2022 log |D| |{d \u2208 D | t \u2208 d}| + 1 (3)\nThese scores can now be used for weighting word types when clustering ( w ), reranking top 100 words ( r ) after, both ( w r ), or neither (simply ). We find that simply using tf outperforms the other weighting schemes (App. C). Our results and subsequent analysis in \u00a76 uses tf for weighting and reranking.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Computational Complexity", "text": "The complexity of KM is O(tknm), and of GMM is O(tknm 3 ), for t iterations, 3 k clusters (topics), n word types (unique vocabulary), and m embedding dimensions. Weighted variants have a oneoff cost of weight initialization, and contribute a constant factor when recalulculating the centroid during clustering. Reranking has an additional O(n \u2022 log(n k )) factor, where n k is the average number of elements in a cluster. In contrast, LDA via collapsed Gibbs sampling has a complexity of O(tkN ), where N is the number of all tokens, so when N n, clustering methods can potentially achieve better performance-complexity tradeoffs.\nNote that running ELMo and BERT over documents also requires iterating over all tokens, but only once, and not for every topic and iteration.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Cost of obtaining Embeddings", "text": "For readily available pretrained word embeddings such as word2vec, FastText, GloVe and Spherical, the embeddings can be considered as 'given' as the practioner does not need to generate these embeddings from scratch. However for contextual embeddings such as ELMo and BERT, there is additional computational cost in obtaining these embeddings before clustering, which requires passing through RNN and transformer layers respectively. This can be trivially parallelised by batching the context window (usually a sentence). We use standard pretrained ELMo and BERT models in our experiments and therefore do not consider the runtime of training these models from scratch.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Setup", "text": "Our implementation is freely available online. 4", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Datasets", "text": "We use the 20 newsgroup dataset (20NG) which contains around 18000 documents and 20 categories, 5 and a subset of Reuters21578 6 which contains around 10000 documents.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Evaluation (Topic Coherence)", "text": "We adopt a standard 60-40 train-test split for 20NG and 70-30 for Reuters.\nThe top 10 words ( \u00a73.1) were evaluated using normalized pointwise mutual information (NPMI; Bouma, 2009) which has been shown to correlate with human judgements (Lau et al., 2014). NPMI ranges from [\u22121, 1] with 1 indicating perfect association. The train split is used to obtain the top topic words in an unsupervised fashion (we do not use any document labels), and the test split is used to evaluate the \"topic coherence\" of these top words. NPMI scores are averaged across all topics.\nFor both datasets we use 20 topics; which gives best NPMI out of 20, 50, 100 topics for Reuters, and is the ground truth number for 20NG. The  NPMI scores presented in Table 1 are averaged across cluster centers initialized using 5 random seeds.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Preprocessing", "text": "We lowercase tokens, remove stopwords, punctuation and digits, and exclude words that appear in less than 5 documents and appear in long sentences of more than 50 words, removing email artifacts and noisy token sequences which are not valid sentences. An analysis on the effect of rare word removal can be found in \u00a76.2. For contextualized word embeddings (BERT and ELMo), sentences served as the context window to obtain the token representations. Subword representations were averaged for BERT, which performs better than just using the first subword.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results and Discussion", "text": "Our main results are shown in Table 1.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Runtime", "text": "Running LDA with MALLET (McCallum, 2002) takes a minute, but performs no better than KM w r , which takes little more than 10 seconds on CPU using sklearn (Pedregosa et al., 2011), and 3-4 seconds using a simple implementation using JAX (Bradbury et al., 2018) on GPU.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Weighting", "text": "From Table 1, we see that reranking and weighting greatly improves clustering performance across different embeddings. As a first step to uncover why, we investigate how sensitive our methods are to restricting the clustering to only frequently appearing word types. Visualized in Fig. 3, we find that as we vary the cutoff term frequency, thus changing the vocabulary size and allowing more rare words on  the x-axis, NPMI is more affected for the models without reweighting. This suggests that reweighting using term frequency is effective for clustering without the need for ad-hoc restriction of infrequent terms-without it, all combinations perform poorly compared to LDA. In general, GMM outperforms KM for both weighted and unweighted variants averaged across all embedding methods (p < 0.05). 7", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Reranking", "text": "For KM, extracted topics before reranking results in reasonable looking themes, but scores poorly on NPMI. Reranking strongly improves KM on average (p < 0.02) for both Reuters and 20NG. Examples before and after reranking are provided in Table 2. This indicates that while cluster centers are centered around valid themes, they are surrounded by low frequency word types. We observe that when applying reranking to GMM w the gains are much less pronounced than KM w . The top topic words before and after reranking for BERT-GMM w have an average Jaccard similarity score of 0.910, indicating that the cluster centers learned by weighted GMMs are already centered at word types of high frequency in the training corpus.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Embeddings", "text": "Spherical embeddings and BERT perform consistently well across both datasets. For 20NG, KM w r Spherical and LDA both achieve 0.26 NPMI. For Reuters, GMM w r BERT achieves the top NPMI score of 0.15 compared to 0.12 of LDA. Word2vec and ELMo (using only the last layer 8 ) perform poorly compared to the other embeddings. Fast-Text and GloVe can achieve similar performance to BERT on 20NG but are slightly inferior on Reuters.\nTraining or fine-tuning embeddings on the given data prior to clustering could potentially achieve better performance, but we leave this to future work.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Qualitative results", "text": "We find that our approach yields a greater diversity within topics as compared to LDA while achieving comparable coherence scores (App. D). Such topics are arguably more valuable for exploratory analysis.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dimensionality Reduction", "text": "We apply PCA to the word embeddings before clustering to investigate the amount of redundancy in the dimensions of large embeddings, which impact clustering complexity ( \u00a74). With reranking, the dimensions of all embeddings can be reduced by more than 80% (Fig. 2).\nWe observe that KM w r can consistently reduce the number of dimensions across different embedding types without loss of performance. Although GMM w does not require reranking for good performance, it's cubic complexity indicates that KM w r might be preferred in practical settings.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "We outlined a methodology for clustering word embeddings for unsupervised document analysis, and presented a systematic comparison of various influential embedding methods and clustering algorithms. Our experiments suggest that pretrained word embeddings (both contextualized and non-contextualized), combined with tf-weighted k-means and tf-based reranking, provide a viable alternative to traditional topic modeling at lower complexity and runtime.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A k-means (KM) vs k-medoids (KD)", "text": "To further understand the effect of other centroid based algorithms on topic coherence, we also applied the k-medoids (KD) clustering algorithm. KD is a hard clustering algorithm similar to KM but less sensitive to outliers.\nAs we can see in Table 3, in all cases KD usually did as well or worse than KM. KD also did relatively poorly after frequency reranking. Where KD did do better than KM, the difference is not very striking and the NPMI scores were still quite below the other top performing models.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B Results for Spherical k-means and Von", "text": "Mises-Fisher Mixture ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C Comparing Different Reranking Schemes", "text": "We present the results for using different reranking schemes for KM (Table 5) and Weighted KM for Frequency (Table 6).\nWe can see that compared to the TF results in the main paper, other schemes for reranking such as aggregated TF-IDF and TF-DF improve over the original hard clustering, but fare worse in comparison with reranking with TF.    of topics due to the greater diversity of words over all the topics.\nTop 10 Word for Each Topic NPMI dollar rate rates exchange currency market dealers central interest point 0.369 year growth rise government economic economy expected domestic inflation report 0.355 gold reserves year tons company production exploration ounces feet mine 0.290 billion year rose dlrs fell marks earlier figures surplus rise -0.005 year tonnes crop production week grain sugar estimated expected area 0.239 dlrs company sale agreement unit acquisition assets agreed subsidiary sell -0.043 bank billion banks money interest market funds credit debt loans 0.239 tonnes wheat export sugar tonne exports sources shipment sales week 0.218 plan bill industry farm proposed government administration told proposal change 0.212 prices production price crude output barrels barrel increase demand industry 0.339 group company investment stake firm told companies capital chairman president 0.191 trade countries foreign officials told official world government imports agreement 0.298 offer company shares share dlrs merger board stock tender shareholders 0.074 shares stock share common dividend company split shareholders record outstanding 0.277 dlrs year quarter earnings company share sales reported expects results -0.037 market analysts time added long analyst term noted high back 0.316 coffee meeting stock producers prices export buffer quotas market price 0.170 loss dlrs profit shrs includes year gain share mths excludes -0.427 spokesman today government strike union state yesterday workers officials told 0.201 program corn dlrs prior futures price loan contract contracts cents -0.287 Top 10 Word for Each Topic NPMI rise increase growth fall change decline drop gains cuts rising 0.238 president chairman minister house baker administration secretary executive chief washington 0.111 make continue result include reduce open support work raise remain 0.101 january march february april december june september october july friday 0.043 year quarter week month earlier months years time period term 0.146 rose fell compared reported increased estimated revised adjusted unchanged raised 0.196 today major made announced recent full previously strong final additional 0.125 share stock shares dividend common cash stake shareholders outstanding preferred 0.281 dlrs billion tonnes marks francs barrels cents tonne barrel tons -0.364 sales earnings business operations companies products markets assets industries operating 0.115 sale acquisition merger sell split sold owned purchase acquire held 0.003 board meeting report general commission annual bill committee association council 0.106 loss profit revs record note oper prior shrs gain includes 0.221 company corp group unit firm management subsidiary trust pacific holdings 0.058 prices price current total lower higher surplus system high average 0.198 offer agreement agreed talks tender plan terms program proposed issue 0.138 bank trade market rate exchange dollar foreign interest rates banks 0.327 told official added department analysts officials spokesman sources statement reuters 0.181 production export exports industry wheat sugar imports output crude domestic 0.262 japan government international world countries american japanese national states united 0.251 ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We thank Aaron Mueller, Pamela Shapiro, Li Ke, Adam Poliak, Kevin Duh and the anonymous reviewers for their feedback.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D Qualitative Comparison of Topics Generated", "text": "We present the different topics generated using LDA (Table 7) and topics generated using BERT KM w r for the Reuters dataset (  ", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Nonparametric spherical topic modeling with word embeddings", "journal": "NIH Public Access", "year": "2016", "authors": "Kayhan Batmanghelich; Ardavan Saeedi; Karthik Narasimhan; Sam Gershman"}, {"title": "Latent dirichlet allocation", "journal": "Journal of machine Learning research", "year": "2003-01", "authors": "M David;  Blei; Y Andrew; Michael I Jordan Ng"}, {"title": "Enriching word vectors with subword information", "journal": "Transactions of the Association for Computational Linguistics", "year": "2017", "authors": "Piotr Bojanowski; Edouard Grave; Armand Joulin; Tomas Mikolov"}, {"title": "Normalized (pointwise) mutual information in collocation extraction", "journal": "", "year": "2009", "authors": "Gerlof Bouma"}, {"title": "Applications of topic models", "journal": "Foundations and Trends\u00ae in Information Retrieval", "year": "2017", "authors": "Jordan Boyd-Graber; Yuening Hu; David Mimno"}, {"title": "JAX: composable transformations of Python+NumPy programs", "journal": "", "year": "2018", "authors": "James Bradbury; Roy Frostig; Peter Hawkins; Matthew James Johnson; Chris Leary; Dougal Maclaurin; Skye Wanderman-Milne"}, {"title": "Language modeling by clustering with word embeddings for text readability assessment", "journal": "", "year": "2017", "authors": "Miriam Cha; Youngjune Gwon; H T Kung"}, {"title": "Gaussian lda for topic models with word embeddings", "journal": "Long Papers", "year": "2015", "authors": "Rajarshi Das; Manzil Zaheer; Chris Dyer"}, {"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Topic modeling in embedding spaces", "journal": "", "year": "2019", "authors": "B Adji;  Dieng; J R Francisco; David M Ruiz;  Blei"}, {"title": "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality", "journal": "", "year": "2014", "authors": "David Jey Han Lau; Timothy Newman;  Baldwin"}, {"title": "Tat-Seng Chua, and Maosong Sun", "journal": "", "year": "2015", "authors": "Yang Liu; Zhiyuan Liu"}, {"title": "Mallet: A machine learning for language toolkit", "journal": "", "year": "2002", "authors": "Andrew Kachites Mccallum"}, {"title": "Spherical text embedding", "journal": "", "year": "2019", "authors": "Yu Meng; Jiaxin Huang; Guangyuan Wang; Chao Zhang; Honglei Zhuang; Lance Kaplan; Jiawei Han"}, {"title": "Efficient estimation of word representations in vector space", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Kai Chen; Greg Corrado; Jeffrey Dean"}, {"title": "Detecting topics in documents by clustering word vectors", "journal": "Springer", "year": "2019", "authors": "Guilherme Raiol De Miranda; Rodrigo Pasti; Leandro Nunes De Castro"}, {"title": "Improving topic models with latent feature word representations", "journal": "Transactions of the Association for Computational Linguistics", "year": "2015", "authors": "Richard Dat Quoc Nguyen; Lan Billingsley; Mark Du;  Johnson"}, {"title": "Scikit-learn: Machine learning in Python", "journal": "Journal of Machine Learning Research", "year": "2011", "authors": "F Pedregosa; G Varoquaux; A Gramfort; V Michel; B Thirion; O Grisel; M Blondel; P Prettenhofer; R Weiss; V Dubourg; J Vanderplas; A Passos; D Cournapeau; M Brucher; M Perrot; E Duchesnay"}, {"title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"title": "Deep contextualized word representations", "journal": "", "year": "2018", "authors": "Matthew E Peters; Mark Neumann; Mohit Iyyer; Matt Gardner; Christopher Clark; Kenton Lee; Luke Zettlemoyer"}, {"title": "Christian Stab, and Iryna Gurevych. 2019. Classification and clustering of arguments with contextualized word embeddings", "journal": "", "year": "", "authors": "Nils Reimers; Benjamin Schiller; Tilman Beck; Johannes Daxenberger"}, {"title": "Distributed document and phrase co-embeddings for descriptive clustering", "journal": "Long Papers", "year": "2017", "authors": "Motoki Sano; J Austin; Georgios Brockmeier; Tingting Kontonatsios;  Mu; Y John; Jun'ichi Goulermas; Sophia Tsujii;  Ananiadou"}, {"title": "Unsupervised topic modeling for short texts using distributed representations of words", "journal": "", "year": "2015", "authors": "Vivek Kumar Rangarajan;  Sridhar"}, {"title": "Cluwords: exploiting semantic word clustering representation for enhanced topic modeling", "journal": "", "year": "2019", "authors": "Felipe Viegas; S\u00e9rgio Canuto; Christian Gomes; Washington Luiz; Thierson Rosa; Sabir Ribas; Leonardo Rocha; Marcos Andr\u00e9 Gon\u00e7alves"}, {"title": "Integrating document clustering and topic modeling", "journal": "", "year": "2013", "authors": "Pengtao Xie; Eric P Xing"}, {"title": "A correlated topic model using word embeddings", "journal": "", "year": "2017", "authors": "Guangxu Xun; Yaliang Li; Wayne Xin Zhao; Jing Gao; Aidong Zhang"}, {"title": "A word embeddings informed focused topic model", "journal": "", "year": "2017", "authors": "He Zhao; Lan Du; Wray Buntine"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: Plots showing the effect of PCA dimension reduction on different embedding and clustering algorithms. KM wr which we advocate over GMMs for efficiency, allows for dimension reduction of up to 80%.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 3 :3Figure3: NPMI as a function of vocabulary size reduced by term frequency on 20NG. Embeddings are more sensitive to noisy vocabulary (infrequent terms) than LDA, but reweighting ( w ) helps to alleviate this.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "NPMI Results (higher is better) for pre-trained word embeddings and k-means (KM), and Gaussian Mixture Models (GMM). w indicates weighted and r indicates reranking of top words. For Reuters (left table), LDA has an NPMI score of 0.12, while GMM w r BERT achieves 0.15. For 20NG (right), both LDA and KM w", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "", "figure_data": ": Top 10 words in a topic on 20NG and overall NPMI, for k-means (KM) before and after reranking (KM r ): reranking clearly improves NPMI for BERT and Spherical."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": "shows the overall bad performance ofspherical clustering methods, specifically Sphericalk-Means (SKM) and von-Mises-Fisher mixtures(VMFM)."}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "NPMI Results (higher is better) for pre-trained word embeddings and Spherical k-means (SKM), and von Mises-Fisher Mixtures (VMFM). w indicates weighted and r indicates reranking of top words.", "figure_data": "TF TF-IDF TF-DFWord2Vec 0.180.150.17FastText0.240.230.23GloVe0.220.170.21BERT0.170.150.17ELMo0.130.090.14Spherical0.250.220.24average0.200.170.19std. dev.0.040.050.04"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "", "figure_data": "w TFw TF-IDFw TF-DFWord2Vec0.190.170.20FastText0.250.250.25GloVe0.230.210.23BERT0.250.240.25ELMo0.160.150.16Spherical0.260.240.25average0.230.210.22std. dev.0.040.040.04: Results for k-means (without weighting) with pre-trained word embeddings using different reranking metrics : TF, TF-IDF, and TF-DF.unlike LDA, which uses the highest posterior prob-ability allowing duplicate words to appear in du-plicate topics, using a hard clustering algorithmfor assignment mean that each word is assigned toone topic only. We can see compared to the LDAtopics which tend to contain topics mostly regard-ing wealth and profits, clustering with BERT KM w r introduces new topics in involving locations andcorporate positions. We see overall that using clus-tering allows for a discovery for a greater diversity"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Results for k-means (weighted) pre-trained word embeddings using different reranking metrics: TF, TF-IDF and TF-DF weighted with term frequency.", "figure_data": ""}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "NPMI Scores and Top 10 words for the topics generated using LDA for the Reuters dataset", "figure_data": ""}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "NPMI Scores and Top 10 words for the topics generated using BERT KM w", "figure_data": ""}], "doi": "10.1162/tacl_a_00140"}