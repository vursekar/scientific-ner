{"authors": "Zihao He; Negar Mokhberian; Ant\u00f3nio C\u00e2mara; Andr\u00e9s Abeliuk; Kristina Lerman", "pub_date": "", "title": "Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings", "abstract": "Growing polarization of the news media has been blamed for fanning disagreement, controversy and even violence. Early identification of polarized topics is thus an urgent matter that can help mitigate conflict. However, accurate measurement of topic-wise polarization is still an open research challenge. To address this gap, we propose Partisanship-aware Contextualized Topic Embeddings (PaCTE), a method to automatically detect polarized topics from partisan news sources. Specifically, utilizing a language model that has been finetuned on recognizing partisanship of the news articles, we represent the ideology of a news corpus on a topic by corpus-contextualized topic embedding and measure the polarization using cosine distance. We apply our method to a dataset of news articles about the COVID-19 pandemic. Extensive experiments on different news sources and topics demonstrate the efficacy of our method to capture topical polarization, as indicated by its effectiveness of retrieving the most polarized topics. 1   ", "sections": [{"heading": "Introduction", "text": "The media environment has grown increasingly polarized in recent years, creating social, cultural and political divisions (Prior, 2013;Fiorina and Abrams, 2008). Although a diversity of opinions is healthy, and even necessary for democratic discourse, unchecked polarization can paralyze society by suppressing consensus required for effective governance (Tworzecki, 2019). In more extreme cases, polarization leads to disagreement, conflict and even violence. The COVID-19 pandemic has exposed many of our vulnerabilities to the pernicious effects of polarization. Public opinions about COVID-19 (Jiang et al., 2020), as well as messaging by political elites (Green et al., 2020;Bhanot 1 Code and data are publicly available at https://github.com/ZagHe568/ pacte-polarized-topics-detection.\nand Hopkins, 2020), are sharply divided along partisan lines. According to a Pew Report (Jurkowitz et al., 2020), partisanship significantly explains attitudes about the costs and benefits of various mitigation strategies, including non-pharmaceutical interventions and lockdowns, and even explains regional differences in the pandemic's toll in the US (Gollwitzer et al., 2020).\nIn mass media a variety of topics is discussed every day, and polarization can form on different topics. Therefore, identifying nascent disagreements and growing controversies of different topics in news media and public discourse would help journalists craft more balanced news coverage (Lorenz-Spreen et al., 2020;. Different from previous works that study polarization from a more coarse-grained perspective, Demszky et al. (2019) were the first to study polarized topics using tweets about 21 mass shootings to show that some topics were more polarized than others. However, their approach to represent semantic information with word frequencies is less expressive than modern methods allow.\nTo better capture the topical polarization among partisan (liberal vs. conservative) media sources, we propose Partisanship-aware Contextualized Topic Embeddings (PaCTE). Specifically, given a text corpus containing news articles from both sides, we first extract a set of topics utilizing LDA topic modeling (Blei et al., 2003). Next, we finetune a pretrained language model (Devlin et al., 2018) to recognize the partisanship of the news articles so as to render it partisanship-aware. Then for each article, we represent its ideology on a topic by a vector, called document-contextualized 2 (DC) topic embedding, by aggregating language model representations of the topic keywords contextualized by the article. Such a representation sheds light primarily on the tokens that appear in the topic keywords and thus concentrates on the topic-oriented local semantics in the context of the article, instead of the global semantics from the article that might contain irrelevant and noisy information. We further represent the ideology of the news corpus on the topic, what we call corpus-contextualized (CC) topic embedding, by aggregating the DC topic embeddings. As a result, the ideology of the news corpus on a topic is represented by a single vector. Finally, we measure the polarization between two news sources on the topic using the cosine distance between such vectors.\nFor evaluation, we create ground truth by annotating the polarization of pairs of partisan news sources on a variety of topics. We evaluate the topic polarization scores produced by PaCTE against the ground truth on the task of polarized topics retrieval. Experiments on nine pairs of partisan news sources demonstrate that compared to baselines, PaCTE is more effective in capturing topic polarization and retrieving polarized topics. We argue that public media watchdogs and social media platforms can utilize such a simple-yet-effective tool to flag discussions that have grown divisive so that action could be taken to reduce partisan divisions and improve civil discourse.", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "Related Work", "text": "The partisan polarization in the US media is a widely studied topic (Hollander, 2008;Stroud, 2011). During the onset of the COVID-19 pandemic, the polarization among the political elites and the news media causes a lot of confusion. For example, Hart et al. (2020) show that COVID-19 media coverage is politicized and polarized. Other works have been studying the polarization in media from different perspectives. Focusing on the differences in the languages of liberals and conservatives, KhudaBukhsh et al. (2020) analyze political polarization on YouTube using machine translation tools. To analyze how the news outlets frame the events differently, Fan et al. (2019) have collected and labeled 100 triplets of news articles each discussing the same event from three news sources bearing different political ideologies.\nIn addition to qualitatively analyzing polarization, different approaches to quantifying polarization have also been proposed.  propose two different ways, namely the leave-out estimator and the multinomial regression, to measure the trends of partisanship in congressional speech. Green et al. (2020) define the po-larization as one's ability to identify the partisanship of a tweet's author based on the contents of tweets and investigate the polarization regarding COVID-19 among political elites on Twitter. Demszky et al. (2019) first measure topic-wise polarization using the leave-out estimator proposed by ; however, they use a token frequency vector to represent an article, which is less expressive and fails to make use of the rich semantics in the context and the pre-knowledge in pretrained language models (Devlin et al., 2018;Liu et al., 2019) or pretrained word embeddings (Mikolov et al., 2013;Pennington et al., 2014); furthermore, they represent the topic using the token frequency vector of the entire document, thus incurring noisy information that might smooth over the target semantics in the locality of topic keywords. In contrast, our method represents the topic embedding in the context of a document, thus generating topic representations with more attention to the target topic keywords as well as making use of the contextualized semantics from the document, as captured by the contextualized embeddings. Some works have proposed contextualized embeddings to enhance the quality of neural topic models (Bianchi et al., 2020;Chaudhary et al., 2020). However, the scope of this work is to generate better contextualize topic embeddings for articles to capture topic polarization, with a given topic model; the exploration of other topic modeling techniques is beyond the scope of this work.", "n_publication_ref": 13, "n_figure_ref": 0}, {"heading": "Methodology", "text": "The proposed PaCTE framework consists of four components: 1) LDA Topic Modeling, 2) Partisanship Learning, 3) Partisanship-aware Contextualized Topic Embedding Generation, and 4) Measuring Polarization and Ranking Topics. The overall framework is illustrated in Figure 1. In this section we elaborate on each component in detail.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Problem Definition", "text": "The input is a liberal news corpus\nD L = {d L i } |D L | i=1\nand a conservative news corpus\nD R = {d R i } |D R | i=1\n(L denotes \"Left\" and R denotes \"Right\"), where d L i is an article from D L and d R i is an article from D R . A news article is represented as a sequence of tokens:\nd k = (w k i ) |d k | i=1\n. Given a topic model trained on the combined corpus D C = D L \u222a D R with a set of modeled topics T = {t i } K i=1 where t i represents a topic, we aim to learn a model f  We train an LDA model on the combined corpus and extract 2 topics. Top-4 keywords on topic t 1 are \"briefing\", \"trump\", \"president\" and \"white_house\". Top-2 most relevant documents on topic t 1 are d L 1 and d L 2 for CNN and d R 1 and d R 2 for Fox. d L 3 and d R 3 are not among the most relevant documents of this topic and are excluded in the embedding generation step. Note that we set K = 2 (No. of topics), m = 4 (No. of keywords), and n = 2 (No. of documents), just for clear demonstration. (b) Partisanship learning. We finetune a pretrained language model to classify the partisanship (liberal vs. conservative) of input documents. (c) Topic embedding generation and similarity measuring. We provide a step by step illustration of DC keyword embedding \u2192 DC topic embedding \u2192 CC topic embedding on topic t 1 . In the two input corpora, the tokens that are among the top-4 keywords of topic t 1 are highlighted in bold. Take document d L 1 from CNN as an example. The weighted average of the DC keyword embeddings (H d L 1 (president), H d L 1 (trump), and H d L 1 (briefing)) is defined as the DC topic embedding H d L 1 (t 1 ) with keyword coefficients given by Eq. 2; note that H d L 1 (criticize) is excluded because \"criticize\" is not among the top-4 keywords of topic t 1 . Similarly we can obtain the DC topic embeddings for d L 2 , d R 1 and d R 2 . The DC topic embeddings are further aggregated into CC topic embeddings H D L (t 1 ) and H D R (t 1 ) (document coefficients are from Eq. 3) and the cosine distance between them is used as a measure of polarization of the two corpora on topic t 1 .\nthat is able to detect the topic polarization between D L and D R on topics in T and output a ranking of topics based on polarization, such that\nf (D L , D R , T ) = (t k ) K k=1 , i > j \u21d4 \u03b2(t i , D L , D R ) < \u03b2(t j , D L , D R ),(1)\nwhere \u03b2(t, D L , D R ) represents the polarization score of topic t between D L and D R .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "LDA Topic Modeling", "text": "We train an LDA topic model using the the combined corpus D C = D L \u222aD R and extract K topics T = {t i } K i=1 , where t i is a topic. The modeled topics T apply to both D L and D R . An example is given in Figure 1(a).\nRepresenting a topic by keywords. A topic t i is represented as a distribution of keywords from the global vocabulary of D C and we only keep the top-m keywords:\nt i = ((p ij , w j )) m j=1 , p ij > p ik \u21d4 j < k, (2)\nwhere p ij is the probability of observing keyword w j given topic t i .\nRepresenting a topic by documents. A document d \u2208 D C is represented as a distribution over the K topics. Accordingly, we renormalize the probabilities and represent each topic t i as an (inverse) distribution of documents in D C and only keep the top-n most relevant documents, such that\nt D C i = ((q D C ij , d j )) n j=1 , q D C ij > q D C ik \u21d4 j < k, (3)\nwhere q D ij is the probability of observing document d j \u2208 D C given topic t i . Because our goal is to study the polarization between D L and D R , instead of using the global documents in D C , we represent a topic by the top-n documents in D L and D R separately and thus obtain t D L i and t D R i accordingly.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Learning Partisanship", "text": "As we will see in Sections 3.4 and 3.5, the contextualized topic embeddings are generated from a pretrained language model (Devlin et al., 2018) and cosine distance between the topic embeddings from two corpora are used as a measure of topic polarization. The idea is inspired by static word embedding models like GloVe (Pennington et al., 2014), where the authors measure the similarity between words by the cosine similarity between the word embeddings.\nHowever, to apply this measure of similarity, the model should be fitted on the target corpus. To fit the pretrained language model on the news corpora, we can use one of the two training tasks: masked language modeling or partisanship recognition. We decide on the second task because 1) it is more time efficient; 2) it informs the language model of the partisan divisions between different news sources, enhancing the language model's ability to encode the polarization arising from partisan differences in its output. This idea is similar to (Webson et al., 2020) where the authors call the embedding space of the language model after finetuning as \"connotation space\". As a result, given a document d \u2208 D C , the model is optimized to classify whether it is from D L or D R by a binary cross-entropy loss, where the [CLS] embedding is used to represent the document, as shown in Figure 1(b).", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "Partisanship-aware Contextualized Topic Embedding Generation", "text": "Denote the ideology embedding of A on B as H A (B), where A represents a news corpus or a document and B represents a topic or a topic keyword. We then represent the ideology of a corpus D on a topic t as corpus-contextualized (CC) topic embedding H D (t), the ideology of a document d on a topic t as document-contextualized (DC) topic embedding H d (t), and the ideology of a document d on a topic keyword w as DC keyword embedding H d (w). We will elaborate on how the CC topic embedding is obtained from a top-down perspective.\nAccording to Equation 3, in order to compute the CC topic embedding H D (t i ), we can rewrite it as\nH D (t i ) = n j=1 q D ij H d j (t i ).(4)\nHence, we decompose a CC topic embedding into DC topic embeddings from the top-n most relevant documents.\nTo obtain the DC topic embedding, Demszky et al. ( 2019) use word frequency vectors; Grootendorst (2020) takes the [CLS] embedding of a pretrained language model that gives a holistic document embedding without encoding the context of a topic. However, while word frequency vectors encode statistical features of words in the document, they neglect their context. In addition, a document is likely to be associated with multiple topics according to the LDA topic model, and therefore using the holistic document embedding as the topic embedding regardless of the specific topic results in identical embeddings for different topics on the same document; moreover, even if a document is only associated with one topic, it might contain information not relevant to that topic and thus the holistic document embedding will encode noisy information. Therefore, we argue that the DC topic embedding should be both contextualized and topic-specific. In this regard, according to Equation 2, we rewrite the DC topic embedding as the weighted sum of DC keyword embeddings where only top-m topic keywords are used instead of all the words in the document, as\nH d j (t i ) = m k=1 p ik H d j (w k ).(5)\nFinally, in terms of the DC keyword embedding H d j (w k ), as can be told from its name, it is precisely what a pretrained language model (Devlin et al., 2018) is designed for. Therefore, we take the corresponding final-layer token embedding of w k when the input to the language model is d j . Due to the self-attention mechanism (Vaswani et al., 2017) in the pretrained language model, H d j (t i ) encodes the global context of the document, but since it only takes the sum of topic keyword embeddings, the encoded information is more oriented towards this specific topic t i , which elegantly resonates with its name \"document-contextualized topic embedding\". The step-by-step illustration of the generation of H d (w), H d (t) and H D (t) is shown in Figure 1 (c).\nBecause the language model used to generate the embeddings is finetuned to encode partisanship, the generated H D (t i ) also contains this information and is more precisely called partisanshipaware corpus-contextualized topic embedding. For brevity we call it corpus-contextualized (CC) topic embedding.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Measuring Polarization and Ranking Topics", "text": "After obtaining the CC topic embeddings H D L (t i ) and H D R (t i ) of the two corpora D L and D R on topic t i , using two different sets of top-n most relevant documents from D L and D R respectively, we measure the ideology similarity (and then polarization) based on the cosine similarity between them, such that\nc = cos_sim(H D L (t i ), H D R (t i )), \u03b2(D L , D R , t i ) = 0.5 * (1 \u2212 c) \u2208 [0, 1].(6)\nA higher value of \u03b2 indicates more polarization. Therefore, the polarization-based ranked topic list f (D L , D R , T ) is computed based on the corresponding polarization scores (\u03b2(D L , D R , t i )) K i=1 .\n4 Experiments and Results", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dataset", "text": "We use the AYLIEN COVID-19 dataset 3 consisting of~1.5M news articles related to the pandemic spanning from Nov 2019 to July 2020 that are from 440 global sources. To discover the polarization between politically divided news media, we select six well-known US publishers evenly split between partisan leanings: CNN, Huffington Post (Huff), New York Times (NYT) as liberal sources vs. Fox, Breitbart (Breit) and New York Post (NYP) as conservative sources. After filtering the publishers and remove duplicate articles, 66,368 articles are left spanning from Jan 2020 to July 2020. The statistics of news articles are shown in Appendix A.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experimental Setup", "text": "Data Preprocessing. We build a global vocabulary containing unigrams and bigrams from the six news sources. We perform lemmatization via LDA Topic Modeling. We train the topic model using articles from all six sources to create a global topic set. The number of topics K is selected from a grid search in [10,50] and the model with K = 39 produces the best coherence value (R\u00f6der et al., 2015). From the 39 topics we remove 9 of them regarding advertisements, sport events, gossip news and recipes, and 30 topics are left; the removed topics are more factual and contain less ideologies from the news media, which is less worth studying. Different from (Demszky et al., 2019) that assigns only one topic with the highest probability to a document, we allow a document to be assigned multiple topics with different probabilities. We represent each topic with its top-10 keywords because given a topic t i we empirically find that 10 j=1 p ij > 0.95; and we keep the top-10 most relevant documents to represent a topic because on some topics, the documents beyond the top-10 list are obviously irrelevant and will bias the polarization study regarding the topic. In Table 1 we show the top-10 keywords of topics that are discussed in this paper. For a complete list of topics please refer to Appendix B.\nLearning Partisanship. We finetune the pretrained bert-base-uncased model from huggingface Transformers (Wolf et al., 2020) to classify the news articles according to their political leanings, or partisanship. To smooth over the differences in style and writing between the sources and render the model primarily sensitive to political divisions, we aggregate CNN, Huff, and NYP to create a holistic Liberal corpus, and similarly aggregate Fox, Breit and NYP to create a holistic Conservative corpus and optimize the model to classify whether an article is from Liberal or Conservative. In fact, finetuning a BERT model to recognize differences only between CNN vs. Fox is likely to make it end up capturing the writing style differences and ignoring political differences, since the former is an easier task. For more details about the training process please refer to Appendix C. Idx Top-10 keywords (and two defined stances)  ", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Annotating Topic Polarization", "text": "As ground truth for the evaluation of PaCTE, we annotate the topic polarization scores on a subset of the 30 modeled topics. We asked three annotators to select 10 topics and define two polarized political stances on each selected topic, and they reached an agreement on T labeled = {t 1 , t 2 , t 8 , t 9 , t 10 , t 11 , t 12 , t 27 , t 30 , t 33 }, as shown in Table 1. Then on each topic in T labeled , we selected 60 relevant documents (10 from each of the six sources), and asked three annotators to decide which stance they belong to (label it as 0/1). If the document does not have a clear stance, it was labeled as \u22121. On each document, the majority label from the annotations was used as the final annotation. Please refer to Appendix D for more details about the annotation process.\nDenoting the number of negative labels (0) and positive labels (1) in corpus D on topic t as N t D (0) and N t D (1) respectively, the leaning of the corpus on the topic is quantified as\nle(D, t) = (N t D (1)\u2212N t D (0))/|D| \u2208 [\u22121, 1]. (7)\nIntuitively, le(D, t) reflects how much the corpus is aligned with the stance labeled as 1. Notably, the documents labeled with \u22121 are not counted because they do not display a clear political standing. Accordingly, the ground-truth polarization score between a liberal corpus D L and a conservative corpus D R on topic t is computed as the difference between the leanings of the two corpora, such that\n\u03b1(D L , D R , t) = |le(D L , t) \u2212 le(D R , t)|/2 \u2208 [0, 1]. (8)\nA higher value of \u03b1 signifies more polarization. As a result, the ground-truth polarization-based topic ranked list l gt (D L , D R , T labeled ) between a liberal corpus D L and a conservative corpus D R is computed based on the corresponding ground-truth polarization scores (\u03b1(D L , D R , t)|t \u2208 T labeled ).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Baselines", "text": "We compare PaCTE to the following three baselines.\nLeave-out estimator (LOE). For a pair of news corpora D L and D R and a given topic t, we take the top-10 most relevant documents from each corpus and feed the token frequency vectors of the documents into the leave-out estimator (Demszky et al., 2019), from which we use estimated partisanship as the polarization score (\u2208 [0, 1]) of topic t between D L and D R , following the idea of measuring within-topic polarization in their paper. Note that different from their method that extracts topic using embedding-based topic assignment, we use the same LDA topic model in PaCTE to extract topics, so as to ensure fair comparison between PaCTE and LOE.\nPaCTE\u00acFT. A variant of PaCTE without finetuning the language model. We compare to it to show the effect of finetuning the language model. PaCTE-PLS. A variant of PaCTE where the language model is finetuned on news articles with partisanship labels shuffled and thus is confused about the partisanship. We compare to it in order to show the effect of rendering the language model partisanship-aware.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Quantitative Evaluation with Labeled Topics", "text": "To quantitatively evaluate the effectiveness of PaCTE and the baselines in capturing topic polarization, we use the 10 manually labeled topics to create a ground truth ranking of polarized topics and score models on their ability to retrieve the most polarized topics on this ranked list. Evaluation protocol. Given a liberal news corpus D L , a conservative news corpus D R , and a list of 10 topics ranked by ground-truth polarization scores, l gt (D L , D R , T labeled ), as described in Section 4.3, we define the top-3 topics in the list as the target polarized topics that deserve more attention and that should be addressed when trying to prevent polarization from escalating. The target polarized topics between different pairs of news sources are shown in Table 2. Then, given a ranked list of topics f pred (D L , D R , T labeled ) predicted by a model, we evaluate how effectively the 3 target polarized topics are retrieved in this model predicted list using recall@3. In other words, we check how much the overlap is between the top-3 topics in the ground-truth ranking and the top-3 topics in the predicted ranking, of the 10 labeled topics. We call this task polarized topics retrieval.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Fox", "text": "Breit NYP CNN 1,9,10 9,1,11 9,10,2 Huff 10,1,8 1,11,9 10,12,30 NYT 10,33,1 11,1,33 11,9,10 Analysis of results. The results of polarized topics retrieval using different methods in nine news corpus pairs are shown in Table 3. The average recall@3 over the nine news source pairs is 0.26, 0.04, 0.26, and 0.52 on LOE, PaCTE\u00acFT, PaCTE-PLS, and PaCTE respectively, where PaCTE outperforms all other baselines.\nComparing the results of LOE and PaCTE, we see that in most pairs PaCTE outperforms or ties with LOE. We argue that the inferior performance of LOE stems from its inability to capture document semantics due to the use of word frequency vectors. For example, in Huff vs. NYP, topic 12 is one of the target polarized topics, where documents from both stances spend the bulk of the content on the fact about the primaries and then use a few words to explicitly or implicitly endorse Biden or Sanders. Based on the use of words it is difficult to differentiate documents from the two stances, leading to the failure of LOE. In contrast, PaCTE is able to capture the contextual semantics in addition to the statistics of word usages. Therefore, even when word usages are statistically similar, PaCTE manages to discern the semantic difference and capture polarization. However, in Huff vs. Breit, compared to LOE, PaCTE fails to retrieve topic 1 regarding \"black lives matter\", which is in the target polarized topics. On topic 1 Huff stresses \"justice\" where the news articles suggest \"police knelt on a black man\", while Breit stresses \"riot\" where the articles suggest \"the protesters loot stores and attack police\". As a result, the word usages of the articles from two stances are significantly different, which is trivial for LOE to capture, and thus LOE ranks topic 1 in a high place in the output list. Despite the difference in word usages, articles from both sources mention \"protests\" and \"violence\" a lot and their \"negative\" semantics is captured by PaCTE, leading to the perceived less polarization by PaCTE.\nThe worst-performing method is PaCTE\u00acFT where the language model is not finetuned. On all topics and in all partisan news source pairs, the polarization scores given by PaCTE\u00acFT are below 0.1 (the full range is [0, 1]) which indicates significant alignment. However, this is contradictory to the well-known polarization in news media. Such a phenomenon demonstrates the necessity of fitting a language model on the target corpus before apply cosine similarity between learned embeddings as a measure of word and topic similarities.\nIn PaCTE-PLS the language model is finetuned on shuffled partisan labels that do not represent real partisanship. Compared to PaCTE\u00acFT where the model is not finetuned at all, the performance of PaCTE-PLS improves significantly, achieving the performance on a par with LOE. However, neither PaCTE\u00acFT nor LOE makes use of information about news partisanship, and compared to PaCTE \n0 1/3 1/3 0 1/3 1/3 0 1/3 1/3 2/3 Huff 1/3 1/3 1/3 2/3 2/3 0 1/3 1/3 0 0 1/3 2/3 NYT 1/3 0 1/3 1 1/3 0 1/3 1/3 0 1/3 0 1/3\nTable 3: Recall@3 on polarized topics retrieval in nine partisan news source pairs using different methods, where we use the polarization-based topic ranked list from a model predictions f pred (D L , D R , T labeled ) to retrieve the top-3 topics from the ground-truth ranked list l gt (D L , D R , T labeled ). The row represents the liberal source and the column represents the conservative source in the news source pair.\nwhere partisanship information is leveraged, they are still outperformed.\nInsights into partisanship learning. We observe that PaCTE, which is finetuned on partisanship labels, outperforms PaCTE\u00acFT and PaCTE-PLS. We hypothesize that during the finetuning process of PaCTE, whereas the direct objective is to separate documents based on partisanship labels, the model implicitly learns the two political stances on each topic in an automatic manner; just like in human annotating, the annotators were given two groups of documents from two partisan lines, and the annotators were able to discover the two political stances after reading the documents. Therefore, after finetuning, while the model differentiates document embeddings based on partisan divisions, it separates DC topic embeddings according to the implicitly and automatically learned political stances, bearing resemblance to human annotators' defining two political stances for topics. As a result, we can use the partisanship-aware model to capture topic polarization arising from the partisan divisions.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Qualitative Analysis with All Topics", "text": "In Section 4.5 we quantitatively demonstrate the effectiveness of PaCTE in retrieving polarized topics when evaluating with the 10 labeled topics. We believe that such success generalizes to the case where the input to the model is the complete topic list T containing 30 topics. In this section, we conduct a case study and retrieve the top-3 most polarized topics from T in CNN vs. Fox, Huff vs. Breit and NYT vs. NYP, by PaCTE. Since we do not have the ground-truth target polarized topics from T , for the retrieved topics, we conduct manual inspections on relevant documents and give explanations about the polarization. For the topics in T labeled , the polarization is formed due to the two political stances. Therefore in this section we only focus on the retrieved topics not in T labeled .\nCNN vs. Fox. The retrieved top-3 topics are topic 28, 6, 10, where topic 10 is in T labeled . The first retrieved topic is topic 28, where CNN suggests the surge of new COVID cases every day but Fox suggests that the state should reopen. On topic 6 CNN reports the serious situation of coronavirus in the US, including the high number of cases and collapse of quarantine hotels, but Fox focuses more on worldwide coronavirus situation and suggests the high number of cases in Michigan is misleading.\nHuff vs. Breit. The retrieved top-3 topics are topic 29, 9, 31, where topic 9 is in T labeled . On topic 29, Huff advocates Pelosi's coronavirus bills while Breit criticizes them. On topic 31, the articles talk about different court cases; however, no clear polarization is discerned between the pair of news sources by manual inspections. We regard it as a failure case of PaCTE. Although the relevant articles are regarding the same topic, they have different subjects or events, and thus misleading PaCTE to perceive polarization between them.\nNYT vs. NYP. The retrieved top-3 topics are topic 28, 12, 10, where topic 12 and 10 are in T labeled . On topic 28, just as in CNN vs. Fox, NYT takes the pandemic more seriously and NYP suggests reopening.\nAs a result, despite a minor error, PaCTE manages to retrieve polarized topics from T on the three pairs of news sources. Although we are not able to verify if the retrieved topics are indeed the groundtruth top-3 most polarized topics, we argue that if given the ground-truth ranking on T , PaCTE will retain its satisfactory quantitative performance in retrieving polarized topics.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ablation Study: Document Embedding vs. DC Topic Embedding", "text": "In Section 3.4 we propose to use the DC topic embedding to represent the ideology of a document on a topic, instead of using the holistic document embedding. In this section we study the difference between them. We denote the variant of PaCTE that uses document embeddings ([CLS] token em-beddings) as PaCTE-DE. First, we show the results of polarized topics retrieval using PaCTE-DE and PaCTE in three partisan news source pairs in Table 4.\nMethod CNN vs. Fox Huff vs. Breit NYT vs. NYP PaCTE-DE 0 0 0 We observe that PaCTE-DE fails to retrieve any polarized topics in all three pairs of news sources, significantly outperformed by PaCTE. We provide more explanations on the advantages of DC topic embeddings over document embeddings from another perspective, in addition to the capability of DC topic embedding to focus more on the topicspecific semantics in a document. We observe that the polarization scores given by PaCTE-DE in three source pairs on all topics are above 0.98 (the range is [0,1]), suggesting that all topics are highly polarized. Therefore, as the polarization scores cluster within the interval of [0.98,1], the gaps between different scores are barely discernible, in which case the output ranked list is more susceptible to random noise during the language model finetuning and is thus more unstable and erratic. However, the output polarization scores from PaCTE are more evenly distributed in [0,1], and thus are more robust to perturbations during partisanship learning; a small perturbation on a polarization score does not affect the output ranking. As a result, PaCTE enjoys a better chance to outperform PaCTE-DE.\nPaCTE 1/3 1/3 1/3\nAs a matter of fact, the large polarization scores from PaCTE-DE on all topics are expected, because the language model is finetuned to directly separate the document embeddings according to partisan line divisions, resulting in low cosine similarities between document embeddings on every topic, as shown in Figure 2(Left). However, despite the prominent separation of document embeddings, the corresponding DC topic embeddings that are used in PaCTE display more alignment, as shown in Figure 2(Right), where we see on some topics the DC topic embeddings are separated while on other topics the embeddings are more close. Thus, we argue that during the finetuning process, on a given topic, DC topic embeddings retain their similarity if the two partisan news articles agree on this topic, because in these articles the topic-related se-mantics does not contribute to the forming of the partisanship and thus maintains its position during partisanship learning, while the non-topical semantics (not captured by DC topic embeddings but captured by document embeddings) that contribute to the document partisanship keeps moving apart in the embedding space. ", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Conclusions and Future Work", "text": "In this paper, we propose a method to automatically discover topic-level polarization between partisan news sources by contextualized topic embeddings.\nFor evaluation, we create annotations on topic polarization scores in different partisan news source pairs on a variety of topics. Compared to the leaveout estimator (Demszky et al., 2019) that is purely based on statistical features, our method can more precisely and meaningfully capture topical polarization as indicated by the performance on polarized topics retrieval. We hope that more NLP and researchers and practitioners can contribute to this research area that is promising but receiving insufficient attention.\nBecause detecting polarized topics between partisan news sources is a less established task in the research community, we articulate the data annotation and the model evaluation in great detail and make the method seemingly \"complicate\". However, we believe that for public media watchdogs and social media platforms to flag the highly polarized topics, our method is simple to implement, because each of the five steps described in Section 3 is based on robust methods in NLP.\nFor future work, we plan to perform our method on more datasets, such as the tweets with noisy texts (Demszky et al., 2019). In addition, we will study how to finetune the language model when when partisanship labels are not available.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "A Data Preprocessing", "text": "The statistics of the dataset is in Table 5. We use the summary of each news article to perform the textual analysis, because the summary contains sufficient information to understand the political stance of the article and the whole text is lengthy for the pretrained language model to handle. For a complete list of all documents, please check our public repository 4 .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B LDA Topic Modeling", "text": "We use MALLET 5 topic modeling. The top-10 keywords of all 39 topics are shown in Table 6. Among them topic 0, 3,4,14,16,26,35,36,37 are not used in further analysis because after reading relevant articles we find that they are more about advertisements, sport events, gossip news and recipes and etc., which are more factual and convey limited media ideologies. 30 topics are left after removing the 9 topics. Table 6 lists the top-10 keywords of the 30 topics.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "B.1 News Article Examples", "text": "On topic 10, we show six examples of news articles, one from each news source. For a complete list of news articles, please refer to our public repository.\nCNN: There has been a concerted effort among aides and allies to get President Donald Trump to stop conducting the daily coronavirus briefings, multiple sources tell CNN. The briefing came a day after Trump had given a lengthy briefing to the media, at one point suggesting it might be possible to treat coronavirus by injecting people with sunlight or disinfectants. Trump asked White House coronavirus task force coordinator Dr. Deborah Birx during Thursday's briefing. A source close to the coronavirus task force said Trump was upset about the \"flack\" he was taking after those comments and that appears to be part of the reason why the President cut Friday's briefing short. During the earlier questioning from reporters on Friday, Trump said he was being \"sarcastic\" with his suggestion that people inject themselves with disinfectant, even though he was clearly being serious during Thursday's briefing.\nFox: White House press secretary Kayleigh McEnany, during her first official briefing, promised that she 'will never lie' to the press in her new role. White House press secretary Kayleigh McEnany, during her first official briefing, promised that she \"will never lie\" to the press in her new role. McEnany took the podium for the first time Friday, after being tapped as White House press secretary from her post as national spokeswoman for President Trump's re-election campaign earlier this month. TRUMP NAMES KAYLEIGH MCENANY AS NEW WHITE HOUSE PRESS SECRETARY \"I will never lie to you,\" McEnany told reporters. McEnany seemed to signal that the White House would scale back on their daily coronavirus task force briefings, which were regularly led by the president himself, and Vice President Pence, with appearances from Dr. Deborah Birx and Dr. Anthony Fauci to provide public health information.\nHuffington Post: President Donald Trump on Sunday tore into former President Barack Obama, calling him \"an incompetent president\" after Obama appeared to criticize his response to the coronavirus crisis during two commencement speeches a day earlier. Asked about Obama's remarks, Trump told reporters on the White House lawn that he \"didn't hear it\" before proceeding to bash his predecessor as \"grossly incompetent.\" President Trump: \"[President Obama] was an incompetent president. But earlier this month, Obama reportedly bashed the Trump administration's response to the pandemic as \"an absolute chaotic disaster\" during a phone call with some of his former White House aides. When a Washington Post reporter last week asked Trump to explain \"Obamagate,\" the president refused.\nBreibart: New York magazine Washington correspondent Olivia Nuzzi responded angrily to criticism from former White House press secretary Ari Fleischer on Monday evening, tweeting at him: \"Oh shut the f*ck up.\" Fleisher, who served under President George W. Bush, criticized Nuzzi after a Rose Garden press briefing on the coronavirus pandemic in which she asked President Donald Trump: \"If an American president loses more Americans over the course of six weeks than died in the entirety of the Vietnam War, does he deserve to be re-elected?\" One example is a \"fake news\" viral photograph of President Lyndon B. Johnson, which was presented by many Trump critics as if Johnson had been expressing grief over the deaths in Vietnam. President Trump is said to be reconsidering  post, twitter, video, facebook, tweet, social_media, share, write, call, make 10 trump, president, white_house, donald, administration, fauci, coronavirus, vice, briefing, task_force 11 covid, dr, coronavirus, health, disease, drug, expert, risk, treatment, director 12 mr, biden, campaign, election, party, democratic, voter, joe_biden, republican, primary 13 school, child, student, university, parent, high, kid, year, family, class 15 american, pandemic, crisis, america, nation, make, policy, job, people, economy 17 time, world, space, launch, turn, center, long, life, leave, moment 18 coronavirus, report, outbreak, accord, ship, official, quarantine, military, force, iran 19 city, york, de_blasio, mayor, resident, area, yorker, coronavirus, people, tuesday 20 mask, people, wear, face, service, social_distance, church, sunday, coronavirus, stay 21 people, time, thing, good, work, make, lot, add, give, feel 22 department, official, national, security, fire, investigation, report, threat, call, director 23 employee, worker, company, restaurant, food, store, work, customer, business, amazon 24 china, chinese, world, outbreak, virus, wuhan, organization, coronavirus, global, government 25 time, series, film, show, year, make, movie, live, race, set 27 year, company, market, stock, price, drop, month, business, global, sale 28 state, coronavirus, cuomo, florida, texas, york, governor, tuesday, week, monday 29 house, coronavirus, republican, member, bill, senate, democrat, wednesday, washington, thursday 30 country, lockdown, government, coronavirus, measure, people, italy, restriction, travel, border 31 claim, court, judge, law, federal, district, rule, chicago, legal, decision 32 health, public, people, work, community, include, protect, provide, group, pandemic 33 hospital, care, health, patient, medical, covid, center, facility, home, doctor 34 program, pay, money, fund, economic, job, business, relief, federal, receive 38 coronavirus, office, letter, pandemic, call, send, statement, issue, write, act his daily press briefings because journalists use them to grandstand and to score political points, rather than to pursue information. The contrast with press briefings for governors and mayors is stark: there, journalists tend to be more deferential and to ask questions aimed at eliciting information rather than assigning political fault.\nNew York Times: WASHINGTON -After several days spent weathering attacks from White House officials, Dr. Anthony S. Fauci hit back on Wednesday, calling recent efforts to discredit him \"bizarre\" and a hindrance to the government's ability to communicate information about the coronavirus pandemic. On Wednesday, Peter Navarro, Mr. Trump's top trade adviser, published a brazen op-ed article in USA Today describing Dr. Fauci as \"wrong about everything.\" All the while, White House officials -including the president and the press secretary -assert in the face of the evidence that there is no concerted effort to attack Dr. Fauci. given \"opposition research\" to discredit Fauci, including his past remarks early on in the pandemic that the public didn't need to wear masks. \"We were asked a very specific question by the Washington Post, and that question was President Trump noted that Dr. Fauci had made some mistakes, and we provided a direct answer to what was a direct question.\"", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B.2 Top-10 Most Relevant Documents on All Topics", "text": "We show the topic-10 most relevant document indices on all 30 topics on each source. On some topics there are less than 10 relevant documents on some sources. Note that such topics are not in the 10 labeled topics and are only used for qualitative analysis; in other words, for quantitative analysis, we ensure that on all the 10 labeled topics, there are 10 relevant documents on each source. Topic 1. CNN: 22873, 62724, 62635, 63979, ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "This project was funded in part by DARPA under contract HR001121C0168. The authors are also grateful to Ves Stoyanov for a productive discussion.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "C Learning Partisanship", "text": "The news articles are split into the training set comprising topical documents and validation set comprising non-topical documents. Non-topical documents have small probabilities (<0.15) categorized to all topics. We do such a split because all documents are assigned a partisanship label, but not all of them are topical. For the topical documents from which we will generate contextualized topic embeddings, we use them as the training data to finetune the language model during the training phase. As a result, train set has 30,571 documents and the validation set has 35,797 documents. The model is trained for 30 epochs and we pick the one with the best performance on validation set for the subsequent topic embedding generation. We train the model using Adam optimizer, with learning rate 1e-5 and weight decay 5e-4. We use a batch size of 64 and train the model on 4 RTX 2080 GPUs. Each epoch takes about 10 minutes. The best validation F1 score on classifying partisanship is 91.3.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D Annotating Topic Polarization", "text": "We recruit 3 annotators that work as academic researchers in the areas of NLP and social science. For each one of the 30 topics, the annotators are provided with the top-10 topic keywords and the summaries of top-10 most relevant documents from each news corpus (as a total of 60 documents). First, the annotators select 15 topics on which they feel it is straightforward to find two polarized political stances by reading the relevant documents. For example, on topic 12 about Democratic primaries, it is intuitive to perceive the two political stances are \"endorsing Biden\" and \"endorsing Sanders\" after reading relevant articles, and then this topic is likely to be selected. We take the overlap of the 15 selected topics from 3 annotators and obtain 10 topics: T labeled = {t 1 , t 2 , t 8 , t 9 , t 10 , t 11 , t 12 , t 27 , t 30 , t 33 } with defined polarized political stances. In other words, the annotators reach an agreement that it is more clear on these 10 topics that there are two political stances. We find that on each of these 10 topics, the two stances defined by 3 annotators reach a complete agreement.\nWe do not annotate all topics because 1) it is difficult for humans to discern the two political stances on some topics, especially when such two stances do not exist at all; 2) we use the vanilla LDA topic modeling which is not the state-of-the-art, so the modeled topics will change using different topic models, in which case the annotating step should be repeated. Nevertheless, we argue that annotating 10 topics is sufficient to quantitatively evaluate the effectiveness of PaCTE.\nGiven a topic t from T labeled , the defined two stances, and its 60 most relevant documents (10 from each of the six news sources), for each document, we ask the annotators to label which stance it belongs to and label it as 0 or 1; if the annotator is not able to perceive a clear political stance, then the annotator will label it as -1. For each document, the majority vote of the three labels with be used as the final annotation. If no majority vote is achieved, in other words, the three annotators give three different labels to a document, then a fourth annotator will read the document again and decide the final label. For a complete list of all document labels on the 10 selected topics, please refer to our public repository.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Partisan polarization and resistance to elite messages: Results from a survey experiment on social distancing", "journal": "", "year": "2020", "authors": "Syon Bhanot; J Daniel;  Hopkins"}, {"title": "Pre-training is a hot topic: Contextualized document embeddings improve topic coherence", "journal": "", "year": "2020", "authors": "Federico Bianchi; Silvia Terragni; Dirk Hovy"}, {"title": "", "journal": "Latent dirichlet allocation. the Journal of machine Learning research", "year": "2003", "authors": "M David;  Blei; Y Andrew; Michael I Jordan Ng"}, {"title": "Topicbert for energy efficient document classification", "journal": "", "year": "2020", "authors": "Yatin Chaudhary; Pankaj Gupta; Khushbu Saxena; Vivek Kulkarni; Thomas Runkler; Hinrich Sch\u00fctze"}, {"title": "Analyzing political bias and unfairness in news articles at different levels of granularity", "journal": "", "year": "2020", "authors": "Wei-Fan Chen; Khalid Al-Khatib; Henning Wachsmuth; Benno Stein"}, {"title": "Analyzing polarization in social media: Method and application to tweets on 21 mass shootings", "journal": "", "year": "2019", "authors": "Dorottya Demszky; Nikhil Garg; Rob Voigt; James Zou; Matthew Gentzkow; Jesse Shapiro; Dan Jurafsky"}, {"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "plain sight: Media bias through the lens of factual reporting", "journal": "", "year": "2019", "authors": "Lisa Fan; Marshall White; Eva Sharma; Ruisi Su; Prafulla Kumar Choubey; Ruihong Huang; Lu Wang"}, {"title": "Political polarization in the american public", "journal": "Annu. Rev. Polit. Sci", "year": "2008", "authors": "P Morris;  Fiorina; J Samuel;  Abrams"}, {"title": "Measuring group differences in highdimensional choices: method and application to congressional speech", "journal": "Econometrica", "year": "2019", "authors": "Matthew Gentzkow; Jesse M Shapiro; Matt Taddy"}, {"title": "Partisan differences in physical distancing are linked to health outcomes during the covid-19 pandemic", "journal": "Nature human behaviour", "year": "2020", "authors": "Anton Gollwitzer; Cameron Martel; J William; Philip Brady;  P\u00e4rnamets; G Isaac; Eric D Freedman; Jay J Van Knowles;  Bavel"}, {"title": "Elusive consensus: Polarization in elite communication on the covid-19 pandemic", "journal": "Science Advances", "year": "2020", "authors": "Jon Green; Jared Edgerton; Daniel Naftel; Kelsey Shoub; Skyler J Cranmer"}, {"title": "Bertopic: Leveraging bert and c-tf-idf to create easily interpretable topics", "journal": "", "year": "2020", "authors": "Maarten Grootendorst"}, {"title": "Politicization and polarization in covid-19 news coverage", "journal": "Science Communication", "year": "2020", "authors": "Sol Hart; Sedona Chinn; Stuart Soroka"}, {"title": "Tuning out or tuning elsewhere? partisanship, polarization, and media migration from", "journal": "Journalism & Mass Communication Quarterly", "year": "1998", "authors": "A Barry;  Hollander"}, {"title": "Political polarization drives online conversations about covid-19 in the united states", "journal": "Human Behavior and Emerging Technologies", "year": "2020", "authors": "Julie Jiang; Emily Chen; Shen Yan; Kristina Lerman; Emilio Ferrara"}, {"title": "U.s. media polarization and the 2020 election: A nation divided", "journal": "", "year": "2020", "authors": "Mark Jurkowitz; Amy Mitchell; Elisa Shearer; Mason Walker"}, {"title": "We don't speak the same language: Interpreting polarization through machine translation", "journal": "", "year": "2020", "authors": "Rupak Ashiqur R Khudabukhsh;  Sarkar; S Mark; Tom M Kamlet;  Mitchell"}, {"title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "How behavioural sciences can promote truth, autonomy and democratic discourse online", "journal": "Nature human behaviour", "year": "2020", "authors": "Philipp Lorenz-Spreen; Stephan Lewandowsky; Ralph Cass R Sunstein;  Hertwig"}, {"title": "Distributed representations of words and phrases and their compositionality", "journal": "", "year": "2013", "authors": "Tomas Mikolov; Ilya Sutskever; Kai Chen; Greg S Corrado; Jeff Dean"}, {"title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher D Manning"}, {"title": "Media and political polarization", "journal": "Annual Review of Political Science", "year": "2013", "authors": "Markus Prior"}, {"title": "Exploring the space of topic coherence measures", "journal": "", "year": "2015", "authors": "Michael R\u00f6der; Andreas Both; Alexander Hinneburg"}, {"title": "Niche news: The politics of news choice", "journal": "Oxford University Press on Demand", "year": "2011", "authors": "Natalie Jomini Stroud"}, {"title": "Poland: a case of topdown polarization", "journal": "The ANNALS of the American Academy of Political and Social Science", "year": "2019", "authors": "Hubert Tworzecki"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Are\" undocumented workers\" the same as\" illegal aliens\"? disentangling denotation and connotation in vector spaces", "journal": "", "year": "2020", "authors": "Albert Webson; Zhizhong Chen; Carsten Eickhoff; Ellie Pavlick"}, {"title": "Transformers: State-of-theart natural language processing", "journal": "", "year": "2020", "authors": "Thomas Wolf; Julien Chaumond; Lysandre Debut; Victor Sanh; Clement Delangue; Anthony Moi; Pierric Cistac; Morgan Funtowicz; Joe Davison; Sam Shleifer"}, {"title": "Topic 19. CNN: 65216, 65919, 47703", "journal": "", "year": "1007", "authors": ""}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Overview of our PaCTE framework to detect polarized topics in media, illustrated by a toy example on CNN vs. Fox, both consisting 3 documents. (a) LDA topic modeling. We train an LDA model on the combined corpus and extract 2 topics. Top-4 keywords on topic t 1 are \"briefing\", \"trump\", \"president\" and \"white_house\". Top-2 most relevant documents on topic t 1 are d L 1 and d L 2 for CNN and d R 1 and d R 2 for Fox. d L 3 and d R 3 are not among the most relevant documents of this topic and are excluded in the embedding generation step. Note that we set K = 2 (No. of topics), m = 4 (No. of keywords), and n = 2 (No. of documents), just for clear demonstration. (b) Partisanship learning. We finetune a pretrained language model to classify the partisanship (liberal vs. conservative) of input documents. (c) Topic embedding generation and similarity measuring. We provide a step by step illustration of DC keyword embedding \u2192 DC topic embedding \u2192 CC topic embedding on topic t 1 . In the two input corpora, the tokens that are among the top-4 keywords of topic t 1 are highlighted in bold. Take document d L", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 2 :2Figure 2: Document embeddings (Left) and DC topic embeddings (Right) on 10 labeled topics in Liberal vs. Conservative. Different colors represent documents categorized to different topics. The original 768-d embeddings are projected into the 2-d space via PCA.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "", "figure_data": "D L : CNN D R : Fox d1 L : criticize, president, trump, briefing d2 L : trump, briefing, ridiculous d3 L : black, life, matter d1 R : defend, white_house, briefing d2 R : president, not, lie, briefing d3 R : protestor, loot, store D L : CNN d1 L : criticize, president, trump, briefing d2 L : trump, briefing, ridiculous d3 L : black, life, matter d1 R : defend, white_house, briefing d2 R : president, not, lie, briefing d3 R : protestor, loot, store D R : FoxD L : CNN D R : Fox d1 L : criticize, president, trump, briefing d2 L : trump, briefing, ridiculous d3 L : black, life, matter d1 R : defend, white_house, briefing d2 R : president, not, lie, briefing d3 R : protestor, loot, store H d1L (t 1 ) BERT H d2L (t 1 ) H d1L (president) H d1L (trump) H d1L (briefing) t 1 : ((0.5, briefing), (0.3, trump), (0.15, president), (0.05, white_house)) t 2 : \u2026 H d1L (criticize) LDA t 1 DL : ((0.7, d 1 L ), (0.3, d 2 L )) t 2 DL : \u2026 t 1 DR : ((0.6, d 1 R ), (0.4, d 2 R )) t 2 DR : \u2026 Finetuned BERT + H d1R (t 1 ) H d2R (t 1 ) *0.7 Conservative? Liberal or H DL (t 1 ) H DR (t 1 ) corpus-contextualized (CC) topic embeddings *0.15 *0.3 *0.5 (b) H d2L (trump) H d2L (briefing) H d2L (ridiculous) + + *0.3 *0.3 *0.5 H DR (t 1 ) keyword embeddings (DC) topic embeddings document-contextualized (DC) document-contextualized H d1R (defend) H d1R (white_house) H d1R (briefing) H d2R (trump) H d2R (not) H d2R (lie) H d2R (briefing) *0.05 *0.5 + + + H DL (t 1 ) *0.6 *0.3 *0.5 *0.4"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "1 keywords: police, officer, man, black, protest, people, arrest, kill, protester, matter stances: protests are for social justice vs. protests are riots 2 keywords: coronavirus, pandemic, federal, supply, government, make, effort, ventilator, response, agency stances: healthcare supplies are in good condition vs. shortage of supplies 6 keywords: case, report, number, death, health, coronavirus, confirm, official, accord, covid 8 keywords: state, order, reopen, county, california, governor, business, open, jersey, guideline stances: pro-lockdown vs. anti-lockdown 9 keywords: post, twitter, video, facebook, tweet, social_media, share, write, call, make stances: fact-checking is helpful vs. fact-checking is misleading 10 keywords: trump, president, white_house, donald, administration, fauci, coronavirus, vice, briefing, task_force stances: critical of white house covid briefings vs. defending them 11 keywords: covid, dr, coronavirus, health, disease, drug, expert, risk, treatment, director stances: drugs promoted by Trump are risky vs. they are helpful 12 keywords: mr, biden, campaign, election, party, democratic, voter, joe_biden, republican, primary stances: endorsing Biden in Democratic primaries vs. endorsing Sanders 27 keywords: year, company, market, stock, price, drop, month, business, global, sale stances: oil/stock prices are falling vs. the prices are going up 28 keywords: state, coronavirus, cuomo, florida, texas, york, governor, tuesday, week, monday 29 keywords: house, coronavirus, republican, member, bill, senate, democrat, wednesday, washington, thursday 30 keywords: country, lockdown, government, coronavirus, measure, people, italy, restriction, travel, border stances: closing borders in Europe vs. opening borders 31 keywords: claim, court, judge, law, federal, district, rule, chicago, legal, decision 33", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "The target polarized topics between different pairs of news sources from human annotations.", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ": Recall@3 on polarized topics retrieval usingPaCTE-DE and PaCTE in three partisan news sourcepairs."}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "The number of news articles from all sources in all months.", "figure_data": "Idx Top-10 Topic Keywords1police, officer, man, black, protest, people, arrest, kill, protester, matter2coronavirus, pandemic, federal, supply, government, make, effort, ventilator, response, agency5coronavirus, virus, test, covid, people, tested_positive, testing, positive, symptom, spread6case, report, number, death, health, coronavirus, confirm, official, accord, covid7event, plan, june, announce, due, july, hold, cancel, pandemic, date8state, order, reopen, county, california, governor, business, open, jersey, guideline9"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "List of all the 39 topics with corresponding top keywords.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "He has not briefed Mr. Trump in weeks, preferring to work with Dr. Deborah L. Birx, who helps coordinate the administration's coronavirus response, or to send his messages through Vice President Mike Pence. In the piece, Mr. Navarro presented what White House officials have been saying privately about Dr. Fauci, and what Mr. Trump has said publicly: They like Dr. Fauci personally, but he has made mistakes.New York Post: President Trump said Wednes-day he has a \"very good relationship\" with White House coronavirus task force member Anthony Fauci, despite an op-ed by one of his top advisers that trashed the immunologist. Trump distanced himself from trade adviser Peter Navarro's op-ed that said Fauci \"has been wrong about everything.\" \"I get along very well with Dr. Fauci,\" Trump told reporters in the Oval Office. On Monday, White House Press Secretary Kayleigh McEnany denied a Washington Post report that said reporters were", "figure_data": ""}], "doi": "10.5281/zenodo.4381785"}