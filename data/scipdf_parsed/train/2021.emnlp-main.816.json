{"authors": "Bruno Taill\u00e9; Vincent Guigue; Geoffrey Scoutheeten; Patrick Gallinari", "pub_date": "", "title": "Separating Retention from Extraction in the Evaluation of End-to-end Relation Extraction", "abstract": "State-of-the-art NLP models can adopt shallow heuristics that limit their generalization capability (McCoy et al., 2019). Such heuristics include lexical overlap with the training set in Named-Entity Recognition (Taill\u00e9 et al., 2020a) and Event or Type heuristics in Relation Extraction (Rosenman et al., 2020). In the more realistic end-to-end RE setting, we can expect yet another heuristic: the mere retention of training relation triples. In this paper we propose several experiments confirming that retention of known facts is a key factor of performance on standard benchmarks. Furthermore, one experiment suggests that a pipeline model able to use intermediate type representations is less prone to over-rely on retention.", "sections": [{"heading": "Introduction", "text": "Information Extraction (IE) aims at converting the information expressed in a text into a predefined structured format of knowledge. This global goal has been divided into subtasks easier to perform automatically and evaluate. Hence, Named Entity Recognition (NER) and Relation Extraction (RE) are two key IE tasks among others such as Coreference Resolution (CR), Entity Linking or Event Extraction. Traditionally performed as a pipeline (Bach and Badaskar, 2007), these two tasks can be tackled jointly in order to model their interdependency, alleviate error propagation and obtain a more realistic evaluation setting (Roth and Yih, 2002;Li and Ji, 2014).\nFollowing the general trend in Natural Language Processing (NLP), the recent quantitative improvements reported on Entity and Relation Extraction benchmarks are at least partly explained by the use of larger and larger pretrained Language Models (LMs) such as BERT (Devlin et al., 2019) to obtain contextual word representations. Concurrently, Code for reproducing our evaluation settings is available at github.com/btaille/retex there is a realization that new evaluation protocols are necessary to better understand the strengths and shortcomings of the obtained neural network models, beyond a single holistic metric on an hold-out test set (Ribeiro et al., 2020).\nIn particular, generalisation to unseen data is a key factor in the evaluation of deep neural networks. It is all the more important in IE tasks that revolve around the extraction of mentions: small spans of words that are likely to occur in both the evaluation and training datasets. This lexical overlap has been shown to be correlated to neural networks performance in NER (Augenstein et al., 2017;Taill\u00e9 et al., 2020a). For pipeline RE, Rosenman et al. (2020) and Peng et al. (2020) expose shallow heuristics in neural models: relying too much on the type of the candidate arguments or on the presence of specific triggers in their contexts.\nIn end-to-end Relation Extraction, we can expect that these NER and RE heuristics are combined. In this work, we argue that current evaluation benchmarks measure both the desired ability to extract information contained in a text but also the capacity of the model to simply retain labeled (head, predicate, tail) triples during training. And when the model is evaluated on a sentence expressing a relation seen during training, it is hard to disentangle which of these two behaviours is predominant. However, we can hypothesize that the model can simply retrieve previously seen information acting like a mere compressed form of knowledge base probed with a relevant query. Thus, testing on too much examples with seen triples can lead to overestimate the generalizability of a model.\nEven without labeled data, LMs are able to learn some relations between words that can be probed with cloze sentences where an argument is masked (Petroni et al., 2019). This raises the additional question of lexical overlap with the orders of magnitude larger unlabeled LM pretraining corpora that will remain out of scope of this paper.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Datasets and Models", "text": "We study three recent end-to-end RE models on CoNLL04 (Roth and Yih, 2004), ACE05 (Walker et al., 2006) and SciERC (Luan et al., 2018). They rely on various pretrained LMs and for a fairer comparison, we use BERT (Devlin et al., 2019) on ACE05 and CoNLL04 and SciBERT (Beltagy et al., 2019) on SciERC 1 .\nPURE (Zhong and Chen, 2021) follows the pipeline approach. The NER model is a classical span-based model (Sohrab and Miwa, 2018). Special tokens corresponding to each predicted entity span are added and used as representation for Relation Classification. For a fairer comparison with other models, we study the approximation model that only requires one pass in each encoder and limits to sentence-level prediction. However, it still requires finetuning and storing two pretrained LMs instead of a single one for the following models.\nSpERT (Eberts and Ulges, 2020) uses a similar span-based NER module. RE is performed based on the filtered representations of candidate arguments as well as a max-pooled representation of their middle context. While Entity Filtering is close to the pipeline approach, the NER and RE modules share a common entity representation and are trained jointly. We also study the ablation of the max-pooled context representation that we denote Ent-SpERT.\nTwo are better than one (TABTO) (Wang and Lu, 2020) intertwines a sequence encoder and a table encoder in a Table Filling approach (Miwa and Sasaki, 2014). Contrary to previous models the pretrained LM is frozen and both the final hidden states and attention weights are used by the encoders. The prediction is finally performed by a Multi-Dimensional RNN (MD-RNN). Because it is not based on span-level predictions, this model cannot detect nested entities, e.g. on SciERC.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Partitioning by Lexical Overlap", "text": "Following (Augenstein et al., 2017;Taill\u00e9 et al., 2020a), we partition the entity mentions in the test set based on lexical overlap with the training set. We distinguish Seen and Unseen mentions and also extend this partition to relations. We denote a relation as an Exact Match if the same (head, predicate, tail) triple appears in the train set; as a Partial 1 More implementation details in Appendix A Match if one of its arguments appears in the same position in a training relation of same type; and as New otherwise.\nWe implement a naive Retention Heuristic that tags an entity mention or a relation exactly present in the training set with its majority label. We report micro-averaged Precision, Recall and F1 scores for both NER and RE in Table 1.\nAn entity mention is considered correct if both its boundaries and type have been correctly predicted. For RE, we report scores in the Boundaries and Strict settings (Bekoulis et al., 2018;Taill\u00e9 et al., 2020b). In the Boundaries setting, a relation is correct if its type is correct and the boundaries of its arguments are correct, without considering the detection of their types. The Strict setting adds the requirement that the entity type of both argument is correct.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "Dataset Specificities", "text": "We first observe very different statistics of Mention and Relation Lexical Overlap in the three datasets, which can be explained by the singularities of their entities and relations. In CoNLL04, mentions are mainly Named Entities denoted with proper names while in ACE05 the surface forms are very often common names or even pronouns, which explains the occurrence of training entity mentions such as \"it\", \"which\", \"people\" in test examples. This also leads to a weaker entity label consistency (Fu et al., 2020a): \"it\" is labeled with every possible entity type and appears mostly unlabeled whereas a mention such as \"President Kennedy\" is always labeled as a person in CoNLL04. Similarly, mentions in SciERC are common names which can be tagged with different labels and they can also be nested. Both the poor label consistency as well as the nested nature of entities hurt the performance of the retention heuristic.\nFor RE, while SciERC has almost no exact overlap between test and train relations, ACE05 and CoNLL04 have similar levels of exact match. The larger proportion of partial match in ACE05 is explained by the pronouns that are more likely to co-occur in several instances. The difference in performance of the heuristic is also explained by a poor relation label consistency.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Lexical Overlap Bias", "text": "As expected, this first evaluation setting enables to expose an important lexical overlap bias, already  SpERT 89.0 0.1 74.1 1.0 86.5 0.2  77.0 1.1 52.2 1.1 38.9 1.0 57.0 0.8  75.1 1.2 48.4 1.0 36.3 2.0 53.9 0.8  SpERT  89.4 0.2 74.2 0.8 86.8 0.2  84.8 0.8 59.6 0.7 42.3 1.1 64.0 0.6  82.6 0.8 55.6 0.7 38.4 1.1 60.6 0.5  TABTO  89.7 0.1 77.4 0.8 87.5 0.2  85.9 0.9 62.6 1.8 44.6 2.9 66.4 1.3  81.6 1.5 58.1 1.6 38.5 3.1 61.7 1.1  PURE  90.5 0.2 80.0 0.3 88.7 0.1  86.0 1.3 60.5 1.0 47.1 1.6 65.1 0.7  84.1 1.1 57.9 1.3 44.0 2.0 62.6   discussed in NER, in end-to-end Relation Extraction. On every dataset and for every model micro F1 scores are the highest for Exact Match relations, then Partial Match and finally totally unseen relations. This is a first confirmation that retention plays an important role in the measured overall performance of end-to-end RE models.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model Comparisons", "text": "While we cannot evaluate TABTO on SciERC because it is unfit for extraction of nested entities, we can notice different hierarchies of models on every dataset suggesting that there is no one-size-fits-all best model, at least in current evaluation settings.\nThe most obvious comparison is between SpERT and Ent-SpERT where the explicit representation of context is ablated. This results in a loss of performance on the RE part and especially on partially matching or new relations for which the entity representations pairs have not been seen. Ent-SpERT is particularly effective on Exact Matches on CoNLL04, suggesting its retention capability.\nOther comparisons are more difficult, given the numerous variations between the very structure of each model as well as training procedures. However, the PURE pipeline setting seems to only be more effective on ACE05 where its NER performance is significantly better, probably because learning a separate NER and RE encoder enables to learn and capture more specific information for each distinctive task. Even then, TABTO yields better Boundaries performance only penalized on the Strict setting by entity types confusions. On the contrary, on CoNLL04, TABTO significantly outperforms its counterparts, especially on unseen relations. This indicates that it proposes a more effective incorporation of contextual information in this case where relation and argument types are mapped bijectively.\nOn SciERC, performance of all models is already compromised at the NER level before the RE step, which makes further distinction between model performance even more difficult.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Swapping Relation Heads and Tails", "text": "A second experiment to validate that retention is used as a heuristic in models' predictions is to modify their input sentences in a controlled manner", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Sentence", "text": "Ground Truth Relation Original John Wilkes Booth , who assassinated President Lincoln , was an actor .\n(John Wilkes Booth, Kill, President Lincoln) Swapped President Lincoln , who assassinated John Wilkes Booth , was an actor .\n(President Lincoln, Kill, John Wilkes Booth)  similarly to what is proposed in (Ribeiro et al., 2020). We propose a very focused experiment that consists in selecting asymmetric relations that occur between entities of same type and swap the head with the tail in the input. If the model predicts the original triple, then it over relies on the retention heuristic, whereas finding the swapped triple is an evidence of broader context incorporation. We show an example in Table 2.\nBecause of the requirements of this experiment, we have to limit to two relations in CoNLL04: \"Kill\" between people and \"Located in\" between locations. Indeed, CoNLL04 is the only dataset with a bijective mapping between the type of a relation and the types of its arguments and the consistent proper nouns mentions makes the swaps mostly grammatically correct. For each relation type, we only consider sentences with exactly one instance of corresponding relation and swap its arguments. We only consider this relation in the RE scores reported in Table 3. We use the strict RE score as well as revRE which measures the extraction of the reverse relation, not expressed in the sentence.\nFor each relation, the hierarchy of models corresponds to the overall CoNLL04. Swapping arguments has a limited effect on NER, mostly for the \"Located in\" relation. However, it leads to a drop in RE for every model and the revRE score indicates that SpERT and TABTO predict the reverse relation more often than the newly expressed one. This is another proof of the retention heuristic of end-to-end models, although it might also be attributed to the language model to the language model. In particular for the \"Located in\" relation, swapped heads and tails are not exactly equivalent since the former are mainly cities and the latter countries.\nOn the contrary, the PURE model is less prone to information retention, as shown by its revRE scores significantly smaller than the standard RE scores on swapped sentences. Hence, it outperforms SpERT and TABTO on swapped sentences despite being the least effective on the original dataset.The important discrepancy in results can be explained by the different types of representations used by these models. The pipeline approach allows the use of argument type representations in the Relation Classifier whereas most end-to-end models use lexical features in a shared entity representation used for both NER and RE.\nThese conclusions from quantitative results are validated qualitatively. We can observe that the four predominant patterns are intuitive behaviours on sentences with swapped relations: retention of the incorrect original triple, prediction of the correct swapped triple and prediction of none or both triples. We report some examples in Table 9 and Table 10 in the Appendix.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Several works on generalization of NER models mention lexical overlap with the training as a key indicator of performance. Augenstein et al. (2017) separate mentions in the test set as seen and unseen during training and measure out-of-domain generalization in an extensive study of two CRF based models and SENNA combining a Convolutional Neural Network with a CRF (Collobert and Weston, 2011). Taill\u00e9 et al. (2020a) compare the effect of introducing contextual embeddings in the classical BiLSTM-CRF architecture in a similar setting and show that they help close the performance gap on unseen mentions and domains. Arora et al. (2020); Fu et al. (2020b,a) study the influence of several properties such as lexical overlap, label consistency and entity length on state-of-the-art models performance. They model these properties as continuous scores associated to each mention and bucketized for evaluation. Lexical overlap has also been mentioned in Coreference Resolution (Moosavi and Strube, 2017) where coreferent mentions tend to co-occur in the test and train sets. In this line of works, the impact of lexical overlap is measured either by separating performance depending on the property of mentions (seen or unseen) or with outof-domain evaluation with a test set from a different dataset with lower lexical overlap with the train set.\nAnother recently proposed method for finegrained evaluation of NLP models beyond a single benchmark score is to modify the test sentences in a controlled manner. McCoy et al. (2019) expose lexical overlap as a shallow heuristic adopted by state-of-the-art Natural Language Inference models, especially by swapping subject and object of verbs in the hypothesis of some examples where the premise entails the hypothesis. While such a modification changes the label of these examples to non-entailment, all models tested show a spectacular drop of accuracy on these models. Ribeiro et al. (2020) propose a broader set of test set modifications to individually test robustness of NLP models to several patterns such as the introduction of negation, swapping words with synonyms, changing tense and much more.\nIn pipeline RE where ground truth candidate arguments are given, models often use intermediate representations based on entity types that reduce lexical overlap issues. However, Rosenman et al. (2020) show that they still tend to adopt shallow heuristics based on the type of the arguments and the presence of triggers indicative of the presence of a relation. They propose hard cases with several mentions of same types for which Relation Classifiers struggle connecting the correct pair. Concurrently, Peng et al. (2020) confirm that RE benchmarks present shallow cues such as the type of the candidate arguments that can be used alone to infer the relation.\nWe propose to extend previous work on NER and RE to the more realistic end-to-end RE setting with two of the previously described approaches: 1) separating performance by lexical overlap of mentions or argument pairs and 2) modifying some CoNLL04 test examples by swapping relations heads and tails.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "In this paper, we study three state-of-the-art endto-end Relation Extraction models in order to highlight their tendency to retain seen relations. We confirm that retention of seen mentions and relations play an important role in overall RE performance and can explain the relatively higher scores on CoNLL04 and ACE05 compared to SciERC. Furthermore, our experiment on swapping relation heads and tails tends to show that the intermediate manipulation of type representations instead of lexical features enabled in the pipeline PURE model makes it less prone to over-rely on retention.\nWhile the limited extend of our swapping experiment is an obvious limitation of this work, it shows limitations of both current benchmarks and models. It is an encouragement to propose new benchmarks that might be easily modified by design to probe such lexical overlap heuristics. Contextual information could for example be contained in templates of that would be filled with different (head, tail) pairs either seen or unseen during training.\nFurthermore, pretrained Language Models can already capture relational information between phrases (Petroni et al., 2019) and further experiments could help distinguish their role in the retention behaviour of RE models.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "A Implementation Details", "text": "For every model, we use the original code associated with the papers with the default best performing hyperparameters unless stated otherwise. We run 5 runs on a single NVIDIA 2080Ti GPU for each of them on each dataset. For CoNLL04 and ACE05, we train each model with both the cased and uncased versions of BERT BASE and only keep the best performing setting. PURE (Zhong and Chen, 2021) 1 We use the approximation model and limit use a context window of 0 to only use the current sentence for prediction and be able to compare with other models. For ACE05, we use the standard bert-base-uncased LM but use the bert-base-cased version on CoNLL04 which results in a significant +2.4 absolute improvement in RE Strict micro F1 score.\nSpERT (Eberts and Ulges, 2020) 2 We use the original implementation as is with bert-base-cased for both ACE05 and CoNLL04 since the uncased version is not beneficial, even on ACE05 where there are fewer proper nouns. For the Ent-SpERT ablation, we simply remove the max-pooled context representation from the final concatenation in the RE module. This modifies the RE classifier's input dimension from the original 2354 to 1586.\nTwo are better than one (TABTO) (Wang and Lu, 2020) 3 We use the original implementation with bert-base-uncased for both ACE05 and CoNLL04 since the cased version is not beneficial on CoNLL04.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "B Datasets Statistics", "text": "We present general datasets statistics in Table 4.\nWe also compute average values of some entity and relation attributes inspired by (Fu et al., 2020a) and reported in Table 5.\nWe report two of their entity attributes: entity length in number of tokens (eLen) and entity label consistency (eCon). Given a test entity mention, its label consistency is the number of occurrences in the training set with the same type divided by its total number of occurrences. It is zero for unseen mentions. Because eCon reflects both the ambiguity of labels for seen entities and the proportion of unseen entities, we propose to introduce the eCon*  score that only averages label consistency of seen mentions and eLex, the proportion of entities with lexical overlap with the train set. We introduce similar scores for relations. Relation label consistency (rCon) extends label consistency for triples. Argument types label constitency (aCon) considers the labels of every pair of mentions of corresponding types in the training set. Because pairs of types are all seen during training we do not decompose aCon into aCon* and aLex. Argument length (aLen) is the sum of the lengths of the head and tail mentions. Argument distance (aDist) is the number of tokens between the head and the tail of a relation.\nWe present a more complete report of overall Precision, Recall and F1 scores that can be interpreted in light of these statistics in Table 6.    ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We thank the anonymous reviewers for their thoughtful comments. Part of this work was performed while Bruno Taill\u00e9 was an employee of BNP Paribas and supported by the French Ministry of Higher Education, Research and Innovation under the CIFRE convention 2018/0327.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Contextual embeddings: When are they worth it?", "journal": "", "year": "2020", "authors": "Simran Arora; Avner May; Jian Zhang; Christopher R\u00e9"}, {"title": "Generalisation in named entity recognition: A quantitative analysis", "journal": "Computer Speech & Language", "year": "2017", "authors": "Isabelle Augenstein; Leon Derczynski; Kalina Bontcheva"}, {"title": "A Review of Relation Extraction. Literature review for Language and Statistics II 2", "journal": "", "year": "2007", "authors": "Nguyen Bach; Sameer Badaskar"}, {"title": "Adversarial training for multi-context joint entity and relation extraction", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Giannis Bekoulis; Johannes Deleu; Thomas Demeester; Chris Develder"}, {"title": "SciB-ERT: A pretrained language model for scientific text", "journal": "", "year": "2019", "authors": "Iz Beltagy; Kyle Lo; Arman Cohan"}, {"title": "Natural language processing (almost) from scratch", "journal": "Journal of Machine Learning Research", "year": "2011", "authors": "Ronan Collobert; Jason Weston"}, {"title": "BERT: Pre-training of deep bidirectional transformers for language understanding", "journal": "Long and Short Papers", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Span-based Joint Entity and Relation Extraction with Transformer Pre-training", "journal": "", "year": "2020", "authors": "Markus Eberts; Adrian Ulges"}, {"title": "Interpretable multi-dataset evaluation for named entity recognition", "journal": "", "year": "2020", "authors": "Jinlan Fu; Pengfei Liu; Graham Neubig"}, {"title": "Rethinking Generalization of Neural Models: A Named Entity Recognition Case Study", "journal": "", "year": "2020", "authors": "Jinlan Fu; Pengfei Liu; Qi Zhang; Xuanjing Huang"}, {"title": "Incremental Joint Extraction of Entity Mentions and Relations", "journal": "Association for Computational Linguistics", "year": "2014", "authors": "Qi Li; Heng Ji"}, {"title": "Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Yi Luan; Luheng He; Mari Ostendorf; Hannaneh Hajishirzi"}, {"title": "Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference", "journal": "", "year": "2019", "authors": "Tom Mccoy; Ellie Pavlick; Tal Linzen"}, {"title": "Modeling joint entity and relation extraction with table representation", "journal": "", "year": "2014", "authors": "Makoto Miwa; Yutaka Sasaki"}, {"title": "Lexical features in coreference resolution: To be used with caution", "journal": "Association for Computational Linguistics", "year": "2017", "authors": "Sadat Nafise; Michael Moosavi;  Strube"}, {"title": "Learning from Context or Names? An Empirical Study on Neural Relation Extraction", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Hao Peng; Tianyu Gao; Xu Han; Yankai Lin; Peng Li; Zhiyuan Liu; Maosong Sun; Jie Zhou"}, {"title": "Association for Computational Linguistics", "journal": "", "year": "2019", "authors": "Fabio Petroni; Tim Rockt\u00e4schel; Sebastian Riedel; Patrick Lewis; Anton Bakhtin; Yuxiang Wu; Alexander Miller"}, {"title": "Beyond accuracy: Behavioral testing of NLP models with CheckList", "journal": "", "year": "2020", "authors": "Tongshuang Marco Tulio Ribeiro; Carlos Wu; Sameer Guestrin;  Singh"}, {"title": "Exposing Shallow Heuristics of Relation Extraction Models with Challenge Data", "journal": "Online. Association for Computational Linguistics", "year": "2020", "authors": "Alon Shachar Rosenman; Yoav Jacovi;  Goldberg"}, {"title": "Probabilistic Reasoning for Entity & Relation Recognition", "journal": "", "year": "2002", "authors": "Dan Roth; Wen-Tau Yih"}, {"title": "A linear programming formulation for global inference in natural language tasks", "journal": "", "year": "2004", "authors": "Dan Roth; Wen-Tau Yih"}, {"title": "Deep exhaustive model for nested named entity recognition", "journal": "", "year": "2018", "authors": "Golam Mohammad; Makoto Sohrab;  Miwa"}, {"title": "Contextualized Embeddings in Named-Entity Recognition: An Empirical Study on Generalization", "journal": "Springer International Publishing", "year": "2020", "authors": "Bruno Taill\u00e9; Vincent Guigue; Patrick Gallinari"}, {"title": "Let's Stop Incorrect Comparisons in End-to-end Relation Extraction!", "journal": "", "year": "2020", "authors": "Bruno Taill\u00e9; Vincent Guigue; Geoffrey Scoutheeten; Patrick Gallinari"}, {"title": "ACE 2005 Multilingual Training Corpus. Linguistic Data Consortium", "journal": "", "year": "2006", "authors": "Christopher Walker; Stephanie Strassel; Julie Medero; Kazuaki Maeda"}, {"title": "Two are better than one: Joint entity and relation extraction with tablesequence encoders", "journal": "", "year": "2020", "authors": "Jue Wang; Wei Lu"}, {"title": "A Frustratingly Easy Approach for Entity and Relation Extraction", "journal": "", "year": "2021", "authors": "Zexuan Zhong; Danqi Chen"}, {"title": "A fired a highpowered rifle at B 's motorcade from the sixth floor of what is now the Dallas County Administration Building", "journal": "", "year": "1963", "authors": ""}], "figures": [{"figure_label": "", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "SpERT 95.9 0.3 81.9 0.2 88.9 0.2 92.3 1.4 60.8 1.4 54.6 1.3 64.8 0.9 92.3 1.4 60.8 1.4 54.2 1.2 64.7 0.8 SpERT 95.4 0.4 81.2 0.4 88.3 0.2 91.4 0.6 67.0 1.1 59.0 1.4 69.3 1.2 91.4 0.6 66.9 1.1 58.5 1.4 69.0 1.2 TABTO 95.4 0.4 83.1 0.7 89.2 0.5 92.6 1.5 72.6 2.1 64.8 1.0 74.0 1.4 92.6 1.5 72.1 1.8 64.7 1.1 73.8 1.", "figure_data": "0.9CoNLL04proportion50%50%23%34%43%23%34%43%heuristic86.0-59.790.9--35.590.9--35.5Ent-SciERCproportion23%77%<1%30%69%<1%30%69%heuristic31.3-20.1---0.7---0.7Ent-SpERT 77.6 1.0 64.0 0.6 67.3 0.6-48.1 0.7 41.9 0.6 43.8 0.5-38.1 1."}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Test NER and RE F1 Scores separated by lexical overlap with the training set. Exact Match RE scores are not reported on SciERC where the support is composed of only 5 exactly seen relation instances. Average and standard deviations on five runs.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Example of Swapped sentence. The Triple (John Wilkes Booth, Kill, President Lincoln) is present in the training set and the retention behaviours lead models to extract this triple when probed with the swapped sentence expressing the reverse relation.", "figure_data": "F1NER \u2191 O SRE \u2191 OSrevRE \u2193 O SKillEnt-SpERT 91.6 91.785.1 35.4-58.5SpERT91.4 92.686.2 35.0-57.8TABTO92.0 92.889.6 27.6-59.5PURE90.5 90.784.1 52.3-14.3Located inEnt-SpERT 90.0 87.078.3 30.3-24.8SpERT88.6 87.775.0 24.9-33.5TABTO90.1 88.985.3 36.1-34.9PURE89.0 83.781.2 59.3-5.1"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "", "figure_data": ""}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Average of some entity and relation attributes in the test set.", "figure_data": "EntitiesRelationseCon eCon* eLexeLenrCon rCon* rLexaConaLenaDistACE0565%78%82%1.115%62%23%7.1%2.32.8CoNLL04 49%98%50%1.521%91%23%29%3.85.8SciERC17%74%23%1.60.4%74%0.5%13%4.75.3NER Table 5: \u00b5F 1 P RF1PRE Boundaries RF1PRE Strict RF1ACE05heuristic44.771.955.123.622.323.021.420.220.8Ent-SpERT86.7 0.386.3 0.386.5 0.256.7 1.057.4 0.757.0 0.853.5 1.054.2 0.853.9 0.8SpERT87.2 0.286.5 0.386.8 0.268.1 1.160.5 0.564.0 0.664.4 1.157.2 0.460.6 0.5TABTO86.7 0.388.3 0.687.5 0.271.0 2.762.5 2.566.4 1.366.1 2.658.1 2.161.8 1.1PURE88.8 0.388.6 0.188.7 0.167.4 0.863.0 0.865.1 0.764.8 1.060.5 1.062.6 0.9CoNLL04heuristic75.949.259.784.122.535.584.122.535.5Ent-SpERT88.4 0.689.3 0.788.9 0.259.3 0.571.3 1.564.8 0.959.2 0.571.2 1.564.7 0.8SpERT87.9 0.688.7 0.388.3 0.269.7 2.369.0 0.569.3 1.269.4 2.368.7 0.669.0 1.2TABTO89.0 0.789.3 0.389.2 0.575.6 3.272.6 1.974.0 1.475.4 3.172.4 1.873.8 1.2PURE88.3 0.488.5 0.588.4 0.268.6 2.068.2 1.668.3 1.068.5 2.068.1 1.568.2 0.9SciERCheuristic18.821.520.13.50.40.73.50.40.7Ent-SpERT68.0 0.366.6 0.967.3 0.644.8 0.742.9 1.043.8 0.532.9 0.931.5 1.532.1 1.2SpERT67.6 0.567.6 0.267.6 0.349.3 1.447.2 1.348.2 1.137.0 1.335.4 1.036.2 1.0PURE68.2 0.666.2 0.967.2 0.450.2 0.945.2 1.047.6 0.337.6 1.233.8 0.735.6 0.6"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Overall micro-averaged Test NER and Strict RE Precision, Recall and F1 scores. Average and standard deviations on five runs. We can observe that the recall of the heuristic is correlated with the proportions of seen entities or triples (eLex or rLex). Its particularly high precision on CoNLL04 seems rather linked to the important label consistency of seen entities and relation (eCon* and rCon*).", "figure_data": "DatasetEntity TypesRelation TypesACE05Facility, Geo-political Entity, Location, Per-Artifact, Gen-affiliation, Org-affiliation, Part-son, Vehicle, Weaponwhole, Person-social, PhysicalCoNLL04 Location, Organization, Other, PersonKill, Live in, Located in, Organization basedin, Work forSciERCGeneric, Material, Method, Metric, OtherCompare*, Conjunction*, Evaluate for, Fea-Scientific Term, Taskture of, Hyponym of, Part of, Used for"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Entity and Relation Types of end-to-end RE datasets. SciERC presents two types of symmetric relations denoted with a *. SpERT 91.3 0.9 92.1 0.7 91.7 0.7 31.8 5.3 40.0 8.3 35.4 6.5 52.8 5.6 65.8 7.2 58.5 5.7 SpERT 92.6 1.8 92.6 0.8 92.6 1.2 33.0 4.4 37.3 7.4 35.0 5.6 54.8 5.1 61.3 4.1 57.8 4.0 TABTO 92.8 0.8 92.7 0.9 92.8 0.7 26.8 3.6 28.4 4.1 27.6 3.8 57.8 3.1 61.3 3.0 59.5 2.8 PURE 92.0 0.5 89.5 1.0 90.7 0.5 65.2 6.0 44.0 7.4 52.3 6.5 17.8 2.3 12.0 2.3 14.3 2.2 SpERT 86.7 1.9 87.4 2.7 87.0 2.1 38.0 8.5 25.4 2.8 30.3 4.6 30.2 5.2 21.1 5.8 24.8 5.7 SpERT 87.3 1.4 88.0 0.9 87.7 1.1 34.8 14.8 19.5 6.7 24.9 9.2 45.6 17.0 26.5 10.5 33.5 13.0 TABTO 89.0 0.6 88.8 0.9 88.9 0.8 46.5 6.6 29.7 5.7 36.1 5.8 45.2 5.2 28.6 3.7 34.9 3.6 PURE 82.7 0.8 84.6 0.8 83.7 0.5 74.9 7.6 49.7 4.7 59.3", "figure_data": "NER \u2191RE Strict \u2191Reverse RE Strict \u2193PRF1PRF1PRFOriginalEnt-SpERT 91.7 0.4 91.5 0.7 91.6 0.4 SpERT 91.7 2.1 91.0 1.0 91.4 1.2 TABTO 91.8 0.6 92.2 0.5 92.0 0.482.9 2.7 87.6 1.8 85.1 0.9 88.1 3.1 84.4 1.4 86.2 1.4 88.8 1.6 90.7 3.3 89.6 1.3---------PURE Ent-SpERT 90.1 0.8 89.8 1.5 90.0 0.7 91.5 0.9 89.6 0.6 90.5 0.6 SpERT 89.8 1.2 87.5 1.5 88.6 1.1 TABTO 90.1 1.3 90.0 1.8 90.1 1.5 PURE 88.6 1.1 89.4 1.8 89.0 1.0 Ent-Located in Kill Swap Original Swap Ent-3.0 87.2 2.1 81.3 1.1 84.1 1.2 80.8 3.7 76.2 3.2 78.3 2.4 77.2 2.8 73.0 3.0 75.0 2.0 93.0 3.3 78.9 4.6 85.3 3.9 89.3 4.0 74.6 3.7 81.2 2.6-----6.5 1.8-----4.3 1.3-----5.1 1.5"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Detailed results of the Swap Relation Experiment with Precision, Recall and F1 scores.", "figure_data": ""}], "doi": "10.18653/v1/2020.acl-main.236"}