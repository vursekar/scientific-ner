{"authors": "Peter Pol\u00e1k; Ngoc-Quan Ngoc; Tuan-Nam Nguyen; Danni Liu; Carlos Mullov; Jan Niehues; Ond\u0159ej Bojar; Alexander Waibel", "pub_date": "", "title": "CUNI-KIT System for Simultaneous Speech Translation Task at IWSLT 2022", "abstract": "In this paper, we describe our submission to the Simultaneous Speech Translation at IWSLT 2022. We explore strategies to utilize an offline model in a simultaneous setting without the need to modify the original model. In our experiments, we show that our onlinization algorithm is almost on par with the offline setting while being 3\u00d7 faster than offline in terms of latency on the test set. We also show that the onlinized offline model outperforms the best IWSLT2021 simultaneous system in medium and high latency regimes and is almost on par in the low latency regime. We make our system publicly available. 1   ", "sections": [{"heading": "Introduction", "text": "This paper describes the CUNI-KIT submission to the Simultaneous Speech Translation task at IWSLT 2022 (Anastasopoulos et al., 2022) by Charles University (CUNI) and Karlsruhe Institute of Technology (KIT).\nRecent work on end-to-end (E2E) simultaneous speech-to-text translation (ST) is focused on training specialized models specifically for this task. The disadvantage is the need of storing an extra model, usually a more difficult training and inference setup, increased computational complexity Liu et al., 2021) and risk of performance degradation if used in offline setting (Liu et al., 2020a).\nIn this work, we base our system on a robust multilingual offline ST model that leverages pretrained wav2vec 2.0 (Baevski et al., 2020) and mBART (Liu et al., 2020b). We revise the onlinization approach by Liu et al. (2020a) and propose an improved technique with a fully controllable qualitylatency trade-off. We demonstrate that without any change to the offline model, our simultaneous system in the mid-and high-latency regimes is on par with the offline performance. At the same time, the model outperforms previous IWSLT systems in medium and high latency regimes and is almost on par in the low latency regime. Finally, we observe a problematic behavior of the average lagging metric for speech translation  when dealing with long hypotheses, resulting in negative values. We propose a minor change to the metric formula to prevent this behavior.\nOur contribution is as follows:\n\u2022 We revise and generalize onlinization proposed by Liu et al. (2020a); Nguyen et al. (2021) and discover parameter enabling quality-latency trade-off,\n\u2022 We demonstrate that one multilingual offline model can serve as simultaneous ST for three language pairs,\n\u2022 We demonstrate that an improvement in the offline model leads also to an improvement in the online regime,\n\u2022 We propose a change to the average lagging metric that avoids negative values.", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Simultaneous speech translation can be implemented either as a (hybrid) cascaded system (Kolss et al., 2008;Elbayad et al., 2020;Liu et al., 2020a;Bahar et al., 2021) or an end-to-end model Liu et al., 2021). Unlike for the offline speech translation where cascade seems to have the best quality, the end-to-end speech translation offers a better qualitylatency trade-off (Ansari et al., 2020;Liu et al., 2021;Anastasopoulos et al., 2021). End-to-end systems use different techniques to perform simultaneous speech translation. Han et al. (2020) uses wait-k (Ma et al., 2019) model and metalearning  to alleviate the data scarcity. Liu et al. (2020a) uses a unidirectional encoder with monotonic cross-attention to limit the dependence on future context. Other work (Liu et al., 2021) proposes Cross Attention augmented Transducer (CAAT) as an extension of RNN-T (Graves, 2012). Nguyen et al. (2021) proposed a hypothesis stability detection for automatic speech recognition (ASR). The shared prefix strategy finds the longest common prefix in all beams. Liu et al. (2020a) explore such strategies in the context of speech recognition and translation. The most promising is the longest common prefix of two consecutive chunks. The downside of this approach is the inability to parametrize the quality-latency trade-off. We directly address this in our work.", "n_publication_ref": 14, "n_figure_ref": 0}, {"heading": "Onlinization", "text": "In this section, we describe the onlinization of the offline model and propose two ways to control the quality-latency trade-off.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Incremental Decoding", "text": "Depending on the language pair, translation tasks may require reordering or a piece of information that might not be apparent until the source utterance ends. In the offline setting, the model processes the whole utterance at once, rendering the strategy most optimal in terms of quality. If applied in online mode, this ultimately leads to a large latency. One approach to reducing the latency is to break the source utterance into chunks and perform the translation on each chunk.\nIn this paper, we follow the incremental decoding framework described by Liu et al. (2020a). We break the input utterance into small fixed-size chunks and decode each time after we receive a new chunk. After each decoding step, we identify a stable part of the hypothesis using stable hypothesis detection. The stable part is sent to the user (\"committed\" in the following) and is no longer changed afterward (i.e., no retranslation). 2 Our current implementation assumes that the whole speech input fits into memory, in other words, we are only adding new chunks as they are arriving. This simplification is possible because the evaluation of the shared task is performed on segmented input, on individual utterances. With each newly arrived input chunk, the decoding starts with forced decoding of the already committed tokens and continues with beam search decoding.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Chunk Size", "text": "Speech recognition and translation use chunking for simultaneous inference with various chunk sizes ranging from 300 ms to 2 seconds (Liu, 2020;Nguyen et al., 2021) although the literature suggests that the turn-taking in conversational speech is shorter, around 200 ms (Levinson and Torreira, 2015). We investigate different chunk sizes in combination with various stable hypothesis detection strategies. As we document later, the chunk size is the principal factor that controls the quality-latency trade-off.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Stable Hypothesis Detection", "text": "Committing hypotheses from incomplete input presents a possible risk of introducing errors. To reduce the instability and trade time for quality, we employ a stable hypothesis detection. Formally, we define a function pref ix(W ) that, given a set of hypotheses (i.e., W c all if we want to consider the whole beam or W c best for the single best hypothesis obtained during the beam search decoding of the c-th chunk), outputs a stable prefix. We investigate several functions: Hold-n (Liu et al., 2020a) Hold-n strategy selects the best hypothesis in the beam and deletes the last n tokens from it:\nprefix(W c best ) = W 0:max(0,|W |\u2212n) ,(1)\nwhere W c best is the best hypothesis obtained in the beam search of c-th chunk. If the hypothesis has only n or fewer tokens, we return an empty string.\nLA-n Local agreement (Liu et al., 2020a) displays the agreeing prefixes of the two consecutive chunks. Unlike the hold-n strategy, the local agreement does not offer any explicit quality-latency trade-off. We generalize the strategy to take the agreeing prefixes of n consecutive chunks.\nDuring the first n \u2212 1 chunks, we do not output any tokens. From the n-th chunk on, we identify the longest common prefix of the best hypothesis of the n consecutive chunks:\nprefix(W c best ) = \u2205, if c < n, LCP(W c\u2212n+1 best , ..., W c best ), otherwise,(2)\nwhere LCP (\u2022) is longest common prefix of the arguments.\nSP-n Shared prefix (Nguyen et al., 2021) strategy displays the longest common prefix of all the items in the beam of a chunk. Similarly to the LA-n strategy, we propose a generalization to the longest common prefix of all items in the beams of the n consecutive chunks:\nprefix(W c all ) = \u2205, if c < n, LCP(W c\u2212n+1 beam 1...B , ..., W c beam 1...B ), otherwise,(3)\ni.e., all beam hypotheses 1, ..., B (where B is the beam size) of all chunks c \u2212 n + 1, ..., c.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Initial Wait", "text": "The limited context of the early chunks might result in an unstable hypothesis and an emission of erroneous tokens. The autoregressive nature of the model might cause further performance degradation in later chunks. One possible solution is to use longer chunks, but it inevitably leads to a higher latency throughout the whole utterance. To mitigate this issue, we explore a lengthening of the first chunk. We call this strategy an initial wait.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments Setup", "text": "In this section, we describe the onlinization experiments.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Evaluation Setup", "text": "We use the SimulEval toolkit . The toolkit provides a simple interface for evaluation of simultaneous (speech) translation. It reports the quality metric BLEU (Papineni et al., 2002;Post, 2018) and latency metrics Average Proportion (AP, Cho and Esipova 2016), Average Lagging (AL, Ma et al. 2019), and Differentiable Average Lagging (DAL, Cherry and Foster 2019) modified for speech source. Specifically, we implement an Agent class. We have to implement two important functions: policy(state) and predict(state), where state is the state of the agent (e.g., read processed input, emitted tokens, ...). The policy function returns the action of the agent: (1) READ to request more input, (2) WRITE to emit new hypothesis tokens.\nWe implement the policy as specified in Algorithm 1. The default action is READ. If there is a new chunk, we perform the inference and use the pref ix(W c ) function to find the stable prefix. If there are new tokens to display (i.e., |pref ix(W c )| > |pref ix(W c\u22121 )|), we return the WRITE action. As soon as our agent emits an endof-sequence (EOS) token, the inference of the utterance is finished by the SimulEval. We noticed that our model was emitting the EOS token quite often, especially in the early chunks. Hence, we ignore the EOS if returned by our model and continue the inference until the end of the source. 3 Algorithm 1 Policy function\nRequire: state if state.new_input > chunk_size then hypothesis \u2190 predict(state) if |hypothesis| > 0 then return W RIT E end if end if return READ", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "Speech Translation Models", "text": "In our experiments, we use two different models. First, we do experiments with a monolingual Model A, then for the submission, we use a multilingual and more robust Model B. 4 Model A is the KIT IWSLT 2020 model for the Offline Speech Translation task. Specifically, it is an end-to-end English to German Transformer model with relative attention. For more described description, refer to Pham et al. (2020b).", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Multilingual Model", "text": "For the submission, we use a multilingual Model B. We construct the SLT architecture with the encoder based on the wav2vec 2.0 (Baevski et al., 2020) and the decoder based on the autoregressive language model pretrained with mBART50 (Tang et al., 2020). wav2vec 2.0 is a Transformer encoder model which receives raw waveforms as input and generates high-level representations. The architecture consists of two main components: first, a convolution-based feature extractor downsamples long audio waveforms into features that have similar lengths with spectrograms.After that, a deep Transformer encoder uses self-attention and feedforward neural network blocks to transform the features without further downsampling.\nDuring the self-supervised training process, the network is trained with a contrastive learning strategy (Baevski et al., 2020), in which the already downsampled features are randomly masked and the model learns to predict the quantized latent representation of the masked time step.\nDuring the supervised learning step, we freeze the feature extraction weights to save memory since the first layers are among the largest ones. We fine-tune all of the weights in the Transformer encoder. Moreover, to make the model more robust to the fluctuation in absolute positions and durations when it comes to audio signals,we added the relative position encodings (Dai et al., 2019;Pham et al., 2020a) to alleviate this problem. 5 Here we used the same pretrained model with the speech recognizer, with the large architecture pretrained with 53k hours of unlabeled data. mBART50 is an encoder-decoder Transformerbased language model. During training, instead of the typical language modeling setting of predicting the next word in the sequence, this model is trained to reconstruct a sequence from its noisy version (Lewis et al., 2019) and later extended to a multilingual version (Liu et al., 2020b;Tang et al., 2020) in which the corpora from multiple languages are combined during training. mBART50 is the version that is pretrained on 50 languages.\nThe mBART50 model follows the Transformer encoder and decoder (Vaswani et al., 2017). During fine-tuning, we combine the mBART50 decoder with the wav2vec 2.0 encoder, where both encoder and decoder know one modality. The crossattention layers connecting the decoder with the encoder are the parts that require extensive finetuning in this case, due to the modality mismatch between pretraining and fine-tuning.\nFinally, we use the model in a multilingual setting, i.e., for English to Chinese, German, and Japanese language pairs by training on the combination of the datasets. The mBART50 vocabulary contains language tokens for all three languages and can be used to control the language output .\nFor more details on the model refer to Pham et al. (2022).", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "Test Data", "text": "For the onlinization experiments, we use MuST-C (Cattoni et al., 2021) tst-COMMON from the v2.0 release. We conduct all the experiments on the English-German language pair.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Experiments and Results", "text": "In this section, we describe the experiments and discuss the results.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Chunks Size", "text": "We experiment with chunk sizes of 250 ms, 500 ms, 1s, and 2 s. We combine the sizes of the chunks with different partial hypothesis selection strategies. The results are shown in Figure 1.\nThe results document that the chunk size parameter has a stronger influence on the trade-off than different prefix strategies. Additionally, this enables constant trade-off strategies (e.g., LA-2) to become flexible. ", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Stable Hypothesis Detection Strategies", "text": "We experiment with three strategies: hold-n (withholds last n tokens), shared prefix (SP-n; finds the longest common prefix of all beams in n consecutive chunks) and local agreement (LA-n; finds the longest common prefix of the best hypothesis in n consecutive chunks). For hold-n, we select n = 3, 6, 12; for SP-n, we select n = 1, 2 (n = 1 corresponds to the strategy by Nguyen et al. (2021)); for LA-n we select n = 2, 3, 4 (n = 2 corresponds to the strategy by Liu et al. (2020a)).\nThe results are in Figures 2 and 3. Hold-n The results suggest (see Figure 2) that the hold-n strategy can use either n or chunk size to control the quality-latency trade-off with equal effect. The only exception seems to be too low n <= 3, which slightly underperforms the options with higher n and shorter chunk size.\nLocal agreement (LA-n) The local agreement seems to outperform all other strategies (see Figure 3). LA-n for all n follows the same qualitylatency trade-off line. The advantage of LA-2 is in reduced computational complexity compared to the other LA-n strategies with n > 2.\nShared prefix (SP-n) SP-1 strongly underperforms other strategies in quality (see Figure 3). While the SP-1 strategy performs well in the ASR task (Nguyen et al., 2021), it is probably too lax for the speech translation task. The generalized and more conservative SP-2 performs much better.\nAlthough, the more relaxed LA-2, which considers only the best item in the beam, has a better qualitylatency trade-off curve than the more conservative SP-2.", "n_publication_ref": 3, "n_figure_ref": 4}, {"heading": "Initial Wait", "text": "As we could see in Section 5.1, the shorter chunk sizes tend to perform worse. One of the reasons might be the limited context of the early chunks. 6 To increase the early context, we prolong the first chunk to 2 seconds. The results are in Table 1. We see a slight (0.3 BLEU) increase in quality for a chunk size of 250  ms, though the initial wait does not improve the BLEU and a considerable increase in the latency.\nThe performance seems to be influenced mainly by the chunk size. The reason for smaller chunks' under-performance might be caused by (1) acoustic uncertainty towards the end of a chunk (e.g., words often get cut in the middle), or (2) insufficient information difference between two consecutive chunks. This is supported by the observation in Figure 3. Increasing the number of consecutive chunks (i.e., increasing the context for the decision) considered in the local agreement strategy (LA-2, 3, 4), improves the quality, while it adds latency.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "Negative Average Lagging", "text": "Interestingly, we noticed that some of the strategies achieved negative average lagging (e.g., LA-2 in Section 5.1) with a chunk size of 250 ms has AL of -36 ms). After a closer examination of the outputs, we found that the negative AL is in utterances where the hypothesis is significantly longer than the reference. Recall the average latency for speech input defined by :\nAL speech = 1 \u03c4 \u2032 (|X|) \u03c4 \u2032 (|X|) i=1 d i \u2212 d * i ,(4)\nwhere d i = j k=1 T k , j is the index of raw audio segment that has been read when generating y i , T k is duration of raw audio segment, \u03c4 \u2032 (|X|) = min{i|d i = |X| j=1 T j } and d * i are the delays of an ideal policy:\nd * i = (i \u2212 1) \u00d7 |X| j=1 T j /|Y * |,(5)\nwhere Y * is reference translation. If the hypothesis is longer than the reference, then d * i > d i , making the sum argument in Equation (4) negative. On the other hand, if we use the length of the hypothesis instead, then a shorter hypothesis would benefit. 7 We, therefore, suggest using the maximum of both to prevent the advantage of either a shorter or a longer hypothesis:\nd * i = (i \u2212 1) \u00d7 |X| j=1 T j /max(|Y|, |Y * |). (6)", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Submitted System", "text": "In this section, we describe the submitted system. We follow the allowed training data and pretrained models and therefore our submission is constrained (see Section 4.2.1 for model description). For stable hypothesis detection, we decided to use the local agreement strategy with n = 2. As shown in Section 5.2, the LA-2 has the best latencyquality trade-off along with other LA-n strategies. To achieve the different latency regimes, we use various chunk sizes, depending on the language pair. We decided not to use larger n > 2 to control the latency, as it increases the computation complexity while having the same effect as using a different chunk size. The results on MuST-C tst-COMMON are in Table 2. The quality-latency trade-off is in Figure 4.\nFrom Table 2 and Figure 4, we can see that the proposed method works well on two different models and various language pairs. We see that an improvement in the offline model (offline BLEU of 31.36 and 33.14 for Model A and B, respectively) leads to improvement in the online regime. Finally, we see that our method beats the best IWSLT 2021 system (USTC-NELSLIP (Liu et al., 2021)) in medium and high latency regimes using both models (i.e., a model trained from scratch and a model based on pretrained wav2vec and mBART), and is almost on par in the low latency regime (Model A is losing 0.35 BLEU and Model B is losing 0.47 BLEU).", "n_publication_ref": 1, "n_figure_ref": 2}, {"heading": "Computationally Aware Latency", "text": "In this paper, we do not report any computationally aware metrics, as our implementation of Transformers is slow. Later, we implemented the same onlinization approach using wav2vec 2.0 and mBART from Huggingface Transformers (Wolf et al., 2020). The new implementation reaches faster than realtime inference speed.   A) and the submitted system (Model B) on the MuST-C v2 tst-COMMON. We also include the best IWSLT 2021 system (USTC-NELSLIP (Liu et al., 2021)).", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "In this paper, we reviewed onlinization strategies for end-to-end speech translation models. We identified the optimal stable hypothesis detection strategy and proposed two separate ways of the qualitylatency trade-off parametrization. We showed that the onlinization of the offline models is easy and performs almost on par with the offline run. We demonstrated that an improvement in the offline model leads to improved online performance. We also showed that our method outperforms a dedicated simultaneous system. Finally, we proposed an improvement in the average latency metric.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "This work has received support from the project \"Grant Schemes at CU\" (reg.\nno. CZ.02.2.69/0.0/0.0/19_073/0016935), the grant 19-26934X (NEUREM3) of the Czech Science Foundation, the European Union's Horizon 2020 Research and Innovation Programme under Grant Agreement No 825460 (ELITR), and partly supported by a Facebook Sponsored Research Agreement \"Language Similarity in Machine Translation\".", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "FINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN", "journal": "", "year": "", "authors": "Antonios Anastasopoulos; Luisa Bentivogli; Z Marcely; Ond\u0159ej Boito; Roldano Bojar; Anna Cattoni; Georgiana Currey; Kevin Dinu; Maha Duh; Marcello Elbayad; Christian Federico; Hongyu Federmann; Roman Gong; Barry Grundkiewicz; Benjamin Haddow; D\u00e1vid Hsu; V\u011bra Javorsk\u00fd;  Kloudov\u00e1; M Surafel; Xutai Lakew; Prashant Ma; Paul Mathur; Kenton Mcnamee; Maria Murray; Satoshi N\u0203dejde; Matteo Nakamura; Jan Negri; Xing Niehues; Juan Niu; Elizabeth Pino; Jiatong Salesky; Sebastian Shi; Katsuhito St\u00fcker; Marco Sudoh;  Turchi"}, {"title": "2021. FINDINGS OF THE IWSLT 2021 EVAL-UATION CAMPAIGN", "journal": "Association for Computational Linguistics", "year": "", "authors": "Antonios Anastasopoulos; Ond\u0159ej Bojar; Jacob Bremerman; Roldano Cattoni; Maha Elbayad; Marcello Federico; Xutai Ma; Satoshi Nakamura; Matteo Negri; Jan Niehues; Juan Pino; Elizabeth Salesky; Sebastian St\u00fcker; Katsuhito Sudoh; Marco Turchi; Alexander Waibel; Changhan Wang; Matthew Wiesner"}, {"title": "2020. FINDINGS OF THE IWSLT 2020 EVAL-UATION CAMPAIGN", "journal": "", "year": "", "authors": "Ebrahim Ansari; Amittai Axelrod; Nguyen Bach; Ond\u0159ej Bojar; Roldano Cattoni; Fahim Dalvi; Nadir Durrani; Marcello Federico; Christian Federmann; Jiatao Gu; Fei Huang; Kevin Knight; Xutai Ma; Ajay Nagesh; Matteo Negri; Jan Niehues; Juan Pino; Elizabeth Salesky; Xing Shi; Sebastian St\u00fcker; Marco Turchi; Alexander Waibel; Changhan Wang"}, {"title": "Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations", "journal": "Advances in Neural Information Processing Systems", "year": "", "authors": "Alexei Baevski; Yuhao Zhou"}, {"title": "Without further ado: Direct and simultaneous speech translation by AppTek in 2021", "journal": "", "year": "2021", "authors": "Parnia Bahar; Patrick Wilken; A Mattia; Evgeny Di Gangi;  Matusov"}, {"title": "Mustc: A multilingual corpus for end-to-end speech translation", "journal": "Computer Speech & Language", "year": "2021", "authors": "Roldano Cattoni; Mattia Antonino Di Gangi; Luisa Bentivogli; Matteo Negri; Marco Turchi"}, {"title": "Thinking slow about latency evaluation for simultaneous machine translation", "journal": "", "year": "2019", "authors": "Colin Cherry; George Foster"}, {"title": "Can neural machine translation do simultaneous translation?", "journal": "", "year": "2016", "authors": "Kyunghyun Cho; Masha Esipova"}, {"title": "Transformer-XL: Attentive language models beyond a fixed-length context", "journal": "", "year": "2019", "authors": "Zihang Dai; Zhilin Yang; Yiming Yang; Jaime Carbonell; Quoc Le; Ruslan Salakhutdinov"}, {"title": "Annual Meeting of the Association for Computational Linguistics (ACL)", "journal": "", "year": "", "authors": ""}, {"title": "ON-TRAC consortium for end-to-end and simultaneous speech translation challenge tasks at IWSLT 2020", "journal": "", "year": "2020", "authors": "Maha Elbayad; Ha Nguyen; Fethi Bougares; Natalia Tomashenko; Antoine Caubri\u00e8re; Benjamin Lecouteux; Yannick Est\u00e8ve; Laurent Besacier"}, {"title": "Sequence transduction with recurrent neural networks", "journal": "", "year": "2012", "authors": "Alex Graves"}, {"title": "Toward multilingual neural machine translation with universal encoder and decoder", "journal": "", "year": "2016", "authors": "Thanh-Le Ha; Jan Niehues; Alexander Waibel"}, {"title": "End-to-end simultaneous translation system for IWSLT2020 using modality agnostic meta-learning", "journal": "", "year": "2020", "authors": "Mohd Abbas Hou Jeung Han; Sathish Zaidi; Nikhil Reddy Indurthi; Beomseok Kumar Lakumarapu; Sangha Lee;  Kim"}, {"title": "End-end speech-to-text translation with modality agnostic meta-learning", "journal": "", "year": "2020", "authors": "Sathish Indurthi; Houjeung Han; Nikhil Kumar Lakumarapu; Beomseok Lee; Insoo Chung; Sangha Kim; Chanwoo Kim"}, {"title": "Stream decoding for simultaneous spoken language translation", "journal": "", "year": "2008", "authors": "Muntsin Kolss; Stephan Vogel; Alex Waibel"}, {"title": "Timing in turn-taking and its implications for processing models of language", "journal": "Frontiers in psychology", "year": "2015", "authors": "C Stephen; Francisco Levinson;  Torreira"}, {"title": "Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension", "journal": "", "year": "2019", "authors": "Mike Lewis; Yinhan Liu; Naman Goyal; Marjan Ghazvininejad; Abdelrahman Mohamed; Omer Levy; Ves Stoyanov; Luke Zettlemoyer"}, {"title": "2021. The USTC-NELSLIP systems for simultaneous speech translation task at IWSLT 2021", "journal": "Association for Computational Linguistics", "year": "", "authors": "Dan Liu; Mengge Du; Xiaoxi Li; Yuchen Hu; Lirong Dai"}, {"title": "Low-latency end-to-end speech recognition with enhanced readability", "journal": "", "year": "2020", "authors": "Danni Liu"}, {"title": "Low-Latency Sequence-to-Sequence Speech Recognition and Translation by Partial Hypothesis Selection", "journal": "", "year": "2020", "authors": "Danni Liu; Gerasimos Spanakis"}, {"title": "Multilingual denoising pre-training for neural machine translation", "journal": "", "year": "2020", "authors": "Yinhan Liu; Jiatao Gu; Naman Goyal; Xian Li; Sergey Edunov; Marjan Ghazvininejad; Mike Lewis; Luke Zettlemoyer"}, {"title": "Stacl: Simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework", "journal": "", "year": "2019", "authors": "Mingbo Ma; Liang Huang; Hao Xiong; Renjie Zheng; Kaibo Liu; Baigong Zheng; Chuanqiang Zhang; Zhongjun He; Hairong Liu; Xing Li"}, {"title": "Simuleval: An evaluation toolkit for simultaneous translation", "journal": "", "year": "2020", "authors": "Xutai Ma; Mohammad Javad Dousti; Changhan Wang; Jiatao Gu; Juan Pino"}, {"title": "Super-Human Performance in Online Low-Latency Recognition of Conversational Speech", "journal": "", "year": "2021", "authors": "Thai-Son Nguyen; Sebastian St\u00fcker; Alex Waibel"}, {"title": "Dynamic Transcription for Low-Latency Speech Translation", "journal": "", "year": "2016", "authors": "Jan Niehues; Eunah Thai Son Nguyen; Thanh-Le Cho; Kevin Ha; Markus Kilgour; Matthias M\u00fcller; Sebastian Sperber; Alex St\u00fcker;  Waibel"}, {"title": "Bleu: a method for automatic evaluation of machine translation", "journal": "", "year": "2002", "authors": "Kishore Papineni; Salim Roukos; Todd Ward; Wei-Jing Zhu"}, {"title": "Relative Positional Encoding for Speech Recognition and Direct Translation", "journal": "", "year": "2020-01", "authors": "Ngoc-Quan Pham; Thanh-Le Ha; Tuan-Nam Nguyen; Thai-Son Nguyen; Elizabeth Salesky; Sebastian St\u00fcker"}, {"title": "Effective combination of pretrained models -KIT@IWSLT2022", "journal": "Association for Computational Linguistics", "year": "2022-01", "authors": "Ngoc-Quan Pham; Tuan-Nam Nguyen; Thai-Binh Nguyen; Danni Liu; Carlos Mullov"}, {"title": "Kit's iwslt 2020 slt translation system", "journal": "", "year": "2020", "authors": "Ngoc-Quan Pham; Felix Schneider; Tuan Nam Nguyen; Thanh-Le Ha; Thai-Son Nguyen; Maximilian Awiszus; Sebastian St\u00fcker; Alex Waibel"}, {"title": "A call for clarity in reporting bleu scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"title": "Multilingual translation with extensible multilingual pretraining and finetuning", "journal": "", "year": "2020", "authors": "Yuqing Tang; Chau Tran; Xian Li; Peng-Jen Chen; Naman Goyal; Vishrav Chaudhary; Jiatao Gu; Angela Fan"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Transformers: State-of-the-art natural language processing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; R\u00e9mi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander M Lhoest;  Rush"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Quality-latency trade-off of different chunk sizes combined with different stable hypothesis detection strategies. The number next to the marks indicates chunk size in milliseconds.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Quality-latency trade-off of hold-n strategy with different values of n. The number next to the marks indicates n. Colored lines connect results with equal chunk size.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Quality-latency trade-off of shared prefix (SP-n) and local agreement (LA-n) with different n and chunk size.", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure4: Quality-latency trade-off on English-German tst-COMMON of our two models: a dedicated English-German model trained from scratch (Model A) and a multilingual model based on wav2vec and mBART (Model B). We also include the best IWSLT 2021 system (USTC-NELSLIP(Liu et al., 2021)).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Quality-latency trade-off of the LA-2 strategy with and without the initial wait.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Results of the older model used for the experiments (Model", "figure_data": ""}], "doi": "10.18653/v1/2021.iwslt-1.1"}