{"authors": "Ruobing Li; Chuan Wang; Yefei Zha; Yonghong Yu; Shiman Guo; Qiang Wang; Yang Liu; Hui Lin", "pub_date": "", "title": "The LAIX Systems in the BEA-2019 GEC Shared Task", "abstract": "In this paper, we describe two systems we developed for the three tracks we have participated in the BEA-2019 GEC Shared Task. We investigate competitive classification models with bi-directional recurrent neural networks (Bi-RNN) and neural machine translation (NMT) models. For different tracks, we use ensemble systems to selectively combine the NMT models, the classification models, and some rules, and demonstrate that an ensemble solution can effectively improve GEC performance over single systems. Our GEC systems ranked the first in the Unrestricted Track, and the third in both the Restricted Track and the Low Resource Track.", "sections": [{"heading": "Introduction", "text": "Grammatical error correction (GEC) is the task of automatically correcting grammatical errors in text. With the increasing number of language learners, GEC has gained more and more attention from educationists and researchers in the past decade. The following is a GEC example: I [fall \u2192 fell] asleep at 11 p.m. last [nigh \u2192 night].\nHere fall needs to be corrected to its past tense form and nigh is a spelling mistake.\nGEC is considered as a mapping task from incorrect sentences to correct sentences. Incorrect sentences can be seen as being produced by adding noises to correct sentences. The added noise does not happen randomly, but occurs when people learn or use the language according to a certain error distribution and language usage bias. Initially, people used rule-based approaches to solve GEC problems (Naber and Mi\u0142kowski, 2005). Rules are relatively easy to make but with poor generalization. Later researchers began to treat GEC as a classification task. According to the grammatical information around the target word, classifiers can be constructed to predict the true grammatical role of the target word. One drawback of the classification methods for GEC is that training different classifiers for different error types may be resource-intensive and inefficient since there are many grammatical error types. Recently, translation methods have become the focus of research, and there is a clear trend that state-of-the-art GEC systems are being shifted from traditional NLP methods to NMT based methods.\nIn recent years, GEC performance has seen significant improvement in some public GEC test sets (Ge et al., 2018). In CoNLL-2013(Ng et al., 2013 and CoNLL-2014(Ng et al., 2014) GEC Shared Task, machine learning based GEC methods emerged with relatively good performance. Classification methods achieved the best result in CoNLL-2013 (Rozovskaya et al., 2013). After that, statistical machine translation (SMT) methods began to show better performance in CoNLL-2014 (Felice et al., 2014). (Chollampatt et al., 2016) was the first study to obtain the state-ofthe-art result with neural networks. Then after (Junczys-Dowmunt and Grundkiewicz, 2016), machine translation methods became the mainstream in GEC solutions. In addition, an RNNbased context model achieved better results than previous traditional classification models (Wang et al., 2017). Using a CNN-based sequenceto-sequence architecture (Gehring et al., 2017), (Chollampatt and Ng, 2018) proposed the first end-to-end NMT model and reported the state-ofthe-art result. As Transformer (Vaswani et al., 2017) plays an increasingly important role in sequence modeling, Transformer-based end-to-end NMT models began to lead the current GEC research Ge et al., 2018;Zhao et al., 2019). It is worth mentioning that (Lichtarge et al., 2019) used Wikipedia ed-its history corpus, which is huge but noisy, and gained a result very close to the state-of-the-art result. Learning a GEC translation model from noisy data is a worthy future direction as the GEC parallel corpus is expensive to obtain.\nThis paper describes our two systems for the three tracks in the BEA-2019 GEC Shared Task (Bryant et al., 2019). We use two popular NMT models and two improved versions of neural classification models to train the basic models. Ensemble strategies are then used to combine outcomes from different models. Our two systems for the three tracks are described in next section. In Section 3, we evaluate the systems on the development data and show the final results on the test data. Section 4 concludes the paper and summarizes the future work.", "n_publication_ref": 19, "n_figure_ref": 0}, {"heading": "System Overview", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Restricted and Unrestricted Track", "text": "We submitted the same system output for the Restricted and Unrestricted tasks. The system uses several ensemble methods to combine the CNNbased and Transformer-based translation models, described in details below.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "CNN-based translation ensemble systems", "text": "We found that CNN-based systems obtained the best results for some error types, likely due to some characteristics derived from CNN. We trained four CNN-based ensemble systems, using the model architecture in (Chollampatt and Ng, 2018), but without reranking. Four best combinations to build the ensemble systems were selected. Unlike (Chollampatt and Ng, 2018), we did not use fastText (Bojanowski et al., 2017) to initialize word embeddings because we found no improvement on the development set by doing that. We tuned parameters for the system, such as batch size, word embedding dimension, etc.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Transformer-based translation systems", "text": "Transformer is currently considered to be one of the most powerful models for sequence modeling. For GEC, some of the best recent results reported on CoNLL-2014 test set are obtained by Transformer-based translation models. We trained eight Transformer-based translation models in a low resource translation paradigm . We tuned parameters for domain and error adaptation. We also compared the results using 2 GPUs and 4 GPUs as the authors reported the difference in their Github repository 1 .", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ensemble methods", "text": "We expect to combine these models trained above into a more powerful system through effective ensemble methods. Our ensemble work mainly focuses on rule-based solutions. We will introduce two main modules first.\nConfidence Table We can obtain the precision and F 0.5 metric on each error type through sentence alignment and error type classification by Errant (Bryant et al., 2017). Errant provides performance statistics based on 55 error types and is also the tool used to evaluate this GEC shared task, thus we use the result of operation and error type span-level (Bryant et al., 2017) for a model or system as the confidence table.\nConflict Solver We often encounter GEC error conflicts when combining multiple models or systems. For example, We love played soccer. One system corrects played to playing, while another system may correct played to to play. When two different corrections occur in the same place, we need to consider which one to choose.\nWe solve this problem in a unified pipeline, which can also be seen as an ensemble way:\n(1) We sort each group of conflicting corrections proposed by all the systems in a reverse order of location index and confidence.\n(2) We apply three sub-strategies:\n\u2022 When combining outcomes from different systems, we treat the precision in a confidence table as the confidence. Each correction has its confidence obtained by looking up the precision of the corresponding type of the correction in the table. If two conflicting corrections are the same, we merge them and add \u03b1 to the confidence of the correction; otherwise, the correction with a lower confidence will be discarded.\n\u2022 After combining outcomes, if the confidence of a correction is lower than \u03b2, the correction is discarded.\n\u2022 \u03b3 is used to distinguish when it is more important to focus on the precision or F 0.5 of a correction. When we move to the final ensemble with confidence tables of existing systems, if the confidence is larger than \u03b3, we select the correction proposed by the system that has the best F 0.5 on the type of this correction. Otherwise, the correction by a system with the best precision is selected.\nIn Figure 1, \u2297 means the outcome is obtained by combining two systems represented by intersecting lines of two different colours. If there are multiple \u2297 on a line, it means the ensemble is over all of these \u2297 on this line. Figure 1 displays three types of ensemble methods based on all of the CNN-based and Transformer-based translation models.\n\u2022 Combine each CNN-based ensemble model with each of the selected five of the Transformer-based models. This is noted as 'ensemble-by-2'.\n\u2022 Perform ensemble over all of the ensemble models relating to either CNN ensemble 1 or CNN ensemble 2, noted as EoE (Ensemble over Ensemble) 1 and 2.\n\u2022 Ensemble each CNN ensemble model with some selected combinations of Transformer-based models to produce 16 strong ensemble system outcomes, represented as 'Hybrid Ensemble' in Figure 1. It is where multiple lines of the same color are merged into one line in Figure 1.\nAfter getting all of the ensemble outcomes, we will do the final ensemble step: select the best confidence for each type from each single or ensemble system to form the strongest final outcome. In this ensemble step, we use the last aforementioned sub-strategy, and discard the error types with very low confidence to boost the final performance.", "n_publication_ref": 2, "n_figure_ref": 4}, {"heading": "Low Resource Track", "text": "For the Low Resource Track we developed different individual systems and used an ensemble method to combine them. For the translation model, we did not obtain very strong performance because the training data is limited. We also explored the noisy Wikipedia edit history corpus for the Transformer-based translation model. However, we noticed that, for some error types with clear definitions, the classifiers trained on a large amount of native corpus have good performance. In addition, we made some grammatical rules to correct errors and adopted an off-the-shelf spelling checker (Kelly, 2006). Finally, we leverage a sim-ple ensemble method to combine all of the classifiers, rules, spelling checker and translation models. Note that for the Restricted and Unrestricted tracks, we did not observe any gain from the classification models or the rule-based methods, therefore only the translation systems were used for those tracks.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Classification model", "text": "After an analysis of the development sets, we decided to build classifiers for eight common error types. Based on (Wang et al., 2017), we developed two classification model structures for the eight error types.\n(A) Bi-GRU context model Figure 2 shows the bi-directional GRU context model we use to determine the right grammatical category for a target word. The concatenated left and right source states of the target word form the contextual semantic vector representation. This is used as a query to calculate the attention weight a t . An attention vector C t is then computed as the weighted average, according to a t , over all the source states. C t is then fed through a fully connected layer and softmax layer to produce the predictive distribution. We use this to train models for the following error types: Subject-verb agreement, Article, Plural or singular noun, Verb form, Preposition substitution, Missing comma and Period comma substitution. Labels for each task were extracted automatically from the native corpus through part-ofspeech tagging tools.\n(B) Pointer context model The classifiers above use the same classification labels for different target words. We also need a classification model to deal with the problem as in the Word form task, where each word has a different set of predictive labels (as shown for word 'gone' in Figure 3). Inspired by the Pointer network model (Vinyals et al., 2015), we proposed the pointer context model. Figure 3 shows the pointer context model that takes the target word's confusion set as the label candidates. The computation path is the same as the Bi-GRU model structure. We concatenate the target word's char-based embedding and C t to obtain C 1 t , and then use it as the query to compute dot product a 1 t with each of the word embeddings in the confusion set. a 1 t is then fed through a softmax layer to produce the predictive distribution. This model is very effective at dealing with varying number of candidates as seen in the Word form task. ", "n_publication_ref": 2, "n_figure_ref": 3}, {"heading": "NMT model", "text": "We use the same Transformer-based translation model mentioned in Subsection 3.2.2.\nDue to the limitation of the corpus, we leverage the Wiked (Grundkiewicz and Junczys-Dowmunt, 2014) as our training corpus for the NMT model.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Rules and spell checker", "text": "We have implemented the following GEC rules.\n(1) 'a' and 'an' substitution. For this problem, we made rules based on the first phoneme of the following word.\n(2) Comma deletion. After a prepositional   phrase at the beginning of a sentence, we add a comma. For example, \"Despite our differences we collaborate well.\" A comma should be added after Despite our differences.\n(3) Orthography mistakes. We obtain statistics of named entities that require initial capitalization and make a white list using the Wikipedia corpus. If a word is on the white list, we will force the conversion to the initial capitalization form.\nIn addition, we use Pyenchant as our spell checker (Kelly, 2006). The top candidate is considered to be the correction.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Ensemble", "text": "We use the conflict solver described above to do the ensemble for all of the outputs of the classifiers, rules, spell checker and NMT model.  (Mizumoto et al., 2012), NUCLE (Ng et al., 2014, W&I+LOCNESS (Bryant et al., 2019) and Common Crawl. We use Common Crawl to pretrain the decoder parameters for the Transformer-based translation model. FCE, Lang-8, NUCLE and W&I are used to train all of the translation models. It is worth noting that we did data augmentation for W&I to train all of the translation models. The data sets used in Low Resource Track include Wiked, Wikipedia Dumps and Common Crawl. All of the classifiers are trained on Wikipedia Dumps and the translation model is trained on Wiked corpus. For Wiked corpus, we did some data cleaning work. We discarded some noisy sentences that include error types such as U:OTHER, R:OTHER, R:NOUN, etc. The development set from W&I+LOCNESS are used in all the tracks. Following the data pre-processing pipeline used to generate the data provided by the shared task, we tokenize all of the data using spaCy 3 .", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Experiments", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Data Sets", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Restricted and Unrestricted Track", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "CNN-based translation ensemble models", "text": "We added the W&I corpus eight times to the training corpus for domain adaptation. Table 2 shows the performance of the single CNN-based translation models. All the parameters in Table 2 are tuned over the W&I+LOCNESS development set.\nTable 3 shows the results of the four CNN-based ensemble systems. We use ensembles in the same way as (Chollampatt and Ng, 2018). The above results prove that the ensemble method has yielded a very large improvement in this task.   ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Transformer-based translation models", "text": "We trained eight Transformer-based translation models in different combinations of error adaptation, domain adaptation, and GPU set.\nIn Table 4, we notice that a smaller error weight yields higher precision and a slight decrease in recall. We set the copy number as 8, 10 and 15, and find that domain adaptation has no significant effect on the results. 4 GPU is obviously better than 2 GPU sets, which is probably because of the larger batch size accumulation for gradient calculation.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ensemble methods", "text": "As described in Section 2.1.3, we need to ensemble all of the CNN-based and Transformer-based translation models. We have already introduced the configuration of the single models in Section 3.2.1 and Section 3.2.2. Next we will describe the configuration of the ensemble system.\nFor the three ensemble types: Ensemble-by-2, EoE and Hybrid Ensemble, as shown in Figure 1, we used different parameters in the conflict solver.\nWe did a small-scale grid search for the parameters in Table 5. When combining two models that are not strong, we expect a higher recall so \u03b2 was not high. For EoE and hybrid ensemble, we expect a higher precision so that they can provide high quality single type performance. Corrections proposed by multiple models are given higher weights (controlled by \u03b1). If the confidence of a correction  finally reaches \u03b2, the correction will be adopted.\nIn the final ensemble, we select the best performance on each type from each single system or ensemble system and discard the corrections with low precision (controlled by \u03b2). To get higher F 0.5 , in the case where the precision is greater than a predefined threshold (controlled by \u03b3), we will choose the model with the highest F 0.5 for the corresponding error type. The final outcome of the data set is then fed through the translation models and ensemble systems again to do a second pass correction.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Results", "text": "Table 6 summarizes some results on the development set and gives the official test result. We can see that the individual CNN or Transformer-based translation models perform reasonably well, and the ensemble methods consistently outperform the individual systems. The second pass correction further improves the performance, and the last post-processing step boosts both recall and F 0.5 .\nStep  ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "NMT model", "text": "A Transformer-based translation model is trained on the filtered Wiked corpus. The model architecture follows that in . Although the performance of the NMT model is not strong, it provides good performance equivalent to the classifiers for some error types.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ensemble", "text": "We use one conflict solver to combine the outputs from all of the systems in this task. Parameters for this ensemble system are shown in Table 7.  sults on the test set. We can see that the base systems are not very strong, and the ensemble system significantly improves the performance. The difference between the development set and test set can still be observed in this task.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Conclusions and Future Work", "text": "We have presented two different systems for the three GEC tracks. When there is a sufficient parallel learner corpus, such as in Restricted Track and Unrestricted Track, the NMT ensemble model is the best choice to implement a GEC system. We have evaluated two kinds of NMT models: CNN-based and Transformer-based translation models. We have also explored different ensemble strategies from multiple base mod-els to maximize the overall system performance. Finally we reached the result of F 0.5 =0.6678 on the official test set in Restricted Track and Unrestricted Track, ranking the third in the Restricted track 4 . It is worth noting that there is a huge gap between the results on the development set and the test set, which suggests that there might be an unneglectable mismatch between the development set and the test set. Indeed, the development set is annotated by one annotator, while the test set is annotated by five, as announced officially.\nFor Low Resource Track, there is a lack of parallel learner corpus, and thus we rely less on the translation models. We have built eight classifiers trained on Wikipedia dumps according to different error types and an NMT model trained on the Wikipedia edits history corpus. By a simple ensemble method, we reached F 0.5 =0.5181, placing our system in the third place in Low Resource Track.\nAlthough GEC has reached the human level performance on some GEC test sets, there is still room for improvement. In a low resource setup, how to deal with the huge but noisy data is worth exploring. (Lichtarge et al., 2019) gave a good solution on this topic, but more work needs to be done. Second, we will investigate methods such as the reinforcement learning based method (Wu et al., 2018) to address the mismatch between the training objectives and evaluation methods in GEC.", "n_publication_ref": 2, "n_figure_ref": 0}], "references": [{"title": "Enriching word vectors with subword information", "journal": "", "year": "2017", "authors": "Piotr Bojanowski; Edouard Grave; Armand Joulin; Tomas Mikolov"}, {"title": "Automatic annotation and evaluation of error types for grammatical error correction", "journal": "", "year": "2017", "authors": "Christopher Bryant; Mariano Felice; Ted Briscoe"}, {"title": "The BEA-2019 Shared Task on Grammatical Error Correction", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Christopher Bryant; Mariano Felice; \u00d8istein E Andersen; Ted Briscoe"}, {"title": "A multilayer convolutional encoder-decoder neural network for grammatical error correction", "journal": "", "year": "2018", "authors": "Shamil Chollampatt; Hwee Tou Ng"}, {"title": "Neural network translation models for grammatical error correction", "journal": "", "year": "2016", "authors": "Shamil Chollampatt; Kaveh Taghipour; Hwee Tou Ng"}, {"title": "Grammatical error correction using hybrid systems and type filtering", "journal": "", "year": "2014", "authors": "Mariano Felice; Zheng Yuan; \u00d8istein E Andersen"}, {"title": "Reaching human-level performance in automatic grammatical error correction: An empirical study. Microsoft Research", "journal": "", "year": "2018", "authors": "Tao Ge; Furu Wei; Ming Zhou"}, {"title": "Convolutional sequence to sequence learning", "journal": "", "year": "2017", "authors": "Jonas Gehring; Michael Auli; David Grangier; Denis Yarats; Yann N Dauphin"}, {"title": "The wiked error corpus: A corpus of corrective wikipedia edits and its application to grammatical error correction", "journal": "Springer", "year": "2014", "authors": "Roman Grundkiewicz; Marcin Junczys-Dowmunt"}, {"title": "Near human-level performance in grammatical error correction with hybrid machine translation", "journal": "", "year": "2018", "authors": "Roman Grundkiewicz; Marcin Junczys-Dowmunt"}, {"title": "Phrase-based machine translation is state-ofthe-art for automatic grammatical error correction", "journal": "", "year": "2016", "authors": "Marcin Junczys; - Dowmunt; Roman Grundkiewicz"}, {"title": "Approaching neural grammatical error correction as a low-resource machine translation task", "journal": "", "year": "2018", "authors": "Marcin Junczys-Dowmunt; Roman Grundkiewicz; Shubha Guha; Kenneth Heafield"}, {"title": "", "journal": "", "year": "2006", "authors": "Ryan Kelly"}, {"title": "Weakly supervised grammatical error correction using iterative decoding", "journal": "", "year": "2019", "authors": "Jared Lichtarge; Christopher Alberti; Shankar Kumar; Noam Shazeer; Niki Parmar"}, {"title": "The effect of learner corpus size in grammatical error correction of esl writings", "journal": "", "year": "2012", "authors": "Tomoya Mizumoto; Yuta Hayashibe; Mamoru Komachi; Masaaki Nagata; Yu Matsumoto"}, {"title": "", "journal": "", "year": "2005", "authors": "Daniel Naber; Marcin Mi\u0142kowski"}, {"title": "The conll-2014 shared task on grammatical error correction", "journal": "", "year": "2014", "authors": " Hwee Tou Ng; Mei Siew; Ted Wu; Christian Briscoe; Raymond Hendy Hadiwinoto; Christopher Susanto;  Bryant"}, {"title": "The conll-2013 shared task on grammatical error correction. Seventeenth Conference on Computational Natural Language Learning", "journal": "", "year": "2013", "authors": " Hwee Tou Ng; Mei Siew; Yuanbin Wu; Christian Wu; Joel Hadiwinoto;  Tetreault"}, {"title": "The university of illinois system in the conll-2013 shared task", "journal": "", "year": "2013", "authors": "Alla Rozovskaya; Kai-Wei Chang; Mark Sammons; Dan Roth"}, {"title": "Attention is all you need. 31st Conference on Neural Information Processing Systems", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"title": "Pointer networks. Proceedings of the 28th International Conference on Neural Information Processing Systems", "journal": "", "year": "2015", "authors": "Oriol Vinyals; Meire Fortunato; Navdeep Jaitly"}, {"title": "Deep context model for grammatical error correction", "journal": "", "year": "2017", "authors": "Chuan Wang; Ruobing Li; Hui Lin"}, {"title": "A study of reinforcement learning for neural machine translation", "journal": "", "year": "2018", "authors": "Lijun Wu; Fei Tian; Tao Qin; Jianhuang Lai; Tie-Yan Liu"}, {"title": "A new dataset and method for automatically grading esol texts", "journal": "", "year": "2011", "authors": "Helen Yannakoudakis; Ted Briscoe; Ben Medlock"}, {"title": "Improving grammatical error correction via pre-training a copy-augmented architecture with unlabeled data", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Wei Zhao; Liang Wang; Kewei Shen; Ruoyu Jia; Jingming Liu"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: The architecture of the ensemble system in Restricted and Unrestricted Tracks.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Bi-GRU Context model structure.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Pointer Context model structure.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_3", "figure_caption": "2https://lang-8.com", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Corpus used for training in corresponding track.", "figure_data": "Seed Batch sizeWord embedding dimensionNumber of input channelsNumber of output channelsNumber of layersF 0.550013212825625670.337050023212825625670.321950033212825625670.337050043212825625670.341150053212825625670.3449501232256512512100.333951023212851251270.332970111625651251270.3328720532256512512140.3328"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "lists the data sets used in Restricted Track and Unrestricted Track, including FCE (Yannakoudakis et al., 2011), Lang-8 2", "figure_data": ""}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Results of CNN-based ensemble systems on the development set.", "figure_data": "Model index Error weightCopy number of W&I trainsetGPU numberPrecision RecallF 0.5131020.45850.3525 0.4325231040.46020.3514 0.433333820.45920.3575 0.434543840.46410.3548 0.4372531520.44940.3479 0.4247631540.46480.3467 0.4352721020.47150.3303 0.4343821040.48680.3412 0.4485"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Results of Transformer-based translation models on the development set.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Parameters in the conflict solver for the ensemble methods in Restricted and Unrestricted Track.", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "", "figure_data": "also shows that there is a big gap betweenthe performance on the development set and testset, partly because the final test set uses a combi-nation of five annotators.3.3 Low Resource Track3.3.1 Classification modelsWe trained classifiers for seven error types:Subject-verb agreement, Article, Plural or singularnoun, Verb form, Preposition substitution, Miss-ing comma and Period comma substitution andWord form. As mentioned in Subsection 2.2.1,Word form model is trained using the Pointer Con-text model. The other error types are trained usingBi-GRU Context model."}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "", "figure_data": "shows results for different systems (forclassification models, different error type classi-fiers) on the development set, and the overall re-"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Results of Low Resource Track.", "figure_data": ""}], "doi": ""}