{"authors": "Casey Kennington", "pub_date": "", "title": "Enriching Language Models with Visually-grounded Word Vectors and the Lancaster Sensorimotor Norms", "abstract": "Language models are trained only on text despite the fact that humans learn their first language in a highly interactive and multimodal environment where the first set of learned words are largely concrete, denoting physical entities and embodied states. To enrich language models with some of this missing experience, we leverage two sources of information: (1) the Lancaster Sensorimotor norms, which provide ratings (means and standard deviations) for over 40,000 English words along several dimensions of embodiment, and which capture the extent to which something is experienced across 11 different sensory modalities, and (2) vectors from coefficients of binary classifiers trained on images for the BERT vocabulary. We pre-trained the ELECTRA model and fine-tuned the RoBERTa model with these two sources of information then evaluate using the established GLUE benchmark and the Visual Dialog benchmark. We find that enriching language models with the Lancaster norms and image vectors improves results in both tasks, with some implications for robust language models that capture holistic linguistic meaning in a language learning context.", "sections": [{"heading": "Introduction", "text": "Children learn their first spoken language in a highly interactive setting where generally the first words children learn are concrete words that denote physical objects, which is an important developmental step in child first language acquisition (Kuperman et al., 2012a;McCune, 2008;Clark, 2013). This is partly because handling the Symbol Grounding Problem-the ablity to connect symbolic knowledge of language with representations of the physical world (Harnad, 1990)-must take place before children learn more abstract concepts later in their cognitive development (Borghi et al., 2019;Ponari et al., 2018). Importantly, the physical world is not just the visual world; children learn that words ground into proprioperceptive states (e.g., a hand grasp around an object has specific muscle activations tied to the word grab), interoceptive states (i.e., affect and valence), as well as all other sensory modalities (e.g., the word stinky grounds into olfactory, the word loud grounds into auditory). These claims are evidenced in a large body of child development and cognitive science literature. Smith and Gasser (2005), for example, identified that babies' experience of the world is profoundly multimodal: babies live in a physical world full of rich regularities that organize perception, action and thought; babies learn in a social world to learn a shared linguistic communicative system that is symbolic. Furthermore, a growing body of literature from linguistics and computational linguistics makes a strong case that the process of language learning (indeed, general human cognition) is embodied, interactive, and enacted; i.e., movement in the world is required (Pulverm\u00fcller, 1999;Lakoff and Johnson, 1999;Barsalou, 2008;Johnson, 2008;Smith and Samuelson, 2009;Di Paolo et al., 2018;Bisk et al., 2020); see also the prior work in developmental robotics research; e.g., Cangelosi and Schlesinger (2015), Chapter 7. 1 Taken together, it is clear that aspects of the physical world are necessary for holistic knowledge of semantic meaning, which has implications for how language is modeled computationally. In particular, what does this mean for language models that are trained purely on text (likely largely written by adults), such as BERT (Devlin et al., 2018) or GPT-3? These models have clearly led to important advances for natural language processing tasks and applications, but it is also clear that language models trained only on text are missing critical semantic information (Bender and Koller, 2020).\nIn this paper, we contribute to a growing body of recent work that attempts to addresses these limitations by (1) leveraging multimodal and sensorimotor knowledge of the Lancaster Sensorimotor Norms (Lynott et al., 2019) and (2) using vectorized representations of images by treating both (1) and (2) as embeddings of language models for GLUE and Visual Dialog benchmarks. In the following section, we explain related work-a growing body of literature that is adding multimodal information to language models, then we explain our two embeddings that we will use. We explore how these embeddings can be used to enrich the ELEC-TRA language model's pre-training and fine-tuning, and evaluate on the GLUE benchmark (Experiment 1, Section 4), and how they can be used to replace input embeddings for a pre-trained RoBERTa model for the Visual Dialog task (Experiment 2, Section 5). Our experiments shed light on how useful multimodal information can be in a task that is text-only (Experiment 1) and a task that is multimodal (Experiment 2). Our results show that our parsimonious method to unifying vision (and sensorimotor knowledge) in existing language models shows improvements in multimodal benchmarks with accessible hardware (i.e., a single GPU) as a step towards models that can be trained in settings similar to that of child language learners.", "n_publication_ref": 19, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Language models are trained on text. G\u00fcnther et al. (2018) took up the question do words inherit sensorimotor activation from purely linguistic context? and showed that experience is necessary for reactivating experiential traces, but this reactivation is not a necessary condition for understanding the corresponding aspects of word meaning. We take this to mean that humans are very adept at learning new concepts from language exposure alone (i.e., abstract concepts); e.g., someone who has never seen a zebra before, but hears them described as \"horses with vertical black and white stripes\" can compose a connotation of what zebra denotes without direct visual exposure. However, this only works if an agent that has learned the language has the knowledge of horses, black, white, stripes, and vertical concepts-i.e., via direct experience, not just through linguistic exposure or encyclopedic definitions. These claims are further backed up by neuroscience research that showed that neural assemblies encode concrete content words (i.e., words that denote visual objects) and verbs (i.e., words that denote actions) are learned and represented in different brain regions (Pulverm\u00fcller, 1999;Borghesani et al., 2019). Rogers et al. (2020) provides a recent primer and overview of research that has attempted to uncover strengths and weaknesses of BERT and related language models (so-called BERTology). While our work does fit into that growing body of literature, our criticisms on current language models specifically lies in the fact that they are only trained on easy-to-obtain text. This criticism is born out in Forbes et al. (2019) which showed that BERT can guess affordances and properties of objects because that information can be found in text (e.g., a typical chair has the affordance of being sittable, and a property of having legs), but has no notion of how objects are related semantically to each other, and Da and Kasai (2019) further showed that real-world perceptual properties are likely to be assumed instead of inferred. Furthermore, Bender and Koller (2020) make a strong case that BERT learns form instead of meaning, and while the fact that BERT performs so well on many tasks is difficult to dispute, models trained on text are missing semantic information crucial for holistic language understanding.\nSince before BERT which has proven powerful in many language processing tasks, efforts have been made to encode multimodal (i.e., more than just text as a learning modality) information into embeddings and language models (Takano and Utsumi, 2016;Kiros et al., 2014;Zellers et al., 2021) and recent, continued efforts towards bridging grounded visual representations to distributional representations of word meanings give credence to the claim that text-only models like BERT are missing crucial semantic information because enriching BERT with visual information improves performance in several known tasks (Kim et al., 2019;Lu et al., 2019;Li et al., 2019). These models usually treat language and vision as separate pipelines; our method directly endows the language model with visual and sensorimotor knowledge.", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "Data", "text": "In this section, we motivate and introduce of multimodal information we will use in our experiments.\nThe Lancaster Sensorimotor Norms The Lancaster Sensorimotor norms (Lynott et al., 2019) provide ratings (means and standard deviations) for 40,000 English words along dimensions of embodiment which capture the extent to which a concept is experienced across 11 different sensory modalities, and measures derived from those categories, listed below (each has an example word that rates highly for that modalitiy):\n\u2022 Auditory -sound; ping \u2022 Gustatory -having to do with eating; cream \u2022 Haptic -muscle movement; handshake \u2022 Interoceptive -having to do with affect or emotion; headache \u2022 Olfactory -smell; incense \u2022 Visual -visual; barcode \u2022 Foot-leg -haptics for foot/leg; run \u2022 Hand-arm -haptics for hand/arm; pointing \u2022 Head -having to do with the head; eye \u2022 Mouth -haptics for mouth; kiss \u2022 Torso -haptics for torso; breath \u2022 Max-strength.perceptual -the highest rating across the 11 sensorimotor dimensions \u2022 Minkowski3.perceptual -treating the 11 modalities as a vector, this represents the distance of the vector from the origin with influence of weaker dimensions attenuated \u2022 Exclusivity.perceptual -the extent to which a concept (out of the 11) which is experienced through a single perceptual modalitiy\nThe last three can be seen as aggregates from the 11 modalities; they also have .action values representing the extent to which a concept is experienced as an action (as opposed to .perceptual), and .sensorimotor values representing the extent a concept is experience as sensorimotor. As these norms were derived from surveys given to adults, these norms represent the degree to which the survey participants assigned those words to those categories. Though this does not represent a neurophysiological grounding of words to those modalities learned through interaction and embodiment, this serves as a useful approximation. The final set is a vocabulary of 39,707 words (after removing rows which had null values), each represented as a vector of length 39 (i.e., 11 mean, 11 stdev columns; Max-strength, Minkowski, and Exclusivity columns for different ways of aggregating the modalities). We normalize each value in the vector independently to a value between 0-1 by dividing each value over its max value. We call this the Lancaster vectors.\nWe performed t-SNE on the Lancaster vectors (mapping to 2 dimensions) to determine if clus-ters would reveal any intuitions about the kinds of semantic relatedness that the words might have with each other. Some clusters emerged such as foods (presumably because they have similar gustatory ratings), leg-movement verbs (e.g., walk, jump, sit), colors with eye-related words (e.g., purple, green, blue, dark, see, eyes), soft things (e.g., hug, tummy, pillow, clothes), audio-related words (e.g., talk, story, sound, music, lie, say), among others. ", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Words-as-Classifiers Image Vectors", "text": "The Words-as-Classifiers (WAC) approach to grounded semantics is quite simple: train a binary classifier for each word in a corpus where the features to that classifier are derived from images (Kennington and Schlangen, 2015). Each classifier is given positive and negative examples of visual denotations of each word by the images and learns a \"fitness\" score by the classifier. For example, the red classifier is given images of objects that are referred to as red in a corpus, and randomly assigned negative examples of things that are not referred to as red, as depicted in Figure 1. We follow Kiros et al. (2018) and use Google Image Search to find images using the BERT vocabulary, resulting in 27,152 words and corresponding images (some words did not result in images, and we did not download images for filler words). For each word, we perform an image search and download the top 100 images. We then follow Schlangen et al. (2016) and process each image by passing them through the recent CLIP (Jia et al., 2021) convolutional neural network (trained on ImageNet, using CLIP's ViT-B/32 model), yielding a vector of size 512 for each image. We use the 100 images as positive examples for each term in our vocabulary and randomly select three negative examples for each positive example. We then use a logistic regression classifier (C=0.25, max iterations=1000), one for each word, trained on the images for each word. After training, we then follow Moro et al. (2019) and extract the coefficients to arrive at a vector of size 513 (all coefficients plus the bias term) which we use in our evaluations below. We call these the WAC vectors.\nThe WAC model is useful because, as explained in Kennington and Schlangen (2015), the classifiers can actually identify objects (something that language models cannot do on their own), the coefficients represent a computed word intension, new words in a vocabulary can easily be added without retraining all other classifiers including adjectives like red which are often missing from pre-trained object classifiers, and the classifiers are effectively learned with only a few examples, making it effective for fast learning of concrete, grounded concepts. However, the WAC model suffers from two assumptions: first, that all words have concrete, visual denotations even though many abstract words like utopia clearly do not, and that all words are independent of each other in terms of linguistic context. We hypothesize in both experiments below that these coefficients used as vectorized embeddings will be useful to a text-only language model because they add necessary visual information; the language model complements WAC by using linguistic context (i.e., text) for training, overcoming WAC's assumptions.\n4 Experiment 1: Tying embedding weights and pre-training ELECTRA, fine-tuning on GLUE\nIn this experiment, and crucially for our ongoing work that aligns with child-inspired language acquisition, we use ELECTRA (Clark et al., 2020) as a language model because it has been shown to be trainable with smaller amounts of data than other language models, yet yield respectable results and can be trained using a single GPU.\nTask & Procedure Wang et al. (2018) introduced the GLUE benchmark which consists of nine English sentence understanding tasks covering several domains (e.g., movie reviews and online question answering). We opt for this benchmark because of its coverage over several domains and to show that adding multimodal knowledge improves tasks that are based on text. 2 Our aim is to achieve improved results over the text-only baseline with a specified number of training steps using the openwebtext data for training. 3 We report results on the development set, as done in Wu et al. (2021). We only report the results for the MRPC (a paraphrase task that uses accuracy and f1 metrics), COLA (a grammatical acceptibility task; uses Matthew's Correlation), and WNLI (ambiguity resolution; uses an accuracy metric) tasks because they are sufficient to illustrate the utility of our method when applied to ELECTRA. To give ELECTRA knowledge about additional modalities from the Lancaster and WAC vectors, we tie the vectors to the the weights of the generator and discriminator of ELECTRA depicted in Figure 2, and vary whether the embeddings are frozen or not during pre-training, then train for 100,000 steps. 4 We then fine-tune the resulting ELECTRA model on the GLUE tasks using the multimodal vectors following standard fine-tuning protocols; that is, we add a linear layer with a softmax to the pre-trained model and use the ADAM solver with a learning rate of 2e-5 for 3 epochs. As the WAC vectors were larger than ELECTRA's expected embedding size of 128, we applied UMAP to reduce the dimensionality to 128; similarly for the WAC and Lancaster concatenated embeddings. For cases where there was no vector for a word (e.g., the [unmapped] words or words outside of the vocabulary of the Lancaster vectors), we simply used zero vectors. For Lancaster vectors, we set the ELECTRA embedding size to 39. We explored freezing the embeddings; our hypothesis is that not freezing the embeddings will lead to better results because the training regime can overpower the embeddings, but retain the multimodal knowledge. For a broader comparison, we also compared to GloVE (Pennington et al., 2014) and several ablations where we concatenate multimodal vectors with the GloVe vectors (we used the evaluation script for GloVE provided by Wang et al. (2018)). We also use the same training and evaluation regime for the WAC and Lancaster vectors, and a concatenation of the two, on their own treating them as word-level embeddings similar to GloVe.\nResults Table 1 shows the results on the GLUE benchmark. The word-level embeddings of GloVe, WAC, and Lancaster are shown in the top 5 rows of the table. As expected, these word-level embeddings are not state-of-the-art, but we notice that both Lancaster and WAC vectors perform comparably against the GloVE vectors despite only being trained on images (WAC) or derived from the Lancaster norms. Of note is a significant advantage of using the Lancaster vectors alone compared to using any other embedding or combination for the WNLI task which is co-reference and natural language inference for fiction books. This suggests that inference on fiction is helped by knowing which modalities affect each word. Interestingly, the best performing model for COLA was GloVE and Lancaster word-level embeddings; COLA is  a grammaticality test, which is important in language understanding, but arguably less critical in early-stage child language acquisition.\nAll other rows show the ELECTRA baseline and ELECTRA that uses some variation of WAC, Lancaster, or both as embeddings (denoted with the EL-prefix). The bottom part of the table compares ELECTRA with a variant of ELECTRA that uses WAC embeddings (both with and without freezing the embedding weights), ELECTRA with lancaster embeddings and ELECTRA with WAC embeddings concatenated with the Lancaster embeddings (where the length of the WAC embeddings plus the size of ELECTRA is 128). Contrary to our hypothesis, we observe that when ELECTRA uses WAC with frozen weights, the performance on the benchmark performs better than all others, including the ELECTRA baseline. This could suggest that ELECTRA can make effective use of the visual and Lancaster embeddings by adjusting weights in the other layers of the model. The EL-lan-wac variant performed well above the ELECTRA baseline, substantiating the hypothesis that enriching the model with multimodal knowledge can improve results. Taken together, we find the results encouraging because the relatively short training regime still yielded respectable results, suggesting that ELECTRA with a visual or other multimodal embedding can be useful with less training as is the case when children learn language.", "n_publication_ref": 14, "n_figure_ref": 2}, {"heading": "Experiment 2: Replacing RoBERTa embeddings, fine-tuninng on Visual Dialog", "text": "The evaluation in Experiment 1 was made up of text-based tasks. In this experiment, we use an evaluation that requires knowledge of the visual world by evaluating the Lancaster and WAC vectors on the Visual Dialog task , termed visdial. Moreover, Experiment 1 used pre-training on a subset of the data for only 100,000 steps. In this experiment, we evaluate using a fully pre-trained RoBERTa model by replacing its embeddings with the WAC and Lancaster vectors.\nTask Following Murahari et al. (2019), given an image, dialogue history consisting of questionanswer pairs, and a follow-up question about the image, the task of visdial is to predict a free-form natural language answer to the question. The visdial dataset introduced in Das et al. ( 2019) also includes evaluation metrics and human-annotated answers to the natural language queries about the image. Five human annotators identified which responses out of 100 candidates could be considered correct. This allows multiple answers to be correct (e.g., yes and yeah are semantically identical).\nMetrics We report the following metrics:\n\u2022 R@1 Rate of times the top-ranked answer is a correct one; i.e., accuracy. \u2022 R@5 Rate of times correct answers are in the top-five ranked answers. \u2022 MRR Mean Reciprocal Rank is the multiplicative inverse of the rank of the first correct answer.\n\u2022 NDCG Normalized Discount Accumulative\nGain is a measure of ranking quality that takes the top K ranked options, where K is the number of answers marked as correct by a least one annotator; in this measure, the fraction of annotators that marked a particular answer as correct is taken into account.\nBaseline and Procedure We report the values for the model described in Murahari et al. (2019) for our baseline-work which builds on VilBERT (Lu et al., 2019), a parallel model of vision and language used for the visual dialogue task-and leverage their model with our custom, multimodal embeddings. Their model uses two transformers, one for the language modality and one for the visual modality. As explained in Lu et al. (2019), the interaction between the two transformers is mediated by two co-attention layers where attention in one modality is conditioned on inputs from the other modality. Murahari et al. (2019) adapted the VilBERT model for the visdial task by using a pre-trained language model trained on English Wikipedia and the BooksCorpus (Zhu et al., 2015) using masked language modeling and next sentence prediction losses. They then frame the task as a next-sentence prediction task (whereas the original VilBERT was modeled to generate descriptions of images). They then use the Conceptual Captions (Sharma et al., 2018) and Visual Question Answering (VQA) (Antol et al., 2015) datasets (using masked image region, masked language modeling, and next sentence prediction losses) to train the two transformers. They then fine-tune on the visdial task (also using masked image region, masked language modeling, and next sentence prediction losses). The underlying architecture uses a pre-trained BERT language model (i.e., bert-base-uncased) as a starting point before training on the Wikipedia, BooksCorpus, Conceptual Captions, and VQA datasets. This constitutes our baseline. We don't consider the dense representations from Murahari et al. (2019) due to hardware limitations. We altered their architecture by replacing the RoBERTa pre-trained embedding layer with the Lancaster and WAC vectors, as depicted in Figure 3. We then fine-tuned on the visdial task using their training regime. 5 We explain how we made vectors compatible with their architecture.\nVocabulary: RoBERTa & AoA Abstract words do not have concrete, visual denotations, such as utopia or justice, so it does not make theoretical sense to include a WAC embedding for words that are clearly abstract because whatever set of images represents those concepts may not have useful semantic information. Moreover, children learning their first language learn concrete concepts before they learn more abstract concepts (Borghi et al., 2019;Ponari et al., 2018). To explore if RoBERTa could make use of a WAC embedding that uses words that are more aimed at a child vocabulary, we report results of filtering out words not in the the Age-of-Acquisition (AoA) list (Kuperman et al., 2012b). AoA a list of 30,000 English words rated for the average age when children first speak those words (avg 11 years; std 3.0, most common words are for ages 2-14). This resulted in 9,627 remaining words in the vocabulary; all other words were set to an embedding of zeros.\nLancaster vectors Similar to AoA, the Lancaster Norms has a predefined vocabulary, which, when compared to the RoBERTa vocabulary results in 11,402 words in both. For each word in the RoBERTa vocabulary that was also in the Lancaster norms, we replaced the RoBERTa embedding with the Lancaster vector for that word; otherwise words retained the original RoBERTa embedding. As their model expects vectors of size 768 (the embedding size for RoBERTa), but the Lancaster vectors are only size 39, we padded the rest of the vector with zeros.\nWAC vectors We use the vocabulary from the RoBERTa tokenizer as with the Lancaster Vectors, which results in a a 27,152-word overlap with the WAC vectors. As the WAC vectors have a dimensionality of 513, smaller than the required size of RoBERTa's 768, we padded zeros after each vector. All vectors that did not exist in the WAC set were zero vectors of size 768. We followed a training regime for two settings:\n\u2022 no-freeze The embedding layer was not frozen so as to allow weight changes during training. \u2022 2-freeze The embedding layer was frozen for two epochs, then the weights were unfrozen for the rest of training; prior work has shown that freezing layers after a certain number of epochs can improve results (Liu et al., 2021); we opt for two because it still allows the finetuning to overpower the exiting embeddings if needed and preliminary results showed that freezing the weights for all epochs resulted in poor model performance.\nWe trained each model for 20 epochs in total, which is the default training setting for this task. We used the settings that were used to train the baseline model (i.e., learning rate of 2e-5). We report the results of the baseline model and the variants of our above changes. 6 Compared to Experiment 1 with the GLUE benchmark, the approach taken in this section fundamentally changes how the Lancaster and WAC embeddings are applied to RoBERTa; here the Lancaster and WAC embeddings are used on a pre-trained model. We hy- 6 Note that the baseline we are comparing to here is lower than what is reported on the leaderboard https://eval.ai/web/challenges/ challenge-page/518/leaderboard. This is partially due to the fact that our training regime was altered due to hardware limitations (i.e., we could only use a batch size of 8 on a single 12GB GPU).\nno freeze MRR R@1 R@5 NDCG  pothesize that RoBERTa will improve with WAC embeddings, as well as the Lancaster concatenated to WAC (denoted lanwac), though the Lancaster embedding on its own may be too small to make a difference. As words that are learned earlier in a child's life are generally more concrete, we hypothesize that RoBERTa will improve when WAC only uses words from the AoA data as more abstract terms are represented by zero vectors.\nResults Table 2 shows the results for the visdial task. Though it is clear that RoBERTa is doing the heavy lifting, when added to RoBERTa, the Lancaster and WAC vectors show improvements over the RoBERTa baseline for some metrics. As noted in Murahari et al. (2019), the NDCG metric is actually counter to MRR, but is important because it takes multiple dialogue response annotations into account. For cases where the Lancaster and WAC models yield better performance, these results suggest that a pre-trained language model can make use of adding multimodal knowledge in the form of vectors derived from multimodal knowledge (Lancaster) and visual (WAC) for the visdial task.\nRoBERTA that uses the WAC embedding especially shows respectable results in the visdial task, particularly when the embedding uses the AoA vocabulary (we only considered AoA for WAC because WAC peformed better than lanwac in this experiment). The WAC vectors were trained on very noisy data, yet despite the noise and the parsimonious model, there is some information about the visual world that enriches the baseline model. The variant trained with frozen weights for 2 epochs, then unfrozen for the remaining 18 epochs had respectable performance across all metrics. 7 7 As a sanity check, we also evaluated using randomly generated embeddings which performed only slightly worse", "n_publication_ref": 17, "n_figure_ref": 1}, {"heading": "Conclusion", "text": "The main contribution of this paper is to explore using the Lancaster Sensorimotor Norms and the Words-as-Classifiers model as vectorized knowledge from the physical world on the GLUE and Visual Dialog tasks. Lancaster norms performed well on their own in one GLUE task compared to other word embeddings like GloVe, and coupled with the WAC vectors as the embedding in an ELECTRA model, they performed respectably on the GLUE task. The WAC vectors, when used as embeddings in the RoBERTa model performed well on the Visual Dialog task, particularly when the vocabulary was more restricted to the Age of Acquisition vocabulary. Crucially, this work differs from other visually grounded models because the grounded knowledge is part of the language model itself (i.e., the embeddings) rather than computed in parallel and added for a task-specific purpose. Moreover, standard language models cannot actually identify denotations when they are present; i.e., ELECTRA and RoBERTa are not actually capable of determining if an object is red or soft from observing that object-a basic ability for a language learning child-simply because those models cannot observe the world outside of text, though the purpose of the WAC (and models like VilBERT) model is to do just that: identify denotations; by coupling WAC with ELECTRA and RoBERTa, both models can make use of that capability.\nThis work is critical in our ongoing efforts towards a model that learns language in a co-located setting in an embodied platform. In particular, our knowledge from this paper informs us that the ELECTRA model with embeddings tied to WAC classifier weights is a good candidate for live interaction of a robot that is learning words from a human collaborator because the ELECTRA-WAC model can function with small amounts of data and the embedding layer can successfully be tied to weights of the WAC classifiers. We leave implementation and evaluation of this model on a robotic platform for future work.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "", "text": "Acknowledgements Thanks to the anonymous reviewers whose comments really helped strengthen the paper. Also thanks to NVIDIA for donating that GPU that was used for the experiments.\nthan baseline when frozen for 2 epochs, but the results of wac-aoa are significantly better.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "VQA: Visual question answering", "journal": "", "year": "2015", "authors": "Stanislaw Antol; Aishwarya Agrawal; Jiasen Lu; Margaret Mitchell; Dhruv Batra; C Lawrence Zitnick; Devi Parikh"}, {"title": "Grounded Cognition", "journal": "Annual Review of Psychology", "year": "2008", "authors": "W Lawrence;  Barsalou"}, {"title": "Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data", "journal": "", "year": "2020", "authors": "M Emily; Alexander Bender;  Koller"}, {"title": "", "journal": "", "year": "", "authors": "Yonatan Bisk; Ari Holtzman; Jesse Thomason; Jacob Andreas; Yoshua Bengio; Joyce Chai; Mirella Lapata; Angeliki Lazaridou"}, {"title": "Conceptual and Perceptual Dimensions of Word Meaning Are Recovered Rapidly and in Parallel during Reading", "journal": "Journal of Cognitive Neuroscience", "year": "2019", "authors": "Valentina Borghesani; Marco Buiatti; Evelyn Eger; Manuela Piazza"}, {"title": "Words as social tools: Language, sociality and inner grounding in abstract concepts", "journal": "Phys. Life Rev", "year": "2019", "authors": "M Anna; Laura Borghi; Ferdinand Barca; Cristiano Binkofski; Giovanni Castelfranchi; Luca Pezzulo;  Tummolini"}, {"title": "Developmental robotics: From babies to robots", "journal": "MIT press", "year": "2015", "authors": "Angelo Cangelosi; Matthew Schlesinger"}, {"title": "First language acquisition", "journal": "Cambridge University Press", "year": "2013", "authors": "V Eve;  Clark"}, {"title": "ELECTRA: Pretraining text encoders as discriminators rather than generators", "journal": "", "year": "2020", "authors": "Kevin Clark; Minh-Thang Luong; V Quoc; Christopher D Le;  Manning"}, {"title": "Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations", "journal": "", "year": "2019", "authors": "Jeff Da; Jungo Kasai"}, {"title": "Visual Dialog. IEEE Transactions on Pattern Analysis and Machine Intelligence", "journal": "", "year": "2019", "authors": "Abhishek Das; Satwik Kottur; Khushi Gupta; Avi Singh; Deshraj Yadav; Stefan Lee; M F Jose; Devi Moura; Dhruv Parikh;  Batra"}, {"title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "journal": "", "year": "2018", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Linguistic bodies: The continuity between life and language", "journal": "Mit Press", "year": "2018", "authors": "Ezequiel A Di Paolo; Elena Clare Cuffari; Hanne De Jaegher"}, {"title": "Where the Action Is: The Foundations of Embodied Interaction. Where the action is the foundations of embodied interaction", "journal": "", "year": "2001", "authors": "Paul Dourish"}, {"title": "Do Neural Language Representations Learn Physical Commonsense? arXiv", "journal": "", "year": "2019", "authors": "Maxwell Forbes; Ari Holtzman; Yejin Choi; G Allen"}, {"title": "Symbol Grounding Without Direct Experience: Do Words Inherit Sensorimotor Activation From Purely Linguistic Context?", "journal": "Cognitive Science", "year": "2018", "authors": "Fritz G\u00fcnther; Carolin Dudschig; Barbara Kaup"}, {"title": "The symbol grounding problem", "journal": "Physica D: Nonlinear Phenomena", "year": "1990", "authors": "Stevan Harnad"}, {"title": "2021. Scaling up visual and Vision-Language representation learning with noisy text supervision", "journal": "", "year": "", "authors": "Chao Jia; Yinfei Yang; Ye Xia; Yi-Ting Chen; Zarana Parekh; Hieu Pham; V Quoc; Yunhsuan Le; Zhen Sung; Tom Li;  Duerig"}, {"title": "The meaning of the body: Aesthetics of human understanding", "journal": "University of Chicago Press", "year": "2008", "authors": "Mark Johnson"}, {"title": "Simple Learning and Compositional Application of Perceptually Grounded Word Meanings for Incremental Reference Resolution", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Casey Kennington; David Schlangen"}, {"title": "", "journal": "", "year": "2019", "authors": "Donghyun Kim; Kuniaki Saito; Kate Saenko; Stan Sclaroff; Bryan A Plummer"}, {"title": "Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search", "journal": "Association for Computational Linguistics", "year": "2018", "authors": "Jamie Ryan Kiros; William Chan; Geoffrey E Hinton"}, {"title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models", "journal": "", "year": "2014", "authors": "Ryan Kiros; Ruslan Salakhutdinov; Richard S Zemel"}, {"title": "Age-of-acquisition ratings for 30,000 English words", "journal": "", "year": "2012", "authors": "Victor Kuperman; Hans Stadthagen-Gonzalez; Marc Brysbaert"}, {"title": "Age-of-acquisition ratings for 30,000 english words", "journal": "Behav. Res. Methods", "year": "2012", "authors": "Victor Kuperman; Hans Stadthagen-Gonzalez; Marc Brysbaert"}, {"title": "Philosophy in the flesh: The embodied mind and its challenge to western thought", "journal": "", "year": "1999", "authors": "George Lakoff; Mark Johnson"}, {"title": "Visualbert: A simple and performant baseline for vision and language. arXiv", "journal": "", "year": "2019", "authors": "Liunian Harold Li; Mark Yatskar; Da Yin; Jui Cho; Kai Wei Hsieh;  Chang"}, {"title": "AutoFreeze: Automatically freezing model blocks to accelerate fine-tuning", "journal": "", "year": "2021", "authors": "Yuhan Liu"}, {"title": "ViLBERT: Pretraining Task", "journal": "", "year": "2019", "authors": "Jiasen Lu; Dhruv Batra; Devi Parikh; Stefan Lee"}, {"title": "Agnostic Visiolinguistic Representations for Visionand-Language Tasks", "journal": "", "year": "", "authors": ""}, {"title": "The Lancaster Sensorimotor Norms: multidimensional measures of perceptual and action strength for", "journal": "", "year": "2019", "authors": "Dermot Lynott; Louise Connell; Marc Brysbaert; James Brand; James Carney"}, {"title": "English words. Behavior Research Methods", "journal": "", "year": "", "authors": ""}, {"title": "How Children Learn to Learn Language", "journal": "Oxford University Press", "year": "2008", "authors": "Lorraine Mccune"}, {"title": "Composing and embedding the words-asclassifiers model of grounded semantics", "journal": "", "year": "2019", "authors": "Daniele Moro; Stacy Black; Casey Kennington"}, {"title": "Large-scale pretraining for visual dialog: A simple state-of-the-art baseline", "journal": "", "year": "2019", "authors": "Vishvak Murahari; Dhruv Batra; Devi Parikh; Abhishek Das"}, {"title": "Glove: Global vectors for word representation", "journal": "", "year": "2014", "authors": "Jeffrey Pennington; Richard Socher; Christopher Manning"}, {"title": "Acquisition of abstract concepts is influenced by emotional valence", "journal": "Dev. Sci", "year": "2018", "authors": "Marta Ponari; Courtenay Frazier Norbury; Gabriella Vigliocco"}, {"title": "Words in the brain's language", "journal": "Behavioral and Brain Sciences", "year": "1999", "authors": "Friedemann Pulverm\u00fcller"}, {"title": "A Primer in BERTology: What we know about how BERT works. arXiv", "journal": "", "year": "2020", "authors": "Anna Rogers; Olga Kovaleva; Anna Rumshisky"}, {"title": "Resolving References to Objects in Photographs using the Words-As-Classifiers Model", "journal": "", "year": "2016", "authors": "David Schlangen; Sina Zarriess; Casey Kennington"}, {"title": "Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning", "journal": "", "year": "2018", "authors": "Piyush Sharma; Nan Ding; Sebastian Goodman; Radu Soricut"}, {"title": "Objects in Space and Mind: From Reaching to Words", "journal": "", "year": "2009", "authors": "L B Smith;  Samuelson"}, {"title": "The Development of Embodied Cognition: Six Lessons from Babies", "journal": "Artificial Life", "year": "2005", "authors": "Linda Smith; Michael Gasser"}, {"title": "Grounded Distributional Semantics for Abstract Words. CogSci", "journal": "", "year": "2016", "authors": "Katsumi Takano; Akira Utsumi"}, {"title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding", "journal": "", "year": "2018", "authors": "Alex Wang; Amanpreet Singh; Julian Michael; Felix Hill; Omer Levy; Samuel R Bowman"}, {"title": "Infusing Finetuning with Semantic Dependencies", "journal": "Transactions of ACL", "year": "2021", "authors": "Zhaofeng Wu; Hao Peng; A Noah; Paul G Smith;  Allen"}, {"title": "PIGLeT: Language grounding through Neuro-Symbolic interaction in a 3D world", "journal": "", "year": "2021", "authors": "Rowan Zellers; Ari Holtzman; Matthew Peters; Roozbeh Mottaghi; Aniruddha Kembhavi; Ali Farhadi; Yejin Choi"}, {"title": "Aligning books and movies: Towards story-like visual explanations by watching movies and reading books", "journal": "", "year": "2015", "authors": "Yukun Zhu; Ryan Kiros; Rich Zemel; Ruslan Salakhutdinov; Raquel Urtasun; Antonio Torralba; Sanja Fidler"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: The red WAC classifiers are trained on positive and negative examples of images from Google Images for the word red; each image is then passed through the CLIP model. We train a binary logistic regression classifier, then extract the coefficients for the red vector.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: Image regions (objects) represented as CLIP vectors are positive and negative train examples for WAC classifier. WAC classifier weights are tied to the embedding layer for the Generator and Discriminator for ELECTRA. Dimensionality Reduction (DR) maps higher dimensional vectors to lower dimensions as needed. Lancaster vectors are represented directly.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Adapted fromMurahari et al. (2019). Our approach uses the same pre-training datasets, architecture, and losses. During the final fine-tuning on the Visual Dialog data, however, we replace the RoBERTa embeddings with the Lancaster and WAC embeddings.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": "GLUE development set results with GloVe,ELECTRA, Lancaster (lan), WAC models and severalcombinations, with (f and without weight freezing dur-ing ELECTRA pre-training."}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "baseline 64.92 50.52 82.98 56.82 lan 64.61 49.98 82.6 58.10 2-freeze MRR R@1 R@5 NDCG lan 63.77 49.03 81.63 57.53 wac 66.38 52.25 83.8 61.47 lanwac 63.93 49.025 82.65 57.73 wac-aoa 66.79 52.75 84.05 60.44", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Experiment 2 results for the visdial task: baseline RoBERTa embedding, Lancaster norms (lan), WAC vectors, and concatenated (lanwac), not frozen, and only frozen for 2 epochs (bottom section).", "figure_data": ""}], "doi": "10.1109/ICCV.2015.279"}