{"authors": "Xiangpeng Wei; Ping Guo; Yunpeng Li; Xingsheng Zhang; Luxi Xing; Yue Hu", "pub_date": "", "title": "IIE's Neural Machine Translation Systems for WMT20", "abstract": "In this paper we introduce the systems IIE submitted for the WMT20 shared task on German\u2194French news translation. Our systems are based on the Transformer architecture with some effective improvements. Multiscale collaborative deep architecture, data selection, back translation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. Our German\u2192French system achieved 35.0 BLEU and ranked the second among all anonymous submissions, and our French\u2192German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions.", "sections": [{"heading": "Introduction", "text": "We participate in the WMT20 shared news translation task in one language pair and two language directions, German\u2192French and French\u2192German. Our methods are based on techniques and approaches used in submissions from past years (Deng et al., 2018;Ng et al., 2019;Sun et al., 2019;Li et al., 2019;Xia et al., 2019), including the use of subword models (Sennrich et al., 2016), iterative back-translation, knowledge distillation, model ensembling and several techniques we proposed recently (Wei et al., 2020b,a).\nFor our submissions of two language directions, we adopt the deep transformer architectures (48layer) based on multiscale collaboration mechanism (Wei et al., 2020b) as our baseline, which outperformed the standard Transformer-Big as well as shallower models significantly in terms of translation quality. We also use an iterative back-translation approach  with the controllable sampling to extend the back translation method by jointly training source-to-target and target-to-source NMT models. Moreover, the knowledge distillation (Freitag et al., 2017) is employed to leverage the source-side monolingual data. For our final models, we apply a domainspecific fine-tuning process and model ensembling, and decode using noisy channel model re-ranking.\nThe paper is structured as follows: Section 2 describes the techniques we used, then section 3 shows the experimental settings and results. Finally, we conclude our work in Section 4.", "n_publication_ref": 9, "n_figure_ref": 0}, {"heading": "Our Techniques", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Multiscale Collaborative Deep Models", "text": "The structure of NMT models has evolved quickly, such as RNN-based (Wu et al., 2016), CNN-based (Gehring et al., 2017) and attentionbased (Vaswani et al., 2017) systems. Deep neural networks have revolutionized the state-of-the-art in various communities, from computer vision to natural language processing. We adopt the deep transformer model proposed by our work (Wei et al., 2020b). Instead of relying on the whole encoder stack to directly learn a desired representation, we let each encoder block learn a fine-grained representation and enhance it by encoding spatial dependencies using a bottom-up network. For coordination, we attend each block of the decoder to both the corresponding representation of the encoder and the contextual representation with spatial dependencies. This not only shortens the path of error propagation, but also helps to prevent the lower level information from being forgotten or diluted. In this section we describe the details (as illustrated in figure 1) of our deep architectures as below:\nBlock-Scale Collaboration. An intuitive extension of naive stacking of layers is to group few stacked layers into a block. We suppose that the encoder and decoder of our model have the same number of blocks (i.e., N ). Each block of the encoder has M n (n \u2208 {1, 2, ..., N }) identical layers, while each decoder block contains one layer. Thus, we can adjust the value of each M n flexibly to increase the depth of the encoder. Formally, for the n-th block of the encoder:\nB n e = BLOCK e (B n\u22121 e ),(1)\nwhere BLOCK e (\u2022) is the block function, in which the layer function F(\u2022) is iterated M n times, i.e. \nwhere l \u2208 {1, 2, ..., M n }, H n,l e and \u0398 n,l e are the representation and parameters of the l-th layer in the n-th block, respectively. The decoder works in a similar way but the layer function G(\u2022) is iterated only once in each block,\nB n d = BLOCK d (B n\u22121 d , B n e ) = G(B n\u22121 d , B n e ; \u0398 n d ) + B n\u22121 d . (3\n)\nEach block of the decoder attends to the corresponding encoder block.\nContextual Collaboration. To model long-term spatial dependencies and reuse global representations, we define a GRU cell Q(c,x), which maps a hidden state c and an additional inputx into a new hidden state:\nC n = Q(C n\u22121 , B n e ), n \u2208 [1, N ] C 0 = E e ,(4)\nwhere E e is the embedding matrix of the source input x. The new state C n can be fused with each layer of the subsequent blocks in both the encoder and the decoder. Formally, B n e in Eq.(1) can be re-calculated in the following way:\nB n e = H n,Mn e , H n,l e = F(H n,l\u22121 e , C n\u22121 ; \u0398 n,l e ) + H n,l\u22121 e , H n,0 e = B n\u22121 e .(5)\nSimilarly, for decoder, we have\nB n d = BLOCK d (B n\u22121 d , B n e ) = G(B n\u22121 d , B n e , C n ; \u0398 n d ) + B n\u22121 d . (6\n)", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "Back-Translation with Controllable Sampling", "text": "Back-translation (BT) is an effective and commonly used data augmentation technique to incorporate monolingual data into a translation system. Back-translation first trains an intermediate targetto-source system that is used to translate monolingual target data into additional synthetic parallel data. This data is used in conjunction with human translated bitext data to train the desired source-totarget system.\nIn our work, we use an iterative back-translation approach to jointly train source-to-target and targetto-source NMT models. The process can be summarized as below:\n\u2022 step 1: we train both a source-to-target model (M 0 x\u2192y ) and a target-to-source model (M 0 y\u2192x ) using the human translated data.\n\u2022 step 2: we use M t x\u2192y to translate source-side monolingual data to target language, and use M t y\u2192x to translate target-side monolingual data to source language, where t starts from 0.\n\u2022 step 3: we combine both the human translated data and pseudo data synthesized in step 2 to further optimize the two NMT models respectively.\n\u2022 Repeat steps 2-3 until the models converge.\nIn practice, we repeat 3 times for steps 2-3. We apply the controllable sampling strategy (Wei et al., 2020a) to synthesize reasonable sentences which are at both high quality and diversity.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Knowledge Distillation and Ensemble", "text": "The early adoption of knowledge distillation (KD) (Kim and Rush, 2016) is for model compression. We use the same method as in Sun et al. (2019) that adopts hybrid heterogeneous teacher: base transformer, deep transformer, big transformer and RNMT+ . For each individual model, we use the other two models as the teacher model to further improve the performance.\nIn addition, model ensemble is also used to boost the performance by combining the predictions of above four models at each decoding step.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "Domain-specific Fine-tuning", "text": "Fine-tuning with domain-specific data is a common and effective method to improve translation quality for a downstream task. After completing training on the bitext and back-translated data, we train for an additional epoch on a smaller in-domain corpus.\nWe first select 100K sentence-pairs from the bilingual as well as pseudo-generated data according to the filter method in Deng et al. (2018) and continue to train the model on the filtered data.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Reranking", "text": "N -best reranking is a method of improving translation quality by scoring and selecting a candidate hypothesis from a list of n-best hypotheses generated by a source-to-target model. For our submissions, we rerank the n-best hypotheses using two aspects as follows:\nlog p(y|x) + \u03bb 1 log p(x|y) + \u03bb 2 log p(y) (7)\nThe weights \u03bb 1 and \u03bb 2 are determined by tuning them with a random search on a validation set and selecting the weights that give the best performance.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "System Overview", "text": "We submit constrained systems to both German to French and French to German translations, with the same techniques.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Dataset", "text": "We use all available bilingual datasets and select 10M bilingual data from WMT'20 corpora using the script filter interactive.py 1 . We share a vocabulary for the two languages and apply BPE for word segmentation with 32K merge  operations. For monolingual data, we use 18M German sentences and 18M French sentences from Newscrawl, and pre-process them in the same way as bilingual data. We split 9k sentences from the \"dev08-14\" as the validation set and use newstest 2019 as the test set.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model Configuration", "text": "We use the PyTorch implementation of Transformer 2 . We choose the Transformer base setting, in which the encoder and decoder are of 48 and 6 layers, respectively. The dropout rate is fixed as 0.1. We set the batch size as 4096 and the parameter --update-freq as 16.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Results", "text": "Results and ablations for De\u2192Fr Fr\u2192De are shown in Table 1 and 2, respectively. We report case-sensitive SacreBLEU scores using Sacre-BLEU (Post, 2018) 3 , using international tokenization for German\u2194French.\nGerman\u2192French For De\u2192Fr, iterative BT improves our baseline performance on newstest 2019 by about 2.5 BLEU. The addition of KD and model ensemble improves single model performance by 0.8 BLEU, but combining this with fine-tuning and reranking gives us a total of 2 BLEU. Our final submission for WMT20 achieves 35.0 BLEU points for German\u2192French translation (ranked in the second place).\nFrench\u2192German For Fr\u2192De, we see similar improvements with iterative BT by about 2.3 BLEU. KD, ensembling, and fine-tuning add an additional 1.4 BLEU, with reranking contributing 0.9 BLEU. Our final submission for WMT20 achieves 36.6 BLEU points for French\u2192German translation (ranked in the fourth among anonymous submissions).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "This paper describes CAS IIE's submission to the WMT20 German\u2194French news translation task. We investigate extremely deep models (with 48 layers) and exploit effective strategies to better utilize parallel data as well as monolingual data. Finally, our German\u2192French system achieved 35.0 BLEU and ranked the second among all anonymous submissions, and our French\u2192German system achieved 36.6 BLEU and ranked the fourth in all anonymous submissions.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "The best of both worlds: Combining recent advances in neural machine translation", "journal": "", "year": "2018", "authors": "Mia Xu Chen; Orhan Firat; Ankur Bapna; Melvin Johnson; Wolfgang Macherey; George Foster; Llion Jones; Mike Schuster; Noam Shazeer; Niki Parmar; Ashish Vaswani; Jakob Uszkoreit; Lukasz Kaiser; Zhifeng Chen; Yonghui Wu; Macduff Hughes"}, {"title": "Alibaba's neural machine translation systems for wmt18", "journal": "", "year": "2018", "authors": "Yongchao Deng; Shanbo Cheng; Jun Lu; Kai Song; Jingang Wang; Shenglan Wu; Liang Yao; Guchun Zhang; Haibo Zhang; Pei Zhang; Changfeng Zhu; Boxing Chen"}, {"title": "Ensemble distillation for neural machine translation", "journal": "", "year": "2017", "authors": "Markus Freitag; Yaser Al-Onaizan; Baskaran Sankaran"}, {"title": "Convolutional sequence to sequence learning", "journal": "", "year": "2017", "authors": "Jonas Gehring; Michael Auli; David Grangier; Denis Yarats; Yann N Dauphin"}, {"title": "Sequencelevel knowledge distillation", "journal": "Association for Computational Linguistics", "year": "2016", "authors": "Yoon Kim; Alexander M Rush"}, {"title": "The niutrans machine translation systems for wmt19", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Bei Li; Yinqiao Li; Chen Xu; Ye Lin; Jiqiang Liu; Hui Liu; Ziyang Wang; Yuhao Zhang; Nuo Xu; Zeyang Wang; Kai Feng; Hexuan Chen; Tengbo Liu; Yanyang Li; Qiang Wang; Tong Xiao; Jingbo Zhu"}, {"title": "Facebook fair's wmt19 news translation task submission", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Nathan Ng; Kyra Yee; Alexei Baevski; Myle Ott; Michael Auli; Sergey Edunov"}, {"title": "A call for clarity in reporting BLEU scores", "journal": "", "year": "2018", "authors": "Matt Post"}, {"title": "Neural machine translation of rare words with subword units", "journal": "Long Papers", "year": "2016", "authors": "Rico Sennrich; Barry Haddow; Alexandra Birch"}, {"title": "Baidu neural machine translation systems for wmt19", "journal": "", "year": "2019", "authors": "Meng Sun; Bojian Jiang; Hao Xiong; Zhongjun He; Hua Wu; Haifeng Wang"}, {"title": "Attention is all you need", "journal": "", "year": "2017-12-09", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"title": "Uncertaintyaware semantic augmentation for neural machine translation", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Xiangpeng Wei; Heng Yu; Yue Hu; Rongxiang Weng; Luxi Xing; Weihua Luo"}, {"title": "Multiscale collaborative deep models for neural machine translation", "journal": "", "year": "2020", "authors": "Xiangpeng Wei; Heng Yu; Yue Hu; Yue Zhang; Rongxiang Weng; Weihua Luo"}, {"title": "Google's neural machine translation system: Bridging the gap between human and machine translation", "journal": "", "year": "2016", "authors": "Yonghui Wu; Mike Schuster; Zhifeng Chen; V Quoc; Mohammad Le; Wolfgang Norouzi; Maxim Macherey; Yuan Krikun; Qin Cao; Klaus Gao;  Macherey"}, {"title": "Microsoft research asia's systems for wmt19", "journal": "", "year": "2019", "authors": "Yingce Xia; Xu Tan; Fei Tian; Fei Gao; Di He; Weicong Chen; Yang Fan; Linyuan Gong; Yichong Leng; Renqian Luo; Yiren Wang; Lijun Wu; Jinhua Zhu; Tao Qin; Tie-Yan Liu"}, {"title": "Joint training for neural machine translation models with monolingual data", "journal": "AAAI Press", "year": "2018-02-02", "authors": "Zhirui Zhang; Shujie Liu; Mu Li; Ming Zhou; Enhong Chen"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Illustration of Multiscale Collaborative Deep NMT Model. N is the number of encoder and decoder blocks. The n-th block of the encoder consists of M n layers, while each decoder block only contains one layer.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "SacreBLEU scores on French\u2192German.", "figure_data": ""}], "doi": "10.18653/v1/P18-1008"}