{"authors": "Yunxiang Zhang; Liangming Pan; Samson Tan; Min-Yen Kan", "pub_date": "", "title": "Interpreting the Robustness of Neural NLP Models to Textual Perturbations", "abstract": "Modern Natural Language Processing (NLP) models are known to be sensitive to input perturbations and their performance can decrease when applied to real-world, noisy data. However, it is still unclear why models are less robust to some perturbations than others. In this work, we test the hypothesis that the extent to which a model is affected by an unseen textual perturbation (robustness) can be explained by the learnability of the perturbation (defined as how well the model learns to identify the perturbation with a small amount of evidence). We further give a causal justification for the learnability metric. We conduct extensive experiments with four prominent NLP models -TextRNN, BERT, RoBERTa and XLNetover eight types of textual perturbations on three datasets. We show that a model which is better at identifying a perturbation (higher learnability) becomes worse at ignoring such a perturbation at test time (lower robustness), providing empirical support for our hypothesis.", "sections": [{"heading": "Introduction", "text": "Despite the success of deep neural models on many Natural Language Processing (NLP) tasks (Liu et al., 2016;Devlin et al., 2019;Liu et al., 2019b), recent work has discovered that these models are not robust to noisy input from the real world and thus their performance will decrease (Prabhakaran et al., 2019;Niu et al., 2020;Ribeiro et al., 2020;Moradi and Samwald, 2021). A reliable NLP system should not be easily fooled by slight noise in the text. Although a wide range of evaluation approaches for robust NLP models have been proposed (Ribeiro et al., 2020;Morris et al., 2020;Goel et al., 2021;, few attempts have been made to understand these benchmark results. Given the difference of robustness between models and perturbations, it is a natural question why models are more sensitive to some perturbations than others. It is crucial to avoid over-sensitivity to input perturbations, and understanding why it happens is useful for revealing the weaknesses of current models and designing more robust training methods. To the best of our knowledge, a quantitative measure to interpret the robustness of NLP models to textual perturbations has yet to be proposed. To improve the robustness under perturbation, it is common practice to leverage data augmentation (Li and Specia, 2019;Min et al., 2020;Tan and Joty, 2021). Similarly, how much data augmentation through the perturbation improves model robustness varies between models and perturbations. In this work, we aim to investigate two Research Questions (RQ):\n\u2022 RQ1: Why are NLP models less robust to some perturbations than others?\n\u2022 RQ2: Why does data augmentation work better at improving the model robustness to some perturbations than others?\nWe test a hypothesis for RQ1 that the extent to which a model is affected by an unseen textual perturbation (robustness) can be explained by the learnability of the perturbation (defined as how well the model learns to identify the perturbation with a small amount of evidence). We also validate another hypothesis for RQ2 that the learnability metric is predictive of the improvement on robust performance brought by data augmentation along a perturbation. Our proposed learnability is inspired by the concepts of Randomized Controlled Trial (RCT) and Average Treatment Effect (ATE) from Causal Inference (Rubin, 1974;Holland, 1986). Estimation of perturbation learnability for a model consists of three steps: \u2460 randomly labelling a dataset, \u2461 perturbing examples of a particular pseudo class with probabilities, and \u2462 using ATE to measure the ease with which the model learns the perturbation. The core intuition for our method is to frame an RCT as a perturbation identification task and formalize the notion of learnability Exp No. Measurement", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "Label", "text": "Perturbation Training Examples Test Examples 0 Standard original l \u2208 \u2205 (x i , 0), (x j , 1) (x i , 0), (x j , 1)\n1 Robustness original l \u2208 {0, 1} (x i , 0), (x j , 1) (x * i , 0), (x * j , 1) 2 Data Augmentation original l \u2208 {0, 1} (x i , 0), (x j , 1) (x * i , 0), (x * j , 1)\n(x * i , 0), (x * j , 1) 3 Learnability random l \u2032 \u2208 {1 \u2032 } (x j , 0 \u2032 ), (x * i , 1 \u2032 ) (x * i , 1 \u2032 ) 4 random l \u2032 \u2208 {1 \u2032 } (x j , 0 \u2032 ), (x * i , 1 \u2032 ) (x i , 1 \u2032 )\nTable 1: Example experiment settings for measuring learnability, robustness and improvement by data augmentation.\nWe perturb an example if its label falls in the set of label(s) in \"Perturbation\" column. \u2205 means no perturbation at all. Training/test examples are the expected input data, assuming we have only one negative (x i , 0) and positive (x j , 1) example in our original training/test set. l \u2032 is a random label and x * is a perturbed example.\nas a causal estimand based on ATE. We conduct extensive experiments on four neural NLP models with eight different perturbations across three datasets and find strong evidence for our two hypotheses. Combining these two findings, we further show that data augmentation is only more effective at improving robustness against perturbations that a model is more sensitive to, contributing to the interpretation of robustness and data augmentation. Learnability provides a clean setup for analysis of the model behaviour under perturbation, which contributes better model interpretation as well.\nContribution. This work provides an empirical explanation for why NLP models are less robust to some perturbations than others. The key to this question is perturbation learnability, which is grounded in the causality framework. We show a statistically significant inverse correlation between learnability and robustness.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Setup and Terminology", "text": "As a pilot study, we consider the task of binary text classification. The training set is denoted as\nD train = {(x 1 , l 1 ), ..., (x n , l n )}\n, where x i is the i-th example and l i \u2208 {0, 1} is the corresponding label. We fit a model f \u2236 (x; \u03b8) \u21a6 {0, 1} with parameters \u03b8 on the training data. A textual perturbation is a transformation g \u2236 (x; \u03b2) \u2192 x * that injects a specific type of noise into an example x with parameters \u03b2 and the resulting perturbed example is x * . We design several experiment settings (Table 1) to answer our research questions. Experiment 0 in Table 1 is the standard learning setup, where we train and evaluate a model on the original dataset. Below we detail other experiment settings.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Definitions", "text": "Robustness. We apply the perturbations to test examples and measure the robustness of model to said perturbations as the decrease in accuracy.\nIn Table 1, Experiment 1 is related to robustness measurement, where we train a model on unperturbed dataset and test it on perturbed examples. We denote the test accuracy of a model f (\u22c5) on examples perturbed by g(\u22c5) in Experiment 1 as\nA 1 (f, g, D * test ).\nSimilarly, the test accuracy in Experiment 0 is A 0 (f, D test ). Consequently, the robustness is calculated as the difference of test accuracies:\nrobustness(f, g, D) = A 1 (f, g, D * test ) \u2212A 0 (f, D test ).\n(1)\nModels usually suffer a performance drop when encountering perturbations, therefore the robustness is usually negative, where lower values indicate decreased robustness.\nImprovement by Data Augmentation (Post Augmentation \u2206). To improve robust accuracy (Tu et al., 2020) (i.e., accuracy on the perturbed test set), it is a common practice to leverage data augmentation (Li and Specia, 2019;Min et al., 2020;Tan and Joty, 2021). We simulate the data augmentation process by appending perturbed data to the training set (Experiment 2 of Table 1). We calculate the improvement on performance after data augmentation as the difference of test accuracies:\n\u2206 post_aug (f, g, D) = A 2 (f, g, D * test ) \u2212A 1 (f, g, D * test ).\n(\n)2\nwhere A 2 (f, g, D * test ) denotes the test accuracy of Experiment 2. \u2206 post_aug is the higher the better.\nLearnability. We want to compare perturbations in terms of how well the model learns to identify them with a small amount of evidence. We cast learnability estimation as a perturbation classification task, where a model is trained to identify the perturbation in an example. We define that the learnability estimation consists of three steps, namely \u2460 assigning random labels, \u2461 perturbing with probabilities, and \u2462 estimating model performance. Below we introduce the procedure and intuition for each step. This estimation framework is further grounded in concepts from the causality literature in Section 3, which justifies our motivations. We summarize our estimation approach formally in Algorithm 1 (Appendix A). \u2460 Assigning Random Labels. We randomly assign pseudo labels to each training example regardless of its original label. Each data point has equal probability of being assigned to positive (l \u2032 = 1) or negative (l \u2032 = 0) pseudo label. This results in a randomly labeled dataset\nD \u2032 train = {(x 1 ; l \u2032 1 ), ..., (x n , l \u2032 n )}, where L \u2032 \u223c Bernoulli(1, 0.5).\nIn this way, we ensure that there is no difference between the two pseudo groups since the data are randomly split. \u2461 Perturbing with Probabilities. We apply the perturbation g(\u22c5) to each training example in one of the pseudo groups (e.g., l \u2032 = 1 in Algorithm 1) 1 . In this way, we create a correlation between the existence of perturbation and label (i.e., the perturbation occurrence is predictive of the label). We control the perturbation probability p \u2208 [0, 1], i.e., an example has a specific probability p of being perturbed. This results in a perturbed training set\nD \u2032 * train = {(x * 1 , l \u2032 1 ), ..., (x * n , l \u2032 n )},\nwhere the perturbed example x * i is:\nZ \u223c U (0, 1), \u2200i \u2208 {1, 2, ..., n} x * i = \u23a7 \u23aa \u23aa \u23a8 \u23aa \u23aa \u23a9 g(x i ) l \u2032 i = 1 \u2227 z < p, x i\notherwise.\n(\n)3\nHere Z is a random variable drawn from a uniform distribution U (0, 1). Due to randomization in the formal step, now the only difference between the two pseudo groups is the occurrence of perturbation. \u2462 Estimating Model Performance. We train a model on the randomly labeled dataset with per-1 Because the training data is randomly split into two pseudo groups, applying perturbations to any one of the groups should yield same result. We assume that we always perturb into the first group (l\n\u2032 = 1) hereafter.\nturbed examples. Since the only difference between the two pseudo groups is the existence of the perturbation, the model is trained to identify the perturbation. The original test examples D test are also assigned random labels and become D \u2032 test . We perturb all of the test examples in one pseudo group (e.g., l \u2032 = 1, as in step 2.1) to produce a perturbed test set D \u2032 * test . Finally, the perturbation learnability is calculated as the difference of accuracies on D \u2032 * test and D \u2032 test , which indicates how much the model learns from the perturbation's co-occurrence with pseudo label:\nlearnability(f, g, p, D) = A 3 (f, g, p, D \u2032 * test ) \u2212A 4 (f, g, p, D \u2032 test ).(4)\nA 4 (f, g, p, D \u2032 * test ) and A 3 (f, g, p, D \u2032 test ) are accuracies measured by Experiment 4 and 3 of Table 1, respectively.\nWe observe that the learnability depends on perturbation probability p. For each modelperturbation pair, we obtain multiple learnability estimates by varying the perturbation probability (Figure 3). However, we expect that learnability of the perturbation (as a concept) should be independent of perturbation probability. To this end, we use the log AU C (area under the curve in log scale) of the p \u2212 learnability curve (Figure 3), termed as \"average learnability\", which summarizes the overall learnability across different perturbation probabilities p 1 , ..., p t :\navg_learnability(f, g, D) \u2236= log AU C({(p i , learnability(f, g, p i , D)) | i \u2208 {1, 2, ..., t}}).(5)\nWe use log AU C rather than AU C because we empirically find that the learnability varies substantially between perturbations when p is small, and a log scale can better capture this nuance. We also introduce learnability at a specific perturbation probability (Learnability @ p) as an alternate summary metric and provide a comparison of this metric against log AU C in Appendix D.", "n_publication_ref": 5, "n_figure_ref": 2}, {"heading": "Hypothesis", "text": "With the above-defined terminologies, we propose hypotheses for RQ1 and RQ2 in Section 1, respectively.\nHypothesis 1 (H1): A model for which a perturbation is more learnable is less robust against the same perturbation at the test time.\nThis is not obvious because the model encounters this perturbation during training in learnability estimation while they do not in robustness measurement.\nHypothesis 2 (H2): A model for which a perturbation is more learnable experiences bigger robustness gains with data augmentation along such a perturbation.\nWe validate both Hypotheses 1 and 2 with experiments on several perturbations and models described in Section 4.1 and 4.2.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Causal View on Perturbation Learnability", "text": "In Section 2.1, we introduce the term \"learnability\" in an intuitive way. Now we map it to a formal, quantitative measure in standard statistical frameworks. Learnability is actually motivated by concepts from the causality literature. We provide a brief introduction to basic concepts of causal inference in Appendix B. In fact, learnability is the causal effect of perturbation on models, which is often difficult to measure due to the confounding latent features. In the language of causality, this is \"correlation is not causation\". Causality provides insight on how to fully decouple the effect of perturbation and other latent features. We introduce the causal motivations for step 2.1 and 2.1 of learnability estimation in the following Section 3.1 and 3.2, respectively.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Causal Explanation for Random Label Assignment", "text": "Natural noise (simulated by perturbations in this work) usually co-occurs with latent features in an example. If we did not assign random labels and simply perturbed one of the original groups, there would be confounding latent features that would prevent us from estimating the causal effect of the perturbation. Figure 1a illustrates this scenario. Both perturbation P and latent feature T may affect the outcome Y , 2 while the latent feature is predictive of label L. Since we make the perturbation P on examples with the same label, P is decided by L. It therefore follows that T is a confounder of the effect of P on Y , resulting in non-causal association flowing along the path P \u2190 L \u2190 T \u2192 Y . However, if we do randomize the labels, P no longer has any causal parents (i.e., incoming edges) (Figure 1b). This is because perturbation is purely 2 Y is later defined in Section 3.2 the language of causality, this is \"correlation is not causation\". Causality provides insight on how to fully decouple the effect of perturbation and other latent features. We introduce the causal motivations for step 1 and 3 of learnability estimation in the following Section 3.1 and 3.2 respectively.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "A Causal Explanation for Random Label Assignment", "text": "Natural noise (simulated by perturbations in this work) usually co-occurs with latent features in an example. If we did not assign random labels and simply perturbed one of the original groups, there would be confounding latent features that would prevent us from estimating the causal effect of the perturbation. Figure 4a illustrates this scenario.\nBoth perturbation P and latent feature T may affect the outcome Y , 3 while the latent feature is predictive of label L. Since we make perturbation P on examples with the same label, P is decided by L. It therefore follows that T is a confounder of the effect of P on Y , resulting in non-causal association flowing along the path P \u2190 L \u2190 T \u2192 Y . However, if we do randomize the labels, P no longer has any causal parents (i.e., incoming edges) (Figure 4b). This is because perturbation is purely random. Without the path represented by P \u2190 L, all of the association that flows from P to Y is causal.\nAs a result, we can directly calculate the causal effect from the observed outcomes (Section 3.2).\nOur randomization experiments allow us to dis-  of the association that flows from P to Y is causal.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "314", "text": "As a result, we can directly calculate the causal 315 effect from the observed outcomes (Section 3.2).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "316", "text": "Our randomization experiments allow us to dis-  Figure 1: Causal graph explanation for decoupling perturbation and latent feature with randomization. P is the perturbation and T is the latent feature. L is the original label and Y is the correctness of the predicted label.\nrandom. Without the path represented by P \u2190 L, all of the association that flows from P to Y is causal. As a result, we can directly calculate the causal effect from the observed outcomes.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Learnability is a Causal Estimand", "text": "We identify learnability as a causal estimand. In causality, the term \"identification\" refers to the process of moving from a causal estimand (Average Treatment Effect, ATE) to an equivalent statistical estimand. We show that the difference of accuracies on D \u2032 * test and D \u2032 test is actually a causal estimand. We define the outcome Y of a test example x i as the correctness of the predicted label:\nY i (0) \u2236= 1 {f (x i )=l \u2032 i } . (6\n)\nwhere 1 {\u22c5} is the indicator function. Similarly, the outcome Y of a perturbed test example\nx * i is: Y i (1) \u2236= 1 {f (x * i )=l \u2032 i } .(7)\nAccording to the definition of Individual Treatment Effect (ITE, see Equation 9of Appendix B), we have\nIT E i = 1 {f (x * i )=l \u2032 i } \u22121 {f (x i )=l \u2032 i } .\nWe then take the average over all the perturbed test examples (half of the test set) 3 . This is our Average Treatment Effect (ATE):\nAT E = E[Y (1)] \u2212 E[Y (0)] = E[1 {f (x * )=l \u2032 } ] \u2212 E[1 {f (x)=l \u2032 } ] = P (f (x * ) = l \u2032 ) \u2212 P (f (x) = l \u2032 ) = A(f, g, p, D \u2032 * test ) \u2212 A(f, g, p, D \u2032 test ).(8)\nPerturbation Example Sentence None His quiet and straightforward demeanor was rare then and would be today.\nduplicate_punctuations His quiet and straightforward demeanor was rare then and would be today.. butter_fingers_perturbation His quiet and straightforward demeanor was rarw then and would be today.\nshuffle_word quiet would and was be and straightforward then demeanor His today. rare random_upper_transformation His quiEt and straightForwARd Demeanor was rare TheN and would be today.\ninsert_abbreviation His quiet and straightforward demeanor wuz rare then and would b today.\nwhitespace_perturbation His quiet and straightforward demean or wa s rare thenand would be today.\nvisual_attack_letters Hi\u1e69 q\u1ee7i\u1ebdt \u1ea7\u057cd str\u1e01igh\u1e6d\u1e1forw\u1eb3r\u0221 d\u0511meano\u0155 w\u0203\u1e63 r\u0227re t\u1e2ben and wou\u1d85d \u03f8\u04d9 t\u0ead\u1e0f\u1ea7\u0233.\nleet_letters His qui3t and strai9htfor3ard d3m3an0r 3as rar3 t43n and 30uld 63 t0da4. where A(f, g, p, D) is the accuracy of model f (\u22c5) trained with perturbation g(\u22c5) at perturbation probability p on test set D. Therefore, we show that ATE is exactly the difference of accuracy on the perturbed and unperturbed test sets with random labels. And the difference is learnability according to Equation 4. We discuss another means of identification of ATE in Appendix C, based on the prediction probability. We compare between the probability-based and accuracy-based metrics there. We find that our accuracy-based metric yields better resolution, so we report this metric in the main text of this paper.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Experiments", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Perturbation methods", "text": "Criteria for Perturbations. We select various character-level and word-level perturbation methods in existing literature that simulate different types of noise an NLP model may encounter in real-world situations. These perturbations are nonadversarial, label-consistent, and can be automatically generated at scale. We note that our perturbations do not require access to the model internal structure. We also assume that the feature of perturbation does not exist in the original data. Not all perturbations in the existing literature are suitable for our task. For example, a perturbation that swaps gender words (i.e., female \u2192 male, male \u2192 female) is not suitable for our experiments since we cannot distinguish the perturbed text from an unperturbed one. In other words, the perturbation function g(\u22c5) should be asymmetric, such that g(g(x)) \u2260 x.\nFigure 2 shows an example sentence with different perturbations. Perturbation of \"dupli-cate_punctuation\" doubles the punctuation by appending a duplicate after each punctuation, e.g., \",\" \u2192 \"\"\"; \"butter_fingers_perturbation\" misspells some words with noise erupting from keyboard typos; \"shuffle_word\" randomly changes the order of word in the text (Moradi and Samwald, 2021); \"random_upper_transformation\" randomly adds upper cased letters (Wei and Zou, 2019); \"in-sert_abbreviation\" implements a rule system that encodes word sequences associated with the replaced abbreviations; \"whitespace_perturbation\" randomly removes or adds whitespaces to text; \"vi-sual_attack_letters\" replaces letters with visually similar, but different, letters (Eger et al., 2019); \"leet_letters\" replaces letters with leet, a common encoding used in gaming (Eger et al., 2019).", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "Experimental Settings", "text": "To test the learnability, robustness and improvement by data augmentation with different NLP models and perturbations, we experiment with four modern and representative neural NLP models: TextRNN (Liu et al., 2016), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019b) and XLNet (Yang et al., 2019). For TextRNN, we use the implementation by an open-source text classification toolkit NeuralClassifier (Liu et al., 2019a). For the other three pretrained models, we use the bert-base-cased, roberta-base,\nxlnet-base-cased versions from Hugging Face (Wolf et al., 2020), respectively. These two platforms support most of the common NLP models, thus facilitating extension studies of more models in future. We use three common binary text classification datasets -IMDB movie reviews (IMDB) (Pang and Lee, 2005), Yelp polarity reviews (YELP) (Zhang et al., 2015), Quora Question Pair (QQP) (Iyer et al., 2017) -as our testbeds. IMDB and YELP datasets present the task of sentiment analysis, where each sentence is labelled     as positive or negative sentiment. QQP is a paraphrase detection task, where each pair of sentences is marked as semantically equivalent or not. To control the effect of dataset size and imbalanced classes, all datasets are randomly subsampled to the same size as IMDB (50k) with balanced classes.\nThe training steps for all experiments are the same as well. We implement perturbations g(\u22c5) with two self-designed ones and six selected ones from the NL-Augmenter library (Dhole et al., 2021). For perturbation probabilities, we choose 0.001, 0.005, 0.01, 0.02, 0.05, 0.10, 0.50, 1.00. We run all experiments across three random seeds and report the average results.", "n_publication_ref": 10, "n_figure_ref": 0}, {"heading": "Perturbation Learnability Analysis", "text": "Figure 3 shows learnability as a function of perturbation probability. Learnability @ p generally increases as we increase the perturbation probability, and when we perturb all the examples (i.e., p = 1.0), every model can easily identify it well, resulting in the maximum learnability of 1.0. This shows that neural NLP models master these perturbations eventually. At lower perturbation probabilities, some models still learn that perturbation alone predicts the label. In fact, the major difference between different p \u2212 learnability curves is the area of lower perturbation probabilities and this provides motivation for using log AU C instead of AU C as the summarization of learnability at different p (Section 2.1).\nTable 2 shows the average learnability over all perturbation probabilities of each modelperturbation pair on IMDB dataset in Figure 3. 4 It reveals the most learnable perturbation for each model. For example, the learnability of \"vi-sual_attack_letters\" and \"leet_letters\" are very high for all four models, likely due to their strong effects on the tokenization process (Salesky et al., 2021). Perturbations like \"white_space_perturbation\" and \"duplicate_punctuations\" are less learnable for pretrained models, probably because they have weaker effects on the subword level tokenization, or they may have encountered similar noise in the pretraining corpora. We observe that \"dupli-cate_punctuations\" already exists in the original text of YELP dataset (e.g., \"The burgers are awesome!!\"), thus violating our assumptions for perturbations in Section 4.1. As a result, the curve for 4 Please refer to Appendix E for benchmark results on YELP (Table 5) and QQP (  this perturbation substantially deviates from others in Figure 3. We do not count this perturbation on YELP dataset in the following analysis. The perturbation learnability experiments provide a clean setup for NLP practitioners to analyze the effect of textual perturbations on models.", "n_publication_ref": 2, "n_figure_ref": 3}, {"heading": "Empirical Findings", "text": "We observe a negative correlation between learnability (Equation 4) and robustness (Equation 1) across all three datasets in Table 2, validating Hypothesis 1. Table 2 also quantifies the trend that data augmentation with a perturbation the model is less robust to has more improvement on robustness (Hypothesis 2). We plot the correlations on IMDB dataset in Figure 4a and 4b. 5 Both the correlations between 1) learnability vs. robustness and 2) learnability vs. improvement by data augmentation are strong (Spearman |\u03c1| > 0.6) and highly significant (p-value < 0.001), which firmly supports our hypotheses. Our findings provide insight about when the model is less robust and when data augmentation works better for improving robustness. Figure 4c shows that the more learnable a perturbation is for a model, the greater the likelihood that its robustness can be improved through data augmentation along this perturbation. We argue that this is not simply because there is more room for improvement by data augmentation. From a causal perspective, learnability acts as a common cause (confounder) for both robustness and improvement by data augmentation. This indicates a potential limitation of using data augmentation for improving robustness to perturbations : data augmentation is only more effective at improving robustness against perturbations more learnable for a model.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "Discussion", "text": "Potential Impacts. Our findings seem intuitive but are non-trivial. The NLP models were not trained on perturbed examples when measuring robustness, but still they display a strong correlation with perturbation learnability. Understanding these findings are important for a more principled evaluation of and control over NLP models . Specifically, the learnability metric complements to the evaluation of newly designed perturbations by revealing model weaknesses in a clean setup. Reducing perturbation learnability is promising for improving robustness of models. Contrastive learning (Gao et al., 2021;Yan et al., 2021) that pulls the representations of the original and perturbed text together, makes it difficult for the model to identify the perturbation (reducing learnability) and thus may help improve robustness. Perturbation can also be viewed as injecting spurious feature into the examples, so the learnability metric also helps to interpret robustness to spurious correlation (Sagawa et al., 2020). Moreover, learnability may facilitate the development of model architectures with explicit inductive biases (Warstadt and Bowman, 2020; to avoid sensitivity to noisy perturbations. Grounding the learnability within the causality framework inspires future researchers to incorporate the causal perspective into model design (Zhang et al., 2020), and make the model robust to different types of perturbations.\nLimitations. In this work, we focus on the robust accuracy (Section 2.1), which is accuracy on the perturbed test set. We do not assume that the test accuracy of the original test set, a.k.a in-distribution accuracy, is invariant invariant against training with augmentation or not. It would be interesting to investigate the trade-off between robust accuracy and in-distribution accuracy in the future. We also note that this work has not established that the relationship between learnability and robustness is causal. This could be explored with other approaches in causal inference for deconfounding besides simulation on randomized control trial, such as working with real data but stratifying it (Frangakis and Rubin, 2002), to bring the learnability experiment closer to more naturalistic settings. Although we restrict to balanced, binary classification for simplicity in this pilot study, our framework can also be extended to imbalanced, multi-class classification.\nWe are aware that computing average learnability is expensive for large models and datasets, which is further discussed in Section 8. We provide a greener solution in Appendix D. We could further verify our assumptions for perturbations with a user study (Moradi and Samwald, 2021) which investigates how understandable the perturbed texts are to humans.", "n_publication_ref": 7, "n_figure_ref": 0}, {"heading": "Related Work", "text": "Robustness of NLP Models to Perturbations. The performance of NLP models can decrease when encountering noisy data in the real world. Recent works (Prabhakaran et al., 2019;Ribeiro et al., 2020;Niu et al., 2020;Moradi and Samwald, 2021) present comprehensive evaluations of the robustness of NLP models to different types of perturbations, including typos, changed entities, negation, etc. Their results reveal the phenomenon that NLP models can handle some specific types of perturbation more effectively than others. However, they do not go into a deeper analysis of the reason behind the difference of robustness between models and perturbations.\nInterpretation of Data Augmentation. Although data augmentation has been widely used in CV (Sato et al., 2015;DeVries and Taylor, 2017;Dwibedi et al., 2017) and NLP (Wang and Yang, 2015;Kobayashi, 2018;Wei and Zou, 2019), the underlying mechanism of its effectiveness remains under-researched. Recent studies aim to quantify intuitions of how data augmentation improves model generalization. Gontijo-Lopes et al. (2020) introduce affinity and diversity, and find a correlation between the two metrics and augmentation performance in image classification. In NLP, Kashefi and Hwa (2020) propose a KL-divergence-based metric to predict augmentation performance. Our proposed learnability metric implies when data augmentation works better and thus acts as a complement to this line of research.", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "Conclusion", "text": "This work targets at an open question in NLP: why models are less robust to some textual perturbations than others? We find that learnability, which causally quantifies how well a model learns to identify a perturbation, is predictive of the model robustness to the perturbation. In future work, we will investigate whether these findings can generalize to other domains, including computer vision.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Ethics Statement", "text": "Computing average learnability requires training a model for multiple times at different perturbation probabilities, which can be computationally intensive if the sizes of the datasets and models are large. This can be a non-trivial problem for NLP practitioners with limited computational resources. We hope that our benchmark results of typical perturbations for NLP models work as a reference for potential users. Collaboratively sharing the results of such metrics on popular models and perturbations in public fora can also help reduce duplicate investigation and coordinate efforts across teams.\nTo alleviate the computational efficiency issue of average learnability estimation, using learnability at selected perturbation probabilities may help at the cost of reduced precision (Appendix D). We are not alone in facing this issue: two similar metrics for interpreting model inductive bias, extractability and s-only error ) also require training the model repeatedly over the whole dataset. Therefore, finding an efficient proxy for average learnability is promising for more practical use of learnability in model interpretation.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A Algorithm for Perturbation Learnability Estimation", "text": "Algorithm 1 Learnability Estimation Input:\ntraining set D train = {(x 1 , l 1 ), ..., (x n , l n )}, test set D test = {(x n+1 , l n+1 ), ..., (x n+m , l n+m )}, D = D train \u222a D test , model f \u2236 (x; \u03b8) \u21a6 {0, 1}, perturbation g \u2236 (x; \u03b2) \u2192 x * , perturbation probability p Output: learnability(f, g, p, D)\n1: // \u2460 assigning random labels 2: Initialize an empty dataset D \u2032 3: for i in {1, 2, ..., n + m} do 4:\nl \u2032 i \u2190 randint[0, 1] 5: D \u2032 \u2190 D \u2032 \u222a {(x i , l \u2032 i\n)} 6: end for 7: // \u2461 perturbing with probabilities 8: Initialize an empty dataset D \u2032 * 9: for i in {1, 2, ..., n + m} do 10: z \u2190 rand(0, 1) 11:\nx * i \u2190 x i 12: if l \u2032 i = 1 \u2227 z < p then 13: x * i \u2190 g(x i ) 14:\nend if 15: \nD \u2032 * \u2190 D \u2032 * \u222a {(x * i , l \u2032 i )\nD \u2032 train , D \u2032 test \u2190 D \u2032 [1 \u2236 n], D \u2032 [n + 1 \u2236 n + m] 19: D \u2032 * train , D \u2032 * test \u2190 D \u2032 * [1 \u2236 n], D \u2032 * [n+1 \u2236 n+m] 20: fit the model f (\u22c5) on D \u2032 * train 21: A(f, g, p, D \u2032 * test ) \u2190 f (\u22c5) accuracy on D \u2032 * test 22: A(f, g, p, D \u2032 test ) \u2190 f (\u22c5) accuracy on D \u2032 test 23: return A(f, g, p, D \u2032 * test ) \u2212 A(f, g, p, D \u2032 test )", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "B Background on Causal Inference", "text": "The aim of causal inference is to investigate how a treatment T affects the outcome Y . Confounder X refers to a variable that influences both treatment T and outcome Y . For example, sleeping with shoes on (T ) is strongly associated with waking up with a headache (Y ), but they both have a common cause: drinking the night before (X) (Neal, 2020). In our work, we aim to study how a perturbation (treatment) affects the model's prediction (outcome). However, the latent features and other noise usually act as confounders.\nCausality offers solutions for two questions: 1) how to eliminate the spurious association and isolate the treatment's causal effect; and 2) how varying T affects Y , given both variables are causallyrelated . We leverage both of these properties in our proposed method. Let us now introduce Randomized Controlled Trial and Average Treatment Effect as key concepts in answering the above two questions, respectively.\nRandomized Controlled Trial (RCT). In an RCT, each participant is randomly assigned to either the treatment group or the non-treatment group.\nIn this way, the only difference between the two groups is the treatment they receive. Randomized experiments ideally guarantee that there is no confounding factor, and thus any observed association is actually causal. We operationalize RCT as a perturbation classification task in Section 3.1.\nAverage Treatment Effect (ATE). In Section 3.2, we apply ATE (Holland, 1986) as a measure of learnability. ATE is based on Individual Treatment Effect (ITE, Equation 9), which is the difference of the outcome with and without treatment.\nIT E i = Y i (1) \u2212 Y i (0).(9)\nHere, Y i (1) is the outcome Y of individual i that receives treatment (T = 1), while Y i (0) is the opposite. In the above example, waking up with a headache (Y = 1) with shoes on (T = 1) means Y i (1) = 1.\nWe calculate the Average Treatment Effect (ATE) by taking an average over ITEs:\nAT E = E[Y (1)] \u2212 E[Y (0)]. (10\n)\nATE quantifies how the outcome Y is expected to change if we modify the treatment T from 0 to 1. We provide specific definitions of ITE and ATE in Section 3.2.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "C Alternate Definition of Perturbation Learnability", "text": "In Section 3.2, we propose an accuracy-based identification of ATE. Now we discuss another probability-based identification and compare between them. We can also define the outcome Y of a test example x i as the predicted probability of (pseudo) true label given by the trained model f (\u22c5):\nY i (0) \u2236= P f (L \u2032 = l \u2032 i | X = x i ) \u2208 (0, 1). (11\n)\nSimilarly, the performance outcome Y of a perturbed test data point x * i is:\nY i (1) \u2236= P f (L \u2032 = l \u2032 i | X = x * i ) \u2208 (0, 1). (12)\nFor example, for a test example (x i , l \u2032 i ) which receives treatment (l \u2032 i = 1), the trained model f (\u22c5) predicts its label as 1 with only a small probability 0.1 before treatment (it has not been perturbed yet), and 0.9 after treatment. So the Individual Treatment Effect (ITE, see Equation 9) of this example is calculated as IT E i = Y i (1) \u2212 Y i (0) = 0.9 \u2212 0.1 = 0.8. We then take an average over all the perturbed test examples (half of the test set) as Average Treatment Effect (ATE, see Equation 10), which is exactly the learnability of a perturbation for a model. To clarify, the two operands in Equation 10 are defined as follows:\nE[Y (1)] \u2236= P(f, g, p, D \u2032 * test ).(13)\nIt means the average predicted probability of (pseudo) true label given by the trained model f (\u22c5) on the perturbed test set\nD \u2032 * test . E[Y (0)] \u2236= P(f, g, p, D \u2032 test ).(14)\nSimilarly, this is the average predicted probability on the randomly labeled test set D \u2032 test . Notice that the accuracy-based definition of outcome Y (Equation 6) can also be written in a similar form to the probability-based one (Equation 11):\nY i (0) \u2236= 1 {f (x i )=l \u2032 i } = 1 {P f (L \u2032 =l \u2032 i |X=x i )>0.5} \u2208 {0, 1}.(15\n) because the correctness of the prediction is equal to whether the predicted probability of true (pseudo) label exceeds a certain threshold (i.e., 0.5).\nThe major difference is that, accuracy-based IT E is a discrete variable falling in {\u22121, 0, 1}, while probability-based IT E is a continuous one ranging from -1 to 1. For example, if a model learns to identify a perturbation and thus changes its prediction from wrong (before perturbation) to correct (after perturbation), accuracy-based IT E will be 1 \u2212 0 = 1 while probability-based IT E will be less than 1. That is to say, accuracy-based AT E tends to vary more drastically than probability-based if inconsistent predictions occur more often, and thus can better capture the nuance of perturbation learnability. Empirically, we find that accuracy-based average learnability varies greatly (\u03c3 = 0.375, Table 4) and thus can better distinguish between different model-perturbation pairs than probabilitybased one (\u03c3 = 0.288, Table 4). As a result, we choose accuracy-based ATE as the primary measurement of learnability in this paper.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "D Investigating Learnability at a Specific Perturbation Probability", "text": "Inspired by Precision @ K in Information Retrieval (IR), we propose a similar metric dubbed Learnability @ p, which is the learnability of a perturbation for a model at a specific perturbation probability p. We are primarily interested in whether a selected p can represent the learnability over different perturbation probabilities and correlates well with robustness and post data augmentation \u2206.\nWe calculate the standard deviation (\u03c3) of Learnability @ p and average learnability (log AU C) over all model-perturbation pairs to measure how well it can distinguish between different models and perturbations. Table 4 shows that average learnability is more diversified than all Learnability @ p and diversity (\u03c3) peaks at p = 0.01 for accuracybased/probability-based measurement. Accuracybased Learnability @ p is generally more diversified across models and perturbations than its counterpart. To investigate the strength of the correlations, we also calculate Spearman \u03c1 between accuracy-based/probability-based learnability @ p vs. average learnability/robustness/post data augmentation \u2206 over all model-perturbation pairs. Table 4 shows that generally average learnability has stronger correlation than Learnability @ p. Correlations with both robustness and post data augmentation \u2206 peak at p = 0.02 for accuracybased/probability-based measurements, and the correlations with average learnability (0.816*/0.886*) are also strong at these perturbation probabilities.\nOverall, Learnability @ p with higher standard deviation correlates better with average learnability, robustness and post data augmentation \u2206. Our analysis shows that if p is carefully selected by \u03c3, Learnability @ p is also a promising metric, though not as accurate as average learnability. One advantage of Learnability @ p over average learnability is that it costs less time to obtain learnability at a single perturbation probability.       3) of each model-perturbation pair on QQP dataset. Rows are sorted by average values over all models. The perturbation for which a model is most learnable is highlighted in bold while the following one is underlined.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "E Additional Experiment Results", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgements", "text": "This research is supported by the National Research Foundation, Singapore under its International Research Centres in Singapore Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. We acknowledge the support of NVIDIA Corporation for their donation of the GeForce RTX 3090 GPU that facilitated this research.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Bert: Pre-training of deep bidirectional transformers for language understanding", "journal": "", "year": "2019", "authors": "Jacob Devlin; Ming-Wei Chang; Kenton Lee; Kristina Toutanova"}, {"title": "Improved regularization of convolutional neural networks with cutout", "journal": "", "year": "2017", "authors": "Terrance Devries; W Graham;  Taylor"}, {"title": "Nl-augmenter: A framework for task-sensitive natural language augmentation", "journal": "", "year": "2021", "authors": "Varun Kaustubh D Dhole; Sebastian Gangal; Aadesh Gehrmann; Zhenhao Gupta; Saad Li; Abinaya Mahamood; Simon Mahendiran; Ashish Mille; Samson Srivastava;  Tan"}, {"title": "Cut, paste and learn: Surprisingly easy synthesis for instance detection", "journal": "IEEE Computer Society", "year": "2017", "authors": "Debidatta Dwibedi; Ishan Misra; Martial Hebert"}, {"title": "Text processing like humans do: Visually attacking and shielding NLP systems", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Steffen Eger; G\u00f6zde G\u00fcl\u015fahin; Andreas R\u00fcckl\u00e9; Ji-Ung Lee; Claudia Schulz; Mohsen Mesgar; Krishnkant Swarnkar; Edwin Simpson; Iryna Gurevych"}, {"title": "Principal stratification in causal inference", "journal": "Biometrics", "year": "2002", "authors": "E Constantine; Donald B Frangakis;  Rubin"}, {"title": "SimCSE: Simple contrastive learning of sentence embeddings", "journal": "Association for Computational Linguistics", "year": "2021", "authors": "Tianyu Gao; Xingcheng Yao; Danqi Chen"}, {"title": "Robustness gym: Unifying the nlp evaluation landscape", "journal": "", "year": "2021", "authors": "Karan Goel; Jesse Nazneen Fatema Rajani; Zachary Vig; Mohit Taschdjian; Christopher Bansal;  R\u00e9"}, {"title": "Tradeoffs in data augmentation: An empirical study", "journal": "", "year": "2020", "authors": "Raphael Gontijo-Lopes; Sylvia Smullin; Ethan Ekin Dogus Cubuk;  Dyer"}, {"title": "Statistics and causal inference", "journal": "Journal of the American statistical Association", "year": "1986", "authors": "W Paul;  Holland"}, {"title": "First quora dataset release: Question pairs", "journal": "", "year": "2017", "authors": "Shankar Iyer; Nikhil Dandekar; Kornel Csernai"}, {"title": "Does data augmentation improve generalization in nlp? arXiv preprint", "journal": "", "year": "2020", "authors": "Rohan Jha; Charles Lovering; Ellie Pavlick"}, {"title": "Quantifying the evaluation of heuristic methods for textual data augmentation", "journal": "", "year": "2020", "authors": "Omid Kashefi; Rebecca Hwa"}, {"title": "Contextual augmentation: Data augmentation by words with paradigmatic relations", "journal": "", "year": "2018", "authors": "Sosuke Kobayashi"}, {"title": "Improving neural machine translation robustness via data augmentation: Beyond back-translation", "journal": "", "year": "2019", "authors": "Zhenhao Li; Lucia Specia"}, {"title": "NeuralClassifier: An open-source neural hierarchical multi-label text classification toolkit", "journal": "Association for Computational Linguistics", "year": "2019", "authors": "Liqun Liu; Funan Mu; Pengyu Li; Xin Mu; Jing Tang; Xingsheng Ai; Ran Fu; Lifeng Wang; Xing Zhou"}, {"title": "Recurrent neural network for text classification with multi-task learning", "journal": "", "year": "2016", "authors": "Pengfei Liu; Xipeng Qiu; Xuanjing Huang"}, {"title": "Everything has a cause: Leveraging causal inference in legal text analysis", "journal": "", "year": "2021", "authors": "Xiao Liu; Yansong Da Yin; Yuting Feng; Dongyan Wu;  Zhao"}, {"title": "Roberta: A robustly optimized bert pretraining approach", "journal": "", "year": "2019", "authors": "Yinhan Liu; Myle Ott; Naman Goyal; Jingfei Du; Mandar Joshi; Danqi Chen; Omer Levy; Mike Lewis; Luke Zettlemoyer; Veselin Stoyanov"}, {"title": "Predicting inductive biases of pretrained models", "journal": "", "year": "2020", "authors": "Charles Lovering; Rohan Jha; Tal Linzen; Ellie Pavlick"}, {"title": "Syntactic data augmentation increases robustness to inference heuristics", "journal": "", "year": "2020", "authors": "Junghyun Min; Thomas Mccoy; Dipanjan Das; Emily Pitler; Tal Linzen"}, {"title": "Evaluating the robustness of neural language models to input perturbations", "journal": "", "year": "2021", "authors": "Milad Moradi; Matthias Samwald"}, {"title": "TextAttack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP", "journal": "", "year": "2020", "authors": "John Morris; Eli Lifland; Jin Yong Yoo; Jake Grigsby; Di Jin; Yanjun Qi"}, {"title": "Introduction to causal inference from a machine learning perspective", "journal": "", "year": "2020", "authors": "Brady Neal"}, {"title": "Evaluating robustness to input perturbations for neural machine translation", "journal": "", "year": "2020", "authors": "Xing Niu; Prashant Mathur; Georgiana Dinu; Yaser Al-Onaizan"}, {"title": "Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales", "journal": "", "year": "2005", "authors": "Bo Pang; Lillian Lee"}, {"title": "Perturbation sensitivity analysis to detect unintended model biases", "journal": "", "year": "2019", "authors": "Ben Vinodkumar Prabhakaran; Margaret Hutchinson;  Mitchell"}, {"title": "Beyond accuracy: Behavioral testing of nlp models with checklist", "journal": "", "year": "2020", "authors": "Tongshuang Marco Tulio Ribeiro; Carlos Wu; Sameer Guestrin;  Singh"}, {"title": "Estimating causal effects of treatments in randomized and nonrandomized studies", "journal": "Journal of educational Psychology", "year": "1974", "authors": " Donald B Rubin"}, {"title": "An investigation of why overparameterization exacerbates spurious correlations", "journal": "PMLR", "year": "2020", "authors": "Shiori Sagawa; Aditi Raghunathan; Pang Wei Koh; Percy Liang"}, {"title": "Robust open-vocabulary translation from visual text representations", "journal": "", "year": "2021", "authors": "Elizabeth Salesky; David Etter; Matt Post"}, {"title": "Apac: Augmented pattern classification with neural networks", "journal": "", "year": "2015", "authors": "Ikuro Sato; Hiroki Nishimura; Kensuke Yokoi"}, {"title": "Code-mixing on sesame street: Dawn of the adversarial polyglots", "journal": "", "year": "2021", "authors": "Samson Tan; Shafiq Joty"}, {"title": "Spandana Gella, and He He. 2020. An empirical study on robustness to spurious correlations using pre-trained language models", "journal": "Transactions of the Association for Computational Linguistics", "year": "", "authors": "Lifu Tu; Garima Lalwani"}, {"title": "That's so annoying!!!: A lexical and frame-semantic embedding based data augmentation approach to automatic categorization of annoying behaviors using #petpeeve tweets", "journal": "Association for Computational Linguistics", "year": "2015", "authors": "Yang William; Diyi Wang;  Yang"}, {"title": "Textflint: Unified multilingual robustness evaluation toolkit for natural language processing", "journal": "", "year": "2021", "authors": "Xiao Wang; Qin Liu; Tao Gui; Qi Zhang"}, {"title": "Can neural networks acquire a structural bias from raw linguistic data?", "journal": "", "year": "2020", "authors": "Alex Warstadt;  Samuel R Bowman"}, {"title": "Eda: Easy data augmentation techniques for boosting performance on text classification tasks", "journal": "", "year": "2019", "authors": "Jason Wei; Kai Zou"}, {"title": "Transformers: State-of-the-art natural language processing", "journal": "Association for Computational Linguistics", "year": "2020", "authors": "Thomas Wolf; Lysandre Debut; Victor Sanh; Julien Chaumond; Clement Delangue; Anthony Moi; Pierric Cistac; Tim Rault; Remi Louf; Morgan Funtowicz; Joe Davison; Sam Shleifer; Clara Patrick Von Platen; Yacine Ma; Julien Jernite; Canwen Plu; Teven Le Xu; Sylvain Scao; Mariama Gugger; Quentin Drame; Alexander Lhoest;  Rush"}, {"title": "ConSERT: A contrastive framework for self-supervised sentence representation transfer", "journal": "Long Papers", "year": "2021", "authors": "Yuanmeng Yan; Rumei Li; Sirui Wang; Fuzheng Zhang; Wei Wu; Weiran Xu"}, {"title": "Xlnet: generalized autoregressive pretraining for language understanding", "journal": "", "year": "2019", "authors": "Zhilin Yang; Zihang Dai; Yiming Yang; Jaime Carbonell; Ruslan Salakhutdinov; Quoc V Le"}, {"title": "A causal view on robustness of neural networks. Advances in Neural Information Processing Systems", "journal": "", "year": "2020", "authors": "Cheng Zhang; Kun Zhang; Yingzhen Li"}, {"title": "Character-level convolutional networks for text classification", "journal": "", "year": "2015", "authors": "Xiang Zhang; Junbo Zhao; Yann Lecun"}], "figures": [{"figure_label": "4", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 4 :4Figure4: Causal graph explanation for decoupling perturbation and latent feature with randomization. P is the perturbation and T is the latent feature. L is the original label and Y is the correctness of the predicted label.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "317 3 Y3is later defined in Section 3.2", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure4: Causal graph explanation for decoupling perturbation and latent feature with randomization. P is the perturbation and T is the latent feature. L is the original label and Y is the correctness of the predicted label.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 2 :2Figure 2: An example sentence with different types of perturbations.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 3 :3Figure 3: Learnability of eight perturbations for four NLP models on three datasets, as a function of perturbation probability.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Learn. vs. Robu. vs. Post Aug \u2206    ", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_7", "figure_caption": "Figure 4 :4Figure 4: Linear regression plots of learnability vs. robustness vs. post data augmentation \u2206 on IMDB dataset. Each point in the plots represents a model-perturbation pair. \u03c1 is Spearman correlation. * indicates high significance (p-value < 0.001).", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_8", "figure_caption": "Figure 5: Linear regression plots of learnability vs. robustness vs. post data augmentation \u2206 on YELP dataset. Each point in the plots represents a model-perturbation pair. \u03c1 is Spearman correlation. * indicates high significance (p-value < 0.001).", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "", "figure_data": ""}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_2", "figure_caption": ") datasets.", "figure_data": "\u03c1IMDBYELPQQPAvg. learnability vs. robustness-0.643* -0.821* -0.695*Avg. learnability vs. post aug \u22060.756* 0.846* 0.750*"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Correlations of average learnability vs. robustness vs. post data augmentation \u2206. \u03c1 is Spearman correlation.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Accuracy-based Learnability @ p Probability-based Learnability @ p \u03c3 Avg Learn. Robu. Post Aug \u2206 \u03c3 Avg Learn. Robu. Post Aug \u2206", "figure_data": "Avg. 0.3751.000*-0.643*0.756*0.2881.000*-0.652*0.727*0.001 0.1820.426*-0.2650.2590.1140.367*-0.2790.2880.005 0.2350.637*-0.383*0.522*0.1920.925*-0.620*0.702*0.010.2630.741*-0.530*0.635*0.1920.893*-0.567*0.586*0.020.2570.816*-0.636*0.743*0.1920.886*-0.686*0.690*0.050.2360.279-0.1580.1360.1210.576*-0.371*0.350*0.10.2410.354*-0.1620.1920.1150.543*-0.2880.2580.50.0940.0240.155-0.1790.037-0.0800.114-0.2581.00.011-0.1990.252-0.3320.019-0.2200.294-0.402*"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Standard deviations (\u03c3) of Learnability @ p and Spearman correlations between accuracy-based/probabilitybased learnability @ p vs. average learnability/robustness/post data augmentation \u2206 over all model-perturbation pairs on IMDB dataset.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Average learnability (log AU C of corresponding curve in Figure3) of each model-perturbation pair on YELP dataset. Rows are sorted by average values over all models. The perturbation for which a model is most learnable is highlighted in bold while the following one is underlined.", "figure_data": "PerturbationRoBERTa TextRNN XLNet BERTAverage over modelswhitespace_perturbation0.7320.3990.562 0.7110.601duplicate_punctuations0.7220.8230.640 0.8720.764butter_fingers_perturbation0.5550.8780.775 1.0220.808insert_abbreviation0.8201.4400.960 1.2061.107random_upper_transformation1.0620.6641.392 1.4831.150shuffle_word1.2310.8161.552 1.6231.306visual_attack_letters leet_letters1.429 1.7201.810 1.6761.744 1.608 1.840 1.7181.648 1.738"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Average learnability (log AU C of corresponding curve in Figure", "figure_data": ""}], "doi": "10.18653/v1/N19-1165"}